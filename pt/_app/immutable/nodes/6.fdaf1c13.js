import{s as pe,n as de,o as ce}from"../chunks/scheduler.d586627e.js";import{S as ue,i as fe,g as o,s as n,r as U,A as ke,h as r,f as s,c as l,j as ie,u as v,x as m,k as me,y as Me,a,v as Z,d as V,t as w,w as J}from"../chunks/index.8589a59c.js";import{C as K}from"../chunks/CodeBlock.47c46d2c.js";import{H as O}from"../chunks/Heading.a70e045b.js";function ge(D){let i,B,_,q,p,F,d,ee='O <code>PreTrainedTokenizerFast</code> depende da biblioteca <a href="https://huggingface.co/docs/tokenizers" rel="nofollow">ü§ó Tokenizers</a>. O Tokenizer obtido da biblioteca ü§ó Tokenizers pode ser carregado facilmente pelo ü§ó Transformers.',I,c,te="Antes de entrar nos detalhes, vamos come√ßar criando um tokenizer fict√≠cio em algumas linhas:",N,u,P,f,se="Agora temos um tokenizer treinado nos arquivos que foram definidos. N√≥s podemos continuar usando nessa execu√ß√£o ou salvar em um arquivo JSON para re-utilizar no futuro.",Q,k,W,M,ae="Vamos ver como aproveitar esse objeto tokenizer na biblioteca ü§ó Transformers. A classe <code>PreTrainedTokenizerFast</code> permite uma instancia√ß√£o f√°cil, aceitando o objeto <em>tokenizer</em> instanciado como um argumento:",E,g,X,j,ne='Esse objeto pode ser utilizado com todos os m√©todos compartilhados pelos tokenizers dos ü§ó Transformers! V√° para <a href="main_classes/tokenizer">a p√°gina do tokenizer</a> para mais informa√ß√µes.',G,y,R,z,le="Para carregar um tokenizer de um arquivo JSON vamos primeiro come√ßar salvando nosso tokenizer:",x,h,H,b,oe="A pasta para qual salvamos esse arquivo pode ser passada para o m√©todo de inicializa√ß√£o do <code>PreTrainedTokenizerFast</code> usando o <code>tokenizer_file</code> par√¢metro:",A,T,S,$,re='Esse objeto pode ser utilizado com todos os m√©todos compartilhados pelos tokenizers dos ü§ó Transformers! V√° para <a href="main_classes/tokenizer">a p√°gina do tokenizer</a> para mais informa√ß√µes.',L,C,Y;return p=new O({props:{title:"Usando os Tokenizers do ü§ó Tokenizers",local:"usando-os-tokenizers-do--tokenizers",headingTag:"h1"}}),u=new K({props:{code:"ZnJvbSUyMHRva2VuaXplcnMlMjBpbXBvcnQlMjBUb2tlbml6ZXIlMEFmcm9tJTIwdG9rZW5pemVycy5tb2RlbHMlMjBpbXBvcnQlMjBCUEUlMEFmcm9tJTIwdG9rZW5pemVycy50cmFpbmVycyUyMGltcG9ydCUyMEJwZVRyYWluZXIlMEFmcm9tJTIwdG9rZW5pemVycy5wcmVfdG9rZW5pemVycyUyMGltcG9ydCUyMFdoaXRlc3BhY2UlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBUb2tlbml6ZXIoQlBFKHVua190b2tlbiUzRCUyMiU1QlVOSyU1RCUyMikpJTBBdHJhaW5lciUyMCUzRCUyMEJwZVRyYWluZXIoc3BlY2lhbF90b2tlbnMlM0QlNUIlMjIlNUJVTkslNUQlMjIlMkMlMjAlMjIlNUJDTFMlNUQlMjIlMkMlMjAlMjIlNUJTRVAlNUQlMjIlMkMlMjAlMjIlNUJQQUQlNUQlMjIlMkMlMjAlMjIlNUJNQVNLJTVEJTIyJTVEKSUwQSUwQXRva2VuaXplci5wcmVfdG9rZW5pemVyJTIwJTNEJTIwV2hpdGVzcGFjZSgpJTBBZmlsZXMlMjAlM0QlMjAlNUIuLi4lNUQlMEF0b2tlbml6ZXIudHJhaW4oZmlsZXMlMkMlMjB0cmFpbmVyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pre_tokenizer = Whitespace()
<span class="hljs-meta">&gt;&gt;&gt; </span>files = [...]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.train(files, trainer)`,wrap:!1}}),k=new O({props:{title:"Carregando diretamente de um objeto tokenizer",local:"carregando-diretamente-de-um-objeto-tokenizer",headingTag:"h2"}}),g=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfb2JqZWN0JTNEdG9rZW5pemVyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`,wrap:!1}}),y=new O({props:{title:"Carregando de um arquivo JSON",local:"carregando-de-um-arquivo-json",headingTag:"h2"}}),h=new K({props:{code:"dG9rZW5pemVyLnNhdmUoJTIydG9rZW5pemVyLmpzb24lMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)',wrap:!1}}),T=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfZmlsZSUzRCUyMnRva2VuaXplci5qc29uJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="hljs-string">&quot;tokenizer.json&quot;</span>)`,wrap:!1}}),{c(){i=o("meta"),B=n(),_=o("p"),q=n(),U(p.$$.fragment),F=n(),d=o("p"),d.innerHTML=ee,I=n(),c=o("p"),c.textContent=te,N=n(),U(u.$$.fragment),P=n(),f=o("p"),f.textContent=se,Q=n(),U(k.$$.fragment),W=n(),M=o("p"),M.innerHTML=ae,E=n(),U(g.$$.fragment),X=n(),j=o("p"),j.innerHTML=ne,G=n(),U(y.$$.fragment),R=n(),z=o("p"),z.textContent=le,x=n(),U(h.$$.fragment),H=n(),b=o("p"),b.innerHTML=oe,A=n(),U(T.$$.fragment),S=n(),$=o("p"),$.innerHTML=re,L=n(),C=o("p"),this.h()},l(e){const t=ke("svelte-u9bgzb",document.head);i=r(t,"META",{name:!0,content:!0}),t.forEach(s),B=l(e),_=r(e,"P",{}),ie(_).forEach(s),q=l(e),v(p.$$.fragment,e),F=l(e),d=r(e,"P",{"data-svelte-h":!0}),m(d)!=="svelte-borl69"&&(d.innerHTML=ee),I=l(e),c=r(e,"P",{"data-svelte-h":!0}),m(c)!=="svelte-1q50rcm"&&(c.textContent=te),N=l(e),v(u.$$.fragment,e),P=l(e),f=r(e,"P",{"data-svelte-h":!0}),m(f)!=="svelte-17y7lov"&&(f.textContent=se),Q=l(e),v(k.$$.fragment,e),W=l(e),M=r(e,"P",{"data-svelte-h":!0}),m(M)!=="svelte-11rm4tu"&&(M.innerHTML=ae),E=l(e),v(g.$$.fragment,e),X=l(e),j=r(e,"P",{"data-svelte-h":!0}),m(j)!=="svelte-bzpmh"&&(j.innerHTML=ne),G=l(e),v(y.$$.fragment,e),R=l(e),z=r(e,"P",{"data-svelte-h":!0}),m(z)!=="svelte-vycu7g"&&(z.textContent=le),x=l(e),v(h.$$.fragment,e),H=l(e),b=r(e,"P",{"data-svelte-h":!0}),m(b)!=="svelte-15so4os"&&(b.innerHTML=oe),A=l(e),v(T.$$.fragment,e),S=l(e),$=r(e,"P",{"data-svelte-h":!0}),m($)!=="svelte-bzpmh"&&($.innerHTML=re),L=l(e),C=r(e,"P",{}),ie(C).forEach(s),this.h()},h(){me(i,"name","hf:doc:metadata"),me(i,"content",je)},m(e,t){Me(document.head,i),a(e,B,t),a(e,_,t),a(e,q,t),Z(p,e,t),a(e,F,t),a(e,d,t),a(e,I,t),a(e,c,t),a(e,N,t),Z(u,e,t),a(e,P,t),a(e,f,t),a(e,Q,t),Z(k,e,t),a(e,W,t),a(e,M,t),a(e,E,t),Z(g,e,t),a(e,X,t),a(e,j,t),a(e,G,t),Z(y,e,t),a(e,R,t),a(e,z,t),a(e,x,t),Z(h,e,t),a(e,H,t),a(e,b,t),a(e,A,t),Z(T,e,t),a(e,S,t),a(e,$,t),a(e,L,t),a(e,C,t),Y=!0},p:de,i(e){Y||(V(p.$$.fragment,e),V(u.$$.fragment,e),V(k.$$.fragment,e),V(g.$$.fragment,e),V(y.$$.fragment,e),V(h.$$.fragment,e),V(T.$$.fragment,e),Y=!0)},o(e){w(p.$$.fragment,e),w(u.$$.fragment,e),w(k.$$.fragment,e),w(g.$$.fragment,e),w(y.$$.fragment,e),w(h.$$.fragment,e),w(T.$$.fragment,e),Y=!1},d(e){e&&(s(B),s(_),s(q),s(F),s(d),s(I),s(c),s(N),s(P),s(f),s(Q),s(W),s(M),s(E),s(X),s(j),s(G),s(R),s(z),s(x),s(H),s(b),s(A),s(S),s($),s(L),s(C)),s(i),J(p,e),J(u,e),J(k,e),J(g,e),J(y,e),J(h,e),J(T,e)}}}const je='{"title":"Usando os Tokenizers do ü§ó Tokenizers","local":"usando-os-tokenizers-do--tokenizers","sections":[{"title":"Carregando diretamente de um objeto tokenizer","local":"carregando-diretamente-de-um-objeto-tokenizer","sections":[],"depth":2},{"title":"Carregando de um arquivo JSON","local":"carregando-de-um-arquivo-json","sections":[],"depth":2}],"depth":1}';function ye(D){return ce(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $e extends ue{constructor(i){super(),fe(this,i,ye,ge,pe,{})}}export{$e as component};
