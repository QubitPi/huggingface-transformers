import{s as Ua,f as Os,n as za,o as Ia}from"../chunks/scheduler.36a0863c.js";import{S as Ca,i as Ga,g as p,s as n,r as L,m as o,H as v,A as Za,h as i,f as e,c as l,j as K,u as S,x as u,n as r,B as w,k as h,y as m,a as t,v as F,d as N,t as Y,w as A}from"../chunks/index.f891bdb2.js";import{C as sa}from"../chunks/CodeBlock.3ec784ea.js";import{D as qa}from"../chunks/DocNotebookDropdown.81c1f0fb.js";import{H as aa}from"../chunks/Heading.3fb90772.js";function Ba(ea){let b,as,O,es,_,ts,j,ns,k,ta='La perplejidad, perplexity en inglés (PPL), es una de las métricas más comunes para evaluar modelos de lenguaje. Antes de sumergirnos, debemos tener en cuenta que esta métrica se aplica específicamente a modelos de lenguaje clásicos (a veces llamados modelos autorregresivos o causales) y no está bien definida para modelos de lenguaje enmascarados como BERT (ver <a href="model_summary">resumen del modelo</a>).',ls,g,Ns,ps,wa='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">X = (x_0, x_1, \\dots, x_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner">…</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>',is,ms,ba='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span></span></span></span>',os,rs,fa='<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>PPL</mtext><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo>⁡</mo><mrow><mo fence="true">{</mo><mrow><mo>−</mo><mfrac><mn>1</mn><mi>t</mi></mfrac><munderover><mo>∑</mo><mi>i</mi><mi>t</mi></munderover><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mo fence="true">}</mo></mrow></mrow><annotation encoding="application/x-tex">\\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{&lt;i}) } \\right\\}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">PPL</span></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.0582em;vertical-align:-1.2777em;"></span><span class="mop">exp</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">{</span></span><span class="mord"><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3214em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">t</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.7806em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">}</span></span></span></span></span></span></span>',cs,y,Ys,ds,Ma='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\\log p_\\theta (x_i|x_{&lt;i})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>',us,hs,xa='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">x_{&lt;i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6079em;vertical-align:-0.1774em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span></span></span></span>',gs,ys,U,na='Esto también es equivalente a la exponenciación de la entropía cruzada entre los datos y las predicciones del modelo. Para obtener más intuición sobre la perplejidad y su relación con los Bits Por Carácter (BPC) y la compresión de datos, echa un vistazo a esta <a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/" rel="nofollow">fantástica publicación en el blog de “The Gradient”</a>.',vs,z,ws,I,la="Si no estuviéramos limitados por el tamaño del contexto de un modelo, evaluaríamos la perplejidad (PPL) del modelo auto regresivamente factorizando una secuencia y condicionándonos en toda la subsecuencia precedente en cada paso, como se muestra a continuación.",bs,f,pa,fs,c,As,C,ia="GPT-2",Ds,Ms,Ta='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mrow><mo>&lt;</mo><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_\\theta(x_t|x_{&lt;t})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mrel mtight">&lt;</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1774em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>',xs,Ts,Ja='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6151em;"></span><span class="mord mathnormal">t</span></span></span></span>',Js,_s,d,Ks,js,_a='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>',ks,Us,ja='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>',zs,Is,ka='<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>',Cs,Gs,M,ma,Zs,G,oa="Esto es rápido de calcular, ya que la perplejidad de cada segmento se puede calcular en un solo pase hacia adelante, pero sirve como una aproximación pobre de la perplejidad completamente factorizada y generalmente dará como resultado una PPL más alta (peor) porque el modelo tendrá menos contexto en la mayoría de los pasos de predicción.",qs,Z,ra="En cambio, la PPL de modelos de longitud fija debería evaluarse con una estrategia de ventana deslizante. Esto implica deslizar repetidamente la ventana de contexto para que el modelo tenga más contexto al hacer cada predicción.",Bs,x,ca,Es,q,da=`Esta es una aproximación más cercana a la verdadera descomposición de la probabilidad de la secuencia y generalmente dará como resultado una puntuación más favorable. La desventaja es que requiere un pase hacia adelante separado para cada token en el corpus. Un buen compromiso práctico es emplear una ventana deslizante estratificada, moviendo el contexto con pasos más grandes en lugar de deslizarse de 1 token a la vez. Esto permite que la computación avance mucho más rápido, mientras le da al modelo un contexto amplio para hacer
predicciones en cada paso.`,Ps,B,$s,E,ua="Demostremos este proceso con GPT-2.",Ws,P,Xs,$,ha="Carguemos el conjunto de datos WikiText-2 y evaluemos la perplejidad utilizando algunas estrategias de ventana deslizante diferentes. Dado que este conjunto de datos es pequeño y solo estamos realizando un pase hacia adelante sobre el conjunto, podemos cargar y codificar todo el conjunto de datos en la memoria.",Rs,W,Vs,X,ga="Con 🤗 Transformers, simplemente podemos pasar los <code>input_ids</code> como las <code>labels</code> a nuestro modelo, y la media negativa del log-likelihood para cada token se devuelve como la pérdida. Sin embargo, con nuestro enfoque de ventana deslizante, hay superposición en los tokens que pasamos al modelo en cada iteración. No queremos que el log-likelihood de los tokens que estamos tratando solo como contexto se incluya en nuestra pérdida, por lo que podemos establecer estos objetivos en <code>-100</code> para que se ignoren. El siguiente es un ejemplo de cómo podríamos hacer esto con un paso de <code>512</code>. Esto significa que el modelo tendrá al menos <code>512</code> tokens como contexto al calcular el log-likelihood condicional de cualquier token (siempre que haya <code>512</code> tokens precedentes disponibles para condicionar).",Hs,R,Qs,V,ya=`Ejecuta esto con la longitud de paso igual a la longitud máxima de entrada es equivalente a la estrategia sub óptima,
sin ventana deslizante, que discutimos anteriormente. Cuanto menor sea el paso, más contexto tendrá el modelo para
realizar cada predicción y, por lo general, mejor será la perplejidad informada.`,Ls,H,va=`Cuando ejecutamos lo anterior con <code>stride = 1024</code>, es decir, sin superposición, la PPL resultante es <code>19.44</code>, que es
aproximadamente la misma que la <code>19.93</code> informada en el artículo de GPT-2. Al utilizar <code>stride = 512</code> y, por lo tanto,
emplear nuestra estrategia de ventana deslizante, esto disminuye a <code>16.45</code>. Esto no solo es una puntuación más favorable, sino que se calcula de una manera más cercana a la verdadera descomposición autorregresiva de la probabilidad de una secuencia.`,Ss,ss,Fs;return _=new aa({props:{title:"Perplejidad de los modelos de longitud fija",local:"perplejidad-de-los-modelos-de-longitud-fija",headingTag:"h1"}}),j=new qa({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/es/perplexity.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/es/pytorch/perplexity.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/es/tensorflow/perplexity.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/es/perplexity.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/es/pytorch/perplexity.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/es/tensorflow/perplexity.ipynb"}]}}),z=new aa({props:{title:"Cálculo de PPL con modelos de longitud fija",local:"cálculo-de-ppl-con-modelos-de-longitud-fija",headingTag:"h2"}}),B=new aa({props:{title:"Ejemplo: Cálculo de la perplejidad con GPT-2 en 🤗 Transformers",local:"ejemplo-cálculo-de-la-perplejidad-con-gpt-2-en--transformers",headingTag:"h2"}}),P=new sa({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVDJMTUhlYWRNb2RlbCUyQyUyMEdQVDJUb2tlbml6ZXJGYXN0JTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyb3BlbmFpLWNvbW11bml0eSUyRmdwdDItbGFyZ2UlMjIlMEFtb2RlbCUyMCUzRCUyMEdQVDJMTUhlYWRNb2RlbC5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpLnRvKGRldmljZSklMEF0b2tlbml6ZXIlMjAlM0QlMjBHUFQyVG9rZW5pemVyRmFzdC5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPT2LMHeadModel, GPT2TokenizerFast

device = <span class="hljs-string">&quot;cuda&quot;</span>
model_id = <span class="hljs-string">&quot;openai-community/gpt2-large&quot;</span>
model = GPT2LMHeadModel.from_pretrained(model_id).to(device)
tokenizer = GPT2TokenizerFast.from_pretrained(model_id)`,wrap:!1}}),W=new sa({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBdGVzdCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJ3aWtpdGV4dCUyMiUyQyUyMCUyMndpa2l0ZXh0LTItcmF3LXYxJTIyJTJDJTIwc3BsaXQlM0QlMjJ0ZXN0JTIyKSUwQWVuY29kaW5ncyUyMCUzRCUyMHRva2VuaXplciglMjIlNUNuJTVDbiUyMi5qb2luKHRlc3QlNUIlMjJ0ZXh0JTIyJTVEKSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIp",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

test = load_dataset(<span class="hljs-string">&quot;wikitext&quot;</span>, <span class="hljs-string">&quot;wikitext-2-raw-v1&quot;</span>, split=<span class="hljs-string">&quot;test&quot;</span>)
encodings = tokenizer(<span class="hljs-string">&quot;\\n\\n&quot;</span>.join(test[<span class="hljs-string">&quot;text&quot;</span>]), return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),R=new sa({props:{code:"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",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm

max_length = model.config.n_positions
stride = <span class="hljs-number">512</span>
seq_len = encodings.input_ids.size(<span class="hljs-number">1</span>)

nlls = []
prev_end_loc = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> begin_loc <span class="hljs-keyword">in</span> tqdm(<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, seq_len, stride)):
    end_loc = <span class="hljs-built_in">min</span>(begin_loc + max_length, seq_len)
    trg_len = end_loc - prev_end_loc  <span class="hljs-comment"># puede ser diferente del paso en el último bucle</span>
    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)
    target_ids = input_ids.clone()
    target_ids[:, :-trg_len] = -<span class="hljs-number">100</span>

    <span class="hljs-keyword">with</span> torch.no_grad():
        outputs = model(input_ids, labels=target_ids)

        <span class="hljs-comment"># la pérdida se calcula utilizando CrossEntropyLoss, que promedia las etiquetas válidas</span>
        <span class="hljs-comment"># N.B. el modelo solo calcula la pérdida sobre trg_len - 1 etiquetas, porque desplaza las etiqueta internamente</span>
        <span class="hljs-comment"># a la izquierda por 1.</span>
        neg_log_likelihood = outputs.loss

    nlls.append(neg_log_likelihood)

    prev_end_loc = end_loc
    <span class="hljs-keyword">if</span> end_loc == seq_len:
        <span class="hljs-keyword">break</span>

ppl = torch.exp(torch.stack(nlls).mean())`,wrap:!1}}),{c(){b=p("meta"),as=n(),O=p("p"),es=n(),L(_.$$.fragment),ts=n(),L(j.$$.fragment),ns=n(),k=p("p"),k.innerHTML=ta,ls=n(),g=p("p"),Ns=o("La perplejidad se define como la media negativa exponenciada del log-likelihood de una secuencia. Si tenemos una secuencia tokenizada"),ps=new v(!1),is=o(", entonces la perplejidad de"),ms=new v(!1),os=o(` es,
`),rs=new v(!1),cs=n(),y=p("p"),Ys=o("donde"),ds=new v(!1),us=o(" es el log-likelihood del token i-ésimo condicionado a los tokens precedentes"),hs=new v(!1),gs=o(" según nuestro modelo. De manera intuitiva, se puede pensar en esto como una evaluación de la capacidad del modelo para predecir de manera uniforme entre el conjunto de tokens especificados en un corpus. Es importante destacar que el procedimiento de tokenización tiene un impacto directo en la perplejidad de un modelo, lo cual siempre debe tenerse en cuenta al comparar diferentes modelos."),ys=n(),U=p("p"),U.innerHTML=na,vs=n(),L(z.$$.fragment),ws=n(),I=p("p"),I.textContent=la,bs=n(),f=p("img"),fs=n(),c=p("p"),As=o("Sin embargo, al trabajar con modelos aproximados, generalmente tenemos una restricción en la cantidad de tokens que el modelo puede procesar. La versión más grande de "),C=p("a"),C.textContent=ia,Ds=o(", por ejemplo, tiene una longitud fija de 1024 tokens, por lo que no podemos calcular"),Ms=new v(!1),xs=o(" directamente cuando"),Ts=new v(!1),Js=o(" es mayor que 1024."),_s=n(),d=p("p"),Ks=o("En cambio, la secuencia se divide típicamente en subsecuencias iguales al tamaño máximo de entrada del modelo. Si el tamaño máximo de entrada, de un modelo es"),js=new v(!1),ks=o(", entonces aproximamos la probabilidad de un token"),Us=new v(!1),zs=o(" condicionándonos solo en los"),Is=new v(!1),Cs=o(" tokens que lo preceden en lugar de todo el contexto. Al evaluar la perplejidad del modelo en una secuencia, un enfoque tentador pero sub óptimo es dividir la secuencia en fragmentos independientes y sumar los log-likelihood descompuestos de cada segmento de manera independiente."),Gs=n(),M=p("img"),Zs=n(),G=p("p"),G.textContent=oa,qs=n(),Z=p("p"),Z.textContent=ra,Bs=n(),x=p("img"),Es=n(),q=p("p"),q.textContent=da,Ps=n(),L(B.$$.fragment),$s=n(),E=p("p"),E.textContent=ua,Ws=n(),L(P.$$.fragment),Xs=n(),$=p("p"),$.textContent=ha,Rs=n(),L(W.$$.fragment),Vs=n(),X=p("p"),X.innerHTML=ga,Hs=n(),L(R.$$.fragment),Qs=n(),V=p("p"),V.textContent=ya,Ls=n(),H=p("p"),H.innerHTML=va,Ss=n(),ss=p("p"),this.h()},l(s){const a=Za("svelte-u9bgzb",document.head);b=i(a,"META",{name:!0,content:!0}),a.forEach(e),as=l(s),O=i(s,"P",{}),K(O).forEach(e),es=l(s),S(_.$$.fragment,s),ts=l(s),S(j.$$.fragment,s),ns=l(s),k=i(s,"P",{"data-svelte-h":!0}),u(k)!=="svelte-1reha6e"&&(k.innerHTML=ta),ls=l(s),g=i(s,"P",{});var Q=K(g);Ns=r(Q,"La perplejidad se define como la media negativa exponenciada del log-likelihood de una secuencia. Si tenemos una secuencia tokenizada"),ps=w(Q,!1),is=r(Q,", entonces la perplejidad de"),ms=w(Q,!1),os=r(Q,` es,
`),rs=w(Q,!1),Q.forEach(e),cs=l(s),y=i(s,"P",{});var D=K(y);Ys=r(D,"donde"),ds=w(D,!1),us=r(D," es el log-likelihood del token i-ésimo condicionado a los tokens precedentes"),hs=w(D,!1),gs=r(D," según nuestro modelo. De manera intuitiva, se puede pensar en esto como una evaluación de la capacidad del modelo para predecir de manera uniforme entre el conjunto de tokens especificados en un corpus. Es importante destacar que el procedimiento de tokenización tiene un impacto directo en la perplejidad de un modelo, lo cual siempre debe tenerse en cuenta al comparar diferentes modelos."),D.forEach(e),ys=l(s),U=i(s,"P",{"data-svelte-h":!0}),u(U)!=="svelte-snllqq"&&(U.innerHTML=na),vs=l(s),S(z.$$.fragment,s),ws=l(s),I=i(s,"P",{"data-svelte-h":!0}),u(I)!=="svelte-hq13j"&&(I.textContent=la),bs=l(s),f=i(s,"IMG",{width:!0,alt:!0,src:!0}),fs=l(s),c=i(s,"P",{});var T=K(c);As=r(T,"Sin embargo, al trabajar con modelos aproximados, generalmente tenemos una restricción en la cantidad de tokens que el modelo puede procesar. La versión más grande de "),C=i(T,"A",{href:!0,"data-svelte-h":!0}),u(C)!=="svelte-1kdeo4m"&&(C.textContent=ia),Ds=r(T,", por ejemplo, tiene una longitud fija de 1024 tokens, por lo que no podemos calcular"),Ms=w(T,!1),xs=r(T," directamente cuando"),Ts=w(T,!1),Js=r(T," es mayor que 1024."),T.forEach(e),_s=l(s),d=i(s,"P",{});var J=K(d);Ks=r(J,"En cambio, la secuencia se divide típicamente en subsecuencias iguales al tamaño máximo de entrada del modelo. Si el tamaño máximo de entrada, de un modelo es"),js=w(J,!1),ks=r(J,", entonces aproximamos la probabilidad de un token"),Us=w(J,!1),zs=r(J," condicionándonos solo en los"),Is=w(J,!1),Cs=r(J," tokens que lo preceden en lugar de todo el contexto. Al evaluar la perplejidad del modelo en una secuencia, un enfoque tentador pero sub óptimo es dividir la secuencia en fragmentos independientes y sumar los log-likelihood descompuestos de cada segmento de manera independiente."),J.forEach(e),Gs=l(s),M=i(s,"IMG",{width:!0,alt:!0,src:!0}),Zs=l(s),G=i(s,"P",{"data-svelte-h":!0}),u(G)!=="svelte-cyw0ih"&&(G.textContent=oa),qs=l(s),Z=i(s,"P",{"data-svelte-h":!0}),u(Z)!=="svelte-g4wjdt"&&(Z.textContent=ra),Bs=l(s),x=i(s,"IMG",{width:!0,alt:!0,src:!0}),Es=l(s),q=i(s,"P",{"data-svelte-h":!0}),u(q)!=="svelte-1mrshih"&&(q.textContent=da),Ps=l(s),S(B.$$.fragment,s),$s=l(s),E=i(s,"P",{"data-svelte-h":!0}),u(E)!=="svelte-1glfcjc"&&(E.textContent=ua),Ws=l(s),S(P.$$.fragment,s),Xs=l(s),$=i(s,"P",{"data-svelte-h":!0}),u($)!=="svelte-nmcyfw"&&($.textContent=ha),Rs=l(s),S(W.$$.fragment,s),Vs=l(s),X=i(s,"P",{"data-svelte-h":!0}),u(X)!=="svelte-l65yd5"&&(X.innerHTML=ga),Hs=l(s),S(R.$$.fragment,s),Qs=l(s),V=i(s,"P",{"data-svelte-h":!0}),u(V)!=="svelte-115s0x8"&&(V.textContent=ya),Ls=l(s),H=i(s,"P",{"data-svelte-h":!0}),u(H)!=="svelte-sun4pd"&&(H.innerHTML=va),Ss=l(s),ss=i(s,"P",{}),K(ss).forEach(e),this.h()},h(){h(b,"name","hf:doc:metadata"),h(b,"content",Ea),ps.a=is,ms.a=os,rs.a=null,ds.a=us,hs.a=gs,h(f,"width","600"),h(f,"alt","Full decomposition of a sequence with unlimited context length"),Os(f.src,pa="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_full.gif")||h(f,"src",pa),h(C,"href","model_doc/gpt2"),Ms.a=xs,Ts.a=Js,js.a=ks,Us.a=zs,Is.a=Cs,h(M,"width","600"),h(M,"alt","Suboptimal PPL not taking advantage of full available context"),Os(M.src,ma="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_chunked.gif")||h(M,"src",ma),h(x,"width","600"),h(x,"alt","Sliding window PPL taking advantage of all available context"),Os(x.src,ca="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_sliding.gif")||h(x,"src",ca)},m(s,a){m(document.head,b),t(s,as,a),t(s,O,a),t(s,es,a),F(_,s,a),t(s,ts,a),F(j,s,a),t(s,ns,a),t(s,k,a),t(s,ls,a),t(s,g,a),m(g,Ns),ps.m(wa,g),m(g,is),ms.m(ba,g),m(g,os),rs.m(fa,g),t(s,cs,a),t(s,y,a),m(y,Ys),ds.m(Ma,y),m(y,us),hs.m(xa,y),m(y,gs),t(s,ys,a),t(s,U,a),t(s,vs,a),F(z,s,a),t(s,ws,a),t(s,I,a),t(s,bs,a),t(s,f,a),t(s,fs,a),t(s,c,a),m(c,As),m(c,C),m(c,Ds),Ms.m(Ta,c),m(c,xs),Ts.m(Ja,c),m(c,Js),t(s,_s,a),t(s,d,a),m(d,Ks),js.m(_a,d),m(d,ks),Us.m(ja,d),m(d,zs),Is.m(ka,d),m(d,Cs),t(s,Gs,a),t(s,M,a),t(s,Zs,a),t(s,G,a),t(s,qs,a),t(s,Z,a),t(s,Bs,a),t(s,x,a),t(s,Es,a),t(s,q,a),t(s,Ps,a),F(B,s,a),t(s,$s,a),t(s,E,a),t(s,Ws,a),F(P,s,a),t(s,Xs,a),t(s,$,a),t(s,Rs,a),F(W,s,a),t(s,Vs,a),t(s,X,a),t(s,Hs,a),F(R,s,a),t(s,Qs,a),t(s,V,a),t(s,Ls,a),t(s,H,a),t(s,Ss,a),t(s,ss,a),Fs=!0},p:za,i(s){Fs||(N(_.$$.fragment,s),N(j.$$.fragment,s),N(z.$$.fragment,s),N(B.$$.fragment,s),N(P.$$.fragment,s),N(W.$$.fragment,s),N(R.$$.fragment,s),Fs=!0)},o(s){Y(_.$$.fragment,s),Y(j.$$.fragment,s),Y(z.$$.fragment,s),Y(B.$$.fragment,s),Y(P.$$.fragment,s),Y(W.$$.fragment,s),Y(R.$$.fragment,s),Fs=!1},d(s){s&&(e(as),e(O),e(es),e(ts),e(ns),e(k),e(ls),e(g),e(cs),e(y),e(ys),e(U),e(vs),e(ws),e(I),e(bs),e(f),e(fs),e(c),e(_s),e(d),e(Gs),e(M),e(Zs),e(G),e(qs),e(Z),e(Bs),e(x),e(Es),e(q),e(Ps),e($s),e(E),e(Ws),e(Xs),e($),e(Rs),e(Vs),e(X),e(Hs),e(Qs),e(V),e(Ls),e(H),e(Ss),e(ss)),e(b),A(_,s),A(j,s),A(z,s),A(B,s),A(P,s),A(W,s),A(R,s)}}}const Ea='{"title":"Perplejidad de los modelos de longitud fija","local":"perplejidad-de-los-modelos-de-longitud-fija","sections":[{"title":"Cálculo de PPL con modelos de longitud fija","local":"cálculo-de-ppl-con-modelos-de-longitud-fija","sections":[],"depth":2},{"title":"Ejemplo: Cálculo de la perplejidad con GPT-2 en 🤗 Transformers","local":"ejemplo-cálculo-de-la-perplejidad-con-gpt-2-en--transformers","sections":[],"depth":2}],"depth":1}';function Pa(ea){return Ia(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ha extends Ca{constructor(b){super(),Ga(this,b,Pa,Ba,Ua,{})}}export{Ha as component};
