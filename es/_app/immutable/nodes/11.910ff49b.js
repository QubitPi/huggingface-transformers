import{s as me,n as de,o as ce}from"../chunks/scheduler.36a0863c.js";import{S as ue,i as fe,g as o,s as n,r as U,A as ge,h as r,f as s,c as l,j as ie,u as w,x as p,k as pe,y as ke,a,v as Z,d as V,t as J,w as v}from"../chunks/index.f891bdb2.js";import{C as K}from"../chunks/CodeBlock.3ec784ea.js";import{H as O}from"../chunks/Heading.3fb90772.js";function Me(D){let i,B,_,F,m,I,d,ee=`<code>PreTrainedTokenizerFast</code> depende de la biblioteca <a href="https://huggingface.co/docs/tokenizers" rel="nofollow">ü§ó Tokenizers</a>. Los tokenizadores obtenidos desde la biblioteca ü§ó Tokenizers pueden ser
cargados de forma muy sencilla en los ü§ó Transformers.`,N,c,te="Antes de entrar en detalles, comencemos creando un tokenizador dummy en unas cuantas l√≠neas:",P,u,Q,f,se=`Ahora tenemos un tokenizador entrenado en los archivos que definimos. Lo podemos seguir utilizando en ese entorno de ejecuci√≥n (runtime en ingl√©s), o puedes guardarlo
en un archivo JSON para reutilizarlo en un futuro.`,W,g,E,k,ae=`Veamos c√≥mo utilizar este objeto tokenizador en la biblioteca ü§ó Transformers. La clase
<code>PreTrainedTokenizerFast</code> permite una instanciaci√≥n f√°cil, al aceptar el objeto
<em>tokenizer</em> instanciado como argumento:`,X,M,G,j,ne='Este objeto ya puede ser utilizado con todos los m√©todos compartidos por los tokenizadores de ü§ó Transformers! Visita la <a href="main_classes/tokenizer">p√°gina sobre tokenizadores</a> para m√°s informaci√≥n.',R,y,q,z,le="Para cargar un tokenizador desde un archivo JSON, comencemos por guardar nuestro tokenizador:",L,h,x,b,oe=`La localizaci√≥n (path en ingl√©s) donde este archivo es guardado puede ser incluida en el m√©todo de inicializaci√≥n de <code>PreTrainedTokenizerFast</code>
utilizando el par√°metro <code>tokenizer_file</code>:`,H,T,S,$,re='Este objeto ya puede ser utilizado con todos los m√©todos compartidos por los tokenizadores de ü§ó Transformers! Visita la <a href="main_classes/tokenizer">p√°gina sobre tokenizadores</a> para m√°s informaci√≥n.',A,C,Y;return m=new O({props:{title:"Usa los tokenizadores de ü§ó Tokenizers",local:"usa-los-tokenizadores-de--tokenizers",headingTag:"h1"}}),u=new K({props:{code:"ZnJvbSUyMHRva2VuaXplcnMlMjBpbXBvcnQlMjBUb2tlbml6ZXIlMEFmcm9tJTIwdG9rZW5pemVycy5tb2RlbHMlMjBpbXBvcnQlMjBCUEUlMEFmcm9tJTIwdG9rZW5pemVycy50cmFpbmVycyUyMGltcG9ydCUyMEJwZVRyYWluZXIlMEFmcm9tJTIwdG9rZW5pemVycy5wcmVfdG9rZW5pemVycyUyMGltcG9ydCUyMFdoaXRlc3BhY2UlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBUb2tlbml6ZXIoQlBFKHVua190b2tlbiUzRCUyMiU1QlVOSyU1RCUyMikpJTBBdHJhaW5lciUyMCUzRCUyMEJwZVRyYWluZXIoc3BlY2lhbF90b2tlbnMlM0QlNUIlMjIlNUJVTkslNUQlMjIlMkMlMjAlMjIlNUJDTFMlNUQlMjIlMkMlMjAlMjIlNUJTRVAlNUQlMjIlMkMlMjAlMjIlNUJQQUQlNUQlMjIlMkMlMjAlMjIlNUJNQVNLJTVEJTIyJTVEKSUwQSUwQXRva2VuaXplci5wcmVfdG9rZW5pemVyJTIwJTNEJTIwV2hpdGVzcGFjZSgpJTBBZmlsZXMlMjAlM0QlMjAlNUIuLi4lNUQlMEF0b2tlbml6ZXIudHJhaW4oZmlsZXMlMkMlMjB0cmFpbmVyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pre_tokenizer = Whitespace()
<span class="hljs-meta">&gt;&gt;&gt; </span>files = [...]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.train(files, trainer)`,wrap:!1}}),g=new O({props:{title:"Cargando directamente desde el objeto tokenizador",local:"cargando-directamente-desde-el-objeto-tokenizador",headingTag:"h2"}}),M=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfb2JqZWN0JTNEdG9rZW5pemVyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`,wrap:!1}}),y=new O({props:{title:"Cargando desde un archivo JSON",local:"cargando-desde-un-archivo-json",headingTag:"h2"}}),h=new K({props:{code:"dG9rZW5pemVyLnNhdmUoJTIydG9rZW5pemVyLmpzb24lMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)',wrap:!1}}),T=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfZmlsZSUzRCUyMnRva2VuaXplci5qc29uJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="hljs-string">&quot;tokenizer.json&quot;</span>)`,wrap:!1}}),{c(){i=o("meta"),B=n(),_=o("p"),F=n(),U(m.$$.fragment),I=n(),d=o("p"),d.innerHTML=ee,N=n(),c=o("p"),c.textContent=te,P=n(),U(u.$$.fragment),Q=n(),f=o("p"),f.textContent=se,W=n(),U(g.$$.fragment),E=n(),k=o("p"),k.innerHTML=ae,X=n(),U(M.$$.fragment),G=n(),j=o("p"),j.innerHTML=ne,R=n(),U(y.$$.fragment),q=n(),z=o("p"),z.textContent=le,L=n(),U(h.$$.fragment),x=n(),b=o("p"),b.innerHTML=oe,H=n(),U(T.$$.fragment),S=n(),$=o("p"),$.innerHTML=re,A=n(),C=o("p"),this.h()},l(e){const t=ge("svelte-u9bgzb",document.head);i=r(t,"META",{name:!0,content:!0}),t.forEach(s),B=l(e),_=r(e,"P",{}),ie(_).forEach(s),F=l(e),w(m.$$.fragment,e),I=l(e),d=r(e,"P",{"data-svelte-h":!0}),p(d)!=="svelte-1jbck1w"&&(d.innerHTML=ee),N=l(e),c=r(e,"P",{"data-svelte-h":!0}),p(c)!=="svelte-161ig80"&&(c.textContent=te),P=l(e),w(u.$$.fragment,e),Q=l(e),f=r(e,"P",{"data-svelte-h":!0}),p(f)!=="svelte-1d3xwh8"&&(f.textContent=se),W=l(e),w(g.$$.fragment,e),E=l(e),k=r(e,"P",{"data-svelte-h":!0}),p(k)!=="svelte-v80ac3"&&(k.innerHTML=ae),X=l(e),w(M.$$.fragment,e),G=l(e),j=r(e,"P",{"data-svelte-h":!0}),p(j)!=="svelte-o2trt2"&&(j.innerHTML=ne),R=l(e),w(y.$$.fragment,e),q=l(e),z=r(e,"P",{"data-svelte-h":!0}),p(z)!=="svelte-19no5wd"&&(z.textContent=le),L=l(e),w(h.$$.fragment,e),x=l(e),b=r(e,"P",{"data-svelte-h":!0}),p(b)!=="svelte-9rgu8a"&&(b.innerHTML=oe),H=l(e),w(T.$$.fragment,e),S=l(e),$=r(e,"P",{"data-svelte-h":!0}),p($)!=="svelte-o2trt2"&&($.innerHTML=re),A=l(e),C=r(e,"P",{}),ie(C).forEach(s),this.h()},h(){pe(i,"name","hf:doc:metadata"),pe(i,"content",je)},m(e,t){ke(document.head,i),a(e,B,t),a(e,_,t),a(e,F,t),Z(m,e,t),a(e,I,t),a(e,d,t),a(e,N,t),a(e,c,t),a(e,P,t),Z(u,e,t),a(e,Q,t),a(e,f,t),a(e,W,t),Z(g,e,t),a(e,E,t),a(e,k,t),a(e,X,t),Z(M,e,t),a(e,G,t),a(e,j,t),a(e,R,t),Z(y,e,t),a(e,q,t),a(e,z,t),a(e,L,t),Z(h,e,t),a(e,x,t),a(e,b,t),a(e,H,t),Z(T,e,t),a(e,S,t),a(e,$,t),a(e,A,t),a(e,C,t),Y=!0},p:de,i(e){Y||(V(m.$$.fragment,e),V(u.$$.fragment,e),V(g.$$.fragment,e),V(M.$$.fragment,e),V(y.$$.fragment,e),V(h.$$.fragment,e),V(T.$$.fragment,e),Y=!0)},o(e){J(m.$$.fragment,e),J(u.$$.fragment,e),J(g.$$.fragment,e),J(M.$$.fragment,e),J(y.$$.fragment,e),J(h.$$.fragment,e),J(T.$$.fragment,e),Y=!1},d(e){e&&(s(B),s(_),s(F),s(I),s(d),s(N),s(c),s(P),s(Q),s(f),s(W),s(E),s(k),s(X),s(G),s(j),s(R),s(q),s(z),s(L),s(x),s(b),s(H),s(S),s($),s(A),s(C)),s(i),v(m,e),v(u,e),v(g,e),v(M,e),v(y,e),v(h,e),v(T,e)}}}const je='{"title":"Usa los tokenizadores de ü§ó Tokenizers","local":"usa-los-tokenizadores-de--tokenizers","sections":[{"title":"Cargando directamente desde el objeto tokenizador","local":"cargando-directamente-desde-el-objeto-tokenizador","sections":[],"depth":2},{"title":"Cargando desde un archivo JSON","local":"cargando-desde-un-archivo-json","sections":[],"depth":2}],"depth":1}';function ye(D){return ce(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $e extends ue{constructor(i){super(),fe(this,i,ye,Me,me,{})}}export{$e as component};
