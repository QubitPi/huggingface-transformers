import{s as Ko,o as er,n as tr}from"../chunks/scheduler.36a0863c.js";import{S as ar,i as nr,g as i,s,r as p,A as sr,h as o,f as a,c as l,j as Do,u as d,x as r,k as Oo,y as lr,a as n,v as m,d as u,t as c,w as f}from"../chunks/index.f891bdb2.js";import{T as ir}from"../chunks/Tip.a8272f7f.js";import{Y as Wa}from"../chunks/Youtube.0cbacd3d.js";import{C as b}from"../chunks/CodeBlock.3ec784ea.js";import{H as $}from"../chunks/Heading.3fb90772.js";function or(Na){let g,j="Las etiquetas de cada modelo pueden ser diferentes, as√≠ que aseg√∫rate siempre de revisar la documentaci√≥n de cada modelo para obtener m√°s informaci√≥n sobre sus etiquetas espec√≠ficas.";return{c(){g=i("p"),g.textContent=j},l(v){g=o(v,"P",{"data-svelte-h":!0}),r(g)!=="svelte-1qfyzp2"&&(g.textContent=j)},m(v,Ja){n(v,g,Ja)},p:tr,d(v){v&&a(g)}}}function rr(Na){let g,j,v,Ja,x,Xa,C,ui="Este glosario define t√©rminos generales de aprendizaje autom√°tico y t√©rminos relacionados con ü§ó Transformers para ayudarte a comprender mejor la documentaci√≥n.",Fa,T,Sa,y,Ba,q,ci="La m√°scara de atenci√≥n es un argumento opcional utilizado al agrupar secuencias.",Ya,_,Qa,M,fi="Este argumento indica al modelo qu√© tokens deben recibir atenci√≥n y cu√°les no.",Aa,w,$i="Por ejemplo, considera estas dos secuencias:",Da,k,Oa,z,gi="Las versiones codificadas tienen longitudes diferentes:",Ka,P,en,U,bi="Por lo tanto, no podemos colocarlas juntas en el mismo tensor tal cual. La primera secuencia necesita ser rellenada hasta la longitud de la segunda, o la segunda necesita ser truncada hasta la longitud de la primera.",tn,L,vi="En el primer caso, la lista de IDs se extender√° con los √≠ndices de relleno. Podemos pasar una lista al tokenizador y pedirle que realice el relleno de esta manera:",an,H,nn,Z,hi="Podemos ver que se han agregado ceros a la derecha de la primera oraci√≥n para que tenga la misma longitud que la segunda:",sn,V,ln,E,ji="Esto luego se puede convertir en un tensor en PyTorch o TensorFlow. La m√°scara de atenci√≥n es un tensor binario que indica la posici√≥n de los √≠ndices de relleno para que el modelo no los tenga en cuenta. Para el <code>BertTokenizer</code>, <code>1</code> indica un valor al que se debe prestar atenci√≥n, mientras que <code>0</code> indica un valor de relleno. Esta m√°scara de atenci√≥n est√° en el diccionario devuelto por el tokenizador bajo la clave ‚Äúattention_mask‚Äù:",on,R,rn,G,pn,J,xi='Consulta <a href="#encoder-models">modelos de codificaci√≥n</a> y <a href="#masked-language-modeling-mlm">modelado de lenguaje enmascarado</a>',dn,I,mn,W,Ci='Consulta <a href="#causal-language-modeling">modelado de lenguaje causal</a> y <a href="#decoder-models">modelos de decodificaci√≥n</a>',un,N,cn,X,fn,F,Ti='La columna vertebral, backbone en ingl√©s, es la red (embeddings y layers) que produce los estados ocultos o caracter√≠sticas crudas. Normalmente, est√° conectado a una <a href="#head">cabecera</a>, que acepta las caracter√≠sticas como entrada para hacer una predicci√≥n. Por ejemplo, <code>ViTModel</code> es una columna vertebral sin una cabecera espec√≠fica encima. Otros modelos tambi√©n pueden usar <code>VitModel</code> como columna vertebral, como por ejemplo <a href="model_doc/dpt">DPT</a>.',$n,S,gn,B,bn,Y,yi="Una tarea de preentrenamiento donde el modelo lee los textos en orden y tiene que predecir la siguiente palabra. Generalmente, se realiza leyendo toda la oraci√≥n, pero utilizando una m√°scara dentro del modelo para ocultar los tokens futuros en un cierto paso de tiempo.",vn,Q,hn,A,qi="Las im√°genes a color est√°n compuestas por alguna combinaci√≥n de valores en tres canales: rojo, verde y azul (RGB), y las im√°genes en escala de grises solo tienen un canal. En ü§ó Transformers, el canal puede ser la primera o √∫ltima dimensi√≥n del tensor de una imagen: [<code>n_channels</code>, <code>height</code>, <code>width</code>] o [<code>height</code>, <code>width</code>, <code>n_channels</code>].",jn,D,xn,O,_i="Un algoritmo que permite que un modelo aprenda sin saber exactamente c√≥mo est√°n alineadas la entrada y la salida; CTC calcula la distribuci√≥n de todas las salidas posibles para una entrada dada y elige la salida m√°s probable de ella. CTC se utiliza com√∫nmente en tareas de reconocimiento de voz porque el habla no siempre se alinea perfectamente con la transcripci√≥n debido a diversas razones, como las diferentes velocidades de habla de los oradores.",Cn,K,Tn,ee,Mi="Un tipo de capa en una red neuronal donde la matriz de entrada se multiplica elemento por elemento por una matriz m√°s peque√±a (n√∫cleo o filtro) y los valores se suman en una nueva matriz. Esto se conoce como una operaci√≥n de convoluci√≥n que se repite sobre toda la matriz de entrada. Cada operaci√≥n se aplica a un segmento diferente de la matriz de entrada. Las redes neuronales convolucionales (CNN) se utilizan com√∫nmente en visi√≥n por computadora.",yn,te,qn,ae,_n,ne,wi="T√©cnica de paralelismo para entrenamiento en m√∫ltiples GPUs donde se replica la misma configuraci√≥n varias veces, con cada instancia recibiendo una porci√≥n de datos √∫nica. El procesamiento se realiza en paralelo y todas las configuraciones se sincronizan al final de cada paso de entrenamiento.",Mn,se,ki='Obt√©n m√°s informaci√≥n sobre c√≥mo funciona el DataParallel <a href="perf_train_gpu_many#dataparallel-vs-distributeddataparallel">aqu√≠</a>.',wn,le,kn,ie,zi="Esta entrada es espec√≠fica para modelos codificador-decodificador y contiene los IDs de entrada que se enviar√°n al decodificador. Estas entradas deben usarse para tareas de secuencia a secuencia, como traducci√≥n o resumen, y generalmente se construyen de una manera espec√≠fica para cada modelo.",zn,oe,Pi="La mayor√≠a de los modelos codificador-decodificador (BART, T5) crean sus <code>decoder_input_ids</code> por s√≠ mismos a partir de las <code>labels</code>. En tales modelos, pasar las <code>labels</code> es la forma preferida de manejar el entrenamiento.",Pn,re,Ui="Consulta la documentaci√≥n de cada modelo para ver c√≥mo manejan estos IDs de entrada para el entrenamiento de secuencia a secuencia.",Un,pe,Ln,de,Li="Tambi√©n conocidos como modelos autorregresivos, los modelos decodificadores involucran una tarea de preentrenamiento (llamada modelado de lenguaje causal) donde el modelo lee los textos en orden y tiene que predecir la siguiente palabra. Generalmente, se realiza leyendo la oraci√≥n completa con una m√°scara para ocultar los tokens futuros en un cierto paso de tiempo.",Hn,me,Zn,ue,Vn,ce,Hi="Algoritmos de aprendizaje autom√°tico que utilizan redes neuronales con varias capas.",En,fe,Rn,$e,Gn,ge,Zi='Tambi√©n conocidos como modelos de codificaci√≥n autom√°tica (autoencoding models), los modelos codificadores toman una entrada (como texto o im√°genes) y las transforman en una representaci√≥n num√©rica condensada llamada embedding. A menudo, los modelos codificadores se entrenan previamente utilizando t√©cnicas como el <a href="#masked-language-modeling-mlm">modelado de lenguaje enmascarado</a>, que enmascara partes de la secuencia de entrada y obliga al modelo a crear representaciones m√°s significativas.',Jn,be,In,ve,Wn,he,Nn,je,Vi="El proceso de seleccionar y transformar datos crudos en un conjunto de caracter√≠sticas m√°s informativas y √∫tiles para algoritmos de aprendizaje autom√°tico. Algunos ejemplos de extracci√≥n de caracter√≠sticas incluyen transformar texto crudo en embeddings de palabras y extraer caracter√≠sticas importantes como bordes o formas de datos de im√°genes/videos.",Xn,xe,Fn,Ce,Ei="En cada bloque de atenci√≥n residual en los transformadores, la capa de autoatenci√≥n suele ir seguida de 2 capas de avance. El tama√±o de embedding intermedio de las capas de avance suele ser mayor que el tama√±o oculto del modelo (por ejemplo, para <code>google-bert/bert-base-uncased</code>).",Sn,Te,Ri='Para una entrada de tama√±o <code>[batch_size, sequence_length]</code>, la memoria requerida para almacenar los embeddings intermedios de avance <code>[batch_size, sequence_length, config.intermediate_size]</code> puede representar una gran fracci√≥n del uso de memoria. Los autores de <a href="https://arxiv.org/abs/2001.04451" rel="nofollow">Reformer: The Efficient Transformer</a> observaron que, dado que el c√°lculo es independiente de la dimensi√≥n <code>sequence_length</code>, es matem√°ticamente equivalente calcular los embeddings de salida de ambas capas de avance  <code>[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n</code> individualmente y concatenarlos despu√©s a <code>[batch_size, sequence_length, config.hidden_size]</code> con <code>n = sequence_length</code>, lo que intercambia el aumento del tiempo de c√°lculo por una reducci√≥n en el uso de memoria, pero produce un resultado matem√°ticamente <strong>equivalente</strong>.',Bn,ye,Gi="Para modelos que utilizan la funci√≥n <code>apply_chunking_to_forward()</code>, el <code>chunk_size</code> define el n√∫mero de embeddings de salida que se calculan en paralelo y, por lo tanto, define el equilibrio entre la complejidad de memoria y tiempo. Si <code>chunk_size</code> se establece en 0, no se realiza ninguna fragmentaci√≥n de avance.",Yn,qe,Qn,_e,Ji='El ajuste fino es una forma de transferencia de aprendizaje que implica tomar un modelo entrenado previamente, congelar sus pesos y reemplazar la capa de salida con una nueva <a href="#head">cabecera de modelo</a> reci√©n a√±adida. La cabecera del modelo se entrena en tu conjunto de datos objetivo.',An,Me,Ii='Consulta el tutorial <a href="https://huggingface.co/docs/transformers/training" rel="nofollow">Ajustar finamente un modelo pre-entrenado</a> para obtener m√°s detalles y aprende c√≥mo ajustar finamente modelos con ü§ó Transformers.',Dn,we,On,ke,Kn,ze,Wi="La cabecera del modelo se refiere a la √∫ltima capa de una red neuronal que acepta los estados ocultos crudos y los proyecta en una dimensi√≥n diferente. Hay una cabecera de modelo diferente para cada tarea. Por ejemplo:",es,Pe,Ni='<li><code>GPT2ForSequenceClassification</code> es una cabecera de clasificaci√≥n de secuencias, es decir, una capa lineal, encima del modelo base <code>GPT2Model</code>.</li> <li><code>ViTForImageClassification</code> es una cabecera de clasificaci√≥n de im√°genes, es decir, una capa lineal encima del estado oculto final del token <code>CLS</code>, encima del modelo base <code>ViTModel</code>.</li> <li><code>Wav2Vec2ForCTC</code> es una cabecera de modelado de lenguaje con <a href="#connectionist-temporal-classification-ctc">CTC</a> encima del modelo base <code>Wav2Vec2Model</code>.</li>',ts,Ue,as,Le,ns,He,Xi="Los modelos de Transformers basados en visi√≥n dividen una imagen en parches m√°s peque√±os que se incorporan linealmente y luego se pasan como una secuencia al modelo. Puedes encontrar el <code>patch_size</code> (o resoluci√≥n del modelo) en su configuraci√≥n.",ss,Ze,ls,Ve,Fi='La inferencia es el proceso de evaluar un modelo en nuevos datos despu√©s de completar el entrenamiento. Consulta el tutorial <a href="https://huggingface.co/docs/transformers/pipeline_tutorial" rel="nofollow">Pipeline for inference</a> para aprender c√≥mo realizar inferencias con ü§ó Transformers.',is,Ee,os,Re,Si="Los IDs de entrada a menudo son los √∫nicos par√°metros necesarios que se deben pasar al modelo como entrada. Son √≠ndices de tokens, representaciones num√©ricas de tokens que construyen las secuencias que se utilizar√°n como entrada por el modelo.",rs,Ge,ps,Je,Bi='Cada tokenizador funciona de manera diferente, pero el mecanismo subyacente sigue siendo el mismo. Aqu√≠ tienes un ejemplo utilizando el tokenizador BERT, que es un tokenizador <a href="https://arxiv.org/pdf/1609.08144.pdf" rel="nofollow">WordPiece</a>:',ds,Ie,ms,We,Yi="El tokenizador se encarga de dividir la secuencia en tokens disponibles en el vocabulario del tokenizador.",us,Ne,cs,Xe,Qi=`Los tokens son palabras o sub palabras. Por ejemplo, ‚ÄúVRAM‚Äù no estaba en el vocabulario del modelo, as√≠ que se dividi√≥
en ‚ÄúV‚Äù, ‚ÄúRA‚Äù y ‚ÄúM‚Äù. Para indicar que estos tokens no son palabras separadas sino partes de la misma palabra, se a√±ade un prefijo de doble almohadilla para ‚ÄúRA‚Äù y ‚ÄúM‚Äù:`,fs,Fe,$s,Se,Ai='Estos tokens luego se pueden convertir en IDs que son comprensibles por el modelo. Esto se puede hacer alimentando directamente la oraci√≥n al tokenizador, que aprovecha la implementaci√≥n en Rust de <a href="https://github.com/huggingface/tokenizers" rel="nofollow">ü§ó Tokenizers</a> para obtener un rendimiento √≥ptimo.',gs,Be,bs,Ye,Di="El tokenizador devuelve un diccionario con todos los argumentos necesarios para que su modelo correspondiente funcione correctamente. Los √≠ndices de los tokens est√°n bajo la clave <code>input_ids</code>:",vs,Qe,hs,Ae,Oi="Ten en cuenta que el tokenizador a√±ade autom√°ticamente ‚Äútokens especiales‚Äù (si el modelo asociado depende de ellos), que son IDs especiales que el modelo utiliza en ocasiones.",js,De,Ki="Si descodificamos la secuencia anterior de IDs,",xs,Oe,Cs,Ke,eo="Veremos",Ts,et,ys,tt,to="Porque esta es la forma en que un <code>BertModel</code> espera sus entradas.",qs,at,_s,nt,Ms,st,ao=`Las etiquetas son un argumento opcional que se puede pasar para que el modelo calcule la p√©rdida por s√≠ mismo. Estas etiquetas deber√≠an ser la predicci√≥n esperada del modelo: usar√° la p√©rdida est√°ndar para calcular la p√©rdida entre sus
predicciones y el valor esperado (la etiqueta).`,ws,lt,no="Estas etiquetas son diferentes seg√∫n la cabecera del modelo, por ejemplo:",ks,it,so=`<li>Para modelos de clasificaci√≥n de secuencias (<code>BertForSequenceClassification</code>), el modelo espera un tensor de dimensi√≥n
<code>(batch_size)</code> con cada valor del lote correspondiente a la etiqueta esperada de toda la secuencia.</li> <li>Para modelos de clasificaci√≥n de tokens (<code>BertForTokenClassification</code>), el modelo espera un tensor de dimensi√≥n
<code>(batch_size, seq_length)</code> con cada valor correspondiente a la etiqueta esperada de cada token individual.</li> <li>Para el modelado de lenguaje enmascarado (<code>BertForMaskedLM</code>), el modelo espera un tensor de dimensi√≥n <code>(batch_size, seq_length)</code> con cada valor correspondiente a la etiqueta esperada de cada token individual: las etiquetas son el ID del token enmascarado y los valores deben ignorarse para el resto (generalmente -100).</li> <li>Para tareas de secuencia a secuencia (<code>BartForConditionalGeneration</code>, <code>MBartForConditionalGeneration</code>), el modelo
espera un tensor de dimensi√≥n <code>(batch_size, tgt_seq_length)</code> con cada valor correspondiente a las secuencias objetivo asociadas con cada secuencia de entrada. Durante el entrenamiento, tanto BART como T5 generar√°n internamente los <code>decoder_input_ids</code> y las m√°scaras de atenci√≥n del decodificador. Por lo general, no es necesario suministrarlos. Esto no se aplica a los modelos que aprovechan el marco codificador-decodificador.</li> <li>Para modelos de clasificaci√≥n de im√°genes (<code>ViTForImageClassification</code>), el modelo espera un tensor de dimensi√≥n
<code>(batch_size)</code> con cada valor del lote correspondiente a la etiqueta esperada de cada imagen individual.</li> <li>Para modelos de segmentaci√≥n sem√°ntica (<code>SegformerForSemanticSegmentation</code>), el modelo espera un tensor de dimensi√≥n
<code>(batch_size, height, width)</code> con cada valor del lote correspondiente a la etiqueta esperada de cada p√≠xel individual.</li> <li>Para modelos de detecci√≥n de objetos (<code>DetrForObjectDetection</code>), el modelo espera una lista de diccionarios con claves <code>class_labels</code> y <code>boxes</code> donde cada valor del lote corresponde a la etiqueta esperada y el n√∫mero de cajas delimitadoras de cada imagen individual.</li> <li>Para modelos de reconocimiento autom√°tico de voz (<code>Wav2Vec2ForCTC</code>), el modelo espera un tensor de dimensi√≥n <code>(batch_size, target_length)</code> con cada valor correspondiente a la etiqueta esperada de cada token individual.</li>`,zs,h,Ps,ot,lo="Los modelos base (<code>BertModel</code>) no aceptan etiquetas, ya que estos son los modelos base de transformadores, que simplemente generan caracter√≠sticas.",Us,rt,Ls,pt,io="Un t√©rmino gen√©rico que se refiere a modelos de lenguaje de transformadores (GPT-3, BLOOM, OPT) que fueron entrenados con una gran cantidad de datos. Estos modelos tambi√©n tienden a tener un gran n√∫mero de par√°metros que se pueden aprender (por ejemplo, 175 mil millones para GPT-3).",Hs,dt,Zs,mt,Vs,ut,oo=`Una tarea de preentrenamiento en la que el modelo ve una versi√≥n corrupta de los textos, generalmente hecha
al enmascarar algunos tokens al azar, y tiene que predecir el texto original.`,Es,ct,Rs,ft,ro="Una tarea que combina textos con otro tipo de entradas (por ejemplo: im√°genes).",Gs,$t,Js,gt,Is,bt,po='Todas las tareas relacionadas con la generaci√≥n de texto (por ejemplo: <a href="https://transformer.huggingface.co/" rel="nofollow">Escribe con Transformers</a> o traducci√≥n).',Ws,vt,Ns,ht,mo="Una forma gen√©rica de decir ‚Äútrabajar con textos‚Äù.",Xs,jt,Fs,xt,uo=`Todas las tareas relacionadas con entender lo que hay en un texto (por ejemplo: clasificar el
texto completo o palabras individuales).`,Ss,Ct,Bs,Tt,Ys,yt,co="Un pipeline en ü§ó Transformers es una abstracci√≥n que se refiere a una serie de pasos que se ejecutan en un orden espec√≠fico para preprocesar y transformar datos y devolver una predicci√≥n de un modelo. Algunas etapas de ejemplo que se encuentran en un pipeline pueden ser el preprocesamiento de datos, la extracci√≥n de caracter√≠sticas y la normalizaci√≥n.",Qs,qt,fo='Para obtener m√°s detalles, consulta <a href="https://huggingface.co/docs/transformers/pipeline_tutorial" rel="nofollow">Pipelines para inferencia</a>.',As,_t,Ds,Mt,$o='T√©cnica de paralelismo en la que el modelo se divide verticalmente (a nivel de capa) en varios GPU, de modo que solo una o varias capas del modelo se colocan en un solo GPU. Cada GPU procesa en paralelo diferentes etapas del pipeline y trabaja en un peque√±o fragmento del lote. Obt√©n m√°s informaci√≥n sobre c√≥mo funciona PipelineParallel <a href="perf_train_gpu_many#from-naive-model-parallelism-to-pipeline-parallelism">aqu√≠</a>.',Os,wt,Ks,kt,go="Un tensor de las representaciones num√©ricas de una imagen que se pasa a un modelo. Los valores de p√≠xeles tienen una forma de [<code>batch_size</code>, <code>num_channels</code>, <code>height</code>, <code>width</code>], y se generan a partir de un procesador de im√°genes.",el,zt,tl,Pt,bo="Una operaci√≥n que reduce una matriz a una matriz m√°s peque√±a, ya sea tomando el m√°ximo o el promedio de la dimensi√≥n (o dimensiones) agrupada(s). Las capas de agrupaci√≥n se encuentran com√∫nmente entre capas convolucionales para reducir la representaci√≥n de caracter√≠sticas.",al,Ut,nl,Lt,vo="A diferencia de las RNN que tienen la posici√≥n de cada token incrustada en ellas, los transformers no son conscientes de la posici√≥n de cada token. Por lo tanto, se utilizan los IDs de posici√≥n (<code>position_ids</code>) para que el modelo identifique la posici√≥n de cada token en la lista de tokens.",sl,Ht,ho="Son un par√°metro opcional. Si no se pasan <code>position_ids</code> al modelo, los IDs se crean autom√°ticamente como embeddings de posici√≥n absolutas.",ll,Zt,jo="Los embeddings de posici√≥n absolutas se seleccionan en el rango <code>[0, config.max_position_embeddings - 1]</code>. Algunos modelos utilizan otros tipos de embeddings de posici√≥n, como embeddings de posici√≥n sinusoidales o embeddings de posici√≥n relativas.",il,Vt,ol,Et,xo='La tarea de preparar datos crudos en un formato que pueda ser f√°cilmente consumido por modelos de aprendizaje autom√°tico. Por ejemplo, el texto se preprocesa t√≠picamente mediante la tokenizaci√≥n. Para tener una mejor idea de c√≥mo es el preprocesamiento para otros tipos de entrada, consulta el tutorial <a href="https://huggingface.co/docs/transformers/preprocessing" rel="nofollow">Pre-procesar</a>.',rl,Rt,pl,Gt,Co='Un modelo que ha sido pre-entrenado en algunos datos (por ejemplo, toda Wikipedia). Los m√©todos de preentrenamiento involucran un objetivo auto-supervisado, que puede ser leer el texto e intentar predecir la siguiente palabra (ver <a href="#causal-language-modeling">modelado de lenguaje causal</a>) o enmascarar algunas palabras e intentar predecirlas (ver <a href="#masked-language-modeling-mlm">modelado de lenguaje enmascarado</a>).',dl,Jt,To="Los modelos de habla y visi√≥n tienen sus propios objetivos de pre-entrenamiento. Por ejemplo, Wav2Vec2 es un modelo de habla pre-entrenado en una tarea contrastiva que requiere que el modelo identifique la representaci√≥n de habla ‚Äúverdadera‚Äù de un conjunto de representaciones de habla ‚Äúfalsas‚Äù. Por otro lado, BEiT es un modelo de visi√≥n pre-entrenado en una tarea de modelado de im√°genes enmascaradas que enmascara algunos de los parches de la imagen y requiere que el modelo prediga los parches enmascarados (similar al objetivo de modelado de lenguaje enmascarado).",ml,It,ul,Wt,cl,Nt,yo="Un tipo de modelo que utiliza un bucle sobre una capa para procesar textos.",fl,Xt,$l,Ft,qo="Un subcampo del aprendizaje autom√°tico que se centra en aprender representaciones significativas de datos en bruto. Algunos ejemplos de t√©cnicas de aprendizaje de representaciones incluyen embeddings de palabras, auto-encoders y Redes Generativas Adversarias (Generative Adversarial Networks, GANs).",gl,St,bl,Bt,vl,Yt,_o="Una medida en hercios del n√∫mero de muestras (la se√±al de audio) tomadas por segundo. La tasa de muestreo es el resultado de aproximar una se√±al continua como el habla.",hl,Qt,jl,At,Mo="Cada elemento de la entrada averigua a cu√°les otros elementos de la entrada debe prestar atenci√≥n.",xl,Dt,Cl,Ot,wo='Una categor√≠a de t√©cnicas de aprendizaje autom√°tico en la que un modelo crea su propio objetivo de aprendizaje a partir de datos no etiquetados. Difiere del <a href="#unsupervised-learning">aprendizaje no supervisado</a> y del <a href="#supervised-learning">aprendizaje supervisado</a> en que el proceso de aprendizaje est√° supervisado, pero no expl√≠citamente por el usuario.',Tl,Kt,ko='Un ejemplo de aprendizaje auto-supervisado es el <a href="#masked-language-modeling-mlm">modelado de lenguaje enmascarado</a>, donde un modelo recibe oraciones con una proporci√≥n de sus tokens eliminados y aprende a predecir los tokens faltantes.',yl,ea,ql,ta,zo='Una amplia categor√≠a de t√©cnicas de entrenamiento de aprendizaje autom√°tico que aprovecha una peque√±a cantidad de datos etiquetados con una mayor cantidad de datos no etiquetados para mejorar la precisi√≥n de un modelo, a diferencia del <a href="#supervised-learning">aprendizaje supervisado</a> y del <a href="#unsupervised-learning">aprendizaje no supervisado</a>.',_l,aa,Po="Un ejemplo de un enfoque de aprendizaje semi-supervisado es ‚Äúauto-entrenamiento‚Äù, en el que un modelo se entrena con datos etiquetados y luego se utiliza para hacer predicciones sobre los datos no etiquetados. La porci√≥n de datos no etiquetados que el modelo predice con mayor confianza se agrega al conjunto de datos etiquetados y se utiliza para volver a entrenar el modelo.",Ml,na,wl,sa,Uo=`Modelos que generan una nueva secuencia a partir de una entrada, como modelos de traducci√≥n o modelos de resumen (como
<a href="model_doc/bart">Bart</a> o <a href="model_doc/t5">T5</a>).`,kl,la,zl,ia,Lo='Otro nombre para el concepto fundamental de <a href="#zero-redundancy-optimizer-zero">ZeRO</a> utilizado por varias otras implementaciones de ZeRO.',Pl,oa,Ul,ra,Ho='En <a href="#convolution">convoluci√≥n</a> o <a href="#pooling">agrupaci√≥n</a>, el paso (stride) se refiere a la distancia que recorre el n√∫cleo sobre una matriz. Un paso de 1 significa que el n√∫cleo se mueve un p√≠xel a la vez, y un paso de 2 significa que el n√∫cleo se mueve dos p√≠xeles a la vez.',Ll,pa,Hl,da,Zo="Una forma de entrenamiento de modelos que utiliza directamente datos etiquetados para corregir y dirigir el rendimiento del modelo. Los datos se introducen en el modelo en entrenamiento, y sus predicciones se comparan con las etiquetas conocidas. El modelo actualiza sus pesos en funci√≥n de cu√°n incorrectas fueron sus predicciones, y el proceso se repite para optimizar el rendimiento del modelo.",Zl,ma,Vl,ua,El,ca,Vo=`T√©cnica de paralelismo para entrenamiento en m√∫ltiples GPU en la que cada tensor se divide en m√∫ltiples fragmentos, de modo que en lugar de tener todo el tensor en una sola GPU, cada fragmento del tensor reside en su GPU designada. Los fragmentos se procesan por separado y en paralelo en diferentes GPU y los resultados se sincronizan al final del paso de procesamiento.Esto es lo que a veces se llama paralelismo horizontal, ya que la divisi√≥n ocurre a nivel horizontal.
Obt√©n m√°s informaci√≥n sobre el Paralelismo de Tensores <a href="perf_train_gpu_many#tensor-parallelism">aqu√≠</a>.`,Rl,fa,Gl,$a,Eo="Parte de una oraci√≥n, generalmente una palabra, pero tambi√©n puede ser una sub-palabra (las palabras no comunes a menudo se dividen en sub-palabras) o un s√≠mbolo de puntuaci√≥n.",Jl,ga,Il,ba,Ro="Algunos modelos tienen como objetivo realizar clasificaci√≥n en pares de oraciones o responder preguntas.",Wl,va,Nl,ha,Go=`Estos requieren que dos secuencias diferentes se unan en una √∫nica entrada ‚Äúinput_ids‚Äù, lo cual generalmente se realiza con
la ayuda de tokens especiales, como el token de clasificaci√≥n (<code>[CLS]</code>) y el token separador (<code>[SEP]</code>). Por ejemplo, el modelo BERT construye sus dos secuencias de entrada de la siguiente manera:`,Xl,ja,Fl,xa,Jo="Podemos utilizar nuestro tokenizador para generar autom√°ticamente una oraci√≥n de este tipo al pasar las dos secuencias a <code>tokenizer</code> como dos argumentos (y no como una lista, como antes) de la siguiente manera:",Sl,Ca,Bl,Ta,Io="Que devolver√°:",Yl,ya,Ql,qa,Wo="Esto es suficiente para que algunos modelos comprendan d√≥nde termina una secuencia y comienza otra. Sin embargo, otros modelos, como BERT, tambi√©n utilizan identificadores de tipo de token (tambi√©n llamados identificadores de segmento). Se representan como una m√°scara binaria que identifica los dos tipos de secuencia en el modelo.",Al,_a,No="El tokenizador devuelve esta m√°scara como la entrada ‚Äútoken_type_ids‚Äù:",Dl,Ma,Ol,wa,Xo="La primera secuencia, el ‚Äúcontexto‚Äù utilizado para la pregunta, tiene todos sus tokens representados por un <code>0</code>, mientras que la segunda secuencia, correspondiente a la ‚Äúpregunta‚Äù, tiene todos sus tokens representados por un <code>1</code>.",Kl,ka,Fo="Algunos modelos, como <code>XLNetModel</code>, utilizan un token adicional representado por un <code>2</code>.",ei,za,ti,Pa,So="Una t√©cnica que implica tomar un modelo pre-entrenado y adaptarlo a un conjunto de datos espec√≠fico para tu tarea. En lugar de entrenar un modelo desde cero, puedes aprovechar el conocimiento obtenido de un modelo existente como punto de partida. Esto acelera el proceso de aprendizaje y reduce la cantidad de datos de entrenamiento necesarios.",ai,Ua,ni,La,Bo="Arquitectura de modelo de aprendizaje profundo basada en auto-atenci√≥n (Self-attention).",si,Ha,li,Za,ii,Va,Yo="Una forma de entrenamiento de modelos en la que los datos proporcionados al modelo no est√°n etiquetados. Las t√©cnicas de aprendizaje no supervisado aprovechan la informaci√≥n estad√≠stica de la distribuci√≥n de datos para encontrar patrones √∫tiles para la tarea en cuesti√≥n.",oi,Ea,ri,Ra,pi,Ga,Qo='T√©cnica de paralelismo que realiza la fragmentaci√≥n de los tensores de manera algo similar a <a href="#tensor-parallelism-tp">TensorParallel</a>, excepto que todo el tensor se reconstruye a tiempo para una computaci√≥n hacia adelante o hacia atr√°s, por lo tanto, el modelo no necesita ser modificado. Este m√©todo tambi√©n admite diversas t√©cnicas de descarga para compensar la memoria limitada de la GPU. Obt√©n m√°s informaci√≥n sobre ZeRO <a href="perf_train_gpu_many#zero-data-parallelism">aqu√≠</a>.',di,Ia,mi;return x=new $({props:{title:"Glosario",local:"glosario",headingTag:"h1"}}),T=new $({props:{title:"A",local:"a",headingTag:"h2"}}),y=new $({props:{title:"attention mask",local:"attention-mask",headingTag:"h3"}}),_=new Wa({props:{id:"M6adb1j2jPI"}}),k=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlcnRUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBCZXJ0VG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEFzZXF1ZW5jZV9hJTIwJTNEJTIwJTIyVGhpcyUyMGlzJTIwYSUyMHNob3J0JTIwc2VxdWVuY2UuJTIyJTBBc2VxdWVuY2VfYiUyMCUzRCUyMCUyMlRoaXMlMjBpcyUyMGElMjByYXRoZXIlMjBsb25nJTIwc2VxdWVuY2UuJTIwSXQlMjBpcyUyMGF0JTIwbGVhc3QlMjBsb25nZXIlMjB0aGFuJTIwdGhlJTIwc2VxdWVuY2UlMjBBLiUyMiUwQSUwQWVuY29kZWRfc2VxdWVuY2VfYSUyMCUzRCUyMHRva2VuaXplcihzZXF1ZW5jZV9hKSU1QiUyMmlucHV0X2lkcyUyMiU1RCUwQWVuY29kZWRfc2VxdWVuY2VfYiUyMCUzRCUyMHRva2VuaXplcihzZXF1ZW5jZV9iKSU1QiUyMmlucHV0X2lkcyUyMiU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_a = <span class="hljs-string">&quot;This is a short sequence.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_b = <span class="hljs-string">&quot;This is a rather long sequence. It is at least longer than the sequence A.&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_sequence_a = tokenizer(sequence_a)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_sequence_b = tokenizer(sequence_b)[<span class="hljs-string">&quot;input_ids&quot;</span>]`,wrap:!1}}),P=new b({props:{code:"bGVuKGVuY29kZWRfc2VxdWVuY2VfYSklMkMlMjBsZW4oZW5jb2RlZF9zZXF1ZW5jZV9iKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">len</span>(encoded_sequence_a), <span class="hljs-built_in">len</span>(encoded_sequence_b)
(<span class="hljs-number">8</span>, <span class="hljs-number">19</span>)`,wrap:!1}}),H=new b({props:{code:"cGFkZGVkX3NlcXVlbmNlcyUyMCUzRCUyMHRva2VuaXplciglNUJzZXF1ZW5jZV9hJTJDJTIwc2VxdWVuY2VfYiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>padded_sequences = tokenizer([sequence_a, sequence_b], padding=<span class="hljs-literal">True</span>)',wrap:!1}}),V=new b({props:{code:"cGFkZGVkX3NlcXVlbmNlcyU1QiUyMmlucHV0X2lkcyUyMiU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>padded_sequences[<span class="hljs-string">&quot;input_ids&quot;</span>]
[[<span class="hljs-number">101</span>, <span class="hljs-number">1188</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">170</span>, <span class="hljs-number">1603</span>, <span class="hljs-number">4954</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">101</span>, <span class="hljs-number">1188</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">170</span>, <span class="hljs-number">1897</span>, <span class="hljs-number">1263</span>, <span class="hljs-number">4954</span>, <span class="hljs-number">119</span>, <span class="hljs-number">1135</span>, <span class="hljs-number">1110</span>, <span class="hljs-number">1120</span>, <span class="hljs-number">1655</span>, <span class="hljs-number">2039</span>, <span class="hljs-number">1190</span>, <span class="hljs-number">1103</span>, <span class="hljs-number">4954</span>, <span class="hljs-number">138</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>]]`,wrap:!1}}),R=new b({props:{code:"cGFkZGVkX3NlcXVlbmNlcyU1QiUyMmF0dGVudGlvbl9tYXNrJTIyJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>padded_sequences[<span class="hljs-string">&quot;attention_mask&quot;</span>]
[[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]`,wrap:!1}}),G=new $({props:{title:"autoencoding models",local:"autoencoding-models",headingTag:"h3"}}),I=new $({props:{title:"autoregressive models",local:"autoregressive-models",headingTag:"h3"}}),N=new $({props:{title:"B",local:"b",headingTag:"h2"}}),X=new $({props:{title:"backbone",local:"backbone",headingTag:"h3"}}),S=new $({props:{title:"C",local:"c",headingTag:"h2"}}),B=new $({props:{title:"causal language modeling",local:"causal-language-modeling",headingTag:"h3"}}),Q=new $({props:{title:"channel",local:"channel",headingTag:"h3"}}),D=new $({props:{title:"connectionist temporal classification (CTC)",local:"connectionist-temporal-classification-ctc",headingTag:"h3"}}),K=new $({props:{title:"convolution",local:"convolution",headingTag:"h3"}}),te=new $({props:{title:"D",local:"d",headingTag:"h2"}}),ae=new $({props:{title:"DataParallel (DP)",local:"dataparallel-dp",headingTag:"h3"}}),le=new $({props:{title:"decoder input IDs",local:"decoder-input-ids",headingTag:"h3"}}),pe=new $({props:{title:"decoder models",local:"decoder-models",headingTag:"h3"}}),me=new Wa({props:{id:"d_ixlCubqQw"}}),ue=new $({props:{title:"deep learning (DL)",local:"deep-learning-dl",headingTag:"h3"}}),fe=new $({props:{title:"E",local:"e",headingTag:"h2"}}),$e=new $({props:{title:"encoder models",local:"encoder-models",headingTag:"h3"}}),be=new Wa({props:{id:"H39Z_720T5s"}}),ve=new $({props:{title:"F",local:"f",headingTag:"h2"}}),he=new $({props:{title:"feature extraction",local:"feature-extraction",headingTag:"h3"}}),xe=new $({props:{title:"feed forward chunking",local:"feed-forward-chunking",headingTag:"h3"}}),qe=new $({props:{title:"finetuned models",local:"finetuned-models",headingTag:"h3"}}),we=new $({props:{title:"H",local:"h",headingTag:"h2"}}),ke=new $({props:{title:"head",local:"head",headingTag:"h3"}}),Ue=new $({props:{title:"I",local:"i",headingTag:"h2"}}),Le=new $({props:{title:"image patch",local:"image-patch",headingTag:"h3"}}),Ze=new $({props:{title:"inference",local:"inference",headingTag:"h3"}}),Ee=new $({props:{title:"input IDs",local:"input-ids",headingTag:"h3"}}),Ge=new Wa({props:{id:"VFp38yj8h3A"}}),Ie=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlcnRUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBCZXJ0VG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEFzZXF1ZW5jZSUyMCUzRCUyMCUyMkElMjBUaXRhbiUyMFJUWCUyMGhhcyUyMDI0R0IlMjBvZiUyMFZSQU0lMjI=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>sequence = <span class="hljs-string">&quot;A Titan RTX has 24GB of VRAM&quot;</span>`,wrap:!1}}),Ne=new b({props:{code:"dG9rZW5pemVkX3NlcXVlbmNlJTIwJTNEJTIwdG9rZW5pemVyLnRva2VuaXplKHNlcXVlbmNlKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_sequence = tokenizer.tokenize(sequence)',wrap:!1}}),Fe=new b({props:{code:"cHJpbnQodG9rZW5pemVkX3NlcXVlbmNlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenized_sequence)
[<span class="hljs-string">&#x27;A&#x27;</span>, <span class="hljs-string">&#x27;Titan&#x27;</span>, <span class="hljs-string">&#x27;R&#x27;</span>, <span class="hljs-string">&#x27;##T&#x27;</span>, <span class="hljs-string">&#x27;##X&#x27;</span>, <span class="hljs-string">&#x27;has&#x27;</span>, <span class="hljs-string">&#x27;24&#x27;</span>, <span class="hljs-string">&#x27;##GB&#x27;</span>, <span class="hljs-string">&#x27;of&#x27;</span>, <span class="hljs-string">&#x27;V&#x27;</span>, <span class="hljs-string">&#x27;##RA&#x27;</span>, <span class="hljs-string">&#x27;##M&#x27;</span>]`,wrap:!1}}),Be=new b({props:{code:"aW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHNlcXVlbmNlKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(sequence)',wrap:!1}}),Qe=new b({props:{code:"ZW5jb2RlZF9zZXF1ZW5jZSUyMCUzRCUyMGlucHV0cyU1QiUyMmlucHV0X2lkcyUyMiU1RCUwQXByaW50KGVuY29kZWRfc2VxdWVuY2Up",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_sequence = inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoded_sequence)
[<span class="hljs-number">101</span>, <span class="hljs-number">138</span>, <span class="hljs-number">18696</span>, <span class="hljs-number">155</span>, <span class="hljs-number">1942</span>, <span class="hljs-number">3190</span>, <span class="hljs-number">1144</span>, <span class="hljs-number">1572</span>, <span class="hljs-number">13745</span>, <span class="hljs-number">1104</span>, <span class="hljs-number">159</span>, <span class="hljs-number">9664</span>, <span class="hljs-number">2107</span>, <span class="hljs-number">102</span>]`,wrap:!1}}),Oe=new b({props:{code:"ZGVjb2RlZF9zZXF1ZW5jZSUyMCUzRCUyMHRva2VuaXplci5kZWNvZGUoZW5jb2RlZF9zZXF1ZW5jZSk=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>decoded_sequence = tokenizer.decode(encoded_sequence)',wrap:!1}}),et=new b({props:{code:"cHJpbnQoZGVjb2RlZF9zZXF1ZW5jZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(decoded_sequence)
[CLS] A Titan RTX has 24GB of VRAM [SEP]`,wrap:!1}}),at=new $({props:{title:"L",local:"l",headingTag:"h2"}}),nt=new $({props:{title:"labels",local:"labels",headingTag:"h3"}}),h=new ir({props:{$$slots:{default:[or]},$$scope:{ctx:Na}}}),rt=new $({props:{title:"large language models (LLM)",local:"large-language-models-llm",headingTag:"h3"}}),dt=new $({props:{title:"M",local:"m",headingTag:"h2"}}),mt=new $({props:{title:"masked language modeling (MLM)",local:"masked-language-modeling-mlm",headingTag:"h3"}}),ct=new $({props:{title:"multimodal",local:"multimodal",headingTag:"h3"}}),$t=new $({props:{title:"N",local:"n",headingTag:"h2"}}),gt=new $({props:{title:"Natural language generation (NLG)",local:"natural-language-generation-nlg",headingTag:"h3"}}),vt=new $({props:{title:"Natural language processing (NLP)",local:"natural-language-processing-nlp",headingTag:"h3"}}),jt=new $({props:{title:"Natural language understanding (NLU)",local:"natural-language-understanding-nlu",headingTag:"h3"}}),Ct=new $({props:{title:"P",local:"p",headingTag:"h2"}}),Tt=new $({props:{title:"Pipeline",local:"pipeline",headingTag:"h3"}}),_t=new $({props:{title:"PipelineParallel (PP)",local:"pipelineparallel-pp",headingTag:"h3"}}),wt=new $({props:{title:"pixel values",local:"pixel-values",headingTag:"h3"}}),zt=new $({props:{title:"pooling",local:"pooling",headingTag:"h3"}}),Ut=new $({props:{title:"position IDs",local:"position-ids",headingTag:"h3"}}),Vt=new $({props:{title:"preprocessing",local:"preprocessing",headingTag:"h3"}}),Rt=new $({props:{title:"pretrained model",local:"pretrained-model",headingTag:"h3"}}),It=new $({props:{title:"R",local:"r",headingTag:"h2"}}),Wt=new $({props:{title:"recurrent neural network (RNN)",local:"recurrent-neural-network-rnn",headingTag:"h3"}}),Xt=new $({props:{title:"representation learning",local:"representation-learning",headingTag:"h3"}}),St=new $({props:{title:"S",local:"s",headingTag:"h2"}}),Bt=new $({props:{title:"sampling rate",local:"sampling-rate",headingTag:"h3"}}),Qt=new $({props:{title:"self-attention",local:"self-attention",headingTag:"h3"}}),Dt=new $({props:{title:"self-supervised learning",local:"self-supervised-learning",headingTag:"h3"}}),ea=new $({props:{title:"semi-supervised learning",local:"semi-supervised-learning",headingTag:"h3"}}),na=new $({props:{title:"sequence-to-sequence (seq2seq)",local:"sequence-to-sequence-seq2seq",headingTag:"h3"}}),la=new $({props:{title:"Sharded DDP",local:"sharded-ddp",headingTag:"h3"}}),oa=new $({props:{title:"stride",local:"stride",headingTag:"h3"}}),pa=new $({props:{title:"supervised learning",local:"supervised-learning",headingTag:"h3"}}),ma=new $({props:{title:"T",local:"t",headingTag:"h2"}}),ua=new $({props:{title:"Tensor Parallelism (TP)",local:"tensor-parallelism-tp",headingTag:"h3"}}),fa=new $({props:{title:"token",local:"token",headingTag:"h3"}}),ga=new $({props:{title:"token Type IDs",local:"token-type-ids",headingTag:"h3"}}),va=new Wa({props:{id:"0u3ioSwev3s"}}),ja=new b({props:{code:"JTIzJTIwJTVCQ0xTJTVEJTIwU0VRVUVOQ0VfQSUyMCU1QlNFUCU1RCUyMFNFUVVFTkNFX0IlMjAlNUJTRVAlNUQ=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]</span>',wrap:!1}}),Ca=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlcnRUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBCZXJ0VG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFzZXF1ZW5jZV9hJTIwJTNEJTIwJTIySHVnZ2luZ0ZhY2UlMjBpcyUyMGJhc2VkJTIwaW4lMjBOWUMlMjIlMEFzZXF1ZW5jZV9iJTIwJTNEJTIwJTIyV2hlcmUlMjBpcyUyMEh1Z2dpbmdGYWNlJTIwYmFzZWQlM0YlMjIlMEElMEFlbmNvZGVkX2RpY3QlMjAlM0QlMjB0b2tlbml6ZXIoc2VxdWVuY2VfYSUyQyUyMHNlcXVlbmNlX2IpJTBBZGVjb2RlZCUyMCUzRCUyMHRva2VuaXplci5kZWNvZGUoZW5jb2RlZF9kaWN0JTVCJTIyaW5wdXRfaWRzJTIyJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_a = <span class="hljs-string">&quot;HuggingFace is based in NYC&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sequence_b = <span class="hljs-string">&quot;Where is HuggingFace based?&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dict = tokenizer(sequence_a, sequence_b)
<span class="hljs-meta">&gt;&gt;&gt; </span>decoded = tokenizer.decode(encoded_dict[<span class="hljs-string">&quot;input_ids&quot;</span>])`,wrap:!1}}),ya=new b({props:{code:"cHJpbnQoZGVjb2RlZCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(decoded)
[CLS] HuggingFace <span class="hljs-keyword">is</span> based <span class="hljs-keyword">in</span> NYC [SEP] Where <span class="hljs-keyword">is</span> HuggingFace based? [SEP]`,wrap:!1}}),Ma=new b({props:{code:"ZW5jb2RlZF9kaWN0JTVCJTIydG9rZW5fdHlwZV9pZHMlMjIlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_dict[<span class="hljs-string">&quot;token_type_ids&quot;</span>]
[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]`,wrap:!1}}),za=new $({props:{title:"transfer learning",local:"transfer-learning",headingTag:"h3"}}),Ua=new $({props:{title:"transformer",local:"transformer",headingTag:"h3"}}),Ha=new $({props:{title:"U",local:"u",headingTag:"h2"}}),Za=new $({props:{title:"unsupervised learning",local:"unsupervised-learning",headingTag:"h3"}}),Ea=new $({props:{title:"Z",local:"z",headingTag:"h2"}}),Ra=new $({props:{title:"Zero Redundancy Optimizer (ZeRO)",local:"zero-redundancy-optimizer-zero",headingTag:"h3"}}),{c(){g=i("meta"),j=s(),v=i("p"),Ja=s(),p(x.$$.fragment),Xa=s(),C=i("p"),C.textContent=ui,Fa=s(),p(T.$$.fragment),Sa=s(),p(y.$$.fragment),Ba=s(),q=i("p"),q.textContent=ci,Ya=s(),p(_.$$.fragment),Qa=s(),M=i("p"),M.textContent=fi,Aa=s(),w=i("p"),w.textContent=$i,Da=s(),p(k.$$.fragment),Oa=s(),z=i("p"),z.textContent=gi,Ka=s(),p(P.$$.fragment),en=s(),U=i("p"),U.textContent=bi,tn=s(),L=i("p"),L.textContent=vi,an=s(),p(H.$$.fragment),nn=s(),Z=i("p"),Z.textContent=hi,sn=s(),p(V.$$.fragment),ln=s(),E=i("p"),E.innerHTML=ji,on=s(),p(R.$$.fragment),rn=s(),p(G.$$.fragment),pn=s(),J=i("p"),J.innerHTML=xi,dn=s(),p(I.$$.fragment),mn=s(),W=i("p"),W.innerHTML=Ci,un=s(),p(N.$$.fragment),cn=s(),p(X.$$.fragment),fn=s(),F=i("p"),F.innerHTML=Ti,$n=s(),p(S.$$.fragment),gn=s(),p(B.$$.fragment),bn=s(),Y=i("p"),Y.textContent=yi,vn=s(),p(Q.$$.fragment),hn=s(),A=i("p"),A.innerHTML=qi,jn=s(),p(D.$$.fragment),xn=s(),O=i("p"),O.textContent=_i,Cn=s(),p(K.$$.fragment),Tn=s(),ee=i("p"),ee.textContent=Mi,yn=s(),p(te.$$.fragment),qn=s(),p(ae.$$.fragment),_n=s(),ne=i("p"),ne.textContent=wi,Mn=s(),se=i("p"),se.innerHTML=ki,wn=s(),p(le.$$.fragment),kn=s(),ie=i("p"),ie.textContent=zi,zn=s(),oe=i("p"),oe.innerHTML=Pi,Pn=s(),re=i("p"),re.textContent=Ui,Un=s(),p(pe.$$.fragment),Ln=s(),de=i("p"),de.textContent=Li,Hn=s(),p(me.$$.fragment),Zn=s(),p(ue.$$.fragment),Vn=s(),ce=i("p"),ce.textContent=Hi,En=s(),p(fe.$$.fragment),Rn=s(),p($e.$$.fragment),Gn=s(),ge=i("p"),ge.innerHTML=Zi,Jn=s(),p(be.$$.fragment),In=s(),p(ve.$$.fragment),Wn=s(),p(he.$$.fragment),Nn=s(),je=i("p"),je.textContent=Vi,Xn=s(),p(xe.$$.fragment),Fn=s(),Ce=i("p"),Ce.innerHTML=Ei,Sn=s(),Te=i("p"),Te.innerHTML=Ri,Bn=s(),ye=i("p"),ye.innerHTML=Gi,Yn=s(),p(qe.$$.fragment),Qn=s(),_e=i("p"),_e.innerHTML=Ji,An=s(),Me=i("p"),Me.innerHTML=Ii,Dn=s(),p(we.$$.fragment),On=s(),p(ke.$$.fragment),Kn=s(),ze=i("p"),ze.textContent=Wi,es=s(),Pe=i("ul"),Pe.innerHTML=Ni,ts=s(),p(Ue.$$.fragment),as=s(),p(Le.$$.fragment),ns=s(),He=i("p"),He.innerHTML=Xi,ss=s(),p(Ze.$$.fragment),ls=s(),Ve=i("p"),Ve.innerHTML=Fi,is=s(),p(Ee.$$.fragment),os=s(),Re=i("p"),Re.textContent=Si,rs=s(),p(Ge.$$.fragment),ps=s(),Je=i("p"),Je.innerHTML=Bi,ds=s(),p(Ie.$$.fragment),ms=s(),We=i("p"),We.textContent=Yi,us=s(),p(Ne.$$.fragment),cs=s(),Xe=i("p"),Xe.textContent=Qi,fs=s(),p(Fe.$$.fragment),$s=s(),Se=i("p"),Se.innerHTML=Ai,gs=s(),p(Be.$$.fragment),bs=s(),Ye=i("p"),Ye.innerHTML=Di,vs=s(),p(Qe.$$.fragment),hs=s(),Ae=i("p"),Ae.textContent=Oi,js=s(),De=i("p"),De.textContent=Ki,xs=s(),p(Oe.$$.fragment),Cs=s(),Ke=i("p"),Ke.textContent=eo,Ts=s(),p(et.$$.fragment),ys=s(),tt=i("p"),tt.innerHTML=to,qs=s(),p(at.$$.fragment),_s=s(),p(nt.$$.fragment),Ms=s(),st=i("p"),st.textContent=ao,ws=s(),lt=i("p"),lt.textContent=no,ks=s(),it=i("ul"),it.innerHTML=so,zs=s(),p(h.$$.fragment),Ps=s(),ot=i("p"),ot.innerHTML=lo,Us=s(),p(rt.$$.fragment),Ls=s(),pt=i("p"),pt.textContent=io,Hs=s(),p(dt.$$.fragment),Zs=s(),p(mt.$$.fragment),Vs=s(),ut=i("p"),ut.textContent=oo,Es=s(),p(ct.$$.fragment),Rs=s(),ft=i("p"),ft.textContent=ro,Gs=s(),p($t.$$.fragment),Js=s(),p(gt.$$.fragment),Is=s(),bt=i("p"),bt.innerHTML=po,Ws=s(),p(vt.$$.fragment),Ns=s(),ht=i("p"),ht.textContent=mo,Xs=s(),p(jt.$$.fragment),Fs=s(),xt=i("p"),xt.textContent=uo,Ss=s(),p(Ct.$$.fragment),Bs=s(),p(Tt.$$.fragment),Ys=s(),yt=i("p"),yt.textContent=co,Qs=s(),qt=i("p"),qt.innerHTML=fo,As=s(),p(_t.$$.fragment),Ds=s(),Mt=i("p"),Mt.innerHTML=$o,Os=s(),p(wt.$$.fragment),Ks=s(),kt=i("p"),kt.innerHTML=go,el=s(),p(zt.$$.fragment),tl=s(),Pt=i("p"),Pt.textContent=bo,al=s(),p(Ut.$$.fragment),nl=s(),Lt=i("p"),Lt.innerHTML=vo,sl=s(),Ht=i("p"),Ht.innerHTML=ho,ll=s(),Zt=i("p"),Zt.innerHTML=jo,il=s(),p(Vt.$$.fragment),ol=s(),Et=i("p"),Et.innerHTML=xo,rl=s(),p(Rt.$$.fragment),pl=s(),Gt=i("p"),Gt.innerHTML=Co,dl=s(),Jt=i("p"),Jt.textContent=To,ml=s(),p(It.$$.fragment),ul=s(),p(Wt.$$.fragment),cl=s(),Nt=i("p"),Nt.textContent=yo,fl=s(),p(Xt.$$.fragment),$l=s(),Ft=i("p"),Ft.textContent=qo,gl=s(),p(St.$$.fragment),bl=s(),p(Bt.$$.fragment),vl=s(),Yt=i("p"),Yt.textContent=_o,hl=s(),p(Qt.$$.fragment),jl=s(),At=i("p"),At.textContent=Mo,xl=s(),p(Dt.$$.fragment),Cl=s(),Ot=i("p"),Ot.innerHTML=wo,Tl=s(),Kt=i("p"),Kt.innerHTML=ko,yl=s(),p(ea.$$.fragment),ql=s(),ta=i("p"),ta.innerHTML=zo,_l=s(),aa=i("p"),aa.textContent=Po,Ml=s(),p(na.$$.fragment),wl=s(),sa=i("p"),sa.innerHTML=Uo,kl=s(),p(la.$$.fragment),zl=s(),ia=i("p"),ia.innerHTML=Lo,Pl=s(),p(oa.$$.fragment),Ul=s(),ra=i("p"),ra.innerHTML=Ho,Ll=s(),p(pa.$$.fragment),Hl=s(),da=i("p"),da.textContent=Zo,Zl=s(),p(ma.$$.fragment),Vl=s(),p(ua.$$.fragment),El=s(),ca=i("p"),ca.innerHTML=Vo,Rl=s(),p(fa.$$.fragment),Gl=s(),$a=i("p"),$a.textContent=Eo,Jl=s(),p(ga.$$.fragment),Il=s(),ba=i("p"),ba.textContent=Ro,Wl=s(),p(va.$$.fragment),Nl=s(),ha=i("p"),ha.innerHTML=Go,Xl=s(),p(ja.$$.fragment),Fl=s(),xa=i("p"),xa.innerHTML=Jo,Sl=s(),p(Ca.$$.fragment),Bl=s(),Ta=i("p"),Ta.textContent=Io,Yl=s(),p(ya.$$.fragment),Ql=s(),qa=i("p"),qa.textContent=Wo,Al=s(),_a=i("p"),_a.textContent=No,Dl=s(),p(Ma.$$.fragment),Ol=s(),wa=i("p"),wa.innerHTML=Xo,Kl=s(),ka=i("p"),ka.innerHTML=Fo,ei=s(),p(za.$$.fragment),ti=s(),Pa=i("p"),Pa.textContent=So,ai=s(),p(Ua.$$.fragment),ni=s(),La=i("p"),La.textContent=Bo,si=s(),p(Ha.$$.fragment),li=s(),p(Za.$$.fragment),ii=s(),Va=i("p"),Va.textContent=Yo,oi=s(),p(Ea.$$.fragment),ri=s(),p(Ra.$$.fragment),pi=s(),Ga=i("p"),Ga.innerHTML=Qo,di=s(),Ia=i("p"),this.h()},l(e){const t=sr("svelte-u9bgzb",document.head);g=o(t,"META",{name:!0,content:!0}),t.forEach(a),j=l(e),v=o(e,"P",{}),Do(v).forEach(a),Ja=l(e),d(x.$$.fragment,e),Xa=l(e),C=o(e,"P",{"data-svelte-h":!0}),r(C)!=="svelte-ut9f2w"&&(C.textContent=ui),Fa=l(e),d(T.$$.fragment,e),Sa=l(e),d(y.$$.fragment,e),Ba=l(e),q=o(e,"P",{"data-svelte-h":!0}),r(q)!=="svelte-1mi96p6"&&(q.textContent=ci),Ya=l(e),d(_.$$.fragment,e),Qa=l(e),M=o(e,"P",{"data-svelte-h":!0}),r(M)!=="svelte-uvy76z"&&(M.textContent=fi),Aa=l(e),w=o(e,"P",{"data-svelte-h":!0}),r(w)!=="svelte-1wvs8oy"&&(w.textContent=$i),Da=l(e),d(k.$$.fragment,e),Oa=l(e),z=o(e,"P",{"data-svelte-h":!0}),r(z)!=="svelte-1vndeee"&&(z.textContent=gi),Ka=l(e),d(P.$$.fragment,e),en=l(e),U=o(e,"P",{"data-svelte-h":!0}),r(U)!=="svelte-opci3x"&&(U.textContent=bi),tn=l(e),L=o(e,"P",{"data-svelte-h":!0}),r(L)!=="svelte-1q9d2v"&&(L.textContent=vi),an=l(e),d(H.$$.fragment,e),nn=l(e),Z=o(e,"P",{"data-svelte-h":!0}),r(Z)!=="svelte-pabfsa"&&(Z.textContent=hi),sn=l(e),d(V.$$.fragment,e),ln=l(e),E=o(e,"P",{"data-svelte-h":!0}),r(E)!=="svelte-1o12pow"&&(E.innerHTML=ji),on=l(e),d(R.$$.fragment,e),rn=l(e),d(G.$$.fragment,e),pn=l(e),J=o(e,"P",{"data-svelte-h":!0}),r(J)!=="svelte-uer5da"&&(J.innerHTML=xi),dn=l(e),d(I.$$.fragment,e),mn=l(e),W=o(e,"P",{"data-svelte-h":!0}),r(W)!=="svelte-1y0da4j"&&(W.innerHTML=Ci),un=l(e),d(N.$$.fragment,e),cn=l(e),d(X.$$.fragment,e),fn=l(e),F=o(e,"P",{"data-svelte-h":!0}),r(F)!=="svelte-13xb649"&&(F.innerHTML=Ti),$n=l(e),d(S.$$.fragment,e),gn=l(e),d(B.$$.fragment,e),bn=l(e),Y=o(e,"P",{"data-svelte-h":!0}),r(Y)!=="svelte-1b8g7ls"&&(Y.textContent=yi),vn=l(e),d(Q.$$.fragment,e),hn=l(e),A=o(e,"P",{"data-svelte-h":!0}),r(A)!=="svelte-1cay77e"&&(A.innerHTML=qi),jn=l(e),d(D.$$.fragment,e),xn=l(e),O=o(e,"P",{"data-svelte-h":!0}),r(O)!=="svelte-1x6fpjk"&&(O.textContent=_i),Cn=l(e),d(K.$$.fragment,e),Tn=l(e),ee=o(e,"P",{"data-svelte-h":!0}),r(ee)!=="svelte-1uccm8f"&&(ee.textContent=Mi),yn=l(e),d(te.$$.fragment,e),qn=l(e),d(ae.$$.fragment,e),_n=l(e),ne=o(e,"P",{"data-svelte-h":!0}),r(ne)!=="svelte-1du4ijv"&&(ne.textContent=wi),Mn=l(e),se=o(e,"P",{"data-svelte-h":!0}),r(se)!=="svelte-1w8a52v"&&(se.innerHTML=ki),wn=l(e),d(le.$$.fragment,e),kn=l(e),ie=o(e,"P",{"data-svelte-h":!0}),r(ie)!=="svelte-wrc0xq"&&(ie.textContent=zi),zn=l(e),oe=o(e,"P",{"data-svelte-h":!0}),r(oe)!=="svelte-1q0ofqf"&&(oe.innerHTML=Pi),Pn=l(e),re=o(e,"P",{"data-svelte-h":!0}),r(re)!=="svelte-1apzmv2"&&(re.textContent=Ui),Un=l(e),d(pe.$$.fragment,e),Ln=l(e),de=o(e,"P",{"data-svelte-h":!0}),r(de)!=="svelte-e16kdo"&&(de.textContent=Li),Hn=l(e),d(me.$$.fragment,e),Zn=l(e),d(ue.$$.fragment,e),Vn=l(e),ce=o(e,"P",{"data-svelte-h":!0}),r(ce)!=="svelte-1ix3h0j"&&(ce.textContent=Hi),En=l(e),d(fe.$$.fragment,e),Rn=l(e),d($e.$$.fragment,e),Gn=l(e),ge=o(e,"P",{"data-svelte-h":!0}),r(ge)!=="svelte-9f2t64"&&(ge.innerHTML=Zi),Jn=l(e),d(be.$$.fragment,e),In=l(e),d(ve.$$.fragment,e),Wn=l(e),d(he.$$.fragment,e),Nn=l(e),je=o(e,"P",{"data-svelte-h":!0}),r(je)!=="svelte-1m4vdtb"&&(je.textContent=Vi),Xn=l(e),d(xe.$$.fragment,e),Fn=l(e),Ce=o(e,"P",{"data-svelte-h":!0}),r(Ce)!=="svelte-1lixoi4"&&(Ce.innerHTML=Ei),Sn=l(e),Te=o(e,"P",{"data-svelte-h":!0}),r(Te)!=="svelte-jbhg0m"&&(Te.innerHTML=Ri),Bn=l(e),ye=o(e,"P",{"data-svelte-h":!0}),r(ye)!=="svelte-bj63nq"&&(ye.innerHTML=Gi),Yn=l(e),d(qe.$$.fragment,e),Qn=l(e),_e=o(e,"P",{"data-svelte-h":!0}),r(_e)!=="svelte-1bpwfoh"&&(_e.innerHTML=Ji),An=l(e),Me=o(e,"P",{"data-svelte-h":!0}),r(Me)!=="svelte-bqskd7"&&(Me.innerHTML=Ii),Dn=l(e),d(we.$$.fragment,e),On=l(e),d(ke.$$.fragment,e),Kn=l(e),ze=o(e,"P",{"data-svelte-h":!0}),r(ze)!=="svelte-elip6s"&&(ze.textContent=Wi),es=l(e),Pe=o(e,"UL",{"data-svelte-h":!0}),r(Pe)!=="svelte-r0u0s6"&&(Pe.innerHTML=Ni),ts=l(e),d(Ue.$$.fragment,e),as=l(e),d(Le.$$.fragment,e),ns=l(e),He=o(e,"P",{"data-svelte-h":!0}),r(He)!=="svelte-o87h1w"&&(He.innerHTML=Xi),ss=l(e),d(Ze.$$.fragment,e),ls=l(e),Ve=o(e,"P",{"data-svelte-h":!0}),r(Ve)!=="svelte-11byj6a"&&(Ve.innerHTML=Fi),is=l(e),d(Ee.$$.fragment,e),os=l(e),Re=o(e,"P",{"data-svelte-h":!0}),r(Re)!=="svelte-1co8pzq"&&(Re.textContent=Si),rs=l(e),d(Ge.$$.fragment,e),ps=l(e),Je=o(e,"P",{"data-svelte-h":!0}),r(Je)!=="svelte-kx44et"&&(Je.innerHTML=Bi),ds=l(e),d(Ie.$$.fragment,e),ms=l(e),We=o(e,"P",{"data-svelte-h":!0}),r(We)!=="svelte-rp22k"&&(We.textContent=Yi),us=l(e),d(Ne.$$.fragment,e),cs=l(e),Xe=o(e,"P",{"data-svelte-h":!0}),r(Xe)!=="svelte-s79j83"&&(Xe.textContent=Qi),fs=l(e),d(Fe.$$.fragment,e),$s=l(e),Se=o(e,"P",{"data-svelte-h":!0}),r(Se)!=="svelte-l080jr"&&(Se.innerHTML=Ai),gs=l(e),d(Be.$$.fragment,e),bs=l(e),Ye=o(e,"P",{"data-svelte-h":!0}),r(Ye)!=="svelte-hyrhjp"&&(Ye.innerHTML=Di),vs=l(e),d(Qe.$$.fragment,e),hs=l(e),Ae=o(e,"P",{"data-svelte-h":!0}),r(Ae)!=="svelte-kxrahe"&&(Ae.textContent=Oi),js=l(e),De=o(e,"P",{"data-svelte-h":!0}),r(De)!=="svelte-148ed3p"&&(De.textContent=Ki),xs=l(e),d(Oe.$$.fragment,e),Cs=l(e),Ke=o(e,"P",{"data-svelte-h":!0}),r(Ke)!=="svelte-1he7qjh"&&(Ke.textContent=eo),Ts=l(e),d(et.$$.fragment,e),ys=l(e),tt=o(e,"P",{"data-svelte-h":!0}),r(tt)!=="svelte-76cdre"&&(tt.innerHTML=to),qs=l(e),d(at.$$.fragment,e),_s=l(e),d(nt.$$.fragment,e),Ms=l(e),st=o(e,"P",{"data-svelte-h":!0}),r(st)!=="svelte-1g5swrc"&&(st.textContent=ao),ws=l(e),lt=o(e,"P",{"data-svelte-h":!0}),r(lt)!=="svelte-8e61a6"&&(lt.textContent=no),ks=l(e),it=o(e,"UL",{"data-svelte-h":!0}),r(it)!=="svelte-154mzkf"&&(it.innerHTML=so),zs=l(e),d(h.$$.fragment,e),Ps=l(e),ot=o(e,"P",{"data-svelte-h":!0}),r(ot)!=="svelte-1v84vb7"&&(ot.innerHTML=lo),Us=l(e),d(rt.$$.fragment,e),Ls=l(e),pt=o(e,"P",{"data-svelte-h":!0}),r(pt)!=="svelte-7d9qeg"&&(pt.textContent=io),Hs=l(e),d(dt.$$.fragment,e),Zs=l(e),d(mt.$$.fragment,e),Vs=l(e),ut=o(e,"P",{"data-svelte-h":!0}),r(ut)!=="svelte-59qo3g"&&(ut.textContent=oo),Es=l(e),d(ct.$$.fragment,e),Rs=l(e),ft=o(e,"P",{"data-svelte-h":!0}),r(ft)!=="svelte-kiyktm"&&(ft.textContent=ro),Gs=l(e),d($t.$$.fragment,e),Js=l(e),d(gt.$$.fragment,e),Is=l(e),bt=o(e,"P",{"data-svelte-h":!0}),r(bt)!=="svelte-1pxabak"&&(bt.innerHTML=po),Ws=l(e),d(vt.$$.fragment,e),Ns=l(e),ht=o(e,"P",{"data-svelte-h":!0}),r(ht)!=="svelte-4ggqfk"&&(ht.textContent=mo),Xs=l(e),d(jt.$$.fragment,e),Fs=l(e),xt=o(e,"P",{"data-svelte-h":!0}),r(xt)!=="svelte-p8pik9"&&(xt.textContent=uo),Ss=l(e),d(Ct.$$.fragment,e),Bs=l(e),d(Tt.$$.fragment,e),Ys=l(e),yt=o(e,"P",{"data-svelte-h":!0}),r(yt)!=="svelte-12zbtl3"&&(yt.textContent=co),Qs=l(e),qt=o(e,"P",{"data-svelte-h":!0}),r(qt)!=="svelte-c5hpdh"&&(qt.innerHTML=fo),As=l(e),d(_t.$$.fragment,e),Ds=l(e),Mt=o(e,"P",{"data-svelte-h":!0}),r(Mt)!=="svelte-inac12"&&(Mt.innerHTML=$o),Os=l(e),d(wt.$$.fragment,e),Ks=l(e),kt=o(e,"P",{"data-svelte-h":!0}),r(kt)!=="svelte-qkaz6e"&&(kt.innerHTML=go),el=l(e),d(zt.$$.fragment,e),tl=l(e),Pt=o(e,"P",{"data-svelte-h":!0}),r(Pt)!=="svelte-15w92yw"&&(Pt.textContent=bo),al=l(e),d(Ut.$$.fragment,e),nl=l(e),Lt=o(e,"P",{"data-svelte-h":!0}),r(Lt)!=="svelte-1w22mpi"&&(Lt.innerHTML=vo),sl=l(e),Ht=o(e,"P",{"data-svelte-h":!0}),r(Ht)!=="svelte-yl5sai"&&(Ht.innerHTML=ho),ll=l(e),Zt=o(e,"P",{"data-svelte-h":!0}),r(Zt)!=="svelte-1ftzyw3"&&(Zt.innerHTML=jo),il=l(e),d(Vt.$$.fragment,e),ol=l(e),Et=o(e,"P",{"data-svelte-h":!0}),r(Et)!=="svelte-p9xu1y"&&(Et.innerHTML=xo),rl=l(e),d(Rt.$$.fragment,e),pl=l(e),Gt=o(e,"P",{"data-svelte-h":!0}),r(Gt)!=="svelte-myqr84"&&(Gt.innerHTML=Co),dl=l(e),Jt=o(e,"P",{"data-svelte-h":!0}),r(Jt)!=="svelte-r96jjh"&&(Jt.textContent=To),ml=l(e),d(It.$$.fragment,e),ul=l(e),d(Wt.$$.fragment,e),cl=l(e),Nt=o(e,"P",{"data-svelte-h":!0}),r(Nt)!=="svelte-1xoy4n7"&&(Nt.textContent=yo),fl=l(e),d(Xt.$$.fragment,e),$l=l(e),Ft=o(e,"P",{"data-svelte-h":!0}),r(Ft)!=="svelte-9kirjo"&&(Ft.textContent=qo),gl=l(e),d(St.$$.fragment,e),bl=l(e),d(Bt.$$.fragment,e),vl=l(e),Yt=o(e,"P",{"data-svelte-h":!0}),r(Yt)!=="svelte-2dq9r8"&&(Yt.textContent=_o),hl=l(e),d(Qt.$$.fragment,e),jl=l(e),At=o(e,"P",{"data-svelte-h":!0}),r(At)!=="svelte-11lsu0h"&&(At.textContent=Mo),xl=l(e),d(Dt.$$.fragment,e),Cl=l(e),Ot=o(e,"P",{"data-svelte-h":!0}),r(Ot)!=="svelte-1fgg4rq"&&(Ot.innerHTML=wo),Tl=l(e),Kt=o(e,"P",{"data-svelte-h":!0}),r(Kt)!=="svelte-y41smw"&&(Kt.innerHTML=ko),yl=l(e),d(ea.$$.fragment,e),ql=l(e),ta=o(e,"P",{"data-svelte-h":!0}),r(ta)!=="svelte-of5bw8"&&(ta.innerHTML=zo),_l=l(e),aa=o(e,"P",{"data-svelte-h":!0}),r(aa)!=="svelte-ebfcm0"&&(aa.textContent=Po),Ml=l(e),d(na.$$.fragment,e),wl=l(e),sa=o(e,"P",{"data-svelte-h":!0}),r(sa)!=="svelte-1mm1nv2"&&(sa.innerHTML=Uo),kl=l(e),d(la.$$.fragment,e),zl=l(e),ia=o(e,"P",{"data-svelte-h":!0}),r(ia)!=="svelte-1a6dujr"&&(ia.innerHTML=Lo),Pl=l(e),d(oa.$$.fragment,e),Ul=l(e),ra=o(e,"P",{"data-svelte-h":!0}),r(ra)!=="svelte-e9yaas"&&(ra.innerHTML=Ho),Ll=l(e),d(pa.$$.fragment,e),Hl=l(e),da=o(e,"P",{"data-svelte-h":!0}),r(da)!=="svelte-1cav1mk"&&(da.textContent=Zo),Zl=l(e),d(ma.$$.fragment,e),Vl=l(e),d(ua.$$.fragment,e),El=l(e),ca=o(e,"P",{"data-svelte-h":!0}),r(ca)!=="svelte-16cg6s2"&&(ca.innerHTML=Vo),Rl=l(e),d(fa.$$.fragment,e),Gl=l(e),$a=o(e,"P",{"data-svelte-h":!0}),r($a)!=="svelte-10r5y0d"&&($a.textContent=Eo),Jl=l(e),d(ga.$$.fragment,e),Il=l(e),ba=o(e,"P",{"data-svelte-h":!0}),r(ba)!=="svelte-nnhntd"&&(ba.textContent=Ro),Wl=l(e),d(va.$$.fragment,e),Nl=l(e),ha=o(e,"P",{"data-svelte-h":!0}),r(ha)!=="svelte-36x1i8"&&(ha.innerHTML=Go),Xl=l(e),d(ja.$$.fragment,e),Fl=l(e),xa=o(e,"P",{"data-svelte-h":!0}),r(xa)!=="svelte-8n4g5a"&&(xa.innerHTML=Jo),Sl=l(e),d(Ca.$$.fragment,e),Bl=l(e),Ta=o(e,"P",{"data-svelte-h":!0}),r(Ta)!=="svelte-440gh7"&&(Ta.textContent=Io),Yl=l(e),d(ya.$$.fragment,e),Ql=l(e),qa=o(e,"P",{"data-svelte-h":!0}),r(qa)!=="svelte-1ycx26d"&&(qa.textContent=Wo),Al=l(e),_a=o(e,"P",{"data-svelte-h":!0}),r(_a)!=="svelte-ogu52y"&&(_a.textContent=No),Dl=l(e),d(Ma.$$.fragment,e),Ol=l(e),wa=o(e,"P",{"data-svelte-h":!0}),r(wa)!=="svelte-1xr44y7"&&(wa.innerHTML=Xo),Kl=l(e),ka=o(e,"P",{"data-svelte-h":!0}),r(ka)!=="svelte-1tqmuy4"&&(ka.innerHTML=Fo),ei=l(e),d(za.$$.fragment,e),ti=l(e),Pa=o(e,"P",{"data-svelte-h":!0}),r(Pa)!=="svelte-1hztim7"&&(Pa.textContent=So),ai=l(e),d(Ua.$$.fragment,e),ni=l(e),La=o(e,"P",{"data-svelte-h":!0}),r(La)!=="svelte-1096wrg"&&(La.textContent=Bo),si=l(e),d(Ha.$$.fragment,e),li=l(e),d(Za.$$.fragment,e),ii=l(e),Va=o(e,"P",{"data-svelte-h":!0}),r(Va)!=="svelte-1obwjrs"&&(Va.textContent=Yo),oi=l(e),d(Ea.$$.fragment,e),ri=l(e),d(Ra.$$.fragment,e),pi=l(e),Ga=o(e,"P",{"data-svelte-h":!0}),r(Ga)!=="svelte-y0v7l3"&&(Ga.innerHTML=Qo),di=l(e),Ia=o(e,"P",{}),Do(Ia).forEach(a),this.h()},h(){Oo(g,"name","hf:doc:metadata"),Oo(g,"content",pr)},m(e,t){lr(document.head,g),n(e,j,t),n(e,v,t),n(e,Ja,t),m(x,e,t),n(e,Xa,t),n(e,C,t),n(e,Fa,t),m(T,e,t),n(e,Sa,t),m(y,e,t),n(e,Ba,t),n(e,q,t),n(e,Ya,t),m(_,e,t),n(e,Qa,t),n(e,M,t),n(e,Aa,t),n(e,w,t),n(e,Da,t),m(k,e,t),n(e,Oa,t),n(e,z,t),n(e,Ka,t),m(P,e,t),n(e,en,t),n(e,U,t),n(e,tn,t),n(e,L,t),n(e,an,t),m(H,e,t),n(e,nn,t),n(e,Z,t),n(e,sn,t),m(V,e,t),n(e,ln,t),n(e,E,t),n(e,on,t),m(R,e,t),n(e,rn,t),m(G,e,t),n(e,pn,t),n(e,J,t),n(e,dn,t),m(I,e,t),n(e,mn,t),n(e,W,t),n(e,un,t),m(N,e,t),n(e,cn,t),m(X,e,t),n(e,fn,t),n(e,F,t),n(e,$n,t),m(S,e,t),n(e,gn,t),m(B,e,t),n(e,bn,t),n(e,Y,t),n(e,vn,t),m(Q,e,t),n(e,hn,t),n(e,A,t),n(e,jn,t),m(D,e,t),n(e,xn,t),n(e,O,t),n(e,Cn,t),m(K,e,t),n(e,Tn,t),n(e,ee,t),n(e,yn,t),m(te,e,t),n(e,qn,t),m(ae,e,t),n(e,_n,t),n(e,ne,t),n(e,Mn,t),n(e,se,t),n(e,wn,t),m(le,e,t),n(e,kn,t),n(e,ie,t),n(e,zn,t),n(e,oe,t),n(e,Pn,t),n(e,re,t),n(e,Un,t),m(pe,e,t),n(e,Ln,t),n(e,de,t),n(e,Hn,t),m(me,e,t),n(e,Zn,t),m(ue,e,t),n(e,Vn,t),n(e,ce,t),n(e,En,t),m(fe,e,t),n(e,Rn,t),m($e,e,t),n(e,Gn,t),n(e,ge,t),n(e,Jn,t),m(be,e,t),n(e,In,t),m(ve,e,t),n(e,Wn,t),m(he,e,t),n(e,Nn,t),n(e,je,t),n(e,Xn,t),m(xe,e,t),n(e,Fn,t),n(e,Ce,t),n(e,Sn,t),n(e,Te,t),n(e,Bn,t),n(e,ye,t),n(e,Yn,t),m(qe,e,t),n(e,Qn,t),n(e,_e,t),n(e,An,t),n(e,Me,t),n(e,Dn,t),m(we,e,t),n(e,On,t),m(ke,e,t),n(e,Kn,t),n(e,ze,t),n(e,es,t),n(e,Pe,t),n(e,ts,t),m(Ue,e,t),n(e,as,t),m(Le,e,t),n(e,ns,t),n(e,He,t),n(e,ss,t),m(Ze,e,t),n(e,ls,t),n(e,Ve,t),n(e,is,t),m(Ee,e,t),n(e,os,t),n(e,Re,t),n(e,rs,t),m(Ge,e,t),n(e,ps,t),n(e,Je,t),n(e,ds,t),m(Ie,e,t),n(e,ms,t),n(e,We,t),n(e,us,t),m(Ne,e,t),n(e,cs,t),n(e,Xe,t),n(e,fs,t),m(Fe,e,t),n(e,$s,t),n(e,Se,t),n(e,gs,t),m(Be,e,t),n(e,bs,t),n(e,Ye,t),n(e,vs,t),m(Qe,e,t),n(e,hs,t),n(e,Ae,t),n(e,js,t),n(e,De,t),n(e,xs,t),m(Oe,e,t),n(e,Cs,t),n(e,Ke,t),n(e,Ts,t),m(et,e,t),n(e,ys,t),n(e,tt,t),n(e,qs,t),m(at,e,t),n(e,_s,t),m(nt,e,t),n(e,Ms,t),n(e,st,t),n(e,ws,t),n(e,lt,t),n(e,ks,t),n(e,it,t),n(e,zs,t),m(h,e,t),n(e,Ps,t),n(e,ot,t),n(e,Us,t),m(rt,e,t),n(e,Ls,t),n(e,pt,t),n(e,Hs,t),m(dt,e,t),n(e,Zs,t),m(mt,e,t),n(e,Vs,t),n(e,ut,t),n(e,Es,t),m(ct,e,t),n(e,Rs,t),n(e,ft,t),n(e,Gs,t),m($t,e,t),n(e,Js,t),m(gt,e,t),n(e,Is,t),n(e,bt,t),n(e,Ws,t),m(vt,e,t),n(e,Ns,t),n(e,ht,t),n(e,Xs,t),m(jt,e,t),n(e,Fs,t),n(e,xt,t),n(e,Ss,t),m(Ct,e,t),n(e,Bs,t),m(Tt,e,t),n(e,Ys,t),n(e,yt,t),n(e,Qs,t),n(e,qt,t),n(e,As,t),m(_t,e,t),n(e,Ds,t),n(e,Mt,t),n(e,Os,t),m(wt,e,t),n(e,Ks,t),n(e,kt,t),n(e,el,t),m(zt,e,t),n(e,tl,t),n(e,Pt,t),n(e,al,t),m(Ut,e,t),n(e,nl,t),n(e,Lt,t),n(e,sl,t),n(e,Ht,t),n(e,ll,t),n(e,Zt,t),n(e,il,t),m(Vt,e,t),n(e,ol,t),n(e,Et,t),n(e,rl,t),m(Rt,e,t),n(e,pl,t),n(e,Gt,t),n(e,dl,t),n(e,Jt,t),n(e,ml,t),m(It,e,t),n(e,ul,t),m(Wt,e,t),n(e,cl,t),n(e,Nt,t),n(e,fl,t),m(Xt,e,t),n(e,$l,t),n(e,Ft,t),n(e,gl,t),m(St,e,t),n(e,bl,t),m(Bt,e,t),n(e,vl,t),n(e,Yt,t),n(e,hl,t),m(Qt,e,t),n(e,jl,t),n(e,At,t),n(e,xl,t),m(Dt,e,t),n(e,Cl,t),n(e,Ot,t),n(e,Tl,t),n(e,Kt,t),n(e,yl,t),m(ea,e,t),n(e,ql,t),n(e,ta,t),n(e,_l,t),n(e,aa,t),n(e,Ml,t),m(na,e,t),n(e,wl,t),n(e,sa,t),n(e,kl,t),m(la,e,t),n(e,zl,t),n(e,ia,t),n(e,Pl,t),m(oa,e,t),n(e,Ul,t),n(e,ra,t),n(e,Ll,t),m(pa,e,t),n(e,Hl,t),n(e,da,t),n(e,Zl,t),m(ma,e,t),n(e,Vl,t),m(ua,e,t),n(e,El,t),n(e,ca,t),n(e,Rl,t),m(fa,e,t),n(e,Gl,t),n(e,$a,t),n(e,Jl,t),m(ga,e,t),n(e,Il,t),n(e,ba,t),n(e,Wl,t),m(va,e,t),n(e,Nl,t),n(e,ha,t),n(e,Xl,t),m(ja,e,t),n(e,Fl,t),n(e,xa,t),n(e,Sl,t),m(Ca,e,t),n(e,Bl,t),n(e,Ta,t),n(e,Yl,t),m(ya,e,t),n(e,Ql,t),n(e,qa,t),n(e,Al,t),n(e,_a,t),n(e,Dl,t),m(Ma,e,t),n(e,Ol,t),n(e,wa,t),n(e,Kl,t),n(e,ka,t),n(e,ei,t),m(za,e,t),n(e,ti,t),n(e,Pa,t),n(e,ai,t),m(Ua,e,t),n(e,ni,t),n(e,La,t),n(e,si,t),m(Ha,e,t),n(e,li,t),m(Za,e,t),n(e,ii,t),n(e,Va,t),n(e,oi,t),m(Ea,e,t),n(e,ri,t),m(Ra,e,t),n(e,pi,t),n(e,Ga,t),n(e,di,t),n(e,Ia,t),mi=!0},p(e,[t]){const Ao={};t&2&&(Ao.$$scope={dirty:t,ctx:e}),h.$set(Ao)},i(e){mi||(u(x.$$.fragment,e),u(T.$$.fragment,e),u(y.$$.fragment,e),u(_.$$.fragment,e),u(k.$$.fragment,e),u(P.$$.fragment,e),u(H.$$.fragment,e),u(V.$$.fragment,e),u(R.$$.fragment,e),u(G.$$.fragment,e),u(I.$$.fragment,e),u(N.$$.fragment,e),u(X.$$.fragment,e),u(S.$$.fragment,e),u(B.$$.fragment,e),u(Q.$$.fragment,e),u(D.$$.fragment,e),u(K.$$.fragment,e),u(te.$$.fragment,e),u(ae.$$.fragment,e),u(le.$$.fragment,e),u(pe.$$.fragment,e),u(me.$$.fragment,e),u(ue.$$.fragment,e),u(fe.$$.fragment,e),u($e.$$.fragment,e),u(be.$$.fragment,e),u(ve.$$.fragment,e),u(he.$$.fragment,e),u(xe.$$.fragment,e),u(qe.$$.fragment,e),u(we.$$.fragment,e),u(ke.$$.fragment,e),u(Ue.$$.fragment,e),u(Le.$$.fragment,e),u(Ze.$$.fragment,e),u(Ee.$$.fragment,e),u(Ge.$$.fragment,e),u(Ie.$$.fragment,e),u(Ne.$$.fragment,e),u(Fe.$$.fragment,e),u(Be.$$.fragment,e),u(Qe.$$.fragment,e),u(Oe.$$.fragment,e),u(et.$$.fragment,e),u(at.$$.fragment,e),u(nt.$$.fragment,e),u(h.$$.fragment,e),u(rt.$$.fragment,e),u(dt.$$.fragment,e),u(mt.$$.fragment,e),u(ct.$$.fragment,e),u($t.$$.fragment,e),u(gt.$$.fragment,e),u(vt.$$.fragment,e),u(jt.$$.fragment,e),u(Ct.$$.fragment,e),u(Tt.$$.fragment,e),u(_t.$$.fragment,e),u(wt.$$.fragment,e),u(zt.$$.fragment,e),u(Ut.$$.fragment,e),u(Vt.$$.fragment,e),u(Rt.$$.fragment,e),u(It.$$.fragment,e),u(Wt.$$.fragment,e),u(Xt.$$.fragment,e),u(St.$$.fragment,e),u(Bt.$$.fragment,e),u(Qt.$$.fragment,e),u(Dt.$$.fragment,e),u(ea.$$.fragment,e),u(na.$$.fragment,e),u(la.$$.fragment,e),u(oa.$$.fragment,e),u(pa.$$.fragment,e),u(ma.$$.fragment,e),u(ua.$$.fragment,e),u(fa.$$.fragment,e),u(ga.$$.fragment,e),u(va.$$.fragment,e),u(ja.$$.fragment,e),u(Ca.$$.fragment,e),u(ya.$$.fragment,e),u(Ma.$$.fragment,e),u(za.$$.fragment,e),u(Ua.$$.fragment,e),u(Ha.$$.fragment,e),u(Za.$$.fragment,e),u(Ea.$$.fragment,e),u(Ra.$$.fragment,e),mi=!0)},o(e){c(x.$$.fragment,e),c(T.$$.fragment,e),c(y.$$.fragment,e),c(_.$$.fragment,e),c(k.$$.fragment,e),c(P.$$.fragment,e),c(H.$$.fragment,e),c(V.$$.fragment,e),c(R.$$.fragment,e),c(G.$$.fragment,e),c(I.$$.fragment,e),c(N.$$.fragment,e),c(X.$$.fragment,e),c(S.$$.fragment,e),c(B.$$.fragment,e),c(Q.$$.fragment,e),c(D.$$.fragment,e),c(K.$$.fragment,e),c(te.$$.fragment,e),c(ae.$$.fragment,e),c(le.$$.fragment,e),c(pe.$$.fragment,e),c(me.$$.fragment,e),c(ue.$$.fragment,e),c(fe.$$.fragment,e),c($e.$$.fragment,e),c(be.$$.fragment,e),c(ve.$$.fragment,e),c(he.$$.fragment,e),c(xe.$$.fragment,e),c(qe.$$.fragment,e),c(we.$$.fragment,e),c(ke.$$.fragment,e),c(Ue.$$.fragment,e),c(Le.$$.fragment,e),c(Ze.$$.fragment,e),c(Ee.$$.fragment,e),c(Ge.$$.fragment,e),c(Ie.$$.fragment,e),c(Ne.$$.fragment,e),c(Fe.$$.fragment,e),c(Be.$$.fragment,e),c(Qe.$$.fragment,e),c(Oe.$$.fragment,e),c(et.$$.fragment,e),c(at.$$.fragment,e),c(nt.$$.fragment,e),c(h.$$.fragment,e),c(rt.$$.fragment,e),c(dt.$$.fragment,e),c(mt.$$.fragment,e),c(ct.$$.fragment,e),c($t.$$.fragment,e),c(gt.$$.fragment,e),c(vt.$$.fragment,e),c(jt.$$.fragment,e),c(Ct.$$.fragment,e),c(Tt.$$.fragment,e),c(_t.$$.fragment,e),c(wt.$$.fragment,e),c(zt.$$.fragment,e),c(Ut.$$.fragment,e),c(Vt.$$.fragment,e),c(Rt.$$.fragment,e),c(It.$$.fragment,e),c(Wt.$$.fragment,e),c(Xt.$$.fragment,e),c(St.$$.fragment,e),c(Bt.$$.fragment,e),c(Qt.$$.fragment,e),c(Dt.$$.fragment,e),c(ea.$$.fragment,e),c(na.$$.fragment,e),c(la.$$.fragment,e),c(oa.$$.fragment,e),c(pa.$$.fragment,e),c(ma.$$.fragment,e),c(ua.$$.fragment,e),c(fa.$$.fragment,e),c(ga.$$.fragment,e),c(va.$$.fragment,e),c(ja.$$.fragment,e),c(Ca.$$.fragment,e),c(ya.$$.fragment,e),c(Ma.$$.fragment,e),c(za.$$.fragment,e),c(Ua.$$.fragment,e),c(Ha.$$.fragment,e),c(Za.$$.fragment,e),c(Ea.$$.fragment,e),c(Ra.$$.fragment,e),mi=!1},d(e){e&&(a(j),a(v),a(Ja),a(Xa),a(C),a(Fa),a(Sa),a(Ba),a(q),a(Ya),a(Qa),a(M),a(Aa),a(w),a(Da),a(Oa),a(z),a(Ka),a(en),a(U),a(tn),a(L),a(an),a(nn),a(Z),a(sn),a(ln),a(E),a(on),a(rn),a(pn),a(J),a(dn),a(mn),a(W),a(un),a(cn),a(fn),a(F),a($n),a(gn),a(bn),a(Y),a(vn),a(hn),a(A),a(jn),a(xn),a(O),a(Cn),a(Tn),a(ee),a(yn),a(qn),a(_n),a(ne),a(Mn),a(se),a(wn),a(kn),a(ie),a(zn),a(oe),a(Pn),a(re),a(Un),a(Ln),a(de),a(Hn),a(Zn),a(Vn),a(ce),a(En),a(Rn),a(Gn),a(ge),a(Jn),a(In),a(Wn),a(Nn),a(je),a(Xn),a(Fn),a(Ce),a(Sn),a(Te),a(Bn),a(ye),a(Yn),a(Qn),a(_e),a(An),a(Me),a(Dn),a(On),a(Kn),a(ze),a(es),a(Pe),a(ts),a(as),a(ns),a(He),a(ss),a(ls),a(Ve),a(is),a(os),a(Re),a(rs),a(ps),a(Je),a(ds),a(ms),a(We),a(us),a(cs),a(Xe),a(fs),a($s),a(Se),a(gs),a(bs),a(Ye),a(vs),a(hs),a(Ae),a(js),a(De),a(xs),a(Cs),a(Ke),a(Ts),a(ys),a(tt),a(qs),a(_s),a(Ms),a(st),a(ws),a(lt),a(ks),a(it),a(zs),a(Ps),a(ot),a(Us),a(Ls),a(pt),a(Hs),a(Zs),a(Vs),a(ut),a(Es),a(Rs),a(ft),a(Gs),a(Js),a(Is),a(bt),a(Ws),a(Ns),a(ht),a(Xs),a(Fs),a(xt),a(Ss),a(Bs),a(Ys),a(yt),a(Qs),a(qt),a(As),a(Ds),a(Mt),a(Os),a(Ks),a(kt),a(el),a(tl),a(Pt),a(al),a(nl),a(Lt),a(sl),a(Ht),a(ll),a(Zt),a(il),a(ol),a(Et),a(rl),a(pl),a(Gt),a(dl),a(Jt),a(ml),a(ul),a(cl),a(Nt),a(fl),a($l),a(Ft),a(gl),a(bl),a(vl),a(Yt),a(hl),a(jl),a(At),a(xl),a(Cl),a(Ot),a(Tl),a(Kt),a(yl),a(ql),a(ta),a(_l),a(aa),a(Ml),a(wl),a(sa),a(kl),a(zl),a(ia),a(Pl),a(Ul),a(ra),a(Ll),a(Hl),a(da),a(Zl),a(Vl),a(El),a(ca),a(Rl),a(Gl),a($a),a(Jl),a(Il),a(ba),a(Wl),a(Nl),a(ha),a(Xl),a(Fl),a(xa),a(Sl),a(Bl),a(Ta),a(Yl),a(Ql),a(qa),a(Al),a(_a),a(Dl),a(Ol),a(wa),a(Kl),a(ka),a(ei),a(ti),a(Pa),a(ai),a(ni),a(La),a(si),a(li),a(ii),a(Va),a(oi),a(ri),a(pi),a(Ga),a(di),a(Ia)),a(g),f(x,e),f(T,e),f(y,e),f(_,e),f(k,e),f(P,e),f(H,e),f(V,e),f(R,e),f(G,e),f(I,e),f(N,e),f(X,e),f(S,e),f(B,e),f(Q,e),f(D,e),f(K,e),f(te,e),f(ae,e),f(le,e),f(pe,e),f(me,e),f(ue,e),f(fe,e),f($e,e),f(be,e),f(ve,e),f(he,e),f(xe,e),f(qe,e),f(we,e),f(ke,e),f(Ue,e),f(Le,e),f(Ze,e),f(Ee,e),f(Ge,e),f(Ie,e),f(Ne,e),f(Fe,e),f(Be,e),f(Qe,e),f(Oe,e),f(et,e),f(at,e),f(nt,e),f(h,e),f(rt,e),f(dt,e),f(mt,e),f(ct,e),f($t,e),f(gt,e),f(vt,e),f(jt,e),f(Ct,e),f(Tt,e),f(_t,e),f(wt,e),f(zt,e),f(Ut,e),f(Vt,e),f(Rt,e),f(It,e),f(Wt,e),f(Xt,e),f(St,e),f(Bt,e),f(Qt,e),f(Dt,e),f(ea,e),f(na,e),f(la,e),f(oa,e),f(pa,e),f(ma,e),f(ua,e),f(fa,e),f(ga,e),f(va,e),f(ja,e),f(Ca,e),f(ya,e),f(Ma,e),f(za,e),f(Ua,e),f(Ha,e),f(Za,e),f(Ea,e),f(Ra,e)}}}const pr='{"title":"Glosario","local":"glosario","sections":[{"title":"A","local":"a","sections":[{"title":"attention mask","local":"attention-mask","sections":[],"depth":3},{"title":"autoencoding models","local":"autoencoding-models","sections":[],"depth":3},{"title":"autoregressive models","local":"autoregressive-models","sections":[],"depth":3}],"depth":2},{"title":"B","local":"b","sections":[{"title":"backbone","local":"backbone","sections":[],"depth":3}],"depth":2},{"title":"C","local":"c","sections":[{"title":"causal language modeling","local":"causal-language-modeling","sections":[],"depth":3},{"title":"channel","local":"channel","sections":[],"depth":3},{"title":"connectionist temporal classification (CTC)","local":"connectionist-temporal-classification-ctc","sections":[],"depth":3},{"title":"convolution","local":"convolution","sections":[],"depth":3}],"depth":2},{"title":"D","local":"d","sections":[{"title":"DataParallel (DP)","local":"dataparallel-dp","sections":[],"depth":3},{"title":"decoder input IDs","local":"decoder-input-ids","sections":[],"depth":3},{"title":"decoder models","local":"decoder-models","sections":[],"depth":3},{"title":"deep learning (DL)","local":"deep-learning-dl","sections":[],"depth":3}],"depth":2},{"title":"E","local":"e","sections":[{"title":"encoder models","local":"encoder-models","sections":[],"depth":3}],"depth":2},{"title":"F","local":"f","sections":[{"title":"feature extraction","local":"feature-extraction","sections":[],"depth":3},{"title":"feed forward chunking","local":"feed-forward-chunking","sections":[],"depth":3},{"title":"finetuned models","local":"finetuned-models","sections":[],"depth":3}],"depth":2},{"title":"H","local":"h","sections":[{"title":"head","local":"head","sections":[],"depth":3}],"depth":2},{"title":"I","local":"i","sections":[{"title":"image patch","local":"image-patch","sections":[],"depth":3},{"title":"inference","local":"inference","sections":[],"depth":3},{"title":"input IDs","local":"input-ids","sections":[],"depth":3}],"depth":2},{"title":"L","local":"l","sections":[{"title":"labels","local":"labels","sections":[],"depth":3},{"title":"large language models (LLM)","local":"large-language-models-llm","sections":[],"depth":3}],"depth":2},{"title":"M","local":"m","sections":[{"title":"masked language modeling (MLM)","local":"masked-language-modeling-mlm","sections":[],"depth":3},{"title":"multimodal","local":"multimodal","sections":[],"depth":3}],"depth":2},{"title":"N","local":"n","sections":[{"title":"Natural language generation (NLG)","local":"natural-language-generation-nlg","sections":[],"depth":3},{"title":"Natural language processing (NLP)","local":"natural-language-processing-nlp","sections":[],"depth":3},{"title":"Natural language understanding (NLU)","local":"natural-language-understanding-nlu","sections":[],"depth":3}],"depth":2},{"title":"P","local":"p","sections":[{"title":"Pipeline","local":"pipeline","sections":[],"depth":3},{"title":"PipelineParallel (PP)","local":"pipelineparallel-pp","sections":[],"depth":3},{"title":"pixel values","local":"pixel-values","sections":[],"depth":3},{"title":"pooling","local":"pooling","sections":[],"depth":3},{"title":"position IDs","local":"position-ids","sections":[],"depth":3},{"title":"preprocessing","local":"preprocessing","sections":[],"depth":3},{"title":"pretrained model","local":"pretrained-model","sections":[],"depth":3}],"depth":2},{"title":"R","local":"r","sections":[{"title":"recurrent neural network (RNN)","local":"recurrent-neural-network-rnn","sections":[],"depth":3},{"title":"representation learning","local":"representation-learning","sections":[],"depth":3}],"depth":2},{"title":"S","local":"s","sections":[{"title":"sampling rate","local":"sampling-rate","sections":[],"depth":3},{"title":"self-attention","local":"self-attention","sections":[],"depth":3},{"title":"self-supervised learning","local":"self-supervised-learning","sections":[],"depth":3},{"title":"semi-supervised learning","local":"semi-supervised-learning","sections":[],"depth":3},{"title":"sequence-to-sequence (seq2seq)","local":"sequence-to-sequence-seq2seq","sections":[],"depth":3},{"title":"Sharded DDP","local":"sharded-ddp","sections":[],"depth":3},{"title":"stride","local":"stride","sections":[],"depth":3},{"title":"supervised learning","local":"supervised-learning","sections":[],"depth":3}],"depth":2},{"title":"T","local":"t","sections":[{"title":"Tensor Parallelism (TP)","local":"tensor-parallelism-tp","sections":[],"depth":3},{"title":"token","local":"token","sections":[],"depth":3},{"title":"token Type IDs","local":"token-type-ids","sections":[],"depth":3},{"title":"transfer learning","local":"transfer-learning","sections":[],"depth":3},{"title":"transformer","local":"transformer","sections":[],"depth":3}],"depth":2},{"title":"U","local":"u","sections":[{"title":"unsupervised learning","local":"unsupervised-learning","sections":[],"depth":3}],"depth":2},{"title":"Z","local":"z","sections":[{"title":"Zero Redundancy Optimizer (ZeRO)","local":"zero-redundancy-optimizer-zero","sections":[],"depth":3}],"depth":2}],"depth":1}';function dr(Na){return er(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class br extends ar{constructor(g){super(),nr(this,g,dr,rr,Ko,{})}}export{br as component};
