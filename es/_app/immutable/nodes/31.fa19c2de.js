import{s as Fs,o as Ys,n as P}from"../chunks/scheduler.36a0863c.js";import{S as Hs,i as Qs,g as w,s as m,r as g,A as Bs,h as T,f as l,c,j as Vs,u as h,x as J,k as Xs,y as qs,a as n,v as y,d as $,t as j,w as b}from"../chunks/index.f891bdb2.js";import{T as Ge}from"../chunks/Tip.a8272f7f.js";import{Y as gs}from"../chunks/Youtube.0cbacd3d.js";import{C as v}from"../chunks/CodeBlock.3ec784ea.js";import{F as hs,M as Re}from"../chunks/Markdown.4127c891.js";import{H as Ce}from"../chunks/Heading.3fb90772.js";function As(G){let s,d='Puedes realizar fine-tuning a otras arquitecturas para modelos de lenguaje como <a href="https://huggingface.co/EleutherAI/gpt-neo-125M" rel="nofollow">GPT-Neo</a>, <a href="https://huggingface.co/EleutherAI/gpt-j-6B" rel="nofollow">GPT-J</a> y <a href="https://huggingface.co/google-bert/bert-base-uncased" rel="nofollow">BERT</a> siguiendo los mismos pasos presentados en esta gu칤a!',t,p,f='Mira la <a href="https://huggingface.co/tasks/text-generation" rel="nofollow">p치gina de tarea</a> para generaci칩n de texto y la <a href="https://huggingface.co/tasks/fill-mask" rel="nofollow">p치gina de tarea</a> para modelos de lenguajes por enmascaramiento para obtener m치s informaci칩n sobre los modelos, datasets, y m칠tricas asociadas.';return{c(){s=w("p"),s.innerHTML=d,t=m(),p=w("p"),p.innerHTML=f},l(u){s=T(u,"P",{"data-svelte-h":!0}),J(s)!=="svelte-139iq5j"&&(s.innerHTML=d),t=c(u),p=T(u,"P",{"data-svelte-h":!0}),J(p)!=="svelte-kfpohj"&&(p.innerHTML=f)},m(u,k){n(u,s,k),n(u,t,k),n(u,p,k)},p:P,d(u){u&&(l(s),l(t),l(p))}}}function Ls(G){let s,d="Puedes usar el token de final de secuencia como el token de relleno y asignar <code>mlm=False</code>. Esto usar치 los inputs como etiquetas movidas un elemento hacia la derecha:",t,p,f,u,k="Para modelados de lenguaje por enmascaramiento usa el mismo <code>DataCollatorForLanguageModeling</code> excepto que deber치s especificar <code>mlm_probability</code> para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos.",x,R,C;return p=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvckxhbmd1YWdlTW9kZWxpbmclMEElMEF0b2tlbml6ZXIucGFkX3Rva2VuJTIwJTNEJTIwdG9rZW5pemVyLmVvc190b2tlbiUwQWRhdGFfY29sbGF0b3IlMjAlM0QlMjBEYXRhQ29sbGF0b3JGb3JMYW5ndWFnZU1vZGVsaW5nKHRva2VuaXplciUzRHRva2VuaXplciUyQyUyMG1sbSUzREZhbHNlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pad_token = tokenizer.eos_token
<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>)`,wrap:!1}}),R=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvckxhbmd1YWdlTW9kZWxpbmclMEElMEF0b2tlbml6ZXIucGFkX3Rva2VuJTIwJTNEJTIwdG9rZW5pemVyLmVvc190b2tlbiUwQWRhdGFfY29sbGF0b3IlMjAlM0QlMjBEYXRhQ29sbGF0b3JGb3JMYW5ndWFnZU1vZGVsaW5nKHRva2VuaXplciUzRHRva2VuaXplciUyQyUyMG1sbV9wcm9iYWJpbGl0eSUzRDAuMTUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pad_token = tokenizer.eos_token
<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=<span class="hljs-number">0.15</span>)`,wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),u=w("p"),u.innerHTML=k,x=m(),g(R.$$.fragment)},l(i){s=T(i,"P",{"data-svelte-h":!0}),J(s)!=="svelte-uvilkn"&&(s.innerHTML=d),t=c(i),h(p.$$.fragment,i),f=c(i),u=T(i,"P",{"data-svelte-h":!0}),J(u)!=="svelte-1sc95x7"&&(u.innerHTML=k),x=c(i),h(R.$$.fragment,i)},m(i,_){n(i,s,_),n(i,t,_),y(p,i,_),n(i,f,_),n(i,u,_),n(i,x,_),y(R,i,_),C=!0},p:P,i(i){C||($(p.$$.fragment,i),$(R.$$.fragment,i),C=!0)},o(i){j(p.$$.fragment,i),j(R.$$.fragment,i),C=!1},d(i){i&&(l(s),l(t),l(f),l(u),l(x)),b(p,i),b(R,i)}}}function Ns(G){let s,d;return s=new Re({props:{$$slots:{default:[Ls]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function Ss(G){let s,d="Puedes usar el token de final de secuencia como el token de relleno y asignar <code>mlm=False</code>. Esto usar치 los inputs como etiquetas movidas un elemento hacia la derecha:",t,p,f,u,k="Para modelados de lenguajes por enmascaramiento usa el mismo <code>DataCollatorForLanguageModeling</code> excepto que deber치s especificar <code>mlm_probability</code> para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos.",x,R,C;return p=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvckxhbmd1YWdlTW9kZWxpbmclMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGF0YUNvbGxhdG9yRm9yTGFuZ3VhZ2VNb2RlbGluZyh0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMjBtbG0lM0RGYWxzZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIydGYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),R=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvckxhbmd1YWdlTW9kZWxpbmclMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGF0YUNvbGxhdG9yRm9yTGFuZ3VhZ2VNb2RlbGluZyh0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMjBtbG0lM0RGYWxzZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIydGYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),u=w("p"),u.innerHTML=k,x=m(),g(R.$$.fragment)},l(i){s=T(i,"P",{"data-svelte-h":!0}),J(s)!=="svelte-uvilkn"&&(s.innerHTML=d),t=c(i),h(p.$$.fragment,i),f=c(i),u=T(i,"P",{"data-svelte-h":!0}),J(u)!=="svelte-1jtycjg"&&(u.innerHTML=k),x=c(i),h(R.$$.fragment,i)},m(i,_){n(i,s,_),n(i,t,_),y(p,i,_),n(i,f,_),n(i,u,_),n(i,x,_),y(R,i,_),C=!0},p:P,i(i){C||($(p.$$.fragment,i),$(R.$$.fragment,i),C=!0)},o(i){j(p.$$.fragment,i),j(R.$$.fragment,i),C=!1},d(i){i&&(l(s),l(t),l(f),l(u),l(x)),b(p,i),b(R,i)}}}function Ds(G){let s,d;return s=new Re({props:{$$slots:{default:[Ss]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function Ps(G){let s,d='Si no est치s familiarizado con el proceso de realizar fine-tuning sobre un modelo con <code>Trainer</code>, considera el tutorial b치sico <a href="../training#finetune-with-trainer">aqu칤</a>!';return{c(){s=w("p"),s.innerHTML=d},l(t){s=T(t,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1rt7ku5"&&(s.innerHTML=d)},m(t,p){n(t,s,p)},p:P,d(t){t&&l(s)}}}function Ks(G){let s,d="Carga DistilGPT2 con <code>AutoModelForCausalLM</code>:",t,p,f,u,k,x,R="A este punto, solo faltan tres pasos:",C,i,_="<li>Definir tus hiperpar치metros de entrenamiento en <code>TrainingArguments</code>.</li> <li>Pasarle los argumentos de entrenamiento a <code>Trainer</code> junto con el modelo, dataset, y el data collator.</li> <li>Realiza la llamada <code>train()</code> para realizar el fine-tuning sobre tu modelo.</li>",Z,I,z;return p=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwVHJhaW5pbmdBcmd1bWVudHMlMkMlMjBUcmFpbmVyJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydCUyRmRpc3RpbGdwdDIlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilbert/distilgpt2&quot;</span>)`,wrap:!1}}),u=new Ge({props:{$$slots:{default:[Ps]},$$scope:{ctx:G}}}),I=new v({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjIuJTJGcmVzdWx0cyUyMiUyQyUwQSUyMCUyMCUyMCUyMGV2YWx1YXRpb25fc3RyYXRlZ3klM0QlMjJlcG9jaCUyMiUyQyUwQSUyMCUyMCUyMCUyMGxlYXJuaW5nX3JhdGUlM0QyZS01JTJDJTBBJTIwJTIwJTIwJTIwd2VpZ2h0X2RlY2F5JTNEMC4wMSUyQyUwQSklMEElMEF0cmFpbmVyJTIwJTNEJTIwVHJhaW5lciglMEElMjAlMjAlMjAlMjBtb2RlbCUzRG1vZGVsJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEbG1fZGF0YXNldCU1QiUyMnRyYWluJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9kYXRhc2V0JTNEbG1fZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBkYXRhX2NvbGxhdG9yJTNEZGF0YV9jb2xsYXRvciUyQyUwQSklMEElMEF0cmFpbmVyLnRyYWluKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=lm_dataset[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=lm_dataset[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),g(u.$$.fragment),k=m(),x=w("p"),x.textContent=R,C=m(),i=w("ol"),i.innerHTML=_,Z=m(),g(I.$$.fragment)},l(o){s=T(o,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1yy4yh2"&&(s.innerHTML=d),t=c(o),h(p.$$.fragment,o),f=c(o),h(u.$$.fragment,o),k=c(o),x=T(o,"P",{"data-svelte-h":!0}),J(x)!=="svelte-1fkdw25"&&(x.textContent=R),C=c(o),i=T(o,"OL",{"data-svelte-h":!0}),J(i)!=="svelte-16tq1zt"&&(i.innerHTML=_),Z=c(o),h(I.$$.fragment,o)},m(o,U){n(o,s,U),n(o,t,U),y(p,o,U),n(o,f,U),y(u,o,U),n(o,k,U),n(o,x,U),n(o,C,U),n(o,i,U),n(o,Z,U),y(I,o,U),z=!0},p(o,U){const W={};U&2&&(W.$$scope={dirty:U,ctx:o}),u.$set(W)},i(o){z||($(p.$$.fragment,o),$(u.$$.fragment,o),$(I.$$.fragment,o),z=!0)},o(o){j(p.$$.fragment,o),j(u.$$.fragment,o),j(I.$$.fragment,o),z=!1},d(o){o&&(l(s),l(t),l(f),l(k),l(x),l(C),l(i),l(Z)),b(p,o),b(u,o),b(I,o)}}}function Os(G){let s,d;return s=new Re({props:{$$slots:{default:[Ks]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function et(G){let s,d='Si no est치s familiarizado con realizar fine-tuning de tus modelos con Keras, considera el tutorial b치sico <a href="training#finetune-with-keras">aqu칤</a>!';return{c(){s=w("p"),s.innerHTML=d},l(t){s=T(t,"P",{"data-svelte-h":!0}),J(s)!=="svelte-txxfse"&&(s.innerHTML=d)},m(t,p){n(t,s,p)},p:P,d(t){t&&l(s)}}}function st(G){let s,d='Para realizar el fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato <code>tf.data.Dataset</code> con <a href="https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset" rel="nofollow"><code>to_tf_dataset</code></a>. Especifica los inputs y etiquetas en <code>columns</code>, ya sea para mezclar el dataset, tama침o de lote, y el data collator:',t,p,f,u,k,x,R="Crea la funci칩n optimizadora, la tasa de aprendizaje, y algunos hiperpar치metros de entrenamiento:",C,i,_,Z,I="Carga DistilGPT2 con <code>TFAutoModelForCausalLM</code>:",z,o,U,W,q='Configura el modelo para entrenamiento con <a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a>:',F,X,Q,E,A='Llama a <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a> para realizar el fine-tuning del modelo:',Y,V,H;return p=new v({props:{code:"dGZfdHJhaW5fc2V0JTIwJTNEJTIwbG1fZGF0YXNldCU1QiUyMnRyYWluJTIyJTVELnRvX3RmX2RhdGFzZXQoJTBBJTIwJTIwJTIwJTIwY29sdW1ucyUzRCU1QiUyMmF0dGVudGlvbl9tYXNrJTIyJTJDJTIwJTIyaW5wdXRfaWRzJTIyJTJDJTIwJTIybGFiZWxzJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZHVtbXlfbGFiZWxzJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHNodWZmbGUlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYmF0Y2hfc2l6ZSUzRDE2JTJDJTBBJTIwJTIwJTIwJTIwY29sbGF0ZV9mbiUzRGRhdGFfY29sbGF0b3IlMkMlMEEpJTBBJTBBdGZfdGVzdF9zZXQlMjAlM0QlMjBsbV9kYXRhc2V0JTVCJTIydGVzdCUyMiU1RC50b190Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMGNvbHVtbnMlM0QlNUIlMjJhdHRlbnRpb25fbWFzayUyMiUyQyUyMCUyMmlucHV0X2lkcyUyMiUyQyUyMCUyMmxhYmVscyUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGR1bW15X2xhYmVscyUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBzaHVmZmxlJTNERmFsc2UlMkMlMEElMjAlMjAlMjAlMjBiYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBjb2xsYXRlX2ZuJTNEZGF0YV9jb2xsYXRvciUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = lm_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = lm_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`,wrap:!1}}),u=new Ge({props:{$$slots:{default:[et]},$$scope:{ctx:G}}}),i=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGNyZWF0ZV9vcHRpbWl6ZXIlMkMlMjBBZGFtV2VpZ2h0RGVjYXklMEElMEFvcHRpbWl6ZXIlMjAlM0QlMjBBZGFtV2VpZ2h0RGVjYXkobGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMjB3ZWlnaHRfZGVjYXlfcmF0ZSUzRDAuMDEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`,wrap:!1}}),o=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxncHQyJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilbert/distilgpt2&quot;</span>)`,wrap:!1}}),X=new v({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzRG9wdGltaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`,wrap:!1}}),V=new v({props:{code:"bW9kZWwuZml0KHglM0R0Zl90cmFpbl9zZXQlMkMlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl90ZXN0X3NldCUyQyUyMGVwb2NocyUzRDMp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)',wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),g(u.$$.fragment),k=m(),x=w("p"),x.textContent=R,C=m(),g(i.$$.fragment),_=m(),Z=w("p"),Z.innerHTML=I,z=m(),g(o.$$.fragment),U=m(),W=w("p"),W.innerHTML=q,F=m(),g(X.$$.fragment),Q=m(),E=w("p"),E.innerHTML=A,Y=m(),g(V.$$.fragment)},l(a){s=T(a,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1tl4sjy"&&(s.innerHTML=d),t=c(a),h(p.$$.fragment,a),f=c(a),h(u.$$.fragment,a),k=c(a),x=T(a,"P",{"data-svelte-h":!0}),J(x)!=="svelte-qa98st"&&(x.textContent=R),C=c(a),h(i.$$.fragment,a),_=c(a),Z=T(a,"P",{"data-svelte-h":!0}),J(Z)!=="svelte-r3087k"&&(Z.innerHTML=I),z=c(a),h(o.$$.fragment,a),U=c(a),W=T(a,"P",{"data-svelte-h":!0}),J(W)!=="svelte-vifi3w"&&(W.innerHTML=q),F=c(a),h(X.$$.fragment,a),Q=c(a),E=T(a,"P",{"data-svelte-h":!0}),J(E)!=="svelte-meb7zc"&&(E.innerHTML=A),Y=c(a),h(V.$$.fragment,a)},m(a,M){n(a,s,M),n(a,t,M),y(p,a,M),n(a,f,M),y(u,a,M),n(a,k,M),n(a,x,M),n(a,C,M),y(i,a,M),n(a,_,M),n(a,Z,M),n(a,z,M),y(o,a,M),n(a,U,M),n(a,W,M),n(a,F,M),y(X,a,M),n(a,Q,M),n(a,E,M),n(a,Y,M),y(V,a,M),H=!0},p(a,M){const B={};M&2&&(B.$$scope={dirty:M,ctx:a}),u.$set(B)},i(a){H||($(p.$$.fragment,a),$(u.$$.fragment,a),$(i.$$.fragment,a),$(o.$$.fragment,a),$(X.$$.fragment,a),$(V.$$.fragment,a),H=!0)},o(a){j(p.$$.fragment,a),j(u.$$.fragment,a),j(i.$$.fragment,a),j(o.$$.fragment,a),j(X.$$.fragment,a),j(V.$$.fragment,a),H=!1},d(a){a&&(l(s),l(t),l(f),l(k),l(x),l(C),l(_),l(Z),l(z),l(U),l(W),l(F),l(Q),l(E),l(Y)),b(p,a),b(u,a),b(i,a),b(o,a),b(X,a),b(V,a)}}}function tt(G){let s,d;return s=new Re({props:{$$slots:{default:[st]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function at(G){let s,d='Si no est치s familiarizado con el proceso de realizar fine-tuning sobre un modelo con <code>Trainer</code>, considera el tutorial b치sico <a href="../training#finetune-with-trainer">aqu칤</a>!';return{c(){s=w("p"),s.innerHTML=d},l(t){s=T(t,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1rt7ku5"&&(s.innerHTML=d)},m(t,p){n(t,s,p)},p:P,d(t){t&&l(s)}}}function lt(G){let s,d="Carga DistilRoBERTa con <code>AutoModelForMaskedlM</code>:",t,p,f,u,k,x,R="A este punto, solo faltan tres pasos:",C,i,_="<li>Definir tus hiperpar치metros de entrenamiento en <code>TrainingArguments</code>.</li> <li>Pasarle los argumentos de entrenamiento a <code>Trainer</code> junto con el modelo, dataset, y el data collator.</li> <li>Realiza la llamada <code>train()</code> para realizar el fine-tuning de tu modelo.</li>",Z,I,z;return p=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvck1hc2tlZExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydCUyRmRpc3RpbHJvYmVydGEtYmFzZSUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert/distilroberta-base&quot;</span>)`,wrap:!1}}),u=new Ge({props:{$$slots:{default:[at]},$$scope:{ctx:G}}}),I=new v({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjIuJTJGcmVzdWx0cyUyMiUyQyUwQSUyMCUyMCUyMCUyMGV2YWx1YXRpb25fc3RyYXRlZ3klM0QlMjJlcG9jaCUyMiUyQyUwQSUyMCUyMCUyMCUyMGxlYXJuaW5nX3JhdGUlM0QyZS01JTJDJTBBJTIwJTIwJTIwJTIwbnVtX3RyYWluX2Vwb2NocyUzRDMlMkMlMEElMjAlMjAlMjAlMjB3ZWlnaHRfZGVjYXklM0QwLjAxJTJDJTBBKSUwQSUwQXRyYWluZXIlMjAlM0QlMjBUcmFpbmVyKCUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEbW9kZWwlMkMlMEElMjAlMjAlMjAlMjBhcmdzJTNEdHJhaW5pbmdfYXJncyUyQyUwQSUyMCUyMCUyMCUyMHRyYWluX2RhdGFzZXQlM0RsbV9kYXRhc2V0JTVCJTIydHJhaW4lMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBldmFsX2RhdGFzZXQlM0RsbV9kYXRhc2V0JTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGRhdGFfY29sbGF0b3IlM0RkYXRhX2NvbGxhdG9yJTJDJTBBKSUwQSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=lm_dataset[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=lm_dataset[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),g(u.$$.fragment),k=m(),x=w("p"),x.textContent=R,C=m(),i=w("ol"),i.innerHTML=_,Z=m(),g(I.$$.fragment)},l(o){s=T(o,"P",{"data-svelte-h":!0}),J(s)!=="svelte-7nepae"&&(s.innerHTML=d),t=c(o),h(p.$$.fragment,o),f=c(o),h(u.$$.fragment,o),k=c(o),x=T(o,"P",{"data-svelte-h":!0}),J(x)!=="svelte-1fkdw25"&&(x.textContent=R),C=c(o),i=T(o,"OL",{"data-svelte-h":!0}),J(i)!=="svelte-11bmfc5"&&(i.innerHTML=_),Z=c(o),h(I.$$.fragment,o)},m(o,U){n(o,s,U),n(o,t,U),y(p,o,U),n(o,f,U),y(u,o,U),n(o,k,U),n(o,x,U),n(o,C,U),n(o,i,U),n(o,Z,U),y(I,o,U),z=!0},p(o,U){const W={};U&2&&(W.$$scope={dirty:U,ctx:o}),u.$set(W)},i(o){z||($(p.$$.fragment,o),$(u.$$.fragment,o),$(I.$$.fragment,o),z=!0)},o(o){j(p.$$.fragment,o),j(u.$$.fragment,o),j(I.$$.fragment,o),z=!1},d(o){o&&(l(s),l(t),l(f),l(k),l(x),l(C),l(i),l(Z)),b(p,o),b(u,o),b(I,o)}}}function nt(G){let s,d;return s=new Re({props:{$$slots:{default:[lt]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function rt(G){let s,d='Si no est치s familiarizado con realizar fine-tuning de tus modelos con Keras, considera el tutorial b치sico <a href="training#finetune-with-keras">aqu칤</a>!';return{c(){s=w("p"),s.innerHTML=d},l(t){s=T(t,"P",{"data-svelte-h":!0}),J(s)!=="svelte-txxfse"&&(s.innerHTML=d)},m(t,p){n(t,s,p)},p:P,d(t){t&&l(s)}}}function pt(G){let s,d='Para realizar el fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato <code>tf.data.Dataset</code> con <a href="https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset" rel="nofollow"><code>to_tf_dataset</code></a>. Especifica los inputs y etiquetas en <code>columns</code>, ya sea para mezclar el dataset, tama침o de lote, y el data collator:',t,p,f,u,k,x,R="Crea la funci칩n optimizadora, la tasa de aprendizaje, y algunos hiperpar치metros de entrenamiento:",C,i,_,Z,I="Carga DistilRoBERTa con <code>TFAutoModelForMaskedLM</code>:",z,o,U,W,q='Configura el modelo para entrenamiento con <a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a>:',F,X,Q,E,A='Llama a <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a> para realizar el fine-tuning del modelo:',Y,V,H;return p=new v({props:{code:"dGZfdHJhaW5fc2V0JTIwJTNEJTIwbG1fZGF0YXNldCU1QiUyMnRyYWluJTIyJTVELnRvX3RmX2RhdGFzZXQoJTBBJTIwJTIwJTIwJTIwY29sdW1ucyUzRCU1QiUyMmF0dGVudGlvbl9tYXNrJTIyJTJDJTIwJTIyaW5wdXRfaWRzJTIyJTJDJTIwJTIybGFiZWxzJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZHVtbXlfbGFiZWxzJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHNodWZmbGUlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYmF0Y2hfc2l6ZSUzRDE2JTJDJTBBJTIwJTIwJTIwJTIwY29sbGF0ZV9mbiUzRGRhdGFfY29sbGF0b3IlMkMlMEEpJTBBJTBBdGZfdGVzdF9zZXQlMjAlM0QlMjBsbV9kYXRhc2V0JTVCJTIydGVzdCUyMiU1RC50b190Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMGNvbHVtbnMlM0QlNUIlMjJhdHRlbnRpb25fbWFzayUyMiUyQyUyMCUyMmlucHV0X2lkcyUyMiUyQyUyMCUyMmxhYmVscyUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGR1bW15X2xhYmVscyUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBzaHVmZmxlJTNERmFsc2UlMkMlMEElMjAlMjAlMjAlMjBiYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBjb2xsYXRlX2ZuJTNEZGF0YV9jb2xsYXRvciUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = lm_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = lm_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`,wrap:!1}}),u=new Ge({props:{$$slots:{default:[rt]},$$scope:{ctx:G}}}),i=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGNyZWF0ZV9vcHRpbWl6ZXIlMkMlMjBBZGFtV2VpZ2h0RGVjYXklMEElMEFvcHRpbWl6ZXIlMjAlM0QlMjBBZGFtV2VpZ2h0RGVjYXkobGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMjB3ZWlnaHRfZGVjYXlfcmF0ZSUzRDAuMDEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`,wrap:!1}}),o=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yTWFza2VkTE0lMEElMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxyb2JlcnRhLWJhc2UlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilbert/distilroberta-base&quot;</span>)`,wrap:!1}}),X=new v({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzRG9wdGltaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`,wrap:!1}}),V=new v({props:{code:"bW9kZWwuZml0KHglM0R0Zl90cmFpbl9zZXQlMkMlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl90ZXN0X3NldCUyQyUyMGVwb2NocyUzRDMp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)',wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),g(u.$$.fragment),k=m(),x=w("p"),x.textContent=R,C=m(),g(i.$$.fragment),_=m(),Z=w("p"),Z.innerHTML=I,z=m(),g(o.$$.fragment),U=m(),W=w("p"),W.innerHTML=q,F=m(),g(X.$$.fragment),Q=m(),E=w("p"),E.innerHTML=A,Y=m(),g(V.$$.fragment)},l(a){s=T(a,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1tl4sjy"&&(s.innerHTML=d),t=c(a),h(p.$$.fragment,a),f=c(a),h(u.$$.fragment,a),k=c(a),x=T(a,"P",{"data-svelte-h":!0}),J(x)!=="svelte-qa98st"&&(x.textContent=R),C=c(a),h(i.$$.fragment,a),_=c(a),Z=T(a,"P",{"data-svelte-h":!0}),J(Z)!=="svelte-1h9i1wk"&&(Z.innerHTML=I),z=c(a),h(o.$$.fragment,a),U=c(a),W=T(a,"P",{"data-svelte-h":!0}),J(W)!=="svelte-vifi3w"&&(W.innerHTML=q),F=c(a),h(X.$$.fragment,a),Q=c(a),E=T(a,"P",{"data-svelte-h":!0}),J(E)!=="svelte-meb7zc"&&(E.innerHTML=A),Y=c(a),h(V.$$.fragment,a)},m(a,M){n(a,s,M),n(a,t,M),y(p,a,M),n(a,f,M),y(u,a,M),n(a,k,M),n(a,x,M),n(a,C,M),y(i,a,M),n(a,_,M),n(a,Z,M),n(a,z,M),y(o,a,M),n(a,U,M),n(a,W,M),n(a,F,M),y(X,a,M),n(a,Q,M),n(a,E,M),n(a,Y,M),y(V,a,M),H=!0},p(a,M){const B={};M&2&&(B.$$scope={dirty:M,ctx:a}),u.$set(B)},i(a){H||($(p.$$.fragment,a),$(u.$$.fragment,a),$(i.$$.fragment,a),$(o.$$.fragment,a),$(X.$$.fragment,a),$(V.$$.fragment,a),H=!0)},o(a){j(p.$$.fragment,a),j(u.$$.fragment,a),j(i.$$.fragment,a),j(o.$$.fragment,a),j(X.$$.fragment,a),j(V.$$.fragment,a),H=!1},d(a){a&&(l(s),l(t),l(f),l(k),l(x),l(C),l(_),l(Z),l(z),l(U),l(W),l(F),l(Q),l(E),l(Y)),b(p,a),b(u,a),b(i,a),b(o,a),b(X,a),b(V,a)}}}function ot(G){let s,d;return s=new Re({props:{$$slots:{default:[pt]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function it(G){let s,d=`Para un ejemplo m치s profundo sobre c칩mo realizar el fine-tuning sobre un modelo de lenguaje causal, considera
<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb" rel="nofollow">PyTorch notebook</a>
o <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb" rel="nofollow">TensorFlow notebook</a>.`;return{c(){s=w("p"),s.innerHTML=d},l(t){s=T(t,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1frn7nq"&&(s.innerHTML=d)},m(t,p){n(t,s,p)},p:P,d(t){t&&l(s)}}}function mt(G){let s,d,t,p,f,u,k,x="El modelado de lenguaje predice palabras en un enunciado. Hay dos formas de modelado de lenguaje.",R,C,i,_,Z="El modelado de lenguaje causal predice el siguiente token en una secuencia de tokens, y el modelo solo puede considerar los tokens a la izquierda.",I,z,o,U,W="El modelado de lenguaje por enmascaramiento predice un token enmascarado en una secuencia, y el modelo puede considerar los tokens bidireccionalmente.",q,F,X='Esta gu칤a te mostrar치 c칩mo realizar fine-tuning <a href="https://huggingface.co/distilbert/distilgpt2" rel="nofollow">DistilGPT2</a> para modelos de lenguaje causales y <a href="https://huggingface.co/distilbert/distilroberta-base" rel="nofollow">DistilRoBERTa</a> para modelos de lenguaje por enmascaramiento en el <a href="https://www.reddit.com/r/askscience/" rel="nofollow">r/askscience</a> subdataset <a href="https://huggingface.co/datasets/eli5" rel="nofollow">ELI5</a>.',Q,E,A,Y,V,H,a="Carga solo los primeros 5000 registros desde la biblioteca 游뱅 Datasets, dado que es bastante grande:",M,B,ve,K,ys="Divide este dataset en subdatasets para el entrenamiento y el test:",ze,O,Ie,ee,$s="Luego observa un ejemplo:",We,se,Ee,te,js="Observa que <code>text</code> es un subcampo anidado dentro del diccionario <code>answers</code>. Cuando preproceses el dataset, deber치s extraer el subcampo <code>text</code> en una columna aparte.",Ve,ae,Xe,le,Fe,ne,bs="Para modelados de lenguaje causales carga el tokenizador DistilGPT2 para procesar el subcampo <code>text</code>:",Ye,re,He,pe,Qe,oe,ws="Para modelados de lenguaje por enmascaramiento carga el tokenizador DistilRoBERTa, en lugar de DistilGPT2:",Be,ie,qe,me,Ts='Extrae el subcampo <code>text</code> desde su estructura anidado con el m칠todo <a href="https://huggingface.co/docs/datasets/process#flatten" rel="nofollow"><code>flatten</code></a>:',Ae,ce,Le,de,Us="Cada subcampo es ahora una columna separada, como lo indica el prefijo <code>answers</code>. Observa que <code>answers.text</code> es una lista. En lugar de tokenizar cada enunciado por separado, convierte la lista en un string para tokenizarlos conjuntamente.",Ne,ue,Js="As칤 es como puedes crear una funci칩n de preprocesamiento para convertir la lista en una cadena y truncar las secuencias para que no superen la longitud m치xima de input de DistilGPT2:",Se,fe,De,Me,xs='Usa de 游뱅 Datasets la funci칩n <a href="https://huggingface.co/docs/datasets/process#map" rel="nofollow"><code>map</code></a> para aplicar la funci칩n de preprocesamiento sobre el dataset en su totalidad. Puedes acelerar la funci칩n <code>map</code> configurando el argumento <code>batched=True</code> para procesar m칰ltiples elementos del dataset a la vez y aumentar la cantidad de procesos con <code>num_proc</code>. Elimina las columnas que no necesitas:',Pe,ge,Ke,he,_s="Ahora necesitas una segunda funci칩n de preprocesamiento para capturar el texto truncado de cualquier ejemplo demasiado largo para evitar cualquier p칠rdida de informaci칩n. Esta funci칩n de preprocesamiento deber칤a:",Oe,ye,ks="<li>Concatenar todo el texto.</li> <li>Dividir el texto concatenado en trozos m치s peque침os definidos por un <code>block_size</code>.</li>",es,$e,ss,je,Cs="Aplica la funci칩n <code>group_texts</code> sobre todo el dataset:",ts,be,as,we,Gs="Para modelados de lenguaje causales, usa <code>DataCollatorForLanguageModeling</code> para crear un lote de ejemplos. Esto tambi칠n <em>rellenar치 din치micamente</em> tu texto a la dimensi칩n del elemento m치s largo del lote para que de esta manera tengan largo uniforme. Si bien es posible rellenar tu texto en la funci칩n <code>tokenizer</code> mediante el argumento <code>padding=True</code>, el rellenado din치mico es m치s eficiente.",ls,L,ns,Te,rs,Ue,Rs='El modelado de lenguaje causal es frecuentemente utilizado para generaci칩n de texto. Esta secci칩n te muestra c칩mo realizar fine-tuning a <a href="https://huggingface.co/distilbert/distilgpt2" rel="nofollow">DistilGPT2</a> para generar nuevo texto.',ps,Je,os,N,is,xe,ms,_e,Zs='El modelado de lenguaje por enmascaramiento es tambi칠n conocido como una tarea de rellenar la m치scara, pues predice un token enmascarado dada una secuencia. Los modelos de lenguaje por enmascaramiento requieren una buena comprensi칩n del contexto de una secuencia entera, en lugar de solo el contexto a la izquierda. Esta secci칩n te ense침a como realizar el fine-tuning de <a href="https://huggingface.co/distilbert/distilroberta-base" rel="nofollow">DistilRoBERTa</a> para predecir una palabra enmascarada.',cs,ke,ds,S,us,D,fs,Ze,Ms;return f=new Ce({props:{title:"Modelado de lenguaje",local:"modelado-de-lenguaje",headingTag:"h1"}}),C=new gs({props:{id:"Vpjb1lu0MDk"}}),z=new gs({props:{id:"mqElG5QJWUg"}}),E=new Ge({props:{$$slots:{default:[As]},$$scope:{ctx:G}}}),Y=new Ce({props:{title:"Carga el dataset ELI5",local:"carga-el-dataset-eli5",headingTag:"h2"}}),B=new v({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZWxpNSUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJlbGk1JTIyJTJDJTIwc3BsaXQlM0QlMjJ0cmFpbl9hc2tzJTVCJTNBNTAwMCU1RCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>eli5 = load_dataset(<span class="hljs-string">&quot;eli5&quot;</span>, split=<span class="hljs-string">&quot;train_asks[:5000]&quot;</span>)`,wrap:!1}}),O=new v({props:{code:"ZWxpNSUyMCUzRCUyMGVsaTUudHJhaW5fdGVzdF9zcGxpdCh0ZXN0X3NpemUlM0QwLjIp",highlighted:'eli5 = eli5.train_test_split(test_size=<span class="hljs-number">0.2</span>)',wrap:!1}}),se=new v({props:{code:"ZWxpNSU1QiUyMnRyYWluJTIyJTVEJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>eli5[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers&#x27;</span>: {<span class="hljs-string">&#x27;a_id&#x27;</span>: [<span class="hljs-string">&#x27;c3d1aib&#x27;</span>, <span class="hljs-string">&#x27;c3d4lya&#x27;</span>],
  <span class="hljs-string">&#x27;score&#x27;</span>: [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>],
  <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&quot;The velocity needed to remain in orbit is equal to the square root of Newton&#x27;s constant times the mass of earth divided by the distance from the center of the earth. I don&#x27;t know the altitude of that specific mission, but they&#x27;re usually around 300 km. That means he&#x27;s going 7-8 km/s.\\n\\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.&quot;</span>,
   <span class="hljs-string">&quot;Hope you don&#x27;t mind me asking another question, but why aren&#x27;t there any stars visible in this photo?&quot;</span>]},
 <span class="hljs-string">&#x27;answers_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: []},
 <span class="hljs-string">&#x27;document&#x27;</span>: <span class="hljs-string">&#x27;&#x27;</span>,
 <span class="hljs-string">&#x27;q_id&#x27;</span>: <span class="hljs-string">&#x27;nyxfp&#x27;</span>,
 <span class="hljs-string">&#x27;selftext&#x27;</span>: <span class="hljs-string">&#x27;_URL_0_\\n\\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?&#x27;</span>,
 <span class="hljs-string">&#x27;selftext_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: [<span class="hljs-string">&#x27;http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg&#x27;</span>]},
 <span class="hljs-string">&#x27;subreddit&#x27;</span>: <span class="hljs-string">&#x27;askscience&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;Few questions about this space walk photograph.&#x27;</span>,
 <span class="hljs-string">&#x27;title_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: []}}`,wrap:!1}}),ae=new Ce({props:{title:"Preprocesamiento",local:"preprocesamiento",headingTag:"h2"}}),le=new gs({props:{id:"ma1TrR7gE7I"}}),re=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsZ3B0MiUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilgpt2&quot;</span>)`,wrap:!1}}),pe=new gs({props:{id:"8PmhEIXhBvI"}}),ie=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlscm9iZXJ0YS1iYXNlJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilroberta-base&quot;</span>)`,wrap:!1}}),ce=new v({props:{code:"ZWxpNSUyMCUzRCUyMGVsaTUuZmxhdHRlbigpJTBBZWxpNSU1QiUyMnRyYWluJTIyJTVEJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>eli5 = eli5.flatten()
<span class="hljs-meta">&gt;&gt;&gt; </span>eli5[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers.a_id&#x27;</span>: [<span class="hljs-string">&#x27;c3d1aib&#x27;</span>, <span class="hljs-string">&#x27;c3d4lya&#x27;</span>],
 <span class="hljs-string">&#x27;answers.score&#x27;</span>: [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>],
 <span class="hljs-string">&#x27;answers.text&#x27;</span>: [<span class="hljs-string">&quot;The velocity needed to remain in orbit is equal to the square root of Newton&#x27;s constant times the mass of earth divided by the distance from the center of the earth. I don&#x27;t know the altitude of that specific mission, but they&#x27;re usually around 300 km. That means he&#x27;s going 7-8 km/s.\\n\\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.&quot;</span>,
  <span class="hljs-string">&quot;Hope you don&#x27;t mind me asking another question, but why aren&#x27;t there any stars visible in this photo?&quot;</span>],
 <span class="hljs-string">&#x27;answers_urls.url&#x27;</span>: [],
 <span class="hljs-string">&#x27;document&#x27;</span>: <span class="hljs-string">&#x27;&#x27;</span>,
 <span class="hljs-string">&#x27;q_id&#x27;</span>: <span class="hljs-string">&#x27;nyxfp&#x27;</span>,
 <span class="hljs-string">&#x27;selftext&#x27;</span>: <span class="hljs-string">&#x27;_URL_0_\\n\\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?&#x27;</span>,
 <span class="hljs-string">&#x27;selftext_urls.url&#x27;</span>: [<span class="hljs-string">&#x27;http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg&#x27;</span>],
 <span class="hljs-string">&#x27;subreddit&#x27;</span>: <span class="hljs-string">&#x27;askscience&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;Few questions about this space walk photograph.&#x27;</span>,
 <span class="hljs-string">&#x27;title_urls.url&#x27;</span>: []}`,wrap:!1}}),fe=new v({props:{code:"ZGVmJTIwcHJlcHJvY2Vzc19mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjByZXR1cm4lMjB0b2tlbml6ZXIoJTVCJTIyJTIwJTIyLmpvaW4oeCklMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlcyU1QiUyMmFuc3dlcnMudGV4dCUyMiU1RCU1RCUyQyUyMHRydW5jYXRpb24lM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer([<span class="hljs-string">&quot; &quot;</span>.join(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;answers.text&quot;</span>]], truncation=<span class="hljs-literal">True</span>)`,wrap:!1}}),ge=new v({props:{code:"dG9rZW5pemVkX2VsaTUlMjAlM0QlMjBlbGk1Lm1hcCglMEElMjAlMjAlMjAlMjBwcmVwcm9jZXNzX2Z1bmN0aW9uJTJDJTBBJTIwJTIwJTIwJTIwYmF0Y2hlZCUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBudW1fcHJvYyUzRDQlMkMlMEElMjAlMjAlMjAlMjByZW1vdmVfY29sdW1ucyUzRGVsaTUlNUIlMjJ0cmFpbiUyMiU1RC5jb2x1bW5fbmFtZXMlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_eli5 = eli5.<span class="hljs-built_in">map</span>(
<span class="hljs-meta">... </span>    preprocess_function,
<span class="hljs-meta">... </span>    batched=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    num_proc=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    remove_columns=eli5[<span class="hljs-string">&quot;train&quot;</span>].column_names,
<span class="hljs-meta">... </span>)`,wrap:!1}}),$e=new v({props:{code:"YmxvY2tfc2l6ZSUyMCUzRCUyMDEyOCUwQSUwQSUwQWRlZiUyMGdyb3VwX3RleHRzKGV4YW1wbGVzKSUzQSUwQSUyMCUyMCUyMCUyMGNvbmNhdGVuYXRlZF9leGFtcGxlcyUyMCUzRCUyMCU3QmslM0ElMjBzdW0oZXhhbXBsZXMlNUJrJTVEJTJDJTIwJTVCJTVEKSUyMGZvciUyMGslMjBpbiUyMGV4YW1wbGVzLmtleXMoKSU3RCUwQSUyMCUyMCUyMCUyMHRvdGFsX2xlbmd0aCUyMCUzRCUyMGxlbihjb25jYXRlbmF0ZWRfZXhhbXBsZXMlNUJsaXN0KGV4YW1wbGVzLmtleXMoKSklNUIwJTVEJTVEKSUwQSUyMCUyMCUyMCUyMHRvdGFsX2xlbmd0aCUyMCUzRCUyMCh0b3RhbF9sZW5ndGglMjAlMkYlMkYlMjBibG9ja19zaXplKSUyMColMjBibG9ja19zaXplJTBBJTIwJTIwJTIwJTIwcmVzdWx0JTIwJTNEJTIwJTdCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwayUzQSUyMCU1QnQlNUJpJTIwJTNBJTIwaSUyMCUyQiUyMGJsb2NrX3NpemUlNUQlMjBmb3IlMjBpJTIwaW4lMjByYW5nZSgwJTJDJTIwdG90YWxfbGVuZ3RoJTJDJTIwYmxvY2tfc2l6ZSklNUQlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBmb3IlMjBrJTJDJTIwdCUyMGluJTIwY29uY2F0ZW5hdGVkX2V4YW1wbGVzLml0ZW1zKCklMEElMjAlMjAlMjAlMjAlN0QlMEElMjAlMjAlMjAlMjByZXN1bHQlNUIlMjJsYWJlbHMlMjIlNUQlMjAlM0QlMjByZXN1bHQlNUIlMjJpbnB1dF9pZHMlMjIlNUQuY29weSgpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwcmVzdWx0",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>block_size = <span class="hljs-number">128</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">group_texts</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    concatenated_examples = {k: <span class="hljs-built_in">sum</span>(examples[k], []) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> examples.keys()}
<span class="hljs-meta">... </span>    total_length = <span class="hljs-built_in">len</span>(concatenated_examples[<span class="hljs-built_in">list</span>(examples.keys())[<span class="hljs-number">0</span>]])
<span class="hljs-meta">... </span>    total_length = (total_length // block_size) * block_size
<span class="hljs-meta">... </span>    result = {
<span class="hljs-meta">... </span>        k: [t[i : i + block_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, total_length, block_size)]
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> concatenated_examples.items()
<span class="hljs-meta">... </span>    }
<span class="hljs-meta">... </span>    result[<span class="hljs-string">&quot;labels&quot;</span>] = result[<span class="hljs-string">&quot;input_ids&quot;</span>].copy()
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> result`,wrap:!1}}),be=new v({props:{code:"bG1fZGF0YXNldCUyMCUzRCUyMHRva2VuaXplZF9lbGk1Lm1hcChncm91cF90ZXh0cyUyQyUyMGJhdGNoZWQlM0RUcnVlJTJDJTIwbnVtX3Byb2MlM0Q0KQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lm_dataset = tokenized_eli5.<span class="hljs-built_in">map</span>(group_texts, batched=<span class="hljs-literal">True</span>, num_proc=<span class="hljs-number">4</span>)',wrap:!1}}),L=new hs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ds],pytorch:[Ns]},$$scope:{ctx:G}}}),Te=new Ce({props:{title:"Modelado de lenguaje causal",local:"modelado-de-lenguaje-causal",headingTag:"h2"}}),Je=new Ce({props:{title:"Entrenamiento",local:"entrenamiento",headingTag:"h3"}}),N=new hs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[tt],pytorch:[Os]},$$scope:{ctx:G}}}),xe=new Ce({props:{title:"Modelado de lenguaje por enmascaramiento",local:"modelado-de-lenguaje-por-enmascaramiento",headingTag:"h2"}}),ke=new Ce({props:{title:"Entrenamiento",local:"entrenamiento",headingTag:"h3"}}),S=new hs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ot],pytorch:[nt]},$$scope:{ctx:G}}}),D=new Ge({props:{$$slots:{default:[it]},$$scope:{ctx:G}}}),{c(){s=w("meta"),d=m(),t=w("p"),p=m(),g(f.$$.fragment),u=m(),k=w("p"),k.textContent=x,R=m(),g(C.$$.fragment),i=m(),_=w("p"),_.textContent=Z,I=m(),g(z.$$.fragment),o=m(),U=w("p"),U.textContent=W,q=m(),F=w("p"),F.innerHTML=X,Q=m(),g(E.$$.fragment),A=m(),g(Y.$$.fragment),V=m(),H=w("p"),H.textContent=a,M=m(),g(B.$$.fragment),ve=m(),K=w("p"),K.textContent=ys,ze=m(),g(O.$$.fragment),Ie=m(),ee=w("p"),ee.textContent=$s,We=m(),g(se.$$.fragment),Ee=m(),te=w("p"),te.innerHTML=js,Ve=m(),g(ae.$$.fragment),Xe=m(),g(le.$$.fragment),Fe=m(),ne=w("p"),ne.innerHTML=bs,Ye=m(),g(re.$$.fragment),He=m(),g(pe.$$.fragment),Qe=m(),oe=w("p"),oe.textContent=ws,Be=m(),g(ie.$$.fragment),qe=m(),me=w("p"),me.innerHTML=Ts,Ae=m(),g(ce.$$.fragment),Le=m(),de=w("p"),de.innerHTML=Us,Ne=m(),ue=w("p"),ue.textContent=Js,Se=m(),g(fe.$$.fragment),De=m(),Me=w("p"),Me.innerHTML=xs,Pe=m(),g(ge.$$.fragment),Ke=m(),he=w("p"),he.textContent=_s,Oe=m(),ye=w("ul"),ye.innerHTML=ks,es=m(),g($e.$$.fragment),ss=m(),je=w("p"),je.innerHTML=Cs,ts=m(),g(be.$$.fragment),as=m(),we=w("p"),we.innerHTML=Gs,ls=m(),g(L.$$.fragment),ns=m(),g(Te.$$.fragment),rs=m(),Ue=w("p"),Ue.innerHTML=Rs,ps=m(),g(Je.$$.fragment),os=m(),g(N.$$.fragment),is=m(),g(xe.$$.fragment),ms=m(),_e=w("p"),_e.innerHTML=Zs,cs=m(),g(ke.$$.fragment),ds=m(),g(S.$$.fragment),us=m(),g(D.$$.fragment),fs=m(),Ze=w("p"),this.h()},l(e){const r=Bs("svelte-u9bgzb",document.head);s=T(r,"META",{name:!0,content:!0}),r.forEach(l),d=c(e),t=T(e,"P",{}),Vs(t).forEach(l),p=c(e),h(f.$$.fragment,e),u=c(e),k=T(e,"P",{"data-svelte-h":!0}),J(k)!=="svelte-1sv8lp6"&&(k.textContent=x),R=c(e),h(C.$$.fragment,e),i=c(e),_=T(e,"P",{"data-svelte-h":!0}),J(_)!=="svelte-o9osiq"&&(_.textContent=Z),I=c(e),h(z.$$.fragment,e),o=c(e),U=T(e,"P",{"data-svelte-h":!0}),J(U)!=="svelte-rdtqpf"&&(U.textContent=W),q=c(e),F=T(e,"P",{"data-svelte-h":!0}),J(F)!=="svelte-zqk5sa"&&(F.innerHTML=X),Q=c(e),h(E.$$.fragment,e),A=c(e),h(Y.$$.fragment,e),V=c(e),H=T(e,"P",{"data-svelte-h":!0}),J(H)!=="svelte-3nech7"&&(H.textContent=a),M=c(e),h(B.$$.fragment,e),ve=c(e),K=T(e,"P",{"data-svelte-h":!0}),J(K)!=="svelte-1fx745g"&&(K.textContent=ys),ze=c(e),h(O.$$.fragment,e),Ie=c(e),ee=T(e,"P",{"data-svelte-h":!0}),J(ee)!=="svelte-62xfwd"&&(ee.textContent=$s),We=c(e),h(se.$$.fragment,e),Ee=c(e),te=T(e,"P",{"data-svelte-h":!0}),J(te)!=="svelte-1pbomcc"&&(te.innerHTML=js),Ve=c(e),h(ae.$$.fragment,e),Xe=c(e),h(le.$$.fragment,e),Fe=c(e),ne=T(e,"P",{"data-svelte-h":!0}),J(ne)!=="svelte-1300qak"&&(ne.innerHTML=bs),Ye=c(e),h(re.$$.fragment,e),He=c(e),h(pe.$$.fragment,e),Qe=c(e),oe=T(e,"P",{"data-svelte-h":!0}),J(oe)!=="svelte-1ihod96"&&(oe.textContent=ws),Be=c(e),h(ie.$$.fragment,e),qe=c(e),me=T(e,"P",{"data-svelte-h":!0}),J(me)!=="svelte-18l6uf0"&&(me.innerHTML=Ts),Ae=c(e),h(ce.$$.fragment,e),Le=c(e),de=T(e,"P",{"data-svelte-h":!0}),J(de)!=="svelte-1qnu3op"&&(de.innerHTML=Us),Ne=c(e),ue=T(e,"P",{"data-svelte-h":!0}),J(ue)!=="svelte-lbchb7"&&(ue.textContent=Js),Se=c(e),h(fe.$$.fragment,e),De=c(e),Me=T(e,"P",{"data-svelte-h":!0}),J(Me)!=="svelte-1rtefwl"&&(Me.innerHTML=xs),Pe=c(e),h(ge.$$.fragment,e),Ke=c(e),he=T(e,"P",{"data-svelte-h":!0}),J(he)!=="svelte-10yu7cu"&&(he.textContent=_s),Oe=c(e),ye=T(e,"UL",{"data-svelte-h":!0}),J(ye)!=="svelte-jdg0mp"&&(ye.innerHTML=ks),es=c(e),h($e.$$.fragment,e),ss=c(e),je=T(e,"P",{"data-svelte-h":!0}),J(je)!=="svelte-bteuj6"&&(je.innerHTML=Cs),ts=c(e),h(be.$$.fragment,e),as=c(e),we=T(e,"P",{"data-svelte-h":!0}),J(we)!=="svelte-1llnvt1"&&(we.innerHTML=Gs),ls=c(e),h(L.$$.fragment,e),ns=c(e),h(Te.$$.fragment,e),rs=c(e),Ue=T(e,"P",{"data-svelte-h":!0}),J(Ue)!=="svelte-1pahbnx"&&(Ue.innerHTML=Rs),ps=c(e),h(Je.$$.fragment,e),os=c(e),h(N.$$.fragment,e),is=c(e),h(xe.$$.fragment,e),ms=c(e),_e=T(e,"P",{"data-svelte-h":!0}),J(_e)!=="svelte-10amrve"&&(_e.innerHTML=Zs),cs=c(e),h(ke.$$.fragment,e),ds=c(e),h(S.$$.fragment,e),us=c(e),h(D.$$.fragment,e),fs=c(e),Ze=T(e,"P",{}),Vs(Ze).forEach(l),this.h()},h(){Xs(s,"name","hf:doc:metadata"),Xs(s,"content",ct)},m(e,r){qs(document.head,s),n(e,d,r),n(e,t,r),n(e,p,r),y(f,e,r),n(e,u,r),n(e,k,r),n(e,R,r),y(C,e,r),n(e,i,r),n(e,_,r),n(e,I,r),y(z,e,r),n(e,o,r),n(e,U,r),n(e,q,r),n(e,F,r),n(e,Q,r),y(E,e,r),n(e,A,r),y(Y,e,r),n(e,V,r),n(e,H,r),n(e,M,r),y(B,e,r),n(e,ve,r),n(e,K,r),n(e,ze,r),y(O,e,r),n(e,Ie,r),n(e,ee,r),n(e,We,r),y(se,e,r),n(e,Ee,r),n(e,te,r),n(e,Ve,r),y(ae,e,r),n(e,Xe,r),y(le,e,r),n(e,Fe,r),n(e,ne,r),n(e,Ye,r),y(re,e,r),n(e,He,r),y(pe,e,r),n(e,Qe,r),n(e,oe,r),n(e,Be,r),y(ie,e,r),n(e,qe,r),n(e,me,r),n(e,Ae,r),y(ce,e,r),n(e,Le,r),n(e,de,r),n(e,Ne,r),n(e,ue,r),n(e,Se,r),y(fe,e,r),n(e,De,r),n(e,Me,r),n(e,Pe,r),y(ge,e,r),n(e,Ke,r),n(e,he,r),n(e,Oe,r),n(e,ye,r),n(e,es,r),y($e,e,r),n(e,ss,r),n(e,je,r),n(e,ts,r),y(be,e,r),n(e,as,r),n(e,we,r),n(e,ls,r),y(L,e,r),n(e,ns,r),y(Te,e,r),n(e,rs,r),n(e,Ue,r),n(e,ps,r),y(Je,e,r),n(e,os,r),y(N,e,r),n(e,is,r),y(xe,e,r),n(e,ms,r),n(e,_e,r),n(e,cs,r),y(ke,e,r),n(e,ds,r),y(S,e,r),n(e,us,r),y(D,e,r),n(e,fs,r),n(e,Ze,r),Ms=!0},p(e,[r]){const vs={};r&2&&(vs.$$scope={dirty:r,ctx:e}),E.$set(vs);const zs={};r&2&&(zs.$$scope={dirty:r,ctx:e}),L.$set(zs);const Is={};r&2&&(Is.$$scope={dirty:r,ctx:e}),N.$set(Is);const Ws={};r&2&&(Ws.$$scope={dirty:r,ctx:e}),S.$set(Ws);const Es={};r&2&&(Es.$$scope={dirty:r,ctx:e}),D.$set(Es)},i(e){Ms||($(f.$$.fragment,e),$(C.$$.fragment,e),$(z.$$.fragment,e),$(E.$$.fragment,e),$(Y.$$.fragment,e),$(B.$$.fragment,e),$(O.$$.fragment,e),$(se.$$.fragment,e),$(ae.$$.fragment,e),$(le.$$.fragment,e),$(re.$$.fragment,e),$(pe.$$.fragment,e),$(ie.$$.fragment,e),$(ce.$$.fragment,e),$(fe.$$.fragment,e),$(ge.$$.fragment,e),$($e.$$.fragment,e),$(be.$$.fragment,e),$(L.$$.fragment,e),$(Te.$$.fragment,e),$(Je.$$.fragment,e),$(N.$$.fragment,e),$(xe.$$.fragment,e),$(ke.$$.fragment,e),$(S.$$.fragment,e),$(D.$$.fragment,e),Ms=!0)},o(e){j(f.$$.fragment,e),j(C.$$.fragment,e),j(z.$$.fragment,e),j(E.$$.fragment,e),j(Y.$$.fragment,e),j(B.$$.fragment,e),j(O.$$.fragment,e),j(se.$$.fragment,e),j(ae.$$.fragment,e),j(le.$$.fragment,e),j(re.$$.fragment,e),j(pe.$$.fragment,e),j(ie.$$.fragment,e),j(ce.$$.fragment,e),j(fe.$$.fragment,e),j(ge.$$.fragment,e),j($e.$$.fragment,e),j(be.$$.fragment,e),j(L.$$.fragment,e),j(Te.$$.fragment,e),j(Je.$$.fragment,e),j(N.$$.fragment,e),j(xe.$$.fragment,e),j(ke.$$.fragment,e),j(S.$$.fragment,e),j(D.$$.fragment,e),Ms=!1},d(e){e&&(l(d),l(t),l(p),l(u),l(k),l(R),l(i),l(_),l(I),l(o),l(U),l(q),l(F),l(Q),l(A),l(V),l(H),l(M),l(ve),l(K),l(ze),l(Ie),l(ee),l(We),l(Ee),l(te),l(Ve),l(Xe),l(Fe),l(ne),l(Ye),l(He),l(Qe),l(oe),l(Be),l(qe),l(me),l(Ae),l(Le),l(de),l(Ne),l(ue),l(Se),l(De),l(Me),l(Pe),l(Ke),l(he),l(Oe),l(ye),l(es),l(ss),l(je),l(ts),l(as),l(we),l(ls),l(ns),l(rs),l(Ue),l(ps),l(os),l(is),l(ms),l(_e),l(cs),l(ds),l(us),l(fs),l(Ze)),l(s),b(f,e),b(C,e),b(z,e),b(E,e),b(Y,e),b(B,e),b(O,e),b(se,e),b(ae,e),b(le,e),b(re,e),b(pe,e),b(ie,e),b(ce,e),b(fe,e),b(ge,e),b($e,e),b(be,e),b(L,e),b(Te,e),b(Je,e),b(N,e),b(xe,e),b(ke,e),b(S,e),b(D,e)}}}const ct='{"title":"Modelado de lenguaje","local":"modelado-de-lenguaje","sections":[{"title":"Carga el dataset ELI5","local":"carga-el-dataset-eli5","sections":[],"depth":2},{"title":"Preprocesamiento","local":"preprocesamiento","sections":[],"depth":2},{"title":"Modelado de lenguaje causal","local":"modelado-de-lenguaje-causal","sections":[{"title":"Entrenamiento","local":"entrenamiento","sections":[],"depth":3}],"depth":2},{"title":"Modelado de lenguaje por enmascaramiento","local":"modelado-de-lenguaje-por-enmascaramiento","sections":[{"title":"Entrenamiento","local":"entrenamiento","sections":[],"depth":3}],"depth":2}],"depth":1}';function dt(G){return Ys(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jt extends Hs{constructor(s){super(),Qs(this,s,dt,mt,Fs,{})}}export{jt as component};
