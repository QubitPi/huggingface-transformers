import{s as Fs,o as Ys,n as P}from"../chunks/scheduler.36a0863c.js";import{S as Hs,i as Qs,g as w,s as m,r as g,A as Bs,h as T,f as l,c,j as Vs,u as h,x as J,k as Xs,y as qs,a as n,v as y,d as $,t as j,w as b}from"../chunks/index.f891bdb2.js";import{T as Ge}from"../chunks/Tip.a8272f7f.js";import{Y as gs}from"../chunks/Youtube.0cbacd3d.js";import{C as v}from"../chunks/CodeBlock.3ec784ea.js";import{F as hs,M as Re}from"../chunks/Markdown.4127c891.js";import{H as Ce}from"../chunks/Heading.3fb90772.js";function As(G){let s,d='Puedes realizar fine-tuning a otras arquitecturas para modelos de lenguaje como <a href="https://huggingface.co/EleutherAI/gpt-neo-125M" rel="nofollow">GPT-Neo</a>, <a href="https://huggingface.co/EleutherAI/gpt-j-6B" rel="nofollow">GPT-J</a> y <a href="https://huggingface.co/google-bert/bert-base-uncased" rel="nofollow">BERT</a> siguiendo los mismos pasos presentados en esta guía!',t,p,f='Mira la <a href="https://huggingface.co/tasks/text-generation" rel="nofollow">página de tarea</a> para generación de texto y la <a href="https://huggingface.co/tasks/fill-mask" rel="nofollow">página de tarea</a> para modelos de lenguajes por enmascaramiento para obtener más información sobre los modelos, datasets, y métricas asociadas.';return{c(){s=w("p"),s.innerHTML=d,t=m(),p=w("p"),p.innerHTML=f},l(u){s=T(u,"P",{"data-svelte-h":!0}),J(s)!=="svelte-139iq5j"&&(s.innerHTML=d),t=c(u),p=T(u,"P",{"data-svelte-h":!0}),J(p)!=="svelte-kfpohj"&&(p.innerHTML=f)},m(u,k){n(u,s,k),n(u,t,k),n(u,p,k)},p:P,d(u){u&&(l(s),l(t),l(p))}}}function Ls(G){let s,d="Puedes usar el token de final de secuencia como el token de relleno y asignar <code>mlm=False</code>. Esto usará los inputs como etiquetas movidas un elemento hacia la derecha:",t,p,f,u,k="Para modelados de lenguaje por enmascaramiento usa el mismo <code>DataCollatorForLanguageModeling</code> excepto que deberás especificar <code>mlm_probability</code> para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos.",x,R,C;return p=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvckxhbmd1YWdlTW9kZWxpbmclMEElMEF0b2tlbml6ZXIucGFkX3Rva2VuJTIwJTNEJTIwdG9rZW5pemVyLmVvc190b2tlbiUwQWRhdGFfY29sbGF0b3IlMjAlM0QlMjBEYXRhQ29sbGF0b3JGb3JMYW5ndWFnZU1vZGVsaW5nKHRva2VuaXplciUzRHRva2VuaXplciUyQyUyMG1sbSUzREZhbHNlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pad_token = tokenizer.eos_token
<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>)`,wrap:!1}}),R=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvckxhbmd1YWdlTW9kZWxpbmclMEElMEF0b2tlbml6ZXIucGFkX3Rva2VuJTIwJTNEJTIwdG9rZW5pemVyLmVvc190b2tlbiUwQWRhdGFfY29sbGF0b3IlMjAlM0QlMjBEYXRhQ29sbGF0b3JGb3JMYW5ndWFnZU1vZGVsaW5nKHRva2VuaXplciUzRHRva2VuaXplciUyQyUyMG1sbV9wcm9iYWJpbGl0eSUzRDAuMTUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pad_token = tokenizer.eos_token
<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=<span class="hljs-number">0.15</span>)`,wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),u=w("p"),u.innerHTML=k,x=m(),g(R.$$.fragment)},l(i){s=T(i,"P",{"data-svelte-h":!0}),J(s)!=="svelte-uvilkn"&&(s.innerHTML=d),t=c(i),h(p.$$.fragment,i),f=c(i),u=T(i,"P",{"data-svelte-h":!0}),J(u)!=="svelte-1sc95x7"&&(u.innerHTML=k),x=c(i),h(R.$$.fragment,i)},m(i,_){n(i,s,_),n(i,t,_),y(p,i,_),n(i,f,_),n(i,u,_),n(i,x,_),y(R,i,_),C=!0},p:P,i(i){C||($(p.$$.fragment,i),$(R.$$.fragment,i),C=!0)},o(i){j(p.$$.fragment,i),j(R.$$.fragment,i),C=!1},d(i){i&&(l(s),l(t),l(f),l(u),l(x)),b(p,i),b(R,i)}}}function Ns(G){let s,d;return s=new Re({props:{$$slots:{default:[Ls]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function Ss(G){let s,d="Puedes usar el token de final de secuencia como el token de relleno y asignar <code>mlm=False</code>. Esto usará los inputs como etiquetas movidas un elemento hacia la derecha:",t,p,f,u,k="Para modelados de lenguajes por enmascaramiento usa el mismo <code>DataCollatorForLanguageModeling</code> excepto que deberás especificar <code>mlm_probability</code> para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos.",x,R,C;return p=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvckxhbmd1YWdlTW9kZWxpbmclMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGF0YUNvbGxhdG9yRm9yTGFuZ3VhZ2VNb2RlbGluZyh0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMjBtbG0lM0RGYWxzZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIydGYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),R=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvckxhbmd1YWdlTW9kZWxpbmclMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGF0YUNvbGxhdG9yRm9yTGFuZ3VhZ2VNb2RlbGluZyh0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMjBtbG0lM0RGYWxzZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIydGYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForLanguageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=<span class="hljs-literal">False</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),u=w("p"),u.innerHTML=k,x=m(),g(R.$$.fragment)},l(i){s=T(i,"P",{"data-svelte-h":!0}),J(s)!=="svelte-uvilkn"&&(s.innerHTML=d),t=c(i),h(p.$$.fragment,i),f=c(i),u=T(i,"P",{"data-svelte-h":!0}),J(u)!=="svelte-1jtycjg"&&(u.innerHTML=k),x=c(i),h(R.$$.fragment,i)},m(i,_){n(i,s,_),n(i,t,_),y(p,i,_),n(i,f,_),n(i,u,_),n(i,x,_),y(R,i,_),C=!0},p:P,i(i){C||($(p.$$.fragment,i),$(R.$$.fragment,i),C=!0)},o(i){j(p.$$.fragment,i),j(R.$$.fragment,i),C=!1},d(i){i&&(l(s),l(t),l(f),l(u),l(x)),b(p,i),b(R,i)}}}function Ds(G){let s,d;return s=new Re({props:{$$slots:{default:[Ss]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function Ps(G){let s,d='Si no estás familiarizado con el proceso de realizar fine-tuning sobre un modelo con <code>Trainer</code>, considera el tutorial básico <a href="../training#finetune-with-trainer">aquí</a>!';return{c(){s=w("p"),s.innerHTML=d},l(t){s=T(t,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1rt7ku5"&&(s.innerHTML=d)},m(t,p){n(t,s,p)},p:P,d(t){t&&l(s)}}}function Ks(G){let s,d="Carga DistilGPT2 con <code>AutoModelForCausalLM</code>:",t,p,f,u,k,x,R="A este punto, solo faltan tres pasos:",C,i,_="<li>Definir tus hiperparámetros de entrenamiento en <code>TrainingArguments</code>.</li> <li>Pasarle los argumentos de entrenamiento a <code>Trainer</code> junto con el modelo, dataset, y el data collator.</li> <li>Realiza la llamada <code>train()</code> para realizar el fine-tuning sobre tu modelo.</li>",Z,I,z;return p=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwVHJhaW5pbmdBcmd1bWVudHMlMkMlMjBUcmFpbmVyJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydCUyRmRpc3RpbGdwdDIlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilbert/distilgpt2&quot;</span>)`,wrap:!1}}),u=new Ge({props:{$$slots:{default:[Ps]},$$scope:{ctx:G}}}),I=new v({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjIuJTJGcmVzdWx0cyUyMiUyQyUwQSUyMCUyMCUyMCUyMGV2YWx1YXRpb25fc3RyYXRlZ3klM0QlMjJlcG9jaCUyMiUyQyUwQSUyMCUyMCUyMCUyMGxlYXJuaW5nX3JhdGUlM0QyZS01JTJDJTBBJTIwJTIwJTIwJTIwd2VpZ2h0X2RlY2F5JTNEMC4wMSUyQyUwQSklMEElMEF0cmFpbmVyJTIwJTNEJTIwVHJhaW5lciglMEElMjAlMjAlMjAlMjBtb2RlbCUzRG1vZGVsJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEbG1fZGF0YXNldCU1QiUyMnRyYWluJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9kYXRhc2V0JTNEbG1fZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBkYXRhX2NvbGxhdG9yJTNEZGF0YV9jb2xsYXRvciUyQyUwQSklMEElMEF0cmFpbmVyLnRyYWluKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=lm_dataset[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=lm_dataset[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),g(u.$$.fragment),k=m(),x=w("p"),x.textContent=R,C=m(),i=w("ol"),i.innerHTML=_,Z=m(),g(I.$$.fragment)},l(o){s=T(o,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1yy4yh2"&&(s.innerHTML=d),t=c(o),h(p.$$.fragment,o),f=c(o),h(u.$$.fragment,o),k=c(o),x=T(o,"P",{"data-svelte-h":!0}),J(x)!=="svelte-1fkdw25"&&(x.textContent=R),C=c(o),i=T(o,"OL",{"data-svelte-h":!0}),J(i)!=="svelte-16tq1zt"&&(i.innerHTML=_),Z=c(o),h(I.$$.fragment,o)},m(o,U){n(o,s,U),n(o,t,U),y(p,o,U),n(o,f,U),y(u,o,U),n(o,k,U),n(o,x,U),n(o,C,U),n(o,i,U),n(o,Z,U),y(I,o,U),z=!0},p(o,U){const W={};U&2&&(W.$$scope={dirty:U,ctx:o}),u.$set(W)},i(o){z||($(p.$$.fragment,o),$(u.$$.fragment,o),$(I.$$.fragment,o),z=!0)},o(o){j(p.$$.fragment,o),j(u.$$.fragment,o),j(I.$$.fragment,o),z=!1},d(o){o&&(l(s),l(t),l(f),l(k),l(x),l(C),l(i),l(Z)),b(p,o),b(u,o),b(I,o)}}}function Os(G){let s,d;return s=new Re({props:{$$slots:{default:[Ks]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function et(G){let s,d='Si no estás familiarizado con realizar fine-tuning de tus modelos con Keras, considera el tutorial básico <a href="training#finetune-with-keras">aquí</a>!';return{c(){s=w("p"),s.innerHTML=d},l(t){s=T(t,"P",{"data-svelte-h":!0}),J(s)!=="svelte-txxfse"&&(s.innerHTML=d)},m(t,p){n(t,s,p)},p:P,d(t){t&&l(s)}}}function st(G){let s,d='Para realizar el fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato <code>tf.data.Dataset</code> con <a href="https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset" rel="nofollow"><code>to_tf_dataset</code></a>. Especifica los inputs y etiquetas en <code>columns</code>, ya sea para mezclar el dataset, tamaño de lote, y el data collator:',t,p,f,u,k,x,R="Crea la función optimizadora, la tasa de aprendizaje, y algunos hiperparámetros de entrenamiento:",C,i,_,Z,I="Carga DistilGPT2 con <code>TFAutoModelForCausalLM</code>:",z,o,U,W,q='Configura el modelo para entrenamiento con <a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a>:',F,X,Q,E,A='Llama a <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a> para realizar el fine-tuning del modelo:',Y,V,H;return p=new v({props:{code:"dGZfdHJhaW5fc2V0JTIwJTNEJTIwbG1fZGF0YXNldCU1QiUyMnRyYWluJTIyJTVELnRvX3RmX2RhdGFzZXQoJTBBJTIwJTIwJTIwJTIwY29sdW1ucyUzRCU1QiUyMmF0dGVudGlvbl9tYXNrJTIyJTJDJTIwJTIyaW5wdXRfaWRzJTIyJTJDJTIwJTIybGFiZWxzJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZHVtbXlfbGFiZWxzJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHNodWZmbGUlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYmF0Y2hfc2l6ZSUzRDE2JTJDJTBBJTIwJTIwJTIwJTIwY29sbGF0ZV9mbiUzRGRhdGFfY29sbGF0b3IlMkMlMEEpJTBBJTBBdGZfdGVzdF9zZXQlMjAlM0QlMjBsbV9kYXRhc2V0JTVCJTIydGVzdCUyMiU1RC50b190Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMGNvbHVtbnMlM0QlNUIlMjJhdHRlbnRpb25fbWFzayUyMiUyQyUyMCUyMmlucHV0X2lkcyUyMiUyQyUyMCUyMmxhYmVscyUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGR1bW15X2xhYmVscyUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBzaHVmZmxlJTNERmFsc2UlMkMlMEElMjAlMjAlMjAlMjBiYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBjb2xsYXRlX2ZuJTNEZGF0YV9jb2xsYXRvciUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = lm_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = lm_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`,wrap:!1}}),u=new Ge({props:{$$slots:{default:[et]},$$scope:{ctx:G}}}),i=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGNyZWF0ZV9vcHRpbWl6ZXIlMkMlMjBBZGFtV2VpZ2h0RGVjYXklMEElMEFvcHRpbWl6ZXIlMjAlM0QlMjBBZGFtV2VpZ2h0RGVjYXkobGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMjB3ZWlnaHRfZGVjYXlfcmF0ZSUzRDAuMDEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`,wrap:!1}}),o=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxncHQyJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilbert/distilgpt2&quot;</span>)`,wrap:!1}}),X=new v({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzRG9wdGltaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`,wrap:!1}}),V=new v({props:{code:"bW9kZWwuZml0KHglM0R0Zl90cmFpbl9zZXQlMkMlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl90ZXN0X3NldCUyQyUyMGVwb2NocyUzRDMp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)',wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),g(u.$$.fragment),k=m(),x=w("p"),x.textContent=R,C=m(),g(i.$$.fragment),_=m(),Z=w("p"),Z.innerHTML=I,z=m(),g(o.$$.fragment),U=m(),W=w("p"),W.innerHTML=q,F=m(),g(X.$$.fragment),Q=m(),E=w("p"),E.innerHTML=A,Y=m(),g(V.$$.fragment)},l(a){s=T(a,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1tl4sjy"&&(s.innerHTML=d),t=c(a),h(p.$$.fragment,a),f=c(a),h(u.$$.fragment,a),k=c(a),x=T(a,"P",{"data-svelte-h":!0}),J(x)!=="svelte-qa98st"&&(x.textContent=R),C=c(a),h(i.$$.fragment,a),_=c(a),Z=T(a,"P",{"data-svelte-h":!0}),J(Z)!=="svelte-r3087k"&&(Z.innerHTML=I),z=c(a),h(o.$$.fragment,a),U=c(a),W=T(a,"P",{"data-svelte-h":!0}),J(W)!=="svelte-vifi3w"&&(W.innerHTML=q),F=c(a),h(X.$$.fragment,a),Q=c(a),E=T(a,"P",{"data-svelte-h":!0}),J(E)!=="svelte-meb7zc"&&(E.innerHTML=A),Y=c(a),h(V.$$.fragment,a)},m(a,M){n(a,s,M),n(a,t,M),y(p,a,M),n(a,f,M),y(u,a,M),n(a,k,M),n(a,x,M),n(a,C,M),y(i,a,M),n(a,_,M),n(a,Z,M),n(a,z,M),y(o,a,M),n(a,U,M),n(a,W,M),n(a,F,M),y(X,a,M),n(a,Q,M),n(a,E,M),n(a,Y,M),y(V,a,M),H=!0},p(a,M){const B={};M&2&&(B.$$scope={dirty:M,ctx:a}),u.$set(B)},i(a){H||($(p.$$.fragment,a),$(u.$$.fragment,a),$(i.$$.fragment,a),$(o.$$.fragment,a),$(X.$$.fragment,a),$(V.$$.fragment,a),H=!0)},o(a){j(p.$$.fragment,a),j(u.$$.fragment,a),j(i.$$.fragment,a),j(o.$$.fragment,a),j(X.$$.fragment,a),j(V.$$.fragment,a),H=!1},d(a){a&&(l(s),l(t),l(f),l(k),l(x),l(C),l(_),l(Z),l(z),l(U),l(W),l(F),l(Q),l(E),l(Y)),b(p,a),b(u,a),b(i,a),b(o,a),b(X,a),b(V,a)}}}function tt(G){let s,d;return s=new Re({props:{$$slots:{default:[st]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function at(G){let s,d='Si no estás familiarizado con el proceso de realizar fine-tuning sobre un modelo con <code>Trainer</code>, considera el tutorial básico <a href="../training#finetune-with-trainer">aquí</a>!';return{c(){s=w("p"),s.innerHTML=d},l(t){s=T(t,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1rt7ku5"&&(s.innerHTML=d)},m(t,p){n(t,s,p)},p:P,d(t){t&&l(s)}}}function lt(G){let s,d="Carga DistilRoBERTa con <code>AutoModelForMaskedlM</code>:",t,p,f,u,k,x,R="A este punto, solo faltan tres pasos:",C,i,_="<li>Definir tus hiperparámetros de entrenamiento en <code>TrainingArguments</code>.</li> <li>Pasarle los argumentos de entrenamiento a <code>Trainer</code> junto con el modelo, dataset, y el data collator.</li> <li>Realiza la llamada <code>train()</code> para realizar el fine-tuning de tu modelo.</li>",Z,I,z;return p=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvck1hc2tlZExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydCUyRmRpc3RpbHJvYmVydGEtYmFzZSUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;distilbert/distilroberta-base&quot;</span>)`,wrap:!1}}),u=new Ge({props:{$$slots:{default:[at]},$$scope:{ctx:G}}}),I=new v({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjIuJTJGcmVzdWx0cyUyMiUyQyUwQSUyMCUyMCUyMCUyMGV2YWx1YXRpb25fc3RyYXRlZ3klM0QlMjJlcG9jaCUyMiUyQyUwQSUyMCUyMCUyMCUyMGxlYXJuaW5nX3JhdGUlM0QyZS01JTJDJTBBJTIwJTIwJTIwJTIwbnVtX3RyYWluX2Vwb2NocyUzRDMlMkMlMEElMjAlMjAlMjAlMjB3ZWlnaHRfZGVjYXklM0QwLjAxJTJDJTBBKSUwQSUwQXRyYWluZXIlMjAlM0QlMjBUcmFpbmVyKCUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEbW9kZWwlMkMlMEElMjAlMjAlMjAlMjBhcmdzJTNEdHJhaW5pbmdfYXJncyUyQyUwQSUyMCUyMCUyMCUyMHRyYWluX2RhdGFzZXQlM0RsbV9kYXRhc2V0JTVCJTIydHJhaW4lMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBldmFsX2RhdGFzZXQlM0RsbV9kYXRhc2V0JTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGRhdGFfY29sbGF0b3IlM0RkYXRhX2NvbGxhdG9yJTJDJTBBKSUwQSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;./results&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=lm_dataset[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=lm_dataset[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),g(u.$$.fragment),k=m(),x=w("p"),x.textContent=R,C=m(),i=w("ol"),i.innerHTML=_,Z=m(),g(I.$$.fragment)},l(o){s=T(o,"P",{"data-svelte-h":!0}),J(s)!=="svelte-7nepae"&&(s.innerHTML=d),t=c(o),h(p.$$.fragment,o),f=c(o),h(u.$$.fragment,o),k=c(o),x=T(o,"P",{"data-svelte-h":!0}),J(x)!=="svelte-1fkdw25"&&(x.textContent=R),C=c(o),i=T(o,"OL",{"data-svelte-h":!0}),J(i)!=="svelte-11bmfc5"&&(i.innerHTML=_),Z=c(o),h(I.$$.fragment,o)},m(o,U){n(o,s,U),n(o,t,U),y(p,o,U),n(o,f,U),y(u,o,U),n(o,k,U),n(o,x,U),n(o,C,U),n(o,i,U),n(o,Z,U),y(I,o,U),z=!0},p(o,U){const W={};U&2&&(W.$$scope={dirty:U,ctx:o}),u.$set(W)},i(o){z||($(p.$$.fragment,o),$(u.$$.fragment,o),$(I.$$.fragment,o),z=!0)},o(o){j(p.$$.fragment,o),j(u.$$.fragment,o),j(I.$$.fragment,o),z=!1},d(o){o&&(l(s),l(t),l(f),l(k),l(x),l(C),l(i),l(Z)),b(p,o),b(u,o),b(I,o)}}}function nt(G){let s,d;return s=new Re({props:{$$slots:{default:[lt]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function rt(G){let s,d='Si no estás familiarizado con realizar fine-tuning de tus modelos con Keras, considera el tutorial básico <a href="training#finetune-with-keras">aquí</a>!';return{c(){s=w("p"),s.innerHTML=d},l(t){s=T(t,"P",{"data-svelte-h":!0}),J(s)!=="svelte-txxfse"&&(s.innerHTML=d)},m(t,p){n(t,s,p)},p:P,d(t){t&&l(s)}}}function pt(G){let s,d='Para realizar el fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato <code>tf.data.Dataset</code> con <a href="https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset" rel="nofollow"><code>to_tf_dataset</code></a>. Especifica los inputs y etiquetas en <code>columns</code>, ya sea para mezclar el dataset, tamaño de lote, y el data collator:',t,p,f,u,k,x,R="Crea la función optimizadora, la tasa de aprendizaje, y algunos hiperparámetros de entrenamiento:",C,i,_,Z,I="Carga DistilRoBERTa con <code>TFAutoModelForMaskedLM</code>:",z,o,U,W,q='Configura el modelo para entrenamiento con <a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a>:',F,X,Q,E,A='Llama a <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a> para realizar el fine-tuning del modelo:',Y,V,H;return p=new v({props:{code:"dGZfdHJhaW5fc2V0JTIwJTNEJTIwbG1fZGF0YXNldCU1QiUyMnRyYWluJTIyJTVELnRvX3RmX2RhdGFzZXQoJTBBJTIwJTIwJTIwJTIwY29sdW1ucyUzRCU1QiUyMmF0dGVudGlvbl9tYXNrJTIyJTJDJTIwJTIyaW5wdXRfaWRzJTIyJTJDJTIwJTIybGFiZWxzJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZHVtbXlfbGFiZWxzJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHNodWZmbGUlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYmF0Y2hfc2l6ZSUzRDE2JTJDJTBBJTIwJTIwJTIwJTIwY29sbGF0ZV9mbiUzRGRhdGFfY29sbGF0b3IlMkMlMEEpJTBBJTBBdGZfdGVzdF9zZXQlMjAlM0QlMjBsbV9kYXRhc2V0JTVCJTIydGVzdCUyMiU1RC50b190Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMGNvbHVtbnMlM0QlNUIlMjJhdHRlbnRpb25fbWFzayUyMiUyQyUyMCUyMmlucHV0X2lkcyUyMiUyQyUyMCUyMmxhYmVscyUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGR1bW15X2xhYmVscyUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBzaHVmZmxlJTNERmFsc2UlMkMlMEElMjAlMjAlMjAlMjBiYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBjb2xsYXRlX2ZuJTNEZGF0YV9jb2xsYXRvciUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = lm_dataset[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = lm_dataset[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;attention_mask&quot;</span>, <span class="hljs-string">&quot;input_ids&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>],
<span class="hljs-meta">... </span>    dummy_labels=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`,wrap:!1}}),u=new Ge({props:{$$slots:{default:[rt]},$$scope:{ctx:G}}}),i=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGNyZWF0ZV9vcHRpbWl6ZXIlMkMlMjBBZGFtV2VpZ2h0RGVjYXklMEElMEFvcHRpbWl6ZXIlMjAlM0QlMjBBZGFtV2VpZ2h0RGVjYXkobGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMjB3ZWlnaHRfZGVjYXlfcmF0ZSUzRDAuMDEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer, AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`,wrap:!1}}),o=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yTWFza2VkTE0lMEElMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxyb2JlcnRhLWJhc2UlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;distilbert/distilroberta-base&quot;</span>)`,wrap:!1}}),X=new v({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzRG9wdGltaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`,wrap:!1}}),V=new v({props:{code:"bW9kZWwuZml0KHglM0R0Zl90cmFpbl9zZXQlMkMlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl90ZXN0X3NldCUyQyUyMGVwb2NocyUzRDMp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>)',wrap:!1}}),{c(){s=w("p"),s.innerHTML=d,t=m(),g(p.$$.fragment),f=m(),g(u.$$.fragment),k=m(),x=w("p"),x.textContent=R,C=m(),g(i.$$.fragment),_=m(),Z=w("p"),Z.innerHTML=I,z=m(),g(o.$$.fragment),U=m(),W=w("p"),W.innerHTML=q,F=m(),g(X.$$.fragment),Q=m(),E=w("p"),E.innerHTML=A,Y=m(),g(V.$$.fragment)},l(a){s=T(a,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1tl4sjy"&&(s.innerHTML=d),t=c(a),h(p.$$.fragment,a),f=c(a),h(u.$$.fragment,a),k=c(a),x=T(a,"P",{"data-svelte-h":!0}),J(x)!=="svelte-qa98st"&&(x.textContent=R),C=c(a),h(i.$$.fragment,a),_=c(a),Z=T(a,"P",{"data-svelte-h":!0}),J(Z)!=="svelte-1h9i1wk"&&(Z.innerHTML=I),z=c(a),h(o.$$.fragment,a),U=c(a),W=T(a,"P",{"data-svelte-h":!0}),J(W)!=="svelte-vifi3w"&&(W.innerHTML=q),F=c(a),h(X.$$.fragment,a),Q=c(a),E=T(a,"P",{"data-svelte-h":!0}),J(E)!=="svelte-meb7zc"&&(E.innerHTML=A),Y=c(a),h(V.$$.fragment,a)},m(a,M){n(a,s,M),n(a,t,M),y(p,a,M),n(a,f,M),y(u,a,M),n(a,k,M),n(a,x,M),n(a,C,M),y(i,a,M),n(a,_,M),n(a,Z,M),n(a,z,M),y(o,a,M),n(a,U,M),n(a,W,M),n(a,F,M),y(X,a,M),n(a,Q,M),n(a,E,M),n(a,Y,M),y(V,a,M),H=!0},p(a,M){const B={};M&2&&(B.$$scope={dirty:M,ctx:a}),u.$set(B)},i(a){H||($(p.$$.fragment,a),$(u.$$.fragment,a),$(i.$$.fragment,a),$(o.$$.fragment,a),$(X.$$.fragment,a),$(V.$$.fragment,a),H=!0)},o(a){j(p.$$.fragment,a),j(u.$$.fragment,a),j(i.$$.fragment,a),j(o.$$.fragment,a),j(X.$$.fragment,a),j(V.$$.fragment,a),H=!1},d(a){a&&(l(s),l(t),l(f),l(k),l(x),l(C),l(_),l(Z),l(z),l(U),l(W),l(F),l(Q),l(E),l(Y)),b(p,a),b(u,a),b(i,a),b(o,a),b(X,a),b(V,a)}}}function ot(G){let s,d;return s=new Re({props:{$$slots:{default:[pt]},$$scope:{ctx:G}}}),{c(){g(s.$$.fragment)},l(t){h(s.$$.fragment,t)},m(t,p){y(s,t,p),d=!0},p(t,p){const f={};p&2&&(f.$$scope={dirty:p,ctx:t}),s.$set(f)},i(t){d||($(s.$$.fragment,t),d=!0)},o(t){j(s.$$.fragment,t),d=!1},d(t){b(s,t)}}}function it(G){let s,d=`Para un ejemplo más profundo sobre cómo realizar el fine-tuning sobre un modelo de lenguaje causal, considera
<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb" rel="nofollow">PyTorch notebook</a>
o <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb" rel="nofollow">TensorFlow notebook</a>.`;return{c(){s=w("p"),s.innerHTML=d},l(t){s=T(t,"P",{"data-svelte-h":!0}),J(s)!=="svelte-1frn7nq"&&(s.innerHTML=d)},m(t,p){n(t,s,p)},p:P,d(t){t&&l(s)}}}function mt(G){let s,d,t,p,f,u,k,x="El modelado de lenguaje predice palabras en un enunciado. Hay dos formas de modelado de lenguaje.",R,C,i,_,Z="El modelado de lenguaje causal predice el siguiente token en una secuencia de tokens, y el modelo solo puede considerar los tokens a la izquierda.",I,z,o,U,W="El modelado de lenguaje por enmascaramiento predice un token enmascarado en una secuencia, y el modelo puede considerar los tokens bidireccionalmente.",q,F,X='Esta guía te mostrará cómo realizar fine-tuning <a href="https://huggingface.co/distilbert/distilgpt2" rel="nofollow">DistilGPT2</a> para modelos de lenguaje causales y <a href="https://huggingface.co/distilbert/distilroberta-base" rel="nofollow">DistilRoBERTa</a> para modelos de lenguaje por enmascaramiento en el <a href="https://www.reddit.com/r/askscience/" rel="nofollow">r/askscience</a> subdataset <a href="https://huggingface.co/datasets/eli5" rel="nofollow">ELI5</a>.',Q,E,A,Y,V,H,a="Carga solo los primeros 5000 registros desde la biblioteca 🤗 Datasets, dado que es bastante grande:",M,B,ve,K,ys="Divide este dataset en subdatasets para el entrenamiento y el test:",ze,O,Ie,ee,$s="Luego observa un ejemplo:",We,se,Ee,te,js="Observa que <code>text</code> es un subcampo anidado dentro del diccionario <code>answers</code>. Cuando preproceses el dataset, deberás extraer el subcampo <code>text</code> en una columna aparte.",Ve,ae,Xe,le,Fe,ne,bs="Para modelados de lenguaje causales carga el tokenizador DistilGPT2 para procesar el subcampo <code>text</code>:",Ye,re,He,pe,Qe,oe,ws="Para modelados de lenguaje por enmascaramiento carga el tokenizador DistilRoBERTa, en lugar de DistilGPT2:",Be,ie,qe,me,Ts='Extrae el subcampo <code>text</code> desde su estructura anidado con el método <a href="https://huggingface.co/docs/datasets/process#flatten" rel="nofollow"><code>flatten</code></a>:',Ae,ce,Le,de,Us="Cada subcampo es ahora una columna separada, como lo indica el prefijo <code>answers</code>. Observa que <code>answers.text</code> es una lista. En lugar de tokenizar cada enunciado por separado, convierte la lista en un string para tokenizarlos conjuntamente.",Ne,ue,Js="Así es como puedes crear una función de preprocesamiento para convertir la lista en una cadena y truncar las secuencias para que no superen la longitud máxima de input de DistilGPT2:",Se,fe,De,Me,xs='Usa de 🤗 Datasets la función <a href="https://huggingface.co/docs/datasets/process#map" rel="nofollow"><code>map</code></a> para aplicar la función de preprocesamiento sobre el dataset en su totalidad. Puedes acelerar la función <code>map</code> configurando el argumento <code>batched=True</code> para procesar múltiples elementos del dataset a la vez y aumentar la cantidad de procesos con <code>num_proc</code>. Elimina las columnas que no necesitas:',Pe,ge,Ke,he,_s="Ahora necesitas una segunda función de preprocesamiento para capturar el texto truncado de cualquier ejemplo demasiado largo para evitar cualquier pérdida de información. Esta función de preprocesamiento debería:",Oe,ye,ks="<li>Concatenar todo el texto.</li> <li>Dividir el texto concatenado en trozos más pequeños definidos por un <code>block_size</code>.</li>",es,$e,ss,je,Cs="Aplica la función <code>group_texts</code> sobre todo el dataset:",ts,be,as,we,Gs="Para modelados de lenguaje causales, usa <code>DataCollatorForLanguageModeling</code> para crear un lote de ejemplos. Esto también <em>rellenará dinámicamente</em> tu texto a la dimensión del elemento más largo del lote para que de esta manera tengan largo uniforme. Si bien es posible rellenar tu texto en la función <code>tokenizer</code> mediante el argumento <code>padding=True</code>, el rellenado dinámico es más eficiente.",ls,L,ns,Te,rs,Ue,Rs='El modelado de lenguaje causal es frecuentemente utilizado para generación de texto. Esta sección te muestra cómo realizar fine-tuning a <a href="https://huggingface.co/distilbert/distilgpt2" rel="nofollow">DistilGPT2</a> para generar nuevo texto.',ps,Je,os,N,is,xe,ms,_e,Zs='El modelado de lenguaje por enmascaramiento es también conocido como una tarea de rellenar la máscara, pues predice un token enmascarado dada una secuencia. Los modelos de lenguaje por enmascaramiento requieren una buena comprensión del contexto de una secuencia entera, en lugar de solo el contexto a la izquierda. Esta sección te enseña como realizar el fine-tuning de <a href="https://huggingface.co/distilbert/distilroberta-base" rel="nofollow">DistilRoBERTa</a> para predecir una palabra enmascarada.',cs,ke,ds,S,us,D,fs,Ze,Ms;return f=new Ce({props:{title:"Modelado de lenguaje",local:"modelado-de-lenguaje",headingTag:"h1"}}),C=new gs({props:{id:"Vpjb1lu0MDk"}}),z=new gs({props:{id:"mqElG5QJWUg"}}),E=new Ge({props:{$$slots:{default:[As]},$$scope:{ctx:G}}}),Y=new Ce({props:{title:"Carga el dataset ELI5",local:"carga-el-dataset-eli5",headingTag:"h2"}}),B=new v({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZWxpNSUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJlbGk1JTIyJTJDJTIwc3BsaXQlM0QlMjJ0cmFpbl9hc2tzJTVCJTNBNTAwMCU1RCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>eli5 = load_dataset(<span class="hljs-string">&quot;eli5&quot;</span>, split=<span class="hljs-string">&quot;train_asks[:5000]&quot;</span>)`,wrap:!1}}),O=new v({props:{code:"ZWxpNSUyMCUzRCUyMGVsaTUudHJhaW5fdGVzdF9zcGxpdCh0ZXN0X3NpemUlM0QwLjIp",highlighted:'eli5 = eli5.train_test_split(test_size=<span class="hljs-number">0.2</span>)',wrap:!1}}),se=new v({props:{code:"ZWxpNSU1QiUyMnRyYWluJTIyJTVEJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>eli5[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers&#x27;</span>: {<span class="hljs-string">&#x27;a_id&#x27;</span>: [<span class="hljs-string">&#x27;c3d1aib&#x27;</span>, <span class="hljs-string">&#x27;c3d4lya&#x27;</span>],
  <span class="hljs-string">&#x27;score&#x27;</span>: [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>],
  <span class="hljs-string">&#x27;text&#x27;</span>: [<span class="hljs-string">&quot;The velocity needed to remain in orbit is equal to the square root of Newton&#x27;s constant times the mass of earth divided by the distance from the center of the earth. I don&#x27;t know the altitude of that specific mission, but they&#x27;re usually around 300 km. That means he&#x27;s going 7-8 km/s.\\n\\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.&quot;</span>,
   <span class="hljs-string">&quot;Hope you don&#x27;t mind me asking another question, but why aren&#x27;t there any stars visible in this photo?&quot;</span>]},
 <span class="hljs-string">&#x27;answers_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: []},
 <span class="hljs-string">&#x27;document&#x27;</span>: <span class="hljs-string">&#x27;&#x27;</span>,
 <span class="hljs-string">&#x27;q_id&#x27;</span>: <span class="hljs-string">&#x27;nyxfp&#x27;</span>,
 <span class="hljs-string">&#x27;selftext&#x27;</span>: <span class="hljs-string">&#x27;_URL_0_\\n\\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?&#x27;</span>,
 <span class="hljs-string">&#x27;selftext_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: [<span class="hljs-string">&#x27;http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg&#x27;</span>]},
 <span class="hljs-string">&#x27;subreddit&#x27;</span>: <span class="hljs-string">&#x27;askscience&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;Few questions about this space walk photograph.&#x27;</span>,
 <span class="hljs-string">&#x27;title_urls&#x27;</span>: {<span class="hljs-string">&#x27;url&#x27;</span>: []}}`,wrap:!1}}),ae=new Ce({props:{title:"Preprocesamiento",local:"preprocesamiento",headingTag:"h2"}}),le=new gs({props:{id:"ma1TrR7gE7I"}}),re=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsZ3B0MiUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilgpt2&quot;</span>)`,wrap:!1}}),pe=new gs({props:{id:"8PmhEIXhBvI"}}),ie=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlscm9iZXJ0YS1iYXNlJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilroberta-base&quot;</span>)`,wrap:!1}}),ce=new v({props:{code:"ZWxpNSUyMCUzRCUyMGVsaTUuZmxhdHRlbigpJTBBZWxpNSU1QiUyMnRyYWluJTIyJTVEJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>eli5 = eli5.flatten()
<span class="hljs-meta">&gt;&gt;&gt; </span>eli5[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;answers.a_id&#x27;</span>: [<span class="hljs-string">&#x27;c3d1aib&#x27;</span>, <span class="hljs-string">&#x27;c3d4lya&#x27;</span>],
 <span class="hljs-string">&#x27;answers.score&#x27;</span>: [<span class="hljs-number">6</span>, <span class="hljs-number">3</span>],
 <span class="hljs-string">&#x27;answers.text&#x27;</span>: [<span class="hljs-string">&quot;The velocity needed to remain in orbit is equal to the square root of Newton&#x27;s constant times the mass of earth divided by the distance from the center of the earth. I don&#x27;t know the altitude of that specific mission, but they&#x27;re usually around 300 km. That means he&#x27;s going 7-8 km/s.\\n\\nIn space there are no other forces acting on either the shuttle or the guy, so they stay in the same position relative to each other. If he were to become unable to return to the ship, he would presumably run out of oxygen, or slowly fall into the atmosphere and burn up.&quot;</span>,
  <span class="hljs-string">&quot;Hope you don&#x27;t mind me asking another question, but why aren&#x27;t there any stars visible in this photo?&quot;</span>],
 <span class="hljs-string">&#x27;answers_urls.url&#x27;</span>: [],
 <span class="hljs-string">&#x27;document&#x27;</span>: <span class="hljs-string">&#x27;&#x27;</span>,
 <span class="hljs-string">&#x27;q_id&#x27;</span>: <span class="hljs-string">&#x27;nyxfp&#x27;</span>,
 <span class="hljs-string">&#x27;selftext&#x27;</span>: <span class="hljs-string">&#x27;_URL_0_\\n\\nThis was on the front page earlier and I have a few questions about it. Is it possible to calculate how fast the astronaut would be orbiting the earth? Also how does he stay close to the shuttle so that he can return safely, i.e is he orbiting at the same speed and can therefore stay next to it? And finally if his propulsion system failed, would he eventually re-enter the atmosphere and presumably die?&#x27;</span>,
 <span class="hljs-string">&#x27;selftext_urls.url&#x27;</span>: [<span class="hljs-string">&#x27;http://apod.nasa.gov/apod/image/1201/freeflyer_nasa_3000.jpg&#x27;</span>],
 <span class="hljs-string">&#x27;subreddit&#x27;</span>: <span class="hljs-string">&#x27;askscience&#x27;</span>,
 <span class="hljs-string">&#x27;title&#x27;</span>: <span class="hljs-string">&#x27;Few questions about this space walk photograph.&#x27;</span>,
 <span class="hljs-string">&#x27;title_urls.url&#x27;</span>: []}`,wrap:!1}}),fe=new v({props:{code:"ZGVmJTIwcHJlcHJvY2Vzc19mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjByZXR1cm4lMjB0b2tlbml6ZXIoJTVCJTIyJTIwJTIyLmpvaW4oeCklMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlcyU1QiUyMmFuc3dlcnMudGV4dCUyMiU1RCU1RCUyQyUyMHRydW5jYXRpb24lM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer([<span class="hljs-string">&quot; &quot;</span>.join(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;answers.text&quot;</span>]], truncation=<span class="hljs-literal">True</span>)`,wrap:!1}}),ge=new v({props:{code:"dG9rZW5pemVkX2VsaTUlMjAlM0QlMjBlbGk1Lm1hcCglMEElMjAlMjAlMjAlMjBwcmVwcm9jZXNzX2Z1bmN0aW9uJTJDJTBBJTIwJTIwJTIwJTIwYmF0Y2hlZCUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBudW1fcHJvYyUzRDQlMkMlMEElMjAlMjAlMjAlMjByZW1vdmVfY29sdW1ucyUzRGVsaTUlNUIlMjJ0cmFpbiUyMiU1RC5jb2x1bW5fbmFtZXMlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_eli5 = eli5.<span class="hljs-built_in">map</span>(
<span class="hljs-meta">... </span>    preprocess_function,
<span class="hljs-meta">... </span>    batched=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    num_proc=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    remove_columns=eli5[<span class="hljs-string">&quot;train&quot;</span>].column_names,
<span class="hljs-meta">... </span>)`,wrap:!1}}),$e=new v({props:{code:"YmxvY2tfc2l6ZSUyMCUzRCUyMDEyOCUwQSUwQSUwQWRlZiUyMGdyb3VwX3RleHRzKGV4YW1wbGVzKSUzQSUwQSUyMCUyMCUyMCUyMGNvbmNhdGVuYXRlZF9leGFtcGxlcyUyMCUzRCUyMCU3QmslM0ElMjBzdW0oZXhhbXBsZXMlNUJrJTVEJTJDJTIwJTVCJTVEKSUyMGZvciUyMGslMjBpbiUyMGV4YW1wbGVzLmtleXMoKSU3RCUwQSUyMCUyMCUyMCUyMHRvdGFsX2xlbmd0aCUyMCUzRCUyMGxlbihjb25jYXRlbmF0ZWRfZXhhbXBsZXMlNUJsaXN0KGV4YW1wbGVzLmtleXMoKSklNUIwJTVEJTVEKSUwQSUyMCUyMCUyMCUyMHRvdGFsX2xlbmd0aCUyMCUzRCUyMCh0b3RhbF9sZW5ndGglMjAlMkYlMkYlMjBibG9ja19zaXplKSUyMColMjBibG9ja19zaXplJTBBJTIwJTIwJTIwJTIwcmVzdWx0JTIwJTNEJTIwJTdCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwayUzQSUyMCU1QnQlNUJpJTIwJTNBJTIwaSUyMCUyQiUyMGJsb2NrX3NpemUlNUQlMjBmb3IlMjBpJTIwaW4lMjByYW5nZSgwJTJDJTIwdG90YWxfbGVuZ3RoJTJDJTIwYmxvY2tfc2l6ZSklNUQlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBmb3IlMjBrJTJDJTIwdCUyMGluJTIwY29uY2F0ZW5hdGVkX2V4YW1wbGVzLml0ZW1zKCklMEElMjAlMjAlMjAlMjAlN0QlMEElMjAlMjAlMjAlMjByZXN1bHQlNUIlMjJsYWJlbHMlMjIlNUQlMjAlM0QlMjByZXN1bHQlNUIlMjJpbnB1dF9pZHMlMjIlNUQuY29weSgpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwcmVzdWx0",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>block_size = <span class="hljs-number">128</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">group_texts</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    concatenated_examples = {k: <span class="hljs-built_in">sum</span>(examples[k], []) <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> examples.keys()}
<span class="hljs-meta">... </span>    total_length = <span class="hljs-built_in">len</span>(concatenated_examples[<span class="hljs-built_in">list</span>(examples.keys())[<span class="hljs-number">0</span>]])
<span class="hljs-meta">... </span>    total_length = (total_length // block_size) * block_size
<span class="hljs-meta">... </span>    result = {
<span class="hljs-meta">... </span>        k: [t[i : i + block_size] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, total_length, block_size)]
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> k, t <span class="hljs-keyword">in</span> concatenated_examples.items()
<span class="hljs-meta">... </span>    }
<span class="hljs-meta">... </span>    result[<span class="hljs-string">&quot;labels&quot;</span>] = result[<span class="hljs-string">&quot;input_ids&quot;</span>].copy()
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> result`,wrap:!1}}),be=new v({props:{code:"bG1fZGF0YXNldCUyMCUzRCUyMHRva2VuaXplZF9lbGk1Lm1hcChncm91cF90ZXh0cyUyQyUyMGJhdGNoZWQlM0RUcnVlJTJDJTIwbnVtX3Byb2MlM0Q0KQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>lm_dataset = tokenized_eli5.<span class="hljs-built_in">map</span>(group_texts, batched=<span class="hljs-literal">True</span>, num_proc=<span class="hljs-number">4</span>)',wrap:!1}}),L=new hs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ds],pytorch:[Ns]},$$scope:{ctx:G}}}),Te=new Ce({props:{title:"Modelado de lenguaje causal",local:"modelado-de-lenguaje-causal",headingTag:"h2"}}),Je=new Ce({props:{title:"Entrenamiento",local:"entrenamiento",headingTag:"h3"}}),N=new hs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[tt],pytorch:[Os]},$$scope:{ctx:G}}}),xe=new Ce({props:{title:"Modelado de lenguaje por enmascaramiento",local:"modelado-de-lenguaje-por-enmascaramiento",headingTag:"h2"}}),ke=new Ce({props:{title:"Entrenamiento",local:"entrenamiento",headingTag:"h3"}}),S=new hs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ot],pytorch:[nt]},$$scope:{ctx:G}}}),D=new Ge({props:{$$slots:{default:[it]},$$scope:{ctx:G}}}),{c(){s=w("meta"),d=m(),t=w("p"),p=m(),g(f.$$.fragment),u=m(),k=w("p"),k.textContent=x,R=m(),g(C.$$.fragment),i=m(),_=w("p"),_.textContent=Z,I=m(),g(z.$$.fragment),o=m(),U=w("p"),U.textContent=W,q=m(),F=w("p"),F.innerHTML=X,Q=m(),g(E.$$.fragment),A=m(),g(Y.$$.fragment),V=m(),H=w("p"),H.textContent=a,M=m(),g(B.$$.fragment),ve=m(),K=w("p"),K.textContent=ys,ze=m(),g(O.$$.fragment),Ie=m(),ee=w("p"),ee.textContent=$s,We=m(),g(se.$$.fragment),Ee=m(),te=w("p"),te.innerHTML=js,Ve=m(),g(ae.$$.fragment),Xe=m(),g(le.$$.fragment),Fe=m(),ne=w("p"),ne.innerHTML=bs,Ye=m(),g(re.$$.fragment),He=m(),g(pe.$$.fragment),Qe=m(),oe=w("p"),oe.textContent=ws,Be=m(),g(ie.$$.fragment),qe=m(),me=w("p"),me.innerHTML=Ts,Ae=m(),g(ce.$$.fragment),Le=m(),de=w("p"),de.innerHTML=Us,Ne=m(),ue=w("p"),ue.textContent=Js,Se=m(),g(fe.$$.fragment),De=m(),Me=w("p"),Me.innerHTML=xs,Pe=m(),g(ge.$$.fragment),Ke=m(),he=w("p"),he.textContent=_s,Oe=m(),ye=w("ul"),ye.innerHTML=ks,es=m(),g($e.$$.fragment),ss=m(),je=w("p"),je.innerHTML=Cs,ts=m(),g(be.$$.fragment),as=m(),we=w("p"),we.innerHTML=Gs,ls=m(),g(L.$$.fragment),ns=m(),g(Te.$$.fragment),rs=m(),Ue=w("p"),Ue.innerHTML=Rs,ps=m(),g(Je.$$.fragment),os=m(),g(N.$$.fragment),is=m(),g(xe.$$.fragment),ms=m(),_e=w("p"),_e.innerHTML=Zs,cs=m(),g(ke.$$.fragment),ds=m(),g(S.$$.fragment),us=m(),g(D.$$.fragment),fs=m(),Ze=w("p"),this.h()},l(e){const r=Bs("svelte-u9bgzb",document.head);s=T(r,"META",{name:!0,content:!0}),r.forEach(l),d=c(e),t=T(e,"P",{}),Vs(t).forEach(l),p=c(e),h(f.$$.fragment,e),u=c(e),k=T(e,"P",{"data-svelte-h":!0}),J(k)!=="svelte-1sv8lp6"&&(k.textContent=x),R=c(e),h(C.$$.fragment,e),i=c(e),_=T(e,"P",{"data-svelte-h":!0}),J(_)!=="svelte-o9osiq"&&(_.textContent=Z),I=c(e),h(z.$$.fragment,e),o=c(e),U=T(e,"P",{"data-svelte-h":!0}),J(U)!=="svelte-rdtqpf"&&(U.textContent=W),q=c(e),F=T(e,"P",{"data-svelte-h":!0}),J(F)!=="svelte-zqk5sa"&&(F.innerHTML=X),Q=c(e),h(E.$$.fragment,e),A=c(e),h(Y.$$.fragment,e),V=c(e),H=T(e,"P",{"data-svelte-h":!0}),J(H)!=="svelte-3nech7"&&(H.textContent=a),M=c(e),h(B.$$.fragment,e),ve=c(e),K=T(e,"P",{"data-svelte-h":!0}),J(K)!=="svelte-1fx745g"&&(K.textContent=ys),ze=c(e),h(O.$$.fragment,e),Ie=c(e),ee=T(e,"P",{"data-svelte-h":!0}),J(ee)!=="svelte-62xfwd"&&(ee.textContent=$s),We=c(e),h(se.$$.fragment,e),Ee=c(e),te=T(e,"P",{"data-svelte-h":!0}),J(te)!=="svelte-1pbomcc"&&(te.innerHTML=js),Ve=c(e),h(ae.$$.fragment,e),Xe=c(e),h(le.$$.fragment,e),Fe=c(e),ne=T(e,"P",{"data-svelte-h":!0}),J(ne)!=="svelte-1300qak"&&(ne.innerHTML=bs),Ye=c(e),h(re.$$.fragment,e),He=c(e),h(pe.$$.fragment,e),Qe=c(e),oe=T(e,"P",{"data-svelte-h":!0}),J(oe)!=="svelte-1ihod96"&&(oe.textContent=ws),Be=c(e),h(ie.$$.fragment,e),qe=c(e),me=T(e,"P",{"data-svelte-h":!0}),J(me)!=="svelte-18l6uf0"&&(me.innerHTML=Ts),Ae=c(e),h(ce.$$.fragment,e),Le=c(e),de=T(e,"P",{"data-svelte-h":!0}),J(de)!=="svelte-1qnu3op"&&(de.innerHTML=Us),Ne=c(e),ue=T(e,"P",{"data-svelte-h":!0}),J(ue)!=="svelte-lbchb7"&&(ue.textContent=Js),Se=c(e),h(fe.$$.fragment,e),De=c(e),Me=T(e,"P",{"data-svelte-h":!0}),J(Me)!=="svelte-1rtefwl"&&(Me.innerHTML=xs),Pe=c(e),h(ge.$$.fragment,e),Ke=c(e),he=T(e,"P",{"data-svelte-h":!0}),J(he)!=="svelte-10yu7cu"&&(he.textContent=_s),Oe=c(e),ye=T(e,"UL",{"data-svelte-h":!0}),J(ye)!=="svelte-jdg0mp"&&(ye.innerHTML=ks),es=c(e),h($e.$$.fragment,e),ss=c(e),je=T(e,"P",{"data-svelte-h":!0}),J(je)!=="svelte-bteuj6"&&(je.innerHTML=Cs),ts=c(e),h(be.$$.fragment,e),as=c(e),we=T(e,"P",{"data-svelte-h":!0}),J(we)!=="svelte-1llnvt1"&&(we.innerHTML=Gs),ls=c(e),h(L.$$.fragment,e),ns=c(e),h(Te.$$.fragment,e),rs=c(e),Ue=T(e,"P",{"data-svelte-h":!0}),J(Ue)!=="svelte-1pahbnx"&&(Ue.innerHTML=Rs),ps=c(e),h(Je.$$.fragment,e),os=c(e),h(N.$$.fragment,e),is=c(e),h(xe.$$.fragment,e),ms=c(e),_e=T(e,"P",{"data-svelte-h":!0}),J(_e)!=="svelte-10amrve"&&(_e.innerHTML=Zs),cs=c(e),h(ke.$$.fragment,e),ds=c(e),h(S.$$.fragment,e),us=c(e),h(D.$$.fragment,e),fs=c(e),Ze=T(e,"P",{}),Vs(Ze).forEach(l),this.h()},h(){Xs(s,"name","hf:doc:metadata"),Xs(s,"content",ct)},m(e,r){qs(document.head,s),n(e,d,r),n(e,t,r),n(e,p,r),y(f,e,r),n(e,u,r),n(e,k,r),n(e,R,r),y(C,e,r),n(e,i,r),n(e,_,r),n(e,I,r),y(z,e,r),n(e,o,r),n(e,U,r),n(e,q,r),n(e,F,r),n(e,Q,r),y(E,e,r),n(e,A,r),y(Y,e,r),n(e,V,r),n(e,H,r),n(e,M,r),y(B,e,r),n(e,ve,r),n(e,K,r),n(e,ze,r),y(O,e,r),n(e,Ie,r),n(e,ee,r),n(e,We,r),y(se,e,r),n(e,Ee,r),n(e,te,r),n(e,Ve,r),y(ae,e,r),n(e,Xe,r),y(le,e,r),n(e,Fe,r),n(e,ne,r),n(e,Ye,r),y(re,e,r),n(e,He,r),y(pe,e,r),n(e,Qe,r),n(e,oe,r),n(e,Be,r),y(ie,e,r),n(e,qe,r),n(e,me,r),n(e,Ae,r),y(ce,e,r),n(e,Le,r),n(e,de,r),n(e,Ne,r),n(e,ue,r),n(e,Se,r),y(fe,e,r),n(e,De,r),n(e,Me,r),n(e,Pe,r),y(ge,e,r),n(e,Ke,r),n(e,he,r),n(e,Oe,r),n(e,ye,r),n(e,es,r),y($e,e,r),n(e,ss,r),n(e,je,r),n(e,ts,r),y(be,e,r),n(e,as,r),n(e,we,r),n(e,ls,r),y(L,e,r),n(e,ns,r),y(Te,e,r),n(e,rs,r),n(e,Ue,r),n(e,ps,r),y(Je,e,r),n(e,os,r),y(N,e,r),n(e,is,r),y(xe,e,r),n(e,ms,r),n(e,_e,r),n(e,cs,r),y(ke,e,r),n(e,ds,r),y(S,e,r),n(e,us,r),y(D,e,r),n(e,fs,r),n(e,Ze,r),Ms=!0},p(e,[r]){const vs={};r&2&&(vs.$$scope={dirty:r,ctx:e}),E.$set(vs);const zs={};r&2&&(zs.$$scope={dirty:r,ctx:e}),L.$set(zs);const Is={};r&2&&(Is.$$scope={dirty:r,ctx:e}),N.$set(Is);const Ws={};r&2&&(Ws.$$scope={dirty:r,ctx:e}),S.$set(Ws);const Es={};r&2&&(Es.$$scope={dirty:r,ctx:e}),D.$set(Es)},i(e){Ms||($(f.$$.fragment,e),$(C.$$.fragment,e),$(z.$$.fragment,e),$(E.$$.fragment,e),$(Y.$$.fragment,e),$(B.$$.fragment,e),$(O.$$.fragment,e),$(se.$$.fragment,e),$(ae.$$.fragment,e),$(le.$$.fragment,e),$(re.$$.fragment,e),$(pe.$$.fragment,e),$(ie.$$.fragment,e),$(ce.$$.fragment,e),$(fe.$$.fragment,e),$(ge.$$.fragment,e),$($e.$$.fragment,e),$(be.$$.fragment,e),$(L.$$.fragment,e),$(Te.$$.fragment,e),$(Je.$$.fragment,e),$(N.$$.fragment,e),$(xe.$$.fragment,e),$(ke.$$.fragment,e),$(S.$$.fragment,e),$(D.$$.fragment,e),Ms=!0)},o(e){j(f.$$.fragment,e),j(C.$$.fragment,e),j(z.$$.fragment,e),j(E.$$.fragment,e),j(Y.$$.fragment,e),j(B.$$.fragment,e),j(O.$$.fragment,e),j(se.$$.fragment,e),j(ae.$$.fragment,e),j(le.$$.fragment,e),j(re.$$.fragment,e),j(pe.$$.fragment,e),j(ie.$$.fragment,e),j(ce.$$.fragment,e),j(fe.$$.fragment,e),j(ge.$$.fragment,e),j($e.$$.fragment,e),j(be.$$.fragment,e),j(L.$$.fragment,e),j(Te.$$.fragment,e),j(Je.$$.fragment,e),j(N.$$.fragment,e),j(xe.$$.fragment,e),j(ke.$$.fragment,e),j(S.$$.fragment,e),j(D.$$.fragment,e),Ms=!1},d(e){e&&(l(d),l(t),l(p),l(u),l(k),l(R),l(i),l(_),l(I),l(o),l(U),l(q),l(F),l(Q),l(A),l(V),l(H),l(M),l(ve),l(K),l(ze),l(Ie),l(ee),l(We),l(Ee),l(te),l(Ve),l(Xe),l(Fe),l(ne),l(Ye),l(He),l(Qe),l(oe),l(Be),l(qe),l(me),l(Ae),l(Le),l(de),l(Ne),l(ue),l(Se),l(De),l(Me),l(Pe),l(Ke),l(he),l(Oe),l(ye),l(es),l(ss),l(je),l(ts),l(as),l(we),l(ls),l(ns),l(rs),l(Ue),l(ps),l(os),l(is),l(ms),l(_e),l(cs),l(ds),l(us),l(fs),l(Ze)),l(s),b(f,e),b(C,e),b(z,e),b(E,e),b(Y,e),b(B,e),b(O,e),b(se,e),b(ae,e),b(le,e),b(re,e),b(pe,e),b(ie,e),b(ce,e),b(fe,e),b(ge,e),b($e,e),b(be,e),b(L,e),b(Te,e),b(Je,e),b(N,e),b(xe,e),b(ke,e),b(S,e),b(D,e)}}}const ct='{"title":"Modelado de lenguaje","local":"modelado-de-lenguaje","sections":[{"title":"Carga el dataset ELI5","local":"carga-el-dataset-eli5","sections":[],"depth":2},{"title":"Preprocesamiento","local":"preprocesamiento","sections":[],"depth":2},{"title":"Modelado de lenguaje causal","local":"modelado-de-lenguaje-causal","sections":[{"title":"Entrenamiento","local":"entrenamiento","sections":[],"depth":3}],"depth":2},{"title":"Modelado de lenguaje por enmascaramiento","local":"modelado-de-lenguaje-por-enmascaramiento","sections":[{"title":"Entrenamiento","local":"entrenamiento","sections":[],"depth":3}],"depth":2}],"depth":1}';function dt(G){return Ys(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jt extends Hs{constructor(s){super(),Qs(this,s,dt,mt,Fs,{})}}export{jt as component};
