<meta charset="utf-8" /><meta name="hf:doc:metadata" content="{&quot;title&quot;:&quot;Comunidad&quot;,&quot;local&quot;:&quot;comunidad&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Los recursos de la comunidad:&quot;,&quot;local&quot;:&quot;los-recursos-de-la-comunidad&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Los cuadernos de la comunidad:&quot;,&quot;local&quot;:&quot;los-cuadernos-de-la-comunidad&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2}],&quot;depth&quot;:1}">
		<link href="/huggingface-transformers/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/entry/start.ede98c4c.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/scheduler.36a0863c.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/singletons.16c0b1b3.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/index.733708bb.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/paths.c3d84205.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/entry/app.83262e28.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/index.f891bdb2.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/nodes/0.a4f7162e.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/each.e59479a4.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/nodes/6.68dda0cf.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/Heading.3fb90772.js"><!-- HEAD_svelte-u9bgzb_START --><meta name="hf:doc:metadata" content="{&quot;title&quot;:&quot;Comunidad&quot;,&quot;local&quot;:&quot;comunidad&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Los recursos de la comunidad:&quot;,&quot;local&quot;:&quot;los-recursos-de-la-comunidad&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Los cuadernos de la comunidad:&quot;,&quot;local&quot;:&quot;los-cuadernos-de-la-comunidad&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2}],&quot;depth&quot;:1}"><!-- HEAD_svelte-u9bgzb_END -->      <p></p>   <h1 class="relative group"><a id="comunidad" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#comunidad"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Comunidad</span></h1> <p data-svelte-h="svelte-g1pyqk">Esta página agrupa los recursos de 🤗 Transformers desarrollados por la comunidad.</p>  <h2 class="relative group"><a id="los-recursos-de-la-comunidad" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#los-recursos-de-la-comunidad"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Los recursos de la comunidad:</span></h2> <table data-svelte-h="svelte-sxbl1y"><thead><tr><th align="left">Recurso</th> <th align="left">Descripción</th> <th align="right">Autor</th></tr></thead> <tbody><tr><td align="left"><a href="https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards" rel="nofollow">Hugging Face Transformers Glossary Flashcards</a></td> <td align="left">Un conjunto de flashcards basadas en el [Glosario de documentos de Transformers] (glosario) que se ha puesto en un formato que se puede aprender/revisar fácilmente usando <a href="https://apps.ankiweb.net/" rel="nofollow">Anki</a> una fuente abierta, aplicación de multiplataforma diseñada específicamente para la retención de conocimientos a largo plazo. Ve este <a href="https://www.youtube.com/watch?v=Dji_h7PILrw" rel="nofollow">Introductory video on how to use the flashcards</a>.</td> <td align="right"><a href="https://www.darigovresearch.com/" rel="nofollow">Darigov Research</a></td></tr></tbody></table>  <h2 class="relative group"><a id="los-cuadernos-de-la-comunidad" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#los-cuadernos-de-la-comunidad"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Los cuadernos de la comunidad:</span></h2> <table data-svelte-h="svelte-rine0h"><thead><tr><th align="left">Cuaderno</th> <th align="left">Descripción</th> <th align="left">Autor</th> <th align="right"></th></tr></thead> <tbody><tr><td align="left"><a href="https://github.com/AlekseyKorshuk/huggingartists" rel="nofollow">Ajustar un transformador preentrenado para generar letras</a></td> <td align="left">Cómo generar letras al estilo de tu artista favorito ajustando un modelo GPT-2</td> <td align="left"><a href="https://github.com/AlekseyKorshuk" rel="nofollow">Aleksey Korshuk</a></td> <td align="right"><a href="https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/snapthat/TF-T5-text-to-text" rel="nofollow">Entrenar T5 en Tensorflow 2</a></td> <td align="left">Cómo entrenar a T5 para cualquier tarea usando Tensorflow 2. Este cuaderno demuestra una tarea de preguntas y respuestas implementada en Tensorflow 2 usando SQUAD</td> <td align="left"><a href="https://github.com/HarrisDePerceptron" rel="nofollow">Muhammad Harris</a></td> <td align="right"><a href="https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb" rel="nofollow">Entrenar T5 en TPU</a></td> <td align="left">Cómo entrenar a T5 en SQUAD con Transformers y Nlp</td> <td align="left"><a href="https://github.com/patil-suraj" rel="nofollow">Suraj Patil</a></td> <td align="right"><a href="https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb" rel="nofollow">Ajustar T5 para Clasificación y Opción Múltiple</a></td> <td align="left">Cómo ajustar T5 para clasificación y tareas de opción múltiple usando un formato de texto a texto con PyTorch Lightning</td> <td align="left"><a href="https://github.com/patil-suraj" rel="nofollow">Suraj Patil</a></td> <td align="right"><a href="https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb" rel="nofollow">Ajustar DialoGPT en nuevos conjuntos de datos e idiomas</a></td> <td align="left">Cómo ajustar el modelo DialoGPT en un nuevo conjunto de datos para chatbots conversacionales de diálogo abierto</td> <td align="left"><a href="https://github.com/ncoop57" rel="nofollow">Nathan Cooper</a></td> <td align="right"><a href="https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb" rel="nofollow">Modelado de secuencias largas con Reformer</a></td> <td align="left">Cómo entrenar en secuencias de hasta 500,000 tokens con Reformer</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb" rel="nofollow">Ajustar BART para resumir</a></td> <td align="left">Cómo ajustar BART para resumir con fastai usando blurr</td> <td align="left"><a href="https://ohmeow.com/" rel="nofollow">Wayde Gilliam</a></td> <td align="right"><a href="https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb" rel="nofollow">Ajustar un Transformador previamente entrenado en los tweets de cualquier persona</a></td> <td align="left">Cómo generar tweets al estilo de tu cuenta de Twitter favorita ajustando un modelo GPT-2</td> <td align="left"><a href="https://github.com/borisdayma" rel="nofollow">Boris Dayma</a></td> <td align="right"><a href="https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb" rel="nofollow">Optimizar 🤗 modelos de Hugging Face con pesos y sesgos</a></td> <td align="left">Un tutorial completo que muestra la integración de W&amp;B con Hugging Face</td> <td align="left"><a href="https://github.com/borisdayma" rel="nofollow">Boris Dayma</a></td> <td align="right"><a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb" rel="nofollow">Preentrenar Longformer</a></td> <td align="left">Cómo construir una versión “larga” de modelos preentrenados existentes</td> <td align="left"><a href="https://beltagy.net" rel="nofollow">Iz Beltagy</a></td> <td align="right"><a href="https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb" rel="nofollow">Ajustar Longformer para control de calidad</a></td> <td align="left">Cómo ajustar el modelo antiguo para la tarea de control de calidad</td> <td align="left"><a href="https://github.com/patil-suraj" rel="nofollow">Suraj Patil</a></td> <td align="right"><a href="https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb" rel="nofollow">Evaluar modelo con 🤗nlp</a></td> <td align="left">Cómo evaluar longformer en TriviaQA con <code>nlp</code></td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb" rel="nofollow">Ajustar fino de T5 para la extracción de amplitud de opinión</a></td> <td align="left">Cómo ajustar T5 para la extracción de intervalos de opiniones mediante un formato de texto a texto con PyTorch Lightning</td> <td align="left"><a href="https://github.com/enzoampil" rel="nofollow">Lorenzo Ampil</a></td> <td align="right"><a href="https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb" rel="nofollow">Ajustar fino de DistilBert para la clasificación multiclase</a></td> <td align="left">Cómo ajustar DistilBert para la clasificación multiclase con PyTorch</td> <td align="left"><a href="https://github.com/abhimishra91" rel="nofollow">Abhishek Kumar Mishra</a></td> <td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb" rel="nofollow">Ajustar BERT para la clasificación de etiquetas múltiples</a></td> <td align="left">Cómo ajustar BERT para la clasificación de múltiples etiquetas usando PyTorch</td> <td align="left"><a href="https://github.com/abhimishra91" rel="nofollow">Abhishek Kumar Mishra</a></td> <td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb" rel="nofollow">Ajustar T5 para resumir</a></td> <td align="left">Cómo ajustar T5 para resumir en PyTorch y realizar un seguimiento de los experimentos con WandB</td> <td align="left"><a href="https://github.com/abhimishra91" rel="nofollow">Abhishek Kumar Mishra</a></td> <td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb" rel="nofollow">Acelerar el ajuste fino en transformadores con Dynamic Padding/Bucketing</a></td> <td align="left">Cómo acelerar el ajuste fino en un factor de 2 usando relleno dinámico/cubetas</td> <td align="left"><a href="https://github.com/pommedeterresautee" rel="nofollow">Michael Benesty</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb" rel="nofollow">Preentrenar Reformer para modelado de lenguaje enmascarado</a></td> <td align="left">Cómo entrenar un modelo Reformer con capas de autoatención bidireccionales</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb" rel="nofollow">Ampliar y ajustar Sci-BERT</a></td> <td align="left">Cómo aumentar el vocabulario de un modelo SciBERT preentrenado de AllenAI en el conjunto de datos CORD y canalizarlo.</td> <td align="left"><a href="https://github.com/lordtt13" rel="nofollow">Tanmay Thakur</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb" rel="nofollow">Ajustar fino de BlenderBotSmall para resúmenes usando la API de Entrenador</a></td> <td align="left">Cómo ajustar BlenderBotSmall para resumir en un conjunto de datos personalizado, utilizando la API de Entrenador.</td> <td align="left"><a href="https://github.com/lordtt13" rel="nofollow">Tanmay Thakur</a></td> <td align="right"><a href="https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb" rel="nofollow">Ajustar Electra e interpreta con gradientes integrados</a></td> <td align="left">Cómo ajustar Electra para el análisis de sentimientos e interpretar predicciones con Captum Integrated Gradients</td> <td align="left"><a href="https://elsanns.github.io" rel="nofollow">Eliza Szczechla</a></td> <td align="right"><a href="https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb" rel="nofollow">ajustar un modelo GPT-2 que no está en inglés con la clase Trainer</a></td> <td align="left">Cómo ajustar un modelo GPT-2 que no está en inglés con la clase Trainer</td> <td align="left"><a href="https://www.philschmid.de" rel="nofollow">Philipp Schmid</a></td> <td align="right"><a href="https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb" rel="nofollow">Ajustar un modelo DistilBERT para la tarea de clasificación de múltiples etiquetas</a></td> <td align="left">Cómo ajustar un modelo DistilBERT para la tarea de clasificación de múltiples etiquetas</td> <td align="left"><a href="https://github.com/DhavalTaunk08" rel="nofollow">Dhaval Taunk</a></td> <td align="right"><a href="https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb" rel="nofollow">Ajustar ALBERT para la clasificación de pares de oraciones</a></td> <td align="left">Cómo ajustar un modelo ALBERT u otro modelo basado en BERT para la tarea de clasificación de pares de oraciones</td> <td align="left"><a href="https://github.com/NadirEM" rel="nofollow">Nadir El Manouzi</a></td> <td align="right"><a href="https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb" rel="nofollow">Ajustar a Roberta para el análisis de sentimientos</a></td> <td align="left">Cómo ajustar un modelo de Roberta para el análisis de sentimientos</td> <td align="left"><a href="https://github.com/DhavalTaunk08" rel="nofollow">Dhaval Taunk</a></td> <td align="right"><a href="https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/flexudy-pipe/qugeev" rel="nofollow">Evaluación de modelos de generación de preguntas</a></td> <td align="left">¿Qué tan precisas son las respuestas a las preguntas generadas por tu modelo de transformador seq2seq?</td> <td align="left"><a href="https://github.com/zolekode" rel="nofollow">Pascal Zoleko</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb" rel="nofollow">Clasificar texto con DistilBERT y Tensorflow</a></td> <td align="left">Cómo ajustar DistilBERT para la clasificación de texto en TensorFlow</td> <td align="left"><a href="https://github.com/peterbayerle" rel="nofollow">Peter Bayerle</a></td> <td align="right"><a href="https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb" rel="nofollow">Aprovechar BERT para el resumen de codificador y decodificador en CNN/Dailymail</a></td> <td align="left">Cómo iniciar en caliente un <em>EncoderDecoderModel</em> con un punto de control <em>google-bert/bert-base-uncased</em> para resumir en CNN/Dailymail</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb" rel="nofollow">Aprovechar RoBERTa para el resumen de codificador-decodificador en BBC XSum</a></td> <td align="left">Cómo iniciar en caliente un <em>EncoderDecoderModel</em> compartido con un punto de control <em>FacebookAI/roberta-base</em> para resumir en BBC/XSum</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb" rel="nofollow">Ajustar TAPAS en Sequential Question Answering (SQA)</a></td> <td align="left">Cómo ajustar <em>TapasForQuestionAnswering</em> con un punto de control <em>tapas-base</em> en el conjunto de datos del Sequential Question Answering (SQA)</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb" rel="nofollow">Evaluar TAPAS en Table Fact Checking (TabFact)</a></td> <td align="left">Cómo evaluar un <em>TapasForSequenceClassification</em> ajustado con un punto de control <em>tapas-base-finetuned-tabfact</em> usando una combinación de 🤗 conjuntos de datos y 🤗 bibliotecas de transformadores</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb" rel="nofollow">Ajustar de mBART para traducción</a></td> <td align="left">Cómo ajustar mBART utilizando Seq2SeqTrainer para la traducción del hindi al inglés</td> <td align="left"><a href="https://github.com/vasudevgupta7" rel="nofollow">Vasudev Gupta</a></td> <td align="right"><a href="https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb" rel="nofollow">Ajustar LayoutLM en FUNSD (a form understanding dataset)</a></td> <td align="left">Cómo ajustar <em>LayoutLMForTokenClassification</em> en el conjunto de datos de FUNSD para la extracción de información de documentos escaneados</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb" rel="nofollow">Ajustar DistilGPT2 y genere texto</a></td> <td align="left">Cómo ajustar DistilGPT2 y generar texto</td> <td align="left"><a href="https://github.com/tripathiaakash" rel="nofollow">Aakash Tripathi</a></td> <td align="right"><a href="https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb" rel="nofollow">Ajustar LED en tokens de hasta 8K</a></td> <td align="left">Cómo ajustar LED en pubmed para resúmenes de largo alcance</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb" rel="nofollow">Evaluar LED en Arxiv</a></td> <td align="left">Cómo evaluar efectivamente LED en resúmenes de largo alcance</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb" rel="nofollow">Ajustar fino de LayoutLM en RVL-CDIP (un conjunto de datos de clasificación de imágenes de documentos)</a></td> <td align="left">Cómo ajustar <em>LayoutLMForSequenceClassification</em> en el conjunto de datos RVL-CDIP para la clasificación de documentos escaneados</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb" rel="nofollow">Decodificación Wav2Vec2 CTC con ajuste GPT2</a></td> <td align="left">Cómo decodificar la secuencia CTC con el ajuste del modelo de lenguaje</td> <td align="left"><a href="https://github.com/voidful" rel="nofollow">Eric Lam</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb" rel="nofollow">Ajustar BART para resúmenes en dos idiomas con la clase Trainer</a></td> <td align="left">Cómo ajustar BART para resúmenes en dos idiomas con la clase Trainer</td> <td align="left"><a href="https://github.com/elsanns" rel="nofollow">Eliza Szczechla</a></td> <td align="right"><a href="https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb" rel="nofollow">Evaluar Big Bird en Trivia QA</a></td> <td align="left">Cómo evaluar BigBird en respuesta a preguntas de documentos largos en Trivia QA</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb" rel="nofollow">Crear subtítulos de video usando Wav2Vec2</a></td> <td align="left">Cómo crear subtítulos de YouTube a partir de cualquier vídeo transcribiendo el audio con Wav2Vec</td> <td align="left"><a href="https://github.com/Muennighoff" rel="nofollow">Niklas Muennighoff</a></td> <td align="right"><a href="https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb" rel="nofollow">Ajustar el transformador de visión en CIFAR-10 usando PyTorch Lightning</a></td> <td align="left">Cómo ajustar el transformador de visión (ViT) en CIFAR-10 usando transformadores HuggingFace, conjuntos de datos y PyTorch Lightning</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb" rel="nofollow">Ajustar el Transformador de visión en CIFAR-10 usando el 🤗 Entrenador</a></td> <td align="left">Cómo ajustar el Vision Transformer (ViT) en CIFAR-10 usando HuggingFace Transformers, Datasets y el 🤗 Trainer</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb" rel="nofollow">Evaluar LUKE en Open Entity, un conjunto de datos de tipificación de entidades</a></td> <td align="left">Cómo evaluar <em>LukeForEntityClassification</em> en el conjunto de datos de entidad abierta</td> <td align="left"><a href="https://github.com/ikuyamada" rel="nofollow">Ikuya Yamada</a></td> <td align="right"><a href="https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb" rel="nofollow">Evaluar LUKE en TACRED, un conjunto de datos de extracción de relaciones</a></td> <td align="left">Cómo evaluar <em>LukeForEntityPairClassification</em> en el conjunto de datos TACRED</td> <td align="left"><a href="https://github.com/ikuyamada" rel="nofollow">Ikuya Yamada</a></td> <td align="right"><a href="https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb" rel="nofollow">Evaluar LUKE en CoNLL-2003, un punto de referencia importante de NER</a></td> <td align="left">Cómo evaluar <em>LukeForEntitySpanClassification</em> en el conjunto de datos CoNLL-2003</td> <td align="left"><a href="https://github.com/ikuyamada" rel="nofollow">Ikuya Yamada</a></td> <td align="right"><a href="https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb" rel="nofollow">Evaluar BigBird-Pegasus en el conjunto de datos de PubMed</a></td> <td align="left">Cómo evaluar <em>BigBirdPegasusForConditionalGeneration</em> en el conjunto de datos de PubMed</td> <td align="left"><a href="https://github.com/vasudevgupta7" rel="nofollow">Vasudev Gupta</a></td> <td align="right"><a href="https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb" rel="nofollow">Clasificación de emociones del habla con Wav2Vec2</a></td> <td align="left">Cómo aprovechar un modelo Wav2Vec2 preentrenado para la clasificación de emociones en el conjunto de datos MEGA</td> <td align="left"><a href="https://github.com/m3hrdadfi" rel="nofollow">Mehrdad Farahani</a></td> <td align="right"><a href="https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb" rel="nofollow">Detectar objetos en una imagen con DETR</a></td> <td align="left">Cómo usar un modelo entrenado <em>DetrForObjectDetection</em> para detectar objetos en una imagen y visualizar la atención</td> <td align="left"><a href="https://github.com/NielsRogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb" rel="nofollow">Ajustar el DETR en un conjunto de datos de detección de objetos personalizados</a></td> <td align="left">Cómo ajustar <em>DetrForObjectDetection</em> en un conjunto de datos de detección de objetos personalizados</td> <td align="left"><a href="https://github.com/NielsRogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr> <tr><td align="left"><a href="https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb" rel="nofollow">Ajustar T5 para el reconocimiento de entidades nombradas</a></td> <td align="left">Cómo ajustar <em>T5</em> en una tarea de reconocimiento de entidad nombrada</td> <td align="left"><a href="https://github.com/ToluClassics" rel="nofollow">Ogundepo Odunayo</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"></a></td></tr></tbody></table>  <p></p> 
			
			<script>
				{
					__sveltekit_1noykze = {
						assets: "/huggingface-transformers/en",
						base: "/huggingface-transformers/en",
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("/huggingface-transformers/en/_app/immutable/entry/start.ede98c4c.js"),
						import("/huggingface-transformers/en/_app/immutable/entry/app.83262e28.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 6],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		
