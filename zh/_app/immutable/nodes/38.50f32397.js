import{s as mr,o as pr,n as Js}from"../chunks/scheduler.9991993c.js";import{S as dr,i as fr,g as s,s as o,r as m,A as cr,h as i,f as n,c as a,j as x,u as p,x as r,k,l as ls,y as b,a as l,v as d,d as f,t as c,w as u,m as ur,n as br}from"../chunks/index.7fc9a5e7.js";import{T as os}from"../chunks/Tip.9de92fc6.js";import{D as U}from"../chunks/Docstring.8180f571.js";import{C as _}from"../chunks/CodeBlock.e11cba92.js";import{H as g}from"../chunks/Heading.e3de321f.js";function gr(Z){let h;return{c(){h=ur("目前，GPTQ量化仅适用于文本模型。此外，量化过程可能会花费很多时间，具体取决于硬件性能（175B模型在NVIDIA A100上需要4小时）。请在Hub上检查是否有模型的GPTQ量化版本。如果没有，您可以在GitHub上提交需求。")},l(M){h=br(M,"目前，GPTQ量化仅适用于文本模型。此外，量化过程可能会花费很多时间，具体取决于硬件性能（175B模型在NVIDIA A100上需要4小时）。请在Hub上检查是否有模型的GPTQ量化版本。如果没有，您可以在GitHub上提交需求。")},m(M,$){l(M,h,$)},d(M){M&&n(h)}}}function hr(Z){let h,M="需要注意的是，一旦模型以 4 位量化方式加载，就无法将量化后的权重推送到 Hub 上。此外，您不能训练 4 位量化权重，因为目前尚不支持此功能。但是，您可以使用 4 位量化模型来训练额外参数，这将在下一部分中介绍。";return{c(){h=s("p"),h.textContent=M},l($){h=i($,"P",{"data-svelte-h":!0}),r(h)!=="svelte-qms63b"&&(h.textContent=M)},m($,T){l($,h,T)},p:Js,d($){$&&n(h)}}}function _r(Z){let h,M="需要注意的是，一旦模型以 8 位量化方式加载，除了使用最新的 <code>transformers</code> 和 <code>bitsandbytes</code> 之外，目前尚无法将量化后的权重推送到 Hub 上。此外，您不能训练 8 位量化权重，因为目前尚不支持此功能。但是，您可以使用 8 位量化模型来训练额外参数，这将在下一部分中介绍。",$,T,Q="注意，<code>device_map</code> 是可选的，但设置 <code>device_map = &#39;auto&#39;</code> 更适合用于推理，因为它将更有效地调度可用资源上的模型。";return{c(){h=s("p"),h.innerHTML=M,$=o(),T=s("p"),T.innerHTML=Q},l(C){h=i(C,"P",{"data-svelte-h":!0}),r(h)!=="svelte-awm97l"&&(h.innerHTML=M),$=a(C),T=i(C,"P",{"data-svelte-h":!0}),r(T)!=="svelte-gophk0"&&(T.innerHTML=Q)},m(C,W){l(C,h,W),l(C,$,W),l(C,T,W)},p:Js,d(C){C&&(n(h),n($),n(T))}}}function $r(Z){let h,M="对大模型，强烈鼓励将 8 位量化模型推送到 Hub 上，以便让社区能够从内存占用减少和加载中受益，例如在 Google Colab 上加载大模型。";return{c(){h=s("p"),h.textContent=M},l($){h=i($,"P",{"data-svelte-h":!0}),r(h)!=="svelte-7qh2nm"&&(h.textContent=M)},m($,T){l($,h,T)},p:Js,d($){$&&n(h)}}}function yr(Z){let h,M,$,T,Q,C,W,ol,D,xs='AWQ方法已经在<a href="https://arxiv.org/abs/2306.00978" rel="nofollow"><em>AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</em>论文</a>中引入。通过AWQ，您可以以4位精度运行模型，同时保留其原始性能（即没有性能降级），并具有比下面介绍的其他量化方法更出色的吞吐量 - 达到与纯<code>float16</code>推理相似的吞吐量。',al,K,ks="我们现在支持使用任何AWQ模型进行推理，这意味着任何人都可以加载和使用在Hub上推送或本地保存的AWQ权重。请注意，使用AWQ需要访问NVIDIA GPU。目前不支持CPU推理。",sl,O,il,ee,Ws="我们建议用户查看生态系统中不同的现有工具，以使用AWQ算法对其模型进行量化，例如：",rl,te,qs='<li><a href="https://github.com/mit-han-lab/llm-awq" rel="nofollow"><code>llm-awq</code></a>，来自MIT Han Lab</li> <li><a href="https://github.com/casper-hansen/AutoAWQ" rel="nofollow"><code>autoawq</code></a>，来自<a href="https://github.com/casper-hansen" rel="nofollow"><code>casper-hansen</code></a></li> <li>Intel neural compressor，来自Intel - 通过<a href="https://huggingface.co/docs/optimum/main/en/intel/optimization_inc" rel="nofollow"><code>optimum-intel</code></a>使用</li>',ml,ne,Us=`生态系统中可能存在许多其他工具，请随时提出PR将它们添加到列表中。
目前与🤗 Transformers的集成仅适用于使用<code>autoawq</code>和<code>llm-awq</code>量化后的模型。大多数使用<code>auto-awq</code>量化的模型可以在🤗 Hub的<a href="https://huggingface.co/TheBloke" rel="nofollow"><code>TheBloke</code></a>命名空间下找到，要使用<code>llm-awq</code>对模型进行量化，请参阅<a href="https://github.com/mit-han-lab/llm-awq/" rel="nofollow"><code>llm-awq</code></a>的示例文件夹中的<a href="https://github.com/mit-han-lab/llm-awq/blob/main/examples/convert_to_hf.py" rel="nofollow"><code>convert_to_hf.py</code></a>脚本。`,pl,le,dl,oe,Zs="您可以使用<code>from_pretrained</code>方法从Hub加载一个量化模型。通过检查模型配置文件（<code>configuration.json</code>）中是否存在<code>quantization_config</code>属性，来进行确认推送的权重是量化的。您可以通过检查字段<code>quantization_config.quant_method</code>来确认模型是否以AWQ格式进行量化，该字段应该设置为<code>&quot;awq&quot;</code>。请注意，为了性能原因，默认情况下加载模型将设置其他权重为<code>float16</code>。如果您想更改这种设置，可以通过将<code>torch_dtype</code>参数设置为<code>torch.float32</code>或<code>torch.bfloat16</code>。在下面的部分中，您可以找到一些示例片段和notebook。",fl,ae,cl,se,Qs='首先，您需要安装<a href="https://github.com/casper-hansen/AutoAWQ" rel="nofollow"><code>autoawq</code></a>库',ul,ie,bl,re,gl,me,Gs="如果您首先将模型加载到CPU上，请确保在使用之前将其移动到GPU设备上。",hl,pe,_l,de,$l,fe,js="您可以将AWQ量化与Flash Attention结合起来，得到一个既被量化又更快速的模型。只需使用<code>from_pretrained</code>加载模型，并传递<code>attn_implementation=&quot;flash_attention_2&quot;</code>参数。",yl,ce,Ml,ue,Tl,be,zs='我们使用<a href="https://github.com/huggingface/optimum-benchmark" rel="nofollow"><code>optimum-benchmark</code></a>库进行了一些速度、吞吐量和延迟基准测试。',wl,ge,Bs="请注意，在编写本文档部分时，可用的量化方法包括：<code>awq</code>、<code>gptq</code>和<code>bitsandbytes</code>。",vl,he,Vs='基准测试在一台NVIDIA-A100实例上运行，使用<a href="https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ" rel="nofollow"><code>TheBloke/Mistral-7B-v0.1-AWQ</code></a>作为AWQ模型，<a href="https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ" rel="nofollow"><code>TheBloke/Mistral-7B-v0.1-GPTQ</code></a>作为GPTQ模型。我们还将其与<code>bitsandbytes</code>量化模型和<code>float16</code>模型进行了对比。以下是一些结果示例：',Cl,j,Xs='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_memory_plot.png"/>',Jl,z,Ls='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_memory_plot.png"/>',xl,B,Hs='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_throughput_plot.png"/>',kl,V,As='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_latency_plot.png"/>',Wl,_e,Rs='你可以在<a href="https://github.com/huggingface/optimum-benchmark/tree/main/examples/running-mistrals" rel="nofollow">此链接</a>中找到完整的结果以及包版本。',ql,$e,Fs="从结果来看，AWQ量化方法是推理、文本生成中最快的量化方法，并且在文本生成的峰值内存方面属于最低。然而，对于每批数据，AWQ似乎有最大的前向延迟。",Ul,ye,Zl,Me,Ns='查看如何在<a href="https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY" rel="nofollow">Google Colab演示</a>中使用此集成！',Ql,Te,Gl,q,we,as,Fn,Is=`This is a wrapper class about all possible attributes and features that you can play with a model that has been
loaded using <code>auto-awq</code> library awq quantization relying on auto_awq backend.`,ss,X,ve,is,Nn,Ps="Safety checker that arguments are correct",jl,Ce,zl,Je,Es="🤗 Transformers已经整合了<code>optimum</code> API，用于对语言模型执行GPTQ量化。您可以以8、4、3甚至2位加载和量化您的模型，而性能无明显下降，并且推理速度更快！这受到大多数GPU硬件的支持。",Bl,xe,Ys="要了解更多关于量化模型的信息，请查看：",Vl,ke,Ss='<li><a href="https://arxiv.org/pdf/2210.17323.pdf" rel="nofollow">GPTQ</a>论文</li> <li><code>optimum</code>关于GPTQ量化的<a href="https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization" rel="nofollow">指南</a></li> <li>用作后端的<a href="https://github.com/PanQiWei/AutoGPTQ" rel="nofollow"><code>AutoGPTQ</code></a>库</li>',Xl,We,Ll,qe,Ds="为了运行下面的代码，您需要安装：",Hl,Ue,Ks=`<li><p>安装最新版本的 <code>AutoGPTQ</code> 库
<code>pip install auto-gptq</code></p></li> <li><p>从源代码安装最新版本的<code>optimum</code> <code>pip install git+https://github.com/huggingface/optimum.git</code></p></li> <li><p>从源代码安装最新版本的<code>transformers</code> <code>pip install git+https://github.com/huggingface/transformers.git</code></p></li> <li><p>安装最新版本的<code>accelerate</code>库：
<code>pip install --upgrade accelerate</code></p></li>`,Al,Ze,Os="请注意，目前GPTQ集成仅支持文本模型，对于视觉、语音或多模态模型可能会遇到预期以外结果。",Rl,Qe,Fl,Ge,ei="GPTQ是一种在使用量化模型之前需要进行权重校准的量化方法。如果您想从头开始对transformers模型进行量化，生成量化模型可能需要一些时间（在Google Colab上对<code>facebook/opt-350m</code>模型量化约为5分钟）。",Nl,je,ti="因此，有两种不同的情况下您可能想使用GPTQ量化模型。第一种情况是加载已经由其他用户在Hub上量化的模型，第二种情况是从头开始对您的模型进行量化并保存或推送到Hub，以便其他用户也可以使用它。",Il,ze,Pl,Be,ni='为了加载和量化一个模型，您需要创建一个<a href="/docs/transformers/main/zh/main_classes/quantization#transformers.GPTQConfig">GPTQConfig</a>。您需要传递<code>bits</code>的数量，一个用于校准量化的<code>dataset</code>，以及模型的<code>tokenizer</code>以准备数据集。',El,Ve,Yl,Xe,li="请注意，您可以将自己的数据集以字符串列表形式传递到模型。然而，强烈建议您使用GPTQ论文中提供的数据集。",Sl,Le,Dl,He,Kl,Ae,oi="您可以通过使用<code>from_pretrained</code>并设置<code>quantization_config</code>来对模型进行量化。",Ol,Re,eo,Fe,ai="请注意，您需要一个GPU来量化模型。我们将模型放在cpu中，并将模块来回移动到gpu中，以便对其进行量化。",to,Ne,si="如果您想在使用 CPU 卸载的同时最大化 GPU 使用率，您可以设置 <code>device_map = &quot;auto&quot;</code>。",no,Ie,lo,Pe,ii='请注意，不支持磁盘卸载。此外，如果由于数据集而内存不足，您可能需要在<code>from_pretrained</code>中设置<code>max_memory</code>。查看这个<a href="https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map" rel="nofollow">指南</a>以了解有关<code>device_map</code>和<code>max_memory</code>的更多信息。',oo,L,ao,Ee,so,Ye,ri="您可以使用<code>push_to_hub</code>将量化模型像任何模型一样推送到Hub。量化配置将与模型一起保存和推送。",io,Se,ro,De,mi="如果您想在本地计算机上保存量化模型，您也可以使用<code>save_pretrained</code>来完成：",mo,Ke,po,Oe,pi="请注意，如果您量化模型时想使用<code>device_map</code>，请确保在保存之前将整个模型移动到您的GPU或CPU之一。",fo,et,co,tt,uo,nt,di=`您可以使用<code>from_pretrained</code>从Hub加载量化模型。
请确保推送权重是量化的，检查模型配置对象中是否存在<code>quantization_config</code>属性。`,bo,lt,go,ot,fi="如果您想更快地加载模型，并且不需要分配比实际需要内存更多的内存，量化模型也使用<code>device_map</code>参数。确保您已安装<code>accelerate</code>库。",ho,at,_o,st,$o,it,ci='保留格式：对于 4 位模型，您可以使用 exllama 内核来提高推理速度。默认情况下，它处于启用状态。您可以通过在 <a href="/docs/transformers/main/zh/main_classes/quantization#transformers.GPTQConfig">GPTQConfig</a> 中传递 <code>use_exllama</code> 来更改此配置。这将覆盖存储在配置中的量化配置。请注意，您只能覆盖与内核相关的属性。此外，如果您想使用 exllama 内核，整个模型需要全部部署在 gpus 上。此外，您可以使用 版本 &gt; 0.4.2 的 Auto-GPTQ 并传递 <code>device_map</code> = “cpu” 来执行 CPU 推理。对于 CPU 推理，您必须在 <code>GPTQConfig</code> 中传递 <code>use_exllama = False</code>。',yo,rt,Mo,mt,ui='随着 exllamav2 内核的发布，与 exllama 内核相比，您可以获得更快的推理速度。您只需在 <a href="/docs/transformers/main/zh/main_classes/quantization#transformers.GPTQConfig">GPTQConfig</a> 中传递 <code>exllama_config={&quot;version&quot;: 2}</code>：',To,pt,wo,dt,bi="请注意，目前仅支持 4 位模型。此外，如果您正在使用 peft 对量化模型进行微调，建议禁用 exllama 内核。",vo,ft,gi='您可以在此找到这些内核的基准测试 <a href="https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark" rel="nofollow">这里</a>',Co,ct,Jo,ut,hi=`在Hugging Face生态系统的官方支持下，您可以使用GPTQ进行量化后的模型进行微调。
请查看<code>peft</code>库了解更多详情。`,xo,bt,ko,gt,_i='请查看 Google Colab <a href="https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94ilkUFu6ZX4ceb?usp=sharing" rel="nofollow">notebook</a>，了解如何使用GPTQ量化您的模型以及如何使用peft微调量化模型。',Wo,ht,qo,v,_t,rs,In,$i=`This is a wrapper class about all possible attributes and features that you can play with a model that has been
loaded using <code>optimum</code> api for gptq quantization relying on auto_gptq backend.`,ms,H,$t,ps,Pn,yi="Get compatible class with optimum gptq config dict",ds,A,yt,fs,En,Mi="Safety checker that arguments are correct",cs,R,Mt,us,Yn,Ti="Get compatible dict for optimum gptq config",Uo,Tt,Zo,wt,wi=`🤗 Transformers 与 <code>bitsandbytes</code> 上最常用的模块紧密集成。您可以使用几行代码以 8 位精度加载您的模型。
自bitsandbytes的0.37.0版本发布以来，大多数GPU硬件都支持这一点。`,Qo,vt,vi='在<a href="https://arxiv.org/abs/2208.07339" rel="nofollow">LLM.int8()</a>论文中了解更多关于量化方法的信息，或者在<a href="https://huggingface.co/blog/hf-bitsandbytes-integration" rel="nofollow">博客文章</a>中了解关于合作的更多信息。',Go,Ct,Ci="自其“0.39.0”版本发布以来，您可以使用FP4数据类型，通过4位量化加载任何支持“device_map”的模型。",jo,Jt,Ji='如果您想量化自己的 pytorch 模型，请查看 🤗 Accelerate 的<a href="https://huggingface.co/docs/accelerate/main/en/usage_guides/quantization" rel="nofollow">文档</a>。',zo,xt,xi="以下是您可以使用“bitsandbytes”集成完成的事情",Bo,kt,Vo,Wt,ki='只要您的模型支持使用 🤗 Accelerate 进行加载并包含 <code>torch.nn.Linear</code> 层，您可以在调用 <a href="/docs/transformers/main/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> 方法时使用 <code>load_in_8bit</code> 或 <code>load_in_4bit</code> 参数来量化模型。这也应该适用于任何模态。',Xo,qt,Lo,Ut,Wi="默认情况下，所有其他模块（例如 <code>torch.nn.LayerNorm</code>）将被转换为 <code>torch.float16</code> 类型。但如果您想更改它们的 <code>dtype</code>，可以重载 <code>torch_dtype</code> 参数：",Ho,Zt,Ao,Qt,Ro,Gt,Fo,jt,qi="确保在运行以下代码段之前已完成以下要求：",No,zt,Ui=`<li><p>最新版本 <code>bitsandbytes</code> 库
<code>pip install bitsandbytes&gt;=0.39.0</code></p></li> <li><p>安装最新版本 <code>accelerate</code> <code>pip install --upgrade accelerate</code></p></li> <li><p>安装最新版本 <code>transformers</code> <code>pip install --upgrade transformers</code></p></li>`,Io,Bt,Po,Vt,Zi='<li><p><strong>高级用法：</strong> 请参考 <a href="https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf" rel="nofollow">此 Google Colab notebook</a> 以获取 4 位量化高级用法和所有可选选项。</p></li> <li><p><strong>使用 <code>batch_size=1</code> 实现更快的推理：</strong> 自 <code>bitsandbytes</code> 的 <code>0.40.0</code> 版本以来，设置 <code>batch_size=1</code>，您可以从快速推理中受益。请查看 <a href="https://github.com/TimDettmers/bitsandbytes/releases/tag/0.40.0" rel="nofollow">这些发布说明</a> ，并确保使用大于 <code>0.40.0</code> 的版本以直接利用此功能。</p></li> <li><p><strong>训练：</strong> 根据 <a href="https://arxiv.org/abs/2305.14314" rel="nofollow">QLoRA 论文</a>，对于4位基模型训练（使用 LoRA 适配器），应使用 <code>bnb_4bit_quant_type=&#39;nf4&#39;</code>。</p></li> <li><p><strong>推理：</strong> 对于推理，<code>bnb_4bit_quant_type</code> 对性能影响不大。但是为了与模型的权重保持一致，请确保使用相同的 <code>bnb_4bit_compute_dtype</code> 和 <code>torch_dtype</code> 参数。</p></li>',Eo,Xt,Yo,Lt,Qi="在调用 <code>.from_pretrained</code> 方法时使用 <code>load_in_4bit=True</code>，可以将您的内存使用量减少到大约原来的 1/4。",So,Ht,Do,F,Ko,At,Oo,Rt,Gi="您可以通过在调用 <code>.from_pretrained</code> 方法时使用 <code>load_in_8bit=True</code> 参数，将内存需求大致减半来加载模型",ea,Ft,ta,Nt,ji="然后，像通常使用 <code>PreTrainedModel</code> 一样使用您的模型。",na,It,zi="您可以使用 <code>get_memory_footprint</code> 方法检查模型的内存占用。",la,Pt,oa,Et,Bi="通过这种集成，我们能够在较小的设备上加载大模型并运行它们而没有任何问题。",aa,N,sa,Yt,ia,St,Vi="在这里，我们将介绍使用 FP4 量化的一些高级用例。",ra,Dt,ma,Kt,Xi="计算数据类型用于改变在进行计算时使用的数据类型。例如，hidden states可以是 <code>float32</code>，但为了加速，计算时可以被设置为 <code>bf16</code>。默认情况下，计算数据类型被设置为 <code>float32</code>。",pa,Ot,da,en,fa,tn,Li="您还可以使用 NF4 数据类型，这是一种针对使用正态分布初始化的权重而适应的新型 4 位数据类型。要运行：",ca,nn,ua,ln,ba,on,Hi="我们还建议用户使用嵌套量化技术。从我们的经验观察来看，这种方法在不增加额外性能的情况下节省更多内存。这使得 llama-13b 模型能够在具有 1024 个序列长度、1 个批次大小和 4 个梯度累积步骤的 NVIDIA-T4 16GB 上进行 fine-tuning。",ga,an,ha,sn,_a,rn,Ai=`您可以使用 <code>push_to_hub</code> 方法将量化模型推送到 Hub 上。这将首先推送量化配置文件，然后推送量化模型权重。
请确保使用 <code>bitsandbytes&gt;0.37.2</code>（在撰写本文时，我们使用的是 <code>bitsandbytes==0.38.0.post1</code>）才能使用此功能。`,$a,mn,ya,I,Ma,pn,Ta,dn,Ri="您可以使用 <code>from_pretrained</code> 方法从 Hub 加载量化模型。请确保推送的权重是量化的，检查模型配置对象中是否存在 <code>quantization_config</code> 属性。",wa,fn,va,cn,Fi=`请注意，在这种情况下，您不需要指定 <code>load_in_8bit=True</code> 参数，但需要确保 <code>bitsandbytes</code> 和 <code>accelerate</code> 已安装。
情注意，<code>device_map</code> 是可选的，但设置 <code>device_map = &#39;auto&#39;</code> 更适合用于推理，因为它将更有效地调度可用资源上的模型。`,Ca,un,Ja,bn,Ni="本节面向希望探索除了加载和运行 8 位模型之外还能做什么的进阶用户。",xa,gn,ka,hn,Ii="此高级用例之一是能够加载模型并将权重分派到 <code>CPU</code> 和 <code>GPU</code> 之间。请注意，将在 CPU 上分派的权重 <strong>不会</strong> 转换为 8 位，因此会保留为 <code>float32</code>。此功能适用于想要适应非常大的模型并将模型分派到 GPU 和 CPU 之间的用户。",Wa,_n,Pi='首先，从 <code>transformers</code> 中加载一个 <a href="/docs/transformers/main/zh/main_classes/quantization#transformers.BitsAndBytesConfig">BitsAndBytesConfig</a>，并将属性 <code>llm_int8_enable_fp32_cpu_offload</code> 设置为 <code>True</code>：',qa,$n,Ua,yn,Ei="假设您想加载 <code>bigscience/bloom-1b7</code> 模型，您的 GPU显存仅足够容纳除了<code>lm_head</code>外的整个模型。因此，您可以按照以下方式编写自定义的 device_map：",Za,Mn,Qa,Tn,Yi="然后如下加载模型：",Ga,wn,ja,vn,Si="这就是全部内容！享受您的模型吧！",za,Cn,Ba,Jn,Di=`您可以使用 <code>llm_int8_threshold</code> 参数来更改异常值的阈值。“异常值”是一个大于特定阈值的<code>hidden state</code>值。
这对应于<code>LLM.int8()</code>论文中描述的异常检测的异常阈值。任何高于此阈值的<code>hidden state</code>值都将被视为异常值，对这些值的操作将在 fp16 中完成。值通常是正态分布的，也就是说，大多数值在 [-3.5, 3.5] 范围内，但有一些额外的系统异常值，对于大模型来说，它们的分布非常不同。这些异常值通常在区间 [-60, -6] 或 [6, 60] 内。Int8 量化对于幅度为 ~5 的值效果很好，但超出这个范围，性能就会明显下降。一个好的默认阈值是 6，但对于更不稳定的模型（小模型、微调）可能需要更低的阈值。
这个参数会影响模型的推理速度。我们建议尝试这个参数，以找到最适合您的用例的参数。`,Va,xn,Xa,kn,La,Wn,Ki="一些模型有几个需要保持未转换状态以确保稳定性的模块。例如，Jukebox 模型有几个 <code>lm_head</code> 模块需要跳过。使用 <code>llm_int8_skip_modules</code> 参数进行相应操作。",Ha,qn,Aa,Un,Ra,Zn,Oi='借助Hugging Face生态系统中适配器（adapters）的官方支持，您可以在8位精度下微调模型。这使得可以在单个Google Colab中微调大模型，例如<code>flan-t5-large</code>或<code>facebook/opt-6.7b</code>。请查看<a href="https://github.com/huggingface/peft" rel="nofollow"><code>peft</code></a>库了解更多详情。',Fa,Qn,er="注意，加载模型进行训练时无需传递<code>device_map</code>。它将自动将您的模型加载到GPU上。如果需要，您可以将设备映射为特定设备（例如<code>cuda:0</code>、<code>0</code>、<code>torch.device(&#39;cuda:0&#39;)</code>）。请注意，<code>device_map=auto</code>仅应用于推理。",Na,Gn,Ia,y,jn,bs,Sn,tr=`This is a wrapper class about all possible attributes and features that you can play with a model that has been
loaded using <code>bitsandbytes</code>.`,gs,Dn,nr="This replaces <code>load_in_8bit</code> or <code>load_in_4bit</code>therefore both options are mutually exclusive.",hs,Kn,lr=`Currently only supports <code>LLM.int8()</code>, <code>FP4</code>, and <code>NF4</code> quantization. If more methods are added to <code>bitsandbytes</code>,
then more arguments will be added to this class.`,_s,P,zn,$s,On,or="Returns <code>True</code> if the model is quantizable, <code>False</code> otherwise.",ys,E,Bn,Ms,el,ar="Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.",Ts,Y,Vn,ws,tl,sr=`This method returns the quantization method used for the model. If the model is not quantizable, it returns
<code>None</code>.`,vs,S,Xn,Cs,nl,ir=`Removes all attributes from config which correspond to the default config attributes for better readability and
serializes to a Python dictionary.`,Pa,Ln,Ea,Hn,rr='请查看<a href="https://huggingface.co/docs/optimum/index" rel="nofollow">Optimum 文档</a>以了解更多关于<code>optimum</code>支持的量化方法，并查看这些方法是否适用于您的用例。',Ya,ll,Sa;return Q=new g({props:{title:"量化 🤗 Transformers 模型",local:"量化--transformers-模型",headingTag:"h1"}}),W=new g({props:{title:"AWQ集成",local:"awq集成",headingTag:"h2"}}),O=new g({props:{title:"量化一个模型",local:"量化一个模型",headingTag:"h3"}}),le=new g({props:{title:"加载一个量化的模型",local:"加载一个量化的模型",headingTag:"h3"}}),ae=new g({props:{title:"示例使用",local:"示例使用",headingTag:"h2"}}),ie=new _({props:{code:"cGlwJTIwaW5zdGFsbCUyMGF1dG9hd3E=",highlighted:"pip install autoawq",wrap:!1}}),re=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyVGhlQmxva2UlMkZ6ZXBoeXItN0ItYWxwaGEtQVdRJTIyJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyY3VkYSUzQTAlMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span>
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;cuda:0&quot;</span>)`,wrap:!1}}),pe=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyVGhlQmxva2UlMkZ6ZXBoeXItN0ItYWxwaGEtQVdRJTIyJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpLnRvKCUyMmN1ZGElM0EwJTIyKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span>
model = AutoModelForCausalLM.from_pretrained(model_id).to(<span class="hljs-string">&quot;cuda:0&quot;</span>)`,wrap:!1}}),de=new g({props:{title:"结合 AWQ 和 Flash Attention",local:"结合-awq-和-flash-attention",headingTag:"h3"}}),ce=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMlRoZUJsb2tlJTJGemVwaHlyLTdCLWFscGhhLUFXUSUyMiUyQyUyMGF0dG5faW1wbGVtZW50YXRpb24lM0QlMjJmbGFzaF9hdHRlbnRpb25fMiUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJjdWRhJTNBMCUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span>, attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>, device_map=<span class="hljs-string">&quot;cuda:0&quot;</span>)`,wrap:!1}}),ue=new g({props:{title:"基准测试",local:"基准测试",headingTag:"h3"}}),ye=new g({props:{title:"Google colab 演示",local:"google-colab-演示",headingTag:"h3"}}),Te=new g({props:{title:"AwqConfig",local:"transformers.AwqConfig",headingTag:"h3"}}),we=new U({props:{name:"class transformers.AwqConfig",anchor:"transformers.AwqConfig",parameters:[{name:"bits",val:": int = 4"},{name:"group_size",val:": int = 128"},{name:"zero_point",val:": bool = True"},{name:"version",val:": AWQLinearVersion = <AWQLinearVersion.GEMM: 'gemm'>"},{name:"backend",val:": AwqBackendPackingMethod = <AwqBackendPackingMethod.AUTOAWQ: 'autoawq'>"},{name:"do_fuse",val:": Optional = None"},{name:"fuse_max_seq_len",val:": Optional = None"},{name:"modules_to_fuse",val:": Optional = None"},{name:"modules_to_not_convert",val:": Optional = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AwqConfig.bits",description:`<strong>bits</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The number of bits to quantize to.`,name:"bits"},{anchor:"transformers.AwqConfig.group_size",description:`<strong>group_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The group size to use for quantization. Recommended value is 128 and -1 uses per-column quantization.`,name:"group_size"},{anchor:"transformers.AwqConfig.zero_point",description:`<strong>zero_point</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use zero point quantization.`,name:"zero_point"},{anchor:"transformers.AwqConfig.version",description:`<strong>version</strong> (<code>AWQLinearVersion</code>, <em>optional</em>, defaults to <code>AWQLinearVersion.GEMM</code>) &#x2014;
The version of the quantization algorithm to use. GEMM is better for big batch_size (e.g. &gt;= 8) otherwise,
GEMV is better (e.g. &lt; 8 )`,name:"version"},{anchor:"transformers.AwqConfig.backend",description:`<strong>backend</strong> (<code>AwqBackendPackingMethod</code>, <em>optional</em>, defaults to <code>AwqBackendPackingMethod.AUTOAWQ</code>) &#x2014;
The quantization backend. Some models might be quantized using <code>llm-awq</code> backend. This is useful for users
that quantize their own models using <code>llm-awq</code> library.`,name:"backend"},{anchor:"transformers.AwqConfig.do_fuse",description:`<strong>do_fuse</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to fuse attention and mlp layers together for faster inference`,name:"do_fuse"},{anchor:"transformers.AwqConfig.fuse_max_seq_len",description:`<strong>fuse_max_seq_len</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The Maximum sequence length to generate when using fusing.`,name:"fuse_max_seq_len"},{anchor:"transformers.AwqConfig.modules_to_fuse",description:`<strong>modules_to_fuse</strong> (<code>dict</code>, <em>optional</em>, default to <code>None</code>) &#x2014;
Overwrite the natively supported fusing scheme with the one specified by the users.`,name:"modules_to_fuse"},{anchor:"transformers.AwqConfig.modules_to_not_convert",description:`<strong>modules_to_not_convert</strong> (<code>list</code>, <em>optional</em>, default to <code>None</code>) &#x2014;
The list of modules to not quantize, useful for quantizing models that explicitly require to have
some modules left in their original precision (e.g. Whisper encoder, Llava encoder, Mixtral gate layers).
Note you cannot quantize directly with transformers, please refer to <code>AutoAWQ</code> documentation for quantizing HF models.`,name:"modules_to_not_convert"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L595"}}),ve=new U({props:{name:"post_init",anchor:"transformers.AwqConfig.post_init",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L658"}}),Ce=new g({props:{title:"AutoGPTQ 集成",local:"autogptq-集成",headingTag:"h2"}}),We=new g({props:{title:"要求",local:"要求",headingTag:"h3"}}),Qe=new g({props:{title:"加载和量化模型",local:"加载和量化模型",headingTag:"h3"}}),ze=new g({props:{title:"GPTQ 配置",local:"gptq-配置",headingTag:"h4"}}),Ve=new _({props:{code:"bW9kZWxfaWQlMjAlM0QlMjAlMjJmYWNlYm9vayUyRm9wdC0xMjVtJTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBZ3B0cV9jb25maWclMjAlM0QlMjBHUFRRQ29uZmlnKGJpdHMlM0Q0JTJDJTIwZGF0YXNldCUyMCUzRCUyMCUyMmM0JTIyJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKQ==",highlighted:`model_id = <span class="hljs-string">&quot;facebook/opt-125m&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>, dataset = <span class="hljs-string">&quot;c4&quot;</span>, tokenizer=tokenizer)`,wrap:!1}}),Le=new _({props:{code:"ZGF0YXNldCUyMCUzRCUyMCU1QiUyMmF1dG8tZ3B0cSUyMGlzJTIwYW4lMjBlYXN5LXRvLXVzZSUyMG1vZGVsJTIwcXVhbnRpemF0aW9uJTIwbGlicmFyeSUyMHdpdGglMjB1c2VyLWZyaWVuZGx5JTIwYXBpcyUyQyUyMGJhc2VkJTIwb24lMjBHUFRRJTIwYWxnb3JpdGhtLiUyMiU1RCUwQXF1YW50aXphdGlvbiUyMCUzRCUyMEdQVFFDb25maWcoYml0cyUzRDQlMkMlMjBkYXRhc2V0JTIwJTNEJTIwZGF0YXNldCUyQyUyMHRva2VuaXplciUzRHRva2VuaXplcik=",highlighted:`dataset = [<span class="hljs-string">&quot;auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.&quot;</span>]
quantization = GPTQConfig(bits=<span class="hljs-number">4</span>, dataset = dataset, tokenizer=tokenizer)`,wrap:!1}}),He=new g({props:{title:"量化",local:"量化",headingTag:"h4"}}),Re=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEZ3B0cV9jb25maWcpJTBB",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=gptq_config)
`,wrap:!1}}),Ie=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMHF1YW50aXphdGlvbl9jb25maWclM0RncHRxX2NvbmZpZyk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config=gptq_config)`,wrap:!1}}),L=new os({props:{warning:!0,$$slots:{default:[gr]},$$scope:{ctx:Z}}}),Ee=new g({props:{title:"推送量化模型到 🤗 Hub",local:"推送量化模型到--hub",headingTag:"h3"}}),Se=new _({props:{code:"cXVhbnRpemVkX21vZGVsLnB1c2hfdG9faHViKCUyMm9wdC0xMjVtLWdwdHElMjIpJTBBdG9rZW5pemVyLnB1c2hfdG9faHViKCUyMm9wdC0xMjVtLWdwdHElMjIp",highlighted:`quantized_model.push_to_hub(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)
tokenizer.push_to_hub(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),Ke=new _({props:{code:"cXVhbnRpemVkX21vZGVsLnNhdmVfcHJldHJhaW5lZCglMjJvcHQtMTI1bS1ncHRxJTIyKSUwQXRva2VuaXplci5zYXZlX3ByZXRyYWluZWQoJTIyb3B0LTEyNW0tZ3B0cSUyMik=",highlighted:`quantized_model.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)
tokenizer.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),et=new _({props:{code:"cXVhbnRpemVkX21vZGVsLnRvKCUyMmNwdSUyMiklMEFxdWFudGl6ZWRfbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKCUyMm9wdC0xMjVtLWdwdHElMjIp",highlighted:`quantized_model.to(<span class="hljs-string">&quot;cpu&quot;</span>)
quantized_model.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),tt=new g({props:{title:"从 🤗 Hub 加载一个量化模型",local:"从--hub-加载一个量化模型",headingTag:"h3"}}),lt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyJTdCeW91cl91c2VybmFtZSU3RCUyRm9wdC0xMjVtLWdwdHElMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>)`,wrap:!1}}),at=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyJTdCeW91cl91c2VybmFtZSU3RCUyRm9wdC0xMjVtLWdwdHElMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),st=new g({props:{title:"Exllama内核加快推理速度",local:"exllama内核加快推理速度",headingTag:"h3"}}),rt=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFncHRxX2NvbmZpZyUyMCUzRCUyMEdQVFFDb25maWcoYml0cyUzRDQpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyJTdCeW91cl91c2VybmFtZSU3RCUyRm9wdC0xMjVtLWdwdHElMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMHF1YW50aXphdGlvbl9jb25maWclM0RncHRxX2NvbmZpZyk=",highlighted:`<span class="hljs-keyword">import</span> torch
gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config=gptq_config)`,wrap:!1}}),pt=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFncHRxX2NvbmZpZyUyMCUzRCUyMEdQVFFDb25maWcoYml0cyUzRDQlMkMlMjBleGxsYW1hX2NvbmZpZyUzRCU3QiUyMnZlcnNpb24lMjIlM0EyJTdEKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMiU3QnlvdXJfdXNlcm5hbWUlN0QlMkZvcHQtMTI1bS1ncHRxJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwZ3B0cV9jb25maWcp",highlighted:`<span class="hljs-keyword">import</span> torch
gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>, exllama_config={<span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-number">2</span>})
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config = gptq_config)`,wrap:!1}}),ct=new g({props:{title:"微调一个量化模型",local:"微调一个量化模型",headingTag:"h4"}}),bt=new g({props:{title:"示例演示",local:"示例演示",headingTag:"h3"}}),ht=new g({props:{title:"GPTQConfig",local:"transformers.GPTQConfig",headingTag:"h3"}}),_t=new U({props:{name:"class transformers.GPTQConfig",anchor:"transformers.GPTQConfig",parameters:[{name:"bits",val:": int"},{name:"tokenizer",val:": Any = None"},{name:"dataset",val:": Union = None"},{name:"group_size",val:": int = 128"},{name:"damp_percent",val:": float = 0.1"},{name:"desc_act",val:": bool = False"},{name:"sym",val:": bool = True"},{name:"true_sequential",val:": bool = True"},{name:"use_cuda_fp16",val:": bool = False"},{name:"model_seqlen",val:": Optional = None"},{name:"block_name_to_quantize",val:": Optional = None"},{name:"module_name_preceding_first_block",val:": Optional = None"},{name:"batch_size",val:": int = 1"},{name:"pad_token_id",val:": Optional = None"},{name:"use_exllama",val:": Optional = None"},{name:"max_input_length",val:": Optional = None"},{name:"exllama_config",val:": Optional = None"},{name:"cache_block_outputs",val:": bool = True"},{name:"modules_in_block_to_quantize",val:": Optional = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTQConfig.bits",description:`<strong>bits</strong> (<code>int</code>) &#x2014;
The number of bits to quantize to, supported numbers are (2, 3, 4, 8).`,name:"bits"},{anchor:"transformers.GPTQConfig.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizerBase</code>, <em>optional</em>) &#x2014;
The tokenizer used to process the dataset. You can pass either:<ul>
<li>A custom tokenizer object.</li>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/main/zh/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"tokenizer"},{anchor:"transformers.GPTQConfig.dataset",description:`<strong>dataset</strong> (<code>Union[List[str]]</code>, <em>optional</em>) &#x2014;
The dataset used for quantization. You can provide your own dataset in a list of string or just use the
original datasets used in GPTQ paper [&#x2018;wikitext2&#x2019;,&#x2018;c4&#x2019;,&#x2018;c4-new&#x2019;,&#x2018;ptb&#x2019;,&#x2018;ptb-new&#x2019;]`,name:"dataset"},{anchor:"transformers.GPTQConfig.group_size",description:`<strong>group_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The group size to use for quantization. Recommended value is 128 and -1 uses per-column quantization.`,name:"group_size"},{anchor:"transformers.GPTQConfig.damp_percent",description:`<strong>damp_percent</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The percent of the average Hessian diagonal to use for dampening. Recommended value is 0.1.`,name:"damp_percent"},{anchor:"transformers.GPTQConfig.desc_act",description:`<strong>desc_act</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to quantize columns in order of decreasing activation size. Setting it to False can significantly
speed up inference but the perplexity may become slightly worse. Also known as act-order.`,name:"desc_act"},{anchor:"transformers.GPTQConfig.sym",description:`<strong>sym</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use symetric quantization.`,name:"sym"},{anchor:"transformers.GPTQConfig.true_sequential",description:`<strong>true_sequential</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to perform sequential quantization even within a single Transformer block. Instead of quantizing
the entire block at once, we perform layer-wise quantization. As a result, each layer undergoes
quantization using inputs that have passed through the previously quantized layers.`,name:"true_sequential"},{anchor:"transformers.GPTQConfig.use_cuda_fp16",description:`<strong>use_cuda_fp16</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use optimized cuda kernel for fp16 model. Need to have model in fp16.`,name:"use_cuda_fp16"},{anchor:"transformers.GPTQConfig.model_seqlen",description:`<strong>model_seqlen</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum sequence length that the model can take.`,name:"model_seqlen"},{anchor:"transformers.GPTQConfig.block_name_to_quantize",description:`<strong>block_name_to_quantize</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The transformers block name to quantize. If None, we will infer the block name using common patterns (e.g. model.layers)`,name:"block_name_to_quantize"},{anchor:"transformers.GPTQConfig.module_name_preceding_first_block",description:`<strong>module_name_preceding_first_block</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
The layers that are preceding the first Transformer block.`,name:"module_name_preceding_first_block"},{anchor:"transformers.GPTQConfig.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The batch size used when processing the dataset`,name:"batch_size"},{anchor:"transformers.GPTQConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The pad token id. Needed to prepare the dataset when <code>batch_size</code> &gt; 1.`,name:"pad_token_id"},{anchor:"transformers.GPTQConfig.use_exllama",description:`<strong>use_exllama</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to use exllama backend. Defaults to <code>True</code> if unset. Only works with <code>bits</code> = 4.`,name:"use_exllama"},{anchor:"transformers.GPTQConfig.max_input_length",description:`<strong>max_input_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum input length. This is needed to initialize a buffer that depends on the maximum expected input
length. It is specific to the exllama backend with act-order.`,name:"max_input_length"},{anchor:"transformers.GPTQConfig.exllama_config",description:`<strong>exllama_config</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The exllama config. You can specify the version of the exllama kernel through the <code>version</code> key. Defaults
to <code>{&quot;version&quot;: 1}</code> if unset.`,name:"exllama_config"},{anchor:"transformers.GPTQConfig.cache_block_outputs",description:`<strong>cache_block_outputs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to cache block outputs to reuse as inputs for the succeeding block.`,name:"cache_block_outputs"},{anchor:"transformers.GPTQConfig.modules_in_block_to_quantize",description:`<strong>modules_in_block_to_quantize</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
List of list of module names to quantize in the specified block. This argument is useful to exclude certain linear modules from being quantized.
The block to quantize can be specified by setting <code>block_name_to_quantize</code>. We will quantize each list sequentially. If not set, we will quantize all linear layers.
Example: <code>modules_in_block_to_quantize =[[&quot;self_attn.k_proj&quot;, &quot;self_attn.v_proj&quot;, &quot;self_attn.q_proj&quot;], [&quot;self_attn.o_proj&quot;]]</code>.
In this example, we will first quantize the q,k,v layers simultaneously since they are independent.
Then, we will quantize <code>self_attn.o_proj</code> layer with the q,k,v layers quantized. This way, we will get
better results since it reflects the real input <code>self_attn.o_proj</code> will get when the model is quantized.`,name:"modules_in_block_to_quantize"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L383"}}),$t=new U({props:{name:"from_dict_optimum",anchor:"transformers.GPTQConfig.from_dict_optimum",parameters:[{name:"config_dict",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L580"}}),yt=new U({props:{name:"post_init",anchor:"transformers.GPTQConfig.post_init",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L497"}}),Mt=new U({props:{name:"to_dict_optimum",anchor:"transformers.GPTQConfig.to_dict_optimum",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L571"}}),Tt=new g({props:{title:"bitsandbytes 集成",local:"bitsandbytes-集成",headingTag:"h2"}}),kt=new g({props:{title:"通用用法",local:"通用用法",headingTag:"h3"}}),qt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm9wdC0zNTBtJTIyJTJDJTIwbG9hZF9pbl84Yml0JTNEVHJ1ZSklMEFtb2RlbF80Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGb3B0LTM1MG0lMjIlMkMlMjBsb2FkX2luXzRiaXQlM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)
model_4bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),Zt=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGb3B0LTM1MG0lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDMyKSUwQW1vZGVsXzhiaXQubW9kZWwuZGVjb2Rlci5sYXllcnMlNUItMSU1RC5maW5hbF9sYXllcl9ub3JtLndlaWdodC5kdHlwZQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model_8bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, torch_dtype=torch.float32)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_8bit.model.decoder.layers[-<span class="hljs-number">1</span>].final_layer_norm.weight.dtype
torch.float32`,wrap:!1}}),Qt=new g({props:{title:"FP4 量化",local:"fp4-量化",headingTag:"h3"}}),Gt=new g({props:{title:"要求",local:"要求",headingTag:"h4"}}),Bt=new g({props:{title:"提示和最佳实践",local:"提示和最佳实践",headingTag:"h4"}}),Xt=new g({props:{title:"加载 4 位量化的大模型",local:"加载-4-位量化的大模型",headingTag:"h4"}}),Ht=new _({props:{code:"JTIzJTIwcGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGFjY2VsZXJhdGUlMjBiaXRzYW5kYnl0ZXMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fNGJpdCUzRFRydWUp",highlighted:`<span class="hljs-comment"># pip install transformers accelerate bitsandbytes</span>
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),F=new os({props:{warning:!0,$$slots:{default:[hr]},$$scope:{ctx:Z}}}),At=new g({props:{title:"加载 8 位量化的大模型",local:"加载-8-位量化的大模型",headingTag:"h3"}}),Ft=new _({props:{code:"JTIzJTIwcGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGFjY2VsZXJhdGUlMjBiaXRzYW5kYnl0ZXMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUp",highlighted:`<span class="hljs-comment"># pip install transformers accelerate bitsandbytes</span>
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),Pt=new _({props:{code:"cHJpbnQobW9kZWwuZ2V0X21lbW9yeV9mb290cHJpbnQoKSk=",highlighted:'<span class="hljs-built_in">print</span>(model.get_memory_footprint())',wrap:!1}}),N=new os({props:{warning:!0,$$slots:{default:[_r]},$$scope:{ctx:Z}}}),Yt=new g({props:{title:"高级用例",local:"高级用例",headingTag:"h4"}}),Dt=new g({props:{title:"更改计算数据类型",local:"更改计算数据类型",headingTag:"h5"}}),Ot=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyhsb2FkX2luXzRiaXQlM0RUcnVlJTJDJTIwYm5iXzRiaXRfY29tcHV0ZV9kdHlwZSUzRHRvcmNoLmJmbG9hdDE2KQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=<span class="hljs-literal">True</span>, bnb_4bit_compute_dtype=torch.bfloat16)`,wrap:!1}}),en=new g({props:{title:"使用 NF4（普通浮点数 4）数据类型",local:"使用-nf4普通浮点数-4数据类型",headingTag:"h4"}}),nn=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW5mNF9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbG9hZF9pbl80Yml0JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGJuYl80Yml0X3F1YW50X3R5cGUlM0QlMjJuZjQlMjIlMkMlMEEpJTBBJTBBbW9kZWxfbmY0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRG5mNF9jb25maWcp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_quant_type=<span class="hljs-string">&quot;nf4&quot;</span>,
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)`,wrap:!1}}),ln=new g({props:{title:"使用嵌套量化进行更高效的内存推理",local:"使用嵌套量化进行更高效的内存推理",headingTag:"h4"}}),an=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQWRvdWJsZV9xdWFudF9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbG9hZF9pbl80Yml0JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGJuYl80Yml0X3VzZV9kb3VibGVfcXVhbnQlM0RUcnVlJTJDJTBBKSUwQSUwQW1vZGVsX2RvdWJsZV9xdWFudCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCUyQyUyMHF1YW50aXphdGlvbl9jb25maWclM0Rkb3VibGVfcXVhbnRfY29uZmlnKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_use_double_quant=<span class="hljs-literal">True</span>,
)

model_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config)`,wrap:!1}}),sn=new g({props:{title:"将量化模型推送到🤗 Hub",local:"将量化模型推送到-hub",headingTag:"h3"}}),mn=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmJpZ3NjaWVuY2UlMkZibG9vbS01NjBtJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmJpZ3NjaWVuY2UlMkZibG9vbS01NjBtJTIyKSUwQSUwQW1vZGVsLnB1c2hfdG9faHViKCUyMmJsb29tLTU2MG0tOGJpdCUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom-560m&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom-560m&quot;</span>)

model.push_to_hub(<span class="hljs-string">&quot;bloom-560m-8bit&quot;</span>)`,wrap:!1}}),I=new os({props:{warning:!0,$$slots:{default:[$r]},$$scope:{ctx:Z}}}),pn=new g({props:{title:"从🤗 Hub加载量化模型",local:"从-hub加载量化模型",headingTag:"h3"}}),fn=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMiU3QnlvdXJfdXNlcm5hbWUlN0QlMkZibG9vbS01NjBtLThiaXQlMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/bloom-560m-8bit&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),un=new g({props:{title:"高级用例",local:"高级用例",headingTag:"h3"}}),gn=new g({props:{title:"在 cpu 和 gpu 之间卸载",local:"在-cpu-和-gpu-之间卸载",headingTag:"h4"}}),$n=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcobGxtX2ludDhfZW5hYmxlX2ZwMzJfY3B1X29mZmxvYWQlM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=<span class="hljs-literal">True</span>)`,wrap:!1}}),Mn=new _({props:{code:"ZGV2aWNlX21hcCUyMCUzRCUyMCU3QiUwQSUyMCUyMCUyMCUyMCUyMnRyYW5zZm9ybWVyLndvcmRfZW1iZWRkaW5ncyUyMiUzQSUyMDAlMkMlMEElMjAlMjAlMjAlMjAlMjJ0cmFuc2Zvcm1lci53b3JkX2VtYmVkZGluZ3NfbGF5ZXJub3JtJTIyJTNBJTIwMCUyQyUwQSUyMCUyMCUyMCUyMCUyMmxtX2hlYWQlMjIlM0ElMjAlMjJjcHUlMjIlMkMlMEElMjAlMjAlMjAlMjAlMjJ0cmFuc2Zvcm1lci5oJTIyJTNBJTIwMCUyQyUwQSUyMCUyMCUyMCUyMCUyMnRyYW5zZm9ybWVyLmxuX2YlMjIlM0ElMjAwJTJDJTBBJTdE",highlighted:`device_map = {
    <span class="hljs-string">&quot;transformer.word_embeddings&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;transformer.word_embeddings_layernorm&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;lm_head&quot;</span>: <span class="hljs-string">&quot;cpu&quot;</span>,
    <span class="hljs-string">&quot;transformer.h&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;transformer.ln_f&quot;</span>: <span class="hljs-number">0</span>,
}`,wrap:!1}}),wn=new _({props:{code:"bW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRGRldmljZV9tYXAlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUyQyUwQSk=",highlighted:`model_8bit = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>,
    device_map=device_map,
    quantization_config=quantization_config,
)`,wrap:!1}}),Cn=new g({props:{title:"使用 llm_int8_threshold",local:"使用-llmint8threshold",headingTag:"h4"}}),xn=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyYmlnc2NpZW5jZSUyRmJsb29tLTFiNyUyMiUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbGxtX2ludDhfdGhyZXNob2xkJTNEMTAlMkMlMEEpJTBBJTBBbW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjBtb2RlbF9pZCUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0RkZXZpY2VfbWFwJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMkMlMEEpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=<span class="hljs-number">10</span>,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)`,wrap:!1}}),kn=new g({props:{title:"跳过某些模块的转换",local:"跳过某些模块的转换",headingTag:"h4"}}),qn=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyYmlnc2NpZW5jZSUyRmJsb29tLTFiNyUyMiUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbGxtX2ludDhfc2tpcF9tb2R1bGVzJTNEJTVCJTIybG1faGVhZCUyMiU1RCUyQyUwQSklMEElMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMG1vZGVsX2lkJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRGRldmljZV9tYXAlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUyQyUwQSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=[<span class="hljs-string">&quot;lm_head&quot;</span>],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)`,wrap:!1}}),Un=new g({props:{title:"微调已加载为8位精度的模型",local:"微调已加载为8位精度的模型",headingTag:"h4"}}),Gn=new g({props:{title:"BitsAndBytesConfig",local:"transformers.BitsAndBytesConfig",headingTag:"h3"}}),jn=new U({props:{name:"class transformers.BitsAndBytesConfig",anchor:"transformers.BitsAndBytesConfig",parameters:[{name:"load_in_8bit",val:" = False"},{name:"load_in_4bit",val:" = False"},{name:"llm_int8_threshold",val:" = 6.0"},{name:"llm_int8_skip_modules",val:" = None"},{name:"llm_int8_enable_fp32_cpu_offload",val:" = False"},{name:"llm_int8_has_fp16_weight",val:" = False"},{name:"bnb_4bit_compute_dtype",val:" = None"},{name:"bnb_4bit_quant_type",val:" = 'fp4'"},{name:"bnb_4bit_use_double_quant",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BitsAndBytesConfig.load_in_8bit",description:`<strong>load_in_8bit</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used to enable 8-bit quantization with LLM.int8().`,name:"load_in_8bit"},{anchor:"transformers.BitsAndBytesConfig.load_in_4bit",description:`<strong>load_in_4bit</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from
<code>bitsandbytes</code>.`,name:"load_in_4bit"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_threshold",description:`<strong>llm_int8_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 6.0) &#x2014;
This corresponds to the outlier threshold for outlier detection as described in <code>LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale</code> paper: <a href="https://arxiv.org/abs/2208.07339" rel="nofollow">https://arxiv.org/abs/2208.07339</a> Any hidden states value
that is above this threshold will be considered an outlier and the operation on those values will be done
in fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], but
there are some exceptional systematic outliers that are very differently distributed for large models.
These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values of
magnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6,
but a lower threshold might be needed for more unstable models (small models, fine-tuning).`,name:"llm_int8_threshold"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_skip_modules",description:`<strong>llm_int8_skip_modules</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such as
Jukebox that has several heads in different places and not necessarily at the last position. For example
for <code>CausalLM</code> models, the last <code>lm_head</code> is kept in its original <code>dtype</code>.`,name:"llm_int8_skip_modules"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_enable_fp32_cpu_offload",description:`<strong>llm_int8_enable_fp32_cpu_offload</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used for advanced use cases and users that are aware of this feature. If you want to split
your model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can use
this flag. This is useful for offloading large models such as <code>google/flan-t5-xxl</code>. Note that the int8
operations will not be run on CPU.`,name:"llm_int8_enable_fp32_cpu_offload"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_has_fp16_weight",description:`<strong>llm_int8_has_fp16_weight</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do not
have to be converted back and forth for the backward pass.`,name:"llm_int8_has_fp16_weight"},{anchor:"transformers.BitsAndBytesConfig.bnb_4bit_compute_dtype",description:`<strong>bnb_4bit_compute_dtype</strong> (<code>torch.dtype</code> or str, <em>optional</em>, defaults to <code>torch.float32</code>) &#x2014;
This sets the computational type which might be different than the input time. For example, inputs might be
fp32, but computation can be set to bf16 for speedups.`,name:"bnb_4bit_compute_dtype"},{anchor:"transformers.BitsAndBytesConfig.bnb_4bit_quant_type",description:`<strong>bnb_4bit_quant_type</strong> (<code>str</code>,  <em>optional</em>, defaults to <code>&quot;fp4&quot;</code>) &#x2014;
This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types
which are specified by <code>fp4</code> or <code>nf4</code>.`,name:"bnb_4bit_quant_type"},{anchor:"transformers.BitsAndBytesConfig.bnb_4bit_use_double_quant",description:`<strong>bnb_4bit_use_double_quant</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used for nested quantization where the quantization constants from the first quantization are
quantized again.`,name:"bnb_4bit_use_double_quant"},{anchor:"transformers.BitsAndBytesConfig.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional parameters from which to initialize the configuration object.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L179"}}),zn=new U({props:{name:"is_quantizable",anchor:"transformers.BitsAndBytesConfig.is_quantizable",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L319"}}),Bn=new U({props:{name:"post_init",anchor:"transformers.BitsAndBytesConfig.post_init",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L288"}}),Vn=new U({props:{name:"quantization_method",anchor:"transformers.BitsAndBytesConfig.quantization_method",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L325"}}),Xn=new U({props:{name:"to_diff_dict",anchor:"transformers.BitsAndBytesConfig.to_diff_dict",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L355",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Dict[str, Any]</code></p>
`}}),Ln=new g({props:{title:"使用 🤗 optimum 进行量化",local:"使用--optimum-进行量化",headingTag:"h2"}}),{c(){h=s("meta"),M=o(),$=s("p"),T=o(),m(Q.$$.fragment),C=o(),m(W.$$.fragment),ol=o(),D=s("p"),D.innerHTML=xs,al=o(),K=s("p"),K.textContent=ks,sl=o(),m(O.$$.fragment),il=o(),ee=s("p"),ee.textContent=Ws,rl=o(),te=s("ul"),te.innerHTML=qs,ml=o(),ne=s("p"),ne.innerHTML=Us,pl=o(),m(le.$$.fragment),dl=o(),oe=s("p"),oe.innerHTML=Zs,fl=o(),m(ae.$$.fragment),cl=o(),se=s("p"),se.innerHTML=Qs,ul=o(),m(ie.$$.fragment),bl=o(),m(re.$$.fragment),gl=o(),me=s("p"),me.textContent=Gs,hl=o(),m(pe.$$.fragment),_l=o(),m(de.$$.fragment),$l=o(),fe=s("p"),fe.innerHTML=js,yl=o(),m(ce.$$.fragment),Ml=o(),m(ue.$$.fragment),Tl=o(),be=s("p"),be.innerHTML=zs,wl=o(),ge=s("p"),ge.innerHTML=Bs,vl=o(),he=s("p"),he.innerHTML=Vs,Cl=o(),j=s("div"),j.innerHTML=Xs,Jl=o(),z=s("div"),z.innerHTML=Ls,xl=o(),B=s("div"),B.innerHTML=Hs,kl=o(),V=s("div"),V.innerHTML=As,Wl=o(),_e=s("p"),_e.innerHTML=Rs,ql=o(),$e=s("p"),$e.textContent=Fs,Ul=o(),m(ye.$$.fragment),Zl=o(),Me=s("p"),Me.innerHTML=Ns,Ql=o(),m(Te.$$.fragment),Gl=o(),q=s("div"),m(we.$$.fragment),as=o(),Fn=s("p"),Fn.innerHTML=Is,ss=o(),X=s("div"),m(ve.$$.fragment),is=o(),Nn=s("p"),Nn.textContent=Ps,jl=o(),m(Ce.$$.fragment),zl=o(),Je=s("p"),Je.innerHTML=Es,Bl=o(),xe=s("p"),xe.textContent=Ys,Vl=o(),ke=s("ul"),ke.innerHTML=Ss,Xl=o(),m(We.$$.fragment),Ll=o(),qe=s("p"),qe.textContent=Ds,Hl=o(),Ue=s("ul"),Ue.innerHTML=Ks,Al=o(),Ze=s("p"),Ze.textContent=Os,Rl=o(),m(Qe.$$.fragment),Fl=o(),Ge=s("p"),Ge.innerHTML=ei,Nl=o(),je=s("p"),je.textContent=ti,Il=o(),m(ze.$$.fragment),Pl=o(),Be=s("p"),Be.innerHTML=ni,El=o(),m(Ve.$$.fragment),Yl=o(),Xe=s("p"),Xe.textContent=li,Sl=o(),m(Le.$$.fragment),Dl=o(),m(He.$$.fragment),Kl=o(),Ae=s("p"),Ae.innerHTML=oi,Ol=o(),m(Re.$$.fragment),eo=o(),Fe=s("p"),Fe.textContent=ai,to=o(),Ne=s("p"),Ne.innerHTML=si,no=o(),m(Ie.$$.fragment),lo=o(),Pe=s("p"),Pe.innerHTML=ii,oo=o(),m(L.$$.fragment),ao=o(),m(Ee.$$.fragment),so=o(),Ye=s("p"),Ye.innerHTML=ri,io=o(),m(Se.$$.fragment),ro=o(),De=s("p"),De.innerHTML=mi,mo=o(),m(Ke.$$.fragment),po=o(),Oe=s("p"),Oe.innerHTML=pi,fo=o(),m(et.$$.fragment),co=o(),m(tt.$$.fragment),uo=o(),nt=s("p"),nt.innerHTML=di,bo=o(),m(lt.$$.fragment),go=o(),ot=s("p"),ot.innerHTML=fi,ho=o(),m(at.$$.fragment),_o=o(),m(st.$$.fragment),$o=o(),it=s("p"),it.innerHTML=ci,yo=o(),m(rt.$$.fragment),Mo=o(),mt=s("p"),mt.innerHTML=ui,To=o(),m(pt.$$.fragment),wo=o(),dt=s("p"),dt.textContent=bi,vo=o(),ft=s("p"),ft.innerHTML=gi,Co=o(),m(ct.$$.fragment),Jo=o(),ut=s("p"),ut.innerHTML=hi,xo=o(),m(bt.$$.fragment),ko=o(),gt=s("p"),gt.innerHTML=_i,Wo=o(),m(ht.$$.fragment),qo=o(),v=s("div"),m(_t.$$.fragment),rs=o(),In=s("p"),In.innerHTML=$i,ms=o(),H=s("div"),m($t.$$.fragment),ps=o(),Pn=s("p"),Pn.textContent=yi,ds=o(),A=s("div"),m(yt.$$.fragment),fs=o(),En=s("p"),En.textContent=Mi,cs=o(),R=s("div"),m(Mt.$$.fragment),us=o(),Yn=s("p"),Yn.textContent=Ti,Uo=o(),m(Tt.$$.fragment),Zo=o(),wt=s("p"),wt.innerHTML=wi,Qo=o(),vt=s("p"),vt.innerHTML=vi,Go=o(),Ct=s("p"),Ct.textContent=Ci,jo=o(),Jt=s("p"),Jt.innerHTML=Ji,zo=o(),xt=s("p"),xt.textContent=xi,Bo=o(),m(kt.$$.fragment),Vo=o(),Wt=s("p"),Wt.innerHTML=ki,Xo=o(),m(qt.$$.fragment),Lo=o(),Ut=s("p"),Ut.innerHTML=Wi,Ho=o(),m(Zt.$$.fragment),Ao=o(),m(Qt.$$.fragment),Ro=o(),m(Gt.$$.fragment),Fo=o(),jt=s("p"),jt.textContent=qi,No=o(),zt=s("ul"),zt.innerHTML=Ui,Io=o(),m(Bt.$$.fragment),Po=o(),Vt=s("ul"),Vt.innerHTML=Zi,Eo=o(),m(Xt.$$.fragment),Yo=o(),Lt=s("p"),Lt.innerHTML=Qi,So=o(),m(Ht.$$.fragment),Do=o(),m(F.$$.fragment),Ko=o(),m(At.$$.fragment),Oo=o(),Rt=s("p"),Rt.innerHTML=Gi,ea=o(),m(Ft.$$.fragment),ta=o(),Nt=s("p"),Nt.innerHTML=ji,na=o(),It=s("p"),It.innerHTML=zi,la=o(),m(Pt.$$.fragment),oa=o(),Et=s("p"),Et.textContent=Bi,aa=o(),m(N.$$.fragment),sa=o(),m(Yt.$$.fragment),ia=o(),St=s("p"),St.textContent=Vi,ra=o(),m(Dt.$$.fragment),ma=o(),Kt=s("p"),Kt.innerHTML=Xi,pa=o(),m(Ot.$$.fragment),da=o(),m(en.$$.fragment),fa=o(),tn=s("p"),tn.textContent=Li,ca=o(),m(nn.$$.fragment),ua=o(),m(ln.$$.fragment),ba=o(),on=s("p"),on.textContent=Hi,ga=o(),m(an.$$.fragment),ha=o(),m(sn.$$.fragment),_a=o(),rn=s("p"),rn.innerHTML=Ai,$a=o(),m(mn.$$.fragment),ya=o(),m(I.$$.fragment),Ma=o(),m(pn.$$.fragment),Ta=o(),dn=s("p"),dn.innerHTML=Ri,wa=o(),m(fn.$$.fragment),va=o(),cn=s("p"),cn.innerHTML=Fi,Ca=o(),m(un.$$.fragment),Ja=o(),bn=s("p"),bn.textContent=Ni,xa=o(),m(gn.$$.fragment),ka=o(),hn=s("p"),hn.innerHTML=Ii,Wa=o(),_n=s("p"),_n.innerHTML=Pi,qa=o(),m($n.$$.fragment),Ua=o(),yn=s("p"),yn.innerHTML=Ei,Za=o(),m(Mn.$$.fragment),Qa=o(),Tn=s("p"),Tn.textContent=Yi,Ga=o(),m(wn.$$.fragment),ja=o(),vn=s("p"),vn.textContent=Si,za=o(),m(Cn.$$.fragment),Ba=o(),Jn=s("p"),Jn.innerHTML=Di,Va=o(),m(xn.$$.fragment),Xa=o(),m(kn.$$.fragment),La=o(),Wn=s("p"),Wn.innerHTML=Ki,Ha=o(),m(qn.$$.fragment),Aa=o(),m(Un.$$.fragment),Ra=o(),Zn=s("p"),Zn.innerHTML=Oi,Fa=o(),Qn=s("p"),Qn.innerHTML=er,Na=o(),m(Gn.$$.fragment),Ia=o(),y=s("div"),m(jn.$$.fragment),bs=o(),Sn=s("p"),Sn.innerHTML=tr,gs=o(),Dn=s("p"),Dn.innerHTML=nr,hs=o(),Kn=s("p"),Kn.innerHTML=lr,_s=o(),P=s("div"),m(zn.$$.fragment),$s=o(),On=s("p"),On.innerHTML=or,ys=o(),E=s("div"),m(Bn.$$.fragment),Ms=o(),el=s("p"),el.textContent=ar,Ts=o(),Y=s("div"),m(Vn.$$.fragment),ws=o(),tl=s("p"),tl.innerHTML=sr,vs=o(),S=s("div"),m(Xn.$$.fragment),Cs=o(),nl=s("p"),nl.textContent=ir,Pa=o(),m(Ln.$$.fragment),Ea=o(),Hn=s("p"),Hn.innerHTML=rr,Ya=o(),ll=s("p"),this.h()},l(e){const t=cr("svelte-u9bgzb",document.head);h=i(t,"META",{name:!0,content:!0}),t.forEach(n),M=a(e),$=i(e,"P",{}),x($).forEach(n),T=a(e),p(Q.$$.fragment,e),C=a(e),p(W.$$.fragment,e),ol=a(e),D=i(e,"P",{"data-svelte-h":!0}),r(D)!=="svelte-ecgix0"&&(D.innerHTML=xs),al=a(e),K=i(e,"P",{"data-svelte-h":!0}),r(K)!=="svelte-8zaqwb"&&(K.textContent=ks),sl=a(e),p(O.$$.fragment,e),il=a(e),ee=i(e,"P",{"data-svelte-h":!0}),r(ee)!=="svelte-thrt6i"&&(ee.textContent=Ws),rl=a(e),te=i(e,"UL",{"data-svelte-h":!0}),r(te)!=="svelte-ads6xy"&&(te.innerHTML=qs),ml=a(e),ne=i(e,"P",{"data-svelte-h":!0}),r(ne)!=="svelte-tuzvmh"&&(ne.innerHTML=Us),pl=a(e),p(le.$$.fragment,e),dl=a(e),oe=i(e,"P",{"data-svelte-h":!0}),r(oe)!=="svelte-ntsut5"&&(oe.innerHTML=Zs),fl=a(e),p(ae.$$.fragment,e),cl=a(e),se=i(e,"P",{"data-svelte-h":!0}),r(se)!=="svelte-kibpw7"&&(se.innerHTML=Qs),ul=a(e),p(ie.$$.fragment,e),bl=a(e),p(re.$$.fragment,e),gl=a(e),me=i(e,"P",{"data-svelte-h":!0}),r(me)!=="svelte-1raiibs"&&(me.textContent=Gs),hl=a(e),p(pe.$$.fragment,e),_l=a(e),p(de.$$.fragment,e),$l=a(e),fe=i(e,"P",{"data-svelte-h":!0}),r(fe)!=="svelte-zx2h9i"&&(fe.innerHTML=js),yl=a(e),p(ce.$$.fragment,e),Ml=a(e),p(ue.$$.fragment,e),Tl=a(e),be=i(e,"P",{"data-svelte-h":!0}),r(be)!=="svelte-1dghds0"&&(be.innerHTML=zs),wl=a(e),ge=i(e,"P",{"data-svelte-h":!0}),r(ge)!=="svelte-ce4li1"&&(ge.innerHTML=Bs),vl=a(e),he=i(e,"P",{"data-svelte-h":!0}),r(he)!=="svelte-9sh2bq"&&(he.innerHTML=Vs),Cl=a(e),j=i(e,"DIV",{style:!0,"data-svelte-h":!0}),r(j)!=="svelte-qy9lc8"&&(j.innerHTML=Xs),Jl=a(e),z=i(e,"DIV",{style:!0,"data-svelte-h":!0}),r(z)!=="svelte-1oan9h4"&&(z.innerHTML=Ls),xl=a(e),B=i(e,"DIV",{style:!0,"data-svelte-h":!0}),r(B)!=="svelte-1p6kg0n"&&(B.innerHTML=Hs),kl=a(e),V=i(e,"DIV",{style:!0,"data-svelte-h":!0}),r(V)!=="svelte-ex9c15"&&(V.innerHTML=As),Wl=a(e),_e=i(e,"P",{"data-svelte-h":!0}),r(_e)!=="svelte-1ft7b39"&&(_e.innerHTML=Rs),ql=a(e),$e=i(e,"P",{"data-svelte-h":!0}),r($e)!=="svelte-3lvbqz"&&($e.textContent=Fs),Ul=a(e),p(ye.$$.fragment,e),Zl=a(e),Me=i(e,"P",{"data-svelte-h":!0}),r(Me)!=="svelte-1vbph39"&&(Me.innerHTML=Ns),Ql=a(e),p(Te.$$.fragment,e),Gl=a(e),q=i(e,"DIV",{class:!0});var G=x(q);p(we.$$.fragment,G),as=a(G),Fn=i(G,"P",{"data-svelte-h":!0}),r(Fn)!=="svelte-1i667it"&&(Fn.innerHTML=Is),ss=a(G),X=i(G,"DIV",{class:!0});var An=x(X);p(ve.$$.fragment,An),is=a(An),Nn=i(An,"P",{"data-svelte-h":!0}),r(Nn)!=="svelte-1ozftb6"&&(Nn.textContent=Ps),An.forEach(n),G.forEach(n),jl=a(e),p(Ce.$$.fragment,e),zl=a(e),Je=i(e,"P",{"data-svelte-h":!0}),r(Je)!=="svelte-17s7r94"&&(Je.innerHTML=Es),Bl=a(e),xe=i(e,"P",{"data-svelte-h":!0}),r(xe)!=="svelte-1ll5cvb"&&(xe.textContent=Ys),Vl=a(e),ke=i(e,"UL",{"data-svelte-h":!0}),r(ke)!=="svelte-1qfg3be"&&(ke.innerHTML=Ss),Xl=a(e),p(We.$$.fragment,e),Ll=a(e),qe=i(e,"P",{"data-svelte-h":!0}),r(qe)!=="svelte-fdtanc"&&(qe.textContent=Ds),Hl=a(e),Ue=i(e,"UL",{"data-svelte-h":!0}),r(Ue)!=="svelte-ncs17j"&&(Ue.innerHTML=Ks),Al=a(e),Ze=i(e,"P",{"data-svelte-h":!0}),r(Ze)!=="svelte-1b2r56w"&&(Ze.textContent=Os),Rl=a(e),p(Qe.$$.fragment,e),Fl=a(e),Ge=i(e,"P",{"data-svelte-h":!0}),r(Ge)!=="svelte-aj21cd"&&(Ge.innerHTML=ei),Nl=a(e),je=i(e,"P",{"data-svelte-h":!0}),r(je)!=="svelte-pcgkdc"&&(je.textContent=ti),Il=a(e),p(ze.$$.fragment,e),Pl=a(e),Be=i(e,"P",{"data-svelte-h":!0}),r(Be)!=="svelte-xief8y"&&(Be.innerHTML=ni),El=a(e),p(Ve.$$.fragment,e),Yl=a(e),Xe=i(e,"P",{"data-svelte-h":!0}),r(Xe)!=="svelte-py00dp"&&(Xe.textContent=li),Sl=a(e),p(Le.$$.fragment,e),Dl=a(e),p(He.$$.fragment,e),Kl=a(e),Ae=i(e,"P",{"data-svelte-h":!0}),r(Ae)!=="svelte-um9zqp"&&(Ae.innerHTML=oi),Ol=a(e),p(Re.$$.fragment,e),eo=a(e),Fe=i(e,"P",{"data-svelte-h":!0}),r(Fe)!=="svelte-18966sx"&&(Fe.textContent=ai),to=a(e),Ne=i(e,"P",{"data-svelte-h":!0}),r(Ne)!=="svelte-1bwqtp4"&&(Ne.innerHTML=si),no=a(e),p(Ie.$$.fragment,e),lo=a(e),Pe=i(e,"P",{"data-svelte-h":!0}),r(Pe)!=="svelte-1r6xrdg"&&(Pe.innerHTML=ii),oo=a(e),p(L.$$.fragment,e),ao=a(e),p(Ee.$$.fragment,e),so=a(e),Ye=i(e,"P",{"data-svelte-h":!0}),r(Ye)!=="svelte-i7fyuc"&&(Ye.innerHTML=ri),io=a(e),p(Se.$$.fragment,e),ro=a(e),De=i(e,"P",{"data-svelte-h":!0}),r(De)!=="svelte-1l0f6g9"&&(De.innerHTML=mi),mo=a(e),p(Ke.$$.fragment,e),po=a(e),Oe=i(e,"P",{"data-svelte-h":!0}),r(Oe)!=="svelte-su51nm"&&(Oe.innerHTML=pi),fo=a(e),p(et.$$.fragment,e),co=a(e),p(tt.$$.fragment,e),uo=a(e),nt=i(e,"P",{"data-svelte-h":!0}),r(nt)!=="svelte-3q2e9r"&&(nt.innerHTML=di),bo=a(e),p(lt.$$.fragment,e),go=a(e),ot=i(e,"P",{"data-svelte-h":!0}),r(ot)!=="svelte-2r05wl"&&(ot.innerHTML=fi),ho=a(e),p(at.$$.fragment,e),_o=a(e),p(st.$$.fragment,e),$o=a(e),it=i(e,"P",{"data-svelte-h":!0}),r(it)!=="svelte-3qi5bz"&&(it.innerHTML=ci),yo=a(e),p(rt.$$.fragment,e),Mo=a(e),mt=i(e,"P",{"data-svelte-h":!0}),r(mt)!=="svelte-bm00vc"&&(mt.innerHTML=ui),To=a(e),p(pt.$$.fragment,e),wo=a(e),dt=i(e,"P",{"data-svelte-h":!0}),r(dt)!=="svelte-nuv2x8"&&(dt.textContent=bi),vo=a(e),ft=i(e,"P",{"data-svelte-h":!0}),r(ft)!=="svelte-1m4itqb"&&(ft.innerHTML=gi),Co=a(e),p(ct.$$.fragment,e),Jo=a(e),ut=i(e,"P",{"data-svelte-h":!0}),r(ut)!=="svelte-mozsqh"&&(ut.innerHTML=hi),xo=a(e),p(bt.$$.fragment,e),ko=a(e),gt=i(e,"P",{"data-svelte-h":!0}),r(gt)!=="svelte-1uhcexr"&&(gt.innerHTML=_i),Wo=a(e),p(ht.$$.fragment,e),qo=a(e),v=i(e,"DIV",{class:!0});var J=x(v);p(_t.$$.fragment,J),rs=a(J),In=i(J,"P",{"data-svelte-h":!0}),r(In)!=="svelte-i3efvr"&&(In.innerHTML=$i),ms=a(J),H=i(J,"DIV",{class:!0});var Rn=x(H);p($t.$$.fragment,Rn),ps=a(Rn),Pn=i(Rn,"P",{"data-svelte-h":!0}),r(Pn)!=="svelte-4jdj2l"&&(Pn.textContent=yi),Rn.forEach(n),ds=a(J),A=i(J,"DIV",{class:!0});var Da=x(A);p(yt.$$.fragment,Da),fs=a(Da),En=i(Da,"P",{"data-svelte-h":!0}),r(En)!=="svelte-1ozftb6"&&(En.textContent=Mi),Da.forEach(n),cs=a(J),R=i(J,"DIV",{class:!0});var Ka=x(R);p(Mt.$$.fragment,Ka),us=a(Ka),Yn=i(Ka,"P",{"data-svelte-h":!0}),r(Yn)!=="svelte-pjgtd6"&&(Yn.textContent=Ti),Ka.forEach(n),J.forEach(n),Uo=a(e),p(Tt.$$.fragment,e),Zo=a(e),wt=i(e,"P",{"data-svelte-h":!0}),r(wt)!=="svelte-1hh96p7"&&(wt.innerHTML=wi),Qo=a(e),vt=i(e,"P",{"data-svelte-h":!0}),r(vt)!=="svelte-1j6dol1"&&(vt.innerHTML=vi),Go=a(e),Ct=i(e,"P",{"data-svelte-h":!0}),r(Ct)!=="svelte-17iiuoe"&&(Ct.textContent=Ci),jo=a(e),Jt=i(e,"P",{"data-svelte-h":!0}),r(Jt)!=="svelte-g1ckp3"&&(Jt.innerHTML=Ji),zo=a(e),xt=i(e,"P",{"data-svelte-h":!0}),r(xt)!=="svelte-10ye29r"&&(xt.textContent=xi),Bo=a(e),p(kt.$$.fragment,e),Vo=a(e),Wt=i(e,"P",{"data-svelte-h":!0}),r(Wt)!=="svelte-1b2dqow"&&(Wt.innerHTML=ki),Xo=a(e),p(qt.$$.fragment,e),Lo=a(e),Ut=i(e,"P",{"data-svelte-h":!0}),r(Ut)!=="svelte-e5nhvd"&&(Ut.innerHTML=Wi),Ho=a(e),p(Zt.$$.fragment,e),Ao=a(e),p(Qt.$$.fragment,e),Ro=a(e),p(Gt.$$.fragment,e),Fo=a(e),jt=i(e,"P",{"data-svelte-h":!0}),r(jt)!=="svelte-hqjahh"&&(jt.textContent=qi),No=a(e),zt=i(e,"UL",{"data-svelte-h":!0}),r(zt)!=="svelte-1rz81mq"&&(zt.innerHTML=Ui),Io=a(e),p(Bt.$$.fragment,e),Po=a(e),Vt=i(e,"UL",{"data-svelte-h":!0}),r(Vt)!=="svelte-10uc6le"&&(Vt.innerHTML=Zi),Eo=a(e),p(Xt.$$.fragment,e),Yo=a(e),Lt=i(e,"P",{"data-svelte-h":!0}),r(Lt)!=="svelte-a38biy"&&(Lt.innerHTML=Qi),So=a(e),p(Ht.$$.fragment,e),Do=a(e),p(F.$$.fragment,e),Ko=a(e),p(At.$$.fragment,e),Oo=a(e),Rt=i(e,"P",{"data-svelte-h":!0}),r(Rt)!=="svelte-1e0nw92"&&(Rt.innerHTML=Gi),ea=a(e),p(Ft.$$.fragment,e),ta=a(e),Nt=i(e,"P",{"data-svelte-h":!0}),r(Nt)!=="svelte-nja6yg"&&(Nt.innerHTML=ji),na=a(e),It=i(e,"P",{"data-svelte-h":!0}),r(It)!=="svelte-1xv0occ"&&(It.innerHTML=zi),la=a(e),p(Pt.$$.fragment,e),oa=a(e),Et=i(e,"P",{"data-svelte-h":!0}),r(Et)!=="svelte-v3jp8e"&&(Et.textContent=Bi),aa=a(e),p(N.$$.fragment,e),sa=a(e),p(Yt.$$.fragment,e),ia=a(e),St=i(e,"P",{"data-svelte-h":!0}),r(St)!=="svelte-1hv6n9l"&&(St.textContent=Vi),ra=a(e),p(Dt.$$.fragment,e),ma=a(e),Kt=i(e,"P",{"data-svelte-h":!0}),r(Kt)!=="svelte-yw7m57"&&(Kt.innerHTML=Xi),pa=a(e),p(Ot.$$.fragment,e),da=a(e),p(en.$$.fragment,e),fa=a(e),tn=i(e,"P",{"data-svelte-h":!0}),r(tn)!=="svelte-6rqah2"&&(tn.textContent=Li),ca=a(e),p(nn.$$.fragment,e),ua=a(e),p(ln.$$.fragment,e),ba=a(e),on=i(e,"P",{"data-svelte-h":!0}),r(on)!=="svelte-1ksuiwo"&&(on.textContent=Hi),ga=a(e),p(an.$$.fragment,e),ha=a(e),p(sn.$$.fragment,e),_a=a(e),rn=i(e,"P",{"data-svelte-h":!0}),r(rn)!=="svelte-14iisu"&&(rn.innerHTML=Ai),$a=a(e),p(mn.$$.fragment,e),ya=a(e),p(I.$$.fragment,e),Ma=a(e),p(pn.$$.fragment,e),Ta=a(e),dn=i(e,"P",{"data-svelte-h":!0}),r(dn)!=="svelte-1iuhck1"&&(dn.innerHTML=Ri),wa=a(e),p(fn.$$.fragment,e),va=a(e),cn=i(e,"P",{"data-svelte-h":!0}),r(cn)!=="svelte-jfdebg"&&(cn.innerHTML=Fi),Ca=a(e),p(un.$$.fragment,e),Ja=a(e),bn=i(e,"P",{"data-svelte-h":!0}),r(bn)!=="svelte-1am730"&&(bn.textContent=Ni),xa=a(e),p(gn.$$.fragment,e),ka=a(e),hn=i(e,"P",{"data-svelte-h":!0}),r(hn)!=="svelte-3pwpc4"&&(hn.innerHTML=Ii),Wa=a(e),_n=i(e,"P",{"data-svelte-h":!0}),r(_n)!=="svelte-6bk37"&&(_n.innerHTML=Pi),qa=a(e),p($n.$$.fragment,e),Ua=a(e),yn=i(e,"P",{"data-svelte-h":!0}),r(yn)!=="svelte-1ekqeba"&&(yn.innerHTML=Ei),Za=a(e),p(Mn.$$.fragment,e),Qa=a(e),Tn=i(e,"P",{"data-svelte-h":!0}),r(Tn)!=="svelte-1ka50bq"&&(Tn.textContent=Yi),Ga=a(e),p(wn.$$.fragment,e),ja=a(e),vn=i(e,"P",{"data-svelte-h":!0}),r(vn)!=="svelte-missd6"&&(vn.textContent=Si),za=a(e),p(Cn.$$.fragment,e),Ba=a(e),Jn=i(e,"P",{"data-svelte-h":!0}),r(Jn)!=="svelte-1h9a4um"&&(Jn.innerHTML=Di),Va=a(e),p(xn.$$.fragment,e),Xa=a(e),p(kn.$$.fragment,e),La=a(e),Wn=i(e,"P",{"data-svelte-h":!0}),r(Wn)!=="svelte-1s6xsxh"&&(Wn.innerHTML=Ki),Ha=a(e),p(qn.$$.fragment,e),Aa=a(e),p(Un.$$.fragment,e),Ra=a(e),Zn=i(e,"P",{"data-svelte-h":!0}),r(Zn)!=="svelte-1rkitfn"&&(Zn.innerHTML=Oi),Fa=a(e),Qn=i(e,"P",{"data-svelte-h":!0}),r(Qn)!=="svelte-28didg"&&(Qn.innerHTML=er),Na=a(e),p(Gn.$$.fragment,e),Ia=a(e),y=i(e,"DIV",{class:!0});var w=x(y);p(jn.$$.fragment,w),bs=a(w),Sn=i(w,"P",{"data-svelte-h":!0}),r(Sn)!=="svelte-woamwr"&&(Sn.innerHTML=tr),gs=a(w),Dn=i(w,"P",{"data-svelte-h":!0}),r(Dn)!=="svelte-ki5gis"&&(Dn.innerHTML=nr),hs=a(w),Kn=i(w,"P",{"data-svelte-h":!0}),r(Kn)!=="svelte-8qsk2q"&&(Kn.innerHTML=lr),_s=a(w),P=i(w,"DIV",{class:!0});var Oa=x(P);p(zn.$$.fragment,Oa),$s=a(Oa),On=i(Oa,"P",{"data-svelte-h":!0}),r(On)!=="svelte-10tvzyv"&&(On.innerHTML=or),Oa.forEach(n),ys=a(w),E=i(w,"DIV",{class:!0});var es=x(E);p(Bn.$$.fragment,es),Ms=a(es),el=i(es,"P",{"data-svelte-h":!0}),r(el)!=="svelte-gy26u4"&&(el.textContent=ar),es.forEach(n),Ts=a(w),Y=i(w,"DIV",{class:!0});var ts=x(Y);p(Vn.$$.fragment,ts),ws=a(ts),tl=i(ts,"P",{"data-svelte-h":!0}),r(tl)!=="svelte-19bn0da"&&(tl.innerHTML=sr),ts.forEach(n),vs=a(w),S=i(w,"DIV",{class:!0});var ns=x(S);p(Xn.$$.fragment,ns),Cs=a(ns),nl=i(ns,"P",{"data-svelte-h":!0}),r(nl)!=="svelte-1p6bdas"&&(nl.textContent=ir),ns.forEach(n),w.forEach(n),Pa=a(e),p(Ln.$$.fragment,e),Ea=a(e),Hn=i(e,"P",{"data-svelte-h":!0}),r(Hn)!=="svelte-1hh7kt7"&&(Hn.innerHTML=rr),Ya=a(e),ll=i(e,"P",{}),x(ll).forEach(n),this.h()},h(){k(h,"name","hf:doc:metadata"),k(h,"content",Mr),ls(j,"text-align","center"),ls(z,"text-align","center"),ls(B,"text-align","center"),ls(V,"text-align","center"),k(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),k(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){b(document.head,h),l(e,M,t),l(e,$,t),l(e,T,t),d(Q,e,t),l(e,C,t),d(W,e,t),l(e,ol,t),l(e,D,t),l(e,al,t),l(e,K,t),l(e,sl,t),d(O,e,t),l(e,il,t),l(e,ee,t),l(e,rl,t),l(e,te,t),l(e,ml,t),l(e,ne,t),l(e,pl,t),d(le,e,t),l(e,dl,t),l(e,oe,t),l(e,fl,t),d(ae,e,t),l(e,cl,t),l(e,se,t),l(e,ul,t),d(ie,e,t),l(e,bl,t),d(re,e,t),l(e,gl,t),l(e,me,t),l(e,hl,t),d(pe,e,t),l(e,_l,t),d(de,e,t),l(e,$l,t),l(e,fe,t),l(e,yl,t),d(ce,e,t),l(e,Ml,t),d(ue,e,t),l(e,Tl,t),l(e,be,t),l(e,wl,t),l(e,ge,t),l(e,vl,t),l(e,he,t),l(e,Cl,t),l(e,j,t),l(e,Jl,t),l(e,z,t),l(e,xl,t),l(e,B,t),l(e,kl,t),l(e,V,t),l(e,Wl,t),l(e,_e,t),l(e,ql,t),l(e,$e,t),l(e,Ul,t),d(ye,e,t),l(e,Zl,t),l(e,Me,t),l(e,Ql,t),d(Te,e,t),l(e,Gl,t),l(e,q,t),d(we,q,null),b(q,as),b(q,Fn),b(q,ss),b(q,X),d(ve,X,null),b(X,is),b(X,Nn),l(e,jl,t),d(Ce,e,t),l(e,zl,t),l(e,Je,t),l(e,Bl,t),l(e,xe,t),l(e,Vl,t),l(e,ke,t),l(e,Xl,t),d(We,e,t),l(e,Ll,t),l(e,qe,t),l(e,Hl,t),l(e,Ue,t),l(e,Al,t),l(e,Ze,t),l(e,Rl,t),d(Qe,e,t),l(e,Fl,t),l(e,Ge,t),l(e,Nl,t),l(e,je,t),l(e,Il,t),d(ze,e,t),l(e,Pl,t),l(e,Be,t),l(e,El,t),d(Ve,e,t),l(e,Yl,t),l(e,Xe,t),l(e,Sl,t),d(Le,e,t),l(e,Dl,t),d(He,e,t),l(e,Kl,t),l(e,Ae,t),l(e,Ol,t),d(Re,e,t),l(e,eo,t),l(e,Fe,t),l(e,to,t),l(e,Ne,t),l(e,no,t),d(Ie,e,t),l(e,lo,t),l(e,Pe,t),l(e,oo,t),d(L,e,t),l(e,ao,t),d(Ee,e,t),l(e,so,t),l(e,Ye,t),l(e,io,t),d(Se,e,t),l(e,ro,t),l(e,De,t),l(e,mo,t),d(Ke,e,t),l(e,po,t),l(e,Oe,t),l(e,fo,t),d(et,e,t),l(e,co,t),d(tt,e,t),l(e,uo,t),l(e,nt,t),l(e,bo,t),d(lt,e,t),l(e,go,t),l(e,ot,t),l(e,ho,t),d(at,e,t),l(e,_o,t),d(st,e,t),l(e,$o,t),l(e,it,t),l(e,yo,t),d(rt,e,t),l(e,Mo,t),l(e,mt,t),l(e,To,t),d(pt,e,t),l(e,wo,t),l(e,dt,t),l(e,vo,t),l(e,ft,t),l(e,Co,t),d(ct,e,t),l(e,Jo,t),l(e,ut,t),l(e,xo,t),d(bt,e,t),l(e,ko,t),l(e,gt,t),l(e,Wo,t),d(ht,e,t),l(e,qo,t),l(e,v,t),d(_t,v,null),b(v,rs),b(v,In),b(v,ms),b(v,H),d($t,H,null),b(H,ps),b(H,Pn),b(v,ds),b(v,A),d(yt,A,null),b(A,fs),b(A,En),b(v,cs),b(v,R),d(Mt,R,null),b(R,us),b(R,Yn),l(e,Uo,t),d(Tt,e,t),l(e,Zo,t),l(e,wt,t),l(e,Qo,t),l(e,vt,t),l(e,Go,t),l(e,Ct,t),l(e,jo,t),l(e,Jt,t),l(e,zo,t),l(e,xt,t),l(e,Bo,t),d(kt,e,t),l(e,Vo,t),l(e,Wt,t),l(e,Xo,t),d(qt,e,t),l(e,Lo,t),l(e,Ut,t),l(e,Ho,t),d(Zt,e,t),l(e,Ao,t),d(Qt,e,t),l(e,Ro,t),d(Gt,e,t),l(e,Fo,t),l(e,jt,t),l(e,No,t),l(e,zt,t),l(e,Io,t),d(Bt,e,t),l(e,Po,t),l(e,Vt,t),l(e,Eo,t),d(Xt,e,t),l(e,Yo,t),l(e,Lt,t),l(e,So,t),d(Ht,e,t),l(e,Do,t),d(F,e,t),l(e,Ko,t),d(At,e,t),l(e,Oo,t),l(e,Rt,t),l(e,ea,t),d(Ft,e,t),l(e,ta,t),l(e,Nt,t),l(e,na,t),l(e,It,t),l(e,la,t),d(Pt,e,t),l(e,oa,t),l(e,Et,t),l(e,aa,t),d(N,e,t),l(e,sa,t),d(Yt,e,t),l(e,ia,t),l(e,St,t),l(e,ra,t),d(Dt,e,t),l(e,ma,t),l(e,Kt,t),l(e,pa,t),d(Ot,e,t),l(e,da,t),d(en,e,t),l(e,fa,t),l(e,tn,t),l(e,ca,t),d(nn,e,t),l(e,ua,t),d(ln,e,t),l(e,ba,t),l(e,on,t),l(e,ga,t),d(an,e,t),l(e,ha,t),d(sn,e,t),l(e,_a,t),l(e,rn,t),l(e,$a,t),d(mn,e,t),l(e,ya,t),d(I,e,t),l(e,Ma,t),d(pn,e,t),l(e,Ta,t),l(e,dn,t),l(e,wa,t),d(fn,e,t),l(e,va,t),l(e,cn,t),l(e,Ca,t),d(un,e,t),l(e,Ja,t),l(e,bn,t),l(e,xa,t),d(gn,e,t),l(e,ka,t),l(e,hn,t),l(e,Wa,t),l(e,_n,t),l(e,qa,t),d($n,e,t),l(e,Ua,t),l(e,yn,t),l(e,Za,t),d(Mn,e,t),l(e,Qa,t),l(e,Tn,t),l(e,Ga,t),d(wn,e,t),l(e,ja,t),l(e,vn,t),l(e,za,t),d(Cn,e,t),l(e,Ba,t),l(e,Jn,t),l(e,Va,t),d(xn,e,t),l(e,Xa,t),d(kn,e,t),l(e,La,t),l(e,Wn,t),l(e,Ha,t),d(qn,e,t),l(e,Aa,t),d(Un,e,t),l(e,Ra,t),l(e,Zn,t),l(e,Fa,t),l(e,Qn,t),l(e,Na,t),d(Gn,e,t),l(e,Ia,t),l(e,y,t),d(jn,y,null),b(y,bs),b(y,Sn),b(y,gs),b(y,Dn),b(y,hs),b(y,Kn),b(y,_s),b(y,P),d(zn,P,null),b(P,$s),b(P,On),b(y,ys),b(y,E),d(Bn,E,null),b(E,Ms),b(E,el),b(y,Ts),b(y,Y),d(Vn,Y,null),b(Y,ws),b(Y,tl),b(y,vs),b(y,S),d(Xn,S,null),b(S,Cs),b(S,nl),l(e,Pa,t),d(Ln,e,t),l(e,Ea,t),l(e,Hn,t),l(e,Ya,t),l(e,ll,t),Sa=!0},p(e,[t]){const G={};t&2&&(G.$$scope={dirty:t,ctx:e}),L.$set(G);const An={};t&2&&(An.$$scope={dirty:t,ctx:e}),F.$set(An);const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),N.$set(J);const Rn={};t&2&&(Rn.$$scope={dirty:t,ctx:e}),I.$set(Rn)},i(e){Sa||(f(Q.$$.fragment,e),f(W.$$.fragment,e),f(O.$$.fragment,e),f(le.$$.fragment,e),f(ae.$$.fragment,e),f(ie.$$.fragment,e),f(re.$$.fragment,e),f(pe.$$.fragment,e),f(de.$$.fragment,e),f(ce.$$.fragment,e),f(ue.$$.fragment,e),f(ye.$$.fragment,e),f(Te.$$.fragment,e),f(we.$$.fragment,e),f(ve.$$.fragment,e),f(Ce.$$.fragment,e),f(We.$$.fragment,e),f(Qe.$$.fragment,e),f(ze.$$.fragment,e),f(Ve.$$.fragment,e),f(Le.$$.fragment,e),f(He.$$.fragment,e),f(Re.$$.fragment,e),f(Ie.$$.fragment,e),f(L.$$.fragment,e),f(Ee.$$.fragment,e),f(Se.$$.fragment,e),f(Ke.$$.fragment,e),f(et.$$.fragment,e),f(tt.$$.fragment,e),f(lt.$$.fragment,e),f(at.$$.fragment,e),f(st.$$.fragment,e),f(rt.$$.fragment,e),f(pt.$$.fragment,e),f(ct.$$.fragment,e),f(bt.$$.fragment,e),f(ht.$$.fragment,e),f(_t.$$.fragment,e),f($t.$$.fragment,e),f(yt.$$.fragment,e),f(Mt.$$.fragment,e),f(Tt.$$.fragment,e),f(kt.$$.fragment,e),f(qt.$$.fragment,e),f(Zt.$$.fragment,e),f(Qt.$$.fragment,e),f(Gt.$$.fragment,e),f(Bt.$$.fragment,e),f(Xt.$$.fragment,e),f(Ht.$$.fragment,e),f(F.$$.fragment,e),f(At.$$.fragment,e),f(Ft.$$.fragment,e),f(Pt.$$.fragment,e),f(N.$$.fragment,e),f(Yt.$$.fragment,e),f(Dt.$$.fragment,e),f(Ot.$$.fragment,e),f(en.$$.fragment,e),f(nn.$$.fragment,e),f(ln.$$.fragment,e),f(an.$$.fragment,e),f(sn.$$.fragment,e),f(mn.$$.fragment,e),f(I.$$.fragment,e),f(pn.$$.fragment,e),f(fn.$$.fragment,e),f(un.$$.fragment,e),f(gn.$$.fragment,e),f($n.$$.fragment,e),f(Mn.$$.fragment,e),f(wn.$$.fragment,e),f(Cn.$$.fragment,e),f(xn.$$.fragment,e),f(kn.$$.fragment,e),f(qn.$$.fragment,e),f(Un.$$.fragment,e),f(Gn.$$.fragment,e),f(jn.$$.fragment,e),f(zn.$$.fragment,e),f(Bn.$$.fragment,e),f(Vn.$$.fragment,e),f(Xn.$$.fragment,e),f(Ln.$$.fragment,e),Sa=!0)},o(e){c(Q.$$.fragment,e),c(W.$$.fragment,e),c(O.$$.fragment,e),c(le.$$.fragment,e),c(ae.$$.fragment,e),c(ie.$$.fragment,e),c(re.$$.fragment,e),c(pe.$$.fragment,e),c(de.$$.fragment,e),c(ce.$$.fragment,e),c(ue.$$.fragment,e),c(ye.$$.fragment,e),c(Te.$$.fragment,e),c(we.$$.fragment,e),c(ve.$$.fragment,e),c(Ce.$$.fragment,e),c(We.$$.fragment,e),c(Qe.$$.fragment,e),c(ze.$$.fragment,e),c(Ve.$$.fragment,e),c(Le.$$.fragment,e),c(He.$$.fragment,e),c(Re.$$.fragment,e),c(Ie.$$.fragment,e),c(L.$$.fragment,e),c(Ee.$$.fragment,e),c(Se.$$.fragment,e),c(Ke.$$.fragment,e),c(et.$$.fragment,e),c(tt.$$.fragment,e),c(lt.$$.fragment,e),c(at.$$.fragment,e),c(st.$$.fragment,e),c(rt.$$.fragment,e),c(pt.$$.fragment,e),c(ct.$$.fragment,e),c(bt.$$.fragment,e),c(ht.$$.fragment,e),c(_t.$$.fragment,e),c($t.$$.fragment,e),c(yt.$$.fragment,e),c(Mt.$$.fragment,e),c(Tt.$$.fragment,e),c(kt.$$.fragment,e),c(qt.$$.fragment,e),c(Zt.$$.fragment,e),c(Qt.$$.fragment,e),c(Gt.$$.fragment,e),c(Bt.$$.fragment,e),c(Xt.$$.fragment,e),c(Ht.$$.fragment,e),c(F.$$.fragment,e),c(At.$$.fragment,e),c(Ft.$$.fragment,e),c(Pt.$$.fragment,e),c(N.$$.fragment,e),c(Yt.$$.fragment,e),c(Dt.$$.fragment,e),c(Ot.$$.fragment,e),c(en.$$.fragment,e),c(nn.$$.fragment,e),c(ln.$$.fragment,e),c(an.$$.fragment,e),c(sn.$$.fragment,e),c(mn.$$.fragment,e),c(I.$$.fragment,e),c(pn.$$.fragment,e),c(fn.$$.fragment,e),c(un.$$.fragment,e),c(gn.$$.fragment,e),c($n.$$.fragment,e),c(Mn.$$.fragment,e),c(wn.$$.fragment,e),c(Cn.$$.fragment,e),c(xn.$$.fragment,e),c(kn.$$.fragment,e),c(qn.$$.fragment,e),c(Un.$$.fragment,e),c(Gn.$$.fragment,e),c(jn.$$.fragment,e),c(zn.$$.fragment,e),c(Bn.$$.fragment,e),c(Vn.$$.fragment,e),c(Xn.$$.fragment,e),c(Ln.$$.fragment,e),Sa=!1},d(e){e&&(n(M),n($),n(T),n(C),n(ol),n(D),n(al),n(K),n(sl),n(il),n(ee),n(rl),n(te),n(ml),n(ne),n(pl),n(dl),n(oe),n(fl),n(cl),n(se),n(ul),n(bl),n(gl),n(me),n(hl),n(_l),n($l),n(fe),n(yl),n(Ml),n(Tl),n(be),n(wl),n(ge),n(vl),n(he),n(Cl),n(j),n(Jl),n(z),n(xl),n(B),n(kl),n(V),n(Wl),n(_e),n(ql),n($e),n(Ul),n(Zl),n(Me),n(Ql),n(Gl),n(q),n(jl),n(zl),n(Je),n(Bl),n(xe),n(Vl),n(ke),n(Xl),n(Ll),n(qe),n(Hl),n(Ue),n(Al),n(Ze),n(Rl),n(Fl),n(Ge),n(Nl),n(je),n(Il),n(Pl),n(Be),n(El),n(Yl),n(Xe),n(Sl),n(Dl),n(Kl),n(Ae),n(Ol),n(eo),n(Fe),n(to),n(Ne),n(no),n(lo),n(Pe),n(oo),n(ao),n(so),n(Ye),n(io),n(ro),n(De),n(mo),n(po),n(Oe),n(fo),n(co),n(uo),n(nt),n(bo),n(go),n(ot),n(ho),n(_o),n($o),n(it),n(yo),n(Mo),n(mt),n(To),n(wo),n(dt),n(vo),n(ft),n(Co),n(Jo),n(ut),n(xo),n(ko),n(gt),n(Wo),n(qo),n(v),n(Uo),n(Zo),n(wt),n(Qo),n(vt),n(Go),n(Ct),n(jo),n(Jt),n(zo),n(xt),n(Bo),n(Vo),n(Wt),n(Xo),n(Lo),n(Ut),n(Ho),n(Ao),n(Ro),n(Fo),n(jt),n(No),n(zt),n(Io),n(Po),n(Vt),n(Eo),n(Yo),n(Lt),n(So),n(Do),n(Ko),n(Oo),n(Rt),n(ea),n(ta),n(Nt),n(na),n(It),n(la),n(oa),n(Et),n(aa),n(sa),n(ia),n(St),n(ra),n(ma),n(Kt),n(pa),n(da),n(fa),n(tn),n(ca),n(ua),n(ba),n(on),n(ga),n(ha),n(_a),n(rn),n($a),n(ya),n(Ma),n(Ta),n(dn),n(wa),n(va),n(cn),n(Ca),n(Ja),n(bn),n(xa),n(ka),n(hn),n(Wa),n(_n),n(qa),n(Ua),n(yn),n(Za),n(Qa),n(Tn),n(Ga),n(ja),n(vn),n(za),n(Ba),n(Jn),n(Va),n(Xa),n(La),n(Wn),n(Ha),n(Aa),n(Ra),n(Zn),n(Fa),n(Qn),n(Na),n(Ia),n(y),n(Pa),n(Ea),n(Hn),n(Ya),n(ll)),n(h),u(Q,e),u(W,e),u(O,e),u(le,e),u(ae,e),u(ie,e),u(re,e),u(pe,e),u(de,e),u(ce,e),u(ue,e),u(ye,e),u(Te,e),u(we),u(ve),u(Ce,e),u(We,e),u(Qe,e),u(ze,e),u(Ve,e),u(Le,e),u(He,e),u(Re,e),u(Ie,e),u(L,e),u(Ee,e),u(Se,e),u(Ke,e),u(et,e),u(tt,e),u(lt,e),u(at,e),u(st,e),u(rt,e),u(pt,e),u(ct,e),u(bt,e),u(ht,e),u(_t),u($t),u(yt),u(Mt),u(Tt,e),u(kt,e),u(qt,e),u(Zt,e),u(Qt,e),u(Gt,e),u(Bt,e),u(Xt,e),u(Ht,e),u(F,e),u(At,e),u(Ft,e),u(Pt,e),u(N,e),u(Yt,e),u(Dt,e),u(Ot,e),u(en,e),u(nn,e),u(ln,e),u(an,e),u(sn,e),u(mn,e),u(I,e),u(pn,e),u(fn,e),u(un,e),u(gn,e),u($n,e),u(Mn,e),u(wn,e),u(Cn,e),u(xn,e),u(kn,e),u(qn,e),u(Un,e),u(Gn,e),u(jn),u(zn),u(Bn),u(Vn),u(Xn),u(Ln,e)}}}const Mr='{"title":"量化 🤗 Transformers 模型","local":"量化--transformers-模型","sections":[{"title":"AWQ集成","local":"awq集成","sections":[{"title":"量化一个模型","local":"量化一个模型","sections":[],"depth":3},{"title":"加载一个量化的模型","local":"加载一个量化的模型","sections":[],"depth":3}],"depth":2},{"title":"示例使用","local":"示例使用","sections":[{"title":"结合 AWQ 和 Flash Attention","local":"结合-awq-和-flash-attention","sections":[],"depth":3},{"title":"基准测试","local":"基准测试","sections":[],"depth":3},{"title":"Google colab 演示","local":"google-colab-演示","sections":[],"depth":3},{"title":"AwqConfig","local":"transformers.AwqConfig","sections":[],"depth":3}],"depth":2},{"title":"AutoGPTQ 集成","local":"autogptq-集成","sections":[{"title":"要求","local":"要求","sections":[],"depth":3},{"title":"加载和量化模型","local":"加载和量化模型","sections":[{"title":"GPTQ 配置","local":"gptq-配置","sections":[],"depth":4},{"title":"量化","local":"量化","sections":[],"depth":4}],"depth":3},{"title":"推送量化模型到 🤗 Hub","local":"推送量化模型到--hub","sections":[],"depth":3},{"title":"从 🤗 Hub 加载一个量化模型","local":"从--hub-加载一个量化模型","sections":[],"depth":3},{"title":"Exllama内核加快推理速度","local":"exllama内核加快推理速度","sections":[{"title":"微调一个量化模型","local":"微调一个量化模型","sections":[],"depth":4}],"depth":3},{"title":"示例演示","local":"示例演示","sections":[],"depth":3},{"title":"GPTQConfig","local":"transformers.GPTQConfig","sections":[],"depth":3}],"depth":2},{"title":"bitsandbytes 集成","local":"bitsandbytes-集成","sections":[{"title":"通用用法","local":"通用用法","sections":[],"depth":3},{"title":"FP4 量化","local":"fp4-量化","sections":[{"title":"要求","local":"要求","sections":[],"depth":4},{"title":"提示和最佳实践","local":"提示和最佳实践","sections":[],"depth":4},{"title":"加载 4 位量化的大模型","local":"加载-4-位量化的大模型","sections":[],"depth":4}],"depth":3},{"title":"加载 8 位量化的大模型","local":"加载-8-位量化的大模型","sections":[{"title":"高级用例","local":"高级用例","sections":[{"title":"更改计算数据类型","local":"更改计算数据类型","sections":[],"depth":5}],"depth":4},{"title":"使用 NF4（普通浮点数 4）数据类型","local":"使用-nf4普通浮点数-4数据类型","sections":[],"depth":4},{"title":"使用嵌套量化进行更高效的内存推理","local":"使用嵌套量化进行更高效的内存推理","sections":[],"depth":4}],"depth":3},{"title":"将量化模型推送到🤗 Hub","local":"将量化模型推送到-hub","sections":[],"depth":3},{"title":"从🤗 Hub加载量化模型","local":"从-hub加载量化模型","sections":[],"depth":3},{"title":"高级用例","local":"高级用例","sections":[{"title":"在 cpu 和 gpu 之间卸载","local":"在-cpu-和-gpu-之间卸载","sections":[],"depth":4},{"title":"使用 llm_int8_threshold","local":"使用-llmint8threshold","sections":[],"depth":4},{"title":"跳过某些模块的转换","local":"跳过某些模块的转换","sections":[],"depth":4},{"title":"微调已加载为8位精度的模型","local":"微调已加载为8位精度的模型","sections":[],"depth":4}],"depth":3},{"title":"BitsAndBytesConfig","local":"transformers.BitsAndBytesConfig","sections":[],"depth":3}],"depth":2},{"title":"使用 🤗 optimum 进行量化","local":"使用--optimum-进行量化","sections":[],"depth":2}],"depth":1}';function Tr(Z){return pr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Wr extends dr{constructor(h){super(),fr(this,h,Tr,yr,mr,{})}}export{Wr as component};
