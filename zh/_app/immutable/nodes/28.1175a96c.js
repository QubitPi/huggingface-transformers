import{s as $r,o as Tr,n as yr}from"../chunks/scheduler.9991993c.js";import{S as Mr,i as Fr,g as o,s as r,r as d,A as wr,h as n,f as s,c as a,j as v,u as l,x,k as b,y as t,a as _,v as m,d as p,t as u,w as f}from"../chunks/index.7fc9a5e7.js";import{T as Ir}from"../chunks/Tip.9de92fc6.js";import{D as $}from"../chunks/Docstring.8180f571.js";import{C as Er}from"../chunks/CodeBlock.e11cba92.js";import{E as zr}from"../chunks/ExampleCodeBlock.a03fccd6.js";import{H as De}from"../chunks/Heading.e3de321f.js";function Jr(ye){let g,P="Examples:",T,F,I;return F=new Er({props:{code:"JTIzJTIwV2UlMjBjYW4ndCUyMGluc3RhbnRpYXRlJTIwZGlyZWN0bHklMjB0aGUlMjBiYXNlJTIwY2xhc3MlMjAqRmVhdHVyZUV4dHJhY3Rpb25NaXhpbiolMjBub3IlMjAqU2VxdWVuY2VGZWF0dXJlRXh0cmFjdG9yKiUyMHNvJTIwbGV0J3MlMjBzaG93JTIwdGhlJTIwZXhhbXBsZXMlMjBvbiUyMGElMEElMjMlMjBkZXJpdmVkJTIwY2xhc3MlM0ElMjAqV2F2MlZlYzJGZWF0dXJlRXh0cmFjdG9yKiUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwV2F2MlZlYzJGZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJmYWNlYm9vayUyRndhdjJ2ZWMyLWJhc2UtOTYwaCUyMiUwQSklMjAlMjAlMjMlMjBEb3dubG9hZCUyMGZlYXR1cmVfZXh0cmFjdGlvbl9jb25maWclMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFmZWF0dXJlX2V4dHJhY3RvciUyMCUzRCUyMFdhdjJWZWMyRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRiUyMiUwQSklMjAlMjAlMjMlMjBFLmcuJTIwZmVhdHVyZV9leHRyYWN0b3IlMjAob3IlMjBtb2RlbCklMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwV2F2MlZlYzJGZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGcHJlcHJvY2Vzc29yX2NvbmZpZy5qc29uJTIyKSUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwV2F2MlZlYzJGZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJmYWNlYm9vayUyRndhdjJ2ZWMyLWJhc2UtOTYwaCUyMiUyQyUyMHJldHVybl9hdHRlbnRpb25fbWFzayUzREZhbHNlJTJDJTIwZm9vJTNERmFsc2UlMEEpJTBBYXNzZXJ0JTIwZmVhdHVyZV9leHRyYWN0b3IucmV0dXJuX2F0dGVudGlvbl9tYXNrJTIwaXMlMjBGYWxzZSUwQWZlYXR1cmVfZXh0cmFjdG9yJTJDJTIwdW51c2VkX2t3YXJncyUyMCUzRCUyMFdhdjJWZWMyRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlLTk2MGglMjIlMkMlMjByZXR1cm5fYXR0ZW50aW9uX21hc2slM0RGYWxzZSUyQyUyMGZvbyUzREZhbHNlJTJDJTIwcmV0dXJuX3VudXNlZF9rd2FyZ3MlM0RUcnVlJTBBKSUwQWFzc2VydCUyMGZlYXR1cmVfZXh0cmFjdG9yLnJldHVybl9hdHRlbnRpb25fbWFzayUyMGlzJTIwRmFsc2UlMEFhc3NlcnQlMjB1bnVzZWRfa3dhcmdzJTIwJTNEJTNEJTIwJTdCJTIyZm9vJTIyJTNBJTIwRmFsc2UlN0Q=",highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *FeatureExtractionMixin* nor *SequenceFeatureExtractor* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *Wav2Vec2FeatureExtractor*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>
)  <span class="hljs-comment"># Download feature_extraction_config from huggingface.co and cache.</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. feature_extractor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
feature_extractor, unused_kwargs = Wav2Vec2FeatureExtractor.from_pretrained(
    <span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>, return_attention_mask=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> feature_extractor.return_attention_mask <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){g=o("p"),g.textContent=P,T=r(),d(F.$$.fragment)},l(y){g=n(y,"P",{"data-svelte-h":!0}),x(g)!=="svelte-kvfsh7"&&(g.textContent=P),T=a(y),l(F.$$.fragment,y)},m(y,z){_(y,g,z),_(y,T,z),m(F,y,z),I=!0},p:yr,i(y){I||(p(F.$$.fragment,y),I=!0)},o(y){u(F.$$.fragment,y),I=!1},d(y){y&&(s(g),s(T)),f(F,y)}}}function Lr(ye){let g,P=`If the <code>processed_features</code> passed are dictionary of numpy arrays, PyTorch tensors or TensorFlow tensors, the
result will use the same type unless you provide a different tensor type with <code>return_tensors</code>. In the case of
PyTorch tensors, you will lose the specific device of your tensors however.`;return{c(){g=o("p"),g.innerHTML=P},l(T){g=n(T,"P",{"data-svelte-h":!0}),x(g)!=="svelte-yvqf1m"&&(g.innerHTML=P)},m(T,F){_(T,g,F)},p:yr,d(T){T&&s(g)}}}function Pr(ye){let g,P,T,F,I,y,z,Kt="Feature Extractor负责为音频或视觉模型准备输入特征。这包括从序列中提取特征，例如，对音频文件进行预处理以生成Log-Mel频谱特征，以及从图像中提取特征，例如，裁剪图像文件，同时还包括填充、归一化和转换为NumPy、PyTorch和TensorFlow张量。",Xe,O,Ye,w,K,gt,$e,er=`This is a feature extraction mixin used to provide saving/loading functionality for sequential and image feature
extractors.`,ht,j,ee,xt,Te,tr=`Instantiate a type of <a href="/docs/transformers/main/zh/main_classes/feature_extractor#transformers.FeatureExtractionMixin">FeatureExtractionMixin</a> from a feature extractor, <em>e.g.</em> a
derived class of <a href="/docs/transformers/main/zh/main_classes/feature_extractor#transformers.SequenceFeatureExtractor">SequenceFeatureExtractor</a>.`,_t,V,vt,C,te,bt,Me,rr=`Save a feature_extractor object to the directory <code>save_directory</code>, so that it can be re-loaded using the
<a href="/docs/transformers/main/zh/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a> class method.`,Ge,re,Se,J,ae,yt,Fe,ar="This is a general feature extraction class for speech recognition.",$t,E,oe,Tt,we,or=`Pad input values / input vectors or a batch of input values / input vectors up to predefined length or to the
max sequence length in the batch.`,Mt,Ie,nr=`Padding side (left/right) padding values are defined at the feature extractor level (with <code>self.padding_side</code>,
<code>self.padding_value</code>)`,Ft,H,Qe,ne,Ae,M,se,wt,Ee,sr='Holds the output of the <a href="/docs/transformers/main/zh/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad">pad()</a> and feature extractor specific <code>__call__</code> methods.',It,ze,ir="This class is derived from a python dictionary and can be used as a dictionary.",Et,q,ie,zt,Je,cr="Convert the inner content to tensors.",Jt,R,ce,Lt,Le,dr=`Send all values to device by calling <code>v.to(*args, **kwargs)</code> (PyTorch only). This should support casting in
different <code>dtypes</code> and sending the <code>BatchFeature</code> to a different <code>device</code>.`,Oe,de,Ke,c,le,Pt,Pe,lr="Mixin that contain utilities for preparing image features.",jt,W,me,Zt,je,mr=`Crops <code>image</code> to the given size using a center crop. Note that if the image is too small to be cropped to the
size given, it will be padded (so the returned result has the size asked).`,kt,N,pe,Vt,Ze,pr="Converts <code>PIL.Image.Image</code> to RGB format.",Ct,U,ue,Ht,ke,ur="Expands 2-dimensional <code>image</code> to 3 dimensions.",qt,B,fe,Rt,Ve,fr=`Flips the channel order of <code>image</code> from RGB to BGR, or vice versa. Note that this will trigger a conversion of
<code>image</code> to a NumPy array if it’s a PIL Image.`,Wt,D,ge,Nt,Ce,gr=`Normalizes <code>image</code> with <code>mean</code> and <code>std</code>. Note that this will trigger a conversion of <code>image</code> to a NumPy array
if it’s a PIL Image.`,Ut,X,he,Bt,He,hr="Rescale a numpy image by scale amount",Dt,Y,xe,Xt,qe,xr="Resizes <code>image</code>. Enforces conversion of input to PIL.Image.",Yt,G,_e,Gt,Re,_r=`Returns a rotated copy of <code>image</code>. This method returns a copy of <code>image</code>, rotated the given number of degrees
counter clockwise around its centre.`,St,S,ve,Qt,We,vr=`Converts <code>image</code> to a numpy array. Optionally rescales it and puts the channel dimension as the first
dimension.`,At,Q,be,Ot,Ne,br=`Converts <code>image</code> to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`,et,Be,tt;return I=new De({props:{title:"Feature Extractor",local:"feature-extractor",headingTag:"h1"}}),O=new De({props:{title:"FeatureExtractionMixin",local:"transformers.FeatureExtractionMixin",headingTag:"h2"}}),K=new $({props:{name:"class transformers.FeatureExtractionMixin",anchor:"transformers.FeatureExtractionMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L240"}}),ee=new $({props:{name:"from_pretrained",anchor:"transformers.FeatureExtractionMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": Union"},{name:"cache_dir",val:": Union = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": Union = None"},{name:"revision",val:": str = 'main'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/main/zh/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.FeatureExtractionMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L264",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A feature extractor of type <a
  href="/docs/transformers/main/zh/main_classes/feature_extractor#transformers.FeatureExtractionMixin"
>FeatureExtractionMixin</a>.</p>
`}}),V=new zr({props:{anchor:"transformers.FeatureExtractionMixin.from_pretrained.example",$$slots:{default:[Jr]},$$scope:{ctx:ye}}}),te=new $({props:{name:"save_pretrained",anchor:"transformers.FeatureExtractionMixin.save_pretrained",parameters:[{name:"save_directory",val:": Union"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FeatureExtractionMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.FeatureExtractionMixin.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/zh/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L376"}}),re=new De({props:{title:"SequenceFeatureExtractor",local:"transformers.SequenceFeatureExtractor",headingTag:"h2"}}),ae=new $({props:{name:"class transformers.SequenceFeatureExtractor",anchor:"transformers.SequenceFeatureExtractor",parameters:[{name:"feature_size",val:": int"},{name:"sampling_rate",val:": int"},{name:"padding_value",val:": float"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.SequenceFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).`,name:"sampling_rate"},{anchor:"transformers.SequenceFeatureExtractor.padding_value",description:`<strong>padding_value</strong> (<code>float</code>) &#x2014;
The value that is used to fill the padding values / vectors.`,name:"padding_value"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_sequence_utils.py#L29"}}),oe=new $({props:{name:"pad",anchor:"transformers.SequenceFeatureExtractor.pad",parameters:[{name:"processed_features",val:": Union"},{name:"padding",val:": Union = True"},{name:"max_length",val:": Optional = None"},{name:"truncation",val:": bool = False"},{name:"pad_to_multiple_of",val:": Optional = None"},{name:"return_attention_mask",val:": Optional = None"},{name:"return_tensors",val:": Union = None"}],parametersDescription:[{anchor:"transformers.SequenceFeatureExtractor.pad.processed_features",description:`<strong>processed_features</strong> (<a href="/docs/transformers/main/zh/main_classes/image_processor#transformers.BatchFeature">BatchFeature</a>, list of <a href="/docs/transformers/main/zh/main_classes/image_processor#transformers.BatchFeature">BatchFeature</a>, <code>Dict[str, List[float]]</code>, <code>Dict[str, List[List[float]]</code> or <code>List[Dict[str, List[float]]]</code>) &#x2014;
Processed inputs. Can represent one input (<a href="/docs/transformers/main/zh/main_classes/image_processor#transformers.BatchFeature">BatchFeature</a> or <code>Dict[str, List[float]]</code>) or a batch of
input values / vectors (list of <a href="/docs/transformers/main/zh/main_classes/image_processor#transformers.BatchFeature">BatchFeature</a>, <em>Dict[str, List[List[float]]]</em> or <em>List[Dict[str,
List[float]]]</em>) so you can use this method during preprocessing as well as in a PyTorch Dataloader
collate function.</p>
<p>Instead of <code>List[float]</code> you can have tensors (numpy arrays, PyTorch tensors or TensorFlow tensors),
see the note above for the return type.`,name:"processed_features"},{anchor:"transformers.SequenceFeatureExtractor.pad.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/zh/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Select a strategy to pad the returned sequences (according to the model&#x2019;s padding side and padding
index) among:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single
sequence if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.SequenceFeatureExtractor.pad.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum length of the returned list and optionally padding length (see above).`,name:"max_length"},{anchor:"transformers.SequenceFeatureExtractor.pad.truncation",description:`<strong>truncation</strong> (<code>bool</code>) &#x2014;
Activates truncation to cut input sequences longer than <code>max_length</code> to <code>max_length</code>.`,name:"truncation"},{anchor:"transformers.SequenceFeatureExtractor.pad.pad_to_multiple_of",description:`<strong>pad_to_multiple_of</strong> (<code>int</code>, <em>optional</em>) &#x2014;
If set will pad the sequence to a multiple of the provided value.</p>
<p>This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability
<code>&gt;= 7.5</code> (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.`,name:"pad_to_multiple_of"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to return the attention mask. If left to the default, will return the attention mask according
to the specific feature_extractor&#x2019;s default.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"},{anchor:"transformers.SequenceFeatureExtractor.pad.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/zh/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_sequence_utils.py#L52"}}),H=new Ir({props:{$$slots:{default:[Lr]},$$scope:{ctx:ye}}}),ne=new De({props:{title:"BatchFeature",local:"transformers.BatchFeature",headingTag:"h2"}}),se=new $({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": Optional = None"},{name:"tensor_type",val:": Union = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L61"}}),ie=new $({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": Union = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/main/zh/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/main/zh/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L164"}}),ce=new $({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BatchFeature.to.args",description:`<strong>args</strong> (<code>Tuple</code>) &#x2014;
Will be passed to the <code>to(...)</code> function of the tensors.`,name:"args"},{anchor:"transformers.BatchFeature.to.kwargs",description:`<strong>kwargs</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>to(...)</code> function of the tensors.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L195",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The same instance after modification.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/zh/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),de=new De({props:{title:"ImageFeatureExtractionMixin",local:"transformers.ImageFeatureExtractionMixin",headingTag:"h2"}}),le=new $({props:{name:"class transformers.ImageFeatureExtractionMixin",anchor:"transformers.ImageFeatureExtractionMixin",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L341"}}),me=new $({props:{name:"center_crop",anchor:"transformers.ImageFeatureExtractionMixin.center_crop",parameters:[{name:"image",val:""},{name:"size",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code> of shape (n_channels, height, width) or (height, width, n_channels)) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.center_crop.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to which crop the image.`,name:"size"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L569",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A center cropped <code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code> of shape: (n_channels,
height, width).</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>new_image</p>
`}}),pe=new $({props:{name:"convert_rgb",anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.convert_rgb.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code>) &#x2014;
The image to convert.`,name:"image"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L383"}}),ue=new $({props:{name:"expand_dims",anchor:"transformers.ImageFeatureExtractionMixin.expand_dims",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.expand_dims.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to expand.`,name:"image"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L436"}}),fe=new $({props:{name:"flip_channel_order",anchor:"transformers.ImageFeatureExtractionMixin.flip_channel_order",parameters:[{name:"image",val:""}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.flip_channel_order.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image whose color channels to flip. If <code>np.ndarray</code> or <code>torch.Tensor</code>, the channel dimension should
be first.`,name:"image"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L644"}}),ge=new $({props:{name:"normalize",anchor:"transformers.ImageFeatureExtractionMixin.normalize",parameters:[{name:"image",val:""},{name:"mean",val:""},{name:"std",val:""},{name:"rescale",val:" = False"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.normalize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.mean",description:`<strong>mean</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The mean (per channel) to use for normalization.`,name:"mean"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.std",description:`<strong>std</strong> (<code>List[float]</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The standard deviation (per channel) to use for normalization.`,name:"std"},{anchor:"transformers.ImageFeatureExtractionMixin.normalize.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to rescale the image to be between 0 and 1. If a PIL image is provided, scaling will
happen automatically.`,name:"rescale"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L456"}}),he=new $({props:{name:"rescale",anchor:"transformers.ImageFeatureExtractionMixin.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": Union"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L397"}}),xe=new $({props:{name:"resize",anchor:"transformers.ImageFeatureExtractionMixin.resize",parameters:[{name:"image",val:""},{name:"size",val:""},{name:"resample",val:" = None"},{name:"default_to_square",val:" = True"},{name:"max_size",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.resize.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.size",description:`<strong>size</strong> (<code>int</code> or <code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image. If <code>size</code> is a sequence like (h, w), output size will be
matched to this.</p>
<p>If <code>size</code> is an int and <code>default_to_square</code> is <code>True</code>, then image will be resized to (size, size). If
<code>size</code> is an int and <code>default_to_square</code> is <code>False</code>, then smaller edge of the image will be matched to
this number. i.e, if height &gt; width, then image will be rescaled to (size * height / width, size).`,name:"size"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PILImageResampling.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.default_to_square",description:`<strong>default_to_square</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
How to convert <code>size</code> when it is a single int. If set to <code>True</code>, the <code>size</code> will be converted to a
square (<code>size</code>,<code>size</code>). If set to <code>False</code>, will replicate
<a href="https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize" rel="nofollow"><code>torchvision.transforms.Resize</code></a>
with support for resizing only the smallest edge and providing an optional <code>max_size</code>.`,name:"default_to_square"},{anchor:"transformers.ImageFeatureExtractionMixin.resize.max_size",description:`<strong>max_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The maximum allowed for the longer edge of the resized image: if the longer edge of the image is
greater than <code>max_size</code> after being resized according to <code>size</code>, then the image is resized again so
that the longer edge is equal to <code>max_size</code>. As a result, <code>size</code> might be overruled, i.e the smaller
edge may be shorter than <code>size</code>. Only used if <code>default_to_square</code> is <code>False</code>.`,name:"max_size"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L502",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A resized <code>PIL.Image.Image</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image</p>
`}}),_e=new $({props:{name:"rotate",anchor:"transformers.ImageFeatureExtractionMixin.rotate",parameters:[{name:"image",val:""},{name:"angle",val:""},{name:"resample",val:" = None"},{name:"expand",val:" = 0"},{name:"center",val:" = None"},{name:"translate",val:" = None"},{name:"fillcolor",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.rotate.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to rotate. If <code>np.ndarray</code> or <code>torch.Tensor</code>, will be converted to <code>PIL.Image.Image</code> before
rotating.`,name:"image"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L661",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A rotated <code>PIL.Image.Image</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image</p>
`}}),ve=new $({props:{name:"to_numpy_array",anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"},{name:"channel_first",val:" = True"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>np.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to a NumPy array.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.). Will
default to <code>True</code> if the image is a PIL Image or an array/tensor of integers, <code>False</code> otherwise.`,name:"rescale"},{anchor:"transformers.ImageFeatureExtractionMixin.to_numpy_array.channel_first",description:`<strong>channel_first</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to permute the dimensions of the image to put the channel dimension first.`,name:"channel_first"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L404"}}),be=new $({props:{name:"to_pil_image",anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image",parameters:[{name:"image",val:""},{name:"rescale",val:" = None"}],parametersDescription:[{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code>) &#x2014;
The image to convert to the PIL Image format.`,name:"image"},{anchor:"transformers.ImageFeatureExtractionMixin.to_pil_image.rescale",description:`<strong>rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will
default to <code>True</code> if the image type is a floating type, <code>False</code> otherwise.`,name:"rescale"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_utils.py#L353"}}),{c(){g=o("meta"),P=r(),T=o("p"),F=r(),d(I.$$.fragment),y=r(),z=o("p"),z.textContent=Kt,Xe=r(),d(O.$$.fragment),Ye=r(),w=o("div"),d(K.$$.fragment),gt=r(),$e=o("p"),$e.textContent=er,ht=r(),j=o("div"),d(ee.$$.fragment),xt=r(),Te=o("p"),Te.innerHTML=tr,_t=r(),d(V.$$.fragment),vt=r(),C=o("div"),d(te.$$.fragment),bt=r(),Me=o("p"),Me.innerHTML=rr,Ge=r(),d(re.$$.fragment),Se=r(),J=o("div"),d(ae.$$.fragment),yt=r(),Fe=o("p"),Fe.textContent=ar,$t=r(),E=o("div"),d(oe.$$.fragment),Tt=r(),we=o("p"),we.textContent=or,Mt=r(),Ie=o("p"),Ie.innerHTML=nr,Ft=r(),d(H.$$.fragment),Qe=r(),d(ne.$$.fragment),Ae=r(),M=o("div"),d(se.$$.fragment),wt=r(),Ee=o("p"),Ee.innerHTML=sr,It=r(),ze=o("p"),ze.textContent=ir,Et=r(),q=o("div"),d(ie.$$.fragment),zt=r(),Je=o("p"),Je.textContent=cr,Jt=r(),R=o("div"),d(ce.$$.fragment),Lt=r(),Le=o("p"),Le.innerHTML=dr,Oe=r(),d(de.$$.fragment),Ke=r(),c=o("div"),d(le.$$.fragment),Pt=r(),Pe=o("p"),Pe.textContent=lr,jt=r(),W=o("div"),d(me.$$.fragment),Zt=r(),je=o("p"),je.innerHTML=mr,kt=r(),N=o("div"),d(pe.$$.fragment),Vt=r(),Ze=o("p"),Ze.innerHTML=pr,Ct=r(),U=o("div"),d(ue.$$.fragment),Ht=r(),ke=o("p"),ke.innerHTML=ur,qt=r(),B=o("div"),d(fe.$$.fragment),Rt=r(),Ve=o("p"),Ve.innerHTML=fr,Wt=r(),D=o("div"),d(ge.$$.fragment),Nt=r(),Ce=o("p"),Ce.innerHTML=gr,Ut=r(),X=o("div"),d(he.$$.fragment),Bt=r(),He=o("p"),He.textContent=hr,Dt=r(),Y=o("div"),d(xe.$$.fragment),Xt=r(),qe=o("p"),qe.innerHTML=xr,Yt=r(),G=o("div"),d(_e.$$.fragment),Gt=r(),Re=o("p"),Re.innerHTML=_r,St=r(),S=o("div"),d(ve.$$.fragment),Qt=r(),We=o("p"),We.innerHTML=vr,At=r(),Q=o("div"),d(be.$$.fragment),Ot=r(),Ne=o("p"),Ne.innerHTML=br,et=r(),Be=o("p"),this.h()},l(e){const i=wr("svelte-u9bgzb",document.head);g=n(i,"META",{name:!0,content:!0}),i.forEach(s),P=a(e),T=n(e,"P",{}),v(T).forEach(s),F=a(e),l(I.$$.fragment,e),y=a(e),z=n(e,"P",{"data-svelte-h":!0}),x(z)!=="svelte-tqffqy"&&(z.textContent=Kt),Xe=a(e),l(O.$$.fragment,e),Ye=a(e),w=n(e,"DIV",{class:!0});var L=v(w);l(K.$$.fragment,L),gt=a(L),$e=n(L,"P",{"data-svelte-h":!0}),x($e)!=="svelte-1u6z21f"&&($e.textContent=er),ht=a(L),j=n(L,"DIV",{class:!0});var k=v(j);l(ee.$$.fragment,k),xt=a(k),Te=n(k,"P",{"data-svelte-h":!0}),x(Te)!=="svelte-1w0xsu5"&&(Te.innerHTML=tr),_t=a(k),l(V.$$.fragment,k),k.forEach(s),vt=a(L),C=n(L,"DIV",{class:!0});var rt=v(C);l(te.$$.fragment,rt),bt=a(rt),Me=n(rt,"P",{"data-svelte-h":!0}),x(Me)!=="svelte-113qaft"&&(Me.innerHTML=rr),rt.forEach(s),L.forEach(s),Ge=a(e),l(re.$$.fragment,e),Se=a(e),J=n(e,"DIV",{class:!0});var Ue=v(J);l(ae.$$.fragment,Ue),yt=a(Ue),Fe=n(Ue,"P",{"data-svelte-h":!0}),x(Fe)!=="svelte-fkistw"&&(Fe.textContent=ar),$t=a(Ue),E=n(Ue,"DIV",{class:!0});var A=v(E);l(oe.$$.fragment,A),Tt=a(A),we=n(A,"P",{"data-svelte-h":!0}),x(we)!=="svelte-1ocd2wm"&&(we.textContent=or),Mt=a(A),Ie=n(A,"P",{"data-svelte-h":!0}),x(Ie)!=="svelte-oeftzc"&&(Ie.innerHTML=nr),Ft=a(A),l(H.$$.fragment,A),A.forEach(s),Ue.forEach(s),Qe=a(e),l(ne.$$.fragment,e),Ae=a(e),M=n(e,"DIV",{class:!0});var Z=v(M);l(se.$$.fragment,Z),wt=a(Z),Ee=n(Z,"P",{"data-svelte-h":!0}),x(Ee)!=="svelte-1y51gyb"&&(Ee.innerHTML=sr),It=a(Z),ze=n(Z,"P",{"data-svelte-h":!0}),x(ze)!=="svelte-saqdtk"&&(ze.textContent=ir),Et=a(Z),q=n(Z,"DIV",{class:!0});var at=v(q);l(ie.$$.fragment,at),zt=a(at),Je=n(at,"P",{"data-svelte-h":!0}),x(Je)!=="svelte-pxfh9u"&&(Je.textContent=cr),at.forEach(s),Jt=a(Z),R=n(Z,"DIV",{class:!0});var ot=v(R);l(ce.$$.fragment,ot),Lt=a(ot),Le=n(ot,"P",{"data-svelte-h":!0}),x(Le)!=="svelte-d0cfhs"&&(Le.innerHTML=dr),ot.forEach(s),Z.forEach(s),Oe=a(e),l(de.$$.fragment,e),Ke=a(e),c=n(e,"DIV",{class:!0});var h=v(c);l(le.$$.fragment,h),Pt=a(h),Pe=n(h,"P",{"data-svelte-h":!0}),x(Pe)!=="svelte-393l39"&&(Pe.textContent=lr),jt=a(h),W=n(h,"DIV",{class:!0});var nt=v(W);l(me.$$.fragment,nt),Zt=a(nt),je=n(nt,"P",{"data-svelte-h":!0}),x(je)!=="svelte-klynef"&&(je.innerHTML=mr),nt.forEach(s),kt=a(h),N=n(h,"DIV",{class:!0});var st=v(N);l(pe.$$.fragment,st),Vt=a(st),Ze=n(st,"P",{"data-svelte-h":!0}),x(Ze)!=="svelte-133v6j"&&(Ze.innerHTML=pr),st.forEach(s),Ct=a(h),U=n(h,"DIV",{class:!0});var it=v(U);l(ue.$$.fragment,it),Ht=a(it),ke=n(it,"P",{"data-svelte-h":!0}),x(ke)!=="svelte-1041ew6"&&(ke.innerHTML=ur),it.forEach(s),qt=a(h),B=n(h,"DIV",{class:!0});var ct=v(B);l(fe.$$.fragment,ct),Rt=a(ct),Ve=n(ct,"P",{"data-svelte-h":!0}),x(Ve)!=="svelte-w2v4kz"&&(Ve.innerHTML=fr),ct.forEach(s),Wt=a(h),D=n(h,"DIV",{class:!0});var dt=v(D);l(ge.$$.fragment,dt),Nt=a(dt),Ce=n(dt,"P",{"data-svelte-h":!0}),x(Ce)!=="svelte-1omk197"&&(Ce.innerHTML=gr),dt.forEach(s),Ut=a(h),X=n(h,"DIV",{class:!0});var lt=v(X);l(he.$$.fragment,lt),Bt=a(lt),He=n(lt,"P",{"data-svelte-h":!0}),x(He)!=="svelte-92qzed"&&(He.textContent=hr),lt.forEach(s),Dt=a(h),Y=n(h,"DIV",{class:!0});var mt=v(Y);l(xe.$$.fragment,mt),Xt=a(mt),qe=n(mt,"P",{"data-svelte-h":!0}),x(qe)!=="svelte-dheqn4"&&(qe.innerHTML=xr),mt.forEach(s),Yt=a(h),G=n(h,"DIV",{class:!0});var pt=v(G);l(_e.$$.fragment,pt),Gt=a(pt),Re=n(pt,"P",{"data-svelte-h":!0}),x(Re)!=="svelte-sbsh56"&&(Re.innerHTML=_r),pt.forEach(s),St=a(h),S=n(h,"DIV",{class:!0});var ut=v(S);l(ve.$$.fragment,ut),Qt=a(ut),We=n(ut,"P",{"data-svelte-h":!0}),x(We)!=="svelte-pgs2u0"&&(We.innerHTML=vr),ut.forEach(s),At=a(h),Q=n(h,"DIV",{class:!0});var ft=v(Q);l(be.$$.fragment,ft),Ot=a(ft),Ne=n(ft,"P",{"data-svelte-h":!0}),x(Ne)!=="svelte-e557ju"&&(Ne.innerHTML=br),ft.forEach(s),h.forEach(s),et=a(e),Be=n(e,"P",{}),v(Be).forEach(s),this.h()},h(){b(g,"name","hf:doc:metadata"),b(g,"content",jr),b(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),b(c,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,i){t(document.head,g),_(e,P,i),_(e,T,i),_(e,F,i),m(I,e,i),_(e,y,i),_(e,z,i),_(e,Xe,i),m(O,e,i),_(e,Ye,i),_(e,w,i),m(K,w,null),t(w,gt),t(w,$e),t(w,ht),t(w,j),m(ee,j,null),t(j,xt),t(j,Te),t(j,_t),m(V,j,null),t(w,vt),t(w,C),m(te,C,null),t(C,bt),t(C,Me),_(e,Ge,i),m(re,e,i),_(e,Se,i),_(e,J,i),m(ae,J,null),t(J,yt),t(J,Fe),t(J,$t),t(J,E),m(oe,E,null),t(E,Tt),t(E,we),t(E,Mt),t(E,Ie),t(E,Ft),m(H,E,null),_(e,Qe,i),m(ne,e,i),_(e,Ae,i),_(e,M,i),m(se,M,null),t(M,wt),t(M,Ee),t(M,It),t(M,ze),t(M,Et),t(M,q),m(ie,q,null),t(q,zt),t(q,Je),t(M,Jt),t(M,R),m(ce,R,null),t(R,Lt),t(R,Le),_(e,Oe,i),m(de,e,i),_(e,Ke,i),_(e,c,i),m(le,c,null),t(c,Pt),t(c,Pe),t(c,jt),t(c,W),m(me,W,null),t(W,Zt),t(W,je),t(c,kt),t(c,N),m(pe,N,null),t(N,Vt),t(N,Ze),t(c,Ct),t(c,U),m(ue,U,null),t(U,Ht),t(U,ke),t(c,qt),t(c,B),m(fe,B,null),t(B,Rt),t(B,Ve),t(c,Wt),t(c,D),m(ge,D,null),t(D,Nt),t(D,Ce),t(c,Ut),t(c,X),m(he,X,null),t(X,Bt),t(X,He),t(c,Dt),t(c,Y),m(xe,Y,null),t(Y,Xt),t(Y,qe),t(c,Yt),t(c,G),m(_e,G,null),t(G,Gt),t(G,Re),t(c,St),t(c,S),m(ve,S,null),t(S,Qt),t(S,We),t(c,At),t(c,Q),m(be,Q,null),t(Q,Ot),t(Q,Ne),_(e,et,i),_(e,Be,i),tt=!0},p(e,[i]){const L={};i&2&&(L.$$scope={dirty:i,ctx:e}),V.$set(L);const k={};i&2&&(k.$$scope={dirty:i,ctx:e}),H.$set(k)},i(e){tt||(p(I.$$.fragment,e),p(O.$$.fragment,e),p(K.$$.fragment,e),p(ee.$$.fragment,e),p(V.$$.fragment,e),p(te.$$.fragment,e),p(re.$$.fragment,e),p(ae.$$.fragment,e),p(oe.$$.fragment,e),p(H.$$.fragment,e),p(ne.$$.fragment,e),p(se.$$.fragment,e),p(ie.$$.fragment,e),p(ce.$$.fragment,e),p(de.$$.fragment,e),p(le.$$.fragment,e),p(me.$$.fragment,e),p(pe.$$.fragment,e),p(ue.$$.fragment,e),p(fe.$$.fragment,e),p(ge.$$.fragment,e),p(he.$$.fragment,e),p(xe.$$.fragment,e),p(_e.$$.fragment,e),p(ve.$$.fragment,e),p(be.$$.fragment,e),tt=!0)},o(e){u(I.$$.fragment,e),u(O.$$.fragment,e),u(K.$$.fragment,e),u(ee.$$.fragment,e),u(V.$$.fragment,e),u(te.$$.fragment,e),u(re.$$.fragment,e),u(ae.$$.fragment,e),u(oe.$$.fragment,e),u(H.$$.fragment,e),u(ne.$$.fragment,e),u(se.$$.fragment,e),u(ie.$$.fragment,e),u(ce.$$.fragment,e),u(de.$$.fragment,e),u(le.$$.fragment,e),u(me.$$.fragment,e),u(pe.$$.fragment,e),u(ue.$$.fragment,e),u(fe.$$.fragment,e),u(ge.$$.fragment,e),u(he.$$.fragment,e),u(xe.$$.fragment,e),u(_e.$$.fragment,e),u(ve.$$.fragment,e),u(be.$$.fragment,e),tt=!1},d(e){e&&(s(P),s(T),s(F),s(y),s(z),s(Xe),s(Ye),s(w),s(Ge),s(Se),s(J),s(Qe),s(Ae),s(M),s(Oe),s(Ke),s(c),s(et),s(Be)),s(g),f(I,e),f(O,e),f(K),f(ee),f(V),f(te),f(re,e),f(ae),f(oe),f(H),f(ne,e),f(se),f(ie),f(ce),f(de,e),f(le),f(me),f(pe),f(ue),f(fe),f(ge),f(he),f(xe),f(_e),f(ve),f(be)}}}const jr='{"title":"Feature Extractor","local":"feature-extractor","sections":[{"title":"FeatureExtractionMixin","local":"transformers.FeatureExtractionMixin","sections":[],"depth":2},{"title":"SequenceFeatureExtractor","local":"transformers.SequenceFeatureExtractor","sections":[],"depth":2},{"title":"BatchFeature","local":"transformers.BatchFeature","sections":[],"depth":2},{"title":"ImageFeatureExtractionMixin","local":"transformers.ImageFeatureExtractionMixin","sections":[],"depth":2}],"depth":1}';function Zr(ye){return Tr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Nr extends Mr{constructor(g){super(),Fr(this,g,Zr,Pr,$r,{})}}export{Nr as component};
