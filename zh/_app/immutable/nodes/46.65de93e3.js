import{s as yn,n as $n,o as vn}from"../chunks/scheduler.9991993c.js";import{S as Cn,i as jn,g as a,s as l,r as g,A as In,h as d,f as n,c as r,j as un,u as o,x as c,k as Be,y as wn,a as i,v as s,d as m,t as h,w as p}from"../chunks/index.7fc9a5e7.js";import{C as Se}from"../chunks/CodeBlock.e11cba92.js";import{H as f}from"../chunks/Heading.e3de321f.js";function Rn(Le){let b,It,Ct,wt,u,Rt,y,Ne='本指南旨在为使用<a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html" rel="nofollow"><code>torch.compile()</code></a>在<a href="https://huggingface.co/models?pipeline_tag=image-classification&amp;library=transformers&amp;sort=trending" rel="nofollow">🤗 Transformers中的计算机视觉模型</a>中引入的推理速度提升提供一个基准。',kt,$,Jt,v,De="根据模型和GPU的不同，<code>torch.compile()</code>在推理过程中可以提高多达30%的速度。要使用<code>torch.compile()</code>，只需安装2.0及以上版本的<code>torch</code>即可。",Et,C,He=`编译模型需要时间，因此如果您只需要编译一次模型而不是每次推理都编译，那么它非常有用。
要编译您选择的任何计算机视觉模型，请按照以下方式调用<code>torch.compile()</code>：`,Ut,j,_t,I,ze='<code>compile()</code> 提供了多种编译模式，它们在编译时间和推理开销上有所不同。<code>max-autotune</code> 比 <code>reduce-overhead</code> 需要更长的时间，但会得到更快的推理速度。默认模式在编译时最快，但在推理时间上与 <code>reduce-overhead</code> 相比效率较低。在本指南中，我们使用了默认模式。您可以在<a href="https://pytorch.org/get-started/pytorch-2.0/#user-experience" rel="nofollow">这里</a>了解更多信息。',Zt,w,We="我们在 PyTorch 2.0.1 版本上使用不同的计算机视觉模型、任务、硬件类型和数据批量大小对 <code>torch.compile</code> 进行了基准测试。",Bt,R,St,k,Ve="以下是每个任务的基准测试代码。我们在推理之前”预热“GPU，并取300次推理的平均值，每次使用相同的图像。",Lt,J,Nt,E,Dt,U,Ht,_,zt,Z,Wt,B,Vt,S,xe="以下是我们进行基准测试的模型列表。",xt,L,Xe="<strong>图像分类</strong>",Xt,N,Oe='<li><a href="https://huggingface.co/google/vit-base-patch16-224" rel="nofollow">google/vit-base-patch16-224</a></li> <li><a href="https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k" rel="nofollow">microsoft/beit-base-patch16-224-pt22k-ft22k</a></li> <li><a href="https://huggingface.co/facebook/convnext-large-224" rel="nofollow">facebook/convnext-large-224</a></li> <li><a href="https://huggingface.co/" rel="nofollow">microsoft/resnet-50</a></li>',Ot,D,Ge="<strong>图像分割</strong>",Gt,H,Ae='<li><a href="https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512" rel="nofollow">nvidia/segformer-b0-finetuned-ade-512-512</a></li> <li><a href="https://huggingface.co/facebook/mask2former-swin-tiny-coco-panoptic" rel="nofollow">facebook/mask2former-swin-tiny-coco-panoptic</a></li> <li><a href="https://huggingface.co/facebook/maskformer-swin-base-ade" rel="nofollow">facebook/maskformer-swin-base-ade</a></li> <li><a href="https://huggingface.co/google/deeplabv3_mobilenet_v2_1.0_513" rel="nofollow">google/deeplabv3_mobilenet_v2_1.0_513</a></li>',At,z,Fe="<strong>目标检测</strong>",Ft,W,Ye='<li><p><a href="https://huggingface.co/google/owlvit-base-patch32" rel="nofollow">google/owlvit-base-patch32</a></p></li> <li><p><a href="https://huggingface.co/facebook/detr-resnet-101" rel="nofollow">facebook/detr-resnet-101</a></p></li> <li><p><a href="https://huggingface.co/microsoft/conditional-detr-resnet-50" rel="nofollow">microsoft/conditional-detr-resnet-50</a></p> <p>下面是使用和不使用<code>torch.compile()</code>的推理持续时间可视化，以及每个模型在不同硬件和数据批量大小下的改进百分比。</p></li>',Yt,T,qe='<div><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/a100_batch_comp.png"/></div> <div><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_batch_comp.png"/></div> <div><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/t4_batch_comp.png"/></div>',qt,M,Pe='<div><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_duration.png"/></div> <div><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_percentage.png"/></div>',Pt,V,Qe='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/v100_1_duration.png" alt="Duration Comparison on V100 with Batch Size of 1"/>',Qt,x,Ke='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/T4_4_percentage.png" alt="Percentage Improvement on T4 with Batch Size of 4"/>',Kt,X,tn="下面可以找到每个模型使用和不使用<code>compile()</code>的推理时间（毫秒）。请注意，OwlViT在大批量大小下会导致内存溢出。",te,O,ee,G,en='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ViT</td> <td align="center">9.325</td> <td align="center">7.584</td></tr> <tr><td align="center">Image Segmentation/Segformer</td> <td align="center">11.759</td> <td align="center">10.500</td></tr> <tr><td align="center">Object Detection/OwlViT</td> <td align="center">24.978</td> <td align="center">18.420</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">11.282</td> <td align="center">8.448</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">34.619</td> <td align="center">19.040</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">10.410</td> <td align="center">10.208</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">6.531</td> <td align="center">4.124</td></tr> <tr><td align="center">Image Segmentation/Mask2former</td> <td align="center">60.188</td> <td align="center">49.117</td></tr> <tr><td align="center">Image Segmentation/Maskformer</td> <td align="center">75.764</td> <td align="center">59.487</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">8.583</td> <td align="center">3.974</td></tr> <tr><td align="center">Object Detection/Resnet-101</td> <td align="center">36.276</td> <td align="center">18.197</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">31.219</td> <td align="center">17.993</td></tr></tbody>',ne,A,ie,F,nn='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ViT</td> <td align="center">14.832</td> <td align="center">14.499</td></tr> <tr><td align="center">Image Segmentation/Segformer</td> <td align="center">18.838</td> <td align="center">16.476</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">13.205</td> <td align="center">13.048</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">48.657</td> <td align="center">32.418</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">22.940</td> <td align="center">21.631</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">6.657</td> <td align="center">4.268</td></tr> <tr><td align="center">Image Segmentation/Mask2former</td> <td align="center">74.277</td> <td align="center">61.781</td></tr> <tr><td align="center">Image Segmentation/Maskformer</td> <td align="center">180.700</td> <td align="center">159.116</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">14.174</td> <td align="center">8.515</td></tr> <tr><td align="center">Object Detection/Resnet-101</td> <td align="center">68.101</td> <td align="center">44.998</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">56.470</td> <td align="center">35.552</td></tr></tbody>',le,Y,re,q,ln='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ViT</td> <td align="center">40.944</td> <td align="center">40.010</td></tr> <tr><td align="center">Image Segmentation/Segformer</td> <td align="center">37.005</td> <td align="center">31.144</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">41.854</td> <td align="center">41.048</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">164.382</td> <td align="center">161.902</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">82.258</td> <td align="center">75.561</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">7.018</td> <td align="center">5.024</td></tr> <tr><td align="center">Image Segmentation/Mask2former</td> <td align="center">178.945</td> <td align="center">154.814</td></tr> <tr><td align="center">Image Segmentation/Maskformer</td> <td align="center">638.570</td> <td align="center">579.826</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">51.693</td> <td align="center">30.310</td></tr> <tr><td align="center">Object Detection/Resnet-101</td> <td align="center">232.887</td> <td align="center">155.021</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">180.491</td> <td align="center">124.032</td></tr></tbody>',ae,P,de,Q,rn='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ViT</td> <td align="center">10.495</td> <td align="center">6.00</td></tr> <tr><td align="center">Image Segmentation/Segformer</td> <td align="center">13.321</td> <td align="center">5.862</td></tr> <tr><td align="center">Object Detection/OwlViT</td> <td align="center">25.769</td> <td align="center">22.395</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">11.347</td> <td align="center">7.234</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">33.951</td> <td align="center">19.388</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">11.623</td> <td align="center">10.412</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">6.484</td> <td align="center">3.820</td></tr> <tr><td align="center">Image Segmentation/Mask2former</td> <td align="center">64.640</td> <td align="center">49.873</td></tr> <tr><td align="center">Image Segmentation/Maskformer</td> <td align="center">95.532</td> <td align="center">72.207</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">9.217</td> <td align="center">4.753</td></tr> <tr><td align="center">Object Detection/Resnet-101</td> <td align="center">52.818</td> <td align="center">28.367</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">39.512</td> <td align="center">20.816</td></tr></tbody>',ce,K,ge,tt,an='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ViT</td> <td align="center">15.181</td> <td align="center">14.501</td></tr> <tr><td align="center">Image Segmentation/Segformer</td> <td align="center">16.787</td> <td align="center">16.188</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">15.171</td> <td align="center">14.753</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">88.529</td> <td align="center">64.195</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">29.574</td> <td align="center">27.085</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">6.109</td> <td align="center">4.731</td></tr> <tr><td align="center">Image Segmentation/Mask2former</td> <td align="center">90.402</td> <td align="center">76.926</td></tr> <tr><td align="center">Image Segmentation/Maskformer</td> <td align="center">234.261</td> <td align="center">205.456</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">24.623</td> <td align="center">14.816</td></tr> <tr><td align="center">Object Detection/Resnet-101</td> <td align="center">134.672</td> <td align="center">101.304</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">97.464</td> <td align="center">69.739</td></tr></tbody>',oe,et,se,nt,dn='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ViT</td> <td align="center">52.209</td> <td align="center">51.633</td></tr> <tr><td align="center">Image Segmentation/Segformer</td> <td align="center">61.013</td> <td align="center">55.499</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">53.938</td> <td align="center">53.581</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">OOM</td> <td align="center">OOM</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">109.682</td> <td align="center">100.771</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">14.857</td> <td align="center">12.089</td></tr> <tr><td align="center">Image Segmentation/Mask2former</td> <td align="center">249.605</td> <td align="center">222.801</td></tr> <tr><td align="center">Image Segmentation/Maskformer</td> <td align="center">831.142</td> <td align="center">743.645</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">93.129</td> <td align="center">55.365</td></tr> <tr><td align="center">Object Detection/Resnet-101</td> <td align="center">482.425</td> <td align="center">361.843</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">344.661</td> <td align="center">255.298</td></tr></tbody>',me,it,he,lt,cn='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ViT</td> <td align="center">16.520</td> <td align="center">15.786</td></tr> <tr><td align="center">Image Segmentation/Segformer</td> <td align="center">16.116</td> <td align="center">14.205</td></tr> <tr><td align="center">Object Detection/OwlViT</td> <td align="center">53.634</td> <td align="center">51.105</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">16.464</td> <td align="center">15.710</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">73.100</td> <td align="center">53.99</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">32.932</td> <td align="center">30.845</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">6.031</td> <td align="center">4.321</td></tr> <tr><td align="center">Image Segmentation/Mask2former</td> <td align="center">79.192</td> <td align="center">66.815</td></tr> <tr><td align="center">Image Segmentation/Maskformer</td> <td align="center">200.026</td> <td align="center">188.268</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">18.908</td> <td align="center">11.997</td></tr> <tr><td align="center">Object Detection/Resnet-101</td> <td align="center">106.622</td> <td align="center">82.566</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">77.594</td> <td align="center">56.984</td></tr></tbody>',pe,rt,fe,at,gn='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ViT</td> <td align="center">43.653</td> <td align="center">43.626</td></tr> <tr><td align="center">Image Segmentation/Segformer</td> <td align="center">45.327</td> <td align="center">42.445</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">52.007</td> <td align="center">51.354</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">277.850</td> <td align="center">268.003</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">119.259</td> <td align="center">105.580</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">13.039</td> <td align="center">11.388</td></tr> <tr><td align="center">Image Segmentation/Mask2former</td> <td align="center">201.540</td> <td align="center">184.670</td></tr> <tr><td align="center">Image Segmentation/Maskformer</td> <td align="center">764.052</td> <td align="center">711.280</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">74.289</td> <td align="center">48.677</td></tr> <tr><td align="center">Object Detection/Resnet-101</td> <td align="center">421.859</td> <td align="center">357.614</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">289.002</td> <td align="center">226.945</td></tr></tbody>',be,dt,Te,ct,on='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ViT</td> <td align="center">163.914</td> <td align="center">160.907</td></tr> <tr><td align="center">Image Segmentation/Segformer</td> <td align="center">192.412</td> <td align="center">163.620</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">188.978</td> <td align="center">187.976</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">OOM</td> <td align="center">OOM</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">422.886</td> <td align="center">388.078</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">44.114</td> <td align="center">37.604</td></tr> <tr><td align="center">Image Segmentation/Mask2former</td> <td align="center">756.337</td> <td align="center">695.291</td></tr> <tr><td align="center">Image Segmentation/Maskformer</td> <td align="center">2842.940</td> <td align="center">2656.88</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">299.003</td> <td align="center">201.942</td></tr> <tr><td align="center">Object Detection/Resnet-101</td> <td align="center">1619.505</td> <td align="center">1262.758</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">1137.513</td> <td align="center">897.390</td></tr></tbody>',Me,gt,ue,ot,sn='我们还在 PyTorch Nightly 版本（2.1.0dev）上进行了基准测试，可以在<a href="https://download.pytorch.org/whl/nightly/cu118" rel="nofollow">这里</a>找到 Nightly 版本的安装包，并观察到了未编译和编译模型的延迟性能改善。',ye,st,$e,mt,mn='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>Batch Size</strong></th> <th align="center"><strong>torch 2.0 - no compile</strong></th> <th align="center"><strong>torch 2.0 -<br/> compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/BeiT</td> <td align="center">Unbatched</td> <td align="center">12.462</td> <td align="center">6.954</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">4</td> <td align="center">14.109</td> <td align="center">12.851</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">16</td> <td align="center">42.179</td> <td align="center">42.147</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">Unbatched</td> <td align="center">30.484</td> <td align="center">15.221</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">4</td> <td align="center">46.816</td> <td align="center">30.942</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">16</td> <td align="center">163.749</td> <td align="center">163.706</td></tr></tbody>',ve,ht,Ce,pt,hn='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>Batch Size</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/BeiT</td> <td align="center">Unbatched</td> <td align="center">14.408</td> <td align="center">14.052</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">4</td> <td align="center">47.381</td> <td align="center">46.604</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">16</td> <td align="center">42.179</td> <td align="center">42.147</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">Unbatched</td> <td align="center">68.382</td> <td align="center">53.481</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">4</td> <td align="center">269.615</td> <td align="center">204.785</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">16</td> <td align="center">OOM</td> <td align="center">OOM</td></tr></tbody>',je,ft,pn="### V100",Ie,bt,fn='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>Batch Size</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/BeiT</td> <td align="center">Unbatched</td> <td align="center">13.477</td> <td align="center">7.926</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">4</td> <td align="center">15.103</td> <td align="center">14.378</td></tr> <tr><td align="center">Image Classification/BeiT</td> <td align="center">16</td> <td align="center">52.517</td> <td align="center">51.691</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">Unbatched</td> <td align="center">28.706</td> <td align="center">19.077</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">4</td> <td align="center">88.402</td> <td align="center">62.949</td></tr> <tr><td align="center">Object Detection/DETR</td> <td align="center">16</td> <td align="center">OOM</td> <td align="center">OOM</td></tr></tbody>',we,Tt,Re,Mt,bn="我们在 PyTorch Nightly 版本中为 A100 和 T4 进行了 <code>reduce-overhead</code> 编译模式的性能基准测试。",ke,ut,Je,yt,Tn='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>Batch Size</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">Unbatched</td> <td align="center">11.758</td> <td align="center">7.335</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">4</td> <td align="center">23.171</td> <td align="center">21.490</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">Unbatched</td> <td align="center">7.435</td> <td align="center">3.801</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">4</td> <td align="center">7.261</td> <td align="center">2.187</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">Unbatched</td> <td align="center">32.823</td> <td align="center">11.627</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">4</td> <td align="center">50.622</td> <td align="center">33.831</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">Unbatched</td> <td align="center">9.869</td> <td align="center">4.244</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">4</td> <td align="center">14.385</td> <td align="center">7.946</td></tr></tbody>',Ee,$t,Ue,vt,Mn='<thead><tr><th align="center"><strong>Task/Model</strong></th> <th align="center"><strong>Batch Size</strong></th> <th align="center"><strong>torch 2.0 - <br/>no compile</strong></th> <th align="center"><strong>torch 2.0 - <br/>compile</strong></th></tr></thead> <tbody><tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">Unbatched</td> <td align="center">32.137</td> <td align="center">31.84</td></tr> <tr><td align="center">Image Classification/ConvNeXT</td> <td align="center">4</td> <td align="center">120.944</td> <td align="center">110.209</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">Unbatched</td> <td align="center">9.761</td> <td align="center">7.698</td></tr> <tr><td align="center">Image Classification/ResNet</td> <td align="center">4</td> <td align="center">15.215</td> <td align="center">13.871</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">Unbatched</td> <td align="center">72.150</td> <td align="center">57.660</td></tr> <tr><td align="center">Object Detection/Conditional-DETR</td> <td align="center">4</td> <td align="center">301.494</td> <td align="center">247.543</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">Unbatched</td> <td align="center">22.266</td> <td align="center">19.339</td></tr> <tr><td align="center">Image Segmentation/MobileNet</td> <td align="center">4</td> <td align="center">78.311</td> <td align="center">50.983</td></tr></tbody>',_e,jt,Ze;return u=new f({props:{title:"使用 torch.compile() 优化推理",local:"使用-torchcompile-优化推理",headingTag:"h1"}}),$=new f({props:{title:"torch.compile 的优势",local:"torchcompile-的优势",headingTag:"h2"}}),j=new Se({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKE1PREVMX0lEKS50byglMjJjdWRhJTIyKSUwQSUyQiUyMG1vZGVsJTIwJTNEJTIwdG9yY2guY29tcGlsZShtb2RlbCk=",highlighted:`from transformers import AutoModelForImageClassification

model = AutoModelForImageClassification.from_pretrained(MODEL_ID).to(&quot;cuda&quot;)
<span class="hljs-addition">+ model = torch.compile(model)</span>`,wrap:!1}}),R=new f({props:{title:"基准测试代码",local:"基准测试代码",headingTag:"h2"}}),J=new f({props:{title:"使用 ViT 进行图像分类",local:"使用-vit-进行图像分类",headingTag:"h3"}}),E=new Se({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWltcG9ydCUyMG51bXB5JTIwYXMlMjBucCUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvSW1hZ2VQcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBJTBBdXJsJTIwJTNEJTIwJ2h0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGcnJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZ2aXQtYmFzZS1wYXRjaDE2LTIyNCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnZpdC1iYXNlLXBhdGNoMTYtMjI0JTIyKS50byglMjJjdWRhJTIyKSUwQW1vZGVsJTIwJTNEJTIwdG9yY2guY29tcGlsZShtb2RlbCklMEElMEFwcm9jZXNzZWRfaW5wdXQlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCdwdCcpLnRvKGRldmljZSUzRCUyMmN1ZGElMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMF8lMjAlM0QlMjBtb2RlbCgqKnByb2Nlc3NlZF9pbnB1dCklMEE=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForImageClassification

url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224&quot;</span>)
model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)
model = torch.<span class="hljs-built_in">compile</span>(model)

processed_input = processor(image, return_tensors=<span class="hljs-string">&#x27;pt&#x27;</span>).to(device=<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-keyword">with</span> torch.no_grad():
    _ = model(**processed_input)
`,wrap:!1}}),U=new f({props:{title:"使用 DETR 进行目标检测",local:"使用-detr-进行目标检测",headingTag:"h4"}}),_=new Se({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbEZvck9iamVjdERldGVjdGlvbiUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZXRyLXJlc25ldC01MCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck9iamVjdERldGVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZXRyLXJlc25ldC01MCUyMikudG8oJTIyY3VkYSUyMiklMEFtb2RlbCUyMCUzRCUyMHRvcmNoLmNvbXBpbGUobW9kZWwpJTBBJTBBdGV4dHMlMjAlM0QlMjAlNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IodGV4dCUzRHRleHRzJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byglMjJjdWRhJTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBfJTIwJTNEJTIwbW9kZWwoKippbnB1dHMp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForObjectDetection

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/detr-resnet-50&quot;</span>)
model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;facebook/detr-resnet-50&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)
model = torch.<span class="hljs-built_in">compile</span>(model)

texts = [<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>]
inputs = processor(text=texts, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-keyword">with</span> torch.no_grad():
    _ = model(**inputs)`,wrap:!1}}),Z=new f({props:{title:"使用 Segformer 进行图像分割",local:"使用-segformer-进行图像分割",headingTag:"h4"}}),B=new Se({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFNlZ2Zvcm1lckltYWdlUHJvY2Vzc29yJTJDJTIwU2VnZm9ybWVyRm9yU2VtYW50aWNTZWdtZW50YXRpb24lMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBTZWdmb3JtZXJJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybnZpZGlhJTJGc2VnZm9ybWVyLWIwLWZpbmV0dW5lZC1hZGUtNTEyLTUxMiUyMiklMEFtb2RlbCUyMCUzRCUyMFNlZ2Zvcm1lckZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJudmlkaWElMkZzZWdmb3JtZXItYjAtZmluZXR1bmVkLWFkZS01MTItNTEyJTIyKS50byglMjJjdWRhJTIyKSUwQW1vZGVsJTIwJTNEJTIwdG9yY2guY29tcGlsZShtb2RlbCklMEFzZWdfaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oJTIyY3VkYSUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwXyUyMCUzRCUyMG1vZGVsKCoqc2VnX2lucHV0cyk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SegformerImageProcessor, SegformerForSemanticSegmentation

processor = SegformerImageProcessor.from_pretrained(<span class="hljs-string">&quot;nvidia/segformer-b0-finetuned-ade-512-512&quot;</span>)
model = SegformerForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;nvidia/segformer-b0-finetuned-ade-512-512&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)
model = torch.<span class="hljs-built_in">compile</span>(model)
seg_inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-keyword">with</span> torch.no_grad():
    _ = model(**seg_inputs)`,wrap:!1}}),O=new f({props:{title:"A100 (batch size: 1)",local:"a100-batch-size-1",headingTag:"h3"}}),A=new f({props:{title:"A100 (batch size: 4)",local:"a100-batch-size-4",headingTag:"h3"}}),Y=new f({props:{title:"A100 (batch size: 16)",local:"a100-batch-size-16",headingTag:"h3"}}),P=new f({props:{title:"V100 (batch size: 1)",local:"v100-batch-size-1",headingTag:"h3"}}),K=new f({props:{title:"V100 (batch size: 4)",local:"v100-batch-size-4",headingTag:"h3"}}),et=new f({props:{title:"V100 (batch size: 16)",local:"v100-batch-size-16",headingTag:"h3"}}),it=new f({props:{title:"T4 (batch size: 1)",local:"t4-batch-size-1",headingTag:"h3"}}),rt=new f({props:{title:"T4 (batch size: 4)",local:"t4-batch-size-4",headingTag:"h3"}}),dt=new f({props:{title:"T4 (batch size: 16)",local:"t4-batch-size-16",headingTag:"h3"}}),gt=new f({props:{title:"PyTorch Nightly",local:"pytorch-nightly",headingTag:"h2"}}),st=new f({props:{title:"A100",local:"a100",headingTag:"h3"}}),ht=new f({props:{title:"T4",local:"t4",headingTag:"h3"}}),Tt=new f({props:{title:"降低开销",local:"降低开销",headingTag:"h2"}}),ut=new f({props:{title:"A100",local:"a100",headingTag:"h3"}}),$t=new f({props:{title:"T4",local:"t4",headingTag:"h3"}}),{c(){b=a("meta"),It=l(),Ct=a("p"),wt=l(),g(u.$$.fragment),Rt=l(),y=a("p"),y.innerHTML=Ne,kt=l(),g($.$$.fragment),Jt=l(),v=a("p"),v.innerHTML=De,Et=l(),C=a("p"),C.innerHTML=He,Ut=l(),g(j.$$.fragment),_t=l(),I=a("p"),I.innerHTML=ze,Zt=l(),w=a("p"),w.innerHTML=We,Bt=l(),g(R.$$.fragment),St=l(),k=a("p"),k.textContent=Ve,Lt=l(),g(J.$$.fragment),Nt=l(),g(E.$$.fragment),Dt=l(),g(U.$$.fragment),Ht=l(),g(_.$$.fragment),zt=l(),g(Z.$$.fragment),Wt=l(),g(B.$$.fragment),Vt=l(),S=a("p"),S.textContent=xe,xt=l(),L=a("p"),L.innerHTML=Xe,Xt=l(),N=a("ul"),N.innerHTML=Oe,Ot=l(),D=a("p"),D.innerHTML=Ge,Gt=l(),H=a("ul"),H.innerHTML=Ae,At=l(),z=a("p"),z.innerHTML=Fe,Ft=l(),W=a("ul"),W.innerHTML=Ye,Yt=l(),T=a("div"),T.innerHTML=qe,qt=l(),M=a("div"),M.innerHTML=Pe,Pt=l(),V=a("p"),V.innerHTML=Qe,Qt=l(),x=a("p"),x.innerHTML=Ke,Kt=l(),X=a("p"),X.innerHTML=tn,te=l(),g(O.$$.fragment),ee=l(),G=a("table"),G.innerHTML=en,ne=l(),g(A.$$.fragment),ie=l(),F=a("table"),F.innerHTML=nn,le=l(),g(Y.$$.fragment),re=l(),q=a("table"),q.innerHTML=ln,ae=l(),g(P.$$.fragment),de=l(),Q=a("table"),Q.innerHTML=rn,ce=l(),g(K.$$.fragment),ge=l(),tt=a("table"),tt.innerHTML=an,oe=l(),g(et.$$.fragment),se=l(),nt=a("table"),nt.innerHTML=dn,me=l(),g(it.$$.fragment),he=l(),lt=a("table"),lt.innerHTML=cn,pe=l(),g(rt.$$.fragment),fe=l(),at=a("table"),at.innerHTML=gn,be=l(),g(dt.$$.fragment),Te=l(),ct=a("table"),ct.innerHTML=on,Me=l(),g(gt.$$.fragment),ue=l(),ot=a("p"),ot.innerHTML=sn,ye=l(),g(st.$$.fragment),$e=l(),mt=a("table"),mt.innerHTML=mn,ve=l(),g(ht.$$.fragment),Ce=l(),pt=a("table"),pt.innerHTML=hn,je=l(),ft=a("p"),ft.textContent=pn,Ie=l(),bt=a("table"),bt.innerHTML=fn,we=l(),g(Tt.$$.fragment),Re=l(),Mt=a("p"),Mt.innerHTML=bn,ke=l(),g(ut.$$.fragment),Je=l(),yt=a("table"),yt.innerHTML=Tn,Ee=l(),g($t.$$.fragment),Ue=l(),vt=a("table"),vt.innerHTML=Mn,_e=l(),jt=a("p"),this.h()},l(t){const e=In("svelte-u9bgzb",document.head);b=d(e,"META",{name:!0,content:!0}),e.forEach(n),It=r(t),Ct=d(t,"P",{}),un(Ct).forEach(n),wt=r(t),o(u.$$.fragment,t),Rt=r(t),y=d(t,"P",{"data-svelte-h":!0}),c(y)!=="svelte-13rra8p"&&(y.innerHTML=Ne),kt=r(t),o($.$$.fragment,t),Jt=r(t),v=d(t,"P",{"data-svelte-h":!0}),c(v)!=="svelte-1v035c2"&&(v.innerHTML=De),Et=r(t),C=d(t,"P",{"data-svelte-h":!0}),c(C)!=="svelte-69z0gw"&&(C.innerHTML=He),Ut=r(t),o(j.$$.fragment,t),_t=r(t),I=d(t,"P",{"data-svelte-h":!0}),c(I)!=="svelte-1ady6mu"&&(I.innerHTML=ze),Zt=r(t),w=d(t,"P",{"data-svelte-h":!0}),c(w)!=="svelte-cqb370"&&(w.innerHTML=We),Bt=r(t),o(R.$$.fragment,t),St=r(t),k=d(t,"P",{"data-svelte-h":!0}),c(k)!=="svelte-1jvd763"&&(k.textContent=Ve),Lt=r(t),o(J.$$.fragment,t),Nt=r(t),o(E.$$.fragment,t),Dt=r(t),o(U.$$.fragment,t),Ht=r(t),o(_.$$.fragment,t),zt=r(t),o(Z.$$.fragment,t),Wt=r(t),o(B.$$.fragment,t),Vt=r(t),S=d(t,"P",{"data-svelte-h":!0}),c(S)!=="svelte-12xtxdc"&&(S.textContent=xe),xt=r(t),L=d(t,"P",{"data-svelte-h":!0}),c(L)!=="svelte-1ugcf6x"&&(L.innerHTML=Xe),Xt=r(t),N=d(t,"UL",{"data-svelte-h":!0}),c(N)!=="svelte-i1xpay"&&(N.innerHTML=Oe),Ot=r(t),D=d(t,"P",{"data-svelte-h":!0}),c(D)!=="svelte-1fqqj48"&&(D.innerHTML=Ge),Gt=r(t),H=d(t,"UL",{"data-svelte-h":!0}),c(H)!=="svelte-1vcuz7e"&&(H.innerHTML=Ae),At=r(t),z=d(t,"P",{"data-svelte-h":!0}),c(z)!=="svelte-1xfnhbz"&&(z.innerHTML=Fe),Ft=r(t),W=d(t,"UL",{"data-svelte-h":!0}),c(W)!=="svelte-rn46wh"&&(W.innerHTML=Ye),Yt=r(t),T=d(t,"DIV",{class:!0,"data-svelte-h":!0}),c(T)!=="svelte-1jw9wmi"&&(T.innerHTML=qe),qt=r(t),M=d(t,"DIV",{class:!0,"data-svelte-h":!0}),c(M)!=="svelte-nlzsqo"&&(M.innerHTML=Pe),Pt=r(t),V=d(t,"P",{"data-svelte-h":!0}),c(V)!=="svelte-gdeipd"&&(V.innerHTML=Qe),Qt=r(t),x=d(t,"P",{"data-svelte-h":!0}),c(x)!=="svelte-1cusdpa"&&(x.innerHTML=Ke),Kt=r(t),X=d(t,"P",{"data-svelte-h":!0}),c(X)!=="svelte-1ju7fpm"&&(X.innerHTML=tn),te=r(t),o(O.$$.fragment,t),ee=r(t),G=d(t,"TABLE",{"data-svelte-h":!0}),c(G)!=="svelte-6uvhqg"&&(G.innerHTML=en),ne=r(t),o(A.$$.fragment,t),ie=r(t),F=d(t,"TABLE",{"data-svelte-h":!0}),c(F)!=="svelte-f4zjoc"&&(F.innerHTML=nn),le=r(t),o(Y.$$.fragment,t),re=r(t),q=d(t,"TABLE",{"data-svelte-h":!0}),c(q)!=="svelte-9ju0ii"&&(q.innerHTML=ln),ae=r(t),o(P.$$.fragment,t),de=r(t),Q=d(t,"TABLE",{"data-svelte-h":!0}),c(Q)!=="svelte-18ncoxq"&&(Q.innerHTML=rn),ce=r(t),o(K.$$.fragment,t),ge=r(t),tt=d(t,"TABLE",{"data-svelte-h":!0}),c(tt)!=="svelte-15udyd3"&&(tt.innerHTML=an),oe=r(t),o(et.$$.fragment,t),se=r(t),nt=d(t,"TABLE",{"data-svelte-h":!0}),c(nt)!=="svelte-rw07j7"&&(nt.innerHTML=dn),me=r(t),o(it.$$.fragment,t),he=r(t),lt=d(t,"TABLE",{"data-svelte-h":!0}),c(lt)!=="svelte-37x5jw"&&(lt.innerHTML=cn),pe=r(t),o(rt.$$.fragment,t),fe=r(t),at=d(t,"TABLE",{"data-svelte-h":!0}),c(at)!=="svelte-1mc5027"&&(at.innerHTML=gn),be=r(t),o(dt.$$.fragment,t),Te=r(t),ct=d(t,"TABLE",{"data-svelte-h":!0}),c(ct)!=="svelte-10eiin7"&&(ct.innerHTML=on),Me=r(t),o(gt.$$.fragment,t),ue=r(t),ot=d(t,"P",{"data-svelte-h":!0}),c(ot)!=="svelte-pkjs63"&&(ot.innerHTML=sn),ye=r(t),o(st.$$.fragment,t),$e=r(t),mt=d(t,"TABLE",{"data-svelte-h":!0}),c(mt)!=="svelte-1cg5nyy"&&(mt.innerHTML=mn),ve=r(t),o(ht.$$.fragment,t),Ce=r(t),pt=d(t,"TABLE",{"data-svelte-h":!0}),c(pt)!=="svelte-1nlzppe"&&(pt.innerHTML=hn),je=r(t),ft=d(t,"P",{"data-svelte-h":!0}),c(ft)!=="svelte-l1xtas"&&(ft.textContent=pn),Ie=r(t),bt=d(t,"TABLE",{"data-svelte-h":!0}),c(bt)!=="svelte-ok1p6e"&&(bt.innerHTML=fn),we=r(t),o(Tt.$$.fragment,t),Re=r(t),Mt=d(t,"P",{"data-svelte-h":!0}),c(Mt)!=="svelte-vw78ma"&&(Mt.innerHTML=bn),ke=r(t),o(ut.$$.fragment,t),Je=r(t),yt=d(t,"TABLE",{"data-svelte-h":!0}),c(yt)!=="svelte-13rnx0"&&(yt.innerHTML=Tn),Ee=r(t),o($t.$$.fragment,t),Ue=r(t),vt=d(t,"TABLE",{"data-svelte-h":!0}),c(vt)!=="svelte-oh2zql"&&(vt.innerHTML=Mn),_e=r(t),jt=d(t,"P",{}),un(jt).forEach(n),this.h()},h(){Be(b,"name","hf:doc:metadata"),Be(b,"content",kn),Be(T,"class","flex"),Be(M,"class","flex")},m(t,e){wn(document.head,b),i(t,It,e),i(t,Ct,e),i(t,wt,e),s(u,t,e),i(t,Rt,e),i(t,y,e),i(t,kt,e),s($,t,e),i(t,Jt,e),i(t,v,e),i(t,Et,e),i(t,C,e),i(t,Ut,e),s(j,t,e),i(t,_t,e),i(t,I,e),i(t,Zt,e),i(t,w,e),i(t,Bt,e),s(R,t,e),i(t,St,e),i(t,k,e),i(t,Lt,e),s(J,t,e),i(t,Nt,e),s(E,t,e),i(t,Dt,e),s(U,t,e),i(t,Ht,e),s(_,t,e),i(t,zt,e),s(Z,t,e),i(t,Wt,e),s(B,t,e),i(t,Vt,e),i(t,S,e),i(t,xt,e),i(t,L,e),i(t,Xt,e),i(t,N,e),i(t,Ot,e),i(t,D,e),i(t,Gt,e),i(t,H,e),i(t,At,e),i(t,z,e),i(t,Ft,e),i(t,W,e),i(t,Yt,e),i(t,T,e),i(t,qt,e),i(t,M,e),i(t,Pt,e),i(t,V,e),i(t,Qt,e),i(t,x,e),i(t,Kt,e),i(t,X,e),i(t,te,e),s(O,t,e),i(t,ee,e),i(t,G,e),i(t,ne,e),s(A,t,e),i(t,ie,e),i(t,F,e),i(t,le,e),s(Y,t,e),i(t,re,e),i(t,q,e),i(t,ae,e),s(P,t,e),i(t,de,e),i(t,Q,e),i(t,ce,e),s(K,t,e),i(t,ge,e),i(t,tt,e),i(t,oe,e),s(et,t,e),i(t,se,e),i(t,nt,e),i(t,me,e),s(it,t,e),i(t,he,e),i(t,lt,e),i(t,pe,e),s(rt,t,e),i(t,fe,e),i(t,at,e),i(t,be,e),s(dt,t,e),i(t,Te,e),i(t,ct,e),i(t,Me,e),s(gt,t,e),i(t,ue,e),i(t,ot,e),i(t,ye,e),s(st,t,e),i(t,$e,e),i(t,mt,e),i(t,ve,e),s(ht,t,e),i(t,Ce,e),i(t,pt,e),i(t,je,e),i(t,ft,e),i(t,Ie,e),i(t,bt,e),i(t,we,e),s(Tt,t,e),i(t,Re,e),i(t,Mt,e),i(t,ke,e),s(ut,t,e),i(t,Je,e),i(t,yt,e),i(t,Ee,e),s($t,t,e),i(t,Ue,e),i(t,vt,e),i(t,_e,e),i(t,jt,e),Ze=!0},p:$n,i(t){Ze||(m(u.$$.fragment,t),m($.$$.fragment,t),m(j.$$.fragment,t),m(R.$$.fragment,t),m(J.$$.fragment,t),m(E.$$.fragment,t),m(U.$$.fragment,t),m(_.$$.fragment,t),m(Z.$$.fragment,t),m(B.$$.fragment,t),m(O.$$.fragment,t),m(A.$$.fragment,t),m(Y.$$.fragment,t),m(P.$$.fragment,t),m(K.$$.fragment,t),m(et.$$.fragment,t),m(it.$$.fragment,t),m(rt.$$.fragment,t),m(dt.$$.fragment,t),m(gt.$$.fragment,t),m(st.$$.fragment,t),m(ht.$$.fragment,t),m(Tt.$$.fragment,t),m(ut.$$.fragment,t),m($t.$$.fragment,t),Ze=!0)},o(t){h(u.$$.fragment,t),h($.$$.fragment,t),h(j.$$.fragment,t),h(R.$$.fragment,t),h(J.$$.fragment,t),h(E.$$.fragment,t),h(U.$$.fragment,t),h(_.$$.fragment,t),h(Z.$$.fragment,t),h(B.$$.fragment,t),h(O.$$.fragment,t),h(A.$$.fragment,t),h(Y.$$.fragment,t),h(P.$$.fragment,t),h(K.$$.fragment,t),h(et.$$.fragment,t),h(it.$$.fragment,t),h(rt.$$.fragment,t),h(dt.$$.fragment,t),h(gt.$$.fragment,t),h(st.$$.fragment,t),h(ht.$$.fragment,t),h(Tt.$$.fragment,t),h(ut.$$.fragment,t),h($t.$$.fragment,t),Ze=!1},d(t){t&&(n(It),n(Ct),n(wt),n(Rt),n(y),n(kt),n(Jt),n(v),n(Et),n(C),n(Ut),n(_t),n(I),n(Zt),n(w),n(Bt),n(St),n(k),n(Lt),n(Nt),n(Dt),n(Ht),n(zt),n(Wt),n(Vt),n(S),n(xt),n(L),n(Xt),n(N),n(Ot),n(D),n(Gt),n(H),n(At),n(z),n(Ft),n(W),n(Yt),n(T),n(qt),n(M),n(Pt),n(V),n(Qt),n(x),n(Kt),n(X),n(te),n(ee),n(G),n(ne),n(ie),n(F),n(le),n(re),n(q),n(ae),n(de),n(Q),n(ce),n(ge),n(tt),n(oe),n(se),n(nt),n(me),n(he),n(lt),n(pe),n(fe),n(at),n(be),n(Te),n(ct),n(Me),n(ue),n(ot),n(ye),n($e),n(mt),n(ve),n(Ce),n(pt),n(je),n(ft),n(Ie),n(bt),n(we),n(Re),n(Mt),n(ke),n(Je),n(yt),n(Ee),n(Ue),n(vt),n(_e),n(jt)),n(b),p(u,t),p($,t),p(j,t),p(R,t),p(J,t),p(E,t),p(U,t),p(_,t),p(Z,t),p(B,t),p(O,t),p(A,t),p(Y,t),p(P,t),p(K,t),p(et,t),p(it,t),p(rt,t),p(dt,t),p(gt,t),p(st,t),p(ht,t),p(Tt,t),p(ut,t),p($t,t)}}}const kn='{"title":"使用 torch.compile() 优化推理","local":"使用-torchcompile-优化推理","sections":[{"title":"torch.compile 的优势","local":"torchcompile-的优势","sections":[],"depth":2},{"title":"基准测试代码","local":"基准测试代码","sections":[{"title":"使用 ViT 进行图像分类","local":"使用-vit-进行图像分类","sections":[{"title":"使用 DETR 进行目标检测","local":"使用-detr-进行目标检测","sections":[],"depth":4},{"title":"使用 Segformer 进行图像分割","local":"使用-segformer-进行图像分割","sections":[],"depth":4}],"depth":3},{"title":"A100 (batch size: 1)","local":"a100-batch-size-1","sections":[],"depth":3},{"title":"A100 (batch size: 4)","local":"a100-batch-size-4","sections":[],"depth":3},{"title":"A100 (batch size: 16)","local":"a100-batch-size-16","sections":[],"depth":3},{"title":"V100 (batch size: 1)","local":"v100-batch-size-1","sections":[],"depth":3},{"title":"V100 (batch size: 4)","local":"v100-batch-size-4","sections":[],"depth":3},{"title":"V100 (batch size: 16)","local":"v100-batch-size-16","sections":[],"depth":3},{"title":"T4 (batch size: 1)","local":"t4-batch-size-1","sections":[],"depth":3},{"title":"T4 (batch size: 4)","local":"t4-batch-size-4","sections":[],"depth":3},{"title":"T4 (batch size: 16)","local":"t4-batch-size-16","sections":[],"depth":3}],"depth":2},{"title":"PyTorch Nightly","local":"pytorch-nightly","sections":[{"title":"A100","local":"a100","sections":[],"depth":3},{"title":"T4","local":"t4","sections":[],"depth":3}],"depth":2},{"title":"降低开销","local":"降低开销","sections":[{"title":"A100","local":"a100","sections":[],"depth":3},{"title":"T4","local":"t4","sections":[],"depth":3}],"depth":2}],"depth":1}';function Jn(Le){return vn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Bn extends Cn{constructor(b){super(),jn(this,b,Jn,Rn,yn,{})}}export{Bn as component};
