import{s as Fe,o as He,n as Ie}from"../chunks/scheduler.9991993c.js";import{S as Qe,i as ze,g as i,s as n,r as o,A as Se,h as p,f as l,c as a,j as ke,u as r,x as m,k as Ge,y as Oe,a as s,v as u,d as c,t as f,w as d}from"../chunks/index.7fc9a5e7.js";import{T as Le}from"../chunks/Tip.9de92fc6.js";import{C as M}from"../chunks/CodeBlock.e11cba92.js";import{H as ft}from"../chunks/Heading.e3de321f.js";function Ye(bt){let b,g="<code>tranformers.onnx</code> 不再进行维护，请如上所述，使用 🤗 Optimum 导出模型。这部分内容将在未来版本中删除。";return{c(){b=i("p"),b.innerHTML=g},l(T){b=p(T,"P",{"data-svelte-h":!0}),m(b)!=="svelte-lsbo4h"&&(b.innerHTML=g)},m(T,ct){s(T,b,ct)},p:Ie,d(T){T&&l(b)}}}function qe(bt){let b,g,T,ct,J,Mt,h,re="在生产环境中部署 🤗 Transformers 模型通常需要或者能够受益于，将模型导出为可在专门的运行时和硬件上加载和执行的序列化格式。",Tt,w,ue="🤗 Optimum 是 Transformers 的扩展，可以通过其 <code>exporters</code> 模块将模型从 PyTorch 或 TensorFlow 导出为 ONNX 及 TFLite 等序列化格式。🤗 Optimum 还提供了一套性能优化工具，可以在目标硬件上以最高效率训练和运行模型。",yt,$,ce='本指南演示了如何使用 🤗 Optimum 将 🤗 Transformers 模型导出为 ONNX。有关将模型导出为 TFLite 的指南，请参考 <a href="tflite">导出为 TFLite 页面</a>。',gt,N,Jt,U,fe='<a href="http://onnx.ai" rel="nofollow">ONNX (Open Neural Network eXchange 开放神经网络交换)</a> 是一个开放的标准，它定义了一组通用的运算符和一种通用的文件格式，用于表示包括 PyTorch 和 TensorFlow 在内的各种框架中的深度学习模型。当一个模型被导出为 ONNX时，这些运算符被用于构建计算图（通常被称为<em>中间表示</em>），该图表示数据在神经网络中的流动。',ht,C,de="通过公开具有标准化运算符和数据类型的图，ONNX使得模型能够轻松在不同深度学习框架间切换。例如，在 PyTorch 中训练的模型可以被导出为 ONNX，然后再导入到 TensorFlow（反之亦然）。",wt,x,be="导出为 ONNX 后，模型可以：",$t,j,Me='<li>通过 <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization" rel="nofollow">图优化（graph optimization）</a> 和 <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization" rel="nofollow">量化（quantization）</a> 等技术进行推理优化。</li> <li>通过 <a href="https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort" rel="nofollow"><code>ORTModelForXXX</code> 类</a> 使用 ONNX Runtime 运行，它同样遵循你熟悉的 Transformers 中的 <code>AutoModel</code> API。</li> <li>使用 <a href="https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines" rel="nofollow">优化推理流水线（pipeline）</a> 运行，其 API 与 🤗 Transformers 中的 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 函数相同。</li>',Nt,X,Te="🤗 Optimum 通过利用配置对象提供对 ONNX 导出的支持。多种模型架构已经有现成的配置对象，并且配置对象也被设计得易于扩展以适用于其他架构。",Ut,V,ye='现有的配置列表请参考 <a href="https://huggingface.co/docs/optimum/exporters/onnx/overview" rel="nofollow">🤗 Optimum 文档</a>。',Ct,_,ge="有两种方式可以将 🤗 Transformers 模型导出为 ONNX，这里我们展示这两种方法：",xt,v,Je="<li>使用 🤗 Optimum 的 CLI（命令行）导出。</li> <li>使用 🤗 Optimum 的 <code>optimum.onnxruntime</code> 模块导出。</li>",jt,Z,Xt,W,he="要将 🤗 Transformers 模型导出为 ONNX，首先需要安装额外的依赖项：",Vt,R,_t,B,we='请参阅 <a href="https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli" rel="nofollow">🤗 Optimum 文档</a> 以查看所有可用参数，或者在命令行中查看帮助：',vt,k,Zt,G,$e="运行以下命令，以从 🤗 Hub 导出模型的检查点（checkpoint），以 <code>distilbert/distilbert-base-uncased-distilled-squad</code> 为例：",Wt,F,Rt,H,Ne="你应该能在日志中看到导出进度以及生成的 <code>model.onnx</code> 文件的保存位置，如下所示：",Bt,I,kt,Q,Ue='上面的示例说明了从 🤗 Hub 导出检查点的过程。导出本地模型时，首先需要确保将模型的权重和分词器文件保存在同一目录（<code>local_path</code>）中。在使用 CLI 时，将 <code>local_path</code> 传递给 <code>model</code> 参数，而不是 🤗 Hub 上的检查点名称，并提供 <code>--task</code> 参数。你可以在 <a href="https://huggingface.co/docs/optimum/exporters/task_manager" rel="nofollow">🤗 Optimum 文档</a>中查看支持的任务列表。如果未提供 <code>task</code> 参数，将默认导出不带特定任务头的模型架构。',Gt,z,Ft,S,Ce='生成的 <code>model.onnx</code> 文件可以在支持 ONNX 标准的 <a href="https://onnx.ai/supported-tools.html#deployModel" rel="nofollow">许多加速引擎（accelerators）</a> 之一上运行。例如，我们可以使用 <a href="https://onnxruntime.ai/" rel="nofollow">ONNX Runtime</a> 加载和运行模型，如下所示：',Ht,O,It,L,xe='从 Hub 导出 TensorFlow 检查点的过程也一样。例如，以下是从 <a href="https://huggingface.co/keras-io" rel="nofollow">Keras 组织</a> 导出纯 TensorFlow 检查点的命令：',Qt,Y,zt,q,St,E,je="除了 CLI 之外，你还可以使用代码将 🤗 Transformers 模型导出为 ONNX，如下所示：",Ot,P,Lt,A,Yt,K,Xe='如果你想要为当前无法导出的模型添加支持，请先检查 <a href="https://huggingface.co/docs/optimum/exporters/onnx/overview" rel="nofollow"><code>optimum.exporters.onnx</code></a> 是否支持该模型，如果不支持，你可以 <a href="https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute" rel="nofollow">直接为 🤗 Optimum 贡献代码</a>。',qt,D,Et,y,Pt,tt,Ve="要使用 <code>tranformers.onnx</code> 将 🤗 Transformers 模型导出为 ONNX，请安装额外的依赖项：",At,et,Kt,lt,_e="将 <code>transformers.onnx</code> 包作为 Python 模块使用，以使用现成的配置导出检查点：",Dt,st,te,nt,ve="以上代码将导出由 <code>--model</code> 参数定义的检查点的 ONNX 图。传入任何 🤗 Hub 上或者存储与本地的检查点。生成的 <code>model.onnx</code> 文件可以在支持 ONNX 标准的众多加速引擎上运行。例如，使用 ONNX Runtime 加载并运行模型，如下所示：",ee,at,le,it,Ze="可以通过查看每个模型的 ONNX 配置来获取所需的输出名（例如 <code>[&quot;last_hidden_state&quot;]</code>）。例如，对于 DistilBERT，可以用以下代码获取输出名称：",se,pt,ne,mt,We="从 Hub 导出 TensorFlow 检查点的过程也一样。导出纯 TensorFlow 检查点的示例代码如下：",ae,ot,ie,rt,Re="要导出本地存储的模型，请将模型的权重和分词器文件保存在同一目录中（例如 <code>local-pt-checkpoint</code>），然后通过将 <code>transformers.onnx</code> 包的 <code>--model</code> 参数指向该目录，将其导出为 ONNX：",pe,ut,me,dt,oe;return J=new ft({props:{title:"导出为 ONNX",local:"导出为-onnx",headingTag:"h1"}}),N=new ft({props:{title:"导出为 ONNX",local:"导出为-onnx",headingTag:"h2"}}),Z=new ft({props:{title:"使用 CLI 将 🤗 Transformers 模型导出为 ONNX",local:"使用-cli-将--transformers-模型导出为-onnx",headingTag:"h3"}}),R=new M({props:{code:"cGlwJTIwaW5zdGFsbCUyMG9wdGltdW0lNUJleHBvcnRlcnMlNUQ=",highlighted:"pip install optimum[exporters]",wrap:!1}}),k=new M({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1oZWxw",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --<span class="hljs-built_in">help</span>',wrap:!1}}),F=new M({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1tb2RlbCUyMGRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC1kaXN0aWxsZWQtc3F1YWQlMjBkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZF9vbm54JTJG",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/',wrap:!1}}),I=new M({props:{code:"VmFsaWRhdGluZyUyME9OTlglMjBtb2RlbCUyMGRpc3RpbGJlcnRfYmFzZV91bmNhc2VkX3NxdWFkX29ubnglMkZtb2RlbC5vbm54Li4uJTBBJTA5LSU1QiVFMiU5QyU5MyU1RCUyME9OTlglMjBtb2RlbCUyMG91dHB1dCUyMG5hbWVzJTIwbWF0Y2glMjByZWZlcmVuY2UlMjBtb2RlbCUyMChzdGFydF9sb2dpdHMlMkMlMjBlbmRfbG9naXRzKSUwQSUwOS0lMjBWYWxpZGF0aW5nJTIwT05OWCUyME1vZGVsJTIwb3V0cHV0JTIwJTIyc3RhcnRfbG9naXRzJTIyJTNBJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMCgyJTJDJTIwMTYpJTIwbWF0Y2hlcyUyMCgyJTJDJTIwMTYpJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMGFsbCUyMHZhbHVlcyUyMGNsb3NlJTIwKGF0b2wlM0ElMjAwLjAwMDEpJTBBJTA5LSUyMFZhbGlkYXRpbmclMjBPTk5YJTIwTW9kZWwlMjBvdXRwdXQlMjAlMjJlbmRfbG9naXRzJTIyJTNBJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMCgyJTJDJTIwMTYpJTIwbWF0Y2hlcyUyMCgyJTJDJTIwMTYpJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMGFsbCUyMHZhbHVlcyUyMGNsb3NlJTIwKGF0b2wlM0ElMjAwLjAwMDEpJTBBVGhlJTIwT05OWCUyMGV4cG9ydCUyMHN1Y2NlZWRlZCUyMGFuZCUyMHRoZSUyMGV4cG9ydGVkJTIwbW9kZWwlMjB3YXMlMjBzYXZlZCUyMGF0JTNBJTIwZGlzdGlsYmVydF9iYXNlX3VuY2FzZWRfc3F1YWRfb25ueA==",highlighted:`Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
	-[✓] ONNX model output names match reference model (start_logits, end_logits)
	- Validating ONNX Model output <span class="hljs-string">&quot;start_logits&quot;</span>:
		-[✓] (2, 16) matches (2, 16)
		-[✓] all values close (atol: 0.0001)
	- Validating ONNX Model output <span class="hljs-string">&quot;end_logits&quot;</span>:
		-[✓] (2, 16) matches (2, 16)
		-[✓] all values close (atol: 0.0001)
The ONNX <span class="hljs-built_in">export</span> succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx`,wrap:!1}}),z=new M({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1tb2RlbCUyMGxvY2FsX3BhdGglMjAtLXRhc2slMjBxdWVzdGlvbi1hbnN3ZXJpbmclMjBkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZF9vbm54JTJG",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/',wrap:!1}}),O=new M({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEFmcm9tJTIwb3B0aW11bS5vbm54cnVudGltZSUyMGltcG9ydCUyME9SVE1vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmclMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZF9vbm54JTIyKSUwQW1vZGVsJTIwJTNEJTIwT1JUTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydF9iYXNlX3VuY2FzZWRfc3F1YWRfb25ueCUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyV2hhdCUyMGFtJTIwSSUyMHVzaW5nJTNGJTIyJTJDJTIwJTIyVXNpbmclMjBEaXN0aWxCRVJUJTIwd2l0aCUyME9OTlglMjBSdW50aW1lISUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert_base_uncased_squad_onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ORTModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert_base_uncased_squad_onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;What am I using?&quot;</span>, <span class="hljs-string">&quot;Using DistilBERT with ONNX Runtime!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)`,wrap:!1}}),Y=new M({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1tb2RlbCUyMGtlcmFzLWlvJTJGdHJhbnNmb3JtZXJzLXFhJTIwZGlzdGlsYmVydF9iYXNlX2Nhc2VkX3NxdWFkX29ubnglMkY=",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/',wrap:!1}}),q=new ft({props:{title:"使用 optimum.onnxruntime 将 🤗 Transformers 模型导出为 ONNX",local:"使用-optimumonnxruntime-将--transformers-模型导出为-onnx",headingTag:"h3"}}),P=new M({props:{code:"ZnJvbSUyMG9wdGltdW0ub25ueHJ1bnRpbWUlMjBpbXBvcnQlMjBPUlRNb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsX2NoZWNrcG9pbnQlMjAlM0QlMjAlMjJkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZCUyMiUwQXNhdmVfZGlyZWN0b3J5JTIwJTNEJTIwJTIyb25ueCUyRiUyMiUwQSUwQSUyMyUyMCVFNCVCQiU4RSUyMHRyYW5zZm9ybWVycyUyMCVFNSU4QSVBMCVFOCVCRCVCRCVFNiVBOCVBMSVFNSU5RSU4QiVFNSVCOSVCNiVFNSVCMCU4NiVFNSU4NSVCNiVFNSVBRiVCQyVFNSU4NyVCQSVFNCVCOCVCQSUyME9OTlglMEFvcnRfbW9kZWwlMjAlM0QlMjBPUlRNb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2NoZWNrcG9pbnQlMkMlMjBleHBvcnQlM0RUcnVlKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2NoZWNrcG9pbnQpJTBBJTBBJTIzJTIwJUU0JUJGJTlEJUU1JUFEJTk4JTIwb25ueCUyMCVFNiVBOCVBMSVFNSU5RSU4QiVFNCVCQiVBNSVFNSU4RiU4QSVFNSU4OCU4NiVFOCVBRiU4RCVFNSU5OSVBOCUwQW9ydF9tb2RlbC5zYXZlX3ByZXRyYWluZWQoc2F2ZV9kaXJlY3RvcnkpJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZChzYXZlX2RpcmVjdG9yeSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_checkpoint = <span class="hljs-string">&quot;distilbert_base_uncased_squad&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>save_directory = <span class="hljs-string">&quot;onnx/&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 从 transformers 加载模型并将其导出为 ONNX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 保存 onnx 模型以及分词器</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model.save_pretrained(save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(save_directory)`,wrap:!1}}),A=new ft({props:{title:"导出尚未支持的架构的模型",local:"导出尚未支持的架构的模型",headingTag:"h3"}}),D=new ft({props:{title:"使用 transformers.onnx 导出模型",local:"使用-transformersonnx-导出模型",headingTag:"h3"}}),y=new Le({props:{warning:!0,$$slots:{default:[Ye]},$$scope:{ctx:bt}}}),et=new M({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyU1Qm9ubnglNUQ=",highlighted:"pip install transformers[onnx]",wrap:!1}}),st=new M({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0RkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjBvbm54JTJG",highlighted:"python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/",wrap:!1}}),at=new M({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEFmcm9tJTIwb25ueHJ1bnRpbWUlMjBpbXBvcnQlMjBJbmZlcmVuY2VTZXNzaW9uJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydCUyRmRpc3RpbGJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQXNlc3Npb24lMjAlM0QlMjBJbmZlcmVuY2VTZXNzaW9uKCUyMm9ubnglMkZtb2RlbC5vbm54JTIyKSUwQSUyMyUyME9OTlglMjBSdW50aW1lJTIwZXhwZWN0cyUyME51bVB5JTIwYXJyYXlzJTIwYXMlMjBpbnB1dCUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJVc2luZyUyMERpc3RpbEJFUlQlMjB3aXRoJTIwT05OWCUyMFJ1bnRpbWUhJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJucCUyMiklMEFvdXRwdXRzJTIwJTNEJTIwc2Vzc2lvbi5ydW4ob3V0cHV0X25hbWVzJTNEJTVCJTIybGFzdF9oaWRkZW5fc3RhdGUlMjIlNUQlMkMlMjBpbnB1dF9mZWVkJTNEZGljdChpbnB1dHMpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> onnxruntime <span class="hljs-keyword">import</span> InferenceSession

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>session = InferenceSession(<span class="hljs-string">&quot;onnx/model.onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># ONNX Runtime expects NumPy arrays as input</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Using DistilBERT with ONNX Runtime!&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = session.run(output_names=[<span class="hljs-string">&quot;last_hidden_state&quot;</span>], input_feed=<span class="hljs-built_in">dict</span>(inputs))`,wrap:!1}}),pt=new M({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5tb2RlbHMuZGlzdGlsYmVydCUyMGltcG9ydCUyMERpc3RpbEJlcnRDb25maWclMkMlMjBEaXN0aWxCZXJ0T25ueENvbmZpZyUwQSUwQWNvbmZpZyUyMCUzRCUyMERpc3RpbEJlcnRDb25maWcoKSUwQW9ubnhfY29uZmlnJTIwJTNEJTIwRGlzdGlsQmVydE9ubnhDb25maWcoY29uZmlnKSUwQXByaW50KGxpc3Qob25ueF9jb25maWcub3V0cHV0cy5rZXlzKCkpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.models.distilbert <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertOnnxConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>config = DistilBertConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_config = DistilBertOnnxConfig(config)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(onnx_config.outputs.keys()))
[<span class="hljs-string">&quot;last_hidden_state&quot;</span>]`,wrap:!1}}),ot=new M({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0RrZXJhcy1pbyUyRnRyYW5zZm9ybWVycy1xYSUyMG9ubnglMkY=",highlighted:"python -m transformers.onnx --model=keras-io/transformers-qa onnx/",wrap:!1}}),ut=new M({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0Rsb2NhbC1wdC1jaGVja3BvaW50JTIwb25ueCUyRg==",highlighted:"python -m transformers.onnx --model=local-pt-checkpoint onnx/",wrap:!1}}),{c(){b=i("meta"),g=n(),T=i("p"),ct=n(),o(J.$$.fragment),Mt=n(),h=i("p"),h.textContent=re,Tt=n(),w=i("p"),w.innerHTML=ue,yt=n(),$=i("p"),$.innerHTML=ce,gt=n(),o(N.$$.fragment),Jt=n(),U=i("p"),U.innerHTML=fe,ht=n(),C=i("p"),C.textContent=de,wt=n(),x=i("p"),x.textContent=be,$t=n(),j=i("ul"),j.innerHTML=Me,Nt=n(),X=i("p"),X.textContent=Te,Ut=n(),V=i("p"),V.innerHTML=ye,Ct=n(),_=i("p"),_.textContent=ge,xt=n(),v=i("ul"),v.innerHTML=Je,jt=n(),o(Z.$$.fragment),Xt=n(),W=i("p"),W.textContent=he,Vt=n(),o(R.$$.fragment),_t=n(),B=i("p"),B.innerHTML=we,vt=n(),o(k.$$.fragment),Zt=n(),G=i("p"),G.innerHTML=$e,Wt=n(),o(F.$$.fragment),Rt=n(),H=i("p"),H.innerHTML=Ne,Bt=n(),o(I.$$.fragment),kt=n(),Q=i("p"),Q.innerHTML=Ue,Gt=n(),o(z.$$.fragment),Ft=n(),S=i("p"),S.innerHTML=Ce,Ht=n(),o(O.$$.fragment),It=n(),L=i("p"),L.innerHTML=xe,Qt=n(),o(Y.$$.fragment),zt=n(),o(q.$$.fragment),St=n(),E=i("p"),E.textContent=je,Ot=n(),o(P.$$.fragment),Lt=n(),o(A.$$.fragment),Yt=n(),K=i("p"),K.innerHTML=Xe,qt=n(),o(D.$$.fragment),Et=n(),o(y.$$.fragment),Pt=n(),tt=i("p"),tt.innerHTML=Ve,At=n(),o(et.$$.fragment),Kt=n(),lt=i("p"),lt.innerHTML=_e,Dt=n(),o(st.$$.fragment),te=n(),nt=i("p"),nt.innerHTML=ve,ee=n(),o(at.$$.fragment),le=n(),it=i("p"),it.innerHTML=Ze,se=n(),o(pt.$$.fragment),ne=n(),mt=i("p"),mt.textContent=We,ae=n(),o(ot.$$.fragment),ie=n(),rt=i("p"),rt.innerHTML=Re,pe=n(),o(ut.$$.fragment),me=n(),dt=i("p"),this.h()},l(t){const e=Se("svelte-u9bgzb",document.head);b=p(e,"META",{name:!0,content:!0}),e.forEach(l),g=a(t),T=p(t,"P",{}),ke(T).forEach(l),ct=a(t),r(J.$$.fragment,t),Mt=a(t),h=p(t,"P",{"data-svelte-h":!0}),m(h)!=="svelte-1nrzlis"&&(h.textContent=re),Tt=a(t),w=p(t,"P",{"data-svelte-h":!0}),m(w)!=="svelte-mov0vr"&&(w.innerHTML=ue),yt=a(t),$=p(t,"P",{"data-svelte-h":!0}),m($)!=="svelte-xncyne"&&($.innerHTML=ce),gt=a(t),r(N.$$.fragment,t),Jt=a(t),U=p(t,"P",{"data-svelte-h":!0}),m(U)!=="svelte-skqm3v"&&(U.innerHTML=fe),ht=a(t),C=p(t,"P",{"data-svelte-h":!0}),m(C)!=="svelte-16jc19n"&&(C.textContent=de),wt=a(t),x=p(t,"P",{"data-svelte-h":!0}),m(x)!=="svelte-1aux7gh"&&(x.textContent=be),$t=a(t),j=p(t,"UL",{"data-svelte-h":!0}),m(j)!=="svelte-1ra7jnx"&&(j.innerHTML=Me),Nt=a(t),X=p(t,"P",{"data-svelte-h":!0}),m(X)!=="svelte-1i7w9pz"&&(X.textContent=Te),Ut=a(t),V=p(t,"P",{"data-svelte-h":!0}),m(V)!=="svelte-1yienai"&&(V.innerHTML=ye),Ct=a(t),_=p(t,"P",{"data-svelte-h":!0}),m(_)!=="svelte-tbpqij"&&(_.textContent=ge),xt=a(t),v=p(t,"UL",{"data-svelte-h":!0}),m(v)!=="svelte-keywtz"&&(v.innerHTML=Je),jt=a(t),r(Z.$$.fragment,t),Xt=a(t),W=p(t,"P",{"data-svelte-h":!0}),m(W)!=="svelte-1umwk9p"&&(W.textContent=he),Vt=a(t),r(R.$$.fragment,t),_t=a(t),B=p(t,"P",{"data-svelte-h":!0}),m(B)!=="svelte-1rzqdp2"&&(B.innerHTML=we),vt=a(t),r(k.$$.fragment,t),Zt=a(t),G=p(t,"P",{"data-svelte-h":!0}),m(G)!=="svelte-qrhowj"&&(G.innerHTML=$e),Wt=a(t),r(F.$$.fragment,t),Rt=a(t),H=p(t,"P",{"data-svelte-h":!0}),m(H)!=="svelte-1t36nwo"&&(H.innerHTML=Ne),Bt=a(t),r(I.$$.fragment,t),kt=a(t),Q=p(t,"P",{"data-svelte-h":!0}),m(Q)!=="svelte-74uhn9"&&(Q.innerHTML=Ue),Gt=a(t),r(z.$$.fragment,t),Ft=a(t),S=p(t,"P",{"data-svelte-h":!0}),m(S)!=="svelte-136n2jd"&&(S.innerHTML=Ce),Ht=a(t),r(O.$$.fragment,t),It=a(t),L=p(t,"P",{"data-svelte-h":!0}),m(L)!=="svelte-60062m"&&(L.innerHTML=xe),Qt=a(t),r(Y.$$.fragment,t),zt=a(t),r(q.$$.fragment,t),St=a(t),E=p(t,"P",{"data-svelte-h":!0}),m(E)!=="svelte-fkxwzd"&&(E.textContent=je),Ot=a(t),r(P.$$.fragment,t),Lt=a(t),r(A.$$.fragment,t),Yt=a(t),K=p(t,"P",{"data-svelte-h":!0}),m(K)!=="svelte-1quqb9i"&&(K.innerHTML=Xe),qt=a(t),r(D.$$.fragment,t),Et=a(t),r(y.$$.fragment,t),Pt=a(t),tt=p(t,"P",{"data-svelte-h":!0}),m(tt)!=="svelte-4do5e5"&&(tt.innerHTML=Ve),At=a(t),r(et.$$.fragment,t),Kt=a(t),lt=p(t,"P",{"data-svelte-h":!0}),m(lt)!=="svelte-od71qv"&&(lt.innerHTML=_e),Dt=a(t),r(st.$$.fragment,t),te=a(t),nt=p(t,"P",{"data-svelte-h":!0}),m(nt)!=="svelte-541la3"&&(nt.innerHTML=ve),ee=a(t),r(at.$$.fragment,t),le=a(t),it=p(t,"P",{"data-svelte-h":!0}),m(it)!=="svelte-1a8l9fp"&&(it.innerHTML=Ze),se=a(t),r(pt.$$.fragment,t),ne=a(t),mt=p(t,"P",{"data-svelte-h":!0}),m(mt)!=="svelte-48qrdk"&&(mt.textContent=We),ae=a(t),r(ot.$$.fragment,t),ie=a(t),rt=p(t,"P",{"data-svelte-h":!0}),m(rt)!=="svelte-jrjkwc"&&(rt.innerHTML=Re),pe=a(t),r(ut.$$.fragment,t),me=a(t),dt=p(t,"P",{}),ke(dt).forEach(l),this.h()},h(){Ge(b,"name","hf:doc:metadata"),Ge(b,"content",Ee)},m(t,e){Oe(document.head,b),s(t,g,e),s(t,T,e),s(t,ct,e),u(J,t,e),s(t,Mt,e),s(t,h,e),s(t,Tt,e),s(t,w,e),s(t,yt,e),s(t,$,e),s(t,gt,e),u(N,t,e),s(t,Jt,e),s(t,U,e),s(t,ht,e),s(t,C,e),s(t,wt,e),s(t,x,e),s(t,$t,e),s(t,j,e),s(t,Nt,e),s(t,X,e),s(t,Ut,e),s(t,V,e),s(t,Ct,e),s(t,_,e),s(t,xt,e),s(t,v,e),s(t,jt,e),u(Z,t,e),s(t,Xt,e),s(t,W,e),s(t,Vt,e),u(R,t,e),s(t,_t,e),s(t,B,e),s(t,vt,e),u(k,t,e),s(t,Zt,e),s(t,G,e),s(t,Wt,e),u(F,t,e),s(t,Rt,e),s(t,H,e),s(t,Bt,e),u(I,t,e),s(t,kt,e),s(t,Q,e),s(t,Gt,e),u(z,t,e),s(t,Ft,e),s(t,S,e),s(t,Ht,e),u(O,t,e),s(t,It,e),s(t,L,e),s(t,Qt,e),u(Y,t,e),s(t,zt,e),u(q,t,e),s(t,St,e),s(t,E,e),s(t,Ot,e),u(P,t,e),s(t,Lt,e),u(A,t,e),s(t,Yt,e),s(t,K,e),s(t,qt,e),u(D,t,e),s(t,Et,e),u(y,t,e),s(t,Pt,e),s(t,tt,e),s(t,At,e),u(et,t,e),s(t,Kt,e),s(t,lt,e),s(t,Dt,e),u(st,t,e),s(t,te,e),s(t,nt,e),s(t,ee,e),u(at,t,e),s(t,le,e),s(t,it,e),s(t,se,e),u(pt,t,e),s(t,ne,e),s(t,mt,e),s(t,ae,e),u(ot,t,e),s(t,ie,e),s(t,rt,e),s(t,pe,e),u(ut,t,e),s(t,me,e),s(t,dt,e),oe=!0},p(t,[e]){const Be={};e&2&&(Be.$$scope={dirty:e,ctx:t}),y.$set(Be)},i(t){oe||(c(J.$$.fragment,t),c(N.$$.fragment,t),c(Z.$$.fragment,t),c(R.$$.fragment,t),c(k.$$.fragment,t),c(F.$$.fragment,t),c(I.$$.fragment,t),c(z.$$.fragment,t),c(O.$$.fragment,t),c(Y.$$.fragment,t),c(q.$$.fragment,t),c(P.$$.fragment,t),c(A.$$.fragment,t),c(D.$$.fragment,t),c(y.$$.fragment,t),c(et.$$.fragment,t),c(st.$$.fragment,t),c(at.$$.fragment,t),c(pt.$$.fragment,t),c(ot.$$.fragment,t),c(ut.$$.fragment,t),oe=!0)},o(t){f(J.$$.fragment,t),f(N.$$.fragment,t),f(Z.$$.fragment,t),f(R.$$.fragment,t),f(k.$$.fragment,t),f(F.$$.fragment,t),f(I.$$.fragment,t),f(z.$$.fragment,t),f(O.$$.fragment,t),f(Y.$$.fragment,t),f(q.$$.fragment,t),f(P.$$.fragment,t),f(A.$$.fragment,t),f(D.$$.fragment,t),f(y.$$.fragment,t),f(et.$$.fragment,t),f(st.$$.fragment,t),f(at.$$.fragment,t),f(pt.$$.fragment,t),f(ot.$$.fragment,t),f(ut.$$.fragment,t),oe=!1},d(t){t&&(l(g),l(T),l(ct),l(Mt),l(h),l(Tt),l(w),l(yt),l($),l(gt),l(Jt),l(U),l(ht),l(C),l(wt),l(x),l($t),l(j),l(Nt),l(X),l(Ut),l(V),l(Ct),l(_),l(xt),l(v),l(jt),l(Xt),l(W),l(Vt),l(_t),l(B),l(vt),l(Zt),l(G),l(Wt),l(Rt),l(H),l(Bt),l(kt),l(Q),l(Gt),l(Ft),l(S),l(Ht),l(It),l(L),l(Qt),l(zt),l(St),l(E),l(Ot),l(Lt),l(Yt),l(K),l(qt),l(Et),l(Pt),l(tt),l(At),l(Kt),l(lt),l(Dt),l(te),l(nt),l(ee),l(le),l(it),l(se),l(ne),l(mt),l(ae),l(ie),l(rt),l(pe),l(me),l(dt)),l(b),d(J,t),d(N,t),d(Z,t),d(R,t),d(k,t),d(F,t),d(I,t),d(z,t),d(O,t),d(Y,t),d(q,t),d(P,t),d(A,t),d(D,t),d(y,t),d(et,t),d(st,t),d(at,t),d(pt,t),d(ot,t),d(ut,t)}}}const Ee='{"title":"导出为 ONNX","local":"导出为-onnx","sections":[{"title":"导出为 ONNX","local":"导出为-onnx","sections":[{"title":"使用 CLI 将 🤗 Transformers 模型导出为 ONNX","local":"使用-cli-将--transformers-模型导出为-onnx","sections":[],"depth":3},{"title":"使用 optimum.onnxruntime 将 🤗 Transformers 模型导出为 ONNX","local":"使用-optimumonnxruntime-将--transformers-模型导出为-onnx","sections":[],"depth":3},{"title":"导出尚未支持的架构的模型","local":"导出尚未支持的架构的模型","sections":[],"depth":3},{"title":"使用 transformers.onnx 导出模型","local":"使用-transformersonnx-导出模型","sections":[],"depth":3}],"depth":2}],"depth":1}';function Pe(bt){return He(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ll extends Qe{constructor(b){super(),ze(this,b,Pe,qe,Fe,{})}}export{ll as component};
