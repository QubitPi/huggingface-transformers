import{s as We,n as Ne,o as Qe}from"../chunks/scheduler.9991993c.js";import{S as Xe,i as Ke,g as p,s as a,r as i,m as ee,H as Re,A as Ye,h as o,f as e,c as l,j as ws,u as c,x as u,n as ne,E as Fe,k as _s,y as Hs,a as n,v as r,d as m,t as q,w as h}from"../chunks/index.7fc9a5e7.js";import{Y as Ft}from"../chunks/Youtube.7934cf81.js";import{C as y}from"../chunks/CodeBlock.e11cba92.js";import{D as Oe}from"../chunks/DocNotebookDropdown.a0cb4c0f.js";import{H as d}from"../chunks/Heading.e3de321f.js";function sn(ae){let M,Es,Ls,Bs,j,As,T,Ss,f,le="在这个页面，我们来仔细研究分词的知识。",Ds,J,Gs,$,pe=`正如我们在<a href="preprocessing">the preprocessing tutorial</a>所看到的那样，对文本进行分词就是将一段文本分割成很多单词或者子单词，
这些单词或者子单词然后会通过一个查询表格被转换到id，将单词或者子单词转换到id是很直截了当的，也就是一个简单的映射，
所以这么来看，我们主要关注将一段文本分割成很多单词或者很多子单词（像：对一段文本进行分词），更加准确的来说，我们将关注
在🤗 Transformers内用到的三种主要类型的分词器：<a href="#byte-pair-encoding">Byte-Pair Encoding (BPE)</a>, <a href="#wordpiece">WordPiece</a>,
and <a href="#sentencepiece">SentencePiece</a>，并且给出了示例，哪个模型用到了哪种类型的分词器。`,Zs,b,oe=`注意到在每个模型的主页，你可以查看文档上相关的分词器，就可以知道预训练模型使用了哪种类型的分词器。
举个例子，如果我们查看<code>BertTokenizer</code>，我们就能看到模型使用了<a href="#wordpiece">WordPiece</a>。`,Vs,v,Ws,w,ue=`将一段文本分词到小块是一个比它看起来更加困难的任务，并且有很多方式来实现分词，举个例子，让我们看看这个句子
<code>&quot;Don&#39;t you love 🤗 Transformers? We sure do.&quot;</code>`,Ns,I,Qs,U,ie="对这段文本分词的一个简单方式，就是使用空格来分词，得到的结果是：",Xs,k,Ks,x,ce=`上面的分词是一个明智的开始，但是如果我们查看token <code>&quot;Transformers?&quot;</code> 和 <code>&quot;do.&quot;</code>，我们可以观察到标点符号附在单词<code>&quot;Transformer&quot;</code>
和 <code>&quot;do&quot;</code>的后面，这并不是最理想的情况。我们应该将标点符号考虑进来，这样一个模型就没必要学习一个单词和每个可能跟在后面的
标点符号的不同的组合，这么组合的话，模型需要学习的组合的数量会急剧上升。将标点符号也考虑进来，对范例文本进行分词的结果就是：`,Rs,C,Ys,P,re=`分词的结果更好了，然而，这么做也是不好的，分词怎么处理单词<code>&quot;Don&#39;t&quot;</code>，<code>&quot;Don&#39;t&quot;</code>的含义是<code>&quot;do not&quot;</code>，所以这么分词<code>[&quot;Do&quot;, &quot;n&#39;t&quot;]</code>
会更好。现在开始事情就开始变得复杂起来了，部分的原因是每个模型都有它自己的分词类型。依赖于我们应用在文本分词上的规则，
相同的文本会产生不同的分词输出。用在训练数据上的分词规则，被用来对输入做分词操作，一个预训练模型才会正确的执行。`,Fs,_,me=`<a href="https://spacy.io/" rel="nofollow">spaCy</a> and <a href="http://www.statmt.org/moses/?n=Development.GetStarted" rel="nofollow">Moses</a> 是两个受欢迎的基于规则的
分词器。将这两个分词器应用在示例文本上，<em>spaCy</em> 和 <em>Moses</em>会输出类似下面的结果：`,Os,H,st,L,qe=`可见上面的分词使用到了空格和标点符号的分词方式，以及基于规则的分词方式。空格和标点符号分词以及基于规则的分词都是单词分词的例子。
不那么严格的来说，单词分词的定义就是将句子分割到很多单词。然而将文本分割到更小的块是符合直觉的，当处理大型文本语料库时，上面的
分词方法会导致很多问题。在这种情况下，空格和标点符号分词通常会产生一个非常大的词典（使用到的所有不重复的单词和tokens的集合）。
像：<a href="model_doc/transformerxl">Transformer XL</a>使用空格和标点符号分词，结果会产生一个大小是267,735的词典！`,tt,z,he=`这么大的一个词典容量，迫使模型有着一个巨大的embedding矩阵，以及巨大的输入和输出层，这会增加内存使用量，也会提高时间复杂度。通常
情况下，transformers模型几乎没有词典容量大于50,000的，特别是只在一种语言上预训练的模型。`,et,E,ge="所以如果简单的空格和标点符号分词让人不满意，为什么不简单的对字符分词？",nt,B,at,A,ye=`尽管字符分词是非常简单的，并且能极大的减少内存使用，降低时间复杂度，但是这样做会让模型很难学到有意义的输入表达。像：
比起学到单词<code>&quot;today&quot;</code>的一个有意义的上下文独立的表达，学到字母<code>&quot;t&quot;</code>的一个有意义的上下文独立的表达是相当困难的。因此，
字符分词经常会伴随着性能的下降。所以为了获得最好的结果，transformers模型在单词级别分词和字符级别分词之间使用了一个折中的方案
被称作<strong>子词</strong>分词。`,lt,S,pt,D,ot,G,Me=`子词分词算法依赖这样的原则：频繁使用的单词不应该被分割成更小的子词，但是很少使用的单词应该被分解到有意义的子词。举个例子：
<code>&quot;annoyingly&quot;</code>能被看作一个很少使用的单词，能被分解成<code>&quot;annoying&quot;</code>和<code>&quot;ly&quot;</code>。<code>&quot;annoying&quot;</code>和<code>&quot;ly&quot;</code>作为独立地子词，出现
的次数都很频繁，而且与此同时单词<code>&quot;annoyingly&quot;</code>的含义可以通过组合<code>&quot;annoying&quot;</code>和<code>&quot;ly&quot;</code>的含义来获得。在粘合和胶水语言上，
像Turkish语言，这么做是相当有用的，在这样的语言里，通过线性组合子词，大多数情况下你能形成任意长的复杂的单词。`,ut,Z,de=`子词分词允许模型有一个合理的词典大小，而且能学到有意义的上下文独立地表达。除此以外，子词分词可以让模型处理以前从来没见过的单词，
方式是通过分解这些单词到已知的子词，举个例子：<code>BertTokenizer</code>对句子<code>&quot;I have a new GPU!&quot;</code>分词的结果如下：`,it,V,ct,W,je=`因为我们正在考虑不区分大小写的模型，句子首先被转换成小写字母形式。我们可以见到单词<code>[&quot;i&quot;, &quot;have&quot;, &quot;a&quot;, &quot;new&quot;]</code>在分词器
的词典内，但是这个单词<code>&quot;gpu&quot;</code>不在词典内。所以，分词器将<code>&quot;gpu&quot;</code>分割成已知的子词<code>[&quot;gp&quot; and &quot;##u&quot;]</code>。<code>&quot;##&quot;</code>意味着剩下的
token应该附着在前面那个token的后面，不带空格的附着（分词的解码或者反向）。`,rt,N,Te="另外一个例子，<code>XLNetTokenizer</code>对前面的文本例子分词结果如下：",mt,Q,qt,X,fe=`当我们查看<a href="#sentencepiece">SentencePiece</a>时会回过头来解释这些<code>&quot;▁&quot;</code>符号的含义。正如你能见到的，很少使用的单词
<code>&quot;Transformers&quot;</code>能被分割到更加频繁使用的子词<code>&quot;Transform&quot;</code>和<code>&quot;ers&quot;</code>。`,ht,K,Je=`现在让我们来看看不同的子词分割算法是怎么工作的，注意到所有的这些分词算法依赖于某些训练的方式，这些训练通常在语料库上完成，
相应的模型也是在这个语料库上训练的。`,gt,Is,yt,R,Mt,Y,$e=`Byte-Pair Encoding (BPE)来自于<a href="https://arxiv.org/abs/1508.07909" rel="nofollow">Neural Machine Translation of Rare Words with Subword Units (Sennrich et
al., 2015)</a>。BPE依赖于一个预分词器，这个预分词器会将训练数据分割成单词。预分词可以是简单的
空格分词，像：：<a href="model_doc/gpt2">GPT-2</a>，<a href="model_doc/roberta">RoBERTa</a>。更加先进的预分词方式包括了基于规则的分词，像： <a href="model_doc/xlm">XLM</a>，<a href="model_doc/flaubert">FlauBERT</a>，FlauBERT在大多数语言使用了Moses，或者<a href="model_doc/gpt">GPT</a>，GPT
使用了Spacy和ftfy，统计了训练语料库中每个单词的频次。`,dt,F,be=`在预分词以后，生成了单词的集合，也确定了训练数据中每个单词出现的频次。下一步，BPE产生了一个基础词典，包含了集合中所有的符号，
BPE学习融合的规则-组合基础词典中的两个符号来形成一个新的符号。BPE会一直学习直到词典的大小满足了期望的词典大小的要求。注意到
期望的词典大小是一个超参数，在训练这个分词器以前就需要人为指定。`,jt,O,ve="举个例子，让我们假设在预分词以后，下面的单词集合以及他们的频次都已经确定好了：",Tt,ss,ft,ts,we="所以，基础的词典是<code>[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;]</code>。将所有单词分割成基础词典内的符号，就可以获得：",Jt,es,$t,ns,Ie=`BPE接着会统计每个可能的符号对的频次，然后挑出出现最频繁的的符号对，在上面的例子中，<code>&quot;h&quot;</code>跟了<code>&quot;u&quot;</code>出现了10 + 5 = 15次
（10次是出现了10次<code>&quot;hug&quot;</code>，5次是出现了5次<code>&quot;hugs&quot;</code>）。然而，最频繁的符号对是<code>&quot;u&quot;</code>后面跟了个<code>&quot;g&quot;</code>，总共出现了10 + 5 + 5
= 20次。因此，分词器学到的第一个融合规则是组合所有的<code>&quot;u&quot;</code>后面跟了个<code>&quot;g&quot;</code>符号。下一步，<code>&quot;ug&quot;</code>被加入到了词典内。单词的集合
就变成了：`,bt,as,vt,ls,Ue=`BPE接着会统计出下一个最普遍的出现频次最大的符号对。也就是<code>&quot;u&quot;</code>后面跟了个<code>&quot;n&quot;</code>，出现了16次。<code>&quot;u&quot;</code>，<code>&quot;n&quot;</code>被融合成了<code>&quot;un&quot;</code>。
也被加入到了词典中，再下一个出现频次最大的符号对是<code>&quot;h&quot;</code>后面跟了个<code>&quot;ug&quot;</code>，出现了15次。又一次这个符号对被融合成了<code>&quot;hug&quot;</code>，
也被加入到了词典中。`,wt,ps,ke="在当前这步，词典是<code>[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;]</code>，我们的单词集合则是：",It,os,Ut,us,xe=`假设，the Byte-Pair Encoding在这个时候停止训练，学到的融合规则并应用到其他新的单词上（只要这些新单词不包括不在基础词典内的符号
就行）。举个例子，单词<code>&quot;bug&quot;</code>会被分词到<code>[&quot;b&quot;, &quot;ug&quot;]</code>，但是<code>&quot;mug&quot;</code>会被分词到<code>[&quot;&lt;unk&gt;&quot;, &quot;ug&quot;]</code>，因为符号<code>&quot;m&quot;</code>不在基础词典内。
通常来看的话，单个字母像<code>&quot;m&quot;</code>不会被<code>&quot;&lt;unk&gt;&quot;</code>符号替换掉，因为训练数据通常包括了每个字母，每个字母至少出现了一次，但是在特殊的符号
中也可能发生像emojis。`,kt,is,Ce=`就像之前提到的那样，词典的大小，举个例子，基础词典的大小 + 融合的数量，是一个需要配置的超参数。举个例子：<a href="model_doc/gpt">GPT</a>
的词典大小是40,478，因为GPT有着478个基础词典内的字符，在40,000次融合以后选择了停止训练。`,xt,cs,Ct,rs,Pe=`一个包含了所有可能的基础字符的基础字典可能会非常大，如果考虑将所有的unicode字符作为基础字符。为了拥有一个更好的基础词典，<a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow">GPT-2</a>使用了字节
作为基础词典，这是一个非常聪明的技巧，迫使基础词典是256大小，而且确保了所有基础字符包含在这个词典内。使用了其他的规则
来处理标点符号，这个GPT2的分词器能对每个文本进行分词，不需要使用到&lt;unk&gt;符号。<a href="model_doc/gpt">GPT-2</a>有一个大小是50,257
的词典，对应到256字节的基础tokens，一个特殊的文本结束token，这些符号经过了50,000次融合学习。`,Pt,Us,_t,ms,Ht,qs,_e=`WordPiece是子词分词算法，被用在<a href="model_doc/bert">BERT</a>，<a href="model_doc/distilbert">DistilBERT</a>，和<a href="model_doc/electra">Electra</a>。
这个算法发布在<a href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf" rel="nofollow">Japanese and Korean
Voice Search (Schuster et al., 2012)</a>
和BPE非常相似。WordPiece首先初始化一个词典，这个词典包含了出现在训练数据中的每个字符，然后递进的学习一个给定数量的融合规则。和BPE相比较，
WordPiece不会选择出现频次最大的符号对，而是选择了加入到字典以后能最大化训练数据似然值的符号对。`,Lt,hs,He=`所以这到底意味着什么？参考前面的例子，最大化训练数据的似然值，等价于找到一个符号对，它们的概率除以这个符号对中第一个符号的概率，
接着除以第二个符号的概率，在所有的符号对中商最大。像：如果<code>&quot;ug&quot;</code>的概率除以<code>&quot;u&quot;</code>除以<code>&quot;g&quot;</code>的概率的商，比其他任何符号对更大，
这个时候才能融合<code>&quot;u&quot;</code>和<code>&quot;g&quot;</code>。直觉上，WordPiece，和BPE有点点不同，WordPiece是评估融合两个符号会失去的量，来确保这么做是值得的。`,zt,ks,Et,gs,Bt,ys,Le=`Unigram是一个子词分词器算法，介绍见<a href="https://arxiv.org/pdf/1804.10959.pdf" rel="nofollow">Subword Regularization: Improving Neural Network Translation
Models with Multiple Subword Candidates (Kudo, 2018)</a>。和BPE或者WordPiece相比较
，Unigram使用大量的符号来初始化它的基础字典，然后逐渐的精简每个符号来获得一个更小的词典。举例来看基础词典能够对应所有的预分词
的单词以及最常见的子字符串。Unigram没有直接用在任何transformers的任何模型中，但是和<a href="#sentencepiece">SentencePiece</a>一起联合使用。`,At,Ms,ze=`在每个训练的步骤，Unigram算法在当前词典的训练数据上定义了一个损失函数（经常定义为log似然函数的），还定义了一个unigram语言模型。
然后，对词典内的每个符号，算法会计算如果这个符号从词典内移除，总的损失会升高多少。Unigram然后会移除百分之p的符号，这些符号的loss
升高是最低的（p通常是10%或者20%），像：这些在训练数据上对总的损失影响最小的符号。重复这个过程，直到词典已经达到了期望的大小。
为了任何单词都能被分词，Unigram算法总是保留基础的字符。`,St,ds,Ee=`因为Unigram不是基于融合规则（和BPE以及WordPiece相比较），在训练以后算法有几种方式来分词，如果一个训练好的Unigram分词器
的词典是这个：`,Dt,js,Gt,Ts,Be=`<code>&quot;hugs&quot;</code>可以被分词成<code>[&quot;hug&quot;, &quot;s&quot;]</code>, <code>[&quot;h&quot;, &quot;ug&quot;, &quot;s&quot;]</code>或者<code>[&quot;h&quot;, &quot;u&quot;, &quot;g&quot;, &quot;s&quot;]</code>。所以选择哪一个呢？Unigram在保存
词典的时候还会保存训练语料库内每个token的概率，所以在训练以后可以计算每个可能的分词结果的概率。实际上算法简单的选择概率
最大的那个分词结果，但是也会提供概率来根据分词结果的概率来采样一个可能的分词结果。`,Zt,g,Ot,xs,Ae="{1}$, $\\dots$, $x",se,Cs,Se=`{i}$
的所有可能的分词结果的集合定义为$S(x`,te,Vt,Ve=`<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="script">L</mi><mo>=</mo><mo>−</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>S</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></munder><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\\mathcal{L} = -\\sum_{i=1}^{N} \\log \\left ( \\sum_{x \\in S(x_{i})} p(x) \\right )</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathcal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:3.6em;vertical-align:-1.55em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.8283em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.05em;"><span class="pstrut" style="height:5.6em;"></span><span style="width:0.875em;height:3.600em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.875em' height='3.600em' viewBox='0 0 875 3600'><path d='M863,9c0,-2,-2,-5,-6,-9c0,0,-17,0,-17,0c-12.7,0,-19.3,0.3,-20,1
c-5.3,5.3,-10.3,11,-15,17c-242.7,294.7,-395.3,682,-458,1162c-21.3,163.3,-33.3,349,
-36,557 l0,84c0.2,6,0,26,0,60c2,159.3,10,310.7,24,454c53.3,528,210,
949.7,470,1265c4.7,6,9.7,11.7,15,17c0.7,0.7,7,1,19,1c0,0,18,0,18,0c4,-4,6,-7,6,-9
c0,-2.7,-3.3,-8.7,-10,-18c-135.3,-192.7,-235.5,-414.3,-300.5,-665c-65,-250.7,-102.5,
-544.7,-112.5,-882c-2,-104,-3,-167,-3,-189
l0,-92c0,-162.7,5.7,-314,17,-454c20.7,-272,63.7,-513,129,-723c65.3,
-210,155.3,-396.3,270,-559c6.7,-9.3,10,-15.3,10,-18z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.05em;"><span style="top:-1.809em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.05764em;">S</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.516em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:2.05em;"><span style="top:-4.05em;"><span class="pstrut" style="height:5.6em;"></span><span style="width:0.875em;height:3.600em;"><svg xmlns="http://www.w3.org/2000/svg" width='0.875em' height='3.600em' viewBox='0 0 875 3600'><path d='M76,0c-16.7,0,-25,3,-25,9c0,2,2,6.3,6,13c21.3,28.7,42.3,60.3,
63,95c96.7,156.7,172.8,332.5,228.5,527.5c55.7,195,92.8,416.5,111.5,664.5
c11.3,139.3,17,290.7,17,454c0,28,1.7,43,3.3,45l0,9
c-3,4,-3.3,16.7,-3.3,38c0,162,-5.7,313.7,-17,455c-18.7,248,-55.8,469.3,-111.5,664
c-55.7,194.7,-131.8,370.3,-228.5,527c-20.7,34.7,-41.7,66.3,-63,95c-2,3.3,-4,7,-6,11
c0,7.3,5.7,11,17,11c0,0,11,0,11,0c9.3,0,14.3,-0.3,15,-1c5.3,-5.3,10.3,-11,15,-17
c242.7,-294.7,395.3,-681.7,458,-1161c21.3,-164.7,33.3,-350.7,36,-558
l0,-144c-2,-159.3,-10,-310.7,-24,-454c-53.3,-528,-210,-949.7,
-470,-1265c-4.7,-6,-9.7,-11.7,-15,-17c-0.7,-0.7,-6.7,-1,-18,-1z'/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.55em;"><span></span></span></span></span></span></span></span></span></span></span></span>`,Wt,Ps,Nt,fs,Qt,Js,De=`目前为止描述的所有分词算法都有相同的问题：它们都假设输入的文本使用空格来分开单词。然而，不是所有的语言都使用空格来分开单词。
一个可能的解决方案是使用某种语言特定的预分词器。像：<a href="model_doc/xlm">XLM</a>使用了一个特定的中文、日语和Thai的预分词器。
为了更加广泛的解决这个问题，<a href="https://arxiv.org/pdf/1808.06226.pdf" rel="nofollow">SentencePiece: A simple and language independent subword tokenizer and
detokenizer for Neural Text Processing (Kudo et al., 2018)</a>
将输入文本看作一个原始的输入流，因此使用的符合集合中也包括了空格。SentencePiece然后会使用BPE或者unigram算法来产生合适的
词典。`,Xt,$s,Ge="举例来说，<code>XLNetTokenizer</code>使用了SentencePiece，这也是为什么上面的例子中<code>&quot;▁&quot;</code>符号包含在词典内。SentencePiece解码是非常容易的，因为所有的tokens能被concatenate起来，然后将<code>&quot;▁&quot;</code>替换成空格。",Kt,bs,Ze=`库内所有使用了SentencePiece的transformers模型，会和unigram组合起来使用，像：使用了SentencePiece的模型是<a href="model_doc/albert">ALBERT</a>,
<a href="model_doc/xlnet">XLNet</a>，<a href="model_doc/marian">Marian</a>，和<a href="model_doc/t5">T5</a>。`,Rt,zs,Yt;return j=new d({props:{title:"分词器的摘要",local:"分词器的摘要",headingTag:"h1"}}),T=new Oe({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/zh/tokenizer_summary.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/zh/pytorch/tokenizer_summary.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/zh/tensorflow/tokenizer_summary.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/zh/tokenizer_summary.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/zh/pytorch/tokenizer_summary.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/zh/tensorflow/tokenizer_summary.ipynb"}]}}),J=new Ft({props:{id:"VFp38yj8h3A"}}),v=new d({props:{title:"介绍",local:"介绍",headingTag:"h2"}}),I=new Ft({props:{id:"nhJxYji1aho"}}),k=new y({props:{code:"JTVCJTIyRG9uJ3QlMjIlMkMlMjAlMjJ5b3UlMjIlMkMlMjAlMjJsb3ZlJTIyJTJDJTIwJTIyJUYwJTlGJUE0JTk3JTIyJTJDJTIwJTIyVHJhbnNmb3JtZXJzJTNGJTIyJTJDJTIwJTIyV2UlMjIlMkMlMjAlMjJzdXJlJTIyJTJDJTIwJTIyZG8uJTIyJTVE",highlighted:'<span class="hljs-selector-attr">[<span class="hljs-string">&quot;Don&#x27;t&quot;</span>, <span class="hljs-string">&quot;you&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;🤗&quot;</span>, <span class="hljs-string">&quot;Transformers?&quot;</span>, <span class="hljs-string">&quot;We&quot;</span>, <span class="hljs-string">&quot;sure&quot;</span>, <span class="hljs-string">&quot;do.&quot;</span>]</span>',wrap:!1}}),C=new y({props:{code:"JTVCJTIyRG9uJTIyJTJDJTIwJTIyJyUyMiUyQyUyMCUyMnQlMjIlMkMlMjAlMjJ5b3UlMjIlMkMlMjAlMjJsb3ZlJTIyJTJDJTIwJTIyJUYwJTlGJUE0JTk3JTIyJTJDJTIwJTIyVHJhbnNmb3JtZXJzJTIyJTJDJTIwJTIyJTNGJTIyJTJDJTIwJTIyV2UlMjIlMkMlMjAlMjJzdXJlJTIyJTJDJTIwJTIyZG8lMjIlMkMlMjAlMjIuJTIyJTVE",highlighted:'<span class="hljs-selector-attr">[<span class="hljs-string">&quot;Don&quot;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&quot;t&quot;</span>, <span class="hljs-string">&quot;you&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;🤗&quot;</span>, <span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;?&quot;</span>, <span class="hljs-string">&quot;We&quot;</span>, <span class="hljs-string">&quot;sure&quot;</span>, <span class="hljs-string">&quot;do&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>]</span>',wrap:!1}}),H=new y({props:{code:"JTVCJTIyRG8lMjIlMkMlMjAlMjJuJ3QlMjIlMkMlMjAlMjJ5b3UlMjIlMkMlMjAlMjJsb3ZlJTIyJTJDJTIwJTIyJUYwJTlGJUE0JTk3JTIyJTJDJTIwJTIyVHJhbnNmb3JtZXJzJTIyJTJDJTIwJTIyJTNGJTIyJTJDJTIwJTIyV2UlMjIlMkMlMjAlMjJzdXJlJTIyJTJDJTIwJTIyZG8lMjIlMkMlMjAlMjIuJTIyJTVE",highlighted:'<span class="hljs-selector-attr">[<span class="hljs-string">&quot;Do&quot;</span>, <span class="hljs-string">&quot;n&#x27;t&quot;</span>, <span class="hljs-string">&quot;you&quot;</span>, <span class="hljs-string">&quot;love&quot;</span>, <span class="hljs-string">&quot;🤗&quot;</span>, <span class="hljs-string">&quot;Transformers&quot;</span>, <span class="hljs-string">&quot;?&quot;</span>, <span class="hljs-string">&quot;We&quot;</span>, <span class="hljs-string">&quot;sure&quot;</span>, <span class="hljs-string">&quot;do&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>]</span>',wrap:!1}}),B=new Ft({props:{id:"ssLq_EK2jLE"}}),S=new d({props:{title:"子词分词",local:"子词分词",headingTag:"h2"}}),D=new Ft({props:{id:"zHvTiHr506c"}}),V=new y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlcnRUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBCZXJ0VG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQXRva2VuaXplci50b2tlbml6ZSglMjJJJTIwaGF2ZSUyMGElMjBuZXclMjBHUFUhJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.tokenize(<span class="hljs-string">&quot;I have a new GPU!&quot;</span>)
[<span class="hljs-string">&quot;i&quot;</span>, <span class="hljs-string">&quot;have&quot;</span>, <span class="hljs-string">&quot;a&quot;</span>, <span class="hljs-string">&quot;new&quot;</span>, <span class="hljs-string">&quot;gp&quot;</span>, <span class="hljs-string">&quot;##u&quot;</span>, <span class="hljs-string">&quot;!&quot;</span>]`,wrap:!1}}),Q=new y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFhMTmV0VG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwWExOZXRUb2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMnhsbmV0JTJGeGxuZXQtYmFzZS1jYXNlZCUyMiklMEF0b2tlbml6ZXIudG9rZW5pemUoJTIyRG9uJ3QlMjB5b3UlMjBsb3ZlJTIwJUYwJTlGJUE0JTk3JTIwVHJhbnNmb3JtZXJzJTNGJTIwV2UlMjBzdXJlJTIwZG8uJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> XLNetTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = XLNetTokenizer.from_pretrained(<span class="hljs-string">&quot;xlnet/xlnet-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.tokenize(<span class="hljs-string">&quot;Don&#x27;t you love 🤗 Transformers? We sure do.&quot;</span>)
[<span class="hljs-string">&quot;▁Don&quot;</span>, <span class="hljs-string">&quot;&#x27;&quot;</span>, <span class="hljs-string">&quot;t&quot;</span>, <span class="hljs-string">&quot;▁you&quot;</span>, <span class="hljs-string">&quot;▁love&quot;</span>, <span class="hljs-string">&quot;▁&quot;</span>, <span class="hljs-string">&quot;🤗&quot;</span>, <span class="hljs-string">&quot;▁&quot;</span>, <span class="hljs-string">&quot;Transform&quot;</span>, <span class="hljs-string">&quot;ers&quot;</span>, <span class="hljs-string">&quot;?&quot;</span>, <span class="hljs-string">&quot;▁We&quot;</span>, <span class="hljs-string">&quot;▁sure&quot;</span>, <span class="hljs-string">&quot;▁do&quot;</span>, <span class="hljs-string">&quot;.&quot;</span>]`,wrap:!1}}),R=new d({props:{title:"Byte-Pair Encoding (BPE)",local:"byte-pair-encoding-bpe",headingTag:"h3"}}),ss=new y({props:{code:"KCUyMmh1ZyUyMiUyQyUyMDEwKSUyQyUyMCglMjJwdWclMjIlMkMlMjA1KSUyQyUyMCglMjJwdW4lMjIlMkMlMjAxMiklMkMlMjAoJTIyYnVuJTIyJTJDJTIwNCklMkMlMjAoJTIyaHVncyUyMiUyQyUyMDUp",highlighted:'(<span class="hljs-string">&quot;hug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;pun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;bun&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;hugs&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)',wrap:!1}}),es=new y({props:{code:"KCUyMmglMjIlMjAlMjJ1JTIyJTIwJTIyZyUyMiUyQyUyMDEwKSUyQyUyMCglMjJwJTIyJTIwJTIydSUyMiUyMCUyMmclMjIlMkMlMjA1KSUyQyUyMCglMjJwJTIyJTIwJTIydSUyMiUyMCUyMm4lMjIlMkMlMjAxMiklMkMlMjAoJTIyYiUyMiUyMCUyMnUlMjIlMjAlMjJuJTIyJTJDJTIwNCklMkMlMjAoJTIyaCUyMiUyMCUyMnUlMjIlMjAlMjJnJTIyJTIwJTIycyUyMiUyQyUyMDUp",highlighted:'(<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;g&quot;</span> <span class="hljs-string">&quot;s&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)',wrap:!1}}),as=new y({props:{code:"KCUyMmglMjIlMjAlMjJ1ZyUyMiUyQyUyMDEwKSUyQyUyMCglMjJwJTIyJTIwJTIydWclMjIlMkMlMjA1KSUyQyUyMCglMjJwJTIyJTIwJTIydSUyMiUyMCUyMm4lMjIlMkMlMjAxMiklMkMlMjAoJTIyYiUyMiUyMCUyMnUlMjIlMjAlMjJuJTIyJTJDJTIwNCklMkMlMjAoJTIyaCUyMiUyMCUyMnVnJTIyJTIwJTIycyUyMiUyQyUyMDUp",highlighted:'(<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;ug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;u&quot;</span> <span class="hljs-string">&quot;n&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;h&quot;</span> <span class="hljs-string">&quot;ug&quot;</span> <span class="hljs-string">&quot;s&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)',wrap:!1}}),os=new y({props:{code:"KCUyMmh1ZyUyMiUyQyUyMDEwKSUyQyUyMCglMjJwJTIyJTIwJTIydWclMjIlMkMlMjA1KSUyQyUyMCglMjJwJTIyJTIwJTIydW4lMjIlMkMlMjAxMiklMkMlMjAoJTIyYiUyMiUyMCUyMnVuJTIyJTJDJTIwNCklMkMlMjAoJTIyaHVnJTIyJTIwJTIycyUyMiUyQyUyMDUp",highlighted:'(<span class="hljs-string">&quot;hug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">10</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;ug&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;p&quot;</span> <span class="hljs-string">&quot;un&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">12</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;b&quot;</span> <span class="hljs-string">&quot;un&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">4</span>)<span class="hljs-punctuation">,</span> (<span class="hljs-string">&quot;hug&quot;</span> <span class="hljs-string">&quot;s&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5</span>)',wrap:!1}}),cs=new d({props:{title:"Byte-level BPE",local:"byte-level-bpe",headingTag:"h4"}}),ms=new d({props:{title:"WordPiece",local:"wordpiece",headingTag:"h3"}}),gs=new d({props:{title:"Unigram",local:"unigram",headingTag:"h3"}}),js=new y({props:{code:"JTVCJTIyYiUyMiUyQyUyMCUyMmclMjIlMkMlMjAlMjJoJTIyJTJDJTIwJTIybiUyMiUyQyUyMCUyMnAlMjIlMkMlMjAlMjJzJTIyJTJDJTIwJTIydSUyMiUyQyUyMCUyMnVnJTIyJTJDJTIwJTIydW4lMjIlMkMlMjAlMjJodWclMjIlNUQlMkM=",highlighted:'<span class="hljs-selector-attr">[<span class="hljs-string">&quot;b&quot;</span>, <span class="hljs-string">&quot;g&quot;</span>, <span class="hljs-string">&quot;h&quot;</span>, <span class="hljs-string">&quot;n&quot;</span>, <span class="hljs-string">&quot;p&quot;</span>, <span class="hljs-string">&quot;s&quot;</span>, <span class="hljs-string">&quot;u&quot;</span>, <span class="hljs-string">&quot;ug&quot;</span>, <span class="hljs-string">&quot;un&quot;</span>, <span class="hljs-string">&quot;hug&quot;</span>]</span>,',wrap:!1}}),fs=new d({props:{title:"SentencePiece",local:"sentencepiece",headingTag:"h3"}}),{c(){M=p("meta"),Es=a(),Ls=p("p"),Bs=a(),i(j.$$.fragment),As=a(),i(T.$$.fragment),Ss=a(),f=p("p"),f.textContent=le,Ds=a(),i(J.$$.fragment),Gs=a(),$=p("p"),$.innerHTML=pe,Zs=a(),b=p("p"),b.innerHTML=oe,Vs=a(),i(v.$$.fragment),Ws=a(),w=p("p"),w.innerHTML=ue,Ns=a(),i(I.$$.fragment),Qs=a(),U=p("p"),U.textContent=ie,Xs=a(),i(k.$$.fragment),Ks=a(),x=p("p"),x.innerHTML=ce,Rs=a(),i(C.$$.fragment),Ys=a(),P=p("p"),P.innerHTML=re,Fs=a(),_=p("p"),_.innerHTML=me,Os=a(),i(H.$$.fragment),st=a(),L=p("p"),L.innerHTML=qe,tt=a(),z=p("p"),z.textContent=he,et=a(),E=p("p"),E.textContent=ge,nt=a(),i(B.$$.fragment),at=a(),A=p("p"),A.innerHTML=ye,lt=a(),i(S.$$.fragment),pt=a(),i(D.$$.fragment),ot=a(),G=p("p"),G.innerHTML=Me,ut=a(),Z=p("p"),Z.innerHTML=de,it=a(),i(V.$$.fragment),ct=a(),W=p("p"),W.innerHTML=je,rt=a(),N=p("p"),N.innerHTML=Te,mt=a(),i(Q.$$.fragment),qt=a(),X=p("p"),X.innerHTML=fe,ht=a(),K=p("p"),K.textContent=Je,gt=a(),Is=p("a"),yt=a(),i(R.$$.fragment),Mt=a(),Y=p("p"),Y.innerHTML=$e,dt=a(),F=p("p"),F.textContent=be,jt=a(),O=p("p"),O.textContent=ve,Tt=a(),i(ss.$$.fragment),ft=a(),ts=p("p"),ts.innerHTML=we,Jt=a(),i(es.$$.fragment),$t=a(),ns=p("p"),ns.innerHTML=Ie,bt=a(),i(as.$$.fragment),vt=a(),ls=p("p"),ls.innerHTML=Ue,wt=a(),ps=p("p"),ps.innerHTML=ke,It=a(),i(os.$$.fragment),Ut=a(),us=p("p"),us.innerHTML=xe,kt=a(),is=p("p"),is.innerHTML=Ce,xt=a(),i(cs.$$.fragment),Ct=a(),rs=p("p"),rs.innerHTML=Pe,Pt=a(),Us=p("a"),_t=a(),i(ms.$$.fragment),Ht=a(),qs=p("p"),qs.innerHTML=_e,Lt=a(),hs=p("p"),hs.innerHTML=He,zt=a(),ks=p("a"),Et=a(),i(gs.$$.fragment),Bt=a(),ys=p("p"),ys.innerHTML=Le,At=a(),Ms=p("p"),Ms.textContent=ze,St=a(),ds=p("p"),ds.textContent=Ee,Dt=a(),i(js.$$.fragment),Gt=a(),Ts=p("p"),Ts.innerHTML=Be,Zt=a(),g=p("p"),Ot=ee("分词器在损失函数上训练，这些损失函数定义了这些概率。假设训练数据包含了这些单词 $x"),xs=p("em"),xs.textContent=Ae,se=ee("{N}$，一个单词$x"),Cs=p("em"),Cs.textContent=Se,te=ee(`{i})$，然后总的损失就可以定义为：
`),Vt=new Re(!1),Wt=a(),Ps=p("a"),Nt=a(),i(fs.$$.fragment),Qt=a(),Js=p("p"),Js.innerHTML=De,Xt=a(),$s=p("p"),$s.innerHTML=Ge,Kt=a(),bs=p("p"),bs.innerHTML=Ze,Rt=a(),zs=p("p"),this.h()},l(s){const t=Ye("svelte-u9bgzb",document.head);M=o(t,"META",{name:!0,content:!0}),t.forEach(e),Es=l(s),Ls=o(s,"P",{}),ws(Ls).forEach(e),Bs=l(s),c(j.$$.fragment,s),As=l(s),c(T.$$.fragment,s),Ss=l(s),f=o(s,"P",{"data-svelte-h":!0}),u(f)!=="svelte-pfsa4m"&&(f.textContent=le),Ds=l(s),c(J.$$.fragment,s),Gs=l(s),$=o(s,"P",{"data-svelte-h":!0}),u($)!=="svelte-jjn54r"&&($.innerHTML=pe),Zs=l(s),b=o(s,"P",{"data-svelte-h":!0}),u(b)!=="svelte-1kzo898"&&(b.innerHTML=oe),Vs=l(s),c(v.$$.fragment,s),Ws=l(s),w=o(s,"P",{"data-svelte-h":!0}),u(w)!=="svelte-vjsjp6"&&(w.innerHTML=ue),Ns=l(s),c(I.$$.fragment,s),Qs=l(s),U=o(s,"P",{"data-svelte-h":!0}),u(U)!=="svelte-tb01sq"&&(U.textContent=ie),Xs=l(s),c(k.$$.fragment,s),Ks=l(s),x=o(s,"P",{"data-svelte-h":!0}),u(x)!=="svelte-58ufza"&&(x.innerHTML=ce),Rs=l(s),c(C.$$.fragment,s),Ys=l(s),P=o(s,"P",{"data-svelte-h":!0}),u(P)!=="svelte-dbij3c"&&(P.innerHTML=re),Fs=l(s),_=o(s,"P",{"data-svelte-h":!0}),u(_)!=="svelte-dfdqo"&&(_.innerHTML=me),Os=l(s),c(H.$$.fragment,s),st=l(s),L=o(s,"P",{"data-svelte-h":!0}),u(L)!=="svelte-zrf8dv"&&(L.innerHTML=qe),tt=l(s),z=o(s,"P",{"data-svelte-h":!0}),u(z)!=="svelte-l0cz7k"&&(z.textContent=he),et=l(s),E=o(s,"P",{"data-svelte-h":!0}),u(E)!=="svelte-r0xyai"&&(E.textContent=ge),nt=l(s),c(B.$$.fragment,s),at=l(s),A=o(s,"P",{"data-svelte-h":!0}),u(A)!=="svelte-1ol5991"&&(A.innerHTML=ye),lt=l(s),c(S.$$.fragment,s),pt=l(s),c(D.$$.fragment,s),ot=l(s),G=o(s,"P",{"data-svelte-h":!0}),u(G)!=="svelte-kq7vff"&&(G.innerHTML=Me),ut=l(s),Z=o(s,"P",{"data-svelte-h":!0}),u(Z)!=="svelte-9aa0t4"&&(Z.innerHTML=de),it=l(s),c(V.$$.fragment,s),ct=l(s),W=o(s,"P",{"data-svelte-h":!0}),u(W)!=="svelte-187mdte"&&(W.innerHTML=je),rt=l(s),N=o(s,"P",{"data-svelte-h":!0}),u(N)!=="svelte-41u6u1"&&(N.innerHTML=Te),mt=l(s),c(Q.$$.fragment,s),qt=l(s),X=o(s,"P",{"data-svelte-h":!0}),u(X)!=="svelte-wliwz4"&&(X.innerHTML=fe),ht=l(s),K=o(s,"P",{"data-svelte-h":!0}),u(K)!=="svelte-83sufb"&&(K.textContent=Je),gt=l(s),Is=o(s,"A",{id:!0}),ws(Is).forEach(e),yt=l(s),c(R.$$.fragment,s),Mt=l(s),Y=o(s,"P",{"data-svelte-h":!0}),u(Y)!=="svelte-c1a0t2"&&(Y.innerHTML=$e),dt=l(s),F=o(s,"P",{"data-svelte-h":!0}),u(F)!=="svelte-172lpbn"&&(F.textContent=be),jt=l(s),O=o(s,"P",{"data-svelte-h":!0}),u(O)!=="svelte-1050gr9"&&(O.textContent=ve),Tt=l(s),c(ss.$$.fragment,s),ft=l(s),ts=o(s,"P",{"data-svelte-h":!0}),u(ts)!=="svelte-1erysxq"&&(ts.innerHTML=we),Jt=l(s),c(es.$$.fragment,s),$t=l(s),ns=o(s,"P",{"data-svelte-h":!0}),u(ns)!=="svelte-19g6azp"&&(ns.innerHTML=Ie),bt=l(s),c(as.$$.fragment,s),vt=l(s),ls=o(s,"P",{"data-svelte-h":!0}),u(ls)!=="svelte-1atd0kw"&&(ls.innerHTML=Ue),wt=l(s),ps=o(s,"P",{"data-svelte-h":!0}),u(ps)!=="svelte-1w998qw"&&(ps.innerHTML=ke),It=l(s),c(os.$$.fragment,s),Ut=l(s),us=o(s,"P",{"data-svelte-h":!0}),u(us)!=="svelte-1o72vbo"&&(us.innerHTML=xe),kt=l(s),is=o(s,"P",{"data-svelte-h":!0}),u(is)!=="svelte-o6latr"&&(is.innerHTML=Ce),xt=l(s),c(cs.$$.fragment,s),Ct=l(s),rs=o(s,"P",{"data-svelte-h":!0}),u(rs)!=="svelte-um3th"&&(rs.innerHTML=Pe),Pt=l(s),Us=o(s,"A",{id:!0}),ws(Us).forEach(e),_t=l(s),c(ms.$$.fragment,s),Ht=l(s),qs=o(s,"P",{"data-svelte-h":!0}),u(qs)!=="svelte-1uwctp"&&(qs.innerHTML=_e),Lt=l(s),hs=o(s,"P",{"data-svelte-h":!0}),u(hs)!=="svelte-1ml2q03"&&(hs.innerHTML=He),zt=l(s),ks=o(s,"A",{id:!0}),ws(ks).forEach(e),Et=l(s),c(gs.$$.fragment,s),Bt=l(s),ys=o(s,"P",{"data-svelte-h":!0}),u(ys)!=="svelte-j55afk"&&(ys.innerHTML=Le),At=l(s),Ms=o(s,"P",{"data-svelte-h":!0}),u(Ms)!=="svelte-1hvgw54"&&(Ms.textContent=ze),St=l(s),ds=o(s,"P",{"data-svelte-h":!0}),u(ds)!=="svelte-16znulm"&&(ds.textContent=Ee),Dt=l(s),c(js.$$.fragment,s),Gt=l(s),Ts=o(s,"P",{"data-svelte-h":!0}),u(Ts)!=="svelte-1sn5ht2"&&(Ts.innerHTML=Be),Zt=l(s),g=o(s,"P",{});var vs=ws(g);Ot=ne(vs,"分词器在损失函数上训练，这些损失函数定义了这些概率。假设训练数据包含了这些单词 $x"),xs=o(vs,"EM",{"data-svelte-h":!0}),u(xs)!=="svelte-mvn35i"&&(xs.textContent=Ae),se=ne(vs,"{N}$，一个单词$x"),Cs=o(vs,"EM",{"data-svelte-h":!0}),u(Cs)!=="svelte-10k1kx9"&&(Cs.textContent=Se),te=ne(vs,`{i})$，然后总的损失就可以定义为：
`),Vt=Fe(vs,!1),vs.forEach(e),Wt=l(s),Ps=o(s,"A",{id:!0}),ws(Ps).forEach(e),Nt=l(s),c(fs.$$.fragment,s),Qt=l(s),Js=o(s,"P",{"data-svelte-h":!0}),u(Js)!=="svelte-1ndj2al"&&(Js.innerHTML=De),Xt=l(s),$s=o(s,"P",{"data-svelte-h":!0}),u($s)!=="svelte-l82h58"&&($s.innerHTML=Ge),Kt=l(s),bs=o(s,"P",{"data-svelte-h":!0}),u(bs)!=="svelte-1sslq9n"&&(bs.innerHTML=Ze),Rt=l(s),zs=o(s,"P",{}),ws(zs).forEach(e),this.h()},h(){_s(M,"name","hf:doc:metadata"),_s(M,"content",tn),_s(Is,"id","byte-pair-encoding"),_s(Us,"id","wordpiece"),_s(ks,"id","unigram"),Vt.a=null,_s(Ps,"id","sentencepiece")},m(s,t){Hs(document.head,M),n(s,Es,t),n(s,Ls,t),n(s,Bs,t),r(j,s,t),n(s,As,t),r(T,s,t),n(s,Ss,t),n(s,f,t),n(s,Ds,t),r(J,s,t),n(s,Gs,t),n(s,$,t),n(s,Zs,t),n(s,b,t),n(s,Vs,t),r(v,s,t),n(s,Ws,t),n(s,w,t),n(s,Ns,t),r(I,s,t),n(s,Qs,t),n(s,U,t),n(s,Xs,t),r(k,s,t),n(s,Ks,t),n(s,x,t),n(s,Rs,t),r(C,s,t),n(s,Ys,t),n(s,P,t),n(s,Fs,t),n(s,_,t),n(s,Os,t),r(H,s,t),n(s,st,t),n(s,L,t),n(s,tt,t),n(s,z,t),n(s,et,t),n(s,E,t),n(s,nt,t),r(B,s,t),n(s,at,t),n(s,A,t),n(s,lt,t),r(S,s,t),n(s,pt,t),r(D,s,t),n(s,ot,t),n(s,G,t),n(s,ut,t),n(s,Z,t),n(s,it,t),r(V,s,t),n(s,ct,t),n(s,W,t),n(s,rt,t),n(s,N,t),n(s,mt,t),r(Q,s,t),n(s,qt,t),n(s,X,t),n(s,ht,t),n(s,K,t),n(s,gt,t),n(s,Is,t),n(s,yt,t),r(R,s,t),n(s,Mt,t),n(s,Y,t),n(s,dt,t),n(s,F,t),n(s,jt,t),n(s,O,t),n(s,Tt,t),r(ss,s,t),n(s,ft,t),n(s,ts,t),n(s,Jt,t),r(es,s,t),n(s,$t,t),n(s,ns,t),n(s,bt,t),r(as,s,t),n(s,vt,t),n(s,ls,t),n(s,wt,t),n(s,ps,t),n(s,It,t),r(os,s,t),n(s,Ut,t),n(s,us,t),n(s,kt,t),n(s,is,t),n(s,xt,t),r(cs,s,t),n(s,Ct,t),n(s,rs,t),n(s,Pt,t),n(s,Us,t),n(s,_t,t),r(ms,s,t),n(s,Ht,t),n(s,qs,t),n(s,Lt,t),n(s,hs,t),n(s,zt,t),n(s,ks,t),n(s,Et,t),r(gs,s,t),n(s,Bt,t),n(s,ys,t),n(s,At,t),n(s,Ms,t),n(s,St,t),n(s,ds,t),n(s,Dt,t),r(js,s,t),n(s,Gt,t),n(s,Ts,t),n(s,Zt,t),n(s,g,t),Hs(g,Ot),Hs(g,xs),Hs(g,se),Hs(g,Cs),Hs(g,te),Vt.m(Ve,g),n(s,Wt,t),n(s,Ps,t),n(s,Nt,t),r(fs,s,t),n(s,Qt,t),n(s,Js,t),n(s,Xt,t),n(s,$s,t),n(s,Kt,t),n(s,bs,t),n(s,Rt,t),n(s,zs,t),Yt=!0},p:Ne,i(s){Yt||(m(j.$$.fragment,s),m(T.$$.fragment,s),m(J.$$.fragment,s),m(v.$$.fragment,s),m(I.$$.fragment,s),m(k.$$.fragment,s),m(C.$$.fragment,s),m(H.$$.fragment,s),m(B.$$.fragment,s),m(S.$$.fragment,s),m(D.$$.fragment,s),m(V.$$.fragment,s),m(Q.$$.fragment,s),m(R.$$.fragment,s),m(ss.$$.fragment,s),m(es.$$.fragment,s),m(as.$$.fragment,s),m(os.$$.fragment,s),m(cs.$$.fragment,s),m(ms.$$.fragment,s),m(gs.$$.fragment,s),m(js.$$.fragment,s),m(fs.$$.fragment,s),Yt=!0)},o(s){q(j.$$.fragment,s),q(T.$$.fragment,s),q(J.$$.fragment,s),q(v.$$.fragment,s),q(I.$$.fragment,s),q(k.$$.fragment,s),q(C.$$.fragment,s),q(H.$$.fragment,s),q(B.$$.fragment,s),q(S.$$.fragment,s),q(D.$$.fragment,s),q(V.$$.fragment,s),q(Q.$$.fragment,s),q(R.$$.fragment,s),q(ss.$$.fragment,s),q(es.$$.fragment,s),q(as.$$.fragment,s),q(os.$$.fragment,s),q(cs.$$.fragment,s),q(ms.$$.fragment,s),q(gs.$$.fragment,s),q(js.$$.fragment,s),q(fs.$$.fragment,s),Yt=!1},d(s){s&&(e(Es),e(Ls),e(Bs),e(As),e(Ss),e(f),e(Ds),e(Gs),e($),e(Zs),e(b),e(Vs),e(Ws),e(w),e(Ns),e(Qs),e(U),e(Xs),e(Ks),e(x),e(Rs),e(Ys),e(P),e(Fs),e(_),e(Os),e(st),e(L),e(tt),e(z),e(et),e(E),e(nt),e(at),e(A),e(lt),e(pt),e(ot),e(G),e(ut),e(Z),e(it),e(ct),e(W),e(rt),e(N),e(mt),e(qt),e(X),e(ht),e(K),e(gt),e(Is),e(yt),e(Mt),e(Y),e(dt),e(F),e(jt),e(O),e(Tt),e(ft),e(ts),e(Jt),e($t),e(ns),e(bt),e(vt),e(ls),e(wt),e(ps),e(It),e(Ut),e(us),e(kt),e(is),e(xt),e(Ct),e(rs),e(Pt),e(Us),e(_t),e(Ht),e(qs),e(Lt),e(hs),e(zt),e(ks),e(Et),e(Bt),e(ys),e(At),e(Ms),e(St),e(ds),e(Dt),e(Gt),e(Ts),e(Zt),e(g),e(Wt),e(Ps),e(Nt),e(Qt),e(Js),e(Xt),e($s),e(Kt),e(bs),e(Rt),e(zs)),e(M),h(j,s),h(T,s),h(J,s),h(v,s),h(I,s),h(k,s),h(C,s),h(H,s),h(B,s),h(S,s),h(D,s),h(V,s),h(Q,s),h(R,s),h(ss,s),h(es,s),h(as,s),h(os,s),h(cs,s),h(ms,s),h(gs,s),h(js,s),h(fs,s)}}}const tn='{"title":"分词器的摘要","local":"分词器的摘要","sections":[{"title":"介绍","local":"介绍","sections":[],"depth":2},{"title":"子词分词","local":"子词分词","sections":[{"title":"Byte-Pair Encoding (BPE)","local":"byte-pair-encoding-bpe","sections":[{"title":"Byte-level BPE","local":"byte-level-bpe","sections":[],"depth":4}],"depth":3},{"title":"WordPiece","local":"wordpiece","sections":[],"depth":3},{"title":"Unigram","local":"unigram","sections":[],"depth":3},{"title":"SentencePiece","local":"sentencepiece","sections":[],"depth":3}],"depth":2}],"depth":1}';function en(ae){return Qe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cn extends Xe{constructor(M){super(),Ke(this,M,en,sn,We,{})}}export{cn as component};
