import{s as va,o as Ca,n as I}from"../chunks/scheduler.9991993c.js";import{S as Ga,i as Ra,g as y,s as o,r as d,A as Wa,h as b,f as a,c as i,j as z,u as $,x as j,k as Ua,y as J,a as r,v as u,d as h,t as g,w as M}from"../chunks/index.7fc9a5e7.js";import{T as St}from"../chunks/Tip.9de92fc6.js";import{Y as Za}from"../chunks/Youtube.7934cf81.js";import{C as _}from"../chunks/CodeBlock.e11cba92.js";import{D as Ha}from"../chunks/DocNotebookDropdown.a0cb4c0f.js";import{F as It,M as E}from"../chunks/Markdown.87f31c7e.js";import{H as B}from"../chunks/Heading.e3de321f.js";function Va(w){let s,p;return s=new _({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRvcmNo",highlighted:"pip install torch",wrap:!1}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p:I,i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function Ia(w){let s,p;return s=new E({props:{$$slots:{default:[Va]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function Xa(w){let s,p;return s=new _({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRlbnNvcmZsb3c=",highlighted:"pip install tensorflow",wrap:!1}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p:I,i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function Fa(w){let s,p;return s=new E({props:{$$slots:{default:[Xa]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function za(w){let s,p="使用 <code>AutoModelForSequenceClassification</code> 和 <code>AutoTokenizer</code> 来加载预训练模型和它关联的分词器（更多信息可以参考下一节的 <code>AutoClass</code>）：",t,n,f;return n=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`,wrap:!1}}),{c(){s=y("p"),s.innerHTML=p,t=o(),d(n.$$.fragment)},l(m){s=b(m,"P",{"data-svelte-h":!0}),j(s)!=="svelte-16qd42h"&&(s.innerHTML=p),t=i(m),$(n.$$.fragment,m)},m(m,k){r(m,s,k),r(m,t,k),u(n,m,k),f=!0},p:I,i(m){f||(h(n.$$.fragment,m),f=!0)},o(m){g(n.$$.fragment,m),f=!1},d(m){m&&(a(s),a(t)),M(n,m)}}}function Na(w){let s,p;return s=new E({props:{$$slots:{default:[za]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function xa(w){let s,p="使用 <code>TFAutoModelForSequenceClassification</code> 和 <code>AutoTokenizer</code> 来加载预训练模型和它关联的分词器（更多信息可以参考下一节的 <code>TFAutoClass</code>）：",t,n,f;return n=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQobW9kZWxfbmFtZSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`,wrap:!1}}),{c(){s=y("p"),s.innerHTML=p,t=o(),d(n.$$.fragment)},l(m){s=b(m,"P",{"data-svelte-h":!0}),j(s)!=="svelte-1scc1z5"&&(s.innerHTML=p),t=i(m),$(n.$$.fragment,m)},m(m,k){r(m,s,k),r(m,t,k),u(n,m,k),f=!0},p:I,i(m){f||(h(n.$$.fragment,m),f=!0)},o(m){g(n.$$.fragment,m),f=!1},d(m){m&&(a(s),a(t)),M(n,m)}}}function Ea(w){let s,p;return s=new E({props:{$$slots:{default:[xa]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function Ya(w){let s,p;return s=new _({props:{code:"cHRfYmF0Y2glMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwJTVCJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiUyQyUyMCUyMldlJTIwaG9wZSUyMHlvdSUyMGRvbid0JTIwaGF0ZSUyMGl0LiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHBhZGRpbmclM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwdHJ1bmNhdGlvbiUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBtYXhfbGVuZ3RoJTNENTEyJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p:I,i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function Ba(w){let s,p;return s=new E({props:{$$slots:{default:[Ya]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function La(w){let s,p;return s=new _({props:{code:"dGZfYmF0Y2glMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwJTVCJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiUyQyUyMCUyMldlJTIwaG9wZSUyMHlvdSUyMGRvbid0JTIwaGF0ZSUyMGl0LiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHBhZGRpbmclM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwdHJ1bmNhdGlvbiUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBtYXhfbGVuZ3RoJTNENTEyJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMiUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p:I,i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function Aa(w){let s,p;return s=new E({props:{$$slots:{default:[La]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function Qa(w){let s,p='查阅<a href="./preprocessing">预处理</a>教程来获得有关分词的更详细的信息，以及如何使用 <code>AutoFeatureExtractor</code> 和 <code>AutoProcessor</code> 来处理图像，音频，还有多模式输入。';return{c(){s=y("p"),s.innerHTML=p},l(t){s=b(t,"P",{"data-svelte-h":!0}),j(s)!=="svelte-l21b6c"&&(s.innerHTML=p)},m(t,n){r(t,s,n)},p:I,d(t){t&&a(s)}}}function qa(w){let s,p='通过 <a href="./task_summary">任务摘要</a> 查找 <code>AutoModel</code> 支持的任务.';return{c(){s=y("p"),s.innerHTML=p},l(t){s=b(t,"P",{"data-svelte-h":!0}),j(s)!=="svelte-1mg580k"&&(s.innerHTML=p)},m(t,n){r(t,s,n)},p:I,d(t){t&&a(s)}}}function Sa(w){let s,p="🤗 Transformers 提供了一种简单统一的方式来加载预训练的实例. 这表示你可以像加载 <code>AutoTokenizer</code> 一样加载 <code>AutoModel</code>。唯一不同的地方是为你的任务选择正确的<code>AutoModel</code>。对于文本（或序列）分类，你应该加载<code>AutoModelForSequenceClassification</code>：",t,n,f,m,k,v,C="现在可以把预处理好的输入批次直接送进模型。你只需要通过 <code>**</code> 来解包字典:",G,T,Z,W,L="模型在 <code>logits</code> 属性输出最终的激活结果. 在 <code>logits</code> 上应用 softmax 函数来查询概率:",H,R,V;return n=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbF9uYW1lJTIwJTNEJTIwJTIybmxwdG93biUyRmJlcnQtYmFzZS1tdWx0aWxpbmd1YWwtdW5jYXNlZC1zZW50aW1lbnQlMjIlMEFwdF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,wrap:!1}}),m=new St({props:{$$slots:{default:[qa]},$$scope:{ctx:w}}}),T=new _({props:{code:"cHRfb3V0cHV0cyUyMCUzRCUyMHB0X21vZGVsKCoqcHRfYmF0Y2gp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)',wrap:!1}}),R=new _({props:{code:"ZnJvbSUyMHRvcmNoJTIwaW1wb3J0JTIwbm4lMEElMEFwdF9wcmVkaWN0aW9ucyUyMCUzRCUyMG5uLmZ1bmN0aW9uYWwuc29mdG1heChwdF9vdXRwdXRzLmxvZ2l0cyUyQyUyMGRpbSUzRC0xKSUwQXByaW50KHB0X3ByZWRpY3Rpb25zKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`,wrap:!1}}),{c(){s=y("p"),s.innerHTML=p,t=o(),d(n.$$.fragment),f=o(),d(m.$$.fragment),k=o(),v=y("p"),v.innerHTML=C,G=o(),d(T.$$.fragment),Z=o(),W=y("p"),W.innerHTML=L,H=o(),d(R.$$.fragment)},l(c){s=b(c,"P",{"data-svelte-h":!0}),j(s)!=="svelte-7kcz0m"&&(s.innerHTML=p),t=i(c),$(n.$$.fragment,c),f=i(c),$(m.$$.fragment,c),k=i(c),v=b(c,"P",{"data-svelte-h":!0}),j(v)!=="svelte-1q5w0a3"&&(v.innerHTML=C),G=i(c),$(T.$$.fragment,c),Z=i(c),W=b(c,"P",{"data-svelte-h":!0}),j(W)!=="svelte-ianwj2"&&(W.innerHTML=L),H=i(c),$(R.$$.fragment,c)},m(c,U){r(c,s,U),r(c,t,U),u(n,c,U),r(c,f,U),u(m,c,U),r(c,k,U),r(c,v,U),r(c,G,U),u(T,c,U),r(c,Z,U),r(c,W,U),r(c,H,U),u(R,c,U),V=!0},p(c,U){const N={};U&2&&(N.$$scope={dirty:U,ctx:c}),m.$set(N)},i(c){V||(h(n.$$.fragment,c),h(m.$$.fragment,c),h(T.$$.fragment,c),h(R.$$.fragment,c),V=!0)},o(c){g(n.$$.fragment,c),g(m.$$.fragment,c),g(T.$$.fragment,c),g(R.$$.fragment,c),V=!1},d(c){c&&(a(s),a(t),a(f),a(k),a(v),a(G),a(Z),a(W),a(H)),M(n,c),M(m,c),M(T,c),M(R,c)}}}function Pa(w){let s,p;return s=new E({props:{$$slots:{default:[Sa]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function Oa(w){let s,p='通过 <a href="./task_summary">任务摘要</a> 查找 <code>AutoModel</code> 支持的任务.';return{c(){s=y("p"),s.innerHTML=p},l(t){s=b(t,"P",{"data-svelte-h":!0}),j(s)!=="svelte-1mg580k"&&(s.innerHTML=p)},m(t,n){r(t,s,n)},p:I,d(t){t&&a(s)}}}function Da(w){let s,p="🤗 Transformers 提供了一种简单统一的方式来加载预训练的实例。这表示你可以像加载 <code>AutoTokenizer</code> 一样加载 <code>TFAutoModel</code>。唯一不同的地方是为你的任务选择正确的 <code>TFAutoModel</code>，对于文本（或序列）分类，你应该加载 <code>TFAutoModelForSequenceClassification</code>：",t,n,f,m,k,v,C="现在通过直接将字典的键传给张量，将预处理的输入批次传给模型。",G,T,Z,W,L="模型在 <code>logits</code> 属性输出最终的激活结果。在 <code>logits</code> 上应用softmax函数来查询概率：",H,R,V;return n=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsX25hbWUlMjAlM0QlMjAlMjJubHB0b3duJTJGYmVydC1iYXNlLW11bHRpbGluZ3VhbC11bmNhc2VkLXNlbnRpbWVudCUyMiUwQXRmX21vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,wrap:!1}}),m=new St({props:{$$slots:{default:[Oa]},$$scope:{ctx:w}}}),T=new _({props:{code:"dGZfb3V0cHV0cyUyMCUzRCUyMHRmX21vZGVsKHRmX2JhdGNoKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)',wrap:!1}}),R=new _({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEF0Zl9wcmVkaWN0aW9ucyUyMCUzRCUyMHRmLm5uLnNvZnRtYXgodGZfb3V0cHV0cy5sb2dpdHMlMkMlMjBheGlzJTNELTEpJTBBdGZfcHJlZGljdGlvbnM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`,wrap:!1}}),{c(){s=y("p"),s.innerHTML=p,t=o(),d(n.$$.fragment),f=o(),d(m.$$.fragment),k=o(),v=y("p"),v.textContent=C,G=o(),d(T.$$.fragment),Z=o(),W=y("p"),W.innerHTML=L,H=o(),d(R.$$.fragment)},l(c){s=b(c,"P",{"data-svelte-h":!0}),j(s)!=="svelte-koi20"&&(s.innerHTML=p),t=i(c),$(n.$$.fragment,c),f=i(c),$(m.$$.fragment,c),k=i(c),v=b(c,"P",{"data-svelte-h":!0}),j(v)!=="svelte-16cvbg6"&&(v.textContent=C),G=i(c),$(T.$$.fragment,c),Z=i(c),W=b(c,"P",{"data-svelte-h":!0}),j(W)!=="svelte-1aahzm4"&&(W.innerHTML=L),H=i(c),$(R.$$.fragment,c)},m(c,U){r(c,s,U),r(c,t,U),u(n,c,U),r(c,f,U),u(m,c,U),r(c,k,U),r(c,v,U),r(c,G,U),u(T,c,U),r(c,Z,U),r(c,W,U),r(c,H,U),u(R,c,U),V=!0},p(c,U){const N={};U&2&&(N.$$scope={dirty:U,ctx:c}),m.$set(N)},i(c){V||(h(n.$$.fragment,c),h(m.$$.fragment,c),h(T.$$.fragment,c),h(R.$$.fragment,c),V=!0)},o(c){g(n.$$.fragment,c),g(m.$$.fragment,c),g(T.$$.fragment,c),g(R.$$.fragment,c),V=!1},d(c){c&&(a(s),a(t),a(f),a(k),a(v),a(G),a(Z),a(W),a(H)),M(n,c),M(m,c),M(T,c),M(R,c)}}}function Ka(w){let s,p;return s=new E({props:{$$slots:{default:[Da]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function en(w){let s,p=`所有 🤗 Transformers 模型（PyTorch 或 TensorFlow）在最终的激活函数（比如 softmax）<em>之前</em> 输出张量，
因为最终的激活函数常常与 loss 融合。模型的输出是特殊的数据类，所以它们的属性可以在 IDE 中被自动补全。模型的输出就像一个元组或字典（你可以通过整数、切片或字符串来索引它），在这种情况下，为 None 的属性会被忽略。`;return{c(){s=y("p"),s.innerHTML=p},l(t){s=b(t,"P",{"data-svelte-h":!0}),j(s)!=="svelte-pdy338"&&(s.innerHTML=p)},m(t,n){r(t,s,n)},p:I,d(t){t&&a(s)}}}function tn(w){let s,p='当你的模型微调完成，你就可以使用 <a href="/docs/transformers/main/zh/main_classes/model#transformers.PreTrainedModel.save_pretrained">PreTrainedModel.save_pretrained()</a> 把它和它的分词器保存下来：',t,n,f,m,k='当你准备再次使用这个模型时，就可以使用 <a href="/docs/transformers/main/zh/main_classes/model#transformers.PreTrainedModel.from_pretrained">PreTrainedModel.from_pretrained()</a> 加载它了：',v,C,G;return n=new _({props:{code:"cHRfc2F2ZV9kaXJlY3RvcnklMjAlM0QlMjAlMjIuJTJGcHRfc2F2ZV9wcmV0cmFpbmVkJTIyJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZChwdF9zYXZlX2RpcmVjdG9yeSklMEFwdF9tb2RlbC5zYXZlX3ByZXRyYWluZWQocHRfc2F2ZV9kaXJlY3Rvcnkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`,wrap:!1}}),C=new _({props:{code:"cHRfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfc2F2ZV9wcmV0cmFpbmVkJTIyKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)',wrap:!1}}),{c(){s=y("p"),s.innerHTML=p,t=o(),d(n.$$.fragment),f=o(),m=y("p"),m.innerHTML=k,v=o(),d(C.$$.fragment)},l(T){s=b(T,"P",{"data-svelte-h":!0}),j(s)!=="svelte-1h1ryry"&&(s.innerHTML=p),t=i(T),$(n.$$.fragment,T),f=i(T),m=b(T,"P",{"data-svelte-h":!0}),j(m)!=="svelte-134z9py"&&(m.innerHTML=k),v=i(T),$(C.$$.fragment,T)},m(T,Z){r(T,s,Z),r(T,t,Z),u(n,T,Z),r(T,f,Z),r(T,m,Z),r(T,v,Z),u(C,T,Z),G=!0},p:I,i(T){G||(h(n.$$.fragment,T),h(C.$$.fragment,T),G=!0)},o(T){g(n.$$.fragment,T),g(C.$$.fragment,T),G=!1},d(T){T&&(a(s),a(t),a(f),a(m),a(v)),M(n,T),M(C,T)}}}function sn(w){let s,p;return s=new E({props:{$$slots:{default:[tn]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function ln(w){let s,p='当你的模型微调完成，你就可以使用 <a href="/docs/transformers/main/zh/main_classes/model#transformers.TFPreTrainedModel.save_pretrained">TFPreTrainedModel.save_pretrained()</a> 把它和它的分词器保存下来：',t,n,f,m,k='当你准备再次使用这个模型时，就可以使用 <a href="/docs/transformers/main/zh/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">TFPreTrainedModel.from_pretrained()</a> 加载它了：',v,C,G;return n=new _({props:{code:"dGZfc2F2ZV9kaXJlY3RvcnklMjAlM0QlMjAlMjIuJTJGdGZfc2F2ZV9wcmV0cmFpbmVkJTIyJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZCh0Zl9zYXZlX2RpcmVjdG9yeSklMEF0Zl9tb2RlbC5zYXZlX3ByZXRyYWluZWQodGZfc2F2ZV9kaXJlY3Rvcnkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`,wrap:!1}}),C=new _({props:{code:"dGZfbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0Zl9zYXZlX3ByZXRyYWluZWQlMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)',wrap:!1}}),{c(){s=y("p"),s.innerHTML=p,t=o(),d(n.$$.fragment),f=o(),m=y("p"),m.innerHTML=k,v=o(),d(C.$$.fragment)},l(T){s=b(T,"P",{"data-svelte-h":!0}),j(s)!=="svelte-1xxt8fu"&&(s.innerHTML=p),t=i(T),$(n.$$.fragment,T),f=i(T),m=b(T,"P",{"data-svelte-h":!0}),j(m)!=="svelte-1j9gjai"&&(m.innerHTML=k),v=i(T),$(C.$$.fragment,T)},m(T,Z){r(T,s,Z),r(T,t,Z),u(n,T,Z),r(T,f,Z),r(T,m,Z),r(T,v,Z),u(C,T,Z),G=!0},p:I,i(T){G||(h(n.$$.fragment,T),h(C.$$.fragment,T),G=!0)},o(T){g(n.$$.fragment,T),g(C.$$.fragment,T),G=!1},d(T){T&&(a(s),a(t),a(f),a(m),a(v)),M(n,T),M(C,T)}}}function an(w){let s,p;return s=new E({props:{$$slots:{default:[ln]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function nn(w){let s,p;return s=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKHRmX3NhdmVfZGlyZWN0b3J5KSUwQXB0X21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQodGZfc2F2ZV9kaXJlY3RvcnklMkMlMjBmcm9tX3RmJTNEVHJ1ZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`,wrap:!1}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p:I,i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function rn(w){let s,p;return s=new E({props:{$$slots:{default:[nn]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function pn(w){let s,p;return s=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQocHRfc2F2ZV9kaXJlY3RvcnkpJTBBdGZfbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKHB0X3NhdmVfZGlyZWN0b3J5JTJDJTIwZnJvbV9wdCUzRFRydWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`,wrap:!1}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p:I,i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function on(w){let s,p;return s=new E({props:{$$slots:{default:[pn]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function mn(w){let s,p="使用 <code>AutoModel.from_config()</code> 根据你的自定义配置创建一个模型：",t,n,f;return n=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUwQSUwQW15X21vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fY29uZmlnKG15X2NvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = AutoModel.from_config(my_config)`,wrap:!1}}),{c(){s=y("p"),s.innerHTML=p,t=o(),d(n.$$.fragment)},l(m){s=b(m,"P",{"data-svelte-h":!0}),j(s)!=="svelte-kq2sqw"&&(s.innerHTML=p),t=i(m),$(n.$$.fragment,m)},m(m,k){r(m,s,k),r(m,t,k),u(n,m,k),f=!0},p:I,i(m){f||(h(n.$$.fragment,m),f=!0)},o(m){g(n.$$.fragment,m),f=!1},d(m){m&&(a(s),a(t)),M(n,m)}}}function cn(w){let s,p;return s=new E({props:{$$slots:{default:[mn]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function fn(w){let s,p="使用 <code>TFAutoModel.from_config()</code> 根据你的自定义配置创建一个模型：",t,n,f;return n=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsJTBBJTBBbXlfbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbC5mcm9tX2NvbmZpZyhteV9jb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = TFAutoModel.from_config(my_config)`,wrap:!1}}),{c(){s=y("p"),s.innerHTML=p,t=o(),d(n.$$.fragment)},l(m){s=b(m,"P",{"data-svelte-h":!0}),j(s)!=="svelte-1obn2o6"&&(s.innerHTML=p),t=i(m),$(n.$$.fragment,m)},m(m,k){r(m,s,k),r(m,t,k),u(n,m,k),f=!0},p:I,i(m){f||(h(n.$$.fragment,m),f=!0)},o(m){g(n.$$.fragment,m),f=!1},d(m){m&&(a(s),a(t)),M(n,m)}}}function dn(w){let s,p;return s=new E({props:{$$slots:{default:[fn]},$$scope:{ctx:w}}}),{c(){d(s.$$.fragment)},l(t){$(s.$$.fragment,t)},m(t,n){u(s,t,n),p=!0},p(t,n){const f={};n&2&&(f.$$scope={dirty:n,ctx:t}),s.$set(f)},i(t){p||(h(s.$$.fragment,t),p=!0)},o(t){g(s.$$.fragment,t),p=!1},d(t){M(s,t)}}}function $n(w){let s,p="对于像翻译或摘要这些使用序列到序列模型的任务，用 <code>Seq2SeqTrainer</code> 和 <code>Seq2SeqTrainingArguments</code> 来替代。";return{c(){s=y("p"),s.innerHTML=p},l(t){s=b(t,"P",{"data-svelte-h":!0}),j(s)!=="svelte-d3rcx4"&&(s.innerHTML=p)},m(t,n){r(t,s,n)},p:I,d(t){t&&a(s)}}}function un(w){let s,p,t,n,f,m,k,v,C,G='快来使用 🤗 Transformers 吧！无论你是开发人员还是日常用户，这篇快速上手教程都将帮助你入门并且向你展示如何使用 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 进行推理，使用 <a href="./model_doc/auto">AutoClass</a> 加载一个预训练模型和预处理器，以及使用 PyTorch 或 TensorFlow 快速训练一个模型。如果你是一个初学者，我们建议你接下来查看我们的教程或者<a href="https://huggingface.co/course/chapter1/1" rel="nofollow">课程</a>，来更深入地了解在这里介绍到的概念。',T,Z,W="在开始之前，确保你已经安装了所有必要的库：",L,H,R,V,c="你还需要安装喜欢的机器学习框架：",U,N,Pt,te,Ot,se,Dt,le,Xl='使用 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 是利用预训练模型进行推理的最简单的方式。你能够将 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 开箱即用地用于跨不同模态的多种任务。来看看它支持的任务列表：',Kt,ae,Fl="<thead><tr><th><strong>任务</strong></th> <th><strong>描述</strong></th> <th><strong>模态</strong></th> <th><strong>Pipeline</strong></th></tr></thead> <tbody><tr><td>文本分类</td> <td>为给定的文本序列分配一个标签</td> <td>NLP</td> <td>pipeline(task=“sentiment-analysis”)</td></tr> <tr><td>文本生成</td> <td>根据给定的提示生成文本</td> <td>NLP</td> <td>pipeline(task=“text-generation”)</td></tr> <tr><td>命名实体识别</td> <td>为序列里的每个 token 分配一个标签（人, 组织, 地址等等）</td> <td>NLP</td> <td>pipeline(task=“ner”)</td></tr> <tr><td>问答系统</td> <td>通过给定的上下文和问题, 在文本中提取答案</td> <td>NLP</td> <td>pipeline(task=“question-answering”)</td></tr> <tr><td>掩盖填充</td> <td>预测出正确的在序列中被掩盖的token</td> <td>NLP</td> <td>pipeline(task=“fill-mask”)</td></tr> <tr><td>文本摘要</td> <td>为文本序列或文档生成总结</td> <td>NLP</td> <td>pipeline(task=“summarization”)</td></tr> <tr><td>文本翻译</td> <td>将文本从一种语言翻译为另一种语言</td> <td>NLP</td> <td>pipeline(task=“translation”)</td></tr> <tr><td>图像分类</td> <td>为图像分配一个标签</td> <td>Computer vision</td> <td>pipeline(task=“image-classification”)</td></tr> <tr><td>图像分割</td> <td>为图像中每个独立的像素分配标签（支持语义、全景和实例分割）</td> <td>Computer vision</td> <td>pipeline(task=“image-segmentation”)</td></tr> <tr><td>目标检测</td> <td>预测图像中目标对象的边界框和类别</td> <td>Computer vision</td> <td>pipeline(task=“object-detection”)</td></tr> <tr><td>音频分类</td> <td>给音频文件分配一个标签</td> <td>Audio</td> <td>pipeline(task=“audio-classification”)</td></tr> <tr><td>自动语音识别</td> <td>将音频文件中的语音提取为文本</td> <td>Audio</td> <td>pipeline(task=“automatic-speech-recognition”)</td></tr> <tr><td>视觉问答</td> <td>给定一个图像和一个问题，正确地回答有关图像的问题</td> <td>Multimodal</td> <td>pipeline(task=“vqa”)</td></tr></tbody>",es,ne,zl='创建一个 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 实例并且指定你想要将它用于的任务，就可以开始了。你可以将 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 用于任何一个上面提到的任务，如果想知道支持的任务的完整列表，可以查阅 <a href="./main_classes/pipelines">pipeline API 参考</a>。不过, 在这篇教程中，你将把 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 用在一个情感分析示例上：',ts,re,ss,pe,Nl='<a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 会下载并缓存一个用于情感分析的默认的<a href="https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english" rel="nofollow">预训练模型</a>和分词器。现在你可以在目标文本上使用 <code>classifier</code> 了：',ls,oe,as,ie,xl='如果你有不止一个输入，可以把所有输入放入一个列表然后传给<a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a>，它将会返回一个字典列表：',ns,me,rs,ce,El='<a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 也可以为任何你喜欢的任务遍历整个数据集。在下面这个示例中，让我们选择自动语音识别作为我们的任务：',ps,fe,os,de,Yl='加载一个你想遍历的音频数据集（查阅 🤗 Datasets <a href="https://huggingface.co/docs/datasets/quickstart#audio" rel="nofollow">快速开始</a> 获得更多信息）。比如，加载 <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">MInDS-14</a> 数据集：',is,$e,ms,ue,Bl='你需要确保数据集中的音频的采样率与 <a href="https://huggingface.co/facebook/wav2vec2-base-960h" rel="nofollow"><code>facebook/wav2vec2-base-960h</code></a> 训练用到的音频的采样率一致：',cs,he,fs,ge,Ll=`当调用 <code>&quot;audio&quot;</code> 列时, 音频文件将会自动加载并重采样。
从前四个样本中提取原始波形数组，将它作为列表传给 pipeline：`,ds,Me,$s,ye,Al='对于输入非常庞大的大型数据集（比如语音或视觉），你会想到使用一个生成器，而不是一个将所有输入都加载进内存的列表。查阅 <a href="./main_classes/pipelines">pipeline API 参考</a> 来获取更多信息。',us,be,hs,Te,Ql='<a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 可以容纳 <a href="https://huggingface.co/models" rel="nofollow">Hub</a> 中的任何模型，这让 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 更容易适用于其他用例。比如，你想要一个能够处理法语文本的模型，就可以使用 Hub 上的标记来筛选出合适的模型。靠前的筛选结果会返回一个为情感分析微调的多语言的 <a href="https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment" rel="nofollow">BERT 模型</a>，你可以将它用于法语文本：',gs,je,Ms,A,ys,we,ql='在 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 中指定模型和分词器，现在你就可以在法语文本上使用 <code>classifier</code> 了：',bs,_e,Ts,Je,Sl='如果你没有找到适合你的模型，就需要在你的数据上微调一个预训练模型了。查看 <a href="./training">微调教程</a> 来学习怎样进行微调。最后，微调完模型后，考虑一下在 Hub 上与社区 <a href="./model_sharing">分享</a> 这个模型，把机器学习普及到每一个人! 🤗',js,ke,ws,Ue,_s,Ze,Pl='在幕后，是由 <code>AutoModelForSequenceClassification</code> 和 <code>AutoTokenizer</code> 一起支持你在上面用到的 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a>。<a href="./model_doc/auto">AutoClass</a> 是一个能够通过预训练模型的名称或路径自动查找其架构的快捷方式。你只需要为你的任务选择合适的 <code>AutoClass</code> 和它关联的预处理类。',Js,ve,Ol='让我们回过头来看上一节的示例，看看怎样使用 <code>AutoClass</code> 来重现使用 <a href="/docs/transformers/main/zh/main_classes/pipelines#transformers.pipeline">pipeline()</a> 的结果。',ks,Ce,Us,Ge,Dl='分词器负责预处理文本，将文本转换为用于输入模型的数字数组。有多个用来管理分词过程的规则，包括如何拆分单词和在什么样的级别上拆分单词（在 <a href="./tokenizer_summary">分词器总结</a> 学习更多关于分词的信息）。要记住最重要的是你需要实例化的分词器要与模型的名称相同, 来确保和模型训练时使用相同的分词规则。',Zs,Re,Kl="使用 <code>AutoTokenizer</code> 加载一个分词器:",vs,We,Cs,He,ea="将文本传入分词器：",Gs,Ve,Rs,Ie,ta="分词器返回了含有如下内容的字典:",Ws,Xe,sa='<li><a href="./glossary#input-ids">input_ids</a>：用数字表示的 token。</li> <li><a href=".glossary#attention-mask">attention_mask</a>：应该关注哪些 token 的指示。</li>',Hs,Fe,la="分词器也可以接受列表作为输入，并填充和截断文本，返回具有统一长度的批次：",Vs,Q,Is,q,Xs,ze,Fs,S,zs,P,Ns,Ne,xs,O,Es,xe,aa="🤗 Transformers 有一个特别酷的功能，它能够保存一个模型，并且将它加载为 PyTorch 或 TensorFlow 模型。<code>from_pt</code> 或 <code>from_tf</code> 参数可以将模型从一个框架转换为另一个框架：",Ys,D,Bs,Ee,Ls,Ye,na="你可以修改模型的配置类来改变模型的构建方式。配置指明了模型的属性，比如隐藏层或者注意力头的数量。当你从自定义的配置类初始化模型时，你就开始自定义模型构建了。模型属性是随机初始化的，你需要先训练模型，然后才能得到有意义的结果。",As,Be,ra="通过导入 <code>AutoConfig</code> 来开始，之后加载你想修改的预训练模型。在 <code>AutoConfig.from_pretrained()</code> 中，你能够指定想要修改的属性，比如注意力头的数量：",Qs,Le,qs,K,Ss,Ae,pa='查阅 <a href="./create_a_model">创建一个自定义结构</a> 指南获取更多关于构建自定义配置的信息。',Ps,Qe,Os,qe,oa='所有的模型都是标准的 <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow"><code>torch.nn.Module</code></a>，所以你可以在任何典型的训练模型中使用它们。当你编写自己的训练循环时，🤗 Transformers 为 PyTorch 提供了一个 <code>Trainer</code> 类，它包含了基础的训练循环并且为诸如分布式训练，混合精度等特性增加了额外的功能。',Ds,Se,ia="取决于你的任务, 你通常可以传递以下的参数给 <code>Trainer</code>：",Ks,X,Pe,Xt,ma='<a href="/docs/transformers/main/zh/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a> 或者 <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow"><code>torch.nn.Module</code></a>：',gl,Oe,Ml,De,Ft,ca="<code>TrainingArguments</code> 含有你可以修改的模型超参数，比如学习率，批次大小和训练时的迭代次数。如果你没有指定训练参数，那么它会使用默认值：",yl,Ke,bl,et,zt,fa="一个预处理类，比如分词器，特征提取器或者处理器：",Tl,tt,jl,st,Nt,da="加载一个数据集：",wl,lt,_l,at,xt,$a="创建一个给数据集分词的函数，并且使用 <code>map</code> 应用到整个数据集：",Jl,nt,kl,rt,Et,ua='用来从数据集中创建批次的 <a href="/docs/transformers/main/zh/main_classes/data_collator#transformers.DataCollatorWithPadding">DataCollatorWithPadding</a>：',Ul,pt,el,ot,ha="现在把所有的类传给 <code>Trainer</code>：",tl,it,sl,mt,ga="一切准备就绪后，调用 <code>train()</code> 进行训练：",ll,ct,al,ee,nl,ft,Ma="你可以通过子类化 <code>Trainer</code> 中的方法来自定义训练循环。这样你就可以自定义像损失函数，优化器和调度器这样的特性。查阅 <code>Trainer</code> 参考手册了解哪些方法能够被子类化。",rl,dt,ya='另一个自定义训练循环的方式是通过<a href="./main_classes/callbacks">回调</a>。你可以使用回调来与其他库集成，查看训练循环来报告进度或提前结束训练。回调不会修改训练循环。如果想自定义损失函数等，就需要子类化 <code>Trainer</code> 了。',pl,$t,ol,ut,ba='所有模型都是标准的 <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow"><code>tf.keras.Model</code></a>，所以你可以通过 <a href="https://keras.io/" rel="nofollow">Keras</a> API 实现在 Tensorflow 中训练。🤗 Transformers 提供了 <a href="/docs/transformers/main/zh/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset">prepare_tf_dataset()</a> 方法来轻松地将数据集加载为 <code>tf.data.Dataset</code>，这样你就可以使用 Keras 的 <a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a> 和 <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a> 方法马上开始训练。',il,x,ht,Yt,Ta='使用 <a href="/docs/transformers/main/zh/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a> 或者 <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow"><code>tf.keras.Model</code></a> 来开始：',Zl,gt,vl,Mt,Bt,ja="一个预处理类，比如分词器，特征提取器或者处理器：",Cl,yt,Gl,bt,Lt,wa="创建一个给数据集分词的函数",Rl,Tt,Wl,jt,At,_a='使用 <code>map</code> 将分词器应用到整个数据集，之后将数据集和分词器传给 <a href="/docs/transformers/main/zh/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset">prepare_tf_dataset()</a>。如果你需要的话，也可以在这里改变批次大小和是否打乱数据集：',Hl,wt,Vl,_t,Qt,Ja="一切准备就绪后，调用 <code>compile</code> 和 <code>fit</code> 开始训练：",Il,Jt,ml,kt,cl,Ut,ka="现在你已经完成了 🤗 Transformers 的快速上手教程，来看看我们的指南并且学习如何做一些更具体的事情，比如写一个自定义模型，为某个任务微调一个模型以及如何使用脚本来训练模型。如果你有兴趣了解更多 🤗 Transformers 的核心章节，那就喝杯咖啡然后来看看我们的概念指南吧！",fl,qt,dl;return f=new B({props:{title:"快速上手",local:"快速上手",headingTag:"h1"}}),k=new Ha({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/zh/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/zh/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/zh/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/zh/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/zh/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/zh/tensorflow/quicktour.ipynb"}]}}),H=new _({props:{code:"IXBpcCUyMGluc3RhbGwlMjB0cmFuc2Zvcm1lcnMlMjBkYXRhc2V0cw==",highlighted:"!pip install transformers datasets",wrap:!1}}),N=new It({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Fa],pytorch:[Ia]},$$scope:{ctx:w}}}),te=new B({props:{title:"Pipeline",local:"pipeline",headingTag:"h2"}}),se=new Za({props:{id:"tiZFewofSLM"}}),re=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMnNlbnRpbWVudC1hbmFseXNpcyUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`,wrap:!1}}),oe=new _({props:{code:"Y2xhc3NpZmllciglMjJXZSUyMGFyZSUyMHZlcnklMjBoYXBweSUyMHRvJTIwc2hvdyUyMHlvdSUyMHRoZSUyMCVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycyUyMGxpYnJhcnkuJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`,wrap:!1}}),me=new _({props:{code:"cmVzdWx0cyUyMCUzRCUyMGNsYXNzaWZpZXIoJTVCJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiUyQyUyMCUyMldlJTIwaG9wZSUyMHlvdSUyMGRvbid0JTIwaGF0ZSUyMGl0LiUyMiU1RCklMEFmb3IlMjByZXN1bHQlMjBpbiUyMHJlc3VsdHMlM0ElMEElMjAlMjAlMjAlMjBwcmludChmJTIybGFiZWwlM0ElMjAlN0JyZXN1bHQlNUInbGFiZWwnJTVEJTdEJTJDJTIwd2l0aCUyMHNjb3JlJTNBJTIwJTdCcm91bmQocmVzdWx0JTVCJ3Njb3JlJyU1RCUyQyUyMDQpJTdEJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`,wrap:!1}}),fe=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFzcGVlY2hfcmVjb2duaXplciUyMCUzRCUyMHBpcGVsaW5lKCUyMmF1dG9tYXRpYy1zcGVlY2gtcmVjb2duaXRpb24lMjIlMkMlMjBtb2RlbCUzRCUyMmZhY2Vib29rJTJGd2F2MnZlYzItYmFzZS05NjBoJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`,wrap:!1}}),$e=new _({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMlBvbHlBSSUyRm1pbmRzMTQlMjIlMkMlMjBuYW1lJTNEJTIyZW4tVVMlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`,wrap:!1}}),he=new _({props:{code:"ZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQuY2FzdF9jb2x1bW4oJTIyYXVkaW8lMjIlMkMlMjBBdWRpbyhzYW1wbGluZ19yYXRlJTNEc3BlZWNoX3JlY29nbml6ZXIuZmVhdHVyZV9leHRyYWN0b3Iuc2FtcGxpbmdfcmF0ZSkp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))',wrap:!1}}),Me=new _({props:{code:"cmVzdWx0JTIwJTNEJTIwc3BlZWNoX3JlY29nbml6ZXIoZGF0YXNldCU1QiUzQTQlNUQlNUIlMjJhdWRpbyUyMiU1RCklMEFwcmludCglNUJkJTVCJTIydGV4dCUyMiU1RCUyMGZvciUyMGQlMjBpbiUyMHJlc3VsdCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>result = speech_recognizer(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([d[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> result])
[<span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>, <span class="hljs-string">&quot;FODING HOW I&#x27;D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>, <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE AP SO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>, <span class="hljs-string">&#x27;HOW DO I THURN A JOIN A COUNT&#x27;</span>]`,wrap:!1}}),be=new B({props:{title:"在 pipeline 中使用另一个模型和分词器",local:"在-pipeline-中使用另一个模型和分词器",headingTag:"h3"}}),je=new _({props:{code:"bW9kZWxfbmFtZSUyMCUzRCUyMCUyMm5scHRvd24lMkZiZXJ0LWJhc2UtbXVsdGlsaW5ndWFsLXVuY2FzZWQtc2VudGltZW50JTIy",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>',wrap:!1}}),A=new It({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ea],pytorch:[Na]},$$scope:{ctx:w}}}),_e=new _({props:{code:"Y2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMnNlbnRpbWVudC1hbmFseXNpcyUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIpJTBBY2xhc3NpZmllciglMjJOb3VzJTIwc29tbWVzJTIwdHIlQzMlQThzJTIwaGV1cmV1eCUyMGRlJTIwdm91cyUyMHByJUMzJUE5c2VudGVyJTIwbGElMjBiaWJsaW90aCVDMyVBOHF1ZSUyMCVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycy4lMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`,wrap:!1}}),ke=new B({props:{title:"AutoClass",local:"autoclass",headingTag:"h2"}}),Ue=new Za({props:{id:"AhChOFRegn4"}}),Ce=new B({props:{title:"AutoTokenizer",local:"autotokenizer",headingTag:"h3"}}),We=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEFtb2RlbF9uYW1lJTIwJTNEJTIwJTIybmxwdG93biUyRmJlcnQtYmFzZS1tdWx0aWxpbmd1YWwtdW5jYXNlZC1zZW50aW1lbnQlMjIlMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`,wrap:!1}}),Ve=new _({props:{code:"ZW5jb2RpbmclMjAlM0QlMjB0b2tlbml6ZXIoJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiklMEFwcmludChlbmNvZGluZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`,wrap:!1}}),Q=new It({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Aa],pytorch:[Ba]},$$scope:{ctx:w}}}),q=new St({props:{$$slots:{default:[Qa]},$$scope:{ctx:w}}}),ze=new B({props:{title:"AutoModel",local:"automodel",headingTag:"h3"}}),S=new It({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ka],pytorch:[Pa]},$$scope:{ctx:w}}}),P=new St({props:{$$slots:{default:[en]},$$scope:{ctx:w}}}),Ne=new B({props:{title:"保存模型",local:"保存模型",headingTag:"h3"}}),O=new It({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[an],pytorch:[sn]},$$scope:{ctx:w}}}),D=new It({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[on],pytorch:[rn]},$$scope:{ctx:w}}}),Ee=new B({props:{title:"自定义模型构建",local:"自定义模型构建",headingTag:"h2"}}),Le=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMEElMEFteV9jb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIlMkMlMjBuX2hlYWRzJTNEMTIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>my_config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>, n_heads=<span class="hljs-number">12</span>)`,wrap:!1}}),K=new It({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[dn],pytorch:[cn]},$$scope:{ctx:w}}}),Qe=new B({props:{title:"Trainer - PyTorch 优化训练循环",local:"trainer---pytorch-优化训练循环",headingTag:"h2"}}),Oe=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),Ke=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluaW5nQXJndW1lbnRzJTBBJTBBdHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJwYXRoJTJGdG8lMkZzYXZlJTJGZm9sZGVyJTJGJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlM0Q4JTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV9ldmFsX2JhdGNoX3NpemUlM0Q4JTJDJTBBJTIwJTIwJTIwJTIwbnVtX3RyYWluX2Vwb2NocyUzRDIlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;path/to/save/folder/&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">8</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">8</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),tt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),lt=new _({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJyb3R0ZW5fdG9tYXRvZXMlMjIpJTIwJTIwJTIzJTIwZG9jdGVzdCUzQSUyMCUyQklHTk9SRV9SRVNVTFQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;rotten_tomatoes&quot;</span>)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>`,wrap:!1}}),nt=new _({props:{code:"ZGVmJTIwdG9rZW5pemVfZGF0YXNldChkYXRhc2V0KSUzQSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMHRva2VuaXplcihkYXRhc2V0JTVCJTIydGV4dCUyMiU1RCklMEElMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5tYXAodG9rZW5pemVfZGF0YXNldCUyQyUyMGJhdGNoZWQlM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_dataset</span>(<span class="hljs-params">dataset</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(dataset[<span class="hljs-string">&quot;text&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(tokenize_dataset, batched=<span class="hljs-literal">True</span>)`,wrap:!1}}),pt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nJTBBJTBBZGF0YV9jb2xsYXRvciUyMCUzRCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nKHRva2VuaXplciUzRHRva2VuaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,wrap:!1}}),it=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluZXIlMEElMEF0cmFpbmVyJTIwJTNEJTIwVHJhaW5lciglMEElMjAlMjAlMjAlMjBtb2RlbCUzRG1vZGVsJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEZGF0YXNldCU1QiUyMnRyYWluJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9kYXRhc2V0JTNEZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlMkMlMEElMjAlMjAlMjAlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMEElMjAlMjAlMjAlMjBkYXRhX2NvbGxhdG9yJTNEZGF0YV9jb2xsYXRvciUyQyUwQSklMjAlMjAlMjMlMjBkb2N0ZXN0JTNBJTIwJTJCU0tJUA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=dataset[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=dataset[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)  <span class="hljs-comment"># doctest: +SKIP</span>`,wrap:!1}}),ct=new _({props:{code:"dHJhaW5lci50cmFpbigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()',wrap:!1}}),ee=new St({props:{$$slots:{default:[$n]},$$scope:{ctx:w}}}),$t=new B({props:{title:"使用 Tensorflow 训练",local:"使用-tensorflow-训练",headingTag:"h2"}}),gt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),yt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),Tt=new _({props:{code:"ZGVmJTIwdG9rZW5pemVfZGF0YXNldChkYXRhc2V0KSUzQSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMHRva2VuaXplcihkYXRhc2V0JTVCJTIydGV4dCUyMiU1RCklMjAlMjAlMjMlMjBkb2N0ZXN0JTNBJTIwJTJCU0tJUA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_dataset</span>(<span class="hljs-params">dataset</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(dataset[<span class="hljs-string">&quot;text&quot;</span>])  <span class="hljs-comment"># doctest: +SKIP</span>`,wrap:!1}}),wt=new _({props:{code:"ZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQubWFwKHRva2VuaXplX2RhdGFzZXQpJTIwJTIwJTIzJTIwZG9jdGVzdCUzQSUyMCUyQlNLSVAlMEF0Zl9kYXRhc2V0JTIwJTNEJTIwbW9kZWwucHJlcGFyZV90Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMGRhdGFzZXQlMkMlMjBiYXRjaF9zaXplJTNEMTYlMkMlMjBzaHVmZmxlJTNEVHJ1ZSUyQyUyMHRva2VuaXplciUzRHRva2VuaXplciUwQSklMjAlMjAlMjMlMjBkb2N0ZXN0JTNBJTIwJTJCU0tJUA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(tokenize_dataset)  <span class="hljs-comment"># doctest: +SKIP</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_dataset = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    dataset, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>, tokenizer=tokenizer
<span class="hljs-meta">... </span>)  <span class="hljs-comment"># doctest: +SKIP</span>`,wrap:!1}}),Jt=new _({props:{code:"ZnJvbSUyMHRlbnNvcmZsb3cua2VyYXMub3B0aW1pemVycyUyMGltcG9ydCUyMEFkYW0lMEElMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzREFkYW0oM2UtNSkpJTBBbW9kZWwuZml0KGRhdGFzZXQpJTIwJTIwJTIzJTIwZG9jdGVzdCUzQSUyMCUyQlNLSVA=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">3e-5</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(dataset)  <span class="hljs-comment"># doctest: +SKIP</span>`,wrap:!1}}),kt=new B({props:{title:"接下来做什么?",local:"接下来做什么",headingTag:"h2"}}),{c(){s=y("meta"),p=o(),t=y("p"),n=o(),d(f.$$.fragment),m=o(),d(k.$$.fragment),v=o(),C=y("p"),C.innerHTML=G,T=o(),Z=y("p"),Z.textContent=W,L=o(),d(H.$$.fragment),R=o(),V=y("p"),V.textContent=c,U=o(),d(N.$$.fragment),Pt=o(),d(te.$$.fragment),Ot=o(),d(se.$$.fragment),Dt=o(),le=y("p"),le.innerHTML=Xl,Kt=o(),ae=y("table"),ae.innerHTML=Fl,es=o(),ne=y("p"),ne.innerHTML=zl,ts=o(),d(re.$$.fragment),ss=o(),pe=y("p"),pe.innerHTML=Nl,ls=o(),d(oe.$$.fragment),as=o(),ie=y("p"),ie.innerHTML=xl,ns=o(),d(me.$$.fragment),rs=o(),ce=y("p"),ce.innerHTML=El,ps=o(),d(fe.$$.fragment),os=o(),de=y("p"),de.innerHTML=Yl,is=o(),d($e.$$.fragment),ms=o(),ue=y("p"),ue.innerHTML=Bl,cs=o(),d(he.$$.fragment),fs=o(),ge=y("p"),ge.innerHTML=Ll,ds=o(),d(Me.$$.fragment),$s=o(),ye=y("p"),ye.innerHTML=Al,us=o(),d(be.$$.fragment),hs=o(),Te=y("p"),Te.innerHTML=Ql,gs=o(),d(je.$$.fragment),Ms=o(),d(A.$$.fragment),ys=o(),we=y("p"),we.innerHTML=ql,bs=o(),d(_e.$$.fragment),Ts=o(),Je=y("p"),Je.innerHTML=Sl,js=o(),d(ke.$$.fragment),ws=o(),d(Ue.$$.fragment),_s=o(),Ze=y("p"),Ze.innerHTML=Pl,Js=o(),ve=y("p"),ve.innerHTML=Ol,ks=o(),d(Ce.$$.fragment),Us=o(),Ge=y("p"),Ge.innerHTML=Dl,Zs=o(),Re=y("p"),Re.innerHTML=Kl,vs=o(),d(We.$$.fragment),Cs=o(),He=y("p"),He.textContent=ea,Gs=o(),d(Ve.$$.fragment),Rs=o(),Ie=y("p"),Ie.textContent=ta,Ws=o(),Xe=y("ul"),Xe.innerHTML=sa,Hs=o(),Fe=y("p"),Fe.textContent=la,Vs=o(),d(Q.$$.fragment),Is=o(),d(q.$$.fragment),Xs=o(),d(ze.$$.fragment),Fs=o(),d(S.$$.fragment),zs=o(),d(P.$$.fragment),Ns=o(),d(Ne.$$.fragment),xs=o(),d(O.$$.fragment),Es=o(),xe=y("p"),xe.innerHTML=aa,Ys=o(),d(D.$$.fragment),Bs=o(),d(Ee.$$.fragment),Ls=o(),Ye=y("p"),Ye.textContent=na,As=o(),Be=y("p"),Be.innerHTML=ra,Qs=o(),d(Le.$$.fragment),qs=o(),d(K.$$.fragment),Ss=o(),Ae=y("p"),Ae.innerHTML=pa,Ps=o(),d(Qe.$$.fragment),Os=o(),qe=y("p"),qe.innerHTML=oa,Ds=o(),Se=y("p"),Se.innerHTML=ia,Ks=o(),X=y("ol"),Pe=y("li"),Xt=y("p"),Xt.innerHTML=ma,gl=o(),d(Oe.$$.fragment),Ml=o(),De=y("li"),Ft=y("p"),Ft.innerHTML=ca,yl=o(),d(Ke.$$.fragment),bl=o(),et=y("li"),zt=y("p"),zt.textContent=fa,Tl=o(),d(tt.$$.fragment),jl=o(),st=y("li"),Nt=y("p"),Nt.textContent=da,wl=o(),d(lt.$$.fragment),_l=o(),at=y("li"),xt=y("p"),xt.innerHTML=$a,Jl=o(),d(nt.$$.fragment),kl=o(),rt=y("li"),Et=y("p"),Et.innerHTML=ua,Ul=o(),d(pt.$$.fragment),el=o(),ot=y("p"),ot.innerHTML=ha,tl=o(),d(it.$$.fragment),sl=o(),mt=y("p"),mt.innerHTML=ga,ll=o(),d(ct.$$.fragment),al=o(),d(ee.$$.fragment),nl=o(),ft=y("p"),ft.innerHTML=Ma,rl=o(),dt=y("p"),dt.innerHTML=ya,pl=o(),d($t.$$.fragment),ol=o(),ut=y("p"),ut.innerHTML=ba,il=o(),x=y("ol"),ht=y("li"),Yt=y("p"),Yt.innerHTML=Ta,Zl=o(),d(gt.$$.fragment),vl=o(),Mt=y("li"),Bt=y("p"),Bt.textContent=ja,Cl=o(),d(yt.$$.fragment),Gl=o(),bt=y("li"),Lt=y("p"),Lt.textContent=wa,Rl=o(),d(Tt.$$.fragment),Wl=o(),jt=y("li"),At=y("p"),At.innerHTML=_a,Hl=o(),d(wt.$$.fragment),Vl=o(),_t=y("li"),Qt=y("p"),Qt.innerHTML=Ja,Il=o(),d(Jt.$$.fragment),ml=o(),d(kt.$$.fragment),cl=o(),Ut=y("p"),Ut.textContent=ka,fl=o(),qt=y("p"),this.h()},l(e){const l=Wa("svelte-u9bgzb",document.head);s=b(l,"META",{name:!0,content:!0}),l.forEach(a),p=i(e),t=b(e,"P",{}),z(t).forEach(a),n=i(e),$(f.$$.fragment,e),m=i(e),$(k.$$.fragment,e),v=i(e),C=b(e,"P",{"data-svelte-h":!0}),j(C)!=="svelte-1yuzc54"&&(C.innerHTML=G),T=i(e),Z=b(e,"P",{"data-svelte-h":!0}),j(Z)!=="svelte-10awe57"&&(Z.textContent=W),L=i(e),$(H.$$.fragment,e),R=i(e),V=b(e,"P",{"data-svelte-h":!0}),j(V)!=="svelte-1qrldcv"&&(V.textContent=c),U=i(e),$(N.$$.fragment,e),Pt=i(e),$(te.$$.fragment,e),Ot=i(e),$(se.$$.fragment,e),Dt=i(e),le=b(e,"P",{"data-svelte-h":!0}),j(le)!=="svelte-ad22ek"&&(le.innerHTML=Xl),Kt=i(e),ae=b(e,"TABLE",{"data-svelte-h":!0}),j(ae)!=="svelte-1cgfltn"&&(ae.innerHTML=Fl),es=i(e),ne=b(e,"P",{"data-svelte-h":!0}),j(ne)!=="svelte-1bed2pj"&&(ne.innerHTML=zl),ts=i(e),$(re.$$.fragment,e),ss=i(e),pe=b(e,"P",{"data-svelte-h":!0}),j(pe)!=="svelte-145ftm9"&&(pe.innerHTML=Nl),ls=i(e),$(oe.$$.fragment,e),as=i(e),ie=b(e,"P",{"data-svelte-h":!0}),j(ie)!=="svelte-a2wfxd"&&(ie.innerHTML=xl),ns=i(e),$(me.$$.fragment,e),rs=i(e),ce=b(e,"P",{"data-svelte-h":!0}),j(ce)!=="svelte-lsjq5w"&&(ce.innerHTML=El),ps=i(e),$(fe.$$.fragment,e),os=i(e),de=b(e,"P",{"data-svelte-h":!0}),j(de)!=="svelte-1x73msc"&&(de.innerHTML=Yl),is=i(e),$($e.$$.fragment,e),ms=i(e),ue=b(e,"P",{"data-svelte-h":!0}),j(ue)!=="svelte-1xxvdtf"&&(ue.innerHTML=Bl),cs=i(e),$(he.$$.fragment,e),fs=i(e),ge=b(e,"P",{"data-svelte-h":!0}),j(ge)!=="svelte-pyqv34"&&(ge.innerHTML=Ll),ds=i(e),$(Me.$$.fragment,e),$s=i(e),ye=b(e,"P",{"data-svelte-h":!0}),j(ye)!=="svelte-1cip30a"&&(ye.innerHTML=Al),us=i(e),$(be.$$.fragment,e),hs=i(e),Te=b(e,"P",{"data-svelte-h":!0}),j(Te)!=="svelte-6on4de"&&(Te.innerHTML=Ql),gs=i(e),$(je.$$.fragment,e),Ms=i(e),$(A.$$.fragment,e),ys=i(e),we=b(e,"P",{"data-svelte-h":!0}),j(we)!=="svelte-1jet3ts"&&(we.innerHTML=ql),bs=i(e),$(_e.$$.fragment,e),Ts=i(e),Je=b(e,"P",{"data-svelte-h":!0}),j(Je)!=="svelte-uyj2qy"&&(Je.innerHTML=Sl),js=i(e),$(ke.$$.fragment,e),ws=i(e),$(Ue.$$.fragment,e),_s=i(e),Ze=b(e,"P",{"data-svelte-h":!0}),j(Ze)!=="svelte-1tbj0ny"&&(Ze.innerHTML=Pl),Js=i(e),ve=b(e,"P",{"data-svelte-h":!0}),j(ve)!=="svelte-zht2o2"&&(ve.innerHTML=Ol),ks=i(e),$(Ce.$$.fragment,e),Us=i(e),Ge=b(e,"P",{"data-svelte-h":!0}),j(Ge)!=="svelte-pupyjv"&&(Ge.innerHTML=Dl),Zs=i(e),Re=b(e,"P",{"data-svelte-h":!0}),j(Re)!=="svelte-1iigdiw"&&(Re.innerHTML=Kl),vs=i(e),$(We.$$.fragment,e),Cs=i(e),He=b(e,"P",{"data-svelte-h":!0}),j(He)!=="svelte-bt7849"&&(He.textContent=ea),Gs=i(e),$(Ve.$$.fragment,e),Rs=i(e),Ie=b(e,"P",{"data-svelte-h":!0}),j(Ie)!=="svelte-1dcb4pt"&&(Ie.textContent=ta),Ws=i(e),Xe=b(e,"UL",{"data-svelte-h":!0}),j(Xe)!=="svelte-1hv4uqa"&&(Xe.innerHTML=sa),Hs=i(e),Fe=b(e,"P",{"data-svelte-h":!0}),j(Fe)!=="svelte-hbhd9z"&&(Fe.textContent=la),Vs=i(e),$(Q.$$.fragment,e),Is=i(e),$(q.$$.fragment,e),Xs=i(e),$(ze.$$.fragment,e),Fs=i(e),$(S.$$.fragment,e),zs=i(e),$(P.$$.fragment,e),Ns=i(e),$(Ne.$$.fragment,e),xs=i(e),$(O.$$.fragment,e),Es=i(e),xe=b(e,"P",{"data-svelte-h":!0}),j(xe)!=="svelte-rne23r"&&(xe.innerHTML=aa),Ys=i(e),$(D.$$.fragment,e),Bs=i(e),$(Ee.$$.fragment,e),Ls=i(e),Ye=b(e,"P",{"data-svelte-h":!0}),j(Ye)!=="svelte-1v6i9vn"&&(Ye.textContent=na),As=i(e),Be=b(e,"P",{"data-svelte-h":!0}),j(Be)!=="svelte-15b3fgp"&&(Be.innerHTML=ra),Qs=i(e),$(Le.$$.fragment,e),qs=i(e),$(K.$$.fragment,e),Ss=i(e),Ae=b(e,"P",{"data-svelte-h":!0}),j(Ae)!=="svelte-n1ja8b"&&(Ae.innerHTML=pa),Ps=i(e),$(Qe.$$.fragment,e),Os=i(e),qe=b(e,"P",{"data-svelte-h":!0}),j(qe)!=="svelte-1bwvvy5"&&(qe.innerHTML=oa),Ds=i(e),Se=b(e,"P",{"data-svelte-h":!0}),j(Se)!=="svelte-1mmhf7y"&&(Se.innerHTML=ia),Ks=i(e),X=b(e,"OL",{});var F=z(X);Pe=b(F,"LI",{});var Zt=z(Pe);Xt=b(Zt,"P",{"data-svelte-h":!0}),j(Xt)!=="svelte-rtad3z"&&(Xt.innerHTML=ma),gl=i(Zt),$(Oe.$$.fragment,Zt),Zt.forEach(a),Ml=i(F),De=b(F,"LI",{});var vt=z(De);Ft=b(vt,"P",{"data-svelte-h":!0}),j(Ft)!=="svelte-gaji1x"&&(Ft.innerHTML=ca),yl=i(vt),$(Ke.$$.fragment,vt),vt.forEach(a),bl=i(F),et=b(F,"LI",{});var Ct=z(et);zt=b(Ct,"P",{"data-svelte-h":!0}),j(zt)!=="svelte-hspqy3"&&(zt.textContent=fa),Tl=i(Ct),$(tt.$$.fragment,Ct),Ct.forEach(a),jl=i(F),st=b(F,"LI",{});var Gt=z(st);Nt=b(Gt,"P",{"data-svelte-h":!0}),j(Nt)!=="svelte-wmvqzp"&&(Nt.textContent=da),wl=i(Gt),$(lt.$$.fragment,Gt),Gt.forEach(a),_l=i(F),at=b(F,"LI",{});var Rt=z(at);xt=b(Rt,"P",{"data-svelte-h":!0}),j(xt)!=="svelte-nju1yy"&&(xt.innerHTML=$a),Jl=i(Rt),$(nt.$$.fragment,Rt),Rt.forEach(a),kl=i(F),rt=b(F,"LI",{});var Wt=z(rt);Et=b(Wt,"P",{"data-svelte-h":!0}),j(Et)!=="svelte-1esdjf4"&&(Et.innerHTML=ua),Ul=i(Wt),$(pt.$$.fragment,Wt),Wt.forEach(a),F.forEach(a),el=i(e),ot=b(e,"P",{"data-svelte-h":!0}),j(ot)!=="svelte-15smi2d"&&(ot.innerHTML=ha),tl=i(e),$(it.$$.fragment,e),sl=i(e),mt=b(e,"P",{"data-svelte-h":!0}),j(mt)!=="svelte-1si5m9v"&&(mt.innerHTML=ga),ll=i(e),$(ct.$$.fragment,e),al=i(e),$(ee.$$.fragment,e),nl=i(e),ft=b(e,"P",{"data-svelte-h":!0}),j(ft)!=="svelte-gx0dd1"&&(ft.innerHTML=Ma),rl=i(e),dt=b(e,"P",{"data-svelte-h":!0}),j(dt)!=="svelte-814nns"&&(dt.innerHTML=ya),pl=i(e),$($t.$$.fragment,e),ol=i(e),ut=b(e,"P",{"data-svelte-h":!0}),j(ut)!=="svelte-e70lvb"&&(ut.innerHTML=ba),il=i(e),x=b(e,"OL",{});var Y=z(x);ht=b(Y,"LI",{});var Ht=z(ht);Yt=b(Ht,"P",{"data-svelte-h":!0}),j(Yt)!=="svelte-17z9jst"&&(Yt.innerHTML=Ta),Zl=i(Ht),$(gt.$$.fragment,Ht),Ht.forEach(a),vl=i(Y),Mt=b(Y,"LI",{});var Vt=z(Mt);Bt=b(Vt,"P",{"data-svelte-h":!0}),j(Bt)!=="svelte-hspqy3"&&(Bt.textContent=ja),Cl=i(Vt),$(yt.$$.fragment,Vt),Vt.forEach(a),Gl=i(Y),bt=b(Y,"LI",{});var $l=z(bt);Lt=b($l,"P",{"data-svelte-h":!0}),j(Lt)!=="svelte-jf0jr8"&&(Lt.textContent=wa),Rl=i($l),$(Tt.$$.fragment,$l),$l.forEach(a),Wl=i(Y),jt=b(Y,"LI",{});var ul=z(jt);At=b(ul,"P",{"data-svelte-h":!0}),j(At)!=="svelte-f7iu5h"&&(At.innerHTML=_a),Hl=i(ul),$(wt.$$.fragment,ul),ul.forEach(a),Vl=i(Y),_t=b(Y,"LI",{});var hl=z(_t);Qt=b(hl,"P",{"data-svelte-h":!0}),j(Qt)!=="svelte-inq7rl"&&(Qt.innerHTML=Ja),Il=i(hl),$(Jt.$$.fragment,hl),hl.forEach(a),Y.forEach(a),ml=i(e),$(kt.$$.fragment,e),cl=i(e),Ut=b(e,"P",{"data-svelte-h":!0}),j(Ut)!=="svelte-t3rs4b"&&(Ut.textContent=ka),fl=i(e),qt=b(e,"P",{}),z(qt).forEach(a),this.h()},h(){Ua(s,"name","hf:doc:metadata"),Ua(s,"content",hn)},m(e,l){J(document.head,s),r(e,p,l),r(e,t,l),r(e,n,l),u(f,e,l),r(e,m,l),u(k,e,l),r(e,v,l),r(e,C,l),r(e,T,l),r(e,Z,l),r(e,L,l),u(H,e,l),r(e,R,l),r(e,V,l),r(e,U,l),u(N,e,l),r(e,Pt,l),u(te,e,l),r(e,Ot,l),u(se,e,l),r(e,Dt,l),r(e,le,l),r(e,Kt,l),r(e,ae,l),r(e,es,l),r(e,ne,l),r(e,ts,l),u(re,e,l),r(e,ss,l),r(e,pe,l),r(e,ls,l),u(oe,e,l),r(e,as,l),r(e,ie,l),r(e,ns,l),u(me,e,l),r(e,rs,l),r(e,ce,l),r(e,ps,l),u(fe,e,l),r(e,os,l),r(e,de,l),r(e,is,l),u($e,e,l),r(e,ms,l),r(e,ue,l),r(e,cs,l),u(he,e,l),r(e,fs,l),r(e,ge,l),r(e,ds,l),u(Me,e,l),r(e,$s,l),r(e,ye,l),r(e,us,l),u(be,e,l),r(e,hs,l),r(e,Te,l),r(e,gs,l),u(je,e,l),r(e,Ms,l),u(A,e,l),r(e,ys,l),r(e,we,l),r(e,bs,l),u(_e,e,l),r(e,Ts,l),r(e,Je,l),r(e,js,l),u(ke,e,l),r(e,ws,l),u(Ue,e,l),r(e,_s,l),r(e,Ze,l),r(e,Js,l),r(e,ve,l),r(e,ks,l),u(Ce,e,l),r(e,Us,l),r(e,Ge,l),r(e,Zs,l),r(e,Re,l),r(e,vs,l),u(We,e,l),r(e,Cs,l),r(e,He,l),r(e,Gs,l),u(Ve,e,l),r(e,Rs,l),r(e,Ie,l),r(e,Ws,l),r(e,Xe,l),r(e,Hs,l),r(e,Fe,l),r(e,Vs,l),u(Q,e,l),r(e,Is,l),u(q,e,l),r(e,Xs,l),u(ze,e,l),r(e,Fs,l),u(S,e,l),r(e,zs,l),u(P,e,l),r(e,Ns,l),u(Ne,e,l),r(e,xs,l),u(O,e,l),r(e,Es,l),r(e,xe,l),r(e,Ys,l),u(D,e,l),r(e,Bs,l),u(Ee,e,l),r(e,Ls,l),r(e,Ye,l),r(e,As,l),r(e,Be,l),r(e,Qs,l),u(Le,e,l),r(e,qs,l),u(K,e,l),r(e,Ss,l),r(e,Ae,l),r(e,Ps,l),u(Qe,e,l),r(e,Os,l),r(e,qe,l),r(e,Ds,l),r(e,Se,l),r(e,Ks,l),r(e,X,l),J(X,Pe),J(Pe,Xt),J(Pe,gl),u(Oe,Pe,null),J(X,Ml),J(X,De),J(De,Ft),J(De,yl),u(Ke,De,null),J(X,bl),J(X,et),J(et,zt),J(et,Tl),u(tt,et,null),J(X,jl),J(X,st),J(st,Nt),J(st,wl),u(lt,st,null),J(X,_l),J(X,at),J(at,xt),J(at,Jl),u(nt,at,null),J(X,kl),J(X,rt),J(rt,Et),J(rt,Ul),u(pt,rt,null),r(e,el,l),r(e,ot,l),r(e,tl,l),u(it,e,l),r(e,sl,l),r(e,mt,l),r(e,ll,l),u(ct,e,l),r(e,al,l),u(ee,e,l),r(e,nl,l),r(e,ft,l),r(e,rl,l),r(e,dt,l),r(e,pl,l),u($t,e,l),r(e,ol,l),r(e,ut,l),r(e,il,l),r(e,x,l),J(x,ht),J(ht,Yt),J(ht,Zl),u(gt,ht,null),J(x,vl),J(x,Mt),J(Mt,Bt),J(Mt,Cl),u(yt,Mt,null),J(x,Gl),J(x,bt),J(bt,Lt),J(bt,Rl),u(Tt,bt,null),J(x,Wl),J(x,jt),J(jt,At),J(jt,Hl),u(wt,jt,null),J(x,Vl),J(x,_t),J(_t,Qt),J(_t,Il),u(Jt,_t,null),r(e,ml,l),u(kt,e,l),r(e,cl,l),r(e,Ut,l),r(e,fl,l),r(e,qt,l),dl=!0},p(e,[l]){const F={};l&2&&(F.$$scope={dirty:l,ctx:e}),N.$set(F);const Zt={};l&2&&(Zt.$$scope={dirty:l,ctx:e}),A.$set(Zt);const vt={};l&2&&(vt.$$scope={dirty:l,ctx:e}),Q.$set(vt);const Ct={};l&2&&(Ct.$$scope={dirty:l,ctx:e}),q.$set(Ct);const Gt={};l&2&&(Gt.$$scope={dirty:l,ctx:e}),S.$set(Gt);const Rt={};l&2&&(Rt.$$scope={dirty:l,ctx:e}),P.$set(Rt);const Wt={};l&2&&(Wt.$$scope={dirty:l,ctx:e}),O.$set(Wt);const Y={};l&2&&(Y.$$scope={dirty:l,ctx:e}),D.$set(Y);const Ht={};l&2&&(Ht.$$scope={dirty:l,ctx:e}),K.$set(Ht);const Vt={};l&2&&(Vt.$$scope={dirty:l,ctx:e}),ee.$set(Vt)},i(e){dl||(h(f.$$.fragment,e),h(k.$$.fragment,e),h(H.$$.fragment,e),h(N.$$.fragment,e),h(te.$$.fragment,e),h(se.$$.fragment,e),h(re.$$.fragment,e),h(oe.$$.fragment,e),h(me.$$.fragment,e),h(fe.$$.fragment,e),h($e.$$.fragment,e),h(he.$$.fragment,e),h(Me.$$.fragment,e),h(be.$$.fragment,e),h(je.$$.fragment,e),h(A.$$.fragment,e),h(_e.$$.fragment,e),h(ke.$$.fragment,e),h(Ue.$$.fragment,e),h(Ce.$$.fragment,e),h(We.$$.fragment,e),h(Ve.$$.fragment,e),h(Q.$$.fragment,e),h(q.$$.fragment,e),h(ze.$$.fragment,e),h(S.$$.fragment,e),h(P.$$.fragment,e),h(Ne.$$.fragment,e),h(O.$$.fragment,e),h(D.$$.fragment,e),h(Ee.$$.fragment,e),h(Le.$$.fragment,e),h(K.$$.fragment,e),h(Qe.$$.fragment,e),h(Oe.$$.fragment,e),h(Ke.$$.fragment,e),h(tt.$$.fragment,e),h(lt.$$.fragment,e),h(nt.$$.fragment,e),h(pt.$$.fragment,e),h(it.$$.fragment,e),h(ct.$$.fragment,e),h(ee.$$.fragment,e),h($t.$$.fragment,e),h(gt.$$.fragment,e),h(yt.$$.fragment,e),h(Tt.$$.fragment,e),h(wt.$$.fragment,e),h(Jt.$$.fragment,e),h(kt.$$.fragment,e),dl=!0)},o(e){g(f.$$.fragment,e),g(k.$$.fragment,e),g(H.$$.fragment,e),g(N.$$.fragment,e),g(te.$$.fragment,e),g(se.$$.fragment,e),g(re.$$.fragment,e),g(oe.$$.fragment,e),g(me.$$.fragment,e),g(fe.$$.fragment,e),g($e.$$.fragment,e),g(he.$$.fragment,e),g(Me.$$.fragment,e),g(be.$$.fragment,e),g(je.$$.fragment,e),g(A.$$.fragment,e),g(_e.$$.fragment,e),g(ke.$$.fragment,e),g(Ue.$$.fragment,e),g(Ce.$$.fragment,e),g(We.$$.fragment,e),g(Ve.$$.fragment,e),g(Q.$$.fragment,e),g(q.$$.fragment,e),g(ze.$$.fragment,e),g(S.$$.fragment,e),g(P.$$.fragment,e),g(Ne.$$.fragment,e),g(O.$$.fragment,e),g(D.$$.fragment,e),g(Ee.$$.fragment,e),g(Le.$$.fragment,e),g(K.$$.fragment,e),g(Qe.$$.fragment,e),g(Oe.$$.fragment,e),g(Ke.$$.fragment,e),g(tt.$$.fragment,e),g(lt.$$.fragment,e),g(nt.$$.fragment,e),g(pt.$$.fragment,e),g(it.$$.fragment,e),g(ct.$$.fragment,e),g(ee.$$.fragment,e),g($t.$$.fragment,e),g(gt.$$.fragment,e),g(yt.$$.fragment,e),g(Tt.$$.fragment,e),g(wt.$$.fragment,e),g(Jt.$$.fragment,e),g(kt.$$.fragment,e),dl=!1},d(e){e&&(a(p),a(t),a(n),a(m),a(v),a(C),a(T),a(Z),a(L),a(R),a(V),a(U),a(Pt),a(Ot),a(Dt),a(le),a(Kt),a(ae),a(es),a(ne),a(ts),a(ss),a(pe),a(ls),a(as),a(ie),a(ns),a(rs),a(ce),a(ps),a(os),a(de),a(is),a(ms),a(ue),a(cs),a(fs),a(ge),a(ds),a($s),a(ye),a(us),a(hs),a(Te),a(gs),a(Ms),a(ys),a(we),a(bs),a(Ts),a(Je),a(js),a(ws),a(_s),a(Ze),a(Js),a(ve),a(ks),a(Us),a(Ge),a(Zs),a(Re),a(vs),a(Cs),a(He),a(Gs),a(Rs),a(Ie),a(Ws),a(Xe),a(Hs),a(Fe),a(Vs),a(Is),a(Xs),a(Fs),a(zs),a(Ns),a(xs),a(Es),a(xe),a(Ys),a(Bs),a(Ls),a(Ye),a(As),a(Be),a(Qs),a(qs),a(Ss),a(Ae),a(Ps),a(Os),a(qe),a(Ds),a(Se),a(Ks),a(X),a(el),a(ot),a(tl),a(sl),a(mt),a(ll),a(al),a(nl),a(ft),a(rl),a(dt),a(pl),a(ol),a(ut),a(il),a(x),a(ml),a(cl),a(Ut),a(fl),a(qt)),a(s),M(f,e),M(k,e),M(H,e),M(N,e),M(te,e),M(se,e),M(re,e),M(oe,e),M(me,e),M(fe,e),M($e,e),M(he,e),M(Me,e),M(be,e),M(je,e),M(A,e),M(_e,e),M(ke,e),M(Ue,e),M(Ce,e),M(We,e),M(Ve,e),M(Q,e),M(q,e),M(ze,e),M(S,e),M(P,e),M(Ne,e),M(O,e),M(D,e),M(Ee,e),M(Le,e),M(K,e),M(Qe,e),M(Oe),M(Ke),M(tt),M(lt),M(nt),M(pt),M(it,e),M(ct,e),M(ee,e),M($t,e),M(gt),M(yt),M(Tt),M(wt),M(Jt),M(kt,e)}}}const hn='{"title":"快速上手","local":"快速上手","sections":[{"title":"Pipeline","local":"pipeline","sections":[{"title":"在 pipeline 中使用另一个模型和分词器","local":"在-pipeline-中使用另一个模型和分词器","sections":[],"depth":3}],"depth":2},{"title":"AutoClass","local":"autoclass","sections":[{"title":"AutoTokenizer","local":"autotokenizer","sections":[],"depth":3},{"title":"AutoModel","local":"automodel","sections":[],"depth":3},{"title":"保存模型","local":"保存模型","sections":[],"depth":3}],"depth":2},{"title":"自定义模型构建","local":"自定义模型构建","sections":[],"depth":2},{"title":"Trainer - PyTorch 优化训练循环","local":"trainer---pytorch-优化训练循环","sections":[],"depth":2},{"title":"使用 Tensorflow 训练","local":"使用-tensorflow-训练","sections":[],"depth":2},{"title":"接下来做什么?","local":"接下来做什么","sections":[],"depth":2}],"depth":1}';function gn(w){return Ca(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class kn extends Ga{constructor(s){super(),Ra(this,s,gn,un,va,{})}}export{kn as component};
