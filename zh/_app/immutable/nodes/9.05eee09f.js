import{s as oe,n as ce,o as fe}from"../chunks/scheduler.9991993c.js";import{S as Me,i as ke,g as r,s as a,r as U,A as ye,h as p,f as s,c as n,j as ie,u as Z,x as m,k as me,y as he,a as l,v as w,d as J,t as V,w as _}from"../chunks/index.7fc9a5e7.js";import{C as K}from"../chunks/CodeBlock.e11cba92.js";import{H as O}from"../chunks/Heading.e3de321f.js";function je(D){let i,v,B,C,o,I,c,ee='<a href="/docs/transformers/main/zh/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> 依赖于 <a href="https://huggingface.co/docs/tokenizers" rel="nofollow">🤗 Tokenizers</a> 库。从 🤗 Tokenizers 库获得的分词器可以被轻松地加载到 🤗 Transformers 中。',P,f,te="在了解具体内容之前，让我们先用几行代码创建一个虚拟的分词器：",N,M,Q,k,se="现在，我们拥有了一个针对我们定义的文件进行训练的分词器。我们可以在当前运行时中继续使用它，或者将其保存到一个 JSON 文件以供将来重复使用。",W,y,X,h,le='让我们看看如何利用 🤗 Transformers 库中的这个分词器对象。<a href="/docs/transformers/main/zh/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> 类允许通过接受已实例化的 <em>tokenizer</em> 对象作为参数，进行轻松实例化：',G,j,q,T,ae='现在可以使用这个对象，使用 🤗 Transformers 分词器共享的所有方法！前往<a href="main_classes/tokenizer">分词器页面</a>了解更多信息。',x,g,E,d,ne="为了从 JSON 文件中加载分词器，让我们先保存我们的分词器：",R,u,H,z,re='我们保存此文件的路径可以通过 <code>tokenizer_file</code> 参数传递给 <a href="/docs/transformers/main/zh/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> 初始化方法：',S,$,L,b,pe='现在可以使用这个对象，使用 🤗 Transformers 分词器共享的所有方法！前往<a href="main_classes/tokenizer">分词器页面</a>了解更多信息。',A,F,Y;return o=new O({props:{title:"使用 🤗 Tokenizers 中的分词器",local:"使用--tokenizers-中的分词器",headingTag:"h1"}}),M=new K({props:{code:"ZnJvbSUyMHRva2VuaXplcnMlMjBpbXBvcnQlMjBUb2tlbml6ZXIlMEFmcm9tJTIwdG9rZW5pemVycy5tb2RlbHMlMjBpbXBvcnQlMjBCUEUlMEFmcm9tJTIwdG9rZW5pemVycy50cmFpbmVycyUyMGltcG9ydCUyMEJwZVRyYWluZXIlMEFmcm9tJTIwdG9rZW5pemVycy5wcmVfdG9rZW5pemVycyUyMGltcG9ydCUyMFdoaXRlc3BhY2UlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBUb2tlbml6ZXIoQlBFKHVua190b2tlbiUzRCUyMiU1QlVOSyU1RCUyMikpJTBBdHJhaW5lciUyMCUzRCUyMEJwZVRyYWluZXIoc3BlY2lhbF90b2tlbnMlM0QlNUIlMjIlNUJVTkslNUQlMjIlMkMlMjAlMjIlNUJDTFMlNUQlMjIlMkMlMjAlMjIlNUJTRVAlNUQlMjIlMkMlMjAlMjIlNUJQQUQlNUQlMjIlMkMlMjAlMjIlNUJNQVNLJTVEJTIyJTVEKSUwQSUwQXRva2VuaXplci5wcmVfdG9rZW5pemVyJTIwJTNEJTIwV2hpdGVzcGFjZSgpJTBBZmlsZXMlMjAlM0QlMjAlNUIuLi4lNUQlMEF0b2tlbml6ZXIudHJhaW4oZmlsZXMlMkMlMjB0cmFpbmVyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pre_tokenizer = Whitespace()
<span class="hljs-meta">&gt;&gt;&gt; </span>files = [...]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.train(files, trainer)`,wrap:!1}}),y=new O({props:{title:"直接从分词器对象加载",local:"直接从分词器对象加载",headingTag:"h2"}}),j=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfb2JqZWN0JTNEdG9rZW5pemVyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`,wrap:!1}}),g=new O({props:{title:"从 JSON 文件加载",local:"从-json-文件加载",headingTag:"h2"}}),u=new K({props:{code:"dG9rZW5pemVyLnNhdmUoJTIydG9rZW5pemVyLmpzb24lMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)',wrap:!1}}),$=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfZmlsZSUzRCUyMnRva2VuaXplci5qc29uJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="hljs-string">&quot;tokenizer.json&quot;</span>)`,wrap:!1}}),{c(){i=r("meta"),v=a(),B=r("p"),C=a(),U(o.$$.fragment),I=a(),c=r("p"),c.innerHTML=ee,P=a(),f=r("p"),f.textContent=te,N=a(),U(M.$$.fragment),Q=a(),k=r("p"),k.textContent=se,W=a(),U(y.$$.fragment),X=a(),h=r("p"),h.innerHTML=le,G=a(),U(j.$$.fragment),q=a(),T=r("p"),T.innerHTML=ae,x=a(),U(g.$$.fragment),E=a(),d=r("p"),d.textContent=ne,R=a(),U(u.$$.fragment),H=a(),z=r("p"),z.innerHTML=re,S=a(),U($.$$.fragment),L=a(),b=r("p"),b.innerHTML=pe,A=a(),F=r("p"),this.h()},l(e){const t=ye("svelte-u9bgzb",document.head);i=p(t,"META",{name:!0,content:!0}),t.forEach(s),v=n(e),B=p(e,"P",{}),ie(B).forEach(s),C=n(e),Z(o.$$.fragment,e),I=n(e),c=p(e,"P",{"data-svelte-h":!0}),m(c)!=="svelte-1xn4qlv"&&(c.innerHTML=ee),P=n(e),f=p(e,"P",{"data-svelte-h":!0}),m(f)!=="svelte-opvc5k"&&(f.textContent=te),N=n(e),Z(M.$$.fragment,e),Q=n(e),k=p(e,"P",{"data-svelte-h":!0}),m(k)!=="svelte-17x9tu2"&&(k.textContent=se),W=n(e),Z(y.$$.fragment,e),X=n(e),h=p(e,"P",{"data-svelte-h":!0}),m(h)!=="svelte-o65hyz"&&(h.innerHTML=le),G=n(e),Z(j.$$.fragment,e),q=n(e),T=p(e,"P",{"data-svelte-h":!0}),m(T)!=="svelte-tb93lc"&&(T.innerHTML=ae),x=n(e),Z(g.$$.fragment,e),E=n(e),d=p(e,"P",{"data-svelte-h":!0}),m(d)!=="svelte-lr3arf"&&(d.textContent=ne),R=n(e),Z(u.$$.fragment,e),H=n(e),z=p(e,"P",{"data-svelte-h":!0}),m(z)!=="svelte-x5pc4q"&&(z.innerHTML=re),S=n(e),Z($.$$.fragment,e),L=n(e),b=p(e,"P",{"data-svelte-h":!0}),m(b)!=="svelte-tb93lc"&&(b.innerHTML=pe),A=n(e),F=p(e,"P",{}),ie(F).forEach(s),this.h()},h(){me(i,"name","hf:doc:metadata"),me(i,"content",Te)},m(e,t){he(document.head,i),l(e,v,t),l(e,B,t),l(e,C,t),w(o,e,t),l(e,I,t),l(e,c,t),l(e,P,t),l(e,f,t),l(e,N,t),w(M,e,t),l(e,Q,t),l(e,k,t),l(e,W,t),w(y,e,t),l(e,X,t),l(e,h,t),l(e,G,t),w(j,e,t),l(e,q,t),l(e,T,t),l(e,x,t),w(g,e,t),l(e,E,t),l(e,d,t),l(e,R,t),w(u,e,t),l(e,H,t),l(e,z,t),l(e,S,t),w($,e,t),l(e,L,t),l(e,b,t),l(e,A,t),l(e,F,t),Y=!0},p:ce,i(e){Y||(J(o.$$.fragment,e),J(M.$$.fragment,e),J(y.$$.fragment,e),J(j.$$.fragment,e),J(g.$$.fragment,e),J(u.$$.fragment,e),J($.$$.fragment,e),Y=!0)},o(e){V(o.$$.fragment,e),V(M.$$.fragment,e),V(y.$$.fragment,e),V(j.$$.fragment,e),V(g.$$.fragment,e),V(u.$$.fragment,e),V($.$$.fragment,e),Y=!1},d(e){e&&(s(v),s(B),s(C),s(I),s(c),s(P),s(f),s(N),s(Q),s(k),s(W),s(X),s(h),s(G),s(q),s(T),s(x),s(E),s(d),s(R),s(H),s(z),s(S),s(L),s(b),s(A),s(F)),s(i),_(o,e),_(M,e),_(y,e),_(j,e),_(g,e),_(u,e),_($,e)}}}const Te='{"title":"使用 🤗 Tokenizers 中的分词器","local":"使用--tokenizers-中的分词器","sections":[{"title":"直接从分词器对象加载","local":"直接从分词器对象加载","sections":[],"depth":2},{"title":"从 JSON 文件加载","local":"从-json-文件加载","sections":[],"depth":2}],"depth":1}';function ge(D){return fe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class be extends Me{constructor(i){super(),ke(this,i,ge,je,oe,{})}}export{be as component};
