import{s as Ro,f as Ho,o as qo,n as wt}from"../chunks/scheduler.9bc65507.js";import{S as Bo,i as Go,g as s,s as a,r as p,A as So,h as i,f as o,c as r,j as w,u as g,x as d,k as x,y as t,a as l,v as f,d as u,t as h,w as _}from"../chunks/index.707bf1b6.js";import{T as io}from"../chunks/Tip.c2ecdbf4.js";import{D as $}from"../chunks/Docstring.17db21ae.js";import{C as Vo}from"../chunks/CodeBlock.54a9f38d.js";import{E as Ao}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as $e}from"../chunks/Heading.342b1fa6.js";function Yo(I){let c,z='The model is identical to <a href="donut">Donut</a> in terms of architecture.';return{c(){c=s("p"),c.innerHTML=z},l(m){c=i(m,"P",{"data-svelte-h":!0}),d(c)!=="svelte-1qwiol9"&&(c.innerHTML=z)},m(m,y){l(m,c,y)},p:wt,d(m){m&&o(c)}}}function Qo(I){let c,z="Example:",m,y,M;return y=new Vo({props:{code:"Y29ycmVjdF90YWJsZXMoJTIyJTVDYmVnaW4lN0J0YWJsZSU3RCUyMCU1Q2JlZ2luJTdCdGFidWxhciU3RCU3QmwlMjBsJTdEJTIwJTI2JTIwJTVDJTIwJTVDZW5kJTdCdGFidWxhciU3RCUyMCU1Q2VuZCU3QnRhYmxlJTdEJTIyKSUwQSUyMiU1Q2JlZ2luJTdCdGFibGUlN0QlMEFhYnVsYXIlN0QlN0JsJTIwbCU3RCUyMCUyNiUyMCU1QyUyMCU1Q2VuZCU3QnRhYnVsYXIlN0QlMEFsZSU3RCUyMg==",highlighted:`correct_tables(<span class="hljs-string">&quot;\\begin{table} \\begin{tabular}{l l} &amp; \\ \\end{tabular} \\end{table}&quot;</span>)
<span class="hljs-string">&quot;\\begin{table}
abular}{l l} &amp; \\ \\end{tabular}
le}&quot;</span>`,wrap:!1}}),{c(){c=s("p"),c.textContent=z,m=a(),p(y.$$.fragment)},l(T){c=i(T,"P",{"data-svelte-h":!0}),d(c)!=="svelte-11lpom8"&&(c.textContent=z),m=r(T),g(y.$$.fragment,T)},m(T,C){l(T,c,C),l(T,m,C),f(y,T,C),M=!0},p:wt,i(T){M||(u(y.$$.fragment,T),M=!0)},o(T){h(y.$$.fragment,T),M=!1},d(T){T&&(o(c),o(m)),_(y,T)}}}function Xo(I){let c,z=`This class method is simply calling the feature extractor
<a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a>, image processor
<a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> and the tokenizer
<code>~tokenization_utils_base.PreTrainedTokenizer.from_pretrained</code> methods. Please refer to the docstrings of the
methods above for more information.`;return{c(){c=s("p"),c.innerHTML=z},l(m){c=i(m,"P",{"data-svelte-h":!0}),d(c)!=="svelte-1ob8jsn"&&(c.innerHTML=z)},m(m,y){l(m,c,y)},p:wt,d(m){m&&o(c)}}}function Oo(I){let c,z=`This class method is simply calling <a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a>. Please refer to the docstrings of the
methods above for more information.`;return{c(){c=s("p"),c.innerHTML=z},l(m){c=i(m,"P",{"data-svelte-h":!0}),d(c)!=="svelte-1qvfzw2"&&(c.innerHTML=z)},m(m,y){l(m,c,y)},p:wt,d(m){m&&o(c)}}}function Ko(I){let c,z,m,y,M,T,C,Ke,Q,lo=`The Nougat model was proposed in <a href="https://arxiv.org/abs/2308.13418" rel="nofollow">Nougat: Neural Optical Understanding for Academic Documents</a> by
Lukas Blecher, Guillem Cucurull, Thomas Scialom, Robert Stojnic. Nougat uses the same architecture as <a href="donut">Donut</a>, meaning an image Transformer
encoder and an autoregressive text Transformer decoder to translate scientific PDFs to markdown, enabling easier access to them.`,et,X,co="The abstract from the paper is the following:",tt,O,mo="<em>Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.</em>",ot,E,po,nt,K,go='Nougat high-level overview. Taken from the <a href="https://arxiv.org/abs/2308.13418">original paper</a>.',at,ee,fo=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>. The original code can be found
<a href="https://github.com/facebookresearch/nougat" rel="nofollow">here</a>.`,rt,te,st,oe,uo=`<li>The quickest way to get started with Nougat is by checking the <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/Nougat" rel="nofollow">tutorial
notebooks</a>, which show how to use the model
at inference time as well as fine-tuning on custom data.</li> <li>Nougat is always used within the <a href="vision-encoder-decoder">VisionEncoderDecoder</a> framework. The model is identical to <a href="donut">Donut</a> in terms of architecture.</li>`,it,ne,lt,ae,ho=`Nougat’s <code>VisionEncoderDecoder</code> model accepts images as input and makes use of
<a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiForCausalLM.generate">generate()</a> to autoregressively generate text given the input image.`,ct,re,_o=`The <a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatImageProcessor">NougatImageProcessor</a> class is responsible for preprocessing the input image and
<a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatTokenizerFast">NougatTokenizerFast</a> decodes the generated target tokens to the target string. The
<a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatProcessor">NougatProcessor</a> wraps <a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatImageProcessor">NougatImageProcessor</a> and <a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatTokenizerFast">NougatTokenizerFast</a> classes
into a single instance to both extract the input features and decode the predicted token ids.`,dt,se,bo="<li>Step-by-step PDF transcription</li>",mt,ie,pt,le,ko='See the <a href="https://huggingface.co/models?filter=nougat" rel="nofollow">model hub</a> to look for Nougat checkpoints.',gt,D,ft,ce,ut,U,de,$t,Me,vo="Constructs a Nougat image processor.",Mt,V,me,Pt,Pe,To="Preprocess an image or batch of images.",ht,pe,_t,b,ge,Ct,Ce,xo="Fast tokenizer for Nougat (backed by HuggingFace tokenizers library).",Ut,Ue,yo=`This tokenizer inherits from <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods. This class mainly adds Nougat-specific
methods for postprocessing the generated text.`,It,Ie,No="Class attributes (overridden by derived classes)",Ft,Fe,zo=`<li><strong>vocab_files_names</strong> (<code>Dict[str, str]</code>) — A dictionary with, as keys, the <code>__init__</code> keyword name of each
vocabulary file required by the model, and as associated values, the filename for saving the associated file
(string).</li> <li><strong>pretrained_vocab_files_map</strong> (<code>Dict[str, Dict[str, str]]</code>) — A dictionary of dictionaries, with the
high-level keys being the <code>__init__</code> keyword name of each vocabulary file required by the model, the
low-level being the <code>short-cut-names</code> of the pretrained models with, as associated values, the <code>url</code> to the
associated pretrained vocabulary file.</li> <li><strong>max_model_input_sizes</strong> (<code>Dict[str, Optional[int]]</code>) — A dictionary with, as keys, the <code>short-cut-names</code>
of the pretrained models, and as associated values, the maximum length of the sequence inputs of this model,
or <code>None</code> if the model has no maximum input size.</li> <li><strong>pretrained_init_configuration</strong> (<code>Dict[str, Dict[str, Any]]</code>) — A dictionary with, as keys, the
<code>short-cut-names</code> of the pretrained models, and as associated values, a dictionary of specific arguments to
pass to the <code>__init__</code> method of the tokenizer class for this pretrained model when loading the tokenizer
with the <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained">from_pretrained()</a> method.</li> <li><strong>model_input_names</strong> (<code>List[str]</code>) — A list of inputs expected in the forward pass of the model.</li> <li><strong>padding_side</strong> (<code>str</code>) — The default value for the side on which the model should have padding applied.
Should be <code>&#39;right&#39;</code> or <code>&#39;left&#39;</code>.</li> <li><strong>truncation_side</strong> (<code>str</code>) — The default value for the side on which the model should have truncation
applied. Should be <code>&#39;right&#39;</code> or <code>&#39;left&#39;</code>.</li>`,jt,F,fe,Jt,je,wo="Takes a generated string and fixes tables/tabulars to make them match the markdown format needed.",Wt,R,Zt,P,ue,Lt,Je,$o="Postprocess a generated text or a list of generated texts.",Et,We,Mo="This function can be used to perform postprocessing on generated text, such as fixing Markdown formatting.",Dt,Ze,Po="Postprocessing is quite slow so it is recommended to use multiprocessing to speed up the process.",Vt,H,he,Rt,Le,Co=`Postprocess a single generated text. Regular expressions used here are taken directly from the Nougat article
authors. These expressions are commented for clarity and tested end-to-end in most cases.`,Ht,j,_e,qt,Ee,Uo="Remove hallucinated or missing references from the text.",Bt,De,Io="This function identifies and removes references that are marked as missing or hallucinated from the input text.",bt,be,kt,k,ke,Gt,Ve,Fo="Constructs a Nougat processor which wraps a Nougat image processor and a Nougat tokenizer into a single processor.",St,Re,jo=`<a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatProcessor">NougatProcessor</a> offers all the functionalities of <a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatImageProcessor">NougatImageProcessor</a> and <a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatTokenizerFast">NougatTokenizerFast</a>. See the
<a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatProcessor.__call__"><strong>call</strong>()</a> and <a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatProcessor.decode">decode()</a> for more information.`,At,He,ve,Yt,J,Te,Qt,qe,Jo="Instantiate a processor associated with a pretrained model.",Xt,q,Ot,W,xe,Kt,Be,Wo=`Saves the attributes of this processor (feature extractor, tokenizer…) in the specified directory so that it
can be reloaded using the <a href="/docs/transformers/main/en/model_doc/mctct#transformers.MCTCTProcessor.from_pretrained">from_pretrained()</a> method.`,eo,B,to,G,ye,oo,Ge,Zo=`This method forwards all its arguments to NougatTokenizer’s <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.batch_decode">batch_decode()</a>. Please refer
to the docstring of this method for more information.`,no,S,Ne,ao,Se,Lo=`This method forwards all its arguments to NougatTokenizer’s <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Tokenizer.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,ro,A,ze,so,Ae,Eo=`This method forwards all its arguments to NougatTokenizer’s <code>~PreTrainedTokenizer.post_process_generation</code>.
Please refer to the docstring of this method for more information.`,vt,Oe,Tt;return M=new $e({props:{title:"Nougat",local:"nougat",headingTag:"h1"}}),C=new $e({props:{title:"Overview",local:"overview",headingTag:"h2"}}),te=new $e({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),ne=new $e({props:{title:"Inference",local:"inference",headingTag:"h2"}}),ie=new Vo({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> re
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NougatProcessor, VisionEncoderDecoderModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = NougatProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/nougat-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VisionEncoderDecoderModel.from_pretrained(<span class="hljs-string">&quot;facebook/nougat-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare PDF image for the model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>filepath = hf_hub_download(repo_id=<span class="hljs-string">&quot;hf-internal-testing/fixtures_docvqa&quot;</span>, filename=<span class="hljs-string">&quot;nougat_paper.png&quot;</span>, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(filepath)
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># generate transcription (here we only generate 30 tokens)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(
<span class="hljs-meta">... </span>    pixel_values.to(device),
<span class="hljs-meta">... </span>    min_length=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    max_new_tokens=<span class="hljs-number">30</span>,
<span class="hljs-meta">... </span>    bad_words_ids=[[processor.tokenizer.unk_token_id]],
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>sequence = processor.batch_decode(outputs, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>sequence = processor.post_process_generation(sequence, fix_markdown=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># note: we&#x27;re using repr here such for the sake of printing the \\n characters, feel free to just print the sequence</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">repr</span>(sequence))
<span class="hljs-string">&#x27;\\n\\n# Nougat: Neural Optical Understanding for Academic Documents\\n\\n Lukas Blecher\\n\\nCorrespondence to: lblecher@&#x27;</span>`,wrap:!1}}),D=new io({props:{$$slots:{default:[Yo]},$$scope:{ctx:I}}}),ce=new $e({props:{title:"NougatImageProcessor",local:"transformers.NougatImageProcessor",headingTag:"h2"}}),de=new $({props:{name:"class transformers.NougatImageProcessor",anchor:"transformers.NougatImageProcessor",parameters:[{name:"do_crop_margin",val:": bool = True"},{name:"do_resize",val:": bool = True"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = <Resampling.BILINEAR: 2>"},{name:"do_thumbnail",val:": bool = True"},{name:"do_align_long_axis",val:": bool = False"},{name:"do_pad",val:": bool = True"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": Union = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.NougatImageProcessor.do_crop_margin",description:`<strong>do_crop_margin</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the image margins.`,name:"do_crop_margin"},{anchor:"transformers.NougatImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by
<code>do_resize</code> in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.NougatImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 896, &quot;width&quot;: 672}</code>):
Size of the image after resizing. Can be overridden by <code>size</code> in the <code>preprocess</code> method.`,name:"size"},{anchor:"transformers.NougatImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BILINEAR</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by <code>resample</code> in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.NougatImageProcessor.do_thumbnail",description:`<strong>do_thumbnail</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image using thumbnail method.`,name:"do_thumbnail"},{anchor:"transformers.NougatImageProcessor.do_align_long_axis",description:`<strong>do_align_long_axis</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to align the long axis of the image with the long axis of <code>size</code> by rotating by 90 degrees.`,name:"do_align_long_axis"},{anchor:"transformers.NougatImageProcessor.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to pad the images to the largest image size in the batch.`,name:"do_pad"},{anchor:"transformers.NougatImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.NougatImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in the
<code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.NougatImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by <code>do_normalize</code> in the <code>preprocess</code> method.`,name:"do_normalize"},{anchor:"transformers.NougatImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_DEFAULT_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.NougatImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_DEFAULT_STD</code>) &#x2014;
Image standard deviation.`,name:"image_std"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/image_processing_nougat.py#L57"}}),me=new $({props:{name:"preprocess",anchor:"transformers.NougatImageProcessor.preprocess",parameters:[{name:"images",val:": Union"},{name:"do_crop_margin",val:": bool = None"},{name:"do_resize",val:": bool = None"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = None"},{name:"do_thumbnail",val:": bool = None"},{name:"do_align_long_axis",val:": bool = None"},{name:"do_pad",val:": bool = None"},{name:"do_rescale",val:": bool = None"},{name:"rescale_factor",val:": Union = None"},{name:"do_normalize",val:": bool = None"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"return_tensors",val:": Union = None"},{name:"data_format",val:": Optional = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.NougatImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255.`,name:"images"},{anchor:"transformers.NougatImageProcessor.preprocess.do_crop_margin",description:`<strong>do_crop_margin</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_crop_margin</code>) &#x2014;
Whether to crop the image margins.`,name:"do_crop_margin"},{anchor:"transformers.NougatImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.NougatImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image after resizing. Shortest edge of the image is resized to min(size[&#x201C;height&#x201D;],
size[&#x201C;width&#x201D;]) with the longest edge resized to keep the input aspect ratio.`,name:"size"},{anchor:"transformers.NougatImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.NougatImageProcessor.preprocess.do_thumbnail",description:`<strong>do_thumbnail</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_thumbnail</code>) &#x2014;
Whether to resize the image using thumbnail method.`,name:"do_thumbnail"},{anchor:"transformers.NougatImageProcessor.preprocess.do_align_long_axis",description:`<strong>do_align_long_axis</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_align_long_axis</code>) &#x2014;
Whether to align the long axis of the image with the long axis of <code>size</code> by rotating by 90 degrees.`,name:"do_align_long_axis"},{anchor:"transformers.NougatImageProcessor.preprocess.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_pad</code>) &#x2014;
Whether to pad the images to the largest image size in the batch.`,name:"do_pad"},{anchor:"transformers.NougatImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>.`,name:"do_rescale"},{anchor:"transformers.NougatImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Scale factor to use if rescaling the image.`,name:"rescale_factor"},{anchor:"transformers.NougatImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.NougatImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean to use for normalization.`,name:"image_mean"},{anchor:"transformers.NougatImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation to use for normalization.`,name:"image_std"},{anchor:"transformers.NougatImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.NougatImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: defaults to the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.NougatImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/image_processing_nougat.py#L358"}}),pe=new $e({props:{title:"NougatTokenizerFast",local:"transformers.NougatTokenizerFast",headingTag:"h2"}}),ge=new $({props:{name:"class transformers.NougatTokenizerFast",anchor:"transformers.NougatTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"clean_up_tokenization_spaces",val:" = False"},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"pad_token",val:" = '<pad>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.NougatTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a .model extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.NougatTokenizerFast.tokenizer_file",description:`<strong>tokenizer_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
<a href="https://github.com/huggingface/tokenizers" rel="nofollow">tokenizers</a> file (generally has a .json extension) that
contains everything needed to load the tokenizer.`,name:"tokenizer_file"},{anchor:"transformers.NougatTokenizerFast.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>str</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Wether to cleanup spaces after decoding, cleanup consists in removing potential artifacts like extra
spaces.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.NougatTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.NougatTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.`,name:"bos_token"},{anchor:"transformers.NougatTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.NougatTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.NougatTokenizerFast.model_max_length",description:`<strong>model_max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is
loaded with <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.from_pretrained">from_pretrained()</a>, this will be set to the
value stored for the associated model in <code>max_model_input_sizes</code> (see above). If no value is provided, will
default to VERY_LARGE_INTEGER (<code>int(1e30)</code>).`,name:"model_max_length"},{anchor:"transformers.NougatTokenizerFast.padding_side",description:`<strong>padding_side</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The side on which the model should have padding applied. Should be selected between [&#x2018;right&#x2019;, &#x2018;left&#x2019;].
Default value is picked from the class attribute of the same name.`,name:"padding_side"},{anchor:"transformers.NougatTokenizerFast.truncation_side",description:`<strong>truncation_side</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The side on which the model should have truncation applied. Should be selected between [&#x2018;right&#x2019;, &#x2018;left&#x2019;].
Default value is picked from the class attribute of the same name.`,name:"truncation_side"},{anchor:"transformers.NougatTokenizerFast.chat_template",description:`<strong>chat_template</strong> (<code>str</code>, <em>optional</em>) &#x2014;
A Jinja template string that will be used to format lists of chat messages. See
<a href="https://huggingface.co/docs/transformers/chat_templating" rel="nofollow">https://huggingface.co/docs/transformers/chat_templating</a> for a full description.`,name:"chat_template"},{anchor:"transformers.NougatTokenizerFast.model_input_names",description:`<strong>model_input_names</strong> (<code>List[string]</code>, <em>optional</em>) &#x2014;
The list of inputs accepted by the forward pass of the model (like <code>&quot;token_type_ids&quot;</code> or
<code>&quot;attention_mask&quot;</code>). Default value is picked from the class attribute of the same name.`,name:"model_input_names"},{anchor:"transformers.NougatTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>) &#x2014;
A special token representing the beginning of a sentence. Will be associated to <code>self.bos_token</code> and
<code>self.bos_token_id</code>.`,name:"bos_token"},{anchor:"transformers.NougatTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>) &#x2014;
A special token representing the end of a sentence. Will be associated to <code>self.eos_token</code> and
<code>self.eos_token_id</code>.`,name:"eos_token"},{anchor:"transformers.NougatTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>) &#x2014;
A special token representing an out-of-vocabulary token. Will be associated to <code>self.unk_token</code> and
<code>self.unk_token_id</code>.`,name:"unk_token"},{anchor:"transformers.NougatTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>) &#x2014;
A special token separating two different sentences in the same input (used by BERT for instance). Will be
associated to <code>self.sep_token</code> and <code>self.sep_token_id</code>.`,name:"sep_token"},{anchor:"transformers.NougatTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>) &#x2014;
A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
attention mechanisms or loss computation. Will be associated to <code>self.pad_token</code> and <code>self.pad_token_id</code>.`,name:"pad_token"},{anchor:"transformers.NougatTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>) &#x2014;
A special token representing the class of the input (used by BERT for instance). Will be associated to
<code>self.cls_token</code> and <code>self.cls_token_id</code>.`,name:"cls_token"},{anchor:"transformers.NougatTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>) &#x2014;
A special token representing a masked token (used by masked-language modeling pretraining objectives, like
BERT). Will be associated to <code>self.mask_token</code> and <code>self.mask_token_id</code>.`,name:"mask_token"},{anchor:"transformers.NougatTokenizerFast.additional_special_tokens",description:`<strong>additional_special_tokens</strong> (tuple or list of <code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>) &#x2014;
A tuple or a list of additional special tokens. Add them here to ensure they are skipped when decoding with
<code>skip_special_tokens</code> is set to True. If they are not part of the vocabulary, they will be added at the end
of the vocabulary.`,name:"additional_special_tokens"},{anchor:"transformers.NougatTokenizerFast.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should cleanup the spaces that were added when splitting the input text during the
tokenization process.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.NougatTokenizerFast.split_special_tokens",description:`<strong>split_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the special tokens should be split during the tokenization process. The default behavior is
to not split special tokens. This means that if <code>&lt;s&gt;</code> is the <code>bos_token</code>, then <code>tokenizer.tokenize(&quot;&lt;s&gt;&quot;) = [&apos;&lt;s&gt;</code>]. Otherwise, if <code>split_special_tokens=True</code>, then <code>tokenizer.tokenize(&quot;&lt;s&gt;&quot;)</code> will be give <code>[&apos;&lt;&apos;, &apos;s&apos;, &apos;&gt;&apos;]</code>. This argument is only supported for <code>slow</code> tokenizers for the moment.`,name:"split_special_tokens"},{anchor:"transformers.NougatTokenizerFast.tokenizer_object",description:`<strong>tokenizer_object</strong> (<a href="https://huggingface.co/docs/tokenizers/main/en/api/tokenizer#tokenizers.Tokenizer" rel="nofollow">tokenizers.Tokenizer</a>) &#x2014;
A <a href="https://huggingface.co/docs/tokenizers/main/en/api/tokenizer#tokenizers.Tokenizer" rel="nofollow">tokenizers.Tokenizer</a> object from &#x1F917; tokenizers to instantiate from. See <a href="../fast_tokenizers">Using tokenizers from &#x1F917;
tokenizers</a> for more information.`,name:"tokenizer_object"},{anchor:"transformers.NougatTokenizerFast.tokenizer_file",description:`<strong>tokenizer_file</strong> (<code>str</code>) &#x2014;
A path to a local JSON file representing a previously serialized <a href="https://huggingface.co/docs/tokenizers/main/en/api/tokenizer#tokenizers.Tokenizer" rel="nofollow">tokenizers.Tokenizer</a> object from &#x1F917;
tokenizers.`,name:"tokenizer_file"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/tokenization_nougat_fast.py#L376"}}),fe=new $({props:{name:"correct_tables",anchor:"transformers.NougatTokenizerFast.correct_tables",parameters:[{name:"generation",val:": str"}],parametersDescription:[{anchor:"transformers.NougatTokenizerFast.correct_tables.generation",description:"<strong>generation</strong> (str) &#x2014; The generated text to be postprocessed.",name:"generation"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/tokenization_nougat_fast.py#L470",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The postprocessed text.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>str</p>
`}}),R=new Ao({props:{anchor:"transformers.NougatTokenizerFast.correct_tables.example",$$slots:{default:[Qo]},$$scope:{ctx:I}}}),ue=new $({props:{name:"post_process_generation",anchor:"transformers.NougatTokenizerFast.post_process_generation",parameters:[{name:"generation",val:": Union"},{name:"fix_markdown",val:": bool = True"},{name:"num_workers",val:": int = None"}],parametersDescription:[{anchor:"transformers.NougatTokenizerFast.post_process_generation.generation",description:`<strong>generation</strong> (Union[str, List[str]]) &#x2014;
The generated text or a list of generated texts.`,name:"generation"},{anchor:"transformers.NougatTokenizerFast.post_process_generation.fix_markdown",description:`<strong>fix_markdown</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to perform Markdown formatting fixes.`,name:"fix_markdown"},{anchor:"transformers.NougatTokenizerFast.post_process_generation.num_workers",description:`<strong>num_workers</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Optional number of workers to pass to leverage multiprocessing (postprocessing several texts in
parallel).`,name:"num_workers"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/tokenization_nougat_fast.py#L600",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The postprocessed text or list of postprocessed texts.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Union[str, List[str]]</p>
`}}),he=new $({props:{name:"post_process_single",anchor:"transformers.NougatTokenizerFast.post_process_single",parameters:[{name:"generation",val:": str"},{name:"fix_markdown",val:": bool = True"}],parametersDescription:[{anchor:"transformers.NougatTokenizerFast.post_process_single.generation",description:"<strong>generation</strong> (str) &#x2014; The generated text to be postprocessed.",name:"generation"},{anchor:"transformers.NougatTokenizerFast.post_process_single.fix_markdown",description:"<strong>fix_markdown</strong> (bool, optional) &#x2014; Whether to perform Markdown formatting fixes. Default is True.",name:"fix_markdown"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/tokenization_nougat_fast.py#L505",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The postprocessed text.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>str</p>
`}}),_e=new $({props:{name:"remove_hallucinated_references",anchor:"transformers.NougatTokenizerFast.remove_hallucinated_references",parameters:[{name:"text",val:": str"}],parametersDescription:[{anchor:"transformers.NougatTokenizerFast.remove_hallucinated_references.text",description:`<strong>text</strong> (<code>str</code>) &#x2014;
The input text containing references.`,name:"text"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/tokenization_nougat_fast.py#L440",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text with hallucinated references removed.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>str</code></p>
`}}),be=new $e({props:{title:"NougatProcessor",local:"transformers.NougatProcessor",headingTag:"h2"}}),ke=new $({props:{name:"class transformers.NougatProcessor",anchor:"transformers.NougatProcessor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.NougatProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatImageProcessor">NougatImageProcessor</a>) &#x2014;
An instance of <a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatImageProcessor">NougatImageProcessor</a>. The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.NougatProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatTokenizerFast">NougatTokenizerFast</a>) &#x2014;
An instance of <a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatTokenizerFast">NougatTokenizerFast</a>. The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/processing_nougat.py#L27"}}),ve=new $({props:{name:"__call__",anchor:"transformers.NougatProcessor.__call__",parameters:[{name:"images",val:" = None"},{name:"text",val:" = None"},{name:"do_crop_margin",val:": bool = None"},{name:"do_resize",val:": bool = None"},{name:"size",val:": Dict = None"},{name:"resample",val:": PILImageResampling = None"},{name:"do_thumbnail",val:": bool = None"},{name:"do_align_long_axis",val:": bool = None"},{name:"do_pad",val:": bool = None"},{name:"do_rescale",val:": bool = None"},{name:"rescale_factor",val:": Union = None"},{name:"do_normalize",val:": bool = None"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"data_format",val:": Optional = 'channels_first'"},{name:"input_data_format",val:": Union = None"},{name:"text_pair",val:": Union = None"},{name:"text_target",val:": Union = None"},{name:"text_pair_target",val:": Union = None"},{name:"add_special_tokens",val:": bool = True"},{name:"padding",val:": Union = False"},{name:"truncation",val:": Union = None"},{name:"max_length",val:": Optional = None"},{name:"stride",val:": int = 0"},{name:"is_split_into_words",val:": bool = False"},{name:"pad_to_multiple_of",val:": Optional = None"},{name:"return_tensors",val:": Union = None"},{name:"return_token_type_ids",val:": Optional = None"},{name:"return_attention_mask",val:": Optional = None"},{name:"return_overflowing_tokens",val:": bool = False"},{name:"return_special_tokens_mask",val:": bool = False"},{name:"return_offsets_mapping",val:": bool = False"},{name:"return_length",val:": bool = False"},{name:"verbose",val:": bool = True"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/processing_nougat.py#L49"}}),Te=new $({props:{name:"from_pretrained",anchor:"transformers.NougatProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": Union"},{name:"cache_dir",val:": Union = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": Union = None"},{name:"revision",val:": str = 'main'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.NougatProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.
**kwargs &#x2014;
Additional keyword arguments passed along to both
<a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.from_pretrained">from_pretrained()</a> and
<code>~tokenization_utils_base.PreTrainedTokenizer.from_pretrained</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/processing_utils.py#L406"}}),q=new io({props:{$$slots:{default:[Xo]},$$scope:{ctx:I}}}),xe=new $({props:{name:"save_pretrained",anchor:"transformers.NougatProcessor.save_pretrained",parameters:[{name:"save_directory",val:""},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.NougatProcessor.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the feature extractor JSON file and the tokenizer files will be saved (directory will
be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.NougatProcessor.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.NougatProcessor.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/processing_utils.py#L167"}}),B=new io({props:{$$slots:{default:[Oo]},$$scope:{ctx:I}}}),ye=new $({props:{name:"batch_decode",anchor:"transformers.NougatProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/processing_nougat.py#L141"}}),Ne=new $({props:{name:"decode",anchor:"transformers.NougatProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/processing_nougat.py#L148"}}),ze=new $({props:{name:"post_process_generation",anchor:"transformers.NougatProcessor.post_process_generation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nougat/processing_nougat.py#L155"}}),{c(){c=s("meta"),z=a(),m=s("p"),y=a(),p(M.$$.fragment),T=a(),p(C.$$.fragment),Ke=a(),Q=s("p"),Q.innerHTML=lo,et=a(),X=s("p"),X.textContent=co,tt=a(),O=s("p"),O.innerHTML=mo,ot=a(),E=s("img"),nt=a(),K=s("small"),K.innerHTML=go,at=a(),ee=s("p"),ee.innerHTML=fo,rt=a(),p(te.$$.fragment),st=a(),oe=s("ul"),oe.innerHTML=uo,it=a(),p(ne.$$.fragment),lt=a(),ae=s("p"),ae.innerHTML=ho,ct=a(),re=s("p"),re.innerHTML=_o,dt=a(),se=s("ul"),se.innerHTML=bo,mt=a(),p(ie.$$.fragment),pt=a(),le=s("p"),le.innerHTML=ko,gt=a(),p(D.$$.fragment),ft=a(),p(ce.$$.fragment),ut=a(),U=s("div"),p(de.$$.fragment),$t=a(),Me=s("p"),Me.textContent=vo,Mt=a(),V=s("div"),p(me.$$.fragment),Pt=a(),Pe=s("p"),Pe.textContent=To,ht=a(),p(pe.$$.fragment),_t=a(),b=s("div"),p(ge.$$.fragment),Ct=a(),Ce=s("p"),Ce.textContent=xo,Ut=a(),Ue=s("p"),Ue.innerHTML=yo,It=a(),Ie=s("p"),Ie.textContent=No,Ft=a(),Fe=s("ul"),Fe.innerHTML=zo,jt=a(),F=s("div"),p(fe.$$.fragment),Jt=a(),je=s("p"),je.textContent=wo,Wt=a(),p(R.$$.fragment),Zt=a(),P=s("div"),p(ue.$$.fragment),Lt=a(),Je=s("p"),Je.textContent=$o,Et=a(),We=s("p"),We.textContent=Mo,Dt=a(),Ze=s("p"),Ze.textContent=Po,Vt=a(),H=s("div"),p(he.$$.fragment),Rt=a(),Le=s("p"),Le.textContent=Co,Ht=a(),j=s("div"),p(_e.$$.fragment),qt=a(),Ee=s("p"),Ee.textContent=Uo,Bt=a(),De=s("p"),De.textContent=Io,bt=a(),p(be.$$.fragment),kt=a(),k=s("div"),p(ke.$$.fragment),Gt=a(),Ve=s("p"),Ve.textContent=Fo,St=a(),Re=s("p"),Re.innerHTML=jo,At=a(),He=s("div"),p(ve.$$.fragment),Yt=a(),J=s("div"),p(Te.$$.fragment),Qt=a(),qe=s("p"),qe.textContent=Jo,Xt=a(),p(q.$$.fragment),Ot=a(),W=s("div"),p(xe.$$.fragment),Kt=a(),Be=s("p"),Be.innerHTML=Wo,eo=a(),p(B.$$.fragment),to=a(),G=s("div"),p(ye.$$.fragment),oo=a(),Ge=s("p"),Ge.innerHTML=Zo,no=a(),S=s("div"),p(Ne.$$.fragment),ao=a(),Se=s("p"),Se.innerHTML=Lo,ro=a(),A=s("div"),p(ze.$$.fragment),so=a(),Ae=s("p"),Ae.innerHTML=Eo,vt=a(),Oe=s("p"),this.h()},l(e){const n=So("svelte-u9bgzb",document.head);c=i(n,"META",{name:!0,content:!0}),n.forEach(o),z=r(e),m=i(e,"P",{}),w(m).forEach(o),y=r(e),g(M.$$.fragment,e),T=r(e),g(C.$$.fragment,e),Ke=r(e),Q=i(e,"P",{"data-svelte-h":!0}),d(Q)!=="svelte-1uufact"&&(Q.innerHTML=lo),et=r(e),X=i(e,"P",{"data-svelte-h":!0}),d(X)!=="svelte-vfdo9a"&&(X.textContent=co),tt=r(e),O=i(e,"P",{"data-svelte-h":!0}),d(O)!=="svelte-1io5hvx"&&(O.innerHTML=mo),ot=r(e),E=i(e,"IMG",{src:!0,alt:!0,width:!0}),nt=r(e),K=i(e,"SMALL",{"data-svelte-h":!0}),d(K)!=="svelte-hlvtr5"&&(K.innerHTML=go),at=r(e),ee=i(e,"P",{"data-svelte-h":!0}),d(ee)!=="svelte-jkp8zk"&&(ee.innerHTML=fo),rt=r(e),g(te.$$.fragment,e),st=r(e),oe=i(e,"UL",{"data-svelte-h":!0}),d(oe)!=="svelte-zxlclw"&&(oe.innerHTML=uo),it=r(e),g(ne.$$.fragment,e),lt=r(e),ae=i(e,"P",{"data-svelte-h":!0}),d(ae)!=="svelte-bbg7pv"&&(ae.innerHTML=ho),ct=r(e),re=i(e,"P",{"data-svelte-h":!0}),d(re)!=="svelte-vq46hj"&&(re.innerHTML=_o),dt=r(e),se=i(e,"UL",{"data-svelte-h":!0}),d(se)!=="svelte-1x92kdi"&&(se.innerHTML=bo),mt=r(e),g(ie.$$.fragment,e),pt=r(e),le=i(e,"P",{"data-svelte-h":!0}),d(le)!=="svelte-5b9r1n"&&(le.innerHTML=ko),gt=r(e),g(D.$$.fragment,e),ft=r(e),g(ce.$$.fragment,e),ut=r(e),U=i(e,"DIV",{class:!0});var Z=w(U);g(de.$$.fragment,Z),$t=r(Z),Me=i(Z,"P",{"data-svelte-h":!0}),d(Me)!=="svelte-1e7ox92"&&(Me.textContent=vo),Mt=r(Z),V=i(Z,"DIV",{class:!0});var we=w(V);g(me.$$.fragment,we),Pt=r(we),Pe=i(we,"P",{"data-svelte-h":!0}),d(Pe)!=="svelte-1x3yxsa"&&(Pe.textContent=To),we.forEach(o),Z.forEach(o),ht=r(e),g(pe.$$.fragment,e),_t=r(e),b=i(e,"DIV",{class:!0});var v=w(b);g(ge.$$.fragment,v),Ct=r(v),Ce=i(v,"P",{"data-svelte-h":!0}),d(Ce)!=="svelte-srzklv"&&(Ce.textContent=xo),Ut=r(v),Ue=i(v,"P",{"data-svelte-h":!0}),d(Ue)!=="svelte-874unm"&&(Ue.innerHTML=yo),It=r(v),Ie=i(v,"P",{"data-svelte-h":!0}),d(Ie)!=="svelte-1ixo79u"&&(Ie.textContent=No),Ft=r(v),Fe=i(v,"UL",{"data-svelte-h":!0}),d(Fe)!=="svelte-5wckzo"&&(Fe.innerHTML=zo),jt=r(v),F=i(v,"DIV",{class:!0});var L=w(F);g(fe.$$.fragment,L),Jt=r(L),je=i(L,"P",{"data-svelte-h":!0}),d(je)!=="svelte-3j23fp"&&(je.textContent=wo),Wt=r(L),g(R.$$.fragment,L),L.forEach(o),Zt=r(v),P=i(v,"DIV",{class:!0});var Y=w(P);g(ue.$$.fragment,Y),Lt=r(Y),Je=i(Y,"P",{"data-svelte-h":!0}),d(Je)!=="svelte-15rcjls"&&(Je.textContent=$o),Et=r(Y),We=i(Y,"P",{"data-svelte-h":!0}),d(We)!=="svelte-vhmx5q"&&(We.textContent=Mo),Dt=r(Y),Ze=i(Y,"P",{"data-svelte-h":!0}),d(Ze)!=="svelte-vjc015"&&(Ze.textContent=Po),Y.forEach(o),Vt=r(v),H=i(v,"DIV",{class:!0});var xt=w(H);g(he.$$.fragment,xt),Rt=r(xt),Le=i(xt,"P",{"data-svelte-h":!0}),d(Le)!=="svelte-44h31"&&(Le.textContent=Co),xt.forEach(o),Ht=r(v),j=i(v,"DIV",{class:!0});var Ye=w(j);g(_e.$$.fragment,Ye),qt=r(Ye),Ee=i(Ye,"P",{"data-svelte-h":!0}),d(Ee)!=="svelte-jqzf81"&&(Ee.textContent=Uo),Bt=r(Ye),De=i(Ye,"P",{"data-svelte-h":!0}),d(De)!=="svelte-1dlctcq"&&(De.textContent=Io),Ye.forEach(o),v.forEach(o),bt=r(e),g(be.$$.fragment,e),kt=r(e),k=i(e,"DIV",{class:!0});var N=w(k);g(ke.$$.fragment,N),Gt=r(N),Ve=i(N,"P",{"data-svelte-h":!0}),d(Ve)!=="svelte-1rx2qxv"&&(Ve.textContent=Fo),St=r(N),Re=i(N,"P",{"data-svelte-h":!0}),d(Re)!=="svelte-15g1wot"&&(Re.innerHTML=jo),At=r(N),He=i(N,"DIV",{class:!0});var Do=w(He);g(ve.$$.fragment,Do),Do.forEach(o),Yt=r(N),J=i(N,"DIV",{class:!0});var Qe=w(J);g(Te.$$.fragment,Qe),Qt=r(Qe),qe=i(Qe,"P",{"data-svelte-h":!0}),d(qe)!=="svelte-1cj8dcb"&&(qe.textContent=Jo),Xt=r(Qe),g(q.$$.fragment,Qe),Qe.forEach(o),Ot=r(N),W=i(N,"DIV",{class:!0});var Xe=w(W);g(xe.$$.fragment,Xe),Kt=r(Xe),Be=i(Xe,"P",{"data-svelte-h":!0}),d(Be)!=="svelte-pd1tp5"&&(Be.innerHTML=Wo),eo=r(Xe),g(B.$$.fragment,Xe),Xe.forEach(o),to=r(N),G=i(N,"DIV",{class:!0});var yt=w(G);g(ye.$$.fragment,yt),oo=r(yt),Ge=i(yt,"P",{"data-svelte-h":!0}),d(Ge)!=="svelte-1l7d5cb"&&(Ge.innerHTML=Zo),yt.forEach(o),no=r(N),S=i(N,"DIV",{class:!0});var Nt=w(S);g(Ne.$$.fragment,Nt),ao=r(Nt),Se=i(Nt,"P",{"data-svelte-h":!0}),d(Se)!=="svelte-1c4vvz2"&&(Se.innerHTML=Lo),Nt.forEach(o),ro=r(N),A=i(N,"DIV",{class:!0});var zt=w(A);g(ze.$$.fragment,zt),so=r(zt),Ae=i(zt,"P",{"data-svelte-h":!0}),d(Ae)!=="svelte-1mcciwe"&&(Ae.innerHTML=Eo),zt.forEach(o),N.forEach(o),vt=r(e),Oe=i(e,"P",{}),w(Oe).forEach(o),this.h()},h(){x(c,"name","hf:doc:metadata"),x(c,"content",en),Ho(E.src,po="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/nougat_architecture.jpg")||x(E,"src",po),x(E,"alt","drawing"),x(E,"width","600"),x(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){t(document.head,c),l(e,z,n),l(e,m,n),l(e,y,n),f(M,e,n),l(e,T,n),f(C,e,n),l(e,Ke,n),l(e,Q,n),l(e,et,n),l(e,X,n),l(e,tt,n),l(e,O,n),l(e,ot,n),l(e,E,n),l(e,nt,n),l(e,K,n),l(e,at,n),l(e,ee,n),l(e,rt,n),f(te,e,n),l(e,st,n),l(e,oe,n),l(e,it,n),f(ne,e,n),l(e,lt,n),l(e,ae,n),l(e,ct,n),l(e,re,n),l(e,dt,n),l(e,se,n),l(e,mt,n),f(ie,e,n),l(e,pt,n),l(e,le,n),l(e,gt,n),f(D,e,n),l(e,ft,n),f(ce,e,n),l(e,ut,n),l(e,U,n),f(de,U,null),t(U,$t),t(U,Me),t(U,Mt),t(U,V),f(me,V,null),t(V,Pt),t(V,Pe),l(e,ht,n),f(pe,e,n),l(e,_t,n),l(e,b,n),f(ge,b,null),t(b,Ct),t(b,Ce),t(b,Ut),t(b,Ue),t(b,It),t(b,Ie),t(b,Ft),t(b,Fe),t(b,jt),t(b,F),f(fe,F,null),t(F,Jt),t(F,je),t(F,Wt),f(R,F,null),t(b,Zt),t(b,P),f(ue,P,null),t(P,Lt),t(P,Je),t(P,Et),t(P,We),t(P,Dt),t(P,Ze),t(b,Vt),t(b,H),f(he,H,null),t(H,Rt),t(H,Le),t(b,Ht),t(b,j),f(_e,j,null),t(j,qt),t(j,Ee),t(j,Bt),t(j,De),l(e,bt,n),f(be,e,n),l(e,kt,n),l(e,k,n),f(ke,k,null),t(k,Gt),t(k,Ve),t(k,St),t(k,Re),t(k,At),t(k,He),f(ve,He,null),t(k,Yt),t(k,J),f(Te,J,null),t(J,Qt),t(J,qe),t(J,Xt),f(q,J,null),t(k,Ot),t(k,W),f(xe,W,null),t(W,Kt),t(W,Be),t(W,eo),f(B,W,null),t(k,to),t(k,G),f(ye,G,null),t(G,oo),t(G,Ge),t(k,no),t(k,S),f(Ne,S,null),t(S,ao),t(S,Se),t(k,ro),t(k,A),f(ze,A,null),t(A,so),t(A,Ae),l(e,vt,n),l(e,Oe,n),Tt=!0},p(e,[n]){const Z={};n&2&&(Z.$$scope={dirty:n,ctx:e}),D.$set(Z);const we={};n&2&&(we.$$scope={dirty:n,ctx:e}),R.$set(we);const v={};n&2&&(v.$$scope={dirty:n,ctx:e}),q.$set(v);const L={};n&2&&(L.$$scope={dirty:n,ctx:e}),B.$set(L)},i(e){Tt||(u(M.$$.fragment,e),u(C.$$.fragment,e),u(te.$$.fragment,e),u(ne.$$.fragment,e),u(ie.$$.fragment,e),u(D.$$.fragment,e),u(ce.$$.fragment,e),u(de.$$.fragment,e),u(me.$$.fragment,e),u(pe.$$.fragment,e),u(ge.$$.fragment,e),u(fe.$$.fragment,e),u(R.$$.fragment,e),u(ue.$$.fragment,e),u(he.$$.fragment,e),u(_e.$$.fragment,e),u(be.$$.fragment,e),u(ke.$$.fragment,e),u(ve.$$.fragment,e),u(Te.$$.fragment,e),u(q.$$.fragment,e),u(xe.$$.fragment,e),u(B.$$.fragment,e),u(ye.$$.fragment,e),u(Ne.$$.fragment,e),u(ze.$$.fragment,e),Tt=!0)},o(e){h(M.$$.fragment,e),h(C.$$.fragment,e),h(te.$$.fragment,e),h(ne.$$.fragment,e),h(ie.$$.fragment,e),h(D.$$.fragment,e),h(ce.$$.fragment,e),h(de.$$.fragment,e),h(me.$$.fragment,e),h(pe.$$.fragment,e),h(ge.$$.fragment,e),h(fe.$$.fragment,e),h(R.$$.fragment,e),h(ue.$$.fragment,e),h(he.$$.fragment,e),h(_e.$$.fragment,e),h(be.$$.fragment,e),h(ke.$$.fragment,e),h(ve.$$.fragment,e),h(Te.$$.fragment,e),h(q.$$.fragment,e),h(xe.$$.fragment,e),h(B.$$.fragment,e),h(ye.$$.fragment,e),h(Ne.$$.fragment,e),h(ze.$$.fragment,e),Tt=!1},d(e){e&&(o(z),o(m),o(y),o(T),o(Ke),o(Q),o(et),o(X),o(tt),o(O),o(ot),o(E),o(nt),o(K),o(at),o(ee),o(rt),o(st),o(oe),o(it),o(lt),o(ae),o(ct),o(re),o(dt),o(se),o(mt),o(pt),o(le),o(gt),o(ft),o(ut),o(U),o(ht),o(_t),o(b),o(bt),o(kt),o(k),o(vt),o(Oe)),o(c),_(M,e),_(C,e),_(te,e),_(ne,e),_(ie,e),_(D,e),_(ce,e),_(de),_(me),_(pe,e),_(ge),_(fe),_(R),_(ue),_(he),_(_e),_(be,e),_(ke),_(ve),_(Te),_(q),_(xe),_(B),_(ye),_(Ne),_(ze)}}}const en='{"title":"Nougat","local":"nougat","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Inference","local":"inference","sections":[],"depth":2},{"title":"NougatImageProcessor","local":"transformers.NougatImageProcessor","sections":[],"depth":2},{"title":"NougatTokenizerFast","local":"transformers.NougatTokenizerFast","sections":[],"depth":2},{"title":"NougatProcessor","local":"transformers.NougatProcessor","sections":[],"depth":2}],"depth":1}';function tn(I){return qo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class dn extends Bo{constructor(c){super(),Go(this,c,tn,Ko,Ro,{})}}export{dn as component};
