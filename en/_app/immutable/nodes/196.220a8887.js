import{s as Ct,o as xt,n as fe}from"../chunks/scheduler.9bc65507.js";import{S as Jt,i as Wt,g as d,s as r,r as g,A as Ut,h as p,f as s,c as i,j as z,u,x as v,k as N,y as m,a as c,v as b,d as _,t as M,w as T}from"../chunks/index.707bf1b6.js";import{T as mt}from"../chunks/Tip.c2ecdbf4.js";import{D as de}from"../chunks/Docstring.17db21ae.js";import{C as Xe}from"../chunks/CodeBlock.54a9f38d.js";import{E as Pe}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as pe}from"../chunks/Heading.342b1fa6.js";function Zt(w){let t,h="Example:",a,l,f;return l=new Xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1vYmlsZVZpVFYyQ29uZmlnJTJDJTIwTW9iaWxlVmlUVjJNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2JpbGV2aXR2Mi1zbWFsbCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBNb2JpbGVWaVRWMkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMG1vYmlsZXZpdHYyLXNtYWxsJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBNb2JpbGVWaVRWMk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MobileViTV2Config, MobileViTV2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a mobilevitv2-small style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MobileViTV2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the mobilevitv2-small style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MobileViTV2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=d("p"),t.textContent=h,a=r(),g(l.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),v(t)!=="svelte-11lpom8"&&(t.textContent=h),a=i(o),u(l.$$.fragment,o)},m(o,V){c(o,t,V),c(o,a,V),b(l,o,V),f=!0},p:fe,i(o){f||(_(l.$$.fragment,o),f=!0)},o(o){M(l.$$.fragment,o),f=!1},d(o){o&&(s(t),s(a)),T(l,o)}}}function It(w){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=h},l(a){t=p(a,"P",{"data-svelte-h":!0}),v(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(a,l){c(a,t,l)},p:fe,d(a){a&&s(t)}}}function Ft(w){let t,h="Example:",a,l,f;return l=new Xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyME1vYmlsZVZpVFYyTW9kZWwlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmFwcGxlJTJGbW9iaWxldml0djItMS4wLWltYWdlbmV0MWstMjU2JTIyKSUwQW1vZGVsJTIwJTNEJTIwTW9iaWxlVmlUVjJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyYXBwbGUlMkZtb2JpbGV2aXR2Mi0xLjAtaW1hZ2VuZXQxay0yNTYlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, MobileViTV2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;apple/mobilevitv2-1.0-imagenet1k-256&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MobileViTV2Model.from_pretrained(<span class="hljs-string">&quot;apple/mobilevitv2-1.0-imagenet1k-256&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">8</span>, <span class="hljs-number">8</span>]`,wrap:!1}}),{c(){t=d("p"),t.textContent=h,a=r(),g(l.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),v(t)!=="svelte-11lpom8"&&(t.textContent=h),a=i(o),u(l.$$.fragment,o)},m(o,V){c(o,t,V),c(o,a,V),b(l,o,V),f=!0},p:fe,i(o){f||(_(l.$$.fragment,o),f=!0)},o(o){M(l.$$.fragment,o),f=!1},d(o){o&&(s(t),s(a)),T(l,o)}}}function kt(w){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=h},l(a){t=p(a,"P",{"data-svelte-h":!0}),v(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(a,l){c(a,t,l)},p:fe,d(a){a&&s(t)}}}function St(w){let t,h="Example:",a,l,f;return l=new Xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyME1vYmlsZVZpVFYyRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyYXBwbGUlMkZtb2JpbGV2aXR2Mi0xLjAtaW1hZ2VuZXQxay0yNTYlMjIpJTBBbW9kZWwlMjAlM0QlMjBNb2JpbGVWaVRWMkZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmFwcGxlJTJGbW9iaWxldml0djItMS4wLWltYWdlbmV0MWstMjU2JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, MobileViTV2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;apple/mobilevitv2-1.0-imagenet1k-256&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MobileViTV2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;apple/mobilevitv2-1.0-imagenet1k-256&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){t=d("p"),t.textContent=h,a=r(),g(l.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),v(t)!=="svelte-11lpom8"&&(t.textContent=h),a=i(o),u(l.$$.fragment,o)},m(o,V){c(o,t,V),c(o,a,V),b(l,o,V),f=!0},p:fe,i(o){f||(_(l.$$.fragment,o),f=!0)},o(o){M(l.$$.fragment,o),f=!1},d(o){o&&(s(t),s(a)),T(l,o)}}}function Bt(w){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=h},l(a){t=p(a,"P",{"data-svelte-h":!0}),v(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(a,l){c(a,t,l)},p:fe,d(a){a&&s(t)}}}function zt(w){let t,h="Examples:",a,l,f;return l=new Xe({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvSW1hZ2VQcm9jZXNzb3IlMkMlMjBNb2JpbGVWaVRWMkZvclNlbWFudGljU2VnbWVudGF0aW9uJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyYXBwbGUlMkZtb2JpbGV2aXR2Mi0xLjAtaW1hZ2VuZXQxay0yNTYlMjIpJTBBbW9kZWwlMjAlM0QlMjBNb2JpbGVWaVRWMkZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJhcHBsZSUyRm1vYmlsZXZpdHYyLTEuMC1pbWFnZW5ldDFrLTI1NiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwbG9naXRzJTIwYXJlJTIwb2YlMjBzaGFwZSUyMChiYXRjaF9zaXplJTJDJTIwbnVtX2xhYmVscyUyQyUyMGhlaWdodCUyQyUyMHdpZHRoKSUwQWxvZ2l0cyUyMCUzRCUyMG91dHB1dHMubG9naXRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, MobileViTV2ForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;apple/mobilevitv2-1.0-imagenet1k-256&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MobileViTV2ForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;apple/mobilevitv2-1.0-imagenet1k-256&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># logits are of shape (batch_size, num_labels, height, width)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`,wrap:!1}}),{c(){t=d("p"),t.textContent=h,a=r(),g(l.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),v(t)!=="svelte-kvfsh7"&&(t.textContent=h),a=i(o),u(l.$$.fragment,o)},m(o,V){c(o,t,V),c(o,a,V),b(l,o,V),f=!0},p:fe,i(o){f||(_(l.$$.fragment,o),f=!0)},o(o){M(l.$$.fragment,o),f=!1},d(o){o&&(s(t),s(a)),T(l,o)}}}function Nt(w){let t,h,a,l,f,o,V,$e,L,dt='The MobileViTV2 model was proposed in <a href="https://arxiv.org/abs/2206.02680" rel="nofollow">Separable Self-attention for Mobile Vision Transformers</a> by Sachin Mehta and Mohammad Rastegari.',je,E,pt="MobileViTV2 is the second version of MobileViT, constructed by replacing the multi-headed self-attention in MobileViT with separable self-attention.",Ce,A,ft="The abstract from the paper is the following:",xe,q,ht="<em>Mobile vision transformers (MobileViT) can achieve state-of-the-art performance across several mobile vision tasks, including classification and detection. Though these models have fewer parameters, they have high latency as compared to convolutional neural network-based models. The main efficiency bottleneck in MobileViT is the multi-headed self-attention (MHA) in transformers, which requires O(k2) time complexity with respect to the number of tokens (or patches) k. Moreover, MHA requires costly operations (e.g., batch-wise matrix multiplication) for computing self-attention, impacting latency on resource-constrained devices. This paper introduces a separable self-attention method with linear complexity, i.e. O(k). A simple yet effective characteristic of the proposed method is that it uses element-wise operations for computing self-attention, making it a good choice for resource-constrained devices. The improved model, MobileViTV2, is state-of-the-art on several mobile vision tasks, including ImageNet object classification and MS-COCO object detection. With about three million parameters, MobileViTV2 achieves a top-1 accuracy of 75.6% on the ImageNet dataset, outperforming MobileViT by about 1% while running 3.2× faster on a mobile device.</em>",Je,O,gt=`This model was contributed by <a href="https://huggingface.co/shehan97" rel="nofollow">shehan97</a>.
The original code can be found <a href="https://github.com/apple/ml-cvnets" rel="nofollow">here</a>.`,We,D,Ue,K,ut='<li>MobileViTV2 is more like a CNN than a Transformer model. It does not work on sequence data but on batches of images. Unlike ViT, there are no embeddings. The backbone model outputs a feature map.</li> <li>One can use <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a> to prepare images for the model. Note that if you do your own preprocessing, the pretrained checkpoints expect images to be in BGR pixel order (not RGB).</li> <li>The available image classification checkpoints are pre-trained on <a href="https://huggingface.co/datasets/imagenet-1k" rel="nofollow">ImageNet-1k</a> (also referred to as ILSVRC 2012, a collection of 1.3 million images and 1,000 classes).</li> <li>The segmentation model uses a <a href="https://arxiv.org/abs/1706.05587" rel="nofollow">DeepLabV3</a> head. The available semantic segmentation checkpoints are pre-trained on <a href="http://host.robots.ox.ac.uk/pascal/VOC/" rel="nofollow">PASCAL VOC</a>.</li>',Ze,ee,Ie,y,te,Qe,he,bt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Model">MobileViTV2Model</a>. It is used to instantiate a
MobileViTV2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the MobileViTV2
<a href="https://huggingface.co/apple/mobilevitv2-1.0" rel="nofollow">apple/mobilevitv2-1.0</a> architecture.`,He,ge,_t=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ye,G,Fe,oe,ke,W,ne,Le,ue,Mt=`The bare MobileViTV2 model outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Ee,C,se,Ae,be,Tt='The <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Model">MobileViTV2Model</a> forward method, overrides the <code>__call__</code> special method.',qe,R,Oe,P,Se,ae,Be,$,re,De,_e,Vt=`MobileViTV2 model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,Ke,Me,vt=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,et,x,ie,tt,Te,wt='The <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForImageClassification">MobileViTV2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',ot,X,nt,Q,ze,le,Ne,j,ce,st,Ve,yt="MobileViTV2 model with a semantic segmentation head on top, e.g. for Pascal VOC.",at,ve,$t=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,rt,J,me,it,we,jt='The <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForSemanticSegmentation">MobileViTV2ForSemanticSegmentation</a> forward method, overrides the <code>__call__</code> special method.',lt,H,ct,Y,Ge,ye,Re;return f=new pe({props:{title:"MobileViTV2",local:"mobilevitv2",headingTag:"h1"}}),V=new pe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),D=new pe({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),ee=new pe({props:{title:"MobileViTV2Config",local:"transformers.MobileViTV2Config",headingTag:"h2"}}),te=new de({props:{name:"class transformers.MobileViTV2Config",anchor:"transformers.MobileViTV2Config",parameters:[{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 256"},{name:"patch_size",val:" = 2"},{name:"expand_ratio",val:" = 2.0"},{name:"hidden_act",val:" = 'swish'"},{name:"conv_kernel_size",val:" = 3"},{name:"output_stride",val:" = 32"},{name:"classifier_dropout_prob",val:" = 0.1"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"aspp_out_channels",val:" = 512"},{name:"atrous_rates",val:" = [6, 12, 18]"},{name:"aspp_dropout_prob",val:" = 0.1"},{name:"semantic_loss_ignore_index",val:" = 255"},{name:"n_attn_blocks",val:" = [2, 4, 3]"},{name:"base_attn_unit_dims",val:" = [128, 192, 256]"},{name:"width_multiplier",val:" = 1.0"},{name:"ffn_multiplier",val:" = 2"},{name:"attn_dropout",val:" = 0.0"},{name:"ffn_dropout",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MobileViTV2Config.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.MobileViTV2Config.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.MobileViTV2Config.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.MobileViTV2Config.expand_ratio",description:`<strong>expand_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 2.0) &#x2014;
Expansion factor for the MobileNetv2 layers.`,name:"expand_ratio"},{anchor:"transformers.MobileViTV2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;swish&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the Transformer encoder and convolution layers.`,name:"hidden_act"},{anchor:"transformers.MobileViTV2Config.conv_kernel_size",description:`<strong>conv_kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The size of the convolutional kernel in the MobileViTV2 layer.`,name:"conv_kernel_size"},{anchor:"transformers.MobileViTV2Config.output_stride",description:`<strong>output_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The ratio of the spatial resolution of the output to the resolution of the input image.`,name:"output_stride"},{anchor:"transformers.MobileViTV2Config.classifier_dropout_prob",description:`<strong>classifier_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for attached classifiers.`,name:"classifier_dropout_prob"},{anchor:"transformers.MobileViTV2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.MobileViTV2Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.MobileViTV2Config.aspp_out_channels",description:`<strong>aspp_out_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Number of output channels used in the ASPP layer for semantic segmentation.`,name:"aspp_out_channels"},{anchor:"transformers.MobileViTV2Config.atrous_rates",description:`<strong>atrous_rates</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[6, 12, 18]</code>) &#x2014;
Dilation (atrous) factors used in the ASPP layer for semantic segmentation.`,name:"atrous_rates"},{anchor:"transformers.MobileViTV2Config.aspp_dropout_prob",description:`<strong>aspp_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the ASPP layer for semantic segmentation.`,name:"aspp_dropout_prob"},{anchor:"transformers.MobileViTV2Config.semantic_loss_ignore_index",description:`<strong>semantic_loss_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to 255) &#x2014;
The index that is ignored by the loss function of the semantic segmentation model.`,name:"semantic_loss_ignore_index"},{anchor:"transformers.MobileViTV2Config.n_attn_blocks",description:`<strong>n_attn_blocks</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[2, 4, 3]</code>) &#x2014;
The number of attention blocks in each MobileViTV2Layer`,name:"n_attn_blocks"},{anchor:"transformers.MobileViTV2Config.base_attn_unit_dims",description:`<strong>base_attn_unit_dims</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[128, 192, 256]</code>) &#x2014;
The base multiplier for dimensions of attention blocks in each MobileViTV2Layer`,name:"base_attn_unit_dims"},{anchor:"transformers.MobileViTV2Config.width_multiplier",description:`<strong>width_multiplier</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The width multiplier for MobileViTV2.`,name:"width_multiplier"},{anchor:"transformers.MobileViTV2Config.ffn_multiplier",description:`<strong>ffn_multiplier</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The FFN multiplier for MobileViTV2.`,name:"ffn_multiplier"},{anchor:"transformers.MobileViTV2Config.attn_dropout",description:`<strong>attn_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout in the attention layer.`,name:"attn_dropout"},{anchor:"transformers.MobileViTV2Config.ffn_dropout",description:`<strong>ffn_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout between FFN layers.`,name:"ffn_dropout"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mobilevitv2/configuration_mobilevitv2.py#L34"}}),G=new Pe({props:{anchor:"transformers.MobileViTV2Config.example",$$slots:{default:[Zt]},$$scope:{ctx:w}}}),oe=new pe({props:{title:"MobileViTV2Model",local:"transformers.MobileViTV2Model",headingTag:"h2"}}),ne=new de({props:{name:"class transformers.MobileViTV2Model",anchor:"transformers.MobileViTV2Model",parameters:[{name:"config",val:": MobileViTV2Config"},{name:"expand_output",val:": bool = True"}],parametersDescription:[{anchor:"transformers.MobileViTV2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L650"}}),se=new de({props:{name:"forward",anchor:"transformers.MobileViTV2Model.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.MobileViTV2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__">MobileViTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.MobileViTV2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MobileViTV2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L688",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config"
>MobileViTV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new mt({props:{$$slots:{default:[It]},$$scope:{ctx:w}}}),P=new Pe({props:{anchor:"transformers.MobileViTV2Model.forward.example",$$slots:{default:[Ft]},$$scope:{ctx:w}}}),ae=new pe({props:{title:"MobileViTV2ForImageClassification",local:"transformers.MobileViTV2ForImageClassification",headingTag:"h2"}}),re=new de({props:{name:"class transformers.MobileViTV2ForImageClassification",anchor:"transformers.MobileViTV2ForImageClassification",parameters:[{name:"config",val:": MobileViTV2Config"}],parametersDescription:[{anchor:"transformers.MobileViTV2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L738"}}),ie=new de({props:{name:"forward",anchor:"transformers.MobileViTV2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.MobileViTV2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__">MobileViTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.MobileViTV2ForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MobileViTV2ForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.MobileViTV2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss). If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L763",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config"
>MobileViTV2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),X=new mt({props:{$$slots:{default:[kt]},$$scope:{ctx:w}}}),Q=new Pe({props:{anchor:"transformers.MobileViTV2ForImageClassification.forward.example",$$slots:{default:[St]},$$scope:{ctx:w}}}),le=new pe({props:{title:"MobileViTV2ForSemanticSegmentation",local:"transformers.MobileViTV2ForSemanticSegmentation",headingTag:"h2"}}),ce=new de({props:{name:"class transformers.MobileViTV2ForSemanticSegmentation",anchor:"transformers.MobileViTV2ForSemanticSegmentation",parameters:[{name:"config",val:": MobileViTV2Config"}],parametersDescription:[{anchor:"transformers.MobileViTV2ForSemanticSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L939"}}),me=new de({props:{name:"forward",anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor.__call__">MobileViTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Ground truth semantic segmentation maps for computing the loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels &gt; 1</code>, a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mobilevitv2/modeling_mobilevitv2.py#L956",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config"
>MobileViTV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels, logits_height, logits_width)</code>) — Classification scores for each pixel.</p>
<Tip warning={true}>
<p>The logits returned do not necessarily have the same size as the <code>pixel_values</code> passed as inputs. This is
to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the
original image size as post-processing. You should always check your logits shape and resize as needed.</p>
</Tip>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, patch_size, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),H=new mt({props:{$$slots:{default:[Bt]},$$scope:{ctx:w}}}),Y=new Pe({props:{anchor:"transformers.MobileViTV2ForSemanticSegmentation.forward.example",$$slots:{default:[zt]},$$scope:{ctx:w}}}),{c(){t=d("meta"),h=r(),a=d("p"),l=r(),g(f.$$.fragment),o=r(),g(V.$$.fragment),$e=r(),L=d("p"),L.innerHTML=dt,je=r(),E=d("p"),E.textContent=pt,Ce=r(),A=d("p"),A.textContent=ft,xe=r(),q=d("p"),q.innerHTML=ht,Je=r(),O=d("p"),O.innerHTML=gt,We=r(),g(D.$$.fragment),Ue=r(),K=d("ul"),K.innerHTML=ut,Ze=r(),g(ee.$$.fragment),Ie=r(),y=d("div"),g(te.$$.fragment),Qe=r(),he=d("p"),he.innerHTML=bt,He=r(),ge=d("p"),ge.innerHTML=_t,Ye=r(),g(G.$$.fragment),Fe=r(),g(oe.$$.fragment),ke=r(),W=d("div"),g(ne.$$.fragment),Le=r(),ue=d("p"),ue.innerHTML=Mt,Ee=r(),C=d("div"),g(se.$$.fragment),Ae=r(),be=d("p"),be.innerHTML=Tt,qe=r(),g(R.$$.fragment),Oe=r(),g(P.$$.fragment),Se=r(),g(ae.$$.fragment),Be=r(),$=d("div"),g(re.$$.fragment),De=r(),_e=d("p"),_e.textContent=Vt,Ke=r(),Me=d("p"),Me.innerHTML=vt,et=r(),x=d("div"),g(ie.$$.fragment),tt=r(),Te=d("p"),Te.innerHTML=wt,ot=r(),g(X.$$.fragment),nt=r(),g(Q.$$.fragment),ze=r(),g(le.$$.fragment),Ne=r(),j=d("div"),g(ce.$$.fragment),st=r(),Ve=d("p"),Ve.textContent=yt,at=r(),ve=d("p"),ve.innerHTML=$t,rt=r(),J=d("div"),g(me.$$.fragment),it=r(),we=d("p"),we.innerHTML=jt,lt=r(),g(H.$$.fragment),ct=r(),g(Y.$$.fragment),Ge=r(),ye=d("p"),this.h()},l(e){const n=Ut("svelte-u9bgzb",document.head);t=p(n,"META",{name:!0,content:!0}),n.forEach(s),h=i(e),a=p(e,"P",{}),z(a).forEach(s),l=i(e),u(f.$$.fragment,e),o=i(e),u(V.$$.fragment,e),$e=i(e),L=p(e,"P",{"data-svelte-h":!0}),v(L)!=="svelte-1mta9ef"&&(L.innerHTML=dt),je=i(e),E=p(e,"P",{"data-svelte-h":!0}),v(E)!=="svelte-1eurxj7"&&(E.textContent=pt),Ce=i(e),A=p(e,"P",{"data-svelte-h":!0}),v(A)!=="svelte-vfdo9a"&&(A.textContent=ft),xe=i(e),q=p(e,"P",{"data-svelte-h":!0}),v(q)!=="svelte-1xe01bc"&&(q.innerHTML=ht),Je=i(e),O=p(e,"P",{"data-svelte-h":!0}),v(O)!=="svelte-rz3bb6"&&(O.innerHTML=gt),We=i(e),u(D.$$.fragment,e),Ue=i(e),K=p(e,"UL",{"data-svelte-h":!0}),v(K)!=="svelte-19amq6z"&&(K.innerHTML=ut),Ze=i(e),u(ee.$$.fragment,e),Ie=i(e),y=p(e,"DIV",{class:!0});var U=z(y);u(te.$$.fragment,U),Qe=i(U),he=p(U,"P",{"data-svelte-h":!0}),v(he)!=="svelte-19xjfh8"&&(he.innerHTML=bt),He=i(U),ge=p(U,"P",{"data-svelte-h":!0}),v(ge)!=="svelte-o55m63"&&(ge.innerHTML=_t),Ye=i(U),u(G.$$.fragment,U),U.forEach(s),Fe=i(e),u(oe.$$.fragment,e),ke=i(e),W=p(e,"DIV",{class:!0});var B=z(W);u(ne.$$.fragment,B),Le=i(B),ue=p(B,"P",{"data-svelte-h":!0}),v(ue)!=="svelte-1vozw9r"&&(ue.innerHTML=Mt),Ee=i(B),C=p(B,"DIV",{class:!0});var Z=z(C);u(se.$$.fragment,Z),Ae=i(Z),be=p(Z,"P",{"data-svelte-h":!0}),v(be)!=="svelte-18sgztq"&&(be.innerHTML=Tt),qe=i(Z),u(R.$$.fragment,Z),Oe=i(Z),u(P.$$.fragment,Z),Z.forEach(s),B.forEach(s),Se=i(e),u(ae.$$.fragment,e),Be=i(e),$=p(e,"DIV",{class:!0});var I=z($);u(re.$$.fragment,I),De=i(I),_e=p(I,"P",{"data-svelte-h":!0}),v(_e)!=="svelte-1ojd07h"&&(_e.textContent=Vt),Ke=i(I),Me=p(I,"P",{"data-svelte-h":!0}),v(Me)!=="svelte-1gjh92c"&&(Me.innerHTML=vt),et=i(I),x=p(I,"DIV",{class:!0});var F=z(x);u(ie.$$.fragment,F),tt=i(F),Te=p(F,"P",{"data-svelte-h":!0}),v(Te)!=="svelte-1jrk6a6"&&(Te.innerHTML=wt),ot=i(F),u(X.$$.fragment,F),nt=i(F),u(Q.$$.fragment,F),F.forEach(s),I.forEach(s),ze=i(e),u(le.$$.fragment,e),Ne=i(e),j=p(e,"DIV",{class:!0});var k=z(j);u(ce.$$.fragment,k),st=i(k),Ve=p(k,"P",{"data-svelte-h":!0}),v(Ve)!=="svelte-w9c1ym"&&(Ve.textContent=yt),at=i(k),ve=p(k,"P",{"data-svelte-h":!0}),v(ve)!=="svelte-1gjh92c"&&(ve.innerHTML=$t),rt=i(k),J=p(k,"DIV",{class:!0});var S=z(J);u(me.$$.fragment,S),it=i(S),we=p(S,"P",{"data-svelte-h":!0}),v(we)!=="svelte-11wqfjy"&&(we.innerHTML=jt),lt=i(S),u(H.$$.fragment,S),ct=i(S),u(Y.$$.fragment,S),S.forEach(s),k.forEach(s),Ge=i(e),ye=p(e,"P",{}),z(ye).forEach(s),this.h()},h(){N(t,"name","hf:doc:metadata"),N(t,"content",Gt),N(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){m(document.head,t),c(e,h,n),c(e,a,n),c(e,l,n),b(f,e,n),c(e,o,n),b(V,e,n),c(e,$e,n),c(e,L,n),c(e,je,n),c(e,E,n),c(e,Ce,n),c(e,A,n),c(e,xe,n),c(e,q,n),c(e,Je,n),c(e,O,n),c(e,We,n),b(D,e,n),c(e,Ue,n),c(e,K,n),c(e,Ze,n),b(ee,e,n),c(e,Ie,n),c(e,y,n),b(te,y,null),m(y,Qe),m(y,he),m(y,He),m(y,ge),m(y,Ye),b(G,y,null),c(e,Fe,n),b(oe,e,n),c(e,ke,n),c(e,W,n),b(ne,W,null),m(W,Le),m(W,ue),m(W,Ee),m(W,C),b(se,C,null),m(C,Ae),m(C,be),m(C,qe),b(R,C,null),m(C,Oe),b(P,C,null),c(e,Se,n),b(ae,e,n),c(e,Be,n),c(e,$,n),b(re,$,null),m($,De),m($,_e),m($,Ke),m($,Me),m($,et),m($,x),b(ie,x,null),m(x,tt),m(x,Te),m(x,ot),b(X,x,null),m(x,nt),b(Q,x,null),c(e,ze,n),b(le,e,n),c(e,Ne,n),c(e,j,n),b(ce,j,null),m(j,st),m(j,Ve),m(j,at),m(j,ve),m(j,rt),m(j,J),b(me,J,null),m(J,it),m(J,we),m(J,lt),b(H,J,null),m(J,ct),b(Y,J,null),c(e,Ge,n),c(e,ye,n),Re=!0},p(e,[n]){const U={};n&2&&(U.$$scope={dirty:n,ctx:e}),G.$set(U);const B={};n&2&&(B.$$scope={dirty:n,ctx:e}),R.$set(B);const Z={};n&2&&(Z.$$scope={dirty:n,ctx:e}),P.$set(Z);const I={};n&2&&(I.$$scope={dirty:n,ctx:e}),X.$set(I);const F={};n&2&&(F.$$scope={dirty:n,ctx:e}),Q.$set(F);const k={};n&2&&(k.$$scope={dirty:n,ctx:e}),H.$set(k);const S={};n&2&&(S.$$scope={dirty:n,ctx:e}),Y.$set(S)},i(e){Re||(_(f.$$.fragment,e),_(V.$$.fragment,e),_(D.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(G.$$.fragment,e),_(oe.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(R.$$.fragment,e),_(P.$$.fragment,e),_(ae.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(X.$$.fragment,e),_(Q.$$.fragment,e),_(le.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(H.$$.fragment,e),_(Y.$$.fragment,e),Re=!0)},o(e){M(f.$$.fragment,e),M(V.$$.fragment,e),M(D.$$.fragment,e),M(ee.$$.fragment,e),M(te.$$.fragment,e),M(G.$$.fragment,e),M(oe.$$.fragment,e),M(ne.$$.fragment,e),M(se.$$.fragment,e),M(R.$$.fragment,e),M(P.$$.fragment,e),M(ae.$$.fragment,e),M(re.$$.fragment,e),M(ie.$$.fragment,e),M(X.$$.fragment,e),M(Q.$$.fragment,e),M(le.$$.fragment,e),M(ce.$$.fragment,e),M(me.$$.fragment,e),M(H.$$.fragment,e),M(Y.$$.fragment,e),Re=!1},d(e){e&&(s(h),s(a),s(l),s(o),s($e),s(L),s(je),s(E),s(Ce),s(A),s(xe),s(q),s(Je),s(O),s(We),s(Ue),s(K),s(Ze),s(Ie),s(y),s(Fe),s(ke),s(W),s(Se),s(Be),s($),s(ze),s(Ne),s(j),s(Ge),s(ye)),s(t),T(f,e),T(V,e),T(D,e),T(ee,e),T(te),T(G),T(oe,e),T(ne),T(se),T(R),T(P),T(ae,e),T(re),T(ie),T(X),T(Q),T(le,e),T(ce),T(me),T(H),T(Y)}}}const Gt='{"title":"MobileViTV2","local":"mobilevitv2","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"MobileViTV2Config","local":"transformers.MobileViTV2Config","sections":[],"depth":2},{"title":"MobileViTV2Model","local":"transformers.MobileViTV2Model","sections":[],"depth":2},{"title":"MobileViTV2ForImageClassification","local":"transformers.MobileViTV2ForImageClassification","sections":[],"depth":2},{"title":"MobileViTV2ForSemanticSegmentation","local":"transformers.MobileViTV2ForSemanticSegmentation","sections":[],"depth":2}],"depth":1}';function Rt(w){return xt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class At extends Jt{constructor(t){super(),Wt(this,t,Rt,Nt,Ct,{})}}export{At as component};
