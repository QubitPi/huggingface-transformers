import{s as Ft,o as It,n as Ht}from"../chunks/scheduler.9bc65507.js";import{S as zt,i as Lt,g as a,s,r as p,A as Qt,h as i,f as n,c as o,j as Gt,u as m,x as r,k as Vt,y as Et,a as l,v as d,d as c,t as u,w as f}from"../chunks/index.707bf1b6.js";import{T as Ot}from"../chunks/Tip.c2ecdbf4.js";import{C as b}from"../chunks/CodeBlock.54a9f38d.js";import{H as ue}from"../chunks/Heading.342b1fa6.js";function Yt(he){let h,w="<code>tranformers.onnx</code> is no longer maintained, please export models with ðŸ¤— Optimum as described above. This section will be removed in the future versions.";return{c(){h=a("p"),h.innerHTML=w},l(g){h=i(g,"P",{"data-svelte-h":!0}),r(h)!=="svelte-1g5f3lj"&&(h.innerHTML=w)},m(g,ce){l(g,h,ce)},p:Ht,d(g){g&&n(h)}}}function qt(he){let h,w,g,ce,M,be,T,mt=`Deploying ðŸ¤— Transformers models in production environments often requires, or can benefit from exporting the models into
a serialized format that can be loaded and executed on specialized runtimes and hardware.`,ge,x,dt=`ðŸ¤— Optimum is an extension of Transformers that enables exporting models from PyTorch or TensorFlow to serialized formats
such as ONNX and TFLite through its <code>exporters</code> module. ðŸ¤— Optimum also provides a set of performance optimization tools to train
and run models on targeted hardware with maximum efficiency.`,ye,J,ct=`This guide demonstrates how you can export ðŸ¤— Transformers models to ONNX with ðŸ¤— Optimum, for the guide on exporting models to TFLite,
please refer to the <a href="tflite">Export to TFLite page</a>.`,we,$,Me,j,ut=`<a href="http://onnx.ai" rel="nofollow">ONNX (Open Neural Network eXchange)</a> is an open standard that defines a common set of operators and a
common file format to represent deep learning models in a wide variety of frameworks, including PyTorch and
TensorFlow. When a model is exported to the ONNX format, these operators are used to
construct a computational graph (often called an <em>intermediate representation</em>) which
represents the flow of data through the neural network.`,Te,v,ft=`By exposing a graph with standardized operators and data types, ONNX makes it easy to
switch between frameworks. For example, a model trained in PyTorch can be exported to
ONNX format and then imported in TensorFlow (and vice versa).`,xe,N,ht="Once exported to ONNX format, a model can be:",Je,X,bt=`<li>optimized for inference via techniques such as <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization" rel="nofollow">graph optimization</a> and <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization" rel="nofollow">quantization</a>.</li> <li>run with ONNX Runtime via <a href="https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort" rel="nofollow"><code>ORTModelForXXX</code> classes</a>,
which follow the same <code>AutoModel</code> API as the one you are used to in ðŸ¤— Transformers.</li> <li>run with <a href="https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines" rel="nofollow">optimized inference pipelines</a>,
which has the same API as the <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> function in ðŸ¤— Transformers.</li>`,$e,U,gt=`ðŸ¤— Optimum provides support for the ONNX export by leveraging configuration objects. These configuration objects come
ready-made for a number of model architectures, and are designed to be easily extendable to other architectures.`,je,k,yt='For the list of ready-made configurations, please refer to <a href="https://huggingface.co/docs/optimum/exporters/onnx/overview" rel="nofollow">ðŸ¤— Optimum documentation</a>.',ve,Z,wt="There are two ways to export a ðŸ¤— Transformers model to ONNX, here we show both:",Ne,C,Mt="<li>export with ðŸ¤— Optimum via CLI.</li> <li>export with ðŸ¤— Optimum with <code>optimum.onnxruntime</code>.</li>",Xe,W,Ue,_,Tt="To export a ðŸ¤— Transformers model to ONNX, first install an extra dependency:",ke,R,Ze,B,xt=`To check out all available arguments, refer to the <a href="https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli" rel="nofollow">ðŸ¤— Optimum docs</a>,
or view help in command line:`,Ce,G,We,V,Jt="To export a modelâ€™s checkpoint from the ðŸ¤— Hub, for example, <code>distilbert/distilbert-base-uncased-distilled-squad</code>, run the following command:",_e,F,Re,I,$t="You should see the logs indicating progress and showing where the resulting <code>model.onnx</code> is saved, like this:",Be,H,Ge,z,jt=`The example above illustrates exporting a checkpoint from ðŸ¤— Hub. When exporting a local model, first make sure that you
saved both the modelâ€™s weights and tokenizer files in the same directory (<code>local_path</code>). When using CLI, pass the
<code>local_path</code> to the <code>model</code> argument instead of the checkpoint name on ðŸ¤— Hub and provide the <code>--task</code> argument.
You can review the list of supported tasks in the <a href="https://huggingface.co/docs/optimum/exporters/task_manager" rel="nofollow">ðŸ¤— Optimum documentation</a>.
If <code>task</code> argument is not provided, it will default to the model architecture without any task specific head.`,Ve,L,Fe,Q,vt=`The resulting <code>model.onnx</code> file can then be run on one of the <a href="https://onnx.ai/supported-tools.html#deployModel" rel="nofollow">many
accelerators</a> that support the ONNX
standard. For example, we can load and run the model with <a href="https://onnxruntime.ai/" rel="nofollow">ONNX
Runtime</a> as follows:`,Ie,E,He,O,Nt=`The process is identical for TensorFlow checkpoints on the Hub. For instance, hereâ€™s how you would
export a pure TensorFlow checkpoint from the <a href="https://huggingface.co/keras-io" rel="nofollow">Keras organization</a>:`,ze,Y,Le,q,Qe,S,Xt="Alternative to CLI, you can export a ðŸ¤— Transformers model to ONNX programmatically like so:",Ee,P,Oe,A,Ye,D,Ut=`If you wish to contribute by adding support for a model that cannot be currently exported, you should first check if it is
supported in <a href="https://huggingface.co/docs/optimum/exporters/onnx/overview" rel="nofollow"><code>optimum.exporters.onnx</code></a>,
and if it is not, <a href="https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute" rel="nofollow">contribute to ðŸ¤— Optimum</a>
directly.`,qe,K,Se,y,Pe,ee,kt="To export a ðŸ¤— Transformers model to ONNX with <code>tranformers.onnx</code>, install extra dependencies:",Ae,te,De,ne,Zt="Use <code>transformers.onnx</code> package as a Python module to export a checkpoint using a ready-made configuration:",Ke,le,et,se,Ct=`This exports an ONNX graph of the checkpoint defined by the <code>--model</code> argument. Pass any checkpoint on the ðŸ¤— Hub or one thatâ€™s stored locally.
The resulting <code>model.onnx</code> file can then be run on one of the many accelerators that support the ONNX standard. For example,
load and run the model with ONNX Runtime as follows:`,tt,oe,nt,ae,Wt=`The required output names (like <code>[&quot;last_hidden_state&quot;]</code>) can be obtained by taking a look at the ONNX configuration of
each model. For example, for DistilBERT we have:`,lt,ie,st,re,_t="The process is identical for TensorFlow checkpoints on the Hub. For example, export a pure TensorFlow checkpoint like so:",ot,pe,at,me,Rt=`To export a model thatâ€™s stored locally, save the modelâ€™s weights and tokenizer files in the same directory (e.g. <code>local-pt-checkpoint</code>),
then export it to ONNX by pointing the <code>--model</code> argument of the <code>transformers.onnx</code> package to the desired directory:`,it,de,rt,fe,pt;return M=new ue({props:{title:"Export to ONNX",local:"export-to-onnx",headingTag:"h1"}}),$=new ue({props:{title:"Export to ONNX",local:"export-to-onnx",headingTag:"h2"}}),W=new ue({props:{title:"Exporting a ðŸ¤— Transformers model to ONNX with CLI",local:"exporting-a--transformers-model-to-onnx-with-cli",headingTag:"h3"}}),R=new b({props:{code:"cGlwJTIwaW5zdGFsbCUyMG9wdGltdW0lNUJleHBvcnRlcnMlNUQ=",highlighted:"pip install optimum[exporters]",wrap:!1}}),G=new b({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1oZWxw",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --<span class="hljs-built_in">help</span>',wrap:!1}}),F=new b({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1tb2RlbCUyMGRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZC1kaXN0aWxsZWQtc3F1YWQlMjBkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZF9vbm54JTJG",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/',wrap:!1}}),H=new b({props:{code:"VmFsaWRhdGluZyUyME9OTlglMjBtb2RlbCUyMGRpc3RpbGJlcnRfYmFzZV91bmNhc2VkX3NxdWFkX29ubnglMkZtb2RlbC5vbm54Li4uJTBBJTA5LSU1QiVFMiU5QyU5MyU1RCUyME9OTlglMjBtb2RlbCUyMG91dHB1dCUyMG5hbWVzJTIwbWF0Y2glMjByZWZlcmVuY2UlMjBtb2RlbCUyMChzdGFydF9sb2dpdHMlMkMlMjBlbmRfbG9naXRzKSUwQSUwOS0lMjBWYWxpZGF0aW5nJTIwT05OWCUyME1vZGVsJTIwb3V0cHV0JTIwJTIyc3RhcnRfbG9naXRzJTIyJTNBJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMCgyJTJDJTIwMTYpJTIwbWF0Y2hlcyUyMCgyJTJDJTIwMTYpJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMGFsbCUyMHZhbHVlcyUyMGNsb3NlJTIwKGF0b2wlM0ElMjAwLjAwMDEpJTBBJTA5LSUyMFZhbGlkYXRpbmclMjBPTk5YJTIwTW9kZWwlMjBvdXRwdXQlMjAlMjJlbmRfbG9naXRzJTIyJTNBJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMCgyJTJDJTIwMTYpJTIwbWF0Y2hlcyUyMCgyJTJDJTIwMTYpJTBBJTA5JTA5LSU1QiVFMiU5QyU5MyU1RCUyMGFsbCUyMHZhbHVlcyUyMGNsb3NlJTIwKGF0b2wlM0ElMjAwLjAwMDEpJTBBVGhlJTIwT05OWCUyMGV4cG9ydCUyMHN1Y2NlZWRlZCUyMGFuZCUyMHRoZSUyMGV4cG9ydGVkJTIwbW9kZWwlMjB3YXMlMjBzYXZlZCUyMGF0JTNBJTIwZGlzdGlsYmVydF9iYXNlX3VuY2FzZWRfc3F1YWRfb25ueA==",highlighted:`Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...
	-[âœ“] ONNX model output names match reference model (start_logits, end_logits)
	- Validating ONNX Model output <span class="hljs-string">&quot;start_logits&quot;</span>:
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
	- Validating ONNX Model output <span class="hljs-string">&quot;end_logits&quot;</span>:
		-[âœ“] (2, 16) matches (2, 16)
		-[âœ“] all values close (atol: 0.0001)
The ONNX <span class="hljs-built_in">export</span> succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx`,wrap:!1}}),L=new b({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1tb2RlbCUyMGxvY2FsX3BhdGglMjAtLXRhc2slMjBxdWVzdGlvbi1hbnN3ZXJpbmclMjBkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZF9vbm54JTJG",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/',wrap:!1}}),E=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEFmcm9tJTIwb3B0aW11bS5vbm54cnVudGltZSUyMGltcG9ydCUyME9SVE1vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmclMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZF9vbm54JTIyKSUwQW1vZGVsJTIwJTNEJTIwT1JUTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydF9iYXNlX3VuY2FzZWRfc3F1YWRfb25ueCUyMiklMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyV2hhdCUyMGFtJTIwSSUyMHVzaW5nJTNGJTIyJTJDJTIwJTIyVXNpbmclMjBEaXN0aWxCRVJUJTIwd2l0aCUyME9OTlglMjBSdW50aW1lISUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert_base_uncased_squad_onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ORTModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;distilbert_base_uncased_squad_onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;What am I using?&quot;</span>, <span class="hljs-string">&quot;Using DistilBERT with ONNX Runtime!&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)`,wrap:!1}}),Y=new b({props:{code:"b3B0aW11bS1jbGklMjBleHBvcnQlMjBvbm54JTIwLS1tb2RlbCUyMGtlcmFzLWlvJTJGdHJhbnNmb3JtZXJzLXFhJTIwZGlzdGlsYmVydF9iYXNlX2Nhc2VkX3NxdWFkX29ubnglMkY=",highlighted:'optimum-cli <span class="hljs-built_in">export</span> onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/',wrap:!1}}),q=new ue({props:{title:"Exporting a ðŸ¤— Transformers model to ONNX with optimum.onnxruntime",local:"exporting-a--transformers-model-to-onnx-with-optimumonnxruntime",headingTag:"h3"}}),P=new b({props:{code:"ZnJvbSUyMG9wdGltdW0ub25ueHJ1bnRpbWUlMjBpbXBvcnQlMjBPUlRNb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsX2NoZWNrcG9pbnQlMjAlM0QlMjAlMjJkaXN0aWxiZXJ0X2Jhc2VfdW5jYXNlZF9zcXVhZCUyMiUwQXNhdmVfZGlyZWN0b3J5JTIwJTNEJTIwJTIyb25ueCUyRiUyMiUwQSUwQSUyMyUyMExvYWQlMjBhJTIwbW9kZWwlMjBmcm9tJTIwdHJhbnNmb3JtZXJzJTIwYW5kJTIwZXhwb3J0JTIwaXQlMjB0byUyME9OTlglMEFvcnRfbW9kZWwlMjAlM0QlMjBPUlRNb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2NoZWNrcG9pbnQlMkMlMjBleHBvcnQlM0RUcnVlKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2NoZWNrcG9pbnQpJTBBJTBBJTIzJTIwU2F2ZSUyMHRoZSUyMG9ubnglMjBtb2RlbCUyMGFuZCUyMHRva2VuaXplciUwQW9ydF9tb2RlbC5zYXZlX3ByZXRyYWluZWQoc2F2ZV9kaXJlY3RvcnkpJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZChzYXZlX2RpcmVjdG9yeSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_checkpoint = <span class="hljs-string">&quot;distilbert_base_uncased_squad&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>save_directory = <span class="hljs-string">&quot;onnx/&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a model from transformers and export it to ONNX</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Save the onnx model and tokenizer</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>ort_model.save_pretrained(save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(save_directory)`,wrap:!1}}),A=new ue({props:{title:"Exporting a model for an unsupported architecture",local:"exporting-a-model-for-an-unsupported-architecture",headingTag:"h3"}}),K=new ue({props:{title:"Exporting a model with transformers.onnx",local:"exporting-a-model-with-transformersonnx",headingTag:"h3"}}),y=new Ot({props:{warning:!0,$$slots:{default:[Yt]},$$scope:{ctx:he}}}),te=new b({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyU1Qm9ubnglNUQ=",highlighted:"pip install transformers[onnx]",wrap:!1}}),le=new b({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0RkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjBvbm54JTJG",highlighted:"python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/",wrap:!1}}),oe=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEFmcm9tJTIwb25ueHJ1bnRpbWUlMjBpbXBvcnQlMjBJbmZlcmVuY2VTZXNzaW9uJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydCUyRmRpc3RpbGJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQXNlc3Npb24lMjAlM0QlMjBJbmZlcmVuY2VTZXNzaW9uKCUyMm9ubnglMkZtb2RlbC5vbm54JTIyKSUwQSUyMyUyME9OTlglMjBSdW50aW1lJTIwZXhwZWN0cyUyME51bVB5JTIwYXJyYXlzJTIwYXMlMjBpbnB1dCUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJVc2luZyUyMERpc3RpbEJFUlQlMjB3aXRoJTIwT05OWCUyMFJ1bnRpbWUhJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJucCUyMiklMEFvdXRwdXRzJTIwJTNEJTIwc2Vzc2lvbi5ydW4ob3V0cHV0X25hbWVzJTNEJTVCJTIybGFzdF9oaWRkZW5fc3RhdGUlMjIlNUQlMkMlMjBpbnB1dF9mZWVkJTNEZGljdChpbnB1dHMpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> onnxruntime <span class="hljs-keyword">import</span> InferenceSession

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>session = InferenceSession(<span class="hljs-string">&quot;onnx/model.onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># ONNX Runtime expects NumPy arrays as input</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Using DistilBERT with ONNX Runtime!&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = session.run(output_names=[<span class="hljs-string">&quot;last_hidden_state&quot;</span>], input_feed=<span class="hljs-built_in">dict</span>(inputs))`,wrap:!1}}),ie=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5tb2RlbHMuZGlzdGlsYmVydCUyMGltcG9ydCUyMERpc3RpbEJlcnRDb25maWclMkMlMjBEaXN0aWxCZXJ0T25ueENvbmZpZyUwQSUwQWNvbmZpZyUyMCUzRCUyMERpc3RpbEJlcnRDb25maWcoKSUwQW9ubnhfY29uZmlnJTIwJTNEJTIwRGlzdGlsQmVydE9ubnhDb25maWcoY29uZmlnKSUwQXByaW50KGxpc3Qob25ueF9jb25maWcub3V0cHV0cy5rZXlzKCkpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.models.distilbert <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertOnnxConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>config = DistilBertConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_config = DistilBertOnnxConfig(config)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(onnx_config.outputs.keys()))
[<span class="hljs-string">&quot;last_hidden_state&quot;</span>]`,wrap:!1}}),pe=new b({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0RrZXJhcy1pbyUyRnRyYW5zZm9ybWVycy1xYSUyMG9ubnglMkY=",highlighted:"python -m transformers.onnx --model=keras-io/transformers-qa onnx/",wrap:!1}}),de=new b({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0Rsb2NhbC1wdC1jaGVja3BvaW50JTIwb25ueCUyRg==",highlighted:"python -m transformers.onnx --model=local-pt-checkpoint onnx/",wrap:!1}}),{c(){h=a("meta"),w=s(),g=a("p"),ce=s(),p(M.$$.fragment),be=s(),T=a("p"),T.textContent=mt,ge=s(),x=a("p"),x.innerHTML=dt,ye=s(),J=a("p"),J.innerHTML=ct,we=s(),p($.$$.fragment),Me=s(),j=a("p"),j.innerHTML=ut,Te=s(),v=a("p"),v.textContent=ft,xe=s(),N=a("p"),N.textContent=ht,Je=s(),X=a("ul"),X.innerHTML=bt,$e=s(),U=a("p"),U.textContent=gt,je=s(),k=a("p"),k.innerHTML=yt,ve=s(),Z=a("p"),Z.textContent=wt,Ne=s(),C=a("ul"),C.innerHTML=Mt,Xe=s(),p(W.$$.fragment),Ue=s(),_=a("p"),_.textContent=Tt,ke=s(),p(R.$$.fragment),Ze=s(),B=a("p"),B.innerHTML=xt,Ce=s(),p(G.$$.fragment),We=s(),V=a("p"),V.innerHTML=Jt,_e=s(),p(F.$$.fragment),Re=s(),I=a("p"),I.innerHTML=$t,Be=s(),p(H.$$.fragment),Ge=s(),z=a("p"),z.innerHTML=jt,Ve=s(),p(L.$$.fragment),Fe=s(),Q=a("p"),Q.innerHTML=vt,Ie=s(),p(E.$$.fragment),He=s(),O=a("p"),O.innerHTML=Nt,ze=s(),p(Y.$$.fragment),Le=s(),p(q.$$.fragment),Qe=s(),S=a("p"),S.textContent=Xt,Ee=s(),p(P.$$.fragment),Oe=s(),p(A.$$.fragment),Ye=s(),D=a("p"),D.innerHTML=Ut,qe=s(),p(K.$$.fragment),Se=s(),p(y.$$.fragment),Pe=s(),ee=a("p"),ee.innerHTML=kt,Ae=s(),p(te.$$.fragment),De=s(),ne=a("p"),ne.innerHTML=Zt,Ke=s(),p(le.$$.fragment),et=s(),se=a("p"),se.innerHTML=Ct,tt=s(),p(oe.$$.fragment),nt=s(),ae=a("p"),ae.innerHTML=Wt,lt=s(),p(ie.$$.fragment),st=s(),re=a("p"),re.textContent=_t,ot=s(),p(pe.$$.fragment),at=s(),me=a("p"),me.innerHTML=Rt,it=s(),p(de.$$.fragment),rt=s(),fe=a("p"),this.h()},l(e){const t=Qt("svelte-u9bgzb",document.head);h=i(t,"META",{name:!0,content:!0}),t.forEach(n),w=o(e),g=i(e,"P",{}),Gt(g).forEach(n),ce=o(e),m(M.$$.fragment,e),be=o(e),T=i(e,"P",{"data-svelte-h":!0}),r(T)!=="svelte-lfqtzw"&&(T.textContent=mt),ge=o(e),x=i(e,"P",{"data-svelte-h":!0}),r(x)!=="svelte-12k3xi1"&&(x.innerHTML=dt),ye=o(e),J=i(e,"P",{"data-svelte-h":!0}),r(J)!=="svelte-7b8cpg"&&(J.innerHTML=ct),we=o(e),m($.$$.fragment,e),Me=o(e),j=i(e,"P",{"data-svelte-h":!0}),r(j)!=="svelte-8enoc5"&&(j.innerHTML=ut),Te=o(e),v=i(e,"P",{"data-svelte-h":!0}),r(v)!=="svelte-wn4m59"&&(v.textContent=ft),xe=o(e),N=i(e,"P",{"data-svelte-h":!0}),r(N)!=="svelte-148fbt4"&&(N.textContent=ht),Je=o(e),X=i(e,"UL",{"data-svelte-h":!0}),r(X)!=="svelte-1y26evw"&&(X.innerHTML=bt),$e=o(e),U=i(e,"P",{"data-svelte-h":!0}),r(U)!=="svelte-tylo0u"&&(U.textContent=gt),je=o(e),k=i(e,"P",{"data-svelte-h":!0}),r(k)!=="svelte-r6gv1"&&(k.innerHTML=yt),ve=o(e),Z=i(e,"P",{"data-svelte-h":!0}),r(Z)!=="svelte-ksmizg"&&(Z.textContent=wt),Ne=o(e),C=i(e,"UL",{"data-svelte-h":!0}),r(C)!=="svelte-18tahix"&&(C.innerHTML=Mt),Xe=o(e),m(W.$$.fragment,e),Ue=o(e),_=i(e,"P",{"data-svelte-h":!0}),r(_)!=="svelte-1sf999h"&&(_.textContent=Tt),ke=o(e),m(R.$$.fragment,e),Ze=o(e),B=i(e,"P",{"data-svelte-h":!0}),r(B)!=="svelte-lc1loi"&&(B.innerHTML=xt),Ce=o(e),m(G.$$.fragment,e),We=o(e),V=i(e,"P",{"data-svelte-h":!0}),r(V)!=="svelte-ef7opj"&&(V.innerHTML=Jt),_e=o(e),m(F.$$.fragment,e),Re=o(e),I=i(e,"P",{"data-svelte-h":!0}),r(I)!=="svelte-1fffsri"&&(I.innerHTML=$t),Be=o(e),m(H.$$.fragment,e),Ge=o(e),z=i(e,"P",{"data-svelte-h":!0}),r(z)!=="svelte-1n3pmj5"&&(z.innerHTML=jt),Ve=o(e),m(L.$$.fragment,e),Fe=o(e),Q=i(e,"P",{"data-svelte-h":!0}),r(Q)!=="svelte-1dzbxfz"&&(Q.innerHTML=vt),Ie=o(e),m(E.$$.fragment,e),He=o(e),O=i(e,"P",{"data-svelte-h":!0}),r(O)!=="svelte-zdo0u7"&&(O.innerHTML=Nt),ze=o(e),m(Y.$$.fragment,e),Le=o(e),m(q.$$.fragment,e),Qe=o(e),S=i(e,"P",{"data-svelte-h":!0}),r(S)!=="svelte-1doea45"&&(S.textContent=Xt),Ee=o(e),m(P.$$.fragment,e),Oe=o(e),m(A.$$.fragment,e),Ye=o(e),D=i(e,"P",{"data-svelte-h":!0}),r(D)!=="svelte-6bg99r"&&(D.innerHTML=Ut),qe=o(e),m(K.$$.fragment,e),Se=o(e),m(y.$$.fragment,e),Pe=o(e),ee=i(e,"P",{"data-svelte-h":!0}),r(ee)!=="svelte-pabw4f"&&(ee.innerHTML=kt),Ae=o(e),m(te.$$.fragment,e),De=o(e),ne=i(e,"P",{"data-svelte-h":!0}),r(ne)!=="svelte-1hq77cg"&&(ne.innerHTML=Zt),Ke=o(e),m(le.$$.fragment,e),et=o(e),se=i(e,"P",{"data-svelte-h":!0}),r(se)!=="svelte-tj8y7s"&&(se.innerHTML=Ct),tt=o(e),m(oe.$$.fragment,e),nt=o(e),ae=i(e,"P",{"data-svelte-h":!0}),r(ae)!=="svelte-8brdq3"&&(ae.innerHTML=Wt),lt=o(e),m(ie.$$.fragment,e),st=o(e),re=i(e,"P",{"data-svelte-h":!0}),r(re)!=="svelte-9ykkz3"&&(re.textContent=_t),ot=o(e),m(pe.$$.fragment,e),at=o(e),me=i(e,"P",{"data-svelte-h":!0}),r(me)!=="svelte-1trh1mg"&&(me.innerHTML=Rt),it=o(e),m(de.$$.fragment,e),rt=o(e),fe=i(e,"P",{}),Gt(fe).forEach(n),this.h()},h(){Vt(h,"name","hf:doc:metadata"),Vt(h,"content",St)},m(e,t){Et(document.head,h),l(e,w,t),l(e,g,t),l(e,ce,t),d(M,e,t),l(e,be,t),l(e,T,t),l(e,ge,t),l(e,x,t),l(e,ye,t),l(e,J,t),l(e,we,t),d($,e,t),l(e,Me,t),l(e,j,t),l(e,Te,t),l(e,v,t),l(e,xe,t),l(e,N,t),l(e,Je,t),l(e,X,t),l(e,$e,t),l(e,U,t),l(e,je,t),l(e,k,t),l(e,ve,t),l(e,Z,t),l(e,Ne,t),l(e,C,t),l(e,Xe,t),d(W,e,t),l(e,Ue,t),l(e,_,t),l(e,ke,t),d(R,e,t),l(e,Ze,t),l(e,B,t),l(e,Ce,t),d(G,e,t),l(e,We,t),l(e,V,t),l(e,_e,t),d(F,e,t),l(e,Re,t),l(e,I,t),l(e,Be,t),d(H,e,t),l(e,Ge,t),l(e,z,t),l(e,Ve,t),d(L,e,t),l(e,Fe,t),l(e,Q,t),l(e,Ie,t),d(E,e,t),l(e,He,t),l(e,O,t),l(e,ze,t),d(Y,e,t),l(e,Le,t),d(q,e,t),l(e,Qe,t),l(e,S,t),l(e,Ee,t),d(P,e,t),l(e,Oe,t),d(A,e,t),l(e,Ye,t),l(e,D,t),l(e,qe,t),d(K,e,t),l(e,Se,t),d(y,e,t),l(e,Pe,t),l(e,ee,t),l(e,Ae,t),d(te,e,t),l(e,De,t),l(e,ne,t),l(e,Ke,t),d(le,e,t),l(e,et,t),l(e,se,t),l(e,tt,t),d(oe,e,t),l(e,nt,t),l(e,ae,t),l(e,lt,t),d(ie,e,t),l(e,st,t),l(e,re,t),l(e,ot,t),d(pe,e,t),l(e,at,t),l(e,me,t),l(e,it,t),d(de,e,t),l(e,rt,t),l(e,fe,t),pt=!0},p(e,[t]){const Bt={};t&2&&(Bt.$$scope={dirty:t,ctx:e}),y.$set(Bt)},i(e){pt||(c(M.$$.fragment,e),c($.$$.fragment,e),c(W.$$.fragment,e),c(R.$$.fragment,e),c(G.$$.fragment,e),c(F.$$.fragment,e),c(H.$$.fragment,e),c(L.$$.fragment,e),c(E.$$.fragment,e),c(Y.$$.fragment,e),c(q.$$.fragment,e),c(P.$$.fragment,e),c(A.$$.fragment,e),c(K.$$.fragment,e),c(y.$$.fragment,e),c(te.$$.fragment,e),c(le.$$.fragment,e),c(oe.$$.fragment,e),c(ie.$$.fragment,e),c(pe.$$.fragment,e),c(de.$$.fragment,e),pt=!0)},o(e){u(M.$$.fragment,e),u($.$$.fragment,e),u(W.$$.fragment,e),u(R.$$.fragment,e),u(G.$$.fragment,e),u(F.$$.fragment,e),u(H.$$.fragment,e),u(L.$$.fragment,e),u(E.$$.fragment,e),u(Y.$$.fragment,e),u(q.$$.fragment,e),u(P.$$.fragment,e),u(A.$$.fragment,e),u(K.$$.fragment,e),u(y.$$.fragment,e),u(te.$$.fragment,e),u(le.$$.fragment,e),u(oe.$$.fragment,e),u(ie.$$.fragment,e),u(pe.$$.fragment,e),u(de.$$.fragment,e),pt=!1},d(e){e&&(n(w),n(g),n(ce),n(be),n(T),n(ge),n(x),n(ye),n(J),n(we),n(Me),n(j),n(Te),n(v),n(xe),n(N),n(Je),n(X),n($e),n(U),n(je),n(k),n(ve),n(Z),n(Ne),n(C),n(Xe),n(Ue),n(_),n(ke),n(Ze),n(B),n(Ce),n(We),n(V),n(_e),n(Re),n(I),n(Be),n(Ge),n(z),n(Ve),n(Fe),n(Q),n(Ie),n(He),n(O),n(ze),n(Le),n(Qe),n(S),n(Ee),n(Oe),n(Ye),n(D),n(qe),n(Se),n(Pe),n(ee),n(Ae),n(De),n(ne),n(Ke),n(et),n(se),n(tt),n(nt),n(ae),n(lt),n(st),n(re),n(ot),n(at),n(me),n(it),n(rt),n(fe)),n(h),f(M,e),f($,e),f(W,e),f(R,e),f(G,e),f(F,e),f(H,e),f(L,e),f(E,e),f(Y,e),f(q,e),f(P,e),f(A,e),f(K,e),f(y,e),f(te,e),f(le,e),f(oe,e),f(ie,e),f(pe,e),f(de,e)}}}const St='{"title":"Export to ONNX","local":"export-to-onnx","sections":[{"title":"Export to ONNX","local":"export-to-onnx","sections":[{"title":"Exporting a ðŸ¤— Transformers model to ONNX with CLI","local":"exporting-a--transformers-model-to-onnx-with-cli","sections":[],"depth":3},{"title":"Exporting a ðŸ¤— Transformers model to ONNX with optimum.onnxruntime","local":"exporting-a--transformers-model-to-onnx-with-optimumonnxruntime","sections":[],"depth":3},{"title":"Exporting a model for an unsupported architecture","local":"exporting-a-model-for-an-unsupported-architecture","sections":[],"depth":3},{"title":"Exporting a model with transformers.onnx","local":"exporting-a-model-with-transformersonnx","sections":[],"depth":3}],"depth":2}],"depth":1}';function Pt(he){return It(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class nn extends zt{constructor(h){super(),Lt(this,h,Pt,qt,Ft,{})}}export{nn as component};
