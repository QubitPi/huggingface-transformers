import{s as jn,o as Hn,n as we}from"../chunks/scheduler.9bc65507.js";import{S as In,i as Zn,g as c,s as r,r as v,A as Nn,h as p,f as s,c as a,j as B,u as $,x as f,k as N,y as i,a as l,v as y,d as R,t as w,w as k}from"../chunks/index.707bf1b6.js";import{T as pt}from"../chunks/Tip.c2ecdbf4.js";import{D as Y}from"../chunks/Docstring.17db21ae.js";import{C as _t}from"../chunks/CodeBlock.54a9f38d.js";import{F as Qn,M as Un}from"../chunks/Markdown.fef84341.js";import{E as gt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as Re}from"../chunks/Heading.342b1fa6.js";function Vn(D){let t,g="Example:",n,d,_;return d=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERQUkNvbmZpZyUyQyUyMERQUkNvbnRleHRFbmNvZGVyJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMERQUiUyMGZhY2Vib29rJTJGZHByLWN0eF9lbmNvZGVyLXNpbmdsZS1ucS1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMERQUkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBmYWNlYm9vayUyRmRwci1jdHhfZW5jb2Rlci1zaW5nbGUtbnEtYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwRFBSQ29udGV4dEVuY29kZXIoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRConfig, DPRContextEncoder

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DPR facebook/dpr-ctx_encoder-single-nq-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DPRConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the facebook/dpr-ctx_encoder-single-nq-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=c("p"),t.textContent=g,n=r(),v(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-11lpom8"&&(t.textContent=g),n=a(o),$(d.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),y(d,o,T),_=!0},p:we,i(o){_||(R(d.$$.fragment,o),_=!0)},o(o){w(d.$$.fragment,o),_=!1},d(o){o&&(s(t),s(n)),k(d,o)}}}function Wn(D){let t,g="with the format:",n,d,_;return d=new _t({props:{code:"JTVCQ0xTJTVEJTIwJTNDcXVlc3Rpb24lMjB0b2tlbiUyMGlkcyUzRSUyMCU1QlNFUCU1RCUyMCUzQ3RpdGxlcyUyMGlkcyUzRSUyMCU1QlNFUCU1RCUyMCUzQ3RleHRzJTIwaWRzJTNF",highlighted:'[CLS] <span class="hljs-tag">&lt;<span class="hljs-name">question</span> <span class="hljs-attr">token</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">titles</span> <span class="hljs-attr">ids</span>&gt;</span> [SEP] <span class="hljs-tag">&lt;<span class="hljs-name">texts</span> <span class="hljs-attr">ids</span>&gt;</span>',wrap:!1}}),{c(){t=c("p"),t.textContent=g,n=r(),v(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-1kqkfm0"&&(t.textContent=g),n=a(o),$(d.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),y(d,o,T),_=!0},p:we,i(o){_||(R(d.$$.fragment,o),_=!0)},o(o){w(d.$$.fragment,o),_=!1},d(o){o&&(s(t),s(n)),k(d,o)}}}function Bn(D){let t,g=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=g},l(n){t=p(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=g)},m(n,d){l(n,t,d)},p:we,d(n){n&&s(t)}}}function Gn(D){let t,g="Examples:",n,d,_;return d=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERQUkNvbnRleHRFbmNvZGVyJTJDJTIwRFBSQ29udGV4dEVuY29kZXJUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBEUFJDb250ZXh0RW5jb2RlclRva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItY3R4X2VuY29kZXItc2luZ2xlLW5xLWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBEUFJDb250ZXh0RW5jb2Rlci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItY3R4X2VuY29kZXItc2luZ2xlLW5xLWJhc2UlMjIpJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkhlbGxvJTJDJTIwaXMlMjBteSUyMGRvZyUyMGN1dGUlMjAlM0YlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSU1QiUyMmlucHV0X2lkcyUyMiU1RCUwQWVtYmVkZGluZ3MlMjAlM0QlMjBtb2RlbChpbnB1dF9pZHMpLnBvb2xlcl9vdXRwdXQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`,wrap:!1}}),{c(){t=c("p"),t.textContent=g,n=r(),v(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-kvfsh7"&&(t.textContent=g),n=a(o),$(d.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),y(d,o,T),_=!0},p:we,i(o){_||(R(d.$$.fragment,o),_=!0)},o(o){w(d.$$.fragment,o),_=!1},d(o){o&&(s(t),s(n)),k(d,o)}}}function Sn(D){let t,g=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=g},l(n){t=p(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=g)},m(n,d){l(n,t,d)},p:we,d(n){n&&s(t)}}}function Xn(D){let t,g="Examples:",n,d,_;return d=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERQUlF1ZXN0aW9uRW5jb2RlciUyQyUyMERQUlF1ZXN0aW9uRW5jb2RlclRva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMERQUlF1ZXN0aW9uRW5jb2RlclRva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItcXVlc3Rpb25fZW5jb2Rlci1zaW5nbGUtbnEtYmFzZSUyMiklMEFtb2RlbCUyMCUzRCUyMERQUlF1ZXN0aW9uRW5jb2Rlci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItcXVlc3Rpb25fZW5jb2Rlci1zaW5nbGUtbnEtYmFzZSUyMiklMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySGVsbG8lMkMlMjBpcyUyMG15JTIwZG9nJTIwY3V0ZSUyMCUzRiUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTVCJTIyaW5wdXRfaWRzJTIyJTVEJTBBZW1iZWRkaW5ncyUyMCUzRCUyMG1vZGVsKGlucHV0X2lkcykucG9vbGVyX291dHB1dA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`,wrap:!1}}),{c(){t=c("p"),t.textContent=g,n=r(),v(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-kvfsh7"&&(t.textContent=g),n=a(o),$(d.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),y(d,o,T),_=!0},p:we,i(o){_||(R(d.$$.fragment,o),_=!0)},o(o){w(d.$$.fragment,o),_=!1},d(o){o&&(s(t),s(n)),k(d,o)}}}function On(D){let t,g=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=g},l(n){t=p(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=g)},m(n,d){l(n,t,d)},p:we,d(n){n&&s(t)}}}function An(D){let t,g="Examples:",n,d,_;return d=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERQUlJlYWRlciUyQyUyMERQUlJlYWRlclRva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMERQUlJlYWRlclRva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItcmVhZGVyLXNpbmdsZS1ucS1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwRFBSUmVhZGVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmRwci1yZWFkZXItc2luZ2xlLW5xLWJhc2UlMjIpJTBBZW5jb2RlZF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwcXVlc3Rpb25zJTNEJTVCJTIyV2hhdCUyMGlzJTIwbG92ZSUyMCUzRiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHRpdGxlcyUzRCU1QiUyMkhhZGRhd2F5JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwdGV4dHMlM0QlNUIlMjInV2hhdCUyMElzJTIwTG92ZSclMjBpcyUyMGElMjBzb25nJTIwcmVjb3JkZWQlMjBieSUyMHRoZSUyMGFydGlzdCUyMEhhZGRhd2F5JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUwQSklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKiplbmNvZGVkX2lucHV0cyklMEFzdGFydF9sb2dpdHMlMjAlM0QlMjBvdXRwdXRzLnN0YXJ0X2xvZ2l0cyUwQWVuZF9sb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmVuZF9sb2dpdHMlMEFyZWxldmFuY2VfbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5yZWxldmFuY2VfbG9naXRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`,wrap:!1}}),{c(){t=c("p"),t.textContent=g,n=r(),v(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-kvfsh7"&&(t.textContent=g),n=a(o),$(d.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),y(d,o,T),_=!0},p:we,i(o){_||(R(d.$$.fragment,o),_=!0)},o(o){w(d.$$.fragment,o),_=!1},d(o){o&&(s(t),s(n)),k(d,o)}}}function Yn(D){let t,g,n,d,_,o,T="The bare DPRContextEncoder transformer outputting pooler outputs as context representations.",ie,q,C=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,K,z,E=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ee,m,P,te,S,at='The <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoder">DPRContextEncoder</a> forward method, overrides the <code>__call__</code> special method.',ze,de,ot,X,ke,ne,be,U,ve,L,le,O="The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations.",pe,We,Be=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ge,Se,Xe=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Oe,ce,me,Fe,je,Q='The <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> forward method, overrides the <code>__call__</code> special method.',xe,se,qe,j,Pe,re,Ee,J,Me,ue,He,V="The bare DPRReader transformer outputting span predictions.",fe,Ae,Ie=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ze,Le,G=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ne,ae,oe,it,Qe,he='The <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader">DPRReader</a> forward method, overrides the <code>__call__</code> special method.',st,H,ge,Je,De;return t=new Re({props:{title:"DPRContextEncoder",local:"transformers.DPRContextEncoder",headingTag:"h2"}}),d=new Y({props:{name:"class transformers.DPRContextEncoder",anchor:"transformers.DPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L433"}}),P=new Y({props:{name:"forward",anchor:"transformers.DPRContextEncoder.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.DPRContextEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L445",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) — The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),de=new pt({props:{$$slots:{default:[Bn]},$$scope:{ctx:D}}}),X=new gt({props:{anchor:"transformers.DPRContextEncoder.forward.example",$$slots:{default:[Gn]},$$scope:{ctx:D}}}),ne=new Re({props:{title:"DPRQuestionEncoder",local:"transformers.DPRQuestionEncoder",headingTag:"h2"}}),ve=new Y({props:{name:"class transformers.DPRQuestionEncoder",anchor:"transformers.DPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L514"}}),me=new Y({props:{name:"forward",anchor:"transformers.DPRQuestionEncoder.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.DPRQuestionEncoder.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L526",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) — The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput"
>transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),se=new pt({props:{$$slots:{default:[Sn]},$$scope:{ctx:D}}}),j=new gt({props:{anchor:"transformers.DPRQuestionEncoder.forward.example",$$slots:{default:[Xn]},$$scope:{ctx:D}}}),re=new Re({props:{title:"DPRReader",local:"transformers.DPRReader",headingTag:"h2"}}),Me=new Y({props:{name:"class transformers.DPRReader",anchor:"transformers.DPRReader",parameters:[{name:"config",val:": DPRConfig"}],parametersDescription:[{anchor:"transformers.DPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L596"}}),oe=new Y({props:{name:"forward",anchor:"transformers.DPRReader.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.DPRReader.forward.input_ids",description:`<strong>input_ids</strong> (<code>Tuple[torch.LongTensor]</code> of shapes <code>(n_passages, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. It has to be a sequence triplet with 1) the question
and 2) the passages titles and 3) the passages texts To match pretraining, DPR <code>input_ids</code> sequence should
be formatted with [CLS] and [SEP] with the format:</p>
<p><code>[CLS] &lt;question token ids&gt; [SEP] &lt;titles ids&gt; [SEP] &lt;texts ids&gt;</code></p>
<p>DPR is a model with absolute position embeddings so it&#x2019;s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderTokenizer">DPRReaderTokenizer</a>. See this class documentation for more details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.DPRReader.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.DPRReader.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.DPRReader.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DPRReader.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DPRReader.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L608",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) — Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) — Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) — Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderOutput"
>transformers.models.dpr.modeling_dpr.DPRReaderOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),H=new pt({props:{$$slots:{default:[On]},$$scope:{ctx:D}}}),Je=new gt({props:{anchor:"transformers.DPRReader.forward.example",$$slots:{default:[An]},$$scope:{ctx:D}}}),{c(){v(t.$$.fragment),g=r(),n=c("div"),v(d.$$.fragment),_=r(),o=c("p"),o.textContent=T,ie=r(),q=c("p"),q.innerHTML=C,K=r(),z=c("p"),z.innerHTML=E,ee=r(),m=c("div"),v(P.$$.fragment),te=r(),S=c("p"),S.innerHTML=at,ze=r(),v(de.$$.fragment),ot=r(),v(X.$$.fragment),ke=r(),v(ne.$$.fragment),be=r(),U=c("div"),v(ve.$$.fragment),L=r(),le=c("p"),le.textContent=O,pe=r(),We=c("p"),We.innerHTML=Be,Ge=r(),Se=c("p"),Se.innerHTML=Xe,Oe=r(),ce=c("div"),v(me.$$.fragment),Fe=r(),je=c("p"),je.innerHTML=Q,xe=r(),v(se.$$.fragment),qe=r(),v(j.$$.fragment),Pe=r(),v(re.$$.fragment),Ee=r(),J=c("div"),v(Me.$$.fragment),ue=r(),He=c("p"),He.textContent=V,fe=r(),Ae=c("p"),Ae.innerHTML=Ie,Ze=r(),Le=c("p"),Le.innerHTML=G,Ne=r(),ae=c("div"),v(oe.$$.fragment),it=r(),Qe=c("p"),Qe.innerHTML=he,st=r(),v(H.$$.fragment),ge=r(),v(Je.$$.fragment),this.h()},l(h){$(t.$$.fragment,h),g=a(h),n=p(h,"DIV",{class:!0});var M=B(n);$(d.$$.fragment,M),_=a(M),o=p(M,"P",{"data-svelte-h":!0}),f(o)!=="svelte-18o6tt0"&&(o.textContent=T),ie=a(M),q=p(M,"P",{"data-svelte-h":!0}),f(q)!=="svelte-6pahdo"&&(q.innerHTML=C),K=a(M),z=p(M,"P",{"data-svelte-h":!0}),f(z)!=="svelte-hswkmf"&&(z.innerHTML=E),ee=a(M),m=p(M,"DIV",{class:!0});var F=B(m);$(P.$$.fragment,F),te=a(F),S=p(F,"P",{"data-svelte-h":!0}),f(S)!=="svelte-19g9wh9"&&(S.innerHTML=at),ze=a(F),$(de.$$.fragment,F),ot=a(F),$(X.$$.fragment,F),F.forEach(s),M.forEach(s),ke=a(h),$(ne.$$.fragment,h),be=a(h),U=p(h,"DIV",{class:!0});var I=B(U);$(ve.$$.fragment,I),L=a(I),le=p(I,"P",{"data-svelte-h":!0}),f(le)!=="svelte-1trrfka"&&(le.textContent=O),pe=a(I),We=p(I,"P",{"data-svelte-h":!0}),f(We)!=="svelte-6pahdo"&&(We.innerHTML=Be),Ge=a(I),Se=p(I,"P",{"data-svelte-h":!0}),f(Se)!=="svelte-hswkmf"&&(Se.innerHTML=Xe),Oe=a(I),ce=p(I,"DIV",{class:!0});var W=B(ce);$(me.$$.fragment,W),Fe=a(W),je=p(W,"P",{"data-svelte-h":!0}),f(je)!=="svelte-1a8np0x"&&(je.innerHTML=Q),xe=a(W),$(se.$$.fragment,W),qe=a(W),$(j.$$.fragment,W),W.forEach(s),I.forEach(s),Pe=a(h),$(re.$$.fragment,h),Ee=a(h),J=p(h,"DIV",{class:!0});var Z=B(J);$(Me.$$.fragment,Z),ue=a(Z),He=p(Z,"P",{"data-svelte-h":!0}),f(He)!=="svelte-sj9qc8"&&(He.textContent=V),fe=a(Z),Ae=p(Z,"P",{"data-svelte-h":!0}),f(Ae)!=="svelte-6pahdo"&&(Ae.innerHTML=Ie),Ze=a(Z),Le=p(Z,"P",{"data-svelte-h":!0}),f(Le)!=="svelte-hswkmf"&&(Le.innerHTML=G),Ne=a(Z),ae=p(Z,"DIV",{class:!0});var u=B(ae);$(oe.$$.fragment,u),it=a(u),Qe=p(u,"P",{"data-svelte-h":!0}),f(Qe)!=="svelte-1utxt0x"&&(Qe.innerHTML=he),st=a(u),$(H.$$.fragment,u),ge=a(u),$(Je.$$.fragment,u),u.forEach(s),Z.forEach(s),this.h()},h(){N(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(n,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(h,M){y(t,h,M),l(h,g,M),l(h,n,M),y(d,n,null),i(n,_),i(n,o),i(n,ie),i(n,q),i(n,K),i(n,z),i(n,ee),i(n,m),y(P,m,null),i(m,te),i(m,S),i(m,ze),y(de,m,null),i(m,ot),y(X,m,null),l(h,ke,M),y(ne,h,M),l(h,be,M),l(h,U,M),y(ve,U,null),i(U,L),i(U,le),i(U,pe),i(U,We),i(U,Ge),i(U,Se),i(U,Oe),i(U,ce),y(me,ce,null),i(ce,Fe),i(ce,je),i(ce,xe),y(se,ce,null),i(ce,qe),y(j,ce,null),l(h,Pe,M),y(re,h,M),l(h,Ee,M),l(h,J,M),y(Me,J,null),i(J,ue),i(J,He),i(J,fe),i(J,Ae),i(J,Ze),i(J,Le),i(J,Ne),i(J,ae),y(oe,ae,null),i(ae,it),i(ae,Qe),i(ae,st),y(H,ae,null),i(ae,ge),y(Je,ae,null),De=!0},p(h,M){const F={};M&2&&(F.$$scope={dirty:M,ctx:h}),de.$set(F);const I={};M&2&&(I.$$scope={dirty:M,ctx:h}),X.$set(I);const W={};M&2&&(W.$$scope={dirty:M,ctx:h}),se.$set(W);const Z={};M&2&&(Z.$$scope={dirty:M,ctx:h}),j.$set(Z);const u={};M&2&&(u.$$scope={dirty:M,ctx:h}),H.$set(u);const x={};M&2&&(x.$$scope={dirty:M,ctx:h}),Je.$set(x)},i(h){De||(R(t.$$.fragment,h),R(d.$$.fragment,h),R(P.$$.fragment,h),R(de.$$.fragment,h),R(X.$$.fragment,h),R(ne.$$.fragment,h),R(ve.$$.fragment,h),R(me.$$.fragment,h),R(se.$$.fragment,h),R(j.$$.fragment,h),R(re.$$.fragment,h),R(Me.$$.fragment,h),R(oe.$$.fragment,h),R(H.$$.fragment,h),R(Je.$$.fragment,h),De=!0)},o(h){w(t.$$.fragment,h),w(d.$$.fragment,h),w(P.$$.fragment,h),w(de.$$.fragment,h),w(X.$$.fragment,h),w(ne.$$.fragment,h),w(ve.$$.fragment,h),w(me.$$.fragment,h),w(se.$$.fragment,h),w(j.$$.fragment,h),w(re.$$.fragment,h),w(Me.$$.fragment,h),w(oe.$$.fragment,h),w(H.$$.fragment,h),w(Je.$$.fragment,h),De=!1},d(h){h&&(s(g),s(n),s(ke),s(be),s(U),s(Pe),s(Ee),s(J)),k(t,h),k(d),k(P),k(de),k(X),k(ne,h),k(ve),k(me),k(se),k(j),k(re,h),k(Me),k(oe),k(H),k(Je)}}}function Kn(D){let t,g;return t=new Un({props:{$$slots:{default:[Yn]},$$scope:{ctx:D}}}),{c(){v(t.$$.fragment)},l(n){$(t.$$.fragment,n)},m(n,d){y(t,n,d),g=!0},p(n,d){const _={};d&2&&(_.$$scope={dirty:d,ctx:n}),t.$set(_)},i(n){g||(R(t.$$.fragment,n),g=!0)},o(n){w(t.$$.fragment,n),g=!1},d(n){k(t,n)}}}function eo(D){let t,g="TensorFlow models and layers in <code>transformers</code> accept two formats as input:",n,d,_="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional argument.</li>",o,T,ie=`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like <code>model.fit()</code> things should “just work” for you - just
pass your inputs and labels in any format that <code>model.fit()</code> supports! If, however, you want to use the second
format outside of Keras methods like <code>fit()</code> and <code>predict()</code>, such as when creating your own layers or models with
the Keras <code>Functional</code> API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`,q,C,K=`<li>a single Tensor with <code>input_ids</code> only and nothing else: <code>model(input_ids)</code></li> <li>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code>model([input_ids, attention_mask])</code> or <code>model([input_ids, attention_mask, token_type_ids])</code></li> <li>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code>model({&quot;input_ids&quot;: input_ids, &quot;token_type_ids&quot;: token_type_ids})</code></li>`,z,E,ee=`Note that when creating models and layers with
<a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="nofollow">subclassing</a> then you don’t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`;return{c(){t=c("p"),t.innerHTML=g,n=r(),d=c("ul"),d.innerHTML=_,o=r(),T=c("p"),T.innerHTML=ie,q=r(),C=c("ul"),C.innerHTML=K,z=r(),E=c("p"),E.innerHTML=ee},l(m){t=p(m,"P",{"data-svelte-h":!0}),f(t)!=="svelte-1ajbfxg"&&(t.innerHTML=g),n=a(m),d=p(m,"UL",{"data-svelte-h":!0}),f(d)!=="svelte-qm1t26"&&(d.innerHTML=_),o=a(m),T=p(m,"P",{"data-svelte-h":!0}),f(T)!=="svelte-1v9qsc5"&&(T.innerHTML=ie),q=a(m),C=p(m,"UL",{"data-svelte-h":!0}),f(C)!=="svelte-15scerc"&&(C.innerHTML=K),z=a(m),E=p(m,"P",{"data-svelte-h":!0}),f(E)!=="svelte-1an3odd"&&(E.innerHTML=ee)},m(m,P){l(m,t,P),l(m,n,P),l(m,d,P),l(m,o,P),l(m,T,P),l(m,q,P),l(m,C,P),l(m,z,P),l(m,E,P)},p:we,d(m){m&&(s(t),s(n),s(d),s(o),s(T),s(q),s(C),s(z),s(E))}}}function to(D){let t,g=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=g},l(n){t=p(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=g)},m(n,d){l(n,t,d)},p:we,d(n){n&&s(t)}}}function no(D){let t,g="Examples:",n,d,_;return d=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGRFBSQ29udGV4dEVuY29kZXIlMkMlMjBEUFJDb250ZXh0RW5jb2RlclRva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMERQUkNvbnRleHRFbmNvZGVyVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmRwci1jdHhfZW5jb2Rlci1zaW5nbGUtbnEtYmFzZSUyMiklMEFtb2RlbCUyMCUzRCUyMFRGRFBSQ29udGV4dEVuY29kZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZHByLWN0eF9lbmNvZGVyLXNpbmdsZS1ucS1iYXNlJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUpJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKCUyMkhlbGxvJTJDJTIwaXMlMjBteSUyMGRvZyUyMGN1dGUlMjAlM0YlMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSU1QiUyMmlucHV0X2lkcyUyMiU1RCUwQWVtYmVkZGluZ3MlMjAlM0QlMjBtb2RlbChpbnB1dF9pZHMpLnBvb2xlcl9vdXRwdXQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRContextEncoder, DPRContextEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRContextEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRContextEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-ctx_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`,wrap:!1}}),{c(){t=c("p"),t.textContent=g,n=r(),v(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-kvfsh7"&&(t.textContent=g),n=a(o),$(d.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),y(d,o,T),_=!0},p:we,i(o){_||(R(d.$$.fragment,o),_=!0)},o(o){w(d.$$.fragment,o),_=!1},d(o){o&&(s(t),s(n)),k(d,o)}}}function oo(D){let t,g="TensorFlow models and layers in <code>transformers</code> accept two formats as input:",n,d,_="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional argument.</li>",o,T,ie=`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like <code>model.fit()</code> things should “just work” for you - just
pass your inputs and labels in any format that <code>model.fit()</code> supports! If, however, you want to use the second
format outside of Keras methods like <code>fit()</code> and <code>predict()</code>, such as when creating your own layers or models with
the Keras <code>Functional</code> API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`,q,C,K=`<li>a single Tensor with <code>input_ids</code> only and nothing else: <code>model(input_ids)</code></li> <li>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code>model([input_ids, attention_mask])</code> or <code>model([input_ids, attention_mask, token_type_ids])</code></li> <li>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code>model({&quot;input_ids&quot;: input_ids, &quot;token_type_ids&quot;: token_type_ids})</code></li>`,z,E,ee=`Note that when creating models and layers with
<a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="nofollow">subclassing</a> then you don’t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`;return{c(){t=c("p"),t.innerHTML=g,n=r(),d=c("ul"),d.innerHTML=_,o=r(),T=c("p"),T.innerHTML=ie,q=r(),C=c("ul"),C.innerHTML=K,z=r(),E=c("p"),E.innerHTML=ee},l(m){t=p(m,"P",{"data-svelte-h":!0}),f(t)!=="svelte-1ajbfxg"&&(t.innerHTML=g),n=a(m),d=p(m,"UL",{"data-svelte-h":!0}),f(d)!=="svelte-qm1t26"&&(d.innerHTML=_),o=a(m),T=p(m,"P",{"data-svelte-h":!0}),f(T)!=="svelte-1v9qsc5"&&(T.innerHTML=ie),q=a(m),C=p(m,"UL",{"data-svelte-h":!0}),f(C)!=="svelte-15scerc"&&(C.innerHTML=K),z=a(m),E=p(m,"P",{"data-svelte-h":!0}),f(E)!=="svelte-1an3odd"&&(E.innerHTML=ee)},m(m,P){l(m,t,P),l(m,n,P),l(m,d,P),l(m,o,P),l(m,T,P),l(m,q,P),l(m,C,P),l(m,z,P),l(m,E,P)},p:we,d(m){m&&(s(t),s(n),s(d),s(o),s(T),s(q),s(C),s(z),s(E))}}}function so(D){let t,g=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=g},l(n){t=p(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=g)},m(n,d){l(n,t,d)},p:we,d(n){n&&s(t)}}}function ro(D){let t,g="Examples:",n,d,_;return d=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGRFBSUXVlc3Rpb25FbmNvZGVyJTJDJTIwRFBSUXVlc3Rpb25FbmNvZGVyVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwRFBSUXVlc3Rpb25FbmNvZGVyVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmRwci1xdWVzdGlvbl9lbmNvZGVyLXNpbmdsZS1ucS1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZEUFJRdWVzdGlvbkVuY29kZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZHByLXF1ZXN0aW9uX2VuY29kZXItc2luZ2xlLW5xLWJhc2UlMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSklMEFpbnB1dF9pZHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySGVsbG8lMkMlMjBpcyUyMG15JTIwZG9nJTIwY3V0ZSUyMCUzRiUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIydGYlMjIpJTVCJTIyaW5wdXRfaWRzJTIyJTVEJTBBZW1iZWRkaW5ncyUyMCUzRCUyMG1vZGVsKGlucHV0X2lkcykucG9vbGVyX291dHB1dA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRQuestionEncoder, DPRQuestionEncoderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRQuestionEncoder.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-question_encoder-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(<span class="hljs-string">&quot;Hello, is my dog cute ?&quot;</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>embeddings = model(input_ids).pooler_output`,wrap:!1}}),{c(){t=c("p"),t.textContent=g,n=r(),v(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-kvfsh7"&&(t.textContent=g),n=a(o),$(d.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),y(d,o,T),_=!0},p:we,i(o){_||(R(d.$$.fragment,o),_=!0)},o(o){w(d.$$.fragment,o),_=!1},d(o){o&&(s(t),s(n)),k(d,o)}}}function ao(D){let t,g="TensorFlow models and layers in <code>transformers</code> accept two formats as input:",n,d,_="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional argument.</li>",o,T,ie=`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like <code>model.fit()</code> things should “just work” for you - just
pass your inputs and labels in any format that <code>model.fit()</code> supports! If, however, you want to use the second
format outside of Keras methods like <code>fit()</code> and <code>predict()</code>, such as when creating your own layers or models with
the Keras <code>Functional</code> API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`,q,C,K=`<li>a single Tensor with <code>input_ids</code> only and nothing else: <code>model(input_ids)</code></li> <li>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code>model([input_ids, attention_mask])</code> or <code>model([input_ids, attention_mask, token_type_ids])</code></li> <li>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code>model({&quot;input_ids&quot;: input_ids, &quot;token_type_ids&quot;: token_type_ids})</code></li>`,z,E,ee=`Note that when creating models and layers with
<a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="nofollow">subclassing</a> then you don’t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`;return{c(){t=c("p"),t.innerHTML=g,n=r(),d=c("ul"),d.innerHTML=_,o=r(),T=c("p"),T.innerHTML=ie,q=r(),C=c("ul"),C.innerHTML=K,z=r(),E=c("p"),E.innerHTML=ee},l(m){t=p(m,"P",{"data-svelte-h":!0}),f(t)!=="svelte-1ajbfxg"&&(t.innerHTML=g),n=a(m),d=p(m,"UL",{"data-svelte-h":!0}),f(d)!=="svelte-qm1t26"&&(d.innerHTML=_),o=a(m),T=p(m,"P",{"data-svelte-h":!0}),f(T)!=="svelte-1v9qsc5"&&(T.innerHTML=ie),q=a(m),C=p(m,"UL",{"data-svelte-h":!0}),f(C)!=="svelte-15scerc"&&(C.innerHTML=K),z=a(m),E=p(m,"P",{"data-svelte-h":!0}),f(E)!=="svelte-1an3odd"&&(E.innerHTML=ee)},m(m,P){l(m,t,P),l(m,n,P),l(m,d,P),l(m,o,P),l(m,T,P),l(m,q,P),l(m,C,P),l(m,z,P),l(m,E,P)},p:we,d(m){m&&(s(t),s(n),s(d),s(o),s(T),s(q),s(C),s(z),s(E))}}}function io(D){let t,g=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=g},l(n){t=p(n,"P",{"data-svelte-h":!0}),f(t)!=="svelte-fincs2"&&(t.innerHTML=g)},m(n,d){l(n,t,d)},p:we,d(n){n&&s(t)}}}function lo(D){let t,g="Examples:",n,d,_;return d=new _t({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGRFBSUmVhZGVyJTJDJTIwRFBSUmVhZGVyVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwRFBSUmVhZGVyVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmRwci1yZWFkZXItc2luZ2xlLW5xLWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkRQUlJlYWRlci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkcHItcmVhZGVyLXNpbmdsZS1ucS1iYXNlJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUpJTBBZW5jb2RlZF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwcXVlc3Rpb25zJTNEJTVCJTIyV2hhdCUyMGlzJTIwbG92ZSUyMCUzRiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHRpdGxlcyUzRCU1QiUyMkhhZGRhd2F5JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwdGV4dHMlM0QlNUIlMjInV2hhdCUyMElzJTIwTG92ZSclMjBpcyUyMGElMjBzb25nJTIwcmVjb3JkZWQlMjBieSUyMHRoZSUyMGFydGlzdCUyMEhhZGRhd2F5JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMiUyQyUwQSklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoZW5jb2RlZF9pbnB1dHMpJTBBc3RhcnRfbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5zdGFydF9sb2dpdHMlMEFlbmRfbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5lbmRfbG9naXRzJTBBcmVsZXZhbmNlX2xvZ2l0cyUyMCUzRCUyMG91dHB1dHMucmVsZXZhbmNlX2xvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFDPRReader, DPRReaderTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = DPRReaderTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDPRReader.from_pretrained(<span class="hljs-string">&quot;facebook/dpr-reader-single-nq-base&quot;</span>, from_pt=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_inputs = tokenizer(
<span class="hljs-meta">... </span>    questions=[<span class="hljs-string">&quot;What is love ?&quot;</span>],
<span class="hljs-meta">... </span>    titles=[<span class="hljs-string">&quot;Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    texts=[<span class="hljs-string">&quot;&#x27;What Is Love&#x27; is a song recorded by the artist Haddaway&quot;</span>],
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(encoded_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>start_logits = outputs.start_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>end_logits = outputs.end_logits
<span class="hljs-meta">&gt;&gt;&gt; </span>relevance_logits = outputs.relevance_logits`,wrap:!1}}),{c(){t=c("p"),t.textContent=g,n=r(),v(d.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),f(t)!=="svelte-kvfsh7"&&(t.textContent=g),n=a(o),$(d.$$.fragment,o)},m(o,T){l(o,t,T),l(o,n,T),y(d,o,T),_=!0},p:we,i(o){_||(R(d.$$.fragment,o),_=!0)},o(o){w(d.$$.fragment,o),_=!1},d(o){o&&(s(t),s(n)),k(d,o)}}}function co(D){let t,g,n,d,_,o,T="The bare DPRContextEncoder transformer outputting pooler outputs as context representations.",ie,q,C=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,K,z,E=`This model is also a Tensorflow <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a>
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`,ee,m,P,te,S,at,ze,de='The <a href="/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRContextEncoder">TFDPRContextEncoder</a> forward method, overrides the <code>__call__</code> special method.',ot,X,ke,ne,be,U,ve,L,le,O,pe,We="The bare DPRQuestionEncoder transformer outputting pooler outputs as question representations.",Be,Ge,Se=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Xe,Oe,ce=`This model is also a Tensorflow <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a>
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`,me,Fe,je,Q,xe,se,qe,j='The <a href="/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> forward method, overrides the <code>__call__</code> special method.',Pe,re,Ee,J,Me,ue,He,V,fe,Ae,Ie,Ze="The bare DPRReader transformer outputting span predictions.",Le,G,Ne=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ae,oe,it=`This model is also a Tensorflow <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a>
subclass. Use it as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to
general usage and behavior.`,Qe,he,st,H,ge,Je,De,h='The <a href="/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRReader">TFDPRReader</a> forward method, overrides the <code>__call__</code> special method.',M,F,I,W,Z;return t=new Re({props:{title:"TFDPRContextEncoder",local:"transformers.TFDPRContextEncoder",headingTag:"h2"}}),d=new Y({props:{name:"class transformers.TFDPRContextEncoder",anchor:"transformers.TFDPRContextEncoder",parameters:[{name:"config",val:": DPRConfig"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L547"}}),m=new pt({props:{$$slots:{default:[eo]},$$scope:{ctx:D}}}),S=new Y({props:{name:"call",anchor:"transformers.TFDPRContextEncoder.call",parameters:[{name:"input_ids",val:": TFModelInputType | None = None"},{name:"attention_mask",val:": tf.Tensor | None = None"},{name:"token_type_ids",val:": tf.Tensor | None = None"},{name:"inputs_embeds",val:": tf.Tensor | None = None"},{name:"output_attentions",val:": bool | None = None"},{name:"output_hidden_states",val:": bool | None = None"},{name:"return_dict",val:": bool | None = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRContextEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L563",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) — The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRContextEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),X=new pt({props:{$$slots:{default:[to]},$$scope:{ctx:D}}}),ne=new gt({props:{anchor:"transformers.TFDPRContextEncoder.call.example",$$slots:{default:[no]},$$scope:{ctx:D}}}),U=new Re({props:{title:"TFDPRQuestionEncoder",local:"transformers.TFDPRQuestionEncoder",headingTag:"h2"}}),le=new Y({props:{name:"class transformers.TFDPRQuestionEncoder",anchor:"transformers.TFDPRQuestionEncoder",parameters:[{name:"config",val:": DPRConfig"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L636"}}),Fe=new pt({props:{$$slots:{default:[oo]},$$scope:{ctx:D}}}),xe=new Y({props:{name:"call",anchor:"transformers.TFDPRQuestionEncoder.call",parameters:[{name:"input_ids",val:": TFModelInputType | None = None"},{name:"attention_mask",val:": tf.Tensor | None = None"},{name:"token_type_ids",val:": tf.Tensor | None = None"},{name:"inputs_embeds",val:": tf.Tensor | None = None"},{name:"output_attentions",val:": bool | None = None"},{name:"output_hidden_states",val:": bool | None = None"},{name:"return_dict",val:": bool | None = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRQuestionEncoder.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. To match pretraining, DPR input sequence should be
formatted with [CLS] and [SEP] tokens as follows:</p>
<p>(a) For sequence pairs (for a pair title+text for example):`,name:"input_ids"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L652",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, embeddings_size)</code>) — The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRQuestionEncoderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),re=new pt({props:{$$slots:{default:[so]},$$scope:{ctx:D}}}),J=new gt({props:{anchor:"transformers.TFDPRQuestionEncoder.call.example",$$slots:{default:[ro]},$$scope:{ctx:D}}}),ue=new Re({props:{title:"TFDPRReader",local:"transformers.TFDPRReader",headingTag:"h2"}}),fe=new Y({props:{name:"class transformers.TFDPRReader",anchor:"transformers.TFDPRReader",parameters:[{name:"config",val:": DPRConfig"},{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDPRReader.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L724"}}),he=new pt({props:{$$slots:{default:[ao]},$$scope:{ctx:D}}}),ge=new Y({props:{name:"call",anchor:"transformers.TFDPRReader.call",parameters:[{name:"input_ids",val:": TFModelInputType | None = None"},{name:"attention_mask",val:": tf.Tensor | None = None"},{name:"inputs_embeds",val:": tf.Tensor | None = None"},{name:"output_attentions",val:": bool | None = None"},{name:"output_hidden_states",val:": bool | None = None"},{name:"return_dict",val:": bool | None = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDPRReader.call.input_ids",description:`<strong>input_ids</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shapes <code>(n_passages, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. It has to be a sequence triplet with 1) the question
and 2) the passages titles and 3) the passages texts To match pretraining, DPR <code>input_ids</code> sequence should
be formatted with [CLS] and [SEP] with the format:</p>
<p><code>[CLS] &lt;question token ids&gt; [SEP] &lt;titles ids&gt; [SEP] &lt;texts ids&gt;</code></p>
<p>DPR is a model with absolute position embeddings so it&#x2019;s usually advised to pad the inputs on the right
rather than the left.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderTokenizer">DPRReaderTokenizer</a>. See this class documentation for more details.`,name:"input_ids"},{anchor:"transformers.TFDPRReader.call.attention_mask",description:`<strong>attention_mask</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFDPRReader.call.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>Numpy array</code> or <code>tf.Tensor</code> of shape <code>(n_passages, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.TFDPRReader.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFDPRReader.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFDPRReader.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_tf_dpr.py#L740",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig"
>DPRConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>start_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) — Logits of the start index of the span for each passage.</p>
</li>
<li>
<p><strong>end_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, sequence_length)</code>) — Logits of the end index of the span for each passage.</p>
</li>
<li>
<p><strong>relevance_logits</strong> (<code>tf.Tensor</code> of shape <code>(n_passages, )</code>) — Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.dpr.modeling_tf_dpr.TFDPRReaderOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),F=new pt({props:{$$slots:{default:[io]},$$scope:{ctx:D}}}),W=new gt({props:{anchor:"transformers.TFDPRReader.call.example",$$slots:{default:[lo]},$$scope:{ctx:D}}}),{c(){v(t.$$.fragment),g=r(),n=c("div"),v(d.$$.fragment),_=r(),o=c("p"),o.textContent=T,ie=r(),q=c("p"),q.innerHTML=C,K=r(),z=c("p"),z.innerHTML=E,ee=r(),v(m.$$.fragment),P=r(),te=c("div"),v(S.$$.fragment),at=r(),ze=c("p"),ze.innerHTML=de,ot=r(),v(X.$$.fragment),ke=r(),v(ne.$$.fragment),be=r(),v(U.$$.fragment),ve=r(),L=c("div"),v(le.$$.fragment),O=r(),pe=c("p"),pe.textContent=We,Be=r(),Ge=c("p"),Ge.innerHTML=Se,Xe=r(),Oe=c("p"),Oe.innerHTML=ce,me=r(),v(Fe.$$.fragment),je=r(),Q=c("div"),v(xe.$$.fragment),se=r(),qe=c("p"),qe.innerHTML=j,Pe=r(),v(re.$$.fragment),Ee=r(),v(J.$$.fragment),Me=r(),v(ue.$$.fragment),He=r(),V=c("div"),v(fe.$$.fragment),Ae=r(),Ie=c("p"),Ie.textContent=Ze,Le=r(),G=c("p"),G.innerHTML=Ne,ae=r(),oe=c("p"),oe.innerHTML=it,Qe=r(),v(he.$$.fragment),st=r(),H=c("div"),v(ge.$$.fragment),Je=r(),De=c("p"),De.innerHTML=h,M=r(),v(F.$$.fragment),I=r(),v(W.$$.fragment),this.h()},l(u){$(t.$$.fragment,u),g=a(u),n=p(u,"DIV",{class:!0});var x=B(n);$(d.$$.fragment,x),_=a(x),o=p(x,"P",{"data-svelte-h":!0}),f(o)!=="svelte-18o6tt0"&&(o.textContent=T),ie=a(x),q=p(x,"P",{"data-svelte-h":!0}),f(q)!=="svelte-1qaxm70"&&(q.innerHTML=C),K=a(x),z=p(x,"P",{"data-svelte-h":!0}),f(z)!=="svelte-12eggov"&&(z.innerHTML=E),ee=a(x),$(m.$$.fragment,x),P=a(x),te=p(x,"DIV",{class:!0});var _e=B(te);$(S.$$.fragment,_e),at=a(_e),ze=p(_e,"P",{"data-svelte-h":!0}),f(ze)!=="svelte-dcx9i1"&&(ze.innerHTML=de),ot=a(_e),$(X.$$.fragment,_e),ke=a(_e),$(ne.$$.fragment,_e),_e.forEach(s),x.forEach(s),be=a(u),$(U.$$.fragment,u),ve=a(u),L=p(u,"DIV",{class:!0});var Te=B(L);$(le.$$.fragment,Te),O=a(Te),pe=p(Te,"P",{"data-svelte-h":!0}),f(pe)!=="svelte-1trrfka"&&(pe.textContent=We),Be=a(Te),Ge=p(Te,"P",{"data-svelte-h":!0}),f(Ge)!=="svelte-1qaxm70"&&(Ge.innerHTML=Se),Xe=a(Te),Oe=p(Te,"P",{"data-svelte-h":!0}),f(Oe)!=="svelte-12eggov"&&(Oe.innerHTML=ce),me=a(Te),$(Fe.$$.fragment,Te),je=a(Te),Q=p(Te,"DIV",{class:!0});var Ue=B(Q);$(xe.$$.fragment,Ue),se=a(Ue),qe=p(Ue,"P",{"data-svelte-h":!0}),f(qe)!=="svelte-1s33grh"&&(qe.innerHTML=j),Pe=a(Ue),$(re.$$.fragment,Ue),Ee=a(Ue),$(J.$$.fragment,Ue),Ue.forEach(s),Te.forEach(s),Me=a(u),$(ue.$$.fragment,u),He=a(u),V=p(u,"DIV",{class:!0});var A=B(V);$(fe.$$.fragment,A),Ae=a(A),Ie=p(A,"P",{"data-svelte-h":!0}),f(Ie)!=="svelte-sj9qc8"&&(Ie.textContent=Ze),Le=a(A),G=p(A,"P",{"data-svelte-h":!0}),f(G)!=="svelte-1qaxm70"&&(G.innerHTML=Ne),ae=a(A),oe=p(A,"P",{"data-svelte-h":!0}),f(oe)!=="svelte-12eggov"&&(oe.innerHTML=it),Qe=a(A),$(he.$$.fragment,A),st=a(A),H=p(A,"DIV",{class:!0});var Ve=B(H);$(ge.$$.fragment,Ve),Je=a(Ve),De=p(Ve,"P",{"data-svelte-h":!0}),f(De)!=="svelte-1g5s7el"&&(De.innerHTML=h),M=a(Ve),$(F.$$.fragment,Ve),I=a(Ve),$(W.$$.fragment,Ve),Ve.forEach(s),A.forEach(s),this.h()},h(){N(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(n,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(u,x){y(t,u,x),l(u,g,x),l(u,n,x),y(d,n,null),i(n,_),i(n,o),i(n,ie),i(n,q),i(n,K),i(n,z),i(n,ee),y(m,n,null),i(n,P),i(n,te),y(S,te,null),i(te,at),i(te,ze),i(te,ot),y(X,te,null),i(te,ke),y(ne,te,null),l(u,be,x),y(U,u,x),l(u,ve,x),l(u,L,x),y(le,L,null),i(L,O),i(L,pe),i(L,Be),i(L,Ge),i(L,Xe),i(L,Oe),i(L,me),y(Fe,L,null),i(L,je),i(L,Q),y(xe,Q,null),i(Q,se),i(Q,qe),i(Q,Pe),y(re,Q,null),i(Q,Ee),y(J,Q,null),l(u,Me,x),y(ue,u,x),l(u,He,x),l(u,V,x),y(fe,V,null),i(V,Ae),i(V,Ie),i(V,Le),i(V,G),i(V,ae),i(V,oe),i(V,Qe),y(he,V,null),i(V,st),i(V,H),y(ge,H,null),i(H,Je),i(H,De),i(H,M),y(F,H,null),i(H,I),y(W,H,null),Z=!0},p(u,x){const _e={};x&2&&(_e.$$scope={dirty:x,ctx:u}),m.$set(_e);const Te={};x&2&&(Te.$$scope={dirty:x,ctx:u}),X.$set(Te);const Ue={};x&2&&(Ue.$$scope={dirty:x,ctx:u}),ne.$set(Ue);const A={};x&2&&(A.$$scope={dirty:x,ctx:u}),Fe.$set(A);const Ve={};x&2&&(Ve.$$scope={dirty:x,ctx:u}),re.$set(Ve);const Tt={};x&2&&(Tt.$$scope={dirty:x,ctx:u}),J.$set(Tt);const rt={};x&2&&(rt.$$scope={dirty:x,ctx:u}),he.$set(rt);const bt={};x&2&&(bt.$$scope={dirty:x,ctx:u}),F.$set(bt);const Ce={};x&2&&(Ce.$$scope={dirty:x,ctx:u}),W.$set(Ce)},i(u){Z||(R(t.$$.fragment,u),R(d.$$.fragment,u),R(m.$$.fragment,u),R(S.$$.fragment,u),R(X.$$.fragment,u),R(ne.$$.fragment,u),R(U.$$.fragment,u),R(le.$$.fragment,u),R(Fe.$$.fragment,u),R(xe.$$.fragment,u),R(re.$$.fragment,u),R(J.$$.fragment,u),R(ue.$$.fragment,u),R(fe.$$.fragment,u),R(he.$$.fragment,u),R(ge.$$.fragment,u),R(F.$$.fragment,u),R(W.$$.fragment,u),Z=!0)},o(u){w(t.$$.fragment,u),w(d.$$.fragment,u),w(m.$$.fragment,u),w(S.$$.fragment,u),w(X.$$.fragment,u),w(ne.$$.fragment,u),w(U.$$.fragment,u),w(le.$$.fragment,u),w(Fe.$$.fragment,u),w(xe.$$.fragment,u),w(re.$$.fragment,u),w(J.$$.fragment,u),w(ue.$$.fragment,u),w(fe.$$.fragment,u),w(he.$$.fragment,u),w(ge.$$.fragment,u),w(F.$$.fragment,u),w(W.$$.fragment,u),Z=!1},d(u){u&&(s(g),s(n),s(be),s(ve),s(L),s(Me),s(He),s(V)),k(t,u),k(d),k(m),k(S),k(X),k(ne),k(U,u),k(le),k(Fe),k(xe),k(re),k(J),k(ue,u),k(fe),k(he),k(ge),k(F),k(W)}}}function po(D){let t,g;return t=new Un({props:{$$slots:{default:[co]},$$scope:{ctx:D}}}),{c(){v(t.$$.fragment)},l(n){$(t.$$.fragment,n)},m(n,d){y(t,n,d),g=!0},p(n,d){const _={};d&2&&(_.$$scope={dirty:d,ctx:n}),t.$set(_)},i(n){g||(R(t.$$.fragment,n),g=!0)},o(n){w(t.$$.fragment,n),g=!1},d(n){k(t,n)}}}function mo(D){let t,g,n,d,_,o,T,ie='<a href="https://huggingface.co/models?filter=dpr"><img alt="Models" src="https://img.shields.io/badge/All_model_pages-dpr-blueviolet"/></a> <a href="https://huggingface.co/spaces/docs-demos/dpr-question_encoder-bert-base-multilingual"><img alt="Spaces" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue"/></a>',q,C,K,z,E=`Dense Passage Retrieval (DPR) is a set of tools and models for state-of-the-art open-domain Q&amp;A research. It was
introduced in <a href="https://arxiv.org/abs/2004.04906" rel="nofollow">Dense Passage Retrieval for Open-Domain Question Answering</a> by
Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, Wen-tau Yih.`,ee,m,P="The abstract from the paper is the following:",te,S,at=`<em>Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional
sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can
be practically implemented using dense representations alone, where embeddings are learned from a small number of
questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets,
our dense retriever outperforms a strong Lucene-BM25 system largely by 9%-19% absolute in terms of top-20 passage
retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA
benchmarks.</em>`,ze,de,ot='This model was contributed by <a href="https://huggingface.co/lhoestq" rel="nofollow">lhoestq</a>. The original code can be found <a href="https://github.com/facebookresearch/DPR" rel="nofollow">here</a>.',X,ke,ne,be,U="<li><p>DPR consists in three models:</p> <ul><li>Question encoder: encode questions as vectors</li> <li>Context encoder: encode contexts as vectors</li> <li>Reader: extract the answer of the questions inside retrieved contexts, along with a relevance score (high if the inferred span actually answers the question).</li></ul></li>",ve,L,le,O,pe,We,Be,Ge='<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> is the configuration class to store the configuration of a <em>DPRModel</em>.',Se,Xe,Oe=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoder">DPRContextEncoder</a>, <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a>, or a
<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader">DPRReader</a>. It is used to instantiate the components of the DPR model according to the specified arguments,
defining the model component architectures. Instantiating a configuration with the defaults will yield a similar
configuration to that of the DPRContextEncoder
<a href="https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base" rel="nofollow">facebook/dpr-ctx_encoder-single-nq-base</a>
architecture.`,ce,me,Fe='This class is a subclass of <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a>. Please check the superclass for the documentation of all kwargs.',je,Q,xe,se,qe,j,Pe,re,Ee,J="Construct a DPRContextEncoder tokenizer.",Me,ue,He=`<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoderTokenizer">DPRContextEncoderTokenizer</a> is identical to <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> and runs end-to-end tokenization: punctuation
splitting and wordpiece.`,V,fe,Ae='Refer to superclass <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> for usage examples and documentation concerning parameters.',Ie,Ze,Le,G,Ne,ae,oe,it="Construct a “fast” DPRContextEncoder tokenizer (backed by HuggingFace’s <em>tokenizers</em> library).",Qe,he,st=`<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRContextEncoderTokenizerFast">DPRContextEncoderTokenizerFast</a> is identical to <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> and runs end-to-end tokenization:
punctuation splitting and wordpiece.`,H,ge,Je='Refer to superclass <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> for usage examples and documentation concerning parameters.',De,h,M,F,I,W,Z,u="Constructs a DPRQuestionEncoder tokenizer.",x,_e,Te=`<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer">DPRQuestionEncoderTokenizer</a> is identical to <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> and runs end-to-end tokenization: punctuation
splitting and wordpiece.`,Ue,A,Ve='Refer to superclass <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> for usage examples and documentation concerning parameters.',Tt,rt,bt,Ce,vt,rn,Dt,yn="Constructs a “fast” DPRQuestionEncoder tokenizer (backed by HuggingFace’s <em>tokenizers</em> library).",an,Ct,Rn=`<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast">DPRQuestionEncoderTokenizerFast</a> is identical to <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> and runs end-to-end tokenization:
punctuation splitting and wordpiece.`,dn,zt,wn='Refer to superclass <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> for usage examples and documentation concerning parameters.',Wt,$t,Bt,$e,yt,ln,Ft,kn="Construct a DPRReader tokenizer.",cn,qt,xn=`<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderTokenizer">DPRReaderTokenizer</a> is almost identical to <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> and runs end-to-end tokenization: punctuation
splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts that are
combined to be fed to the <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader">DPRReader</a> model.`,pn,Et,Pn='Refer to superclass <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> for usage examples and documentation concerning parameters.',mn,Lt,Mn=`Return a dictionary with the token ids of the input strings and other information to give to <code>.decode_best_spans</code>.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting <code>input_ids</code> is a matrix of size <code>(n_passages, sequence_length)</code>`,un,mt,Gt,Rt,St,ye,wt,fn,Jt,Dn="Constructs a “fast” DPRReader tokenizer (backed by HuggingFace’s <em>tokenizers</em> library).",hn,Ut,Cn=`<a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReaderTokenizerFast">DPRReaderTokenizerFast</a> is almost identical to <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> and runs end-to-end tokenization:
punctuation splitting and wordpiece. The difference is that is has three inputs strings: question, titles and texts
that are combined to be fed to the <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRReader">DPRReader</a> model.`,gn,jt,zn='Refer to superclass <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> for usage examples and documentation concerning parameters.',_n,Ht,Fn=`Return a dictionary with the token ids of the input strings and other information to give to <code>.decode_best_spans</code>.
It converts the strings of a question and different passages (title and text) in a sequence of IDs (integers),
using the tokenizer and vocabulary. The resulting <code>input_ids</code> is a matrix of size <code>(n_passages, sequence_length)</code>
with the format:`,Tn,It,qn="[CLS] <question token ids> [SEP] <titles ids> [SEP] <texts ids>",Xt,kt,Ot,dt,xt,bn,Zt,En='Class for outputs of <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a>.',At,lt,Pt,vn,Nt,Ln='Class for outputs of <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a>.',Yt,ct,Mt,$n,Qt,Jn='Class for outputs of <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a>.',Kt,ut,en,Vt,tn;return _=new Re({props:{title:"DPR",local:"dpr",headingTag:"h1"}}),C=new Re({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ke=new Re({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),L=new Re({props:{title:"DPRConfig",local:"transformers.DPRConfig",headingTag:"h2"}}),pe=new Y({props:{name:"class transformers.DPRConfig",anchor:"transformers.DPRConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"projection_dim",val:": int = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the DPR model. Defines the different tokens that can be represented by the <em>inputs_ids</em>
passed to the forward method of <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"vocab_size"},{anchor:"transformers.DPRConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DPRConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DPRConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DPRConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DPRConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DPRConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DPRConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DPRConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.DPRConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.DPRConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DPRConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DPRConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.DPRConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.DPRConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Dimension of the projection for the context and question encoders. If it is set to zero (default), then no
projection is done.`,name:"projection_dim"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/configuration_dpr.py#L45"}}),Q=new gt({props:{anchor:"transformers.DPRConfig.example",$$slots:{default:[Vn]},$$scope:{ctx:D}}}),se=new Re({props:{title:"DPRContextEncoderTokenizer",local:"transformers.DPRContextEncoderTokenizer",headingTag:"h2"}}),Pe=new Y({props:{name:"class transformers.DPRContextEncoderTokenizer",anchor:"transformers.DPRContextEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/tokenization_dpr.py#L113"}}),Ze=new Re({props:{title:"DPRContextEncoderTokenizerFast",local:"transformers.DPRContextEncoderTokenizerFast",headingTag:"h2"}}),Ne=new Y({props:{name:"class transformers.DPRContextEncoderTokenizerFast",anchor:"transformers.DPRContextEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/tokenization_dpr_fast.py#L114"}}),h=new Re({props:{title:"DPRQuestionEncoderTokenizer",local:"transformers.DPRQuestionEncoderTokenizer",headingTag:"h2"}}),I=new Y({props:{name:"class transformers.DPRQuestionEncoderTokenizer",anchor:"transformers.DPRQuestionEncoderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/tokenization_dpr.py#L129"}}),rt=new Re({props:{title:"DPRQuestionEncoderTokenizerFast",local:"transformers.DPRQuestionEncoderTokenizerFast",headingTag:"h2"}}),vt=new Y({props:{name:"class transformers.DPRQuestionEncoderTokenizerFast",anchor:"transformers.DPRQuestionEncoderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/tokenization_dpr_fast.py#L131"}}),$t=new Re({props:{title:"DPRReaderTokenizer",local:"transformers.DPRReaderTokenizer",headingTag:"h2"}}),yt=new Y({props:{name:"class transformers.DPRReaderTokenizer",anchor:"transformers.DPRReaderTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizer.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizer.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizer.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizer.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizer.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizer.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizer.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizer.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/tokenization_dpr.py#L394",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Dict[str, List[List[int]]]</code></p>
`}}),mt=new gt({props:{anchor:"transformers.DPRReaderTokenizer.example",$$slots:{default:[Wn]},$$scope:{ctx:D}}}),Rt=new Re({props:{title:"DPRReaderTokenizerFast",local:"transformers.DPRReaderTokenizerFast",headingTag:"h2"}}),wt=new Y({props:{name:"class transformers.DPRReaderTokenizerFast",anchor:"transformers.DPRReaderTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DPRReaderTokenizerFast.questions",description:`<strong>questions</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The questions to be encoded. You can specify one question for many passages. In this case, the question
will be duplicated like <code>[questions] * n_passages</code>. Otherwise you have to specify as many questions as in
<code>titles</code> or <code>texts</code>.`,name:"questions"},{anchor:"transformers.DPRReaderTokenizerFast.titles",description:`<strong>titles</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages titles to be encoded. This can be a string or a list of strings if there are several passages.`,name:"titles"},{anchor:"transformers.DPRReaderTokenizerFast.texts",description:`<strong>texts</strong> (<code>str</code> or <code>List[str]</code>) &#x2014;
The passages texts to be encoded. This can be a string or a list of strings if there are several passages.`,name:"texts"},{anchor:"transformers.DPRReaderTokenizerFast.padding",description:`<strong>padding</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.utils.PaddingStrategy">PaddingStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls padding. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest&apos;</code>: Pad to the longest sequence in the batch (or no padding if only a single sequence
if provided).</li>
<li><code>&apos;max_length&apos;</code>: Pad to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided.</li>
<li><code>False</code> or <code>&apos;do_not_pad&apos;</code> (default): No padding (i.e., can output a batch with sequences of different
lengths).</li>
</ul>`,name:"padding"},{anchor:"transformers.DPRReaderTokenizerFast.truncation",description:`<strong>truncation</strong> (<code>bool</code>, <code>str</code> or <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.tokenization_utils_base.TruncationStrategy">TruncationStrategy</a>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Activates and controls truncation. Accepts the following values:</p>
<ul>
<li><code>True</code> or <code>&apos;longest_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to
the maximum acceptable input length for the model if that argument is not provided. This will truncate
token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch
of pairs) is provided.</li>
<li><code>&apos;only_first&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the first
sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>&apos;only_second&apos;</code>: Truncate to a maximum length specified with the argument <code>max_length</code> or to the maximum
acceptable input length for the model if that argument is not provided. This will only truncate the
second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.</li>
<li><code>False</code> or <code>&apos;do_not_truncate&apos;</code> (default): No truncation (i.e., can output batch with sequence lengths
greater than the model maximum admissible input size).</li>
</ul>`,name:"truncation"},{anchor:"transformers.DPRReaderTokenizerFast.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Controls the maximum length to use by one of the truncation/padding parameters.</p>
<p>If left unset or set to <code>None</code>, this will use the predefined model maximum length if a maximum length
is required by one of the truncation/padding parameters. If the model has no specific maximum input
length (like XLNet) truncation/padding to a maximum length will be deactivated.`,name:"max_length"},{anchor:"transformers.DPRReaderTokenizerFast.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DPRReaderTokenizerFast.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attention mask. If not set, will return the attention mask according to the
specific tokenizer&#x2019;s default, defined by the <code>return_outputs</code> attribute.</p>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"return_attention_mask"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/tokenization_dpr_fast.py#L392",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A dictionary with the following keys:</p>
<ul>
<li><code>input_ids</code>: List of token ids to be fed to a model.</li>
<li><code>attention_mask</code>: List of indices specifying which tokens should be attended to by the model.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Dict[str, List[List[int]]]</code></p>
`}}),kt=new Re({props:{title:"DPR specific outputs",local:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",headingTag:"h2"}}),xt=new Y({props:{name:"class transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": Optional = None"},{name:"attentions",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the context representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L61"}}),Pt=new Y({props:{name:"class transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput",parameters:[{name:"pooler_output",val:": FloatTensor"},{name:"hidden_states",val:": Optional = None"},{name:"attentions",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, embeddings_size)</code>) &#x2014;
The DPR encoder outputs the <em>pooler_output</em> that corresponds to the question representation. Last layer
hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.
This output is to be used to embed questions for nearest neighbors queries with context embeddings.`,name:"pooler_output"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.dpr.modeling_dpr.DPRQuestionEncoderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L89"}}),Mt=new Y({props:{name:"class transformers.DPRReaderOutput",anchor:"transformers.DPRReaderOutput",parameters:[{name:"start_logits",val:": FloatTensor"},{name:"end_logits",val:": FloatTensor = None"},{name:"relevance_logits",val:": FloatTensor = None"},{name:"hidden_states",val:": Optional = None"},{name:"attentions",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.DPRReaderOutput.start_logits",description:`<strong>start_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the start index of the span for each passage.`,name:"start_logits"},{anchor:"transformers.DPRReaderOutput.end_logits",description:`<strong>end_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, sequence_length)</code>) &#x2014;
Logits of the end index of the span for each passage.`,name:"end_logits"},{anchor:"transformers.DPRReaderOutput.relevance_logits",description:`<strong>relevance_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(n_passages, )</code>) &#x2014;
Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the
question, compared to all the other passages.`,name:"relevance_logits"},{anchor:"transformers.DPRReaderOutput.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.DPRReaderOutput.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dpr/modeling_dpr.py#L117"}}),ut=new Qn({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[po],pytorch:[Kn]},$$scope:{ctx:D}}}),{c(){t=c("meta"),g=r(),n=c("p"),d=r(),v(_.$$.fragment),o=r(),T=c("div"),T.innerHTML=ie,q=r(),v(C.$$.fragment),K=r(),z=c("p"),z.innerHTML=E,ee=r(),m=c("p"),m.textContent=P,te=r(),S=c("p"),S.innerHTML=at,ze=r(),de=c("p"),de.innerHTML=ot,X=r(),v(ke.$$.fragment),ne=r(),be=c("ul"),be.innerHTML=U,ve=r(),v(L.$$.fragment),le=r(),O=c("div"),v(pe.$$.fragment),We=r(),Be=c("p"),Be.innerHTML=Ge,Se=r(),Xe=c("p"),Xe.innerHTML=Oe,ce=r(),me=c("p"),me.innerHTML=Fe,je=r(),v(Q.$$.fragment),xe=r(),v(se.$$.fragment),qe=r(),j=c("div"),v(Pe.$$.fragment),re=r(),Ee=c("p"),Ee.textContent=J,Me=r(),ue=c("p"),ue.innerHTML=He,V=r(),fe=c("p"),fe.innerHTML=Ae,Ie=r(),v(Ze.$$.fragment),Le=r(),G=c("div"),v(Ne.$$.fragment),ae=r(),oe=c("p"),oe.innerHTML=it,Qe=r(),he=c("p"),he.innerHTML=st,H=r(),ge=c("p"),ge.innerHTML=Je,De=r(),v(h.$$.fragment),M=r(),F=c("div"),v(I.$$.fragment),W=r(),Z=c("p"),Z.textContent=u,x=r(),_e=c("p"),_e.innerHTML=Te,Ue=r(),A=c("p"),A.innerHTML=Ve,Tt=r(),v(rt.$$.fragment),bt=r(),Ce=c("div"),v(vt.$$.fragment),rn=r(),Dt=c("p"),Dt.innerHTML=yn,an=r(),Ct=c("p"),Ct.innerHTML=Rn,dn=r(),zt=c("p"),zt.innerHTML=wn,Wt=r(),v($t.$$.fragment),Bt=r(),$e=c("div"),v(yt.$$.fragment),ln=r(),Ft=c("p"),Ft.textContent=kn,cn=r(),qt=c("p"),qt.innerHTML=xn,pn=r(),Et=c("p"),Et.innerHTML=Pn,mn=r(),Lt=c("p"),Lt.innerHTML=Mn,un=r(),v(mt.$$.fragment),Gt=r(),v(Rt.$$.fragment),St=r(),ye=c("div"),v(wt.$$.fragment),fn=r(),Jt=c("p"),Jt.innerHTML=Dn,hn=r(),Ut=c("p"),Ut.innerHTML=Cn,gn=r(),jt=c("p"),jt.innerHTML=zn,_n=r(),Ht=c("p"),Ht.innerHTML=Fn,Tn=r(),It=c("p"),It.textContent=qn,Xt=r(),v(kt.$$.fragment),Ot=r(),dt=c("div"),v(xt.$$.fragment),bn=r(),Zt=c("p"),Zt.innerHTML=En,At=r(),lt=c("div"),v(Pt.$$.fragment),vn=r(),Nt=c("p"),Nt.innerHTML=Ln,Yt=r(),ct=c("div"),v(Mt.$$.fragment),$n=r(),Qt=c("p"),Qt.innerHTML=Jn,Kt=r(),v(ut.$$.fragment),en=r(),Vt=c("p"),this.h()},l(e){const b=Nn("svelte-u9bgzb",document.head);t=p(b,"META",{name:!0,content:!0}),b.forEach(s),g=a(e),n=p(e,"P",{}),B(n).forEach(s),d=a(e),$(_.$$.fragment,e),o=a(e),T=p(e,"DIV",{class:!0,"data-svelte-h":!0}),f(T)!=="svelte-162ccfz"&&(T.innerHTML=ie),q=a(e),$(C.$$.fragment,e),K=a(e),z=p(e,"P",{"data-svelte-h":!0}),f(z)!=="svelte-4gki3s"&&(z.innerHTML=E),ee=a(e),m=p(e,"P",{"data-svelte-h":!0}),f(m)!=="svelte-vfdo9a"&&(m.textContent=P),te=a(e),S=p(e,"P",{"data-svelte-h":!0}),f(S)!=="svelte-1xle5de"&&(S.innerHTML=at),ze=a(e),de=p(e,"P",{"data-svelte-h":!0}),f(de)!=="svelte-21k9xm"&&(de.innerHTML=ot),X=a(e),$(ke.$$.fragment,e),ne=a(e),be=p(e,"UL",{"data-svelte-h":!0}),f(be)!=="svelte-zqyul3"&&(be.innerHTML=U),ve=a(e),$(L.$$.fragment,e),le=a(e),O=p(e,"DIV",{class:!0});var Ye=B(O);$(pe.$$.fragment,Ye),We=a(Ye),Be=p(Ye,"P",{"data-svelte-h":!0}),f(Be)!=="svelte-ctq7b7"&&(Be.innerHTML=Ge),Se=a(Ye),Xe=p(Ye,"P",{"data-svelte-h":!0}),f(Xe)!=="svelte-1d1n2m4"&&(Xe.innerHTML=Oe),ce=a(Ye),me=p(Ye,"P",{"data-svelte-h":!0}),f(me)!=="svelte-1pbgxbb"&&(me.innerHTML=Fe),je=a(Ye),$(Q.$$.fragment,Ye),Ye.forEach(s),xe=a(e),$(se.$$.fragment,e),qe=a(e),j=p(e,"DIV",{class:!0});var tt=B(j);$(Pe.$$.fragment,tt),re=a(tt),Ee=p(tt,"P",{"data-svelte-h":!0}),f(Ee)!=="svelte-1dv8gby"&&(Ee.textContent=J),Me=a(tt),ue=p(tt,"P",{"data-svelte-h":!0}),f(ue)!=="svelte-1t625e3"&&(ue.innerHTML=He),V=a(tt),fe=p(tt,"P",{"data-svelte-h":!0}),f(fe)!=="svelte-12xl00s"&&(fe.innerHTML=Ae),tt.forEach(s),Ie=a(e),$(Ze.$$.fragment,e),Le=a(e),G=p(e,"DIV",{class:!0});var nt=B(G);$(Ne.$$.fragment,nt),ae=a(nt),oe=p(nt,"P",{"data-svelte-h":!0}),f(oe)!=="svelte-1flugr"&&(oe.innerHTML=it),Qe=a(nt),he=p(nt,"P",{"data-svelte-h":!0}),f(he)!=="svelte-4banv3"&&(he.innerHTML=st),H=a(nt),ge=p(nt,"P",{"data-svelte-h":!0}),f(ge)!=="svelte-x4qlyg"&&(ge.innerHTML=Je),nt.forEach(s),De=a(e),$(h.$$.fragment,e),M=a(e),F=p(e,"DIV",{class:!0});var ft=B(F);$(I.$$.fragment,ft),W=a(ft),Z=p(ft,"P",{"data-svelte-h":!0}),f(Z)!=="svelte-1fcy66e"&&(Z.textContent=u),x=a(ft),_e=p(ft,"P",{"data-svelte-h":!0}),f(_e)!=="svelte-10m291f"&&(_e.innerHTML=Te),Ue=a(ft),A=p(ft,"P",{"data-svelte-h":!0}),f(A)!=="svelte-12xl00s"&&(A.innerHTML=Ve),ft.forEach(s),Tt=a(e),$(rt.$$.fragment,e),bt=a(e),Ce=p(e,"DIV",{class:!0});var ht=B(Ce);$(vt.$$.fragment,ht),rn=a(ht),Dt=p(ht,"P",{"data-svelte-h":!0}),f(Dt)!=="svelte-137cq53"&&(Dt.innerHTML=yn),an=a(ht),Ct=p(ht,"P",{"data-svelte-h":!0}),f(Ct)!=="svelte-1dqy07b"&&(Ct.innerHTML=Rn),dn=a(ht),zt=p(ht,"P",{"data-svelte-h":!0}),f(zt)!=="svelte-x4qlyg"&&(zt.innerHTML=wn),ht.forEach(s),Wt=a(e),$($t.$$.fragment,e),Bt=a(e),$e=p(e,"DIV",{class:!0});var Ke=B($e);$(yt.$$.fragment,Ke),ln=a(Ke),Ft=p(Ke,"P",{"data-svelte-h":!0}),f(Ft)!=="svelte-7on9jw"&&(Ft.textContent=kn),cn=a(Ke),qt=p(Ke,"P",{"data-svelte-h":!0}),f(qt)!=="svelte-1a4crd8"&&(qt.innerHTML=xn),pn=a(Ke),Et=p(Ke,"P",{"data-svelte-h":!0}),f(Et)!=="svelte-12xl00s"&&(Et.innerHTML=Pn),mn=a(Ke),Lt=p(Ke,"P",{"data-svelte-h":!0}),f(Lt)!=="svelte-q0w208"&&(Lt.innerHTML=Mn),un=a(Ke),$(mt.$$.fragment,Ke),Ke.forEach(s),Gt=a(e),$(Rt.$$.fragment,e),St=a(e),ye=p(e,"DIV",{class:!0});var et=B(ye);$(wt.$$.fragment,et),fn=a(et),Jt=p(et,"P",{"data-svelte-h":!0}),f(Jt)!=="svelte-els1hy"&&(Jt.innerHTML=Dn),hn=a(et),Ut=p(et,"P",{"data-svelte-h":!0}),f(Ut)!=="svelte-16jfvek"&&(Ut.innerHTML=Cn),gn=a(et),jt=p(et,"P",{"data-svelte-h":!0}),f(jt)!=="svelte-x4qlyg"&&(jt.innerHTML=zn),_n=a(et),Ht=p(et,"P",{"data-svelte-h":!0}),f(Ht)!=="svelte-1ov4u9q"&&(Ht.innerHTML=Fn),Tn=a(et),It=p(et,"P",{"data-svelte-h":!0}),f(It)!=="svelte-kc4w4c"&&(It.textContent=qn),et.forEach(s),Xt=a(e),$(kt.$$.fragment,e),Ot=a(e),dt=p(e,"DIV",{class:!0});var nn=B(dt);$(xt.$$.fragment,nn),bn=a(nn),Zt=p(nn,"P",{"data-svelte-h":!0}),f(Zt)!=="svelte-1tj7skp"&&(Zt.innerHTML=En),nn.forEach(s),At=a(e),lt=p(e,"DIV",{class:!0});var on=B(lt);$(Pt.$$.fragment,on),vn=a(on),Nt=p(on,"P",{"data-svelte-h":!0}),f(Nt)!=="svelte-1tj7skp"&&(Nt.innerHTML=Ln),on.forEach(s),Yt=a(e),ct=p(e,"DIV",{class:!0});var sn=B(ct);$(Mt.$$.fragment,sn),$n=a(sn),Qt=p(sn,"P",{"data-svelte-h":!0}),f(Qt)!=="svelte-1tj7skp"&&(Qt.innerHTML=Jn),sn.forEach(s),Kt=a(e),$(ut.$$.fragment,e),en=a(e),Vt=p(e,"P",{}),B(Vt).forEach(s),this.h()},h(){N(t,"name","hf:doc:metadata"),N(t,"content",uo),N(T,"class","flex flex-wrap space-x-1"),N(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),N(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,b){i(document.head,t),l(e,g,b),l(e,n,b),l(e,d,b),y(_,e,b),l(e,o,b),l(e,T,b),l(e,q,b),y(C,e,b),l(e,K,b),l(e,z,b),l(e,ee,b),l(e,m,b),l(e,te,b),l(e,S,b),l(e,ze,b),l(e,de,b),l(e,X,b),y(ke,e,b),l(e,ne,b),l(e,be,b),l(e,ve,b),y(L,e,b),l(e,le,b),l(e,O,b),y(pe,O,null),i(O,We),i(O,Be),i(O,Se),i(O,Xe),i(O,ce),i(O,me),i(O,je),y(Q,O,null),l(e,xe,b),y(se,e,b),l(e,qe,b),l(e,j,b),y(Pe,j,null),i(j,re),i(j,Ee),i(j,Me),i(j,ue),i(j,V),i(j,fe),l(e,Ie,b),y(Ze,e,b),l(e,Le,b),l(e,G,b),y(Ne,G,null),i(G,ae),i(G,oe),i(G,Qe),i(G,he),i(G,H),i(G,ge),l(e,De,b),y(h,e,b),l(e,M,b),l(e,F,b),y(I,F,null),i(F,W),i(F,Z),i(F,x),i(F,_e),i(F,Ue),i(F,A),l(e,Tt,b),y(rt,e,b),l(e,bt,b),l(e,Ce,b),y(vt,Ce,null),i(Ce,rn),i(Ce,Dt),i(Ce,an),i(Ce,Ct),i(Ce,dn),i(Ce,zt),l(e,Wt,b),y($t,e,b),l(e,Bt,b),l(e,$e,b),y(yt,$e,null),i($e,ln),i($e,Ft),i($e,cn),i($e,qt),i($e,pn),i($e,Et),i($e,mn),i($e,Lt),i($e,un),y(mt,$e,null),l(e,Gt,b),y(Rt,e,b),l(e,St,b),l(e,ye,b),y(wt,ye,null),i(ye,fn),i(ye,Jt),i(ye,hn),i(ye,Ut),i(ye,gn),i(ye,jt),i(ye,_n),i(ye,Ht),i(ye,Tn),i(ye,It),l(e,Xt,b),y(kt,e,b),l(e,Ot,b),l(e,dt,b),y(xt,dt,null),i(dt,bn),i(dt,Zt),l(e,At,b),l(e,lt,b),y(Pt,lt,null),i(lt,vn),i(lt,Nt),l(e,Yt,b),l(e,ct,b),y(Mt,ct,null),i(ct,$n),i(ct,Qt),l(e,Kt,b),y(ut,e,b),l(e,en,b),l(e,Vt,b),tn=!0},p(e,[b]){const Ye={};b&2&&(Ye.$$scope={dirty:b,ctx:e}),Q.$set(Ye);const tt={};b&2&&(tt.$$scope={dirty:b,ctx:e}),mt.$set(tt);const nt={};b&2&&(nt.$$scope={dirty:b,ctx:e}),ut.$set(nt)},i(e){tn||(R(_.$$.fragment,e),R(C.$$.fragment,e),R(ke.$$.fragment,e),R(L.$$.fragment,e),R(pe.$$.fragment,e),R(Q.$$.fragment,e),R(se.$$.fragment,e),R(Pe.$$.fragment,e),R(Ze.$$.fragment,e),R(Ne.$$.fragment,e),R(h.$$.fragment,e),R(I.$$.fragment,e),R(rt.$$.fragment,e),R(vt.$$.fragment,e),R($t.$$.fragment,e),R(yt.$$.fragment,e),R(mt.$$.fragment,e),R(Rt.$$.fragment,e),R(wt.$$.fragment,e),R(kt.$$.fragment,e),R(xt.$$.fragment,e),R(Pt.$$.fragment,e),R(Mt.$$.fragment,e),R(ut.$$.fragment,e),tn=!0)},o(e){w(_.$$.fragment,e),w(C.$$.fragment,e),w(ke.$$.fragment,e),w(L.$$.fragment,e),w(pe.$$.fragment,e),w(Q.$$.fragment,e),w(se.$$.fragment,e),w(Pe.$$.fragment,e),w(Ze.$$.fragment,e),w(Ne.$$.fragment,e),w(h.$$.fragment,e),w(I.$$.fragment,e),w(rt.$$.fragment,e),w(vt.$$.fragment,e),w($t.$$.fragment,e),w(yt.$$.fragment,e),w(mt.$$.fragment,e),w(Rt.$$.fragment,e),w(wt.$$.fragment,e),w(kt.$$.fragment,e),w(xt.$$.fragment,e),w(Pt.$$.fragment,e),w(Mt.$$.fragment,e),w(ut.$$.fragment,e),tn=!1},d(e){e&&(s(g),s(n),s(d),s(o),s(T),s(q),s(K),s(z),s(ee),s(m),s(te),s(S),s(ze),s(de),s(X),s(ne),s(be),s(ve),s(le),s(O),s(xe),s(qe),s(j),s(Ie),s(Le),s(G),s(De),s(M),s(F),s(Tt),s(bt),s(Ce),s(Wt),s(Bt),s($e),s(Gt),s(St),s(ye),s(Xt),s(Ot),s(dt),s(At),s(lt),s(Yt),s(ct),s(Kt),s(en),s(Vt)),s(t),k(_,e),k(C,e),k(ke,e),k(L,e),k(pe),k(Q),k(se,e),k(Pe),k(Ze,e),k(Ne),k(h,e),k(I),k(rt,e),k(vt),k($t,e),k(yt),k(mt),k(Rt,e),k(wt),k(kt,e),k(xt),k(Pt),k(Mt),k(ut,e)}}}const uo='{"title":"DPR","local":"dpr","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"DPRConfig","local":"transformers.DPRConfig","sections":[],"depth":2},{"title":"DPRContextEncoderTokenizer","local":"transformers.DPRContextEncoderTokenizer","sections":[],"depth":2},{"title":"DPRContextEncoderTokenizerFast","local":"transformers.DPRContextEncoderTokenizerFast","sections":[],"depth":2},{"title":"DPRQuestionEncoderTokenizer","local":"transformers.DPRQuestionEncoderTokenizer","sections":[],"depth":2},{"title":"DPRQuestionEncoderTokenizerFast","local":"transformers.DPRQuestionEncoderTokenizerFast","sections":[],"depth":2},{"title":"DPRReaderTokenizer","local":"transformers.DPRReaderTokenizer","sections":[],"depth":2},{"title":"DPRReaderTokenizerFast","local":"transformers.DPRReaderTokenizerFast","sections":[],"depth":2},{"title":"DPR specific outputs","local":"transformers.models.dpr.modeling_dpr.DPRContextEncoderOutput","sections":[],"depth":2},{"title":"DPRContextEncoder","local":"transformers.DPRContextEncoder","sections":[],"depth":2},{"title":"DPRQuestionEncoder","local":"transformers.DPRQuestionEncoder","sections":[],"depth":2},{"title":"DPRReader","local":"transformers.DPRReader","sections":[],"depth":2},{"title":"TFDPRContextEncoder","local":"transformers.TFDPRContextEncoder","sections":[],"depth":2},{"title":"TFDPRQuestionEncoder","local":"transformers.TFDPRQuestionEncoder","sections":[],"depth":2},{"title":"TFDPRReader","local":"transformers.TFDPRReader","sections":[],"depth":2}],"depth":1}';function fo(D){return Hn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ro extends In{constructor(t){super(),Zn(this,t,fo,mo,jn,{})}}export{Ro as component};
