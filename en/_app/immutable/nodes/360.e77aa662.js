import{s as _l,o as Bl,n as ss}from"../chunks/scheduler.9bc65507.js";import{S as vl,i as Gl,g as M,s as i,r as J,A as Nl,h,f as e,c as r,j as ke,u as g,x as T,k as ys,y as js,a,v as y,d as j,t as d,w as u,m as Yl,n as Rl}from"../chunks/index.707bf1b6.js";import{T as Sa}from"../chunks/Tip.c2ecdbf4.js";import{Y as El}from"../chunks/Youtube.e1129c6f.js";import{C as k}from"../chunks/CodeBlock.54a9f38d.js";import{D as Xl}from"../chunks/DocNotebookDropdown.41f65cb5.js";import{F as ds,M as us}from"../chunks/Markdown.fef84341.js";import{H as O}from"../chunks/Heading.342b1fa6.js";function Ql(x){let n,o,t='<a href="../model_doc/beit">BEiT</a>, <a href="../model_doc/data2vec-vision">Data2VecVision</a>, <a href="../model_doc/dpt">DPT</a>, <a href="../model_doc/mobilenet_v2">MobileNetV2</a>, <a href="../model_doc/mobilevit">MobileViT</a>, <a href="../model_doc/mobilevitv2">MobileViTV2</a>, <a href="../model_doc/segformer">SegFormer</a>, <a href="../model_doc/upernet">UPerNet</a>';return{c(){n=Yl(`The task illustrated in this tutorial is supported by the following model architectures:

`),o=M("p"),o.innerHTML=t},l(m){n=Rl(m,`The task illustrated in this tutorial is supported by the following model architectures:

`),o=h(m,"P",{"data-svelte-h":!0}),T(o)!=="svelte-19l83ao"&&(o.innerHTML=t)},m(m,f){a(m,n,f),a(m,o,f)},p:ss,d(m){m&&(e(n),e(o))}}}function Fl(x){let n,o='It is common to apply some data augmentations to an image dataset to make a model more robust against overfitting. In this guide, youâ€™ll use the <a href="https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html" rel="nofollow"><code>ColorJitter</code></a> function from <a href="https://pytorch.org/vision/stable/index.html" rel="nofollow">torchvision</a> to randomly change the color properties of an image, but you can also use any image library you like.',t,m,f,C,v="Now create two preprocessing functions to prepare the images and annotations for the model. These functions convert the images into <code>pixel_values</code> and annotations to <code>labels</code>. For the training set, <code>jitter</code> is applied before providing the images to the image processor. For the test set, the image processor crops and normalizes the <code>images</code>, and only crops the <code>labels</code> because no data augmentation is applied during testing.",Z,$,N,W,Y='To apply the <code>jitter</code> over the entire dataset, use the ðŸ¤— Datasets <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.set_transform" rel="nofollow">set_transform</a> function. The transform is applied on the fly which is faster and consumes less disk space:',_,I,E;return m=new k({props:{code:"ZnJvbSUyMHRvcmNodmlzaW9uLnRyYW5zZm9ybXMlMjBpbXBvcnQlMjBDb2xvckppdHRlciUwQSUwQWppdHRlciUyMCUzRCUyMENvbG9ySml0dGVyKGJyaWdodG5lc3MlM0QwLjI1JTJDJTIwY29udHJhc3QlM0QwLjI1JTJDJTIwc2F0dXJhdGlvbiUzRDAuMjUlMkMlMjBodWUlM0QwLjEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ColorJitter

<span class="hljs-meta">&gt;&gt;&gt; </span>jitter = ColorJitter(brightness=<span class="hljs-number">0.25</span>, contrast=<span class="hljs-number">0.25</span>, saturation=<span class="hljs-number">0.25</span>, hue=<span class="hljs-number">0.1</span>)`,wrap:!1}}),$=new k({props:{code:"ZGVmJTIwdHJhaW5fdHJhbnNmb3JtcyhleGFtcGxlX2JhdGNoKSUzQSUwQSUyMCUyMCUyMCUyMGltYWdlcyUyMCUzRCUyMCU1QmppdHRlcih4KSUyMGZvciUyMHglMjBpbiUyMGV4YW1wbGVfYmF0Y2glNUIlMjJpbWFnZSUyMiU1RCU1RCUwQSUyMCUyMCUyMCUyMGxhYmVscyUyMCUzRCUyMCU1QnglMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlX2JhdGNoJTVCJTIyYW5ub3RhdGlvbiUyMiU1RCU1RCUwQSUyMCUyMCUyMCUyMGlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZXMlMkMlMjBsYWJlbHMpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwaW5wdXRzJTBBJTBBJTBBZGVmJTIwdmFsX3RyYW5zZm9ybXMoZXhhbXBsZV9iYXRjaCklM0ElMEElMjAlMjAlMjAlMjBpbWFnZXMlMjAlM0QlMjAlNUJ4JTIwZm9yJTIweCUyMGluJTIwZXhhbXBsZV9iYXRjaCU1QiUyMmltYWdlJTIyJTVEJTVEJTBBJTIwJTIwJTIwJTIwbGFiZWxzJTIwJTNEJTIwJTVCeCUyMGZvciUyMHglMjBpbiUyMGV4YW1wbGVfYmF0Y2glNUIlMjJhbm5vdGF0aW9uJTIyJTVEJTVEJTBBJTIwJTIwJTIwJTIwaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUyQyUyMGxhYmVscyklMEElMjAlMjAlMjAlMjByZXR1cm4lMjBpbnB1dHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_transforms</span>(<span class="hljs-params">example_batch</span>):
<span class="hljs-meta">... </span>    images = [jitter(x) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    labels = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;annotation&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = image_processor(images, labels)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">val_transforms</span>(<span class="hljs-params">example_batch</span>):
<span class="hljs-meta">... </span>    images = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    labels = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;annotation&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = image_processor(images, labels)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`,wrap:!1}}),I=new k({props:{code:"dHJhaW5fZHMuc2V0X3RyYW5zZm9ybSh0cmFpbl90cmFuc2Zvcm1zKSUwQXRlc3RfZHMuc2V0X3RyYW5zZm9ybSh2YWxfdHJhbnNmb3Jtcyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_ds.set_transform(train_transforms)
<span class="hljs-meta">&gt;&gt;&gt; </span>test_ds.set_transform(val_transforms)`,wrap:!1}}),{c(){n=M("p"),n.innerHTML=o,t=i(),J(m.$$.fragment),f=i(),C=M("p"),C.innerHTML=v,Z=i(),J($.$$.fragment),N=i(),W=M("p"),W.innerHTML=Y,_=i(),J(I.$$.fragment)},l(p){n=h(p,"P",{"data-svelte-h":!0}),T(n)!=="svelte-43dxhy"&&(n.innerHTML=o),t=r(p),g(m.$$.fragment,p),f=r(p),C=h(p,"P",{"data-svelte-h":!0}),T(C)!=="svelte-qerdt3"&&(C.innerHTML=v),Z=r(p),g($.$$.fragment,p),N=r(p),W=h(p,"P",{"data-svelte-h":!0}),T(W)!=="svelte-tijv00"&&(W.innerHTML=Y),_=r(p),g(I.$$.fragment,p)},m(p,w){a(p,n,w),a(p,t,w),y(m,p,w),a(p,f,w),a(p,C,w),a(p,Z,w),y($,p,w),a(p,N,w),a(p,W,w),a(p,_,w),y(I,p,w),E=!0},p:ss,i(p){E||(j(m.$$.fragment,p),j($.$$.fragment,p),j(I.$$.fragment,p),E=!0)},o(p){d(m.$$.fragment,p),d($.$$.fragment,p),d(I.$$.fragment,p),E=!1},d(p){p&&(e(n),e(t),e(f),e(C),e(Z),e(N),e(W),e(_)),u(m,p),u($,p),u(I,p)}}}function zl(x){let n,o;return n=new us({props:{$$slots:{default:[Fl]},$$scope:{ctx:x}}}),{c(){J(n.$$.fragment)},l(t){g(n.$$.fragment,t)},m(t,m){y(n,t,m),o=!0},p(t,m){const f={};m&2&&(f.$$scope={dirty:m,ctx:t}),n.$set(f)},i(t){o||(j(n.$$.fragment,t),o=!0)},o(t){d(n.$$.fragment,t),o=!1},d(t){u(n,t)}}}function Vl(x){let n,o=`It is common to apply some data augmentations to an image dataset to make a model more robust against overfitting.
In this guide, youâ€™ll use <a href="https://www.tensorflow.org/api_docs/python/tf/image" rel="nofollow"><code>tf.image</code></a> to randomly change the color properties of an image, but you can also use any image
library you like.
Define two separate transformation functions:`,t,m,f="<li>training data transformations that include image augmentation</li> <li>validation data transformations that only transpose the images, since computer vision models in ðŸ¤— Transformers expect channels-first layout</li>",C,v,Z,$,N=`Next, create two preprocessing functions to prepare batches of images and annotations for the model. These functions apply
the image transformations and use the earlier loaded <code>image_processor</code> to convert the images into <code>pixel_values</code> and
annotations to <code>labels</code>. <code>ImageProcessor</code> also takes care of resizing and normalizing the images.`,W,Y,_,I,E=`To apply the preprocessing transformations over the entire dataset, use the ðŸ¤— Datasets <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.set_transform" rel="nofollow">set_transform</a> function.
The transform is applied on the fly which is faster and consumes less disk space:`,p,w,X;return v=new k({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEElMEFkZWYlMjBhdWdfdHJhbnNmb3JtcyhpbWFnZSklM0ElMEElMjAlMjAlMjAlMjBpbWFnZSUyMCUzRCUyMHRmLmtlcmFzLnV0aWxzLmltZ190b19hcnJheShpbWFnZSklMEElMjAlMjAlMjAlMjBpbWFnZSUyMCUzRCUyMHRmLmltYWdlLnJhbmRvbV9icmlnaHRuZXNzKGltYWdlJTJDJTIwMC4yNSklMEElMjAlMjAlMjAlMjBpbWFnZSUyMCUzRCUyMHRmLmltYWdlLnJhbmRvbV9jb250cmFzdChpbWFnZSUyQyUyMDAuNSUyQyUyMDIuMCklMEElMjAlMjAlMjAlMjBpbWFnZSUyMCUzRCUyMHRmLmltYWdlLnJhbmRvbV9zYXR1cmF0aW9uKGltYWdlJTJDJTIwMC43NSUyQyUyMDEuMjUpJTBBJTIwJTIwJTIwJTIwaW1hZ2UlMjAlM0QlMjB0Zi5pbWFnZS5yYW5kb21faHVlKGltYWdlJTJDJTIwMC4xKSUwQSUyMCUyMCUyMCUyMGltYWdlJTIwJTNEJTIwdGYudHJhbnNwb3NlKGltYWdlJTJDJTIwKDIlMkMlMjAwJTJDJTIwMSkpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwaW1hZ2UlMEElMEElMEFkZWYlMjB0cmFuc2Zvcm1zKGltYWdlKSUzQSUwQSUyMCUyMCUyMCUyMGltYWdlJTIwJTNEJTIwdGYua2VyYXMudXRpbHMuaW1nX3RvX2FycmF5KGltYWdlKSUwQSUyMCUyMCUyMCUyMGltYWdlJTIwJTNEJTIwdGYudHJhbnNwb3NlKGltYWdlJTJDJTIwKDIlMkMlMjAwJTJDJTIwMSkpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwaW1hZ2U=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">aug_transforms</span>(<span class="hljs-params">image</span>):
<span class="hljs-meta">... </span>    image = tf.keras.utils.img_to_array(image)
<span class="hljs-meta">... </span>    image = tf.image.random_brightness(image, <span class="hljs-number">0.25</span>)
<span class="hljs-meta">... </span>    image = tf.image.random_contrast(image, <span class="hljs-number">0.5</span>, <span class="hljs-number">2.0</span>)
<span class="hljs-meta">... </span>    image = tf.image.random_saturation(image, <span class="hljs-number">0.75</span>, <span class="hljs-number">1.25</span>)
<span class="hljs-meta">... </span>    image = tf.image.random_hue(image, <span class="hljs-number">0.1</span>)
<span class="hljs-meta">... </span>    image = tf.transpose(image, (<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> image


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">image</span>):
<span class="hljs-meta">... </span>    image = tf.keras.utils.img_to_array(image)
<span class="hljs-meta">... </span>    image = tf.transpose(image, (<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>))
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> image`,wrap:!1}}),Y=new k({props:{code:"ZGVmJTIwdHJhaW5fdHJhbnNmb3JtcyhleGFtcGxlX2JhdGNoKSUzQSUwQSUyMCUyMCUyMCUyMGltYWdlcyUyMCUzRCUyMCU1QmF1Z190cmFuc2Zvcm1zKHguY29udmVydCglMjJSR0IlMjIpKSUyMGZvciUyMHglMjBpbiUyMGV4YW1wbGVfYmF0Y2glNUIlMjJpbWFnZSUyMiU1RCU1RCUwQSUyMCUyMCUyMCUyMGxhYmVscyUyMCUzRCUyMCU1QnglMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlX2JhdGNoJTVCJTIyYW5ub3RhdGlvbiUyMiU1RCU1RCUwQSUyMCUyMCUyMCUyMGlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZXMlMkMlMjBsYWJlbHMpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwaW5wdXRzJTBBJTBBJTBBZGVmJTIwdmFsX3RyYW5zZm9ybXMoZXhhbXBsZV9iYXRjaCklM0ElMEElMjAlMjAlMjAlMjBpbWFnZXMlMjAlM0QlMjAlNUJ0cmFuc2Zvcm1zKHguY29udmVydCglMjJSR0IlMjIpKSUyMGZvciUyMHglMjBpbiUyMGV4YW1wbGVfYmF0Y2glNUIlMjJpbWFnZSUyMiU1RCU1RCUwQSUyMCUyMCUyMCUyMGxhYmVscyUyMCUzRCUyMCU1QnglMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlX2JhdGNoJTVCJTIyYW5ub3RhdGlvbiUyMiU1RCU1RCUwQSUyMCUyMCUyMCUyMGlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZXMlMkMlMjBsYWJlbHMpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwaW5wdXRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">train_transforms</span>(<span class="hljs-params">example_batch</span>):
<span class="hljs-meta">... </span>    images = [aug_transforms(x.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    labels = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;annotation&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = image_processor(images, labels)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">val_transforms</span>(<span class="hljs-params">example_batch</span>):
<span class="hljs-meta">... </span>    images = [transforms(x.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    labels = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;annotation&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = image_processor(images, labels)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`,wrap:!1}}),w=new k({props:{code:"dHJhaW5fZHMuc2V0X3RyYW5zZm9ybSh0cmFpbl90cmFuc2Zvcm1zKSUwQXRlc3RfZHMuc2V0X3RyYW5zZm9ybSh2YWxfdHJhbnNmb3Jtcyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_ds.set_transform(train_transforms)
<span class="hljs-meta">&gt;&gt;&gt; </span>test_ds.set_transform(val_transforms)`,wrap:!1}}),{c(){n=M("p"),n.innerHTML=o,t=i(),m=M("ul"),m.innerHTML=f,C=i(),J(v.$$.fragment),Z=i(),$=M("p"),$.innerHTML=N,W=i(),J(Y.$$.fragment),_=i(),I=M("p"),I.innerHTML=E,p=i(),J(w.$$.fragment)},l(U){n=h(U,"P",{"data-svelte-h":!0}),T(n)!=="svelte-1rj6vla"&&(n.innerHTML=o),t=r(U),m=h(U,"UL",{"data-svelte-h":!0}),T(m)!=="svelte-lmmfid"&&(m.innerHTML=f),C=r(U),g(v.$$.fragment,U),Z=r(U),$=h(U,"P",{"data-svelte-h":!0}),T($)!=="svelte-hpi41l"&&($.innerHTML=N),W=r(U),g(Y.$$.fragment,U),_=r(U),I=h(U,"P",{"data-svelte-h":!0}),T(I)!=="svelte-19lox7v"&&(I.innerHTML=E),p=r(U),g(w.$$.fragment,U)},m(U,R){a(U,n,R),a(U,t,R),a(U,m,R),a(U,C,R),y(v,U,R),a(U,Z,R),a(U,$,R),a(U,W,R),y(Y,U,R),a(U,_,R),a(U,I,R),a(U,p,R),y(w,U,R),X=!0},p:ss,i(U){X||(j(v.$$.fragment,U),j(Y.$$.fragment,U),j(w.$$.fragment,U),X=!0)},o(U){d(v.$$.fragment,U),d(Y.$$.fragment,U),d(w.$$.fragment,U),X=!1},d(U){U&&(e(n),e(t),e(m),e(C),e(Z),e($),e(W),e(_),e(I),e(p)),u(v,U),u(Y,U),u(w,U)}}}function Sl(x){let n,o;return n=new us({props:{$$slots:{default:[Vl]},$$scope:{ctx:x}}}),{c(){J(n.$$.fragment)},l(t){g(n.$$.fragment,t)},m(t,m){y(n,t,m),o=!0},p(t,m){const f={};m&2&&(f.$$scope={dirty:m,ctx:t}),n.$set(f)},i(t){o||(j(n.$$.fragment,t),o=!0)},o(t){d(n.$$.fragment,t),o=!1},d(t){u(n,t)}}}function Al(x){let n,o;return n=new k({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdG9yY2glMjBpbXBvcnQlMjBubiUwQSUwQWRlZiUyMGNvbXB1dGVfbWV0cmljcyhldmFsX3ByZWQpJTNBJTBBJTIwJTIwJTIwJTIwd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGxvZ2l0cyUyQyUyMGxhYmVscyUyMCUzRCUyMGV2YWxfcHJlZCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGxvZ2l0c190ZW5zb3IlMjAlM0QlMjB0b3JjaC5mcm9tX251bXB5KGxvZ2l0cyklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBsb2dpdHNfdGVuc29yJTIwJTNEJTIwbm4uZnVuY3Rpb25hbC5pbnRlcnBvbGF0ZSglMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBsb2dpdHNfdGVuc29yJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwc2l6ZSUzRGxhYmVscy5zaGFwZSU1Qi0yJTNBJTVEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbW9kZSUzRCUyMmJpbGluZWFyJTIyJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYWxpZ25fY29ybmVycyUzREZhbHNlJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwKS5hcmdtYXgoZGltJTNEMSklMEElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBwcmVkX2xhYmVscyUyMCUzRCUyMGxvZ2l0c190ZW5zb3IuZGV0YWNoKCkuY3B1KCkubnVtcHkoKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMG1ldHJpY3MlMjAlM0QlMjBtZXRyaWMuY29tcHV0ZSglMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUzRHByZWRfbGFiZWxzJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcmVmZXJlbmNlcyUzRGxhYmVscyUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMG51bV9sYWJlbHMlM0RudW1fbGFiZWxzJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwaWdub3JlX2luZGV4JTNEMjU1JTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcmVkdWNlX2xhYmVscyUzREZhbHNlJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGZvciUyMGtleSUyQyUyMHZhbHVlJTIwaW4lMjBtZXRyaWNzLml0ZW1zKCklM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBpZiUyMGlzaW5zdGFuY2UodmFsdWUlMkMlMjBucC5uZGFycmF5KSUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMG1ldHJpY3MlNUJrZXklNUQlMjAlM0QlMjB2YWx1ZS50b2xpc3QoKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHJldHVybiUyMG1ldHJpY3M=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>        logits, labels = eval_pred
<span class="hljs-meta">... </span>        logits_tensor = torch.from_numpy(logits)
<span class="hljs-meta">... </span>        logits_tensor = nn.functional.interpolate(
<span class="hljs-meta">... </span>            logits_tensor,
<span class="hljs-meta">... </span>            size=labels.shape[-<span class="hljs-number">2</span>:],
<span class="hljs-meta">... </span>            mode=<span class="hljs-string">&quot;bilinear&quot;</span>,
<span class="hljs-meta">... </span>            align_corners=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>        ).argmax(dim=<span class="hljs-number">1</span>)

<span class="hljs-meta">... </span>        pred_labels = logits_tensor.detach().cpu().numpy()
<span class="hljs-meta">... </span>        metrics = metric.compute(
<span class="hljs-meta">... </span>            predictions=pred_labels,
<span class="hljs-meta">... </span>            references=labels,
<span class="hljs-meta">... </span>            num_labels=num_labels,
<span class="hljs-meta">... </span>            ignore_index=<span class="hljs-number">255</span>,
<span class="hljs-meta">... </span>            reduce_labels=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>        )
<span class="hljs-meta">... </span>        <span class="hljs-keyword">for</span> key, value <span class="hljs-keyword">in</span> metrics.items():
<span class="hljs-meta">... </span>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(value, np.ndarray):
<span class="hljs-meta">... </span>                metrics[key] = value.tolist()
<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> metrics`,wrap:!1}}),{c(){J(n.$$.fragment)},l(t){g(n.$$.fragment,t)},m(t,m){y(n,t,m),o=!0},p:ss,i(t){o||(j(n.$$.fragment,t),o=!0)},o(t){d(n.$$.fragment,t),o=!1},d(t){u(n,t)}}}function Hl(x){let n,o;return n=new us({props:{$$slots:{default:[Al]},$$scope:{ctx:x}}}),{c(){J(n.$$.fragment)},l(t){g(n.$$.fragment,t)},m(t,m){y(n,t,m),o=!0},p(t,m){const f={};m&2&&(f.$$scope={dirty:m,ctx:t}),n.$set(f)},i(t){o||(j(n.$$.fragment,t),o=!0)},o(t){d(n.$$.fragment,t),o=!1},d(t){u(n,t)}}}function Ll(x){let n,o;return n=new k({props:{code:"ZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMkMlMjBsYWJlbHMlMjAlM0QlMjBldmFsX3ByZWQlMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjB0Zi50cmFuc3Bvc2UobG9naXRzJTJDJTIwcGVybSUzRCU1QjAlMkMlMjAyJTJDJTIwMyUyQyUyMDElNUQpJTBBJTIwJTIwJTIwJTIwbG9naXRzX3Jlc2l6ZWQlMjAlM0QlMjB0Zi5pbWFnZS5yZXNpemUoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbG9naXRzJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwc2l6ZSUzRHRmLnNoYXBlKGxhYmVscyklNUIxJTNBJTVEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbWV0aG9kJTNEJTIyYmlsaW5lYXIlMjIlMkMlMEElMjAlMjAlMjAlMjApJTBBJTBBJTIwJTIwJTIwJTIwcHJlZF9sYWJlbHMlMjAlM0QlMjB0Zi5hcmdtYXgobG9naXRzX3Jlc2l6ZWQlMkMlMjBheGlzJTNELTEpJTBBJTIwJTIwJTIwJTIwbWV0cmljcyUyMCUzRCUyMG1ldHJpYy5jb21wdXRlKCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHByZWRpY3Rpb25zJTNEcHJlZF9sYWJlbHMlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjByZWZlcmVuY2VzJTNEbGFiZWxzJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbnVtX2xhYmVscyUzRG51bV9sYWJlbHMlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBpZ25vcmVfaW5kZXglM0QtMSUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHJlZHVjZV9sYWJlbHMlM0RpbWFnZV9wcm9jZXNzb3IuZG9fcmVkdWNlX2xhYmVscyUyQyUwQSUyMCUyMCUyMCUyMCklMEElMEElMjAlMjAlMjAlMjBwZXJfY2F0ZWdvcnlfYWNjdXJhY3klMjAlM0QlMjBtZXRyaWNzLnBvcCglMjJwZXJfY2F0ZWdvcnlfYWNjdXJhY3klMjIpLnRvbGlzdCgpJTBBJTIwJTIwJTIwJTIwcGVyX2NhdGVnb3J5X2lvdSUyMCUzRCUyMG1ldHJpY3MucG9wKCUyMnBlcl9jYXRlZ29yeV9pb3UlMjIpLnRvbGlzdCgpJTBBJTBBJTIwJTIwJTIwJTIwbWV0cmljcy51cGRhdGUoJTdCZiUyMmFjY3VyYWN5XyU3QmlkMmxhYmVsJTVCaSU1RCU3RCUyMiUzQSUyMHYlMjBmb3IlMjBpJTJDJTIwdiUyMGluJTIwZW51bWVyYXRlKHBlcl9jYXRlZ29yeV9hY2N1cmFjeSklN0QpJTBBJTIwJTIwJTIwJTIwbWV0cmljcy51cGRhdGUoJTdCZiUyMmlvdV8lN0JpZDJsYWJlbCU1QmklNUQlN0QlMjIlM0ElMjB2JTIwZm9yJTIwaSUyQyUyMHYlMjBpbiUyMGVudW1lcmF0ZShwZXJfY2F0ZWdvcnlfaW91KSU3RCklMEElMjAlMjAlMjAlMjByZXR1cm4lMjAlN0IlMjJ2YWxfJTIyJTIwJTJCJTIwayUzQSUyMHYlMjBmb3IlMjBrJTJDJTIwdiUyMGluJTIwbWV0cmljcy5pdGVtcygpJTdE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    logits, labels = eval_pred
<span class="hljs-meta">... </span>    logits = tf.transpose(logits, perm=[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>])
<span class="hljs-meta">... </span>    logits_resized = tf.image.resize(
<span class="hljs-meta">... </span>        logits,
<span class="hljs-meta">... </span>        size=tf.shape(labels)[<span class="hljs-number">1</span>:],
<span class="hljs-meta">... </span>        method=<span class="hljs-string">&quot;bilinear&quot;</span>,
<span class="hljs-meta">... </span>    )

<span class="hljs-meta">... </span>    pred_labels = tf.argmax(logits_resized, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    metrics = metric.compute(
<span class="hljs-meta">... </span>        predictions=pred_labels,
<span class="hljs-meta">... </span>        references=labels,
<span class="hljs-meta">... </span>        num_labels=num_labels,
<span class="hljs-meta">... </span>        ignore_index=-<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>        reduce_labels=image_processor.do_reduce_labels,
<span class="hljs-meta">... </span>    )

<span class="hljs-meta">... </span>    per_category_accuracy = metrics.pop(<span class="hljs-string">&quot;per_category_accuracy&quot;</span>).tolist()
<span class="hljs-meta">... </span>    per_category_iou = metrics.pop(<span class="hljs-string">&quot;per_category_iou&quot;</span>).tolist()

<span class="hljs-meta">... </span>    metrics.update({<span class="hljs-string">f&quot;accuracy_<span class="hljs-subst">{id2label[i]}</span>&quot;</span>: v <span class="hljs-keyword">for</span> i, v <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(per_category_accuracy)})
<span class="hljs-meta">... </span>    metrics.update({<span class="hljs-string">f&quot;iou_<span class="hljs-subst">{id2label[i]}</span>&quot;</span>: v <span class="hljs-keyword">for</span> i, v <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(per_category_iou)})
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;val_&quot;</span> + k: v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> metrics.items()}`,wrap:!1}}),{c(){J(n.$$.fragment)},l(t){g(n.$$.fragment,t)},m(t,m){y(n,t,m),o=!0},p:ss,i(t){o||(j(n.$$.fragment,t),o=!0)},o(t){d(n.$$.fragment,t),o=!1},d(t){u(n,t)}}}function Dl(x){let n,o;return n=new us({props:{$$slots:{default:[Ll]},$$scope:{ctx:x}}}),{c(){J(n.$$.fragment)},l(t){g(n.$$.fragment,t)},m(t,m){y(n,t,m),o=!0},p(t,m){const f={};m&2&&(f.$$scope={dirty:m,ctx:t}),n.$set(f)},i(t){o||(j(n.$$.fragment,t),o=!0)},o(t){d(n.$$.fragment,t),o=!1},d(t){u(n,t)}}}function ql(x){let n,o='If you arenâ€™t familiar with finetuning a model with the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a>, take a look at the basic tutorial <a href="../training#finetune-with-trainer">here</a>!';return{c(){n=M("p"),n.innerHTML=o},l(t){n=h(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-inlhq6"&&(n.innerHTML=o)},m(t,m){a(t,n,m)},p:ss,d(t){t&&e(n)}}}function Pl(x){let n,o,t,m='Youâ€™re ready to start training your model now! Load SegFormer with <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSemanticSegmentation">AutoModelForSemanticSegmentation</a>, and pass the model the mapping between label ids and label classes:',f,C,v,Z,$="At this point, only three steps remain:",N,W,Y='<li>Define your training hyperparameters in <a href="/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>. It is important you donâ€™t remove unused columns because thisâ€™ll drop the <code>image</code> column. Without the <code>image</code> column, you canâ€™t create <code>pixel_values</code>. Set <code>remove_unused_columns=False</code> to prevent this behavior! The only other required parameter is <code>output_dir</code> which specifies where to save your model. Youâ€™ll push this model to the Hub by setting <code>push_to_hub=True</code> (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> will evaluate the IoU metric and save the training checkpoint.</li> <li>Pass the training arguments to <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> along with the model, dataset, tokenizer, data collator, and <code>compute_metrics</code> function.</li> <li>Call <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train">train()</a> to finetune your model.</li>',_,I,E,p,w='Once training is completed, share your model to the Hub with the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub">push_to_hub()</a> method so everyone can use your model:',X,U,R;return n=new Sa({props:{$$slots:{default:[ql]},$$scope:{ctx:x}}}),C=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uJTJDJTIwVHJhaW5pbmdBcmd1bWVudHMlMkMlMjBUcmFpbmVyJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCUyQyUyMGlkMmxhYmVsJTNEaWQybGFiZWwlMkMlMjBsYWJlbDJpZCUzRGxhYmVsMmlkKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSemanticSegmentation, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)`,wrap:!1}}),I=new k({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJzZWdmb3JtZXItYjAtc2NlbmUtcGFyc2UtMTUwJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDZlLTUlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNENTAlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlM0QyJTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV9ldmFsX2JhdGNoX3NpemUlM0QyJTJDJTBBJTIwJTIwJTIwJTIwc2F2ZV90b3RhbF9saW1pdCUzRDMlMkMlMEElMjAlMjAlMjAlMjBldmFsdWF0aW9uX3N0cmF0ZWd5JTNEJTIyc3RlcHMlMjIlMkMlMEElMjAlMjAlMjAlMjBzYXZlX3N0cmF0ZWd5JTNEJTIyc3RlcHMlMjIlMkMlMEElMjAlMjAlMjAlMjBzYXZlX3N0ZXBzJTNEMjAlMkMlMEElMjAlMjAlMjAlMjBldmFsX3N0ZXBzJTNEMjAlMkMlMEElMjAlMjAlMjAlMjBsb2dnaW5nX3N0ZXBzJTNEMSUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfYWNjdW11bGF0aW9uX3N0ZXBzJTNENSUyQyUwQSUyMCUyMCUyMCUyMHJlbW92ZV91bnVzZWRfY29sdW1ucyUzREZhbHNlJTJDJTBBJTIwJTIwJTIwJTIwcHVzaF90b19odWIlM0RUcnVlJTJDJTBBKSUwQSUwQXRyYWluZXIlMjAlM0QlMjBUcmFpbmVyKCUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEbW9kZWwlMkMlMEElMjAlMjAlMjAlMjBhcmdzJTNEdHJhaW5pbmdfYXJncyUyQyUwQSUyMCUyMCUyMCUyMHRyYWluX2RhdGFzZXQlM0R0cmFpbl9kcyUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfZGF0YXNldCUzRHRlc3RfZHMlMkMlMEElMjAlMjAlMjAlMjBjb21wdXRlX21ldHJpY3MlM0Rjb21wdXRlX21ldHJpY3MlMkMlMEEpJTBBJTBBdHJhaW5lci50cmFpbigp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;segformer-b0-scene-parse-150&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">6e-5</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">50</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    save_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    save_steps=<span class="hljs-number">20</span>,
<span class="hljs-meta">... </span>    eval_steps=<span class="hljs-number">20</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">1</span>,
<span class="hljs-meta">... </span>    eval_accumulation_steps=<span class="hljs-number">5</span>,
<span class="hljs-meta">... </span>    remove_unused_columns=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=train_ds,
<span class="hljs-meta">... </span>    eval_dataset=test_ds,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),U=new k({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),{c(){J(n.$$.fragment),o=i(),t=M("p"),t.innerHTML=m,f=i(),J(C.$$.fragment),v=i(),Z=M("p"),Z.textContent=$,N=i(),W=M("ol"),W.innerHTML=Y,_=i(),J(I.$$.fragment),E=i(),p=M("p"),p.innerHTML=w,X=i(),J(U.$$.fragment)},l(b){g(n.$$.fragment,b),o=r(b),t=h(b,"P",{"data-svelte-h":!0}),T(t)!=="svelte-1bjkl0k"&&(t.innerHTML=m),f=r(b),g(C.$$.fragment,b),v=r(b),Z=h(b,"P",{"data-svelte-h":!0}),T(Z)!=="svelte-l42k0i"&&(Z.textContent=$),N=r(b),W=h(b,"OL",{"data-svelte-h":!0}),T(W)!=="svelte-6v7ckc"&&(W.innerHTML=Y),_=r(b),g(I.$$.fragment,b),E=r(b),p=h(b,"P",{"data-svelte-h":!0}),T(p)!=="svelte-1v13hlo"&&(p.innerHTML=w),X=r(b),g(U.$$.fragment,b)},m(b,G){y(n,b,G),a(b,o,G),a(b,t,G),a(b,f,G),y(C,b,G),a(b,v,G),a(b,Z,G),a(b,N,G),a(b,W,G),a(b,_,G),y(I,b,G),a(b,E,G),a(b,p,G),a(b,X,G),y(U,b,G),R=!0},p(b,G){const Q={};G&2&&(Q.$$scope={dirty:G,ctx:b}),n.$set(Q)},i(b){R||(j(n.$$.fragment,b),j(C.$$.fragment,b),j(I.$$.fragment,b),j(U.$$.fragment,b),R=!0)},o(b){d(n.$$.fragment,b),d(C.$$.fragment,b),d(I.$$.fragment,b),d(U.$$.fragment,b),R=!1},d(b){b&&(e(o),e(t),e(f),e(v),e(Z),e(N),e(W),e(_),e(E),e(p),e(X)),u(n,b),u(C,b),u(I,b),u(U,b)}}}function Kl(x){let n,o;return n=new us({props:{$$slots:{default:[Pl]},$$scope:{ctx:x}}}),{c(){J(n.$$.fragment)},l(t){g(n.$$.fragment,t)},m(t,m){y(n,t,m),o=!0},p(t,m){const f={};m&2&&(f.$$scope={dirty:m,ctx:t}),n.$set(f)},i(t){o||(j(n.$$.fragment,t),o=!0)},o(t){d(n.$$.fragment,t),o=!1},d(t){u(n,t)}}}function Ol(x){let n,o='If you are unfamiliar with fine-tuning a model with Keras, check out the <a href="./training#train-a-tensorflow-model-with-keras">basic tutorial</a> first!';return{c(){n=M("p"),n.innerHTML=o},l(t){n=h(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-1egt5s9"&&(n.innerHTML=o)},m(t,m){a(t,n,m)},p:ss,d(t){t&&e(n)}}}function st(x){let n,o,t,m="To fine-tune a model in TensorFlow, follow these steps:",f,C,v="<li>Define the training hyperparameters, and set up an optimizer and a learning rate schedule.</li> <li>Instantiate a pretrained model.</li> <li>Convert a ðŸ¤— Dataset to a <code>tf.data.Dataset</code>.</li> <li>Compile your model.</li> <li>Add callbacks to calculate metrics and upload your model to ðŸ¤— Hub</li> <li>Use the <code>fit()</code> method to run the training.</li>",Z,$,N="Start by defining the hyperparameters, optimizer and learning rate schedule:",W,Y,_,I,E=`Then, load SegFormer with <a href="/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSemanticSegmentation">TFAutoModelForSemanticSegmentation</a> along with the label mappings, and compile it with the
optimizer. Note that Transformers models all have a default task-relevant loss function, so you donâ€™t need to specify one unless you want to:`,p,w,X,U,R='Convert your datasets to the <code>tf.data.Dataset</code> format using the <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset" rel="nofollow">to_tf_dataset</a> and the <a href="/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator">DefaultDataCollator</a>:',b,G,Q,z,L=`To compute the accuracy from the predictions and push your model to the ðŸ¤— Hub, use <a href="../main_classes/keras_callbacks">Keras callbacks</a>.
Pass your <code>compute_metrics</code> function to <a href="/docs/transformers/main/en/main_classes/keras_callbacks#transformers.KerasMetricCallback">KerasMetricCallback</a>,
and use the <a href="/docs/transformers/main/en/main_classes/keras_callbacks#transformers.PushToHubCallback">PushToHubCallback</a> to upload the model:`,Ts,V,S,A,D=`Finally, you are ready to train your model! Call <code>fit()</code> with your training and validation datasets, the number of epochs,
and your callbacks to fine-tune the model:`,P,F,fs,H,q="Congratulations! You have fine-tuned your model and shared it on the ðŸ¤— Hub. You can now use it for inference!",K;return n=new Sa({props:{$$slots:{default:[Ol]},$$scope:{ctx:x}}}),Y=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGNyZWF0ZV9vcHRpbWl6ZXIlMEElMEFiYXRjaF9zaXplJTIwJTNEJTIwMiUwQW51bV9lcG9jaHMlMjAlM0QlMjA1MCUwQW51bV90cmFpbl9zdGVwcyUyMCUzRCUyMGxlbih0cmFpbl9kcyklMjAqJTIwbnVtX2Vwb2NocyUwQWxlYXJuaW5nX3JhdGUlMjAlM0QlMjA2ZS01JTBBd2VpZ2h0X2RlY2F5X3JhdGUlMjAlM0QlMjAwLjAxJTBBJTBBb3B0aW1pemVyJTJDJTIwbHJfc2NoZWR1bGUlMjAlM0QlMjBjcmVhdGVfb3B0aW1pemVyKCUwQSUyMCUyMCUyMCUyMGluaXRfbHIlM0RsZWFybmluZ19yYXRlJTJDJTBBJTIwJTIwJTIwJTIwbnVtX3RyYWluX3N0ZXBzJTNEbnVtX3RyYWluX3N0ZXBzJTJDJTBBJTIwJTIwJTIwJTIwd2VpZ2h0X2RlY2F5X3JhdGUlM0R3ZWlnaHRfZGVjYXlfcmF0ZSUyQyUwQSUyMCUyMCUyMCUyMG51bV93YXJtdXBfc3RlcHMlM0QwJTJDJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">50</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_train_steps = <span class="hljs-built_in">len</span>(train_ds) * num_epochs
<span class="hljs-meta">&gt;&gt;&gt; </span>learning_rate = <span class="hljs-number">6e-5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>weight_decay_rate = <span class="hljs-number">0.01</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, lr_schedule = create_optimizer(
<span class="hljs-meta">... </span>    init_lr=learning_rate,
<span class="hljs-meta">... </span>    num_train_steps=num_train_steps,
<span class="hljs-meta">... </span>    weight_decay_rate=weight_decay_rate,
<span class="hljs-meta">... </span>    num_warmup_steps=<span class="hljs-number">0</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),w=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VtYW50aWNTZWdtZW50YXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU2VtYW50aWNTZWdtZW50YXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMGNoZWNrcG9pbnQlMkMlMEElMjAlMjAlMjAlMjBpZDJsYWJlbCUzRGlkMmxhYmVsJTJDJTBBJTIwJTIwJTIwJTIwbGFiZWwyaWQlM0RsYWJlbDJpZCUyQyUwQSklMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzRG9wdGltaXplciklMjAlMjAlMjMlMjBObyUyMGxvc3MlMjBhcmd1bWVudCE=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    checkpoint,
<span class="hljs-meta">... </span>    id2label=id2label,
<span class="hljs-meta">... </span>    label2id=label2id,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)  <span class="hljs-comment"># No loss argument!</span>`,wrap:!1}}),G=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlZmF1bHREYXRhQ29sbGF0b3IlMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGVmYXVsdERhdGFDb2xsYXRvcihyZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQSUwQXRmX3RyYWluX2RhdGFzZXQlMjAlM0QlMjB0cmFpbl9kcy50b190Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMGNvbHVtbnMlM0QlNUIlMjJwaXhlbF92YWx1ZXMlMjIlMkMlMjAlMjJsYWJlbCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHNodWZmbGUlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYmF0Y2hfc2l6ZSUzRGJhdGNoX3NpemUlMkMlMEElMjAlMjAlMjAlMjBjb2xsYXRlX2ZuJTNEZGF0YV9jb2xsYXRvciUyQyUwQSklMEElMEF0Zl9ldmFsX2RhdGFzZXQlMjAlM0QlMjB0ZXN0X2RzLnRvX3RmX2RhdGFzZXQoJTBBJTIwJTIwJTIwJTIwY29sdW1ucyUzRCU1QiUyMnBpeGVsX3ZhbHVlcyUyMiUyQyUyMCUyMmxhYmVsJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwc2h1ZmZsZSUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBiYXRjaF9zaXplJTNEYmF0Y2hfc2l6ZSUyQyUwQSUyMCUyMCUyMCUyMGNvbGxhdGVfZm4lM0RkYXRhX2NvbGxhdG9yJTJDJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DefaultDataCollator

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DefaultDataCollator(return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_dataset = train_ds.to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;pixel_values&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=batch_size,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_eval_dataset = test_ds.to_tf_dataset(
<span class="hljs-meta">... </span>    columns=[<span class="hljs-string">&quot;pixel_values&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=batch_size,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`,wrap:!1}}),V=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBLZXJhc01ldHJpY0NhbGxiYWNrJTJDJTIwUHVzaFRvSHViQ2FsbGJhY2slMEElMEFtZXRyaWNfY2FsbGJhY2slMjAlM0QlMjBLZXJhc01ldHJpY0NhbGxiYWNrKCUwQSUyMCUyMCUyMCUyMG1ldHJpY19mbiUzRGNvbXB1dGVfbWV0cmljcyUyQyUyMGV2YWxfZGF0YXNldCUzRHRmX2V2YWxfZGF0YXNldCUyQyUyMGJhdGNoX3NpemUlM0RiYXRjaF9zaXplJTJDJTIwbGFiZWxfY29scyUzRCU1QiUyMmxhYmVscyUyMiU1RCUwQSklMEElMEFwdXNoX3RvX2h1Yl9jYWxsYmFjayUyMCUzRCUyMFB1c2hUb0h1YkNhbGxiYWNrKG91dHB1dF9kaXIlM0QlMjJzY2VuZV9zZWdtZW50YXRpb24lMjIlMkMlMjB0b2tlbml6ZXIlM0RpbWFnZV9wcm9jZXNzb3IpJTBBJTBBY2FsbGJhY2tzJTIwJTNEJTIwJTVCbWV0cmljX2NhbGxiYWNrJTJDJTIwcHVzaF90b19odWJfY2FsbGJhY2slNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> KerasMetricCallback, PushToHubCallback

<span class="hljs-meta">&gt;&gt;&gt; </span>metric_callback = KerasMetricCallback(
<span class="hljs-meta">... </span>    metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=[<span class="hljs-string">&quot;labels&quot;</span>]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>push_to_hub_callback = PushToHubCallback(output_dir=<span class="hljs-string">&quot;scene_segmentation&quot;</span>, tokenizer=image_processor)

<span class="hljs-meta">&gt;&gt;&gt; </span>callbacks = [metric_callback, push_to_hub_callback]`,wrap:!1}}),F=new k({props:{code:"bW9kZWwuZml0KCUwQSUyMCUyMCUyMCUyMHRmX3RyYWluX2RhdGFzZXQlMkMlMEElMjAlMjAlMjAlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl9ldmFsX2RhdGFzZXQlMkMlMEElMjAlMjAlMjAlMjBjYWxsYmFja3MlM0RjYWxsYmFja3MlMkMlMEElMjAlMjAlMjAlMjBlcG9jaHMlM0RudW1fZXBvY2hzJTJDJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(
<span class="hljs-meta">... </span>    tf_train_dataset,
<span class="hljs-meta">... </span>    validation_data=tf_eval_dataset,
<span class="hljs-meta">... </span>    callbacks=callbacks,
<span class="hljs-meta">... </span>    epochs=num_epochs,
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){J(n.$$.fragment),o=i(),t=M("p"),t.textContent=m,f=i(),C=M("ol"),C.innerHTML=v,Z=i(),$=M("p"),$.textContent=N,W=i(),J(Y.$$.fragment),_=i(),I=M("p"),I.innerHTML=E,p=i(),J(w.$$.fragment),X=i(),U=M("p"),U.innerHTML=R,b=i(),J(G.$$.fragment),Q=i(),z=M("p"),z.innerHTML=L,Ts=i(),J(V.$$.fragment),S=i(),A=M("p"),A.innerHTML=D,P=i(),J(F.$$.fragment),fs=i(),H=M("p"),H.textContent=q},l(c){g(n.$$.fragment,c),o=r(c),t=h(c,"P",{"data-svelte-h":!0}),T(t)!=="svelte-s07fxj"&&(t.textContent=m),f=r(c),C=h(c,"OL",{"data-svelte-h":!0}),T(C)!=="svelte-1e3w1hr"&&(C.innerHTML=v),Z=r(c),$=h(c,"P",{"data-svelte-h":!0}),T($)!=="svelte-ccl3wn"&&($.textContent=N),W=r(c),g(Y.$$.fragment,c),_=r(c),I=h(c,"P",{"data-svelte-h":!0}),T(I)!=="svelte-1xg5478"&&(I.innerHTML=E),p=r(c),g(w.$$.fragment,c),X=r(c),U=h(c,"P",{"data-svelte-h":!0}),T(U)!=="svelte-7cvdbm"&&(U.innerHTML=R),b=r(c),g(G.$$.fragment,c),Q=r(c),z=h(c,"P",{"data-svelte-h":!0}),T(z)!=="svelte-1b1pfl3"&&(z.innerHTML=L),Ts=r(c),g(V.$$.fragment,c),S=r(c),A=h(c,"P",{"data-svelte-h":!0}),T(A)!=="svelte-1occr1z"&&(A.innerHTML=D),P=r(c),g(F.$$.fragment,c),fs=r(c),H=h(c,"P",{"data-svelte-h":!0}),T(H)!=="svelte-1r99pbn"&&(H.textContent=q)},m(c,B){y(n,c,B),a(c,o,B),a(c,t,B),a(c,f,B),a(c,C,B),a(c,Z,B),a(c,$,B),a(c,W,B),y(Y,c,B),a(c,_,B),a(c,I,B),a(c,p,B),y(w,c,B),a(c,X,B),a(c,U,B),a(c,b,B),y(G,c,B),a(c,Q,B),a(c,z,B),a(c,Ts,B),y(V,c,B),a(c,S,B),a(c,A,B),a(c,P,B),y(F,c,B),a(c,fs,B),a(c,H,B),K=!0},p(c,B){const ws={};B&2&&(ws.$$scope={dirty:B,ctx:c}),n.$set(ws)},i(c){K||(j(n.$$.fragment,c),j(Y.$$.fragment,c),j(w.$$.fragment,c),j(G.$$.fragment,c),j(V.$$.fragment,c),j(F.$$.fragment,c),K=!0)},o(c){d(n.$$.fragment,c),d(Y.$$.fragment,c),d(w.$$.fragment,c),d(G.$$.fragment,c),d(V.$$.fragment,c),d(F.$$.fragment,c),K=!1},d(c){c&&(e(o),e(t),e(f),e(C),e(Z),e($),e(W),e(_),e(I),e(p),e(X),e(U),e(b),e(Q),e(z),e(Ts),e(S),e(A),e(P),e(fs),e(H)),u(n,c),u(Y,c),u(w,c),u(G,c),u(V,c),u(F,c)}}}function et(x){let n,o;return n=new us({props:{$$slots:{default:[st]},$$scope:{ctx:x}}}),{c(){J(n.$$.fragment)},l(t){g(n.$$.fragment,t)},m(t,m){y(n,t,m),o=!0},p(t,m){const f={};m&2&&(f.$$scope={dirty:m,ctx:t}),n.$set(f)},i(t){o||(j(n.$$.fragment,t),o=!0)},o(t){d(n.$$.fragment,t),o=!1},d(t){u(n,t)}}}function at(x){let n,o="We will now see how to infer without a pipeline. Process the image with an image processor and place the <code>pixel_values</code> on a GPU:",t,m,f,C,v="Pass your input to the model and return the <code>logits</code>:",Z,$,N,W,Y="Next, rescale the logits to the original image size:",_,I,E;return m=new k({props:{code:"ZGV2aWNlJTIwJTNEJTIwdG9yY2guZGV2aWNlKCUyMmN1ZGElMjIlMjBpZiUyMHRvcmNoLmN1ZGEuaXNfYXZhaWxhYmxlKCklMjBlbHNlJTIwJTIyY3B1JTIyKSUyMCUyMCUyMyUyMHVzZSUyMEdQVSUyMGlmJTIwYXZhaWxhYmxlJTJDJTIwb3RoZXJ3aXNlJTIwdXNlJTIwYSUyMENQVSUwQWVuY29kaW5nJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFwaXhlbF92YWx1ZXMlMjAlM0QlMjBlbmNvZGluZy5waXhlbF92YWx1ZXMudG8oZGV2aWNlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)  <span class="hljs-comment"># use GPU if available, otherwise use a CPU</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = encoding.pixel_values.to(device)`,wrap:!1}}),$=new k({props:{code:"b3V0cHV0cyUyMCUzRCUyMG1vZGVsKHBpeGVsX3ZhbHVlcyUzRHBpeGVsX3ZhbHVlcyklMEFsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cy5jcHUoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values=pixel_values)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits.cpu()`,wrap:!1}}),I=new k({props:{code:"dXBzYW1wbGVkX2xvZ2l0cyUyMCUzRCUyMG5uLmZ1bmN0aW9uYWwuaW50ZXJwb2xhdGUoJTBBJTIwJTIwJTIwJTIwbG9naXRzJTJDJTBBJTIwJTIwJTIwJTIwc2l6ZSUzRGltYWdlLnNpemUlNUIlM0ElM0EtMSU1RCUyQyUwQSUyMCUyMCUyMCUyMG1vZGUlM0QlMjJiaWxpbmVhciUyMiUyQyUwQSUyMCUyMCUyMCUyMGFsaWduX2Nvcm5lcnMlM0RGYWxzZSUyQyUwQSklMEElMEFwcmVkX3NlZyUyMCUzRCUyMHVwc2FtcGxlZF9sb2dpdHMuYXJnbWF4KGRpbSUzRDEpJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>upsampled_logits = nn.functional.interpolate(
<span class="hljs-meta">... </span>    logits,
<span class="hljs-meta">... </span>    size=image.size[::-<span class="hljs-number">1</span>],
<span class="hljs-meta">... </span>    mode=<span class="hljs-string">&quot;bilinear&quot;</span>,
<span class="hljs-meta">... </span>    align_corners=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>pred_seg = upsampled_logits.argmax(dim=<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]`,wrap:!1}}),{c(){n=M("p"),n.innerHTML=o,t=i(),J(m.$$.fragment),f=i(),C=M("p"),C.innerHTML=v,Z=i(),J($.$$.fragment),N=i(),W=M("p"),W.textContent=Y,_=i(),J(I.$$.fragment)},l(p){n=h(p,"P",{"data-svelte-h":!0}),T(n)!=="svelte-appsul"&&(n.innerHTML=o),t=r(p),g(m.$$.fragment,p),f=r(p),C=h(p,"P",{"data-svelte-h":!0}),T(C)!=="svelte-oyplyw"&&(C.innerHTML=v),Z=r(p),g($.$$.fragment,p),N=r(p),W=h(p,"P",{"data-svelte-h":!0}),T(W)!=="svelte-tk6q3q"&&(W.textContent=Y),_=r(p),g(I.$$.fragment,p)},m(p,w){a(p,n,w),a(p,t,w),y(m,p,w),a(p,f,w),a(p,C,w),a(p,Z,w),y($,p,w),a(p,N,w),a(p,W,w),a(p,_,w),y(I,p,w),E=!0},p:ss,i(p){E||(j(m.$$.fragment,p),j($.$$.fragment,p),j(I.$$.fragment,p),E=!0)},o(p){d(m.$$.fragment,p),d($.$$.fragment,p),d(I.$$.fragment,p),E=!1},d(p){p&&(e(n),e(t),e(f),e(C),e(Z),e(N),e(W),e(_)),u(m,p),u($,p),u(I,p)}}}function lt(x){let n,o;return n=new us({props:{$$slots:{default:[at]},$$scope:{ctx:x}}}),{c(){J(n.$$.fragment)},l(t){g(n.$$.fragment,t)},m(t,m){y(n,t,m),o=!0},p(t,m){const f={};m&2&&(f.$$scope={dirty:m,ctx:t}),n.$set(f)},i(t){o||(j(n.$$.fragment,t),o=!0)},o(t){d(n.$$.fragment,t),o=!1},d(t){u(n,t)}}}function tt(x){let n,o="Load an image processor to preprocess the image and return the input as TensorFlow tensors:",t,m,f,C,v="Pass your input to the model and return the <code>logits</code>:",Z,$,N,W,Y="Next, rescale the logits to the original image size and apply argmax on the class dimension:",_,I,E;return m=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyTWFyaWFLJTJGc2NlbmVfc2VnbWVudGF0aW9uJTIyKSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIydGYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;MariaK/scene_segmentation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),$=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VtYW50aWNTZWdtZW50YXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU2VtYW50aWNTZWdtZW50YXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMk1hcmlhSyUyRnNjZW5lX3NlZ21lbnRhdGlvbiUyMiklMEFsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;MariaK/scene_segmentation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`,wrap:!1}}),I=new k({props:{code:"bG9naXRzJTIwJTNEJTIwdGYudHJhbnNwb3NlKGxvZ2l0cyUyQyUyMCU1QjAlMkMlMjAyJTJDJTIwMyUyQyUyMDElNUQpJTBBJTBBdXBzYW1wbGVkX2xvZ2l0cyUyMCUzRCUyMHRmLmltYWdlLnJlc2l6ZSglMEElMjAlMjAlMjAlMjBsb2dpdHMlMkMlMEElMjAlMjAlMjAlMjAlMjMlMjBXZSUyMHJldmVyc2UlMjB0aGUlMjBzaGFwZSUyMG9mJTIwJTYwaW1hZ2UlNjAlMjBiZWNhdXNlJTIwJTYwaW1hZ2Uuc2l6ZSU2MCUyMHJldHVybnMlMjB3aWR0aCUyMGFuZCUyMGhlaWdodC4lMEElMjAlMjAlMjAlMjBpbWFnZS5zaXplJTVCJTNBJTNBLTElNUQlMkMlMEEpJTBBJTBBcHJlZF9zZWclMjAlM0QlMjB0Zi5tYXRoLmFyZ21heCh1cHNhbXBsZWRfbG9naXRzJTJDJTIwYXhpcyUzRC0xKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>logits = tf.transpose(logits, [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>upsampled_logits = tf.image.resize(
<span class="hljs-meta">... </span>    logits,
<span class="hljs-meta">... </span>    <span class="hljs-comment"># We reverse the shape of \`image\` because \`image.size\` returns width and height.</span>
<span class="hljs-meta">... </span>    image.size[::-<span class="hljs-number">1</span>],
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>pred_seg = tf.math.argmax(upsampled_logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]`,wrap:!1}}),{c(){n=M("p"),n.textContent=o,t=i(),J(m.$$.fragment),f=i(),C=M("p"),C.innerHTML=v,Z=i(),J($.$$.fragment),N=i(),W=M("p"),W.textContent=Y,_=i(),J(I.$$.fragment)},l(p){n=h(p,"P",{"data-svelte-h":!0}),T(n)!=="svelte-1r2yss0"&&(n.textContent=o),t=r(p),g(m.$$.fragment,p),f=r(p),C=h(p,"P",{"data-svelte-h":!0}),T(C)!=="svelte-oyplyw"&&(C.innerHTML=v),Z=r(p),g($.$$.fragment,p),N=r(p),W=h(p,"P",{"data-svelte-h":!0}),T(W)!=="svelte-enih9r"&&(W.textContent=Y),_=r(p),g(I.$$.fragment,p)},m(p,w){a(p,n,w),a(p,t,w),y(m,p,w),a(p,f,w),a(p,C,w),a(p,Z,w),y($,p,w),a(p,N,w),a(p,W,w),a(p,_,w),y(I,p,w),E=!0},p:ss,i(p){E||(j(m.$$.fragment,p),j($.$$.fragment,p),j(I.$$.fragment,p),E=!0)},o(p){d(m.$$.fragment,p),d($.$$.fragment,p),d(I.$$.fragment,p),E=!1},d(p){p&&(e(n),e(t),e(f),e(C),e(Z),e(N),e(W),e(_)),u(m,p),u($,p),u(I,p)}}}function nt(x){let n,o;return n=new us({props:{$$slots:{default:[tt]},$$scope:{ctx:x}}}),{c(){J(n.$$.fragment)},l(t){g(n.$$.fragment,t)},m(t,m){y(n,t,m),o=!0},p(t,m){const f={};m&2&&(f.$$scope={dirty:m,ctx:t}),n.$set(f)},i(t){o||(j(n.$$.fragment,t),o=!0)},o(t){d(n.$$.fragment,t),o=!1},d(t){u(n,t)}}}function pt(x){let n,o,t,m,f,C,v,Z,$,N,W,Y="Image segmentation models separate areas corresponding to different areas of interest in an image. These models work by assigning a label to each pixel. There are several types of segmentation: semantic segmentation, instance segmentation, and panoptic segmentation.",_,I,E="In this guide, we will:",p,w,X='<li><a href="#types-of-segmentation">Take a look at different types of segmentation</a>.</li> <li><a href="#fine-tuning-a-model-for-segmentation">Have an end-to-end fine-tuning example for semantic segmentation</a>.</li>',U,R,b="Before you begin, make sure you have all the necessary libraries installed:",G,Q,z,L,Ts="We encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:",V,S,A,D,P,F,fs=`Semantic segmentation assigns a label or class to every single pixel in an image. Letâ€™s take a look at a semantic segmentation model output. It will assign the same class to every instance of an object it comes across in an image, for example, all cats will be labeled as â€œcatâ€ instead of â€œcat-1â€, â€œcat-2â€.
We can use transformersâ€™ image segmentation pipeline to quickly infer a semantic segmentation model. Letâ€™s take a look at the example image.`,H,q,K,c,B='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg" alt="Segmentation Input"/>',ws,Us,Aa='We will use <a href="https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024" rel="nofollow">nvidia/segformer-b1-finetuned-cityscapes-1024-1024</a>.',xe,bs,Ze,Is,Ha="The segmentation pipeline output includes a mask for every predicted class.",_e,$s,Be,Cs,La="Taking a look at the mask for the car class, we can see every car is classified with the same mask.",ve,Ws,Ge,es,Da='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/semantic_segmentation_output.png" alt="Semantic Segmentation Output"/>',Ne,ks,qa='In instance segmentation, the goal is not to classify every pixel, but to predict a mask for <strong>every instance of an object</strong> in a given image. It works very similar to object detection, where there is a bounding box for every instance, thereâ€™s a segmentation mask instead. We will use <a href="https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance" rel="nofollow">facebook/mask2former-swin-large-cityscapes-instance</a> for this.',Ye,xs,Re,Zs,Pa="As you can see below, there are multiple cars classified, and thereâ€™s no classification for pixels other than pixels that belong to car and person instances.",Ee,_s,Xe,Bs,Ka="Checking out one of the car masks below.",Qe,vs,Fe,as,Oa='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/instance_segmentation_output.png" alt="Semantic Segmentation Output"/>',ze,Gs,sl='Panoptic segmentation combines semantic segmentation and instance segmentation, where every pixel is classified into a class and an instance of that class, and there are multiple masks for each instance of a class. We can use <a href="https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic" rel="nofollow">facebook/mask2former-swin-large-cityscapes-panoptic</a> for this.',Ve,Ns,Se,Ys,el="As you can see below, we have more classes. We will later illustrate to see that every pixel is classified into one of the classes.",Ae,Rs,He,Es,al="Letâ€™s have a side by side comparison for all types of segmentation.",Le,ls,ll='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation-comparison.png" alt="Segmentation Maps Compared"/>',De,Xs,tl="Seeing all types of segmentation, letâ€™s have a deep dive on fine-tuning a model for semantic segmentation.",qe,Qs,nl="Common real-world applications of semantic segmentation include training self-driving cars to identify pedestrians and important traffic information, identifying cells and abnormalities in medical imagery, and monitoring environmental changes from satellite imagery.",Pe,Fs,Ke,zs,pl="We will now:",Oe,Vs,il='<li>Finetune <a href="https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer" rel="nofollow">SegFormer</a> on the <a href="https://huggingface.co/datasets/scene_parse_150" rel="nofollow">SceneParse150</a> dataset.</li> <li>Use your fine-tuned model for inference.</li>',sa,ts,ea,Ss,aa,As,rl="Start by loading a smaller subset of the SceneParse150 dataset from the ðŸ¤— Datasets library. Thisâ€™ll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.",la,Hs,ta,Ls,ml='Split the datasetâ€™s <code>train</code> split into a train and test set with the <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split" rel="nofollow">train_test_split</a> method:',na,Ds,pa,qs,ol="Then take a look at an example:",ia,Ps,ra,Ks,cl="<li><code>image</code>: a PIL image of the scene.</li> <li><code>annotation</code>: a PIL image of the segmentation map, which is also the modelâ€™s target.</li> <li><code>scene_category</code>: a category id that describes the image scene like â€œkitchenâ€ or â€œofficeâ€. In this guide, youâ€™ll only need <code>image</code> and <code>annotation</code>, both of which are PIL images.</li>",ma,Os,Ml="Youâ€™ll also want to create a dictionary that maps a label id to a label class which will be useful when you set up the model later. Download the mappings from the Hub and create the <code>id2label</code> and <code>label2id</code> dictionaries:",oa,se,ca,ee,Ma,ae,hl='You could also create and use your own dataset if you prefer to train with the <a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py" rel="nofollow">run_semantic_segmentation.py</a> script instead of a notebook instance. The script requires:',ha,ns,le,$e,Jl='a <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.DatasetDict" rel="nofollow">DatasetDict</a> with two <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Image" rel="nofollow">Image</a> columns, â€œimageâ€ and â€œlabelâ€',Fa,te,za,ne,Ce,gl="an id2label dictionary mapping the class integers to their class names",Va,pe,Ja,ie,yl='As an example, take a look at this <a href="https://huggingface.co/datasets/nielsr/ade20k-demo" rel="nofollow">example dataset</a> which was created with the steps shown above.',ga,re,ya,me,jl="The next step is to load a SegFormer image processor to prepare the images and annotations for the model. Some datasets, like this one, use the zero-index as the background class. However, the background class isnâ€™t actually included in the 150 classes, so youâ€™ll need to set <code>reduce_labels=True</code> to subtract one from all the labels. The zero-index is replaced by <code>255</code> so itâ€™s ignored by SegFormerâ€™s loss function:",ja,oe,da,ps,ua,is,Ta,ce,fa,Me,dl='Including a metric during training is often helpful for evaluating your modelâ€™s performance. You can quickly load an evaluation method with the ðŸ¤— <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> library. For this task, load the <a href="https://huggingface.co/spaces/evaluate-metric/accuracy" rel="nofollow">mean Intersection over Union</a> (IoU) metric (see the ðŸ¤— Evaluate <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">quick tour</a> to learn more about how to load and compute a metric):',wa,he,Ua,Je,ul=`Then create a function to <a href="https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute" rel="nofollow">compute</a> the metrics. Your predictions need to be converted to
logits first, and then reshaped to match the size of the labels before you can call <a href="https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute" rel="nofollow">compute</a>:`,ba,rs,Ia,ms,$a,ge,Tl="Your <code>compute_metrics</code> function is ready to go now, and youâ€™ll return to it when you setup your training.",Ca,ye,Wa,os,ka,cs,xa,je,Za,de,fl="Great, now that youâ€™ve finetuned a model, you can use it for inference!",_a,ue,wl="Load an image for inference:",Ba,Te,va,Ms,Ul='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png" alt="Image of bedroom"/>',Ga,hs,Na,Js,Ya,fe,bl='To visualize the results, load the <a href="https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51" rel="nofollow">dataset color palette</a> as <code>ade_palette()</code> that maps each class to their RGB values. Then you can combine and plot your image and the predicted segmentation map:',Ra,we,Ea,gs,Il='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png" alt="Image of bedroom overlaid with segmentation map"/>',Xa,We,Qa;return f=new O({props:{title:"Image Segmentation",local:"image-segmentation",headingTag:"h1"}}),v=new Xl({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/semantic_segmentation.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/semantic_segmentation.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/semantic_segmentation.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/semantic_segmentation.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/semantic_segmentation.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/semantic_segmentation.ipynb"}]}}),$=new El({props:{id:"dKE8SIt9C-w"}}),Q=new k({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1xJTIwZGF0YXNldHMlMjB0cmFuc2Zvcm1lcnMlMjBldmFsdWF0ZQ==",highlighted:"pip install -q datasets transformers evaluate",wrap:!1}}),S=new k({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),D=new O({props:{title:"Types of Segmentation",local:"types-of-segmentation",headingTag:"h2"}}),q=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZodWdnaW5nZmFjZSUyRmRvY3VtZW50YXRpb24taW1hZ2VzJTJGcmVzb2x2ZSUyRm1haW4lMkZ0cmFuc2Zvcm1lcnMlMkZ0YXNrcyUyRnNlZ21lbnRhdGlvbl9pbnB1dC5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEFpbWFnZQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

url = <span class="hljs-string">&quot;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
image`,wrap:!1}}),bs=new k({props:{code:"c2VtYW50aWNfc2VnbWVudGF0aW9uJTIwJTNEJTIwcGlwZWxpbmUoJTIyaW1hZ2Utc2VnbWVudGF0aW9uJTIyJTJDJTIwJTIybnZpZGlhJTJGc2VnZm9ybWVyLWIxLWZpbmV0dW5lZC1jaXR5c2NhcGVzLTEwMjQtMTAyNCUyMiklMEFyZXN1bHRzJTIwJTNEJTIwc2VtYW50aWNfc2VnbWVudGF0aW9uKGltYWdlKSUwQXJlc3VsdHM=",highlighted:`semantic_segmentation = pipeline(<span class="hljs-string">&quot;image-segmentation&quot;</span>, <span class="hljs-string">&quot;nvidia/segformer-b1-finetuned-cityscapes-1024-1024&quot;</span>)
results = semantic_segmentation(image)
results`,wrap:!1}}),$s=new k({props:{code:"JTVCJTdCJ3Njb3JlJyUzQSUyME5vbmUlMkMlMEElMjAlMjAnbGFiZWwnJTNBJTIwJ3JvYWQnJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyME5vbmUlMkMlMEElMjAlMjAnbGFiZWwnJTNBJTIwJ3NpZGV3YWxrJyUyQyUwQSUyMCUyMCdtYXNrJyUzQSUyMCUzQ1BJTC5JbWFnZS5JbWFnZSUyMGltYWdlJTIwbW9kZSUzREwlMjBzaXplJTNENjEyeDQxNSUzRSU3RCUyQyUwQSUyMCU3QidzY29yZSclM0ElMjBOb25lJTJDJTBBJTIwJTIwJ2xhYmVsJyUzQSUyMCdidWlsZGluZyclMkMlMEElMjAlMjAnbWFzayclM0ElMjAlM0NQSUwuSW1hZ2UuSW1hZ2UlMjBpbWFnZSUyMG1vZGUlM0RMJTIwc2l6ZSUzRDYxMng0MTUlM0UlN0QlMkMlMEElMjAlN0Inc2NvcmUnJTNBJTIwTm9uZSUyQyUwQSUyMCUyMCdsYWJlbCclM0ElMjAnd2FsbCclMkMlMEElMjAlMjAnbWFzayclM0ElMjAlM0NQSUwuSW1hZ2UuSW1hZ2UlMjBpbWFnZSUyMG1vZGUlM0RMJTIwc2l6ZSUzRDYxMng0MTUlM0UlN0QlMkMlMEElMjAlN0Inc2NvcmUnJTNBJTIwTm9uZSUyQyUwQSUyMCUyMCdsYWJlbCclM0ElMjAncG9sZSclMkMlMEElMjAlMjAnbWFzayclM0ElMjAlM0NQSUwuSW1hZ2UuSW1hZ2UlMjBpbWFnZSUyMG1vZGUlM0RMJTIwc2l6ZSUzRDYxMng0MTUlM0UlN0QlMkMlMEElMjAlN0Inc2NvcmUnJTNBJTIwTm9uZSUyQyUwQSUyMCUyMCdsYWJlbCclM0ElMjAndHJhZmZpYyUyMHNpZ24nJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyME5vbmUlMkMlMEElMjAlMjAnbGFiZWwnJTNBJTIwJ3ZlZ2V0YXRpb24nJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyME5vbmUlMkMlMEElMjAlMjAnbGFiZWwnJTNBJTIwJ3RlcnJhaW4nJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyME5vbmUlMkMlMEElMjAlMjAnbGFiZWwnJTNBJTIwJ3NreSclMkMlMEElMjAlMjAnbWFzayclM0ElMjAlM0NQSUwuSW1hZ2UuSW1hZ2UlMjBpbWFnZSUyMG1vZGUlM0RMJTIwc2l6ZSUzRDYxMng0MTUlM0UlN0QlMkMlMEElMjAlN0Inc2NvcmUnJTNBJTIwTm9uZSUyQyUwQSUyMCUyMCdsYWJlbCclM0ElMjAnY2FyJyUyQyUwQSUyMCUyMCdtYXNrJyUzQSUyMCUzQ1BJTC5JbWFnZS5JbWFnZSUyMGltYWdlJTIwbW9kZSUzREwlMjBzaXplJTNENjEyeDQxNSUzRSU3RCU1RA==",highlighted:`[{<span class="hljs-string">&#x27;score&#x27;</span>: None,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;road&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: None,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;sidewalk&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: None,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;building&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: None,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;wall&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: None,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;pole&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: None,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;traffic sign&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: None,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;vegetation&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: None,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;terrain&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: None,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;sky&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: None,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;car&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;}]`,wrap:!1}}),Ws=new k({props:{code:"cmVzdWx0cyU1Qi0xJTVEJTVCJTIybWFzayUyMiU1RA==",highlighted:'results[-<span class="hljs-number">1</span>][<span class="hljs-string">&quot;mask&quot;</span>]',wrap:!1}}),xs=new k({props:{code:"aW5zdGFuY2Vfc2VnbWVudGF0aW9uJTIwJTNEJTIwcGlwZWxpbmUoJTIyaW1hZ2Utc2VnbWVudGF0aW9uJTIyJTJDJTIwJTIyZmFjZWJvb2slMkZtYXNrMmZvcm1lci1zd2luLWxhcmdlLWNpdHlzY2FwZXMtaW5zdGFuY2UlMjIpJTBBcmVzdWx0cyUyMCUzRCUyMGluc3RhbmNlX3NlZ21lbnRhdGlvbihJbWFnZS5vcGVuKGltYWdlKSklMEFyZXN1bHRz",highlighted:`instance_segmentation = pipeline(<span class="hljs-string">&quot;image-segmentation&quot;</span>, <span class="hljs-string">&quot;facebook/mask2former-swin-large-cityscapes-instance&quot;</span>)
results = instance_segmentation(Image.<span class="hljs-built_in">open</span>(image))
results`,wrap:!1}}),_s=new k({props:{code:"JTVCJTdCJ3Njb3JlJyUzQSUyMDAuOTk5OTQ0JTJDJTBBJTIwJTIwJ2xhYmVsJyUzQSUyMCdjYXInJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyMDAuOTk5OTQ1JTJDJTBBJTIwJTIwJ2xhYmVsJyUzQSUyMCdjYXInJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyMDAuOTk5NjUyJTJDJTBBJTIwJTIwJ2xhYmVsJyUzQSUyMCdjYXInJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyMDAuOTAzNTI5JTJDJTBBJTIwJTIwJ2xhYmVsJyUzQSUyMCdwZXJzb24nJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTVE",highlighted:`[{<span class="hljs-string">&#x27;score&#x27;</span>: 0.999944,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;car&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.999945,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;car&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.999652,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;car&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.903529,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;person&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;}]`,wrap:!1}}),vs=new k({props:{code:"cmVzdWx0cyU1QjIlNUQlNUIlMjJtYXNrJTIyJTVE",highlighted:'results[<span class="hljs-number">2</span>][<span class="hljs-string">&quot;mask&quot;</span>]',wrap:!1}}),Ns=new k({props:{code:"cGFub3B0aWNfc2VnbWVudGF0aW9uJTIwJTNEJTIwcGlwZWxpbmUoJTIyaW1hZ2Utc2VnbWVudGF0aW9uJTIyJTJDJTIwJTIyZmFjZWJvb2slMkZtYXNrMmZvcm1lci1zd2luLWxhcmdlLWNpdHlzY2FwZXMtcGFub3B0aWMlMjIpJTBBcmVzdWx0cyUyMCUzRCUyMHBhbm9wdGljX3NlZ21lbnRhdGlvbihJbWFnZS5vcGVuKGltYWdlKSklMEFyZXN1bHRz",highlighted:`panoptic_segmentation = pipeline(<span class="hljs-string">&quot;image-segmentation&quot;</span>, <span class="hljs-string">&quot;facebook/mask2former-swin-large-cityscapes-panoptic&quot;</span>)
results = panoptic_segmentation(Image.<span class="hljs-built_in">open</span>(image))
results`,wrap:!1}}),Rs=new k({props:{code:"JTVCJTdCJ3Njb3JlJyUzQSUyMDAuOTk5OTgxJTJDJTBBJTIwJTIwJ2xhYmVsJyUzQSUyMCdjYXInJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyMDAuOTk5OTU4JTJDJTBBJTIwJTIwJ2xhYmVsJyUzQSUyMCdjYXInJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyMDAuOTk5OTclMkMlMEElMjAlMjAnbGFiZWwnJTNBJTIwJ3ZlZ2V0YXRpb24nJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyMDAuOTk5NTc1JTJDJTBBJTIwJTIwJ2xhYmVsJyUzQSUyMCdwb2xlJyUyQyUwQSUyMCUyMCdtYXNrJyUzQSUyMCUzQ1BJTC5JbWFnZS5JbWFnZSUyMGltYWdlJTIwbW9kZSUzREwlMjBzaXplJTNENjEyeDQxNSUzRSU3RCUyQyUwQSUyMCU3QidzY29yZSclM0ElMjAwLjk5OTk1OCUyQyUwQSUyMCUyMCdsYWJlbCclM0ElMjAnYnVpbGRpbmcnJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyMDAuOTk5NjM0JTJDJTBBJTIwJTIwJ2xhYmVsJyUzQSUyMCdyb2FkJyUyQyUwQSUyMCUyMCdtYXNrJyUzQSUyMCUzQ1BJTC5JbWFnZS5JbWFnZSUyMGltYWdlJTIwbW9kZSUzREwlMjBzaXplJTNENjEyeDQxNSUzRSU3RCUyQyUwQSUyMCU3QidzY29yZSclM0ElMjAwLjk5NjA5MiUyQyUwQSUyMCUyMCdsYWJlbCclM0ElMjAnc2lkZXdhbGsnJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyMDAuOTk5MjIxJTJDJTBBJTIwJTIwJ2xhYmVsJyUzQSUyMCdjYXInJTJDJTBBJTIwJTIwJ21hc2snJTNBJTIwJTNDUElMLkltYWdlLkltYWdlJTIwaW1hZ2UlMjBtb2RlJTNETCUyMHNpemUlM0Q2MTJ4NDE1JTNFJTdEJTJDJTBBJTIwJTdCJ3Njb3JlJyUzQSUyMDAuOTk5ODclMkMlMEElMjAlMjAnbGFiZWwnJTNBJTIwJ3NreSclMkMlMEElMjAlMjAnbWFzayclM0ElMjAlM0NQSUwuSW1hZ2UuSW1hZ2UlMjBpbWFnZSUyMG1vZGUlM0RMJTIwc2l6ZSUzRDYxMng0MTUlM0UlN0QlNUQ=",highlighted:`[{<span class="hljs-string">&#x27;score&#x27;</span>: 0.999981,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;car&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.999958,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;car&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.99997,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;vegetation&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.999575,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;pole&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.999958,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;building&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.999634,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;road&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.996092,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;sidewalk&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.999221,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;car&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;},
 {<span class="hljs-string">&#x27;score&#x27;</span>: 0.99987,
  <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;sky&#x27;</span>,
  <span class="hljs-string">&#x27;mask&#x27;</span>: &lt;PIL.Image.Image image mode=L size=612x415&gt;}]`,wrap:!1}}),Fs=new O({props:{title:"Fine-tuning a Model for Segmentation",local:"fine-tuning-a-model-for-segmentation",headingTag:"h2"}}),ts=new Sa({props:{$$slots:{default:[Ql]},$$scope:{ctx:x}}}),Ss=new O({props:{title:"Load SceneParse150 dataset",local:"load-sceneparse150-dataset",headingTag:"h3"}}),Hs=new k({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZHMlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyc2NlbmVfcGFyc2VfMTUwJTIyJTJDJTIwc3BsaXQlM0QlMjJ0cmFpbiU1QiUzQTUwJTVEJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;scene_parse_150&quot;</span>, split=<span class="hljs-string">&quot;train[:50]&quot;</span>)`,wrap:!1}}),Ds=new k({props:{code:"ZHMlMjAlM0QlMjBkcy50cmFpbl90ZXN0X3NwbGl0KHRlc3Rfc2l6ZSUzRDAuMiklMEF0cmFpbl9kcyUyMCUzRCUyMGRzJTVCJTIydHJhaW4lMjIlNUQlMEF0ZXN0X2RzJTIwJTNEJTIwZHMlNUIlMjJ0ZXN0JTIyJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds = ds.train_test_split(test_size=<span class="hljs-number">0.2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>train_ds = ds[<span class="hljs-string">&quot;train&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>test_ds = ds[<span class="hljs-string">&quot;test&quot;</span>]`,wrap:!1}}),Ps=new k({props:{code:"dHJhaW5fZHMlNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_ds[<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at <span class="hljs-number">0x7F9B0C201F90</span>&gt;,
 <span class="hljs-string">&#x27;annotation&#x27;</span>: &lt;PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at <span class="hljs-number">0x7F9B0C201DD0</span>&gt;,
 <span class="hljs-string">&#x27;scene_category&#x27;</span>: <span class="hljs-number">368</span>}`,wrap:!1}}),se=new k({props:{code:"aW1wb3J0JTIwanNvbiUwQWZyb20lMjBodWdnaW5nZmFjZV9odWIlMjBpbXBvcnQlMjBjYWNoZWRfZG93bmxvYWQlMkMlMjBoZl9odWJfdXJsJTBBJTBBcmVwb19pZCUyMCUzRCUyMCUyMmh1Z2dpbmdmYWNlJTJGbGFiZWwtZmlsZXMlMjIlMEFmaWxlbmFtZSUyMCUzRCUyMCUyMmFkZTIway1pZDJsYWJlbC5qc29uJTIyJTBBaWQybGFiZWwlMjAlM0QlMjBqc29uLmxvYWQob3BlbihjYWNoZWRfZG93bmxvYWQoaGZfaHViX3VybChyZXBvX2lkJTJDJTIwZmlsZW5hbWUlMkMlMjByZXBvX3R5cGUlM0QlMjJkYXRhc2V0JTIyKSklMkMlMjAlMjJyJTIyKSklMEFpZDJsYWJlbCUyMCUzRCUyMCU3QmludChrKSUzQSUyMHYlMjBmb3IlMjBrJTJDJTIwdiUyMGluJTIwaWQybGFiZWwuaXRlbXMoKSU3RCUwQWxhYmVsMmlkJTIwJTNEJTIwJTdCdiUzQSUyMGslMjBmb3IlMjBrJTJDJTIwdiUyMGluJTIwaWQybGFiZWwuaXRlbXMoKSU3RCUwQW51bV9sYWJlbHMlMjAlM0QlMjBsZW4oaWQybGFiZWwp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> json
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> cached_download, hf_hub_url

<span class="hljs-meta">&gt;&gt;&gt; </span>repo_id = <span class="hljs-string">&quot;huggingface/label-files&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>filename = <span class="hljs-string">&quot;ade20k-id2label.json&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>id2label = json.load(<span class="hljs-built_in">open</span>(cached_download(hf_hub_url(repo_id, filename, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>)), <span class="hljs-string">&quot;r&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>id2label = {<span class="hljs-built_in">int</span>(k): v <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> id2label.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {v: k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> id2label.items()}
<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(id2label)`,wrap:!1}}),ee=new O({props:{title:"Custom dataset",local:"custom-dataset",headingTag:"h4"}}),te=new k({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> Dataset, DatasetDict, Image

image_paths_train = [<span class="hljs-string">&quot;path/to/image_1.jpg/jpg&quot;</span>, <span class="hljs-string">&quot;path/to/image_2.jpg/jpg&quot;</span>, ..., <span class="hljs-string">&quot;path/to/image_n.jpg/jpg&quot;</span>]
label_paths_train = [<span class="hljs-string">&quot;path/to/annotation_1.png&quot;</span>, <span class="hljs-string">&quot;path/to/annotation_2.png&quot;</span>, ..., <span class="hljs-string">&quot;path/to/annotation_n.png&quot;</span>]

image_paths_validation = [...]
label_paths_validation = [...]

<span class="hljs-keyword">def</span> <span class="hljs-title function_">create_dataset</span>(<span class="hljs-params">image_paths, label_paths</span>):
    dataset = Dataset.from_dict({<span class="hljs-string">&quot;image&quot;</span>: <span class="hljs-built_in">sorted</span>(image_paths),
                                <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-built_in">sorted</span>(label_paths)})
    dataset = dataset.cast_column(<span class="hljs-string">&quot;image&quot;</span>, Image())
    dataset = dataset.cast_column(<span class="hljs-string">&quot;label&quot;</span>, Image())
    <span class="hljs-keyword">return</span> dataset

<span class="hljs-comment"># step 1: create Dataset objects</span>
train_dataset = create_dataset(image_paths_train, label_paths_train)
validation_dataset = create_dataset(image_paths_validation, label_paths_validation)

<span class="hljs-comment"># step 2: create DatasetDict</span>
dataset = DatasetDict({
     <span class="hljs-string">&quot;train&quot;</span>: train_dataset,
     <span class="hljs-string">&quot;validation&quot;</span>: validation_dataset,
     }
)

<span class="hljs-comment"># step 3: push to Hub (assumes you have ran the huggingface-cli login command in a terminal/notebook)</span>
dataset.push_to_hub(<span class="hljs-string">&quot;your-name/dataset-repo&quot;</span>)

<span class="hljs-comment"># optionally, you can push to a private repo on the Hub</span>
<span class="hljs-comment"># dataset.push_to_hub(&quot;name of repo on the hub&quot;, private=True)</span>`,wrap:!1}}),pe=new k({props:{code:"aW1wb3J0JTIwanNvbiUwQSUyMyUyMHNpbXBsZSUyMGV4YW1wbGUlMEFpZDJsYWJlbCUyMCUzRCUyMCU3QjAlM0ElMjAnY2F0JyUyQyUyMDElM0ElMjAnZG9nJyU3RCUwQXdpdGglMjBvcGVuKCdpZDJsYWJlbC5qc29uJyUyQyUyMCd3JyklMjBhcyUyMGZwJTNBJTBBanNvbi5kdW1wKGlkMmxhYmVsJTJDJTIwZnAp",highlighted:`<span class="hljs-keyword">import</span> json
<span class="hljs-comment"># simple example</span>
id2label = {<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;cat&#x27;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&#x27;dog&#x27;</span>}
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;id2label.json&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>) <span class="hljs-keyword">as</span> fp:
json.dump(id2label, fp)`,wrap:!1}}),re=new O({props:{title:"Preprocess",local:"preprocess",headingTag:"h3"}}),oe=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQWNoZWNrcG9pbnQlMjAlM0QlMjAlMjJudmlkaWElMkZtaXQtYjAlMjIlMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQlMkMlMjByZWR1Y2VfbGFiZWxzJTNEVHJ1ZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;nvidia/mit-b0&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(checkpoint, reduce_labels=<span class="hljs-literal">True</span>)`,wrap:!1}}),ps=new ds({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[zl]},$$scope:{ctx:x}}}),is=new ds({props:{pytorch:!1,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Sl]},$$scope:{ctx:x}}}),ce=new O({props:{title:"Evaluate",local:"evaluate",headingTag:"h3"}}),he=new k({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFtZXRyaWMlMjAlM0QlMjBldmFsdWF0ZS5sb2FkKCUyMm1lYW5faW91JTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;mean_iou&quot;</span>)`,wrap:!1}}),rs=new ds({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Hl]},$$scope:{ctx:x}}}),ms=new ds({props:{pytorch:!1,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Dl]},$$scope:{ctx:x}}}),ye=new O({props:{title:"Train",local:"train",headingTag:"h3"}}),os=new ds({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Kl]},$$scope:{ctx:x}}}),cs=new ds({props:{pytorch:!1,tensorflow:!0,jax:!1,$$slots:{tensorflow:[et]},$$scope:{ctx:x}}}),je=new O({props:{title:"Inference",local:"inference",headingTag:"h3"}}),Te=new k({props:{code:"aW1hZ2UlMjAlM0QlMjBkcyU1QjAlNUQlNUIlMjJpbWFnZSUyMiU1RCUwQWltYWdl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>image = ds[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;image&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>image`,wrap:!1}}),hs=new ds({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[lt]},$$scope:{ctx:x}}}),Js=new ds({props:{pytorch:!1,tensorflow:!0,jax:!1,$$slots:{tensorflow:[nt]},$$scope:{ctx:x}}}),we=new k({props:{code:"aW1wb3J0JTIwbWF0cGxvdGxpYi5weXBsb3QlMjBhcyUyMHBsdCUwQWltcG9ydCUyMG51bXB5JTIwYXMlMjBucCUwQSUwQWNvbG9yX3NlZyUyMCUzRCUyMG5wLnplcm9zKChwcmVkX3NlZy5zaGFwZSU1QjAlNUQlMkMlMjBwcmVkX3NlZy5zaGFwZSU1QjElNUQlMkMlMjAzKSUyQyUyMGR0eXBlJTNEbnAudWludDgpJTBBcGFsZXR0ZSUyMCUzRCUyMG5wLmFycmF5KGFkZV9wYWxldHRlKCkpJTBBZm9yJTIwbGFiZWwlMkMlMjBjb2xvciUyMGluJTIwZW51bWVyYXRlKHBhbGV0dGUpJTNBJTBBJTIwJTIwJTIwJTIwY29sb3Jfc2VnJTVCcHJlZF9zZWclMjAlM0QlM0QlMjBsYWJlbCUyQyUyMCUzQSU1RCUyMCUzRCUyMGNvbG9yJTBBY29sb3Jfc2VnJTIwJTNEJTIwY29sb3Jfc2VnJTVCLi4uJTJDJTIwJTNBJTNBLTElNUQlMjAlMjAlMjMlMjBjb252ZXJ0JTIwdG8lMjBCR1IlMEElMEFpbWclMjAlM0QlMjBucC5hcnJheShpbWFnZSklMjAqJTIwMC41JTIwJTJCJTIwY29sb3Jfc2VnJTIwKiUyMDAuNSUyMCUyMCUyMyUyMHBsb3QlMjB0aGUlMjBpbWFnZSUyMHdpdGglMjB0aGUlMjBzZWdtZW50YXRpb24lMjBtYXAlMEFpbWclMjAlM0QlMjBpbWcuYXN0eXBlKG5wLnVpbnQ4KSUwQSUwQXBsdC5maWd1cmUoZmlnc2l6ZSUzRCgxNSUyQyUyMDEwKSklMEFwbHQuaW1zaG93KGltZyklMEFwbHQuc2hvdygp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-meta">&gt;&gt;&gt; </span>color_seg = np.zeros((pred_seg.shape[<span class="hljs-number">0</span>], pred_seg.shape[<span class="hljs-number">1</span>], <span class="hljs-number">3</span>), dtype=np.uint8)
<span class="hljs-meta">&gt;&gt;&gt; </span>palette = np.array(ade_palette())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> label, color <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(palette):
<span class="hljs-meta">... </span>    color_seg[pred_seg == label, :] = color
<span class="hljs-meta">&gt;&gt;&gt; </span>color_seg = color_seg[..., ::-<span class="hljs-number">1</span>]  <span class="hljs-comment"># convert to BGR</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>img = np.array(image) * <span class="hljs-number">0.5</span> + color_seg * <span class="hljs-number">0.5</span>  <span class="hljs-comment"># plot the image with the segmentation map</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>img = img.astype(np.uint8)

<span class="hljs-meta">&gt;&gt;&gt; </span>plt.figure(figsize=(<span class="hljs-number">15</span>, <span class="hljs-number">10</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>plt.imshow(img)
<span class="hljs-meta">&gt;&gt;&gt; </span>plt.show()`,wrap:!1}}),{c(){n=M("meta"),o=i(),t=M("p"),m=i(),J(f.$$.fragment),C=i(),J(v.$$.fragment),Z=i(),J($.$$.fragment),N=i(),W=M("p"),W.textContent=Y,_=i(),I=M("p"),I.textContent=E,p=i(),w=M("ol"),w.innerHTML=X,U=i(),R=M("p"),R.textContent=b,G=i(),J(Q.$$.fragment),z=i(),L=M("p"),L.textContent=Ts,V=i(),J(S.$$.fragment),A=i(),J(D.$$.fragment),P=i(),F=M("p"),F.textContent=fs,H=i(),J(q.$$.fragment),K=i(),c=M("div"),c.innerHTML=B,ws=i(),Us=M("p"),Us.innerHTML=Aa,xe=i(),J(bs.$$.fragment),Ze=i(),Is=M("p"),Is.textContent=Ha,_e=i(),J($s.$$.fragment),Be=i(),Cs=M("p"),Cs.textContent=La,ve=i(),J(Ws.$$.fragment),Ge=i(),es=M("div"),es.innerHTML=Da,Ne=i(),ks=M("p"),ks.innerHTML=qa,Ye=i(),J(xs.$$.fragment),Re=i(),Zs=M("p"),Zs.textContent=Pa,Ee=i(),J(_s.$$.fragment),Xe=i(),Bs=M("p"),Bs.textContent=Ka,Qe=i(),J(vs.$$.fragment),Fe=i(),as=M("div"),as.innerHTML=Oa,ze=i(),Gs=M("p"),Gs.innerHTML=sl,Ve=i(),J(Ns.$$.fragment),Se=i(),Ys=M("p"),Ys.textContent=el,Ae=i(),J(Rs.$$.fragment),He=i(),Es=M("p"),Es.textContent=al,Le=i(),ls=M("div"),ls.innerHTML=ll,De=i(),Xs=M("p"),Xs.textContent=tl,qe=i(),Qs=M("p"),Qs.textContent=nl,Pe=i(),J(Fs.$$.fragment),Ke=i(),zs=M("p"),zs.textContent=pl,Oe=i(),Vs=M("ol"),Vs.innerHTML=il,sa=i(),J(ts.$$.fragment),ea=i(),J(Ss.$$.fragment),aa=i(),As=M("p"),As.textContent=rl,la=i(),J(Hs.$$.fragment),ta=i(),Ls=M("p"),Ls.innerHTML=ml,na=i(),J(Ds.$$.fragment),pa=i(),qs=M("p"),qs.textContent=ol,ia=i(),J(Ps.$$.fragment),ra=i(),Ks=M("ul"),Ks.innerHTML=cl,ma=i(),Os=M("p"),Os.innerHTML=Ml,oa=i(),J(se.$$.fragment),ca=i(),J(ee.$$.fragment),Ma=i(),ae=M("p"),ae.innerHTML=hl,ha=i(),ns=M("ol"),le=M("li"),$e=M("p"),$e.innerHTML=Jl,Fa=i(),J(te.$$.fragment),za=i(),ne=M("li"),Ce=M("p"),Ce.textContent=gl,Va=i(),J(pe.$$.fragment),Ja=i(),ie=M("p"),ie.innerHTML=yl,ga=i(),J(re.$$.fragment),ya=i(),me=M("p"),me.innerHTML=jl,ja=i(),J(oe.$$.fragment),da=i(),J(ps.$$.fragment),ua=i(),J(is.$$.fragment),Ta=i(),J(ce.$$.fragment),fa=i(),Me=M("p"),Me.innerHTML=dl,wa=i(),J(he.$$.fragment),Ua=i(),Je=M("p"),Je.innerHTML=ul,ba=i(),J(rs.$$.fragment),Ia=i(),J(ms.$$.fragment),$a=i(),ge=M("p"),ge.innerHTML=Tl,Ca=i(),J(ye.$$.fragment),Wa=i(),J(os.$$.fragment),ka=i(),J(cs.$$.fragment),xa=i(),J(je.$$.fragment),Za=i(),de=M("p"),de.textContent=fl,_a=i(),ue=M("p"),ue.textContent=wl,Ba=i(),J(Te.$$.fragment),va=i(),Ms=M("div"),Ms.innerHTML=Ul,Ga=i(),J(hs.$$.fragment),Na=i(),J(Js.$$.fragment),Ya=i(),fe=M("p"),fe.innerHTML=bl,Ra=i(),J(we.$$.fragment),Ea=i(),gs=M("div"),gs.innerHTML=Il,Xa=i(),We=M("p"),this.h()},l(s){const l=Nl("svelte-u9bgzb",document.head);n=h(l,"META",{name:!0,content:!0}),l.forEach(e),o=r(s),t=h(s,"P",{}),ke(t).forEach(e),m=r(s),g(f.$$.fragment,s),C=r(s),g(v.$$.fragment,s),Z=r(s),g($.$$.fragment,s),N=r(s),W=h(s,"P",{"data-svelte-h":!0}),T(W)!=="svelte-i2v9l4"&&(W.textContent=Y),_=r(s),I=h(s,"P",{"data-svelte-h":!0}),T(I)!=="svelte-1ad2x5z"&&(I.textContent=E),p=r(s),w=h(s,"OL",{"data-svelte-h":!0}),T(w)!=="svelte-phis9l"&&(w.innerHTML=X),U=r(s),R=h(s,"P",{"data-svelte-h":!0}),T(R)!=="svelte-1c9nexd"&&(R.textContent=b),G=r(s),g(Q.$$.fragment,s),z=r(s),L=h(s,"P",{"data-svelte-h":!0}),T(L)!=="svelte-27hn0u"&&(L.textContent=Ts),V=r(s),g(S.$$.fragment,s),A=r(s),g(D.$$.fragment,s),P=r(s),F=h(s,"P",{"data-svelte-h":!0}),T(F)!=="svelte-1uhznwp"&&(F.textContent=fs),H=r(s),g(q.$$.fragment,s),K=r(s),c=h(s,"DIV",{class:!0,"data-svelte-h":!0}),T(c)!=="svelte-xdljqs"&&(c.innerHTML=B),ws=r(s),Us=h(s,"P",{"data-svelte-h":!0}),T(Us)!=="svelte-1l32cnk"&&(Us.innerHTML=Aa),xe=r(s),g(bs.$$.fragment,s),Ze=r(s),Is=h(s,"P",{"data-svelte-h":!0}),T(Is)!=="svelte-10ezwds"&&(Is.textContent=Ha),_e=r(s),g($s.$$.fragment,s),Be=r(s),Cs=h(s,"P",{"data-svelte-h":!0}),T(Cs)!=="svelte-55zn24"&&(Cs.textContent=La),ve=r(s),g(Ws.$$.fragment,s),Ge=r(s),es=h(s,"DIV",{class:!0,"data-svelte-h":!0}),T(es)!=="svelte-v81gtv"&&(es.innerHTML=Da),Ne=r(s),ks=h(s,"P",{"data-svelte-h":!0}),T(ks)!=="svelte-ztqps9"&&(ks.innerHTML=qa),Ye=r(s),g(xs.$$.fragment,s),Re=r(s),Zs=h(s,"P",{"data-svelte-h":!0}),T(Zs)!=="svelte-iszmk4"&&(Zs.textContent=Pa),Ee=r(s),g(_s.$$.fragment,s),Xe=r(s),Bs=h(s,"P",{"data-svelte-h":!0}),T(Bs)!=="svelte-76cxem"&&(Bs.textContent=Ka),Qe=r(s),g(vs.$$.fragment,s),Fe=r(s),as=h(s,"DIV",{class:!0,"data-svelte-h":!0}),T(as)!=="svelte-9fwyws"&&(as.innerHTML=Oa),ze=r(s),Gs=h(s,"P",{"data-svelte-h":!0}),T(Gs)!=="svelte-1oc30s7"&&(Gs.innerHTML=sl),Ve=r(s),g(Ns.$$.fragment,s),Se=r(s),Ys=h(s,"P",{"data-svelte-h":!0}),T(Ys)!=="svelte-13r9j62"&&(Ys.textContent=el),Ae=r(s),g(Rs.$$.fragment,s),He=r(s),Es=h(s,"P",{"data-svelte-h":!0}),T(Es)!=="svelte-knvn6c"&&(Es.textContent=al),Le=r(s),ls=h(s,"DIV",{class:!0,"data-svelte-h":!0}),T(ls)!=="svelte-6p9cmp"&&(ls.innerHTML=ll),De=r(s),Xs=h(s,"P",{"data-svelte-h":!0}),T(Xs)!=="svelte-1j96k2e"&&(Xs.textContent=tl),qe=r(s),Qs=h(s,"P",{"data-svelte-h":!0}),T(Qs)!=="svelte-1bguxrb"&&(Qs.textContent=nl),Pe=r(s),g(Fs.$$.fragment,s),Ke=r(s),zs=h(s,"P",{"data-svelte-h":!0}),T(zs)!=="svelte-7mxw3i"&&(zs.textContent=pl),Oe=r(s),Vs=h(s,"OL",{"data-svelte-h":!0}),T(Vs)!=="svelte-ofzxxl"&&(Vs.innerHTML=il),sa=r(s),g(ts.$$.fragment,s),ea=r(s),g(Ss.$$.fragment,s),aa=r(s),As=h(s,"P",{"data-svelte-h":!0}),T(As)!=="svelte-ldhwp2"&&(As.textContent=rl),la=r(s),g(Hs.$$.fragment,s),ta=r(s),Ls=h(s,"P",{"data-svelte-h":!0}),T(Ls)!=="svelte-rugbz4"&&(Ls.innerHTML=ml),na=r(s),g(Ds.$$.fragment,s),pa=r(s),qs=h(s,"P",{"data-svelte-h":!0}),T(qs)!=="svelte-1m91ua0"&&(qs.textContent=ol),ia=r(s),g(Ps.$$.fragment,s),ra=r(s),Ks=h(s,"UL",{"data-svelte-h":!0}),T(Ks)!=="svelte-1gb3b0f"&&(Ks.innerHTML=cl),ma=r(s),Os=h(s,"P",{"data-svelte-h":!0}),T(Os)!=="svelte-j46pio"&&(Os.innerHTML=Ml),oa=r(s),g(se.$$.fragment,s),ca=r(s),g(ee.$$.fragment,s),Ma=r(s),ae=h(s,"P",{"data-svelte-h":!0}),T(ae)!=="svelte-1j7tkla"&&(ae.innerHTML=hl),ha=r(s),ns=h(s,"OL",{});var Ue=ke(ns);le=h(Ue,"LI",{});var be=ke(le);$e=h(be,"P",{"data-svelte-h":!0}),T($e)!=="svelte-137f4ce"&&($e.innerHTML=Jl),Fa=r(be),g(te.$$.fragment,be),be.forEach(e),za=r(Ue),ne=h(Ue,"LI",{});var Ie=ke(ne);Ce=h(Ie,"P",{"data-svelte-h":!0}),T(Ce)!=="svelte-1je4dsn"&&(Ce.textContent=gl),Va=r(Ie),g(pe.$$.fragment,Ie),Ie.forEach(e),Ue.forEach(e),Ja=r(s),ie=h(s,"P",{"data-svelte-h":!0}),T(ie)!=="svelte-1k7ggj5"&&(ie.innerHTML=yl),ga=r(s),g(re.$$.fragment,s),ya=r(s),me=h(s,"P",{"data-svelte-h":!0}),T(me)!=="svelte-7ebr"&&(me.innerHTML=jl),ja=r(s),g(oe.$$.fragment,s),da=r(s),g(ps.$$.fragment,s),ua=r(s),g(is.$$.fragment,s),Ta=r(s),g(ce.$$.fragment,s),fa=r(s),Me=h(s,"P",{"data-svelte-h":!0}),T(Me)!=="svelte-vg2xml"&&(Me.innerHTML=dl),wa=r(s),g(he.$$.fragment,s),Ua=r(s),Je=h(s,"P",{"data-svelte-h":!0}),T(Je)!=="svelte-aq72n8"&&(Je.innerHTML=ul),ba=r(s),g(rs.$$.fragment,s),Ia=r(s),g(ms.$$.fragment,s),$a=r(s),ge=h(s,"P",{"data-svelte-h":!0}),T(ge)!=="svelte-183aynn"&&(ge.innerHTML=Tl),Ca=r(s),g(ye.$$.fragment,s),Wa=r(s),g(os.$$.fragment,s),ka=r(s),g(cs.$$.fragment,s),xa=r(s),g(je.$$.fragment,s),Za=r(s),de=h(s,"P",{"data-svelte-h":!0}),T(de)!=="svelte-633ppb"&&(de.textContent=fl),_a=r(s),ue=h(s,"P",{"data-svelte-h":!0}),T(ue)!=="svelte-1g0hugc"&&(ue.textContent=wl),Ba=r(s),g(Te.$$.fragment,s),va=r(s),Ms=h(s,"DIV",{class:!0,"data-svelte-h":!0}),T(Ms)!=="svelte-11jfm1f"&&(Ms.innerHTML=Ul),Ga=r(s),g(hs.$$.fragment,s),Na=r(s),g(Js.$$.fragment,s),Ya=r(s),fe=h(s,"P",{"data-svelte-h":!0}),T(fe)!=="svelte-1ng77tx"&&(fe.innerHTML=bl),Ra=r(s),g(we.$$.fragment,s),Ea=r(s),gs=h(s,"DIV",{class:!0,"data-svelte-h":!0}),T(gs)!=="svelte-nsecok"&&(gs.innerHTML=Il),Xa=r(s),We=h(s,"P",{}),ke(We).forEach(e),this.h()},h(){ys(n,"name","hf:doc:metadata"),ys(n,"content",it),ys(c,"class","flex justify-center"),ys(es,"class","flex justify-center"),ys(as,"class","flex justify-center"),ys(ls,"class","flex justify-center"),ys(Ms,"class","flex justify-center"),ys(gs,"class","flex justify-center")},m(s,l){js(document.head,n),a(s,o,l),a(s,t,l),a(s,m,l),y(f,s,l),a(s,C,l),y(v,s,l),a(s,Z,l),y($,s,l),a(s,N,l),a(s,W,l),a(s,_,l),a(s,I,l),a(s,p,l),a(s,w,l),a(s,U,l),a(s,R,l),a(s,G,l),y(Q,s,l),a(s,z,l),a(s,L,l),a(s,V,l),y(S,s,l),a(s,A,l),y(D,s,l),a(s,P,l),a(s,F,l),a(s,H,l),y(q,s,l),a(s,K,l),a(s,c,l),a(s,ws,l),a(s,Us,l),a(s,xe,l),y(bs,s,l),a(s,Ze,l),a(s,Is,l),a(s,_e,l),y($s,s,l),a(s,Be,l),a(s,Cs,l),a(s,ve,l),y(Ws,s,l),a(s,Ge,l),a(s,es,l),a(s,Ne,l),a(s,ks,l),a(s,Ye,l),y(xs,s,l),a(s,Re,l),a(s,Zs,l),a(s,Ee,l),y(_s,s,l),a(s,Xe,l),a(s,Bs,l),a(s,Qe,l),y(vs,s,l),a(s,Fe,l),a(s,as,l),a(s,ze,l),a(s,Gs,l),a(s,Ve,l),y(Ns,s,l),a(s,Se,l),a(s,Ys,l),a(s,Ae,l),y(Rs,s,l),a(s,He,l),a(s,Es,l),a(s,Le,l),a(s,ls,l),a(s,De,l),a(s,Xs,l),a(s,qe,l),a(s,Qs,l),a(s,Pe,l),y(Fs,s,l),a(s,Ke,l),a(s,zs,l),a(s,Oe,l),a(s,Vs,l),a(s,sa,l),y(ts,s,l),a(s,ea,l),y(Ss,s,l),a(s,aa,l),a(s,As,l),a(s,la,l),y(Hs,s,l),a(s,ta,l),a(s,Ls,l),a(s,na,l),y(Ds,s,l),a(s,pa,l),a(s,qs,l),a(s,ia,l),y(Ps,s,l),a(s,ra,l),a(s,Ks,l),a(s,ma,l),a(s,Os,l),a(s,oa,l),y(se,s,l),a(s,ca,l),y(ee,s,l),a(s,Ma,l),a(s,ae,l),a(s,ha,l),a(s,ns,l),js(ns,le),js(le,$e),js(le,Fa),y(te,le,null),js(ns,za),js(ns,ne),js(ne,Ce),js(ne,Va),y(pe,ne,null),a(s,Ja,l),a(s,ie,l),a(s,ga,l),y(re,s,l),a(s,ya,l),a(s,me,l),a(s,ja,l),y(oe,s,l),a(s,da,l),y(ps,s,l),a(s,ua,l),y(is,s,l),a(s,Ta,l),y(ce,s,l),a(s,fa,l),a(s,Me,l),a(s,wa,l),y(he,s,l),a(s,Ua,l),a(s,Je,l),a(s,ba,l),y(rs,s,l),a(s,Ia,l),y(ms,s,l),a(s,$a,l),a(s,ge,l),a(s,Ca,l),y(ye,s,l),a(s,Wa,l),y(os,s,l),a(s,ka,l),y(cs,s,l),a(s,xa,l),y(je,s,l),a(s,Za,l),a(s,de,l),a(s,_a,l),a(s,ue,l),a(s,Ba,l),y(Te,s,l),a(s,va,l),a(s,Ms,l),a(s,Ga,l),y(hs,s,l),a(s,Na,l),y(Js,s,l),a(s,Ya,l),a(s,fe,l),a(s,Ra,l),y(we,s,l),a(s,Ea,l),a(s,gs,l),a(s,Xa,l),a(s,We,l),Qa=!0},p(s,[l]){const Ue={};l&2&&(Ue.$$scope={dirty:l,ctx:s}),ts.$set(Ue);const be={};l&2&&(be.$$scope={dirty:l,ctx:s}),ps.$set(be);const Ie={};l&2&&(Ie.$$scope={dirty:l,ctx:s}),is.$set(Ie);const $l={};l&2&&($l.$$scope={dirty:l,ctx:s}),rs.$set($l);const Cl={};l&2&&(Cl.$$scope={dirty:l,ctx:s}),ms.$set(Cl);const Wl={};l&2&&(Wl.$$scope={dirty:l,ctx:s}),os.$set(Wl);const kl={};l&2&&(kl.$$scope={dirty:l,ctx:s}),cs.$set(kl);const xl={};l&2&&(xl.$$scope={dirty:l,ctx:s}),hs.$set(xl);const Zl={};l&2&&(Zl.$$scope={dirty:l,ctx:s}),Js.$set(Zl)},i(s){Qa||(j(f.$$.fragment,s),j(v.$$.fragment,s),j($.$$.fragment,s),j(Q.$$.fragment,s),j(S.$$.fragment,s),j(D.$$.fragment,s),j(q.$$.fragment,s),j(bs.$$.fragment,s),j($s.$$.fragment,s),j(Ws.$$.fragment,s),j(xs.$$.fragment,s),j(_s.$$.fragment,s),j(vs.$$.fragment,s),j(Ns.$$.fragment,s),j(Rs.$$.fragment,s),j(Fs.$$.fragment,s),j(ts.$$.fragment,s),j(Ss.$$.fragment,s),j(Hs.$$.fragment,s),j(Ds.$$.fragment,s),j(Ps.$$.fragment,s),j(se.$$.fragment,s),j(ee.$$.fragment,s),j(te.$$.fragment,s),j(pe.$$.fragment,s),j(re.$$.fragment,s),j(oe.$$.fragment,s),j(ps.$$.fragment,s),j(is.$$.fragment,s),j(ce.$$.fragment,s),j(he.$$.fragment,s),j(rs.$$.fragment,s),j(ms.$$.fragment,s),j(ye.$$.fragment,s),j(os.$$.fragment,s),j(cs.$$.fragment,s),j(je.$$.fragment,s),j(Te.$$.fragment,s),j(hs.$$.fragment,s),j(Js.$$.fragment,s),j(we.$$.fragment,s),Qa=!0)},o(s){d(f.$$.fragment,s),d(v.$$.fragment,s),d($.$$.fragment,s),d(Q.$$.fragment,s),d(S.$$.fragment,s),d(D.$$.fragment,s),d(q.$$.fragment,s),d(bs.$$.fragment,s),d($s.$$.fragment,s),d(Ws.$$.fragment,s),d(xs.$$.fragment,s),d(_s.$$.fragment,s),d(vs.$$.fragment,s),d(Ns.$$.fragment,s),d(Rs.$$.fragment,s),d(Fs.$$.fragment,s),d(ts.$$.fragment,s),d(Ss.$$.fragment,s),d(Hs.$$.fragment,s),d(Ds.$$.fragment,s),d(Ps.$$.fragment,s),d(se.$$.fragment,s),d(ee.$$.fragment,s),d(te.$$.fragment,s),d(pe.$$.fragment,s),d(re.$$.fragment,s),d(oe.$$.fragment,s),d(ps.$$.fragment,s),d(is.$$.fragment,s),d(ce.$$.fragment,s),d(he.$$.fragment,s),d(rs.$$.fragment,s),d(ms.$$.fragment,s),d(ye.$$.fragment,s),d(os.$$.fragment,s),d(cs.$$.fragment,s),d(je.$$.fragment,s),d(Te.$$.fragment,s),d(hs.$$.fragment,s),d(Js.$$.fragment,s),d(we.$$.fragment,s),Qa=!1},d(s){s&&(e(o),e(t),e(m),e(C),e(Z),e(N),e(W),e(_),e(I),e(p),e(w),e(U),e(R),e(G),e(z),e(L),e(V),e(A),e(P),e(F),e(H),e(K),e(c),e(ws),e(Us),e(xe),e(Ze),e(Is),e(_e),e(Be),e(Cs),e(ve),e(Ge),e(es),e(Ne),e(ks),e(Ye),e(Re),e(Zs),e(Ee),e(Xe),e(Bs),e(Qe),e(Fe),e(as),e(ze),e(Gs),e(Ve),e(Se),e(Ys),e(Ae),e(He),e(Es),e(Le),e(ls),e(De),e(Xs),e(qe),e(Qs),e(Pe),e(Ke),e(zs),e(Oe),e(Vs),e(sa),e(ea),e(aa),e(As),e(la),e(ta),e(Ls),e(na),e(pa),e(qs),e(ia),e(ra),e(Ks),e(ma),e(Os),e(oa),e(ca),e(Ma),e(ae),e(ha),e(ns),e(Ja),e(ie),e(ga),e(ya),e(me),e(ja),e(da),e(ua),e(Ta),e(fa),e(Me),e(wa),e(Ua),e(Je),e(ba),e(Ia),e($a),e(ge),e(Ca),e(Wa),e(ka),e(xa),e(Za),e(de),e(_a),e(ue),e(Ba),e(va),e(Ms),e(Ga),e(Na),e(Ya),e(fe),e(Ra),e(Ea),e(gs),e(Xa),e(We)),e(n),u(f,s),u(v,s),u($,s),u(Q,s),u(S,s),u(D,s),u(q,s),u(bs,s),u($s,s),u(Ws,s),u(xs,s),u(_s,s),u(vs,s),u(Ns,s),u(Rs,s),u(Fs,s),u(ts,s),u(Ss,s),u(Hs,s),u(Ds,s),u(Ps,s),u(se,s),u(ee,s),u(te),u(pe),u(re,s),u(oe,s),u(ps,s),u(is,s),u(ce,s),u(he,s),u(rs,s),u(ms,s),u(ye,s),u(os,s),u(cs,s),u(je,s),u(Te,s),u(hs,s),u(Js,s),u(we,s)}}}const it='{"title":"Image Segmentation","local":"image-segmentation","sections":[{"title":"Types of Segmentation","local":"types-of-segmentation","sections":[],"depth":2},{"title":"Fine-tuning a Model for Segmentation","local":"fine-tuning-a-model-for-segmentation","sections":[{"title":"Load SceneParse150 dataset","local":"load-sceneparse150-dataset","sections":[{"title":"Custom dataset","local":"custom-dataset","sections":[],"depth":4}],"depth":3},{"title":"Preprocess","local":"preprocess","sections":[],"depth":3},{"title":"Evaluate","local":"evaluate","sections":[],"depth":3},{"title":"Train","local":"train","sections":[],"depth":3},{"title":"Inference","local":"inference","sections":[],"depth":3}],"depth":2}],"depth":1}';function rt(x){return Bl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jt extends vl{constructor(n){super(),Gl(this,n,rt,pt,_l,{})}}export{jt as component};
