import{s as er,o as tr,n as G}from"../chunks/scheduler.9bc65507.js";import{S as nr,i as or,g as i,s as o,r as u,A as ar,h as l,f as s,c as a,j as $,u as h,x as m,k as L,y as t,a as d,v as f,d as g,t as _,w as b}from"../chunks/index.707bf1b6.js";import{T as $e}from"../chunks/Tip.c2ecdbf4.js";import{D as x}from"../chunks/Docstring.17db21ae.js";import{C as Le}from"../chunks/CodeBlock.54a9f38d.js";import{E as Cn}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as Ks}from"../chunks/PipelineTag.44585822.js";import{H as D}from"../chunks/Heading.342b1fa6.js";function sr(v){let r,y;return r=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMExsYW1hTW9kZWwlMkMlMjBMbGFtYUNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBMTGFNQSUyMGxsYW1hLTdiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMExsYW1hQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjBmcm9tJTIwdGhlJTIwbGxhbWEtN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMExsYW1hTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlamaModel, LlamaConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a LLaMA llama-7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = LlamaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the llama-7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = LlamaModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){u(r.$$.fragment)},l(c){h(r.$$.fragment,c)},m(c,k){f(r,c,k),y=!0},p:G,i(c){y||(g(r.$$.fragment,c),y=!0)},o(c){_(r.$$.fragment,c),y=!1},d(c){b(r,c)}}}function rr(v){let r,y="sequence pair mask has the following format:",c,k,T;return k=new Le({props:{code:"MCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMEElN0MlMjBmaXJzdCUyMHNlcXVlbmNlJTIwJTIwJTIwJTIwJTdDJTIwc2Vjb25kJTIwc2VxdWVuY2UlMjAlN0M=",highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`,wrap:!1}}),{c(){r=i("p"),r.textContent=y,c=o(),u(k.$$.fragment)},l(p){r=l(p,"P",{"data-svelte-h":!0}),m(r)!=="svelte-16klr56"&&(r.textContent=y),c=a(p),h(k.$$.fragment,p)},m(p,z){d(p,r,z),d(p,c,z),f(k,p,z),T=!0},p:G,i(p){T||(g(k.$$.fragment,p),T=!0)},o(p){_(k.$$.fragment,p),T=!1},d(p){p&&(s(r),s(c)),b(k,p)}}}function ir(v){let r,y;return r=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMExsYW1hVG9rZW5pemVyRmFzdCUwQSUwQXRva2VuaXplciUyMCUzRCUyMExsYW1hVG9rZW5pemVyRmFzdC5mcm9tX3ByZXRyYWluZWQoJTIyaGYtaW50ZXJuYWwtdGVzdGluZyUyRmxsYW1hLXRva2VuaXplciUyMiklMEF0b2tlbml6ZXIuZW5jb2RlKCUyMkhlbGxvJTIwdGhpcyUyMGlzJTIwYSUyMHRlc3QlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlamaTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = LlamaTokenizerFast.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/llama-tokenizer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.encode(<span class="hljs-string">&quot;Hello this is a test&quot;</span>)
[<span class="hljs-number">1</span>, <span class="hljs-number">15043</span>, <span class="hljs-number">445</span>, <span class="hljs-number">338</span>, <span class="hljs-number">263</span>, <span class="hljs-number">1243</span>]`,wrap:!1}}),{c(){u(r.$$.fragment)},l(c){h(r.$$.fragment,c)},m(c,k){f(r,c,k),y=!0},p:G,i(c){y||(g(r.$$.fragment,c),y=!0)},o(c){_(r.$$.fragment,c),y=!1},d(c){b(r,c)}}}function lr(v){let r,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=y},l(c){r=l(c,"P",{"data-svelte-h":!0}),m(r)!=="svelte-fincs2"&&(r.innerHTML=y)},m(c,k){d(c,r,k)},p:G,d(c){c&&s(r)}}}function dr(v){let r,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=y},l(c){r=l(c,"P",{"data-svelte-h":!0}),m(r)!=="svelte-fincs2"&&(r.innerHTML=y)},m(c,k){d(c,r,k)},p:G,d(c){c&&s(r)}}}function cr(v){let r,y="Example:",c,k,T;return k=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBMbGFtYUZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBMbGFtYUZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1oZiUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1oZiUyMiklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJIZXklMkMlMjBhcmUlMjB5b3UlMjBjb25zY2lvdXMlM0YlMjBDYW4lMjB5b3UlMjB0YWxrJTIwdG8lMjBtZSUzRiUyMiUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQSUyMyUyMEdlbmVyYXRlJTBBZ2VuZXJhdGVfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoaW5wdXRzLmlucHV0X2lkcyUyQyUyMG1heF9sZW5ndGglM0QzMCklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlJTJDJTIwY2xlYW5fdXBfdG9rZW5pemF0aW9uX3NwYWNlcyUzREZhbHNlKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, LlamaForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = LlamaForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?\\nI&#x27;m not conscious, but I can talk to you.&quot;</span>`,wrap:!1}}),{c(){r=i("p"),r.textContent=y,c=o(),u(k.$$.fragment)},l(p){r=l(p,"P",{"data-svelte-h":!0}),m(r)!=="svelte-11lpom8"&&(r.textContent=y),c=a(p),h(k.$$.fragment,p)},m(p,z){d(p,r,z),d(p,c,z),f(k,p,z),T=!0},p:G,i(p){T||(g(k.$$.fragment,p),T=!0)},o(p){_(k.$$.fragment,p),T=!1},d(p){p&&(s(r),s(c)),b(k,p)}}}function mr(v){let r,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=y},l(c){r=l(c,"P",{"data-svelte-h":!0}),m(r)!=="svelte-fincs2"&&(r.innerHTML=y)},m(c,k){d(c,r,k)},p:G,d(c){c&&s(r)}}}function pr(v){let r,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=y},l(c){r=l(c,"P",{"data-svelte-h":!0}),m(r)!=="svelte-fincs2"&&(r.innerHTML=y)},m(c,k){d(c,r,k)},p:G,d(c){c&&s(r)}}}function ur(v){let r,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=y},l(c){r=l(c,"P",{"data-svelte-h":!0}),m(r)!=="svelte-fincs2"&&(r.innerHTML=y)},m(c,k){d(c,r,k)},p:G,d(c){c&&s(r)}}}function hr(v){let r,y=`This example uses a random model as the real ones are all very big. To get proper results, you should use
openlm-research/open_llama_3b_v2 instead of afmck/testing-llama-tiny. If you get out-of-memory when loading that checkpoint, you can try
adding <code>device_map=&quot;auto&quot;</code> in the <code>from_pretrained</code> call.`;return{c(){r=i("p"),r.innerHTML=y},l(c){r=l(c,"P",{"data-svelte-h":!0}),m(r)!=="svelte-29cnl4"&&(r.innerHTML=y)},m(c,k){d(c,r,k)},p:G,d(c){c&&s(r)}}}function fr(v){let r,y="Example:",c,k,T;return k=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBGbGF4TGxhbWFNb2RlbCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmFmbWNrJTJGdGVzdGluZy1sbGFtYS10aW55JTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheExsYW1hTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmFmbWNrJTJGdGVzdGluZy1sbGFtYS10aW55JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJqYXglMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGU=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxLlamaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;afmck/testing-llama-tiny&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxLlamaModel.from_pretrained(<span class="hljs-string">&quot;afmck/testing-llama-tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){r=i("p"),r.textContent=y,c=o(),u(k.$$.fragment)},l(p){r=l(p,"P",{"data-svelte-h":!0}),m(r)!=="svelte-11lpom8"&&(r.textContent=y),c=a(p),h(k.$$.fragment,p)},m(p,z){d(p,r,z),d(p,c,z),f(k,p,z),T=!0},p:G,i(p){T||(g(k.$$.fragment,p),T=!0)},o(p){_(k.$$.fragment,p),T=!1},d(p){p&&(s(r),s(c)),b(k,p)}}}function gr(v){let r,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){r=i("p"),r.innerHTML=y},l(c){r=l(c,"P",{"data-svelte-h":!0}),m(r)!=="svelte-fincs2"&&(r.innerHTML=y)},m(c,k){d(c,r,k)},p:G,d(c){c&&s(r)}}}function _r(v){let r,y=`This example uses a random model as the real ones are all very big. To get proper results, you should use
openlm-research/open_llama_3b_v2 instead of afmck/testing-llama-tiny. If you get out-of-memory when loading that checkpoint, you can try
adding <code>device_map=&quot;auto&quot;</code> in the <code>from_pretrained</code> call.`;return{c(){r=i("p"),r.innerHTML=y},l(c){r=l(c,"P",{"data-svelte-h":!0}),m(r)!=="svelte-29cnl4"&&(r.innerHTML=y)},m(c,k){d(c,r,k)},p:G,d(c){c&&s(r)}}}function br(v){let r,y="Example:",c,k,T;return k=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBGbGF4TGxhbWFGb3JDYXVzYWxMTSUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmFmbWNrJTJGdGVzdGluZy1sbGFtYS10aW55JTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheExsYW1hRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmFmbWNrJTJGdGVzdGluZy1sbGFtYS10aW55JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJucCUyMiklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwcmV0cmlldmUlMjBsb2d0cyUyMGZvciUyMG5leHQlMjB0b2tlbiUwQW5leHRfdG9rZW5fbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlNUIlM0ElMkMlMjAtMSU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxLlamaForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;afmck/testing-llama-tiny&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxLlamaForCausalLM.from_pretrained(<span class="hljs-string">&quot;afmck/testing-llama-tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve logts for next token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>next_token_logits = outputs.logits[:, -<span class="hljs-number">1</span>]`,wrap:!1}}),{c(){r=i("p"),r.textContent=y,c=o(),u(k.$$.fragment)},l(p){r=l(p,"P",{"data-svelte-h":!0}),m(r)!=="svelte-11lpom8"&&(r.textContent=y),c=a(p),h(k.$$.fragment,p)},m(p,z){d(p,r,z),d(p,c,z),f(k,p,z),T=!0},p:G,i(p){T||(g(k.$$.fragment,p),T=!0)},o(p){_(k.$$.fragment,p),T=!1},d(p){p&&(s(r),s(c)),b(k,p)}}}function kr(v){let r,y,c,k,T,p,z,In,Me,Va='The LLaMA model was proposed in <a href="https://arxiv.org/abs/2302.13971" rel="nofollow">LLaMA: Open and Efficient Foundation Language Models</a> by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample. It is a collection of foundation language models ranging from 7B to 65B parameters.',Un,xe,Za="The abstract from the paper is the following:",An,ze,Qa="<em>We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.</em>",Wn,Ce,Ra='This model was contributed by <a href="https://huggingface.co/zphang" rel="nofollow">zphang</a> with contributions from <a href="https://huggingface.co/BlackSamorez" rel="nofollow">BlackSamorez</a>. The code of the implementation in Hugging Face is based on GPT-NeoX <a href="https://github.com/EleutherAI/gpt-neox" rel="nofollow">here</a>. The original code of the authors can be found <a href="https://github.com/facebookresearch/llama" rel="nofollow">here</a>.',Hn,Fe,Jn,Pe,Ba='<li>Weights for the LLaMA models can be obtained from by filling out <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form" rel="nofollow">this form</a></li> <li>After downloading the weights, they will need to be converted to the Hugging Face Transformers format using the <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py" rel="nofollow">conversion script</a>. The script can be called with the following (example) command:</li>',Sn,je,Gn,qe,Oa="<li>After conversion, the model and tokenizer can be loaded via:</li>",Nn,Ie,En,Ue,Xa=`Note that executing the script requires enough CPU RAM to host the whole model in float16 precision (even if the biggest versions
come in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM). For the 65B model, it‚Äôs thus 130GB of RAM needed.`,Dn,Ae,Ya='<li>The LLaMA tokenizer is a BPE model based on <a href="https://github.com/google/sentencepiece" rel="nofollow">sentencepiece</a>. One quirk of sentencepiece is that when decoding a sequence, if the first token is the start of the word (e.g. ‚ÄúBanana‚Äù), the tokenizer does not prepend the prefix space to the string.</li>',Vn,We,Ka='This model was contributed by <a href="https://huggingface.co/zphang" rel="nofollow">zphang</a> with contributions from <a href="https://huggingface.co/BlackSamorez" rel="nofollow">BlackSamorez</a>. The code of the implementation in Hugging Face is based on GPT-NeoX <a href="https://github.com/EleutherAI/gpt-neox" rel="nofollow">here</a>. The original code of the authors can be found <a href="https://github.com/facebookresearch/llama" rel="nofollow">here</a>. The Flax version of the implementation was contributed by <a href="https://huggingface.co/afmck" rel="nofollow">afmck</a> with the code in the implementation based on Hugging Face‚Äôs Flax GPT-Neo.',Zn,He,es="Based on the original LLaMA model, Meta AI has released some follow-up works:",Qn,Je,ts='<li><strong>Llama2</strong>: Llama2 is an improved version of Llama with some architectural tweaks (Grouped Query Attention), and is pre-trained on 2Trillion tokens. Refer to the documentation of Llama2 which can be found <a href="llama2">here</a>.</li>',Rn,Se,Bn,Ge,ns="A list of official Hugging Face and community (indicated by üåé) resources to help you get started with LLaMA. If you‚Äôre interested in submitting a resource to be included here, please feel free to open a Pull Request and we‚Äôll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",On,Ne,Xn,Ee,os='<li>A <a href="https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb#scrollTo=f04ba4d2" rel="nofollow">notebook</a> on how to use prompt tuning to adapt the LLaMA model for text classification task. üåé</li>',Yn,De,Kn,Ve,as='<li><a href="https://huggingface.co/blog/stackllama#stackllama-a-hands-on-guide-to-train-llama-with-rlhf" rel="nofollow">StackLLaMA: A hands-on guide to train LLaMA with RLHF</a>, a blog post about how to train LLaMA to answer questions on <a href="https://stackexchange.com/" rel="nofollow">Stack Exchange</a> with RLHF.</li>',eo,Ze,ss="‚öóÔ∏è Optimization",to,Qe,rs='<li>A <a href="https://colab.research.google.com/drive/1SQUXq1AMZPSLD4mk3A3swUIc6Y2dclme?usp=sharing" rel="nofollow">notebook</a> on how to fine-tune LLaMA model using xturing library on GPU which has limited memory. üåé</li>',no,Re,is="‚ö°Ô∏è Inference",oo,Be,ls='<li>A <a href="https://colab.research.google.com/github/DominguesM/alpaca-lora-ptbr-7b/blob/main/notebooks/02%20-%20Evaluate.ipynb" rel="nofollow">notebook</a> on how to run the LLaMA Model using PeftModel from the ü§ó PEFT library. üåé</li> <li>A <a href="https://colab.research.google.com/drive/1l2GiSSPbajVyp2Nk3CFT4t3uH6-5TiBe?usp=sharing" rel="nofollow">notebook</a> on how to load a PEFT adapter LLaMA model with LangChain. üåé</li>',ao,Oe,ds="üöÄ Deploy",so,Xe,cs='<li>A <a href="https://colab.research.google.com/github/lxe/simple-llama-finetuner/blob/master/Simple_LLaMA_FineTuner.ipynb#scrollTo=3PM_DilAZD8T" rel="nofollow">notebook</a> on how to fine-tune LLaMA model using LoRA method via the ü§ó PEFT library with intuitive UI. üåé</li> <li>A <a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb" rel="nofollow">notebook</a> on how to deploy Open-LLaMA model for text generation on Amazon SageMaker. üåé</li>',ro,Ye,io,N,Ke,zo,Ut,ms=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel">LlamaModel</a>. It is used to instantiate an LLaMA
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the LLaMA-7B.`,Co,At,ps=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Fo,se,lo,et,co,j,tt,Po,Wt,us=`Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is
no padding token in the original model.`,jo,Ht,nt,qo,re,ot,Io,Jt,hs=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Uo,V,at,Ao,St,fs="Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT",Wo,ie,Ho,Gt,gs="if token_ids_1 is None, only returns the first portion of the mask (0s).",Jo,le,st,So,Nt,_s="Save the vocabulary and special tokens file to a directory.",mo,rt,po,w,it,Go,Et,bs="Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding.",No,Dt,ks="This uses notably ByteFallback and no normalization.",Eo,de,Do,Vt,ys=`If you want to change the <code>bos_token</code> or the <code>eos_token</code>, make sure to specify them when initializing the model, or
call <code>tokenizer.update_post_processor()</code> to make sure that the post-processing is correctly done (otherwise the
values of the first token and final token of an encoded sequence will not be correct). For more details, checkout
[post-processors] (<a href="https://huggingface.co/docs/tokenizers/api/post-processors" rel="nofollow">https://huggingface.co/docs/tokenizers/api/post-processors</a>) documentation.`,Vo,Zt,vs=`This tokenizer inherits from <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`,Zo,Qt,lt,Qo,ce,dt,Ro,Rt,Ts=`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> or <code>encode_plus</code> methods.`,Bo,B,ct,Oo,Bt,ws=`Create the token type IDs corresponding to the sequences passed. <a href="../glossary#token-type-ids">What are token type
IDs?</a>`,Xo,Ot,$s="Should be overridden in a subclass if the model has a special way of building those.",Yo,me,mt,Ko,Xt,Ls="Updates the underlying post processor with the current <code>bos_token</code> and <code>eos_token</code>.",ea,Yt,pt,uo,ut,ho,q,ht,ta,Kt,Ms=`The bare LLaMA Model outputting raw hidden-states without any specific head on top.
This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,na,en,xs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,oa,tn,zs="Transformer decoder consisting of <em>config.num_hidden_layers</em> layers. Each layer is a <code>LlamaDecoderLayer</code>",aa,O,ft,sa,nn,Cs='The <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel">LlamaModel</a> forward method, overrides the <code>__call__</code> special method.',ra,pe,fo,gt,go,ne,_t,ia,Z,bt,la,on,Fs='The <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForCausalLM">LlamaForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',da,ue,ca,he,_o,kt,bo,C,yt,ma,an,Ps="The LLaMa Model transformer with a sequence classification head on top (linear layer).",pa,sn,js=`<a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForSequenceClassification">LlamaForSequenceClassification</a> uses the last token in order to do the classification, as other causal models
(e.g. GPT-2) do.`,ua,rn,qs=`Since it does classification on the last token, it requires to know the position of the last token. If a
<code>pad_token_id</code> is defined in the configuration, it finds the last token that is not a padding token in each row. If
no <code>pad_token_id</code> is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when <code>inputs_embeds</code> are passed instead of <code>input_ids</code>, it does the same (take the last value in
each row of the batch).`,ha,ln,Is=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,fa,dn,Us=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ga,X,vt,_a,cn,As='The <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForSequenceClassification">LlamaForSequenceClassification</a> forward method, overrides the <code>__call__</code> special method.',ba,fe,ko,Tt,yo,I,wt,ka,mn,Ws=`The Llama Model transformer with a span classification head on top for extractive question-answering tasks like
SQuAD (a linear layer on top of the hidden-states output to compute <code>span start logits</code> and <code>span end logits</code>).`,ya,pn,Hs=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,va,un,Js=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ta,Y,$t,wa,hn,Ss='The <a href="/docs/transformers/main/en/model_doc/llama#transformers.LlamaForQuestionAnswering">LlamaForQuestionAnswering</a> forward method, overrides the <code>__call__</code> special method.',$a,ge,vo,Lt,To,F,Mt,La,fn,Gs="The bare Llama Model transformer outputting raw hidden-states without any specific head on top.",Ma,gn,Ns=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,xa,_n,Es=`This model is also a Flax Linen
<a href="https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html" rel="nofollow">flax.nn.Module</a> subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`,za,bn,Ds="Finally, this model supports inherent JAX features such as:",Ca,kn,Vs='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',Fa,A,xt,Pa,yn,Zs="The <code>FlaxLlamaPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",ja,_e,qa,be,Ia,ke,wo,zt,$o,P,Ct,Ua,vn,Qs="The Llama Model transformer with a language modeling head (linear layer) on top.",Aa,Tn,Rs=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Wa,wn,Bs=`This model is also a Flax Linen
<a href="https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html" rel="nofollow">flax.nn.Module</a> subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`,Ha,$n,Os="Finally, this model supports inherent JAX features such as:",Ja,Ln,Xs='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',Sa,W,Ft,Ga,Mn,Ys="The <code>FlaxLlamaPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",Na,ye,Ea,ve,Da,Te,Lo,Fn,Mo;return T=new D({props:{title:"LLaMA",local:"llama",headingTag:"h1"}}),z=new D({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Fe=new D({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),je=new Le({props:{code:"cHl0aG9uJTIwc3JjJTJGdHJhbnNmb3JtZXJzJTJGbW9kZWxzJTJGbGxhbWElMkZjb252ZXJ0X2xsYW1hX3dlaWdodHNfdG9faGYucHklMjAlNUMlMEElMjAlMjAlMjAlMjAtLWlucHV0X2RpciUyMCUyRnBhdGglMkZ0byUyRmRvd25sb2FkZWQlMkZsbGFtYSUyRndlaWdodHMlMjAtLW1vZGVsX3NpemUlMjA3QiUyMC0tb3V0cHV0X2RpciUyMCUyRm91dHB1dCUyRnBhdGg=",highlighted:`python src/transformers/models/llama/convert_llama_weights_to_hf.py \\
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path`,wrap:!1}}),Ie=new Le({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMExsYW1hRm9yQ2F1c2FsTE0lMkMlMjBMbGFtYVRva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMExsYW1hVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjIlMkZvdXRwdXQlMkZwYXRoJTIyKSUwQW1vZGVsJTIwJTNEJTIwTGxhbWFGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyJTJGb3V0cHV0JTJGcGF0aCUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained(<span class="hljs-string">&quot;/output/path&quot;</span>)
model = LlamaForCausalLM.from_pretrained(<span class="hljs-string">&quot;/output/path&quot;</span>)`,wrap:!1}}),Se=new D({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Ne=new Ks({props:{pipeline:"text-classification"}}),De=new Ks({props:{pipeline:"question-answering"}}),Ye=new D({props:{title:"LlamaConfig",local:"transformers.LlamaConfig",headingTag:"h2"}}),Ke=new x({props:{name:"class transformers.LlamaConfig",anchor:"transformers.LlamaConfig",parameters:[{name:"vocab_size",val:" = 32000"},{name:"hidden_size",val:" = 4096"},{name:"intermediate_size",val:" = 11008"},{name:"num_hidden_layers",val:" = 32"},{name:"num_attention_heads",val:" = 32"},{name:"num_key_value_heads",val:" = None"},{name:"hidden_act",val:" = 'silu'"},{name:"max_position_embeddings",val:" = 2048"},{name:"initializer_range",val:" = 0.02"},{name:"rms_norm_eps",val:" = 1e-06"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = None"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"pretraining_tp",val:" = 1"},{name:"tie_word_embeddings",val:" = False"},{name:"rope_theta",val:" = 10000.0"},{name:"rope_scaling",val:" = None"},{name:"attention_bias",val:" = False"},{name:"attention_dropout",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LlamaConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel">LlamaModel</a>`,name:"vocab_size"},{anchor:"transformers.LlamaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimension of the hidden representations.`,name:"hidden_size"},{anchor:"transformers.LlamaConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 11008) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.LlamaConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the Transformer decoder.`,name:"num_hidden_layers"},{anchor:"transformers.LlamaConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"num_attention_heads"},{anchor:"transformers.LlamaConfig.num_key_value_heads",description:`<strong>num_key_value_heads</strong> (<code>int</code>, <em>optional</em>) &#x2014;
This is the number of key_value heads that should be used to implement Grouped Query Attention. If
<code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if
<code>num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group. For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to </code>num_attention_heads\`.`,name:"num_key_value_heads"},{anchor:"transformers.LlamaConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the decoder.`,name:"hidden_act"},{anchor:"transformers.LlamaConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,
Llama 2 up to 4096, CodeLlama up to 16384.`,name:"max_position_embeddings"},{anchor:"transformers.LlamaConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.LlamaConfig.rms_norm_eps",description:`<strong>rms_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the rms normalization layers.`,name:"rms_norm_eps"},{anchor:"transformers.LlamaConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.LlamaConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.LlamaConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.LlamaConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.LlamaConfig.pretraining_tp",description:`<strong>pretraining_tp</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Experimental feature. Tensor parallelism rank used during pretraining. Please refer to <a href="https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism" rel="nofollow">this
document</a> to understand more about it. This value is
necessary to ensure exact reproducibility of the pretraining results. Please refer to <a href="https://github.com/pytorch/pytorch/issues/76232" rel="nofollow">this
issue</a>.`,name:"pretraining_tp"},{anchor:"transformers.LlamaConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to tie weight embeddings`,name:"tie_word_embeddings"},{anchor:"transformers.LlamaConfig.rope_theta",description:`<strong>rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 10000.0) &#x2014;
The base period of the RoPE embeddings.`,name:"rope_theta"},{anchor:"transformers.LlamaConfig.rope_scaling",description:`<strong>rope_scaling</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling
strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is
<code>{&quot;type&quot;: strategy name, &quot;factor&quot;: scaling factor}</code>. When using this flag, don&#x2019;t update
<code>max_position_embeddings</code> to the expected new maximum. See the following thread for more information on how
these scaling strategies behave:
<a href="https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/" rel="nofollow">https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/</a>. This is an
experimental feature, subject to breaking API changes in future versions.`,name:"rope_scaling"},{anchor:"transformers.LlamaConfig.attention_bias",description:`<strong>attention_bias</strong> (<code>bool</code>, defaults to <code>False</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a bias in the query, key, value and output projection layers during self-attention.`,name:"attention_bias"},{anchor:"transformers.LlamaConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/configuration_llama.py#L31"}}),se=new Cn({props:{anchor:"transformers.LlamaConfig.example",$$slots:{default:[sr]},$$scope:{ctx:v}}}),et=new D({props:{title:"LlamaTokenizer",local:"transformers.LlamaTokenizer",headingTag:"h2"}}),tt=new x({props:{name:"class transformers.LlamaTokenizer",anchor:"transformers.LlamaTokenizer",parameters:[{name:"vocab_file",val:""},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"pad_token",val:" = None"},{name:"sp_model_kwargs",val:": Optional = None"},{name:"add_bos_token",val:" = True"},{name:"add_eos_token",val:" = False"},{name:"clean_up_tokenization_spaces",val:" = False"},{name:"use_default_system_prompt",val:" = False"},{name:"spaces_between_special_tokens",val:" = False"},{name:"legacy",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LlamaTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.LlamaTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.LlamaTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.`,name:"bos_token"},{anchor:"transformers.LlamaTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.LlamaTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>) &#x2014;
A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
attention mechanisms or loss computation.`,name:"pad_token"},{anchor:"transformers.LlamaTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>Dict[str, Any]</code>, <code>Optional</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.LlamaTokenizer.add_bos_token",description:`<strong>add_bos_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to add an <code>bos_token</code> at the start of sequences.`,name:"add_bos_token"},{anchor:"transformers.LlamaTokenizer.add_eos_token",description:`<strong>add_eos_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add an <code>eos_token</code> at the end of sequences.`,name:"add_eos_token"},{anchor:"transformers.LlamaTokenizer.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like
extra spaces.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.LlamaTokenizer.use_default_system_prompt",description:`<strong>use_default_system_prompt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the default system prompt for Llama should be used.`,name:"use_default_system_prompt"},{anchor:"transformers.LlamaTokenizer.spaces_between_special_tokens",description:`<strong>spaces_between_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add spaces between special tokens.`,name:"spaces_between_special_tokens"},{anchor:"transformers.LlamaTokenizer.legacy",description:`<strong>legacy</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not the <code>legacy</code> behavior of the tokenizer should be used. Legacy is before the merge of #24622
and #25224 which includes fixes to properly handle tokens that appear after special tokens. A simple
example:</p>
<ul>
<li><code>legacy=True</code>:</li>
</ul>`,name:"legacy"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L66"}}),nt=new x({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.LlamaTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:""},{name:"token_ids_1",val:" = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L333"}}),ot=new x({props:{name:"get_special_tokens_mask",anchor:"transformers.LlamaTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.LlamaTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.LlamaTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.LlamaTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L344",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),at=new x({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.LlamaTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of ids.`,name:"token_ids_0"},{anchor:"transformers.LlamaTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L381",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),ie=new Cn({props:{anchor:"transformers.LlamaTokenizer.create_token_type_ids_from_sequences.example",$$slots:{default:[rr]},$$scope:{ctx:v}}}),st=new x({props:{name:"save_vocabulary",anchor:"transformers.LlamaTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:""},{name:"filename_prefix",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaTokenizer.save_vocabulary.save_directory",description:`<strong>save_directory</strong> (<code>str</code>) &#x2014;
The directory in which to save the vocabulary.`,name:"save_directory"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L306",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Paths to the files saved.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Tuple(str)</code></p>
`}}),rt=new D({props:{title:"LlamaTokenizerFast",local:"transformers.LlamaTokenizerFast",headingTag:"h2"}}),it=new x({props:{name:"class transformers.LlamaTokenizerFast",anchor:"transformers.LlamaTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"clean_up_tokenization_spaces",val:" = False"},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"add_bos_token",val:" = True"},{name:"add_eos_token",val:" = False"},{name:"use_default_system_prompt",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LlamaTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a .model extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.LlamaTokenizerFast.tokenizer_file",description:`<strong>tokenizer_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
<a href="https://github.com/huggingface/tokenizers" rel="nofollow">tokenizers</a> file (generally has a .json extension) that
contains everything needed to load the tokenizer.`,name:"tokenizer_file"},{anchor:"transformers.LlamaTokenizerFast.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like
extra spaces.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.LlamaTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.LlamaTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.`,name:"bos_token"},{anchor:"transformers.LlamaTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.LlamaTokenizerFast.add_bos_token",description:`<strong>add_bos_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to add an <code>bos_token</code> at the start of sequences.`,name:"add_bos_token"},{anchor:"transformers.LlamaTokenizerFast.add_eos_token",description:`<strong>add_eos_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add an <code>eos_token</code> at the end of sequences.`,name:"add_eos_token"},{anchor:"transformers.LlamaTokenizerFast.use_default_system_prompt",description:`<strong>use_default_system_prompt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the default system prompt for Llama should be used.`,name:"use_default_system_prompt"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama_fast.py#L57"}}),de=new Cn({props:{anchor:"transformers.LlamaTokenizerFast.example",$$slots:{default:[ir]},$$scope:{ctx:v}}}),lt=new x({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.LlamaTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:""},{name:"token_ids_1",val:" = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama_fast.py#L272"}}),dt=new x({props:{name:"get_special_tokens_mask",anchor:"transformers.LlamaTokenizerFast.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.LlamaTokenizerFast.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of ids of the first sequence.`,name:"token_ids_0"},{anchor:"transformers.LlamaTokenizerFast.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
List of ids of the second sequence.`,name:"token_ids_1"},{anchor:"transformers.LlamaTokenizerFast.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/tokenization_utils_base.py#L3798",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]</p>
`}}),ct=new x({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.LlamaTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.LlamaTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/tokenization_utils_base.py#L3328",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The token type ids.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),mt=new x({props:{name:"update_post_processor",anchor:"transformers.LlamaTokenizerFast.update_post_processor",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama_fast.py#L146"}}),pt=new x({props:{name:"save_vocabulary",anchor:"transformers.LlamaTokenizerFast.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama_fast.py#L190"}}),ut=new D({props:{title:"LlamaModel",local:"transformers.LlamaModel",headingTag:"h2"}}),ht=new x({props:{name:"class transformers.LlamaModel",anchor:"transformers.LlamaModel",parameters:[{name:"config",val:": LlamaConfig"}],parametersDescription:[{anchor:"transformers.LlamaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.
config &#x2014; LlamaConfig`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L879"}}),ft=new x({props:{name:"forward",anchor:"transformers.LlamaModel.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"},{name:"cache_position",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.LlamaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.LlamaModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.LlamaModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <a href="/docs/transformers/main/en/internal/generation_utils#transformers.Cache">Cache</a> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.LlamaModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.LlamaModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.LlamaModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.LlamaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.LlamaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L915"}}),pe=new $e({props:{$$slots:{default:[lr]},$$scope:{ctx:v}}}),gt=new D({props:{title:"LlamaForCausalLM",local:"transformers.LlamaForCausalLM",headingTag:"h2"}}),_t=new x({props:{name:"class transformers.LlamaForCausalLM",anchor:"transformers.LlamaForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1070"}}),bt=new x({props:{name:"forward",anchor:"transformers.LlamaForCausalLM.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"},{name:"cache_position",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.LlamaForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.LlamaForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.LlamaForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <a href="/docs/transformers/main/en/internal/generation_utils#transformers.Cache">Cache</a> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.LlamaForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.LlamaForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.LlamaForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.LlamaForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.LlamaForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.</p>
<p>Args &#x2014;
labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1100",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig"
>LlamaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) ‚Äî Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) ‚Äî Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) ‚Äî Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ue=new $e({props:{$$slots:{default:[dr]},$$scope:{ctx:v}}}),he=new Cn({props:{anchor:"transformers.LlamaForCausalLM.forward.example",$$slots:{default:[cr]},$$scope:{ctx:v}}}),kt=new D({props:{title:"LlamaForSequenceClassification",local:"transformers.LlamaForSequenceClassification",headingTag:"h2"}}),yt=new x({props:{name:"class transformers.LlamaForSequenceClassification",anchor:"transformers.LlamaForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.LlamaForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1277"}}),vt=new x({props:{name:"forward",anchor:"transformers.LlamaForSequenceClassification.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.LlamaForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.LlamaForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.LlamaForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <a href="/docs/transformers/main/en/internal/generation_utils#transformers.Cache">Cache</a> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.LlamaForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.LlamaForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.LlamaForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.LlamaForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.LlamaForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.LlamaForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1308"}}),fe=new $e({props:{$$slots:{default:[mr]},$$scope:{ctx:v}}}),Tt=new D({props:{title:"LlamaForQuestionAnswering",local:"transformers.LlamaForQuestionAnswering",headingTag:"h2"}}),wt=new x({props:{name:"class transformers.LlamaForQuestionAnswering",anchor:"transformers.LlamaForQuestionAnswering",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.LlamaForQuestionAnswering.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1400"}}),$t=new x({props:{name:"forward",anchor:"transformers.LlamaForQuestionAnswering.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"start_positions",val:": Optional = None"},{name:"end_positions",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaForQuestionAnswering.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.LlamaForQuestionAnswering.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.LlamaForQuestionAnswering.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.LlamaForQuestionAnswering.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <a href="/docs/transformers/main/en/internal/generation_utils#transformers.Cache">Cache</a> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.LlamaForQuestionAnswering.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.LlamaForQuestionAnswering.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.LlamaForQuestionAnswering.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.LlamaForQuestionAnswering.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.LlamaForQuestionAnswering.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.LlamaForQuestionAnswering.forward.start_positions",description:`<strong>start_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the start of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"start_positions"},{anchor:"transformers.LlamaForQuestionAnswering.forward.end_positions",description:`<strong>end_positions</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for position (index) of the end of the labelled span for computing the token classification loss.
Positions are clamped to the length of the sequence (<code>sequence_length</code>). Position outside of the sequence
are not taken into account for computing the loss.`,name:"end_positions"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1423"}}),ge=new $e({props:{$$slots:{default:[pr]},$$scope:{ctx:v}}}),Lt=new D({props:{title:"FlaxLlamaModel",local:"transformers.FlaxLlamaModel",headingTag:"h2"}}),Mt=new x({props:{name:"class transformers.FlaxLlamaModel",anchor:"transformers.FlaxLlamaModel",parameters:[{name:"config",val:": LlamaConfig"},{name:"input_shape",val:": Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxLlamaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxLlamaModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code>, or
<code>jax.numpy.bfloat16</code>.</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_flax_llama.py#L634"}}),xt=new x({props:{name:"__call__",anchor:"transformers.FlaxLlamaModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlaxLlamaModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxLlamaModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlaxLlamaModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlaxLlamaModel.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxLlamaModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxLlamaModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxLlamaModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_flax_llama.py#L458",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput"
>transformers.modeling_flax_outputs.FlaxBaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig"
>LlamaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) ‚Äî Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutput"
>transformers.modeling_flax_outputs.FlaxBaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),_e=new $e({props:{$$slots:{default:[ur]},$$scope:{ctx:v}}}),be=new $e({props:{warning:!0,$$slots:{default:[hr]},$$scope:{ctx:v}}}),ke=new Cn({props:{anchor:"transformers.FlaxLlamaModel.__call__.example",$$slots:{default:[fr]},$$scope:{ctx:v}}}),zt=new D({props:{title:"FlaxLlamaForCausalLM",local:"transformers.FlaxLlamaForCausalLM",headingTag:"h2"}}),Ct=new x({props:{name:"class transformers.FlaxLlamaForCausalLM",anchor:"transformers.FlaxLlamaForCausalLM",parameters:[{name:"config",val:": LlamaConfig"},{name:"input_shape",val:": Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxLlamaForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxLlamaForCausalLM.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code>, or
<code>jax.numpy.bfloat16</code>.</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_flax_llama.py#L695"}}),Ft=new x({props:{name:"__call__",anchor:"transformers.FlaxLlamaForCausalLM.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlaxLlamaForCausalLM.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxLlamaForCausalLM.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlaxLlamaForCausalLM.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlaxLlamaForCausalLM.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxLlamaForCausalLM.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxLlamaForCausalLM.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxLlamaForCausalLM.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_flax_llama.py#L458",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig"
>LlamaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) ‚Äî Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ye=new $e({props:{$$slots:{default:[gr]},$$scope:{ctx:v}}}),ve=new $e({props:{warning:!0,$$slots:{default:[_r]},$$scope:{ctx:v}}}),Te=new Cn({props:{anchor:"transformers.FlaxLlamaForCausalLM.__call__.example",$$slots:{default:[br]},$$scope:{ctx:v}}}),{c(){r=i("meta"),y=o(),c=i("p"),k=o(),u(T.$$.fragment),p=o(),u(z.$$.fragment),In=o(),Me=i("p"),Me.innerHTML=Va,Un=o(),xe=i("p"),xe.textContent=Za,An=o(),ze=i("p"),ze.innerHTML=Qa,Wn=o(),Ce=i("p"),Ce.innerHTML=Ra,Hn=o(),u(Fe.$$.fragment),Jn=o(),Pe=i("ul"),Pe.innerHTML=Ba,Sn=o(),u(je.$$.fragment),Gn=o(),qe=i("ul"),qe.innerHTML=Oa,Nn=o(),u(Ie.$$.fragment),En=o(),Ue=i("p"),Ue.textContent=Xa,Dn=o(),Ae=i("ul"),Ae.innerHTML=Ya,Vn=o(),We=i("p"),We.innerHTML=Ka,Zn=o(),He=i("p"),He.textContent=es,Qn=o(),Je=i("ul"),Je.innerHTML=ts,Rn=o(),u(Se.$$.fragment),Bn=o(),Ge=i("p"),Ge.textContent=ns,On=o(),u(Ne.$$.fragment),Xn=o(),Ee=i("ul"),Ee.innerHTML=os,Yn=o(),u(De.$$.fragment),Kn=o(),Ve=i("ul"),Ve.innerHTML=as,eo=o(),Ze=i("p"),Ze.textContent=ss,to=o(),Qe=i("ul"),Qe.innerHTML=rs,no=o(),Re=i("p"),Re.textContent=is,oo=o(),Be=i("ul"),Be.innerHTML=ls,ao=o(),Oe=i("p"),Oe.textContent=ds,so=o(),Xe=i("ul"),Xe.innerHTML=cs,ro=o(),u(Ye.$$.fragment),io=o(),N=i("div"),u(Ke.$$.fragment),zo=o(),Ut=i("p"),Ut.innerHTML=ms,Co=o(),At=i("p"),At.innerHTML=ps,Fo=o(),u(se.$$.fragment),lo=o(),u(et.$$.fragment),co=o(),j=i("div"),u(tt.$$.fragment),Po=o(),Wt=i("p"),Wt.textContent=us,jo=o(),Ht=i("div"),u(nt.$$.fragment),qo=o(),re=i("div"),u(ot.$$.fragment),Io=o(),Jt=i("p"),Jt.innerHTML=hs,Uo=o(),V=i("div"),u(at.$$.fragment),Ao=o(),St=i("p"),St.textContent=fs,Wo=o(),u(ie.$$.fragment),Ho=o(),Gt=i("p"),Gt.textContent=gs,Jo=o(),le=i("div"),u(st.$$.fragment),So=o(),Nt=i("p"),Nt.textContent=_s,mo=o(),u(rt.$$.fragment),po=o(),w=i("div"),u(it.$$.fragment),Go=o(),Et=i("p"),Et.textContent=bs,No=o(),Dt=i("p"),Dt.textContent=ks,Eo=o(),u(de.$$.fragment),Do=o(),Vt=i("p"),Vt.innerHTML=ys,Vo=o(),Zt=i("p"),Zt.innerHTML=vs,Zo=o(),Qt=i("div"),u(lt.$$.fragment),Qo=o(),ce=i("div"),u(dt.$$.fragment),Ro=o(),Rt=i("p"),Rt.innerHTML=Ts,Bo=o(),B=i("div"),u(ct.$$.fragment),Oo=o(),Bt=i("p"),Bt.innerHTML=ws,Xo=o(),Ot=i("p"),Ot.textContent=$s,Yo=o(),me=i("div"),u(mt.$$.fragment),Ko=o(),Xt=i("p"),Xt.innerHTML=Ls,ea=o(),Yt=i("div"),u(pt.$$.fragment),uo=o(),u(ut.$$.fragment),ho=o(),q=i("div"),u(ht.$$.fragment),ta=o(),Kt=i("p"),Kt.innerHTML=Ms,na=o(),en=i("p"),en.innerHTML=xs,oa=o(),tn=i("p"),tn.innerHTML=zs,aa=o(),O=i("div"),u(ft.$$.fragment),sa=o(),nn=i("p"),nn.innerHTML=Cs,ra=o(),u(pe.$$.fragment),fo=o(),u(gt.$$.fragment),go=o(),ne=i("div"),u(_t.$$.fragment),ia=o(),Z=i("div"),u(bt.$$.fragment),la=o(),on=i("p"),on.innerHTML=Fs,da=o(),u(ue.$$.fragment),ca=o(),u(he.$$.fragment),_o=o(),u(kt.$$.fragment),bo=o(),C=i("div"),u(yt.$$.fragment),ma=o(),an=i("p"),an.textContent=Ps,pa=o(),sn=i("p"),sn.innerHTML=js,ua=o(),rn=i("p"),rn.innerHTML=qs,ha=o(),ln=i("p"),ln.innerHTML=Is,fa=o(),dn=i("p"),dn.innerHTML=Us,ga=o(),X=i("div"),u(vt.$$.fragment),_a=o(),cn=i("p"),cn.innerHTML=As,ba=o(),u(fe.$$.fragment),ko=o(),u(Tt.$$.fragment),yo=o(),I=i("div"),u(wt.$$.fragment),ka=o(),mn=i("p"),mn.innerHTML=Ws,ya=o(),pn=i("p"),pn.innerHTML=Hs,va=o(),un=i("p"),un.innerHTML=Js,Ta=o(),Y=i("div"),u($t.$$.fragment),wa=o(),hn=i("p"),hn.innerHTML=Ss,$a=o(),u(ge.$$.fragment),vo=o(),u(Lt.$$.fragment),To=o(),F=i("div"),u(Mt.$$.fragment),La=o(),fn=i("p"),fn.textContent=Gs,Ma=o(),gn=i("p"),gn.innerHTML=Ns,xa=o(),_n=i("p"),_n.innerHTML=Es,za=o(),bn=i("p"),bn.textContent=Ds,Ca=o(),kn=i("ul"),kn.innerHTML=Vs,Fa=o(),A=i("div"),u(xt.$$.fragment),Pa=o(),yn=i("p"),yn.innerHTML=Zs,ja=o(),u(_e.$$.fragment),qa=o(),u(be.$$.fragment),Ia=o(),u(ke.$$.fragment),wo=o(),u(zt.$$.fragment),$o=o(),P=i("div"),u(Ct.$$.fragment),Ua=o(),vn=i("p"),vn.textContent=Qs,Aa=o(),Tn=i("p"),Tn.innerHTML=Rs,Wa=o(),wn=i("p"),wn.innerHTML=Bs,Ha=o(),$n=i("p"),$n.textContent=Os,Ja=o(),Ln=i("ul"),Ln.innerHTML=Xs,Sa=o(),W=i("div"),u(Ft.$$.fragment),Ga=o(),Mn=i("p"),Mn.innerHTML=Ys,Na=o(),u(ye.$$.fragment),Ea=o(),u(ve.$$.fragment),Da=o(),u(Te.$$.fragment),Lo=o(),Fn=i("p"),this.h()},l(e){const n=ar("svelte-u9bgzb",document.head);r=l(n,"META",{name:!0,content:!0}),n.forEach(s),y=a(e),c=l(e,"P",{}),$(c).forEach(s),k=a(e),h(T.$$.fragment,e),p=a(e),h(z.$$.fragment,e),In=a(e),Me=l(e,"P",{"data-svelte-h":!0}),m(Me)!=="svelte-1mfapnf"&&(Me.innerHTML=Va),Un=a(e),xe=l(e,"P",{"data-svelte-h":!0}),m(xe)!=="svelte-vfdo9a"&&(xe.textContent=Za),An=a(e),ze=l(e,"P",{"data-svelte-h":!0}),m(ze)!=="svelte-sl0nly"&&(ze.innerHTML=Qa),Wn=a(e),Ce=l(e,"P",{"data-svelte-h":!0}),m(Ce)!=="svelte-1ivivso"&&(Ce.innerHTML=Ra),Hn=a(e),h(Fe.$$.fragment,e),Jn=a(e),Pe=l(e,"UL",{"data-svelte-h":!0}),m(Pe)!=="svelte-oqmv1y"&&(Pe.innerHTML=Ba),Sn=a(e),h(je.$$.fragment,e),Gn=a(e),qe=l(e,"UL",{"data-svelte-h":!0}),m(qe)!=="svelte-yjbkch"&&(qe.innerHTML=Oa),Nn=a(e),h(Ie.$$.fragment,e),En=a(e),Ue=l(e,"P",{"data-svelte-h":!0}),m(Ue)!=="svelte-3h7jh3"&&(Ue.textContent=Xa),Dn=a(e),Ae=l(e,"UL",{"data-svelte-h":!0}),m(Ae)!=="svelte-1xmggxn"&&(Ae.innerHTML=Ya),Vn=a(e),We=l(e,"P",{"data-svelte-h":!0}),m(We)!=="svelte-1s478nn"&&(We.innerHTML=Ka),Zn=a(e),He=l(e,"P",{"data-svelte-h":!0}),m(He)!=="svelte-a39vkx"&&(He.textContent=es),Qn=a(e),Je=l(e,"UL",{"data-svelte-h":!0}),m(Je)!=="svelte-y8u21a"&&(Je.innerHTML=ts),Rn=a(e),h(Se.$$.fragment,e),Bn=a(e),Ge=l(e,"P",{"data-svelte-h":!0}),m(Ge)!=="svelte-h1kz0m"&&(Ge.textContent=ns),On=a(e),h(Ne.$$.fragment,e),Xn=a(e),Ee=l(e,"UL",{"data-svelte-h":!0}),m(Ee)!=="svelte-dp296g"&&(Ee.innerHTML=os),Yn=a(e),h(De.$$.fragment,e),Kn=a(e),Ve=l(e,"UL",{"data-svelte-h":!0}),m(Ve)!=="svelte-ab7kge"&&(Ve.innerHTML=as),eo=a(e),Ze=l(e,"P",{"data-svelte-h":!0}),m(Ze)!=="svelte-liq2q7"&&(Ze.textContent=ss),to=a(e),Qe=l(e,"UL",{"data-svelte-h":!0}),m(Qe)!=="svelte-10a5770"&&(Qe.innerHTML=rs),no=a(e),Re=l(e,"P",{"data-svelte-h":!0}),m(Re)!=="svelte-1wntqpp"&&(Re.textContent=is),oo=a(e),Be=l(e,"UL",{"data-svelte-h":!0}),m(Be)!=="svelte-1t43wyu"&&(Be.innerHTML=ls),ao=a(e),Oe=l(e,"P",{"data-svelte-h":!0}),m(Oe)!=="svelte-lk14e4"&&(Oe.textContent=ds),so=a(e),Xe=l(e,"UL",{"data-svelte-h":!0}),m(Xe)!=="svelte-162yhfz"&&(Xe.innerHTML=cs),ro=a(e),h(Ye.$$.fragment,e),io=a(e),N=l(e,"DIV",{class:!0});var Q=$(N);h(Ke.$$.fragment,Q),zo=a(Q),Ut=l(Q,"P",{"data-svelte-h":!0}),m(Ut)!=="svelte-193togw"&&(Ut.innerHTML=ms),Co=a(Q),At=l(Q,"P",{"data-svelte-h":!0}),m(At)!=="svelte-o55m63"&&(At.innerHTML=ps),Fo=a(Q),h(se.$$.fragment,Q),Q.forEach(s),lo=a(e),h(et.$$.fragment,e),co=a(e),j=l(e,"DIV",{class:!0});var U=$(j);h(tt.$$.fragment,U),Po=a(U),Wt=l(U,"P",{"data-svelte-h":!0}),m(Wt)!=="svelte-qfiu5a"&&(Wt.textContent=us),jo=a(U),Ht=l(U,"DIV",{class:!0});var Pn=$(Ht);h(nt.$$.fragment,Pn),Pn.forEach(s),qo=a(U),re=l(U,"DIV",{class:!0});var Pt=$(re);h(ot.$$.fragment,Pt),Io=a(Pt),Jt=l(Pt,"P",{"data-svelte-h":!0}),m(Jt)!=="svelte-1f4f5kp"&&(Jt.innerHTML=hs),Pt.forEach(s),Uo=a(U),V=l(U,"DIV",{class:!0});var R=$(V);h(at.$$.fragment,R),Ao=a(R),St=l(R,"P",{"data-svelte-h":!0}),m(St)!=="svelte-13bfd60"&&(St.textContent=fs),Wo=a(R),h(ie.$$.fragment,R),Ho=a(R),Gt=l(R,"P",{"data-svelte-h":!0}),m(Gt)!=="svelte-wtrslu"&&(Gt.textContent=gs),R.forEach(s),Jo=a(U),le=l(U,"DIV",{class:!0});var jt=$(le);h(st.$$.fragment,jt),So=a(jt),Nt=l(jt,"P",{"data-svelte-h":!0}),m(Nt)!=="svelte-1slb66l"&&(Nt.textContent=_s),jt.forEach(s),U.forEach(s),mo=a(e),h(rt.$$.fragment,e),po=a(e),w=l(e,"DIV",{class:!0});var M=$(w);h(it.$$.fragment,M),Go=a(M),Et=l(M,"P",{"data-svelte-h":!0}),m(Et)!=="svelte-15tdcz8"&&(Et.textContent=bs),No=a(M),Dt=l(M,"P",{"data-svelte-h":!0}),m(Dt)!=="svelte-llhmpa"&&(Dt.textContent=ks),Eo=a(M),h(de.$$.fragment,M),Do=a(M),Vt=l(M,"P",{"data-svelte-h":!0}),m(Vt)!=="svelte-cnb6q1"&&(Vt.innerHTML=ys),Vo=a(M),Zt=l(M,"P",{"data-svelte-h":!0}),m(Zt)!=="svelte-fh0aq"&&(Zt.innerHTML=vs),Zo=a(M),Qt=l(M,"DIV",{class:!0});var jn=$(Qt);h(lt.$$.fragment,jn),jn.forEach(s),Qo=a(M),ce=l(M,"DIV",{class:!0});var qt=$(ce);h(dt.$$.fragment,qt),Ro=a(qt),Rt=l(qt,"P",{"data-svelte-h":!0}),m(Rt)!=="svelte-1wmjg8a"&&(Rt.innerHTML=Ts),qt.forEach(s),Bo=a(M),B=l(M,"DIV",{class:!0});var oe=$(B);h(ct.$$.fragment,oe),Oo=a(oe),Bt=l(oe,"P",{"data-svelte-h":!0}),m(Bt)!=="svelte-zj1vf1"&&(Bt.innerHTML=ws),Xo=a(oe),Ot=l(oe,"P",{"data-svelte-h":!0}),m(Ot)!=="svelte-9vptpw"&&(Ot.textContent=$s),oe.forEach(s),Yo=a(M),me=l(M,"DIV",{class:!0});var It=$(me);h(mt.$$.fragment,It),Ko=a(It),Xt=l(It,"P",{"data-svelte-h":!0}),m(Xt)!=="svelte-nfci2w"&&(Xt.innerHTML=Ls),It.forEach(s),ea=a(M),Yt=l(M,"DIV",{class:!0});var qn=$(Yt);h(pt.$$.fragment,qn),qn.forEach(s),M.forEach(s),uo=a(e),h(ut.$$.fragment,e),ho=a(e),q=l(e,"DIV",{class:!0});var E=$(q);h(ht.$$.fragment,E),ta=a(E),Kt=l(E,"P",{"data-svelte-h":!0}),m(Kt)!=="svelte-4n5ccz"&&(Kt.innerHTML=Ms),na=a(E),en=l(E,"P",{"data-svelte-h":!0}),m(en)!=="svelte-hswkmf"&&(en.innerHTML=xs),oa=a(E),tn=l(E,"P",{"data-svelte-h":!0}),m(tn)!=="svelte-eom0yk"&&(tn.innerHTML=zs),aa=a(E),O=l(E,"DIV",{class:!0});var ae=$(O);h(ft.$$.fragment,ae),sa=a(ae),nn=l(ae,"P",{"data-svelte-h":!0}),m(nn)!=="svelte-9rph7s"&&(nn.innerHTML=Cs),ra=a(ae),h(pe.$$.fragment,ae),ae.forEach(s),E.forEach(s),fo=a(e),h(gt.$$.fragment,e),go=a(e),ne=l(e,"DIV",{class:!0});var xo=$(ne);h(_t.$$.fragment,xo),ia=a(xo),Z=l(xo,"DIV",{class:!0});var we=$(Z);h(bt.$$.fragment,we),la=a(we),on=l(we,"P",{"data-svelte-h":!0}),m(on)!=="svelte-151hngo"&&(on.innerHTML=Fs),da=a(we),h(ue.$$.fragment,we),ca=a(we),h(he.$$.fragment,we),we.forEach(s),xo.forEach(s),_o=a(e),h(kt.$$.fragment,e),bo=a(e),C=l(e,"DIV",{class:!0});var H=$(C);h(yt.$$.fragment,H),ma=a(H),an=l(H,"P",{"data-svelte-h":!0}),m(an)!=="svelte-62must"&&(an.textContent=Ps),pa=a(H),sn=l(H,"P",{"data-svelte-h":!0}),m(sn)!=="svelte-32qpeb"&&(sn.innerHTML=js),ua=a(H),rn=l(H,"P",{"data-svelte-h":!0}),m(rn)!=="svelte-10ugs3m"&&(rn.innerHTML=qs),ha=a(H),ln=l(H,"P",{"data-svelte-h":!0}),m(ln)!=="svelte-6pahdo"&&(ln.innerHTML=Is),fa=a(H),dn=l(H,"P",{"data-svelte-h":!0}),m(dn)!=="svelte-hswkmf"&&(dn.innerHTML=Us),ga=a(H),X=l(H,"DIV",{class:!0});var xn=$(X);h(vt.$$.fragment,xn),_a=a(xn),cn=l(xn,"P",{"data-svelte-h":!0}),m(cn)!=="svelte-g0af0e"&&(cn.innerHTML=As),ba=a(xn),h(fe.$$.fragment,xn),xn.forEach(s),H.forEach(s),ko=a(e),h(Tt.$$.fragment,e),yo=a(e),I=l(e,"DIV",{class:!0});var K=$(I);h(wt.$$.fragment,K),ka=a(K),mn=l(K,"P",{"data-svelte-h":!0}),m(mn)!=="svelte-2ba709"&&(mn.innerHTML=Ws),ya=a(K),pn=l(K,"P",{"data-svelte-h":!0}),m(pn)!=="svelte-6pahdo"&&(pn.innerHTML=Hs),va=a(K),un=l(K,"P",{"data-svelte-h":!0}),m(un)!=="svelte-hswkmf"&&(un.innerHTML=Js),Ta=a(K),Y=l(K,"DIV",{class:!0});var zn=$(Y);h($t.$$.fragment,zn),wa=a(zn),hn=l(zn,"P",{"data-svelte-h":!0}),m(hn)!=="svelte-1b2uhda"&&(hn.innerHTML=Ss),$a=a(zn),h(ge.$$.fragment,zn),zn.forEach(s),K.forEach(s),vo=a(e),h(Lt.$$.fragment,e),To=a(e),F=l(e,"DIV",{class:!0});var J=$(F);h(Mt.$$.fragment,J),La=a(J),fn=l(J,"P",{"data-svelte-h":!0}),m(fn)!=="svelte-1g8y7nc"&&(fn.textContent=Gs),Ma=a(J),gn=l(J,"P",{"data-svelte-h":!0}),m(gn)!=="svelte-18ki9f4"&&(gn.innerHTML=Ns),xa=a(J),_n=l(J,"P",{"data-svelte-h":!0}),m(_n)!=="svelte-idybz1"&&(_n.innerHTML=Es),za=a(J),bn=l(J,"P",{"data-svelte-h":!0}),m(bn)!=="svelte-1pplc4a"&&(bn.textContent=Ds),Ca=a(J),kn=l(J,"UL",{"data-svelte-h":!0}),m(kn)!=="svelte-1w7z84m"&&(kn.innerHTML=Vs),Fa=a(J),A=l(J,"DIV",{class:!0});var ee=$(A);h(xt.$$.fragment,ee),Pa=a(ee),yn=l(ee,"P",{"data-svelte-h":!0}),m(yn)!=="svelte-9bmwma"&&(yn.innerHTML=Zs),ja=a(ee),h(_e.$$.fragment,ee),qa=a(ee),h(be.$$.fragment,ee),Ia=a(ee),h(ke.$$.fragment,ee),ee.forEach(s),J.forEach(s),wo=a(e),h(zt.$$.fragment,e),$o=a(e),P=l(e,"DIV",{class:!0});var S=$(P);h(Ct.$$.fragment,S),Ua=a(S),vn=l(S,"P",{"data-svelte-h":!0}),m(vn)!=="svelte-1as0j1x"&&(vn.textContent=Qs),Aa=a(S),Tn=l(S,"P",{"data-svelte-h":!0}),m(Tn)!=="svelte-18ki9f4"&&(Tn.innerHTML=Rs),Wa=a(S),wn=l(S,"P",{"data-svelte-h":!0}),m(wn)!=="svelte-idybz1"&&(wn.innerHTML=Bs),Ha=a(S),$n=l(S,"P",{"data-svelte-h":!0}),m($n)!=="svelte-1pplc4a"&&($n.textContent=Os),Ja=a(S),Ln=l(S,"UL",{"data-svelte-h":!0}),m(Ln)!=="svelte-1w7z84m"&&(Ln.innerHTML=Xs),Sa=a(S),W=l(S,"DIV",{class:!0});var te=$(W);h(Ft.$$.fragment,te),Ga=a(te),Mn=l(te,"P",{"data-svelte-h":!0}),m(Mn)!=="svelte-9bmwma"&&(Mn.innerHTML=Ys),Na=a(te),h(ye.$$.fragment,te),Ea=a(te),h(ve.$$.fragment,te),Da=a(te),h(Te.$$.fragment,te),te.forEach(s),S.forEach(s),Lo=a(e),Fn=l(e,"P",{}),$(Fn).forEach(s),this.h()},h(){L(r,"name","hf:doc:metadata"),L(r,"content",yr),L(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(Ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(Qt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(Yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),L(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){t(document.head,r),d(e,y,n),d(e,c,n),d(e,k,n),f(T,e,n),d(e,p,n),f(z,e,n),d(e,In,n),d(e,Me,n),d(e,Un,n),d(e,xe,n),d(e,An,n),d(e,ze,n),d(e,Wn,n),d(e,Ce,n),d(e,Hn,n),f(Fe,e,n),d(e,Jn,n),d(e,Pe,n),d(e,Sn,n),f(je,e,n),d(e,Gn,n),d(e,qe,n),d(e,Nn,n),f(Ie,e,n),d(e,En,n),d(e,Ue,n),d(e,Dn,n),d(e,Ae,n),d(e,Vn,n),d(e,We,n),d(e,Zn,n),d(e,He,n),d(e,Qn,n),d(e,Je,n),d(e,Rn,n),f(Se,e,n),d(e,Bn,n),d(e,Ge,n),d(e,On,n),f(Ne,e,n),d(e,Xn,n),d(e,Ee,n),d(e,Yn,n),f(De,e,n),d(e,Kn,n),d(e,Ve,n),d(e,eo,n),d(e,Ze,n),d(e,to,n),d(e,Qe,n),d(e,no,n),d(e,Re,n),d(e,oo,n),d(e,Be,n),d(e,ao,n),d(e,Oe,n),d(e,so,n),d(e,Xe,n),d(e,ro,n),f(Ye,e,n),d(e,io,n),d(e,N,n),f(Ke,N,null),t(N,zo),t(N,Ut),t(N,Co),t(N,At),t(N,Fo),f(se,N,null),d(e,lo,n),f(et,e,n),d(e,co,n),d(e,j,n),f(tt,j,null),t(j,Po),t(j,Wt),t(j,jo),t(j,Ht),f(nt,Ht,null),t(j,qo),t(j,re),f(ot,re,null),t(re,Io),t(re,Jt),t(j,Uo),t(j,V),f(at,V,null),t(V,Ao),t(V,St),t(V,Wo),f(ie,V,null),t(V,Ho),t(V,Gt),t(j,Jo),t(j,le),f(st,le,null),t(le,So),t(le,Nt),d(e,mo,n),f(rt,e,n),d(e,po,n),d(e,w,n),f(it,w,null),t(w,Go),t(w,Et),t(w,No),t(w,Dt),t(w,Eo),f(de,w,null),t(w,Do),t(w,Vt),t(w,Vo),t(w,Zt),t(w,Zo),t(w,Qt),f(lt,Qt,null),t(w,Qo),t(w,ce),f(dt,ce,null),t(ce,Ro),t(ce,Rt),t(w,Bo),t(w,B),f(ct,B,null),t(B,Oo),t(B,Bt),t(B,Xo),t(B,Ot),t(w,Yo),t(w,me),f(mt,me,null),t(me,Ko),t(me,Xt),t(w,ea),t(w,Yt),f(pt,Yt,null),d(e,uo,n),f(ut,e,n),d(e,ho,n),d(e,q,n),f(ht,q,null),t(q,ta),t(q,Kt),t(q,na),t(q,en),t(q,oa),t(q,tn),t(q,aa),t(q,O),f(ft,O,null),t(O,sa),t(O,nn),t(O,ra),f(pe,O,null),d(e,fo,n),f(gt,e,n),d(e,go,n),d(e,ne,n),f(_t,ne,null),t(ne,ia),t(ne,Z),f(bt,Z,null),t(Z,la),t(Z,on),t(Z,da),f(ue,Z,null),t(Z,ca),f(he,Z,null),d(e,_o,n),f(kt,e,n),d(e,bo,n),d(e,C,n),f(yt,C,null),t(C,ma),t(C,an),t(C,pa),t(C,sn),t(C,ua),t(C,rn),t(C,ha),t(C,ln),t(C,fa),t(C,dn),t(C,ga),t(C,X),f(vt,X,null),t(X,_a),t(X,cn),t(X,ba),f(fe,X,null),d(e,ko,n),f(Tt,e,n),d(e,yo,n),d(e,I,n),f(wt,I,null),t(I,ka),t(I,mn),t(I,ya),t(I,pn),t(I,va),t(I,un),t(I,Ta),t(I,Y),f($t,Y,null),t(Y,wa),t(Y,hn),t(Y,$a),f(ge,Y,null),d(e,vo,n),f(Lt,e,n),d(e,To,n),d(e,F,n),f(Mt,F,null),t(F,La),t(F,fn),t(F,Ma),t(F,gn),t(F,xa),t(F,_n),t(F,za),t(F,bn),t(F,Ca),t(F,kn),t(F,Fa),t(F,A),f(xt,A,null),t(A,Pa),t(A,yn),t(A,ja),f(_e,A,null),t(A,qa),f(be,A,null),t(A,Ia),f(ke,A,null),d(e,wo,n),f(zt,e,n),d(e,$o,n),d(e,P,n),f(Ct,P,null),t(P,Ua),t(P,vn),t(P,Aa),t(P,Tn),t(P,Wa),t(P,wn),t(P,Ha),t(P,$n),t(P,Ja),t(P,Ln),t(P,Sa),t(P,W),f(Ft,W,null),t(W,Ga),t(W,Mn),t(W,Na),f(ye,W,null),t(W,Ea),f(ve,W,null),t(W,Da),f(Te,W,null),d(e,Lo,n),d(e,Fn,n),Mo=!0},p(e,[n]){const Q={};n&2&&(Q.$$scope={dirty:n,ctx:e}),se.$set(Q);const U={};n&2&&(U.$$scope={dirty:n,ctx:e}),ie.$set(U);const Pn={};n&2&&(Pn.$$scope={dirty:n,ctx:e}),de.$set(Pn);const Pt={};n&2&&(Pt.$$scope={dirty:n,ctx:e}),pe.$set(Pt);const R={};n&2&&(R.$$scope={dirty:n,ctx:e}),ue.$set(R);const jt={};n&2&&(jt.$$scope={dirty:n,ctx:e}),he.$set(jt);const M={};n&2&&(M.$$scope={dirty:n,ctx:e}),fe.$set(M);const jn={};n&2&&(jn.$$scope={dirty:n,ctx:e}),ge.$set(jn);const qt={};n&2&&(qt.$$scope={dirty:n,ctx:e}),_e.$set(qt);const oe={};n&2&&(oe.$$scope={dirty:n,ctx:e}),be.$set(oe);const It={};n&2&&(It.$$scope={dirty:n,ctx:e}),ke.$set(It);const qn={};n&2&&(qn.$$scope={dirty:n,ctx:e}),ye.$set(qn);const E={};n&2&&(E.$$scope={dirty:n,ctx:e}),ve.$set(E);const ae={};n&2&&(ae.$$scope={dirty:n,ctx:e}),Te.$set(ae)},i(e){Mo||(g(T.$$.fragment,e),g(z.$$.fragment,e),g(Fe.$$.fragment,e),g(je.$$.fragment,e),g(Ie.$$.fragment,e),g(Se.$$.fragment,e),g(Ne.$$.fragment,e),g(De.$$.fragment,e),g(Ye.$$.fragment,e),g(Ke.$$.fragment,e),g(se.$$.fragment,e),g(et.$$.fragment,e),g(tt.$$.fragment,e),g(nt.$$.fragment,e),g(ot.$$.fragment,e),g(at.$$.fragment,e),g(ie.$$.fragment,e),g(st.$$.fragment,e),g(rt.$$.fragment,e),g(it.$$.fragment,e),g(de.$$.fragment,e),g(lt.$$.fragment,e),g(dt.$$.fragment,e),g(ct.$$.fragment,e),g(mt.$$.fragment,e),g(pt.$$.fragment,e),g(ut.$$.fragment,e),g(ht.$$.fragment,e),g(ft.$$.fragment,e),g(pe.$$.fragment,e),g(gt.$$.fragment,e),g(_t.$$.fragment,e),g(bt.$$.fragment,e),g(ue.$$.fragment,e),g(he.$$.fragment,e),g(kt.$$.fragment,e),g(yt.$$.fragment,e),g(vt.$$.fragment,e),g(fe.$$.fragment,e),g(Tt.$$.fragment,e),g(wt.$$.fragment,e),g($t.$$.fragment,e),g(ge.$$.fragment,e),g(Lt.$$.fragment,e),g(Mt.$$.fragment,e),g(xt.$$.fragment,e),g(_e.$$.fragment,e),g(be.$$.fragment,e),g(ke.$$.fragment,e),g(zt.$$.fragment,e),g(Ct.$$.fragment,e),g(Ft.$$.fragment,e),g(ye.$$.fragment,e),g(ve.$$.fragment,e),g(Te.$$.fragment,e),Mo=!0)},o(e){_(T.$$.fragment,e),_(z.$$.fragment,e),_(Fe.$$.fragment,e),_(je.$$.fragment,e),_(Ie.$$.fragment,e),_(Se.$$.fragment,e),_(Ne.$$.fragment,e),_(De.$$.fragment,e),_(Ye.$$.fragment,e),_(Ke.$$.fragment,e),_(se.$$.fragment,e),_(et.$$.fragment,e),_(tt.$$.fragment,e),_(nt.$$.fragment,e),_(ot.$$.fragment,e),_(at.$$.fragment,e),_(ie.$$.fragment,e),_(st.$$.fragment,e),_(rt.$$.fragment,e),_(it.$$.fragment,e),_(de.$$.fragment,e),_(lt.$$.fragment,e),_(dt.$$.fragment,e),_(ct.$$.fragment,e),_(mt.$$.fragment,e),_(pt.$$.fragment,e),_(ut.$$.fragment,e),_(ht.$$.fragment,e),_(ft.$$.fragment,e),_(pe.$$.fragment,e),_(gt.$$.fragment,e),_(_t.$$.fragment,e),_(bt.$$.fragment,e),_(ue.$$.fragment,e),_(he.$$.fragment,e),_(kt.$$.fragment,e),_(yt.$$.fragment,e),_(vt.$$.fragment,e),_(fe.$$.fragment,e),_(Tt.$$.fragment,e),_(wt.$$.fragment,e),_($t.$$.fragment,e),_(ge.$$.fragment,e),_(Lt.$$.fragment,e),_(Mt.$$.fragment,e),_(xt.$$.fragment,e),_(_e.$$.fragment,e),_(be.$$.fragment,e),_(ke.$$.fragment,e),_(zt.$$.fragment,e),_(Ct.$$.fragment,e),_(Ft.$$.fragment,e),_(ye.$$.fragment,e),_(ve.$$.fragment,e),_(Te.$$.fragment,e),Mo=!1},d(e){e&&(s(y),s(c),s(k),s(p),s(In),s(Me),s(Un),s(xe),s(An),s(ze),s(Wn),s(Ce),s(Hn),s(Jn),s(Pe),s(Sn),s(Gn),s(qe),s(Nn),s(En),s(Ue),s(Dn),s(Ae),s(Vn),s(We),s(Zn),s(He),s(Qn),s(Je),s(Rn),s(Bn),s(Ge),s(On),s(Xn),s(Ee),s(Yn),s(Kn),s(Ve),s(eo),s(Ze),s(to),s(Qe),s(no),s(Re),s(oo),s(Be),s(ao),s(Oe),s(so),s(Xe),s(ro),s(io),s(N),s(lo),s(co),s(j),s(mo),s(po),s(w),s(uo),s(ho),s(q),s(fo),s(go),s(ne),s(_o),s(bo),s(C),s(ko),s(yo),s(I),s(vo),s(To),s(F),s(wo),s($o),s(P),s(Lo),s(Fn)),s(r),b(T,e),b(z,e),b(Fe,e),b(je,e),b(Ie,e),b(Se,e),b(Ne,e),b(De,e),b(Ye,e),b(Ke),b(se),b(et,e),b(tt),b(nt),b(ot),b(at),b(ie),b(st),b(rt,e),b(it),b(de),b(lt),b(dt),b(ct),b(mt),b(pt),b(ut,e),b(ht),b(ft),b(pe),b(gt,e),b(_t),b(bt),b(ue),b(he),b(kt,e),b(yt),b(vt),b(fe),b(Tt,e),b(wt),b($t),b(ge),b(Lt,e),b(Mt),b(xt),b(_e),b(be),b(ke),b(zt,e),b(Ct),b(Ft),b(ye),b(ve),b(Te)}}}const yr='{"title":"LLaMA","local":"llama","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"LlamaConfig","local":"transformers.LlamaConfig","sections":[],"depth":2},{"title":"LlamaTokenizer","local":"transformers.LlamaTokenizer","sections":[],"depth":2},{"title":"LlamaTokenizerFast","local":"transformers.LlamaTokenizerFast","sections":[],"depth":2},{"title":"LlamaModel","local":"transformers.LlamaModel","sections":[],"depth":2},{"title":"LlamaForCausalLM","local":"transformers.LlamaForCausalLM","sections":[],"depth":2},{"title":"LlamaForSequenceClassification","local":"transformers.LlamaForSequenceClassification","sections":[],"depth":2},{"title":"LlamaForQuestionAnswering","local":"transformers.LlamaForQuestionAnswering","sections":[],"depth":2},{"title":"FlaxLlamaModel","local":"transformers.FlaxLlamaModel","sections":[],"depth":2},{"title":"FlaxLlamaForCausalLM","local":"transformers.FlaxLlamaForCausalLM","sections":[],"depth":2}],"depth":1}';function vr(v){return tr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fr extends nr{constructor(r){super(),or(this,r,vr,kr,er,{})}}export{Fr as component};
