import{s as rt,o as at,n as it}from"../chunks/scheduler.9bc65507.js";import{S as lt,i as dt,g as a,s as o,r as f,A as mt,h as i,f as n,c as r,j as T,u as g,x as p,k as v,y as t,a as m,v as u,d as h,t as k,w as _}from"../chunks/index.707bf1b6.js";import{T as ct}from"../chunks/Tip.c2ecdbf4.js";import{D as I}from"../chunks/Docstring.17db21ae.js";import{C as pt}from"../chunks/CodeBlock.54a9f38d.js";import{H as Me}from"../chunks/Heading.342b1fa6.js";function ft(le){let c,q=`This implementation is the same as BERT, except for tokenization method. Refer to <a href="bert">BERT documentation</a> for
API reference information.`;return{c(){c=a("p"),c.innerHTML=q},l(b){c=i(b,"P",{"data-svelte-h":!0}),p(c)!=="svelte-wqppg4"&&(c.innerHTML=q)},m(b,A){m(b,c,A)},p:it,d(b){b&&n(c)}}}function gt(le){let c,q,b,A,C,de,U,me,E,De='The BERTweet model was proposed in <a href="https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf" rel="nofollow">BERTweet: A pre-trained language model for English Tweets</a> by Dat Quoc Nguyen, Thanh Vu, Anh Tuan Nguyen.',ce,j,We="The abstract from the paper is the following:",pe,R,Ne=`<em>We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having
the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et
al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al.,
2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks:
Part-of-speech tagging, Named-entity recognition and text classification.</em>`,fe,V,Se='This model was contributed by <a href="https://huggingface.co/dqnguyen" rel="nofollow">dqnguyen</a>. The original code can be found <a href="https://github.com/VinAIResearch/BERTweet" rel="nofollow">here</a>.',ge,L,ue,Z,he,y,ke,F,_e,l,P,ze,G,Ae="Constructs a BERTweet tokenizer, using Byte-Pair-Encoding.",Be,Y,Ge=`This tokenizer inherits from <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,Ie,$,H,qe,O,Ye="Loads a pre-existing dictionary from a text file and adds its symbols to this instance.",Ce,w,Q,Ue,K,Oe=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`,Ee,ee,Ke="<li>single sequence: <code>&lt;s&gt; X &lt;/s&gt;</code></li> <li>pair of sequences: <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code></li>",je,x,X,Re,te,et="Converts a sequence of tokens (string) in a single string.",Ve,J,D,Le,ne,tt=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`,Ze,M,W,Fe,se,nt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Pe,z,N,He,oe,st="Normalize tokens in a Tweet",Qe,B,S,Xe,re,ot="Normalize a raw Tweet",be,ie,we;return C=new Me({props:{title:"BERTweet",local:"bertweet",headingTag:"h1"}}),U=new Me({props:{title:"Overview",local:"overview",headingTag:"h2"}}),L=new Me({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),Z=new pt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQWJlcnR3ZWV0JTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJ2aW5haSUyRmJlcnR3ZWV0LWJhc2UlMjIpJTBBJTBBJTIzJTIwRm9yJTIwdHJhbnNmb3JtZXJzJTIwdjQueCUyQiUzQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMnZpbmFpJTJGYmVydHdlZXQtYmFzZSUyMiUyQyUyMHVzZV9mYXN0JTNERmFsc2UpJTBBJTBBJTIzJTIwRm9yJTIwdHJhbnNmb3JtZXJzJTIwdjMueCUzQSUwQSUyMyUyMHRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMnZpbmFpJTJGYmVydHdlZXQtYmFzZSUyMiklMEElMEElMjMlMjBJTlBVVCUyMFRXRUVUJTIwSVMlMjBBTFJFQURZJTIwTk9STUFMSVpFRCElMEFsaW5lJTIwJTNEJTIwJTIyU0MlMjBoYXMlMjBmaXJzdCUyMHR3byUyMHByZXN1bXB0aXZlJTIwY2FzZXMlMjBvZiUyMGNvcm9uYXZpcnVzJTIwJTJDJTIwREhFQyUyMGNvbmZpcm1zJTIwSFRUUFVSTCUyMHZpYSUyMCU0MFVTRVIlMjAlM0FjcnklM0ElMjIlMEElMEFpbnB1dF9pZHMlMjAlM0QlMjB0b3JjaC50ZW5zb3IoJTVCdG9rZW5pemVyLmVuY29kZShsaW5lKSU1RCklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwZmVhdHVyZXMlMjAlM0QlMjBiZXJ0d2VldChpbnB1dF9pZHMpJTIwJTIwJTIzJTIwTW9kZWxzJTIwb3V0cHV0cyUyMGFyZSUyMG5vdyUyMHR1cGxlcyUwQSUwQSUyMyUyMFdpdGglMjBUZW5zb3JGbG93JTIwMi4wJTJCJTNBJTBBJTIzJTIwZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsJTBBJTIzJTIwYmVydHdlZXQlMjAlM0QlMjBURkF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIydmluYWklMkZiZXJ0d2VldC1iYXNlJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>bertweet = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For transformers v4.x+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>, use_fast=<span class="hljs-literal">False</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For transformers v3.x:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/bertweet-base&quot;)</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># INPUT TWEET IS ALREADY NORMALIZED!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(line)])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    features = bertweet(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With TensorFlow 2.0+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># from transformers import TFAutoModel</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># bertweet = TFAutoModel.from_pretrained(&quot;vinai/bertweet-base&quot;)</span>`,wrap:!1}}),y=new ct({props:{$$slots:{default:[ft]},$$scope:{ctx:le}}}),F=new Me({props:{title:"BertweetTokenizer",local:"transformers.BertweetTokenizer",headingTag:"h2"}}),P=new I({props:{name:"class transformers.BertweetTokenizer",anchor:"transformers.BertweetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"normalization",val:" = False"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertweetTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.BertweetTokenizer.normalization",description:`<strong>normalization</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply a normalization preprocess.`,name:"normalization"},{anchor:"transformers.BertweetTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BertweetTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BertweetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BertweetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BertweetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BertweetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BertweetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L68"}}),H=new I({props:{name:"add_from_file",anchor:"transformers.BertweetTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L418"}}),Q=new I({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L183",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),X=new I({props:{name:"convert_tokens_to_string",anchor:"transformers.BertweetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L384"}}),D=new I({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L237",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),W=new I({props:{name:"get_special_tokens_mask",anchor:"transformers.BertweetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L209",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),N=new I({props:{name:"normalizeToken",anchor:"transformers.BertweetTokenizer.normalizeToken",parameters:[{name:"token",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L357"}}),S=new I({props:{name:"normalizeTweet",anchor:"transformers.BertweetTokenizer.normalizeTweet",parameters:[{name:"tweet",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L323"}}),{c(){c=a("meta"),q=o(),b=a("p"),A=o(),f(C.$$.fragment),de=o(),f(U.$$.fragment),me=o(),E=a("p"),E.innerHTML=De,ce=o(),j=a("p"),j.textContent=We,pe=o(),R=a("p"),R.innerHTML=Ne,fe=o(),V=a("p"),V.innerHTML=Se,ge=o(),f(L.$$.fragment),ue=o(),f(Z.$$.fragment),he=o(),f(y.$$.fragment),ke=o(),f(F.$$.fragment),_e=o(),l=a("div"),f(P.$$.fragment),ze=o(),G=a("p"),G.textContent=Ae,Be=o(),Y=a("p"),Y.innerHTML=Ge,Ie=o(),$=a("div"),f(H.$$.fragment),qe=o(),O=a("p"),O.textContent=Ye,Ce=o(),w=a("div"),f(Q.$$.fragment),Ue=o(),K=a("p"),K.textContent=Oe,Ee=o(),ee=a("ul"),ee.innerHTML=Ke,je=o(),x=a("div"),f(X.$$.fragment),Re=o(),te=a("p"),te.textContent=et,Ve=o(),J=a("div"),f(D.$$.fragment),Le=o(),ne=a("p"),ne.textContent=tt,Ze=o(),M=a("div"),f(W.$$.fragment),Fe=o(),se=a("p"),se.innerHTML=nt,Pe=o(),z=a("div"),f(N.$$.fragment),He=o(),oe=a("p"),oe.textContent=st,Qe=o(),B=a("div"),f(S.$$.fragment),Xe=o(),re=a("p"),re.textContent=ot,be=o(),ie=a("p"),this.h()},l(e){const s=mt("svelte-u9bgzb",document.head);c=i(s,"META",{name:!0,content:!0}),s.forEach(n),q=r(e),b=i(e,"P",{}),T(b).forEach(n),A=r(e),g(C.$$.fragment,e),de=r(e),g(U.$$.fragment,e),me=r(e),E=i(e,"P",{"data-svelte-h":!0}),p(E)!=="svelte-1tvfkok"&&(E.innerHTML=De),ce=r(e),j=i(e,"P",{"data-svelte-h":!0}),p(j)!=="svelte-vfdo9a"&&(j.textContent=We),pe=r(e),R=i(e,"P",{"data-svelte-h":!0}),p(R)!=="svelte-eh682o"&&(R.innerHTML=Ne),fe=r(e),V=i(e,"P",{"data-svelte-h":!0}),p(V)!=="svelte-fl8xrj"&&(V.innerHTML=Se),ge=r(e),g(L.$$.fragment,e),ue=r(e),g(Z.$$.fragment,e),he=r(e),g(y.$$.fragment,e),ke=r(e),g(F.$$.fragment,e),_e=r(e),l=i(e,"DIV",{class:!0});var d=T(l);g(P.$$.fragment,d),ze=r(d),G=i(d,"P",{"data-svelte-h":!0}),p(G)!=="svelte-b8riyv"&&(G.textContent=Ae),Be=r(d),Y=i(d,"P",{"data-svelte-h":!0}),p(Y)!=="svelte-1c3t5ty"&&(Y.innerHTML=Ge),Ie=r(d),$=i(d,"DIV",{class:!0});var Te=T($);g(H.$$.fragment,Te),qe=r(Te),O=i(Te,"P",{"data-svelte-h":!0}),p(O)!=="svelte-ooaeix"&&(O.textContent=Ye),Te.forEach(n),Ce=r(d),w=i(d,"DIV",{class:!0});var ae=T(w);g(Q.$$.fragment,ae),Ue=r(ae),K=i(ae,"P",{"data-svelte-h":!0}),p(K)!=="svelte-zjm6uf"&&(K.textContent=Oe),Ee=r(ae),ee=i(ae,"UL",{"data-svelte-h":!0}),p(ee)!=="svelte-rq8uot"&&(ee.innerHTML=Ke),ae.forEach(n),je=r(d),x=i(d,"DIV",{class:!0});var ve=T(x);g(X.$$.fragment,ve),Re=r(ve),te=i(ve,"P",{"data-svelte-h":!0}),p(te)!=="svelte-b3k2yi"&&(te.textContent=et),ve.forEach(n),Ve=r(d),J=i(d,"DIV",{class:!0});var ye=T(J);g(D.$$.fragment,ye),Le=r(ye),ne=i(ye,"P",{"data-svelte-h":!0}),p(ne)!=="svelte-vpfvn5"&&(ne.textContent=tt),ye.forEach(n),Ze=r(d),M=i(d,"DIV",{class:!0});var $e=T(M);g(W.$$.fragment,$e),Fe=r($e),se=i($e,"P",{"data-svelte-h":!0}),p(se)!=="svelte-1f4f5kp"&&(se.innerHTML=nt),$e.forEach(n),Pe=r(d),z=i(d,"DIV",{class:!0});var xe=T(z);g(N.$$.fragment,xe),He=r(xe),oe=i(xe,"P",{"data-svelte-h":!0}),p(oe)!=="svelte-1jdrmaw"&&(oe.textContent=st),xe.forEach(n),Qe=r(d),B=i(d,"DIV",{class:!0});var Je=T(B);g(S.$$.fragment,Je),Xe=r(Je),re=i(Je,"P",{"data-svelte-h":!0}),p(re)!=="svelte-15su17z"&&(re.textContent=ot),Je.forEach(n),d.forEach(n),be=r(e),ie=i(e,"P",{}),T(ie).forEach(n),this.h()},h(){v(c,"name","hf:doc:metadata"),v(c,"content",ut),v($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(l,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){t(document.head,c),m(e,q,s),m(e,b,s),m(e,A,s),u(C,e,s),m(e,de,s),u(U,e,s),m(e,me,s),m(e,E,s),m(e,ce,s),m(e,j,s),m(e,pe,s),m(e,R,s),m(e,fe,s),m(e,V,s),m(e,ge,s),u(L,e,s),m(e,ue,s),u(Z,e,s),m(e,he,s),u(y,e,s),m(e,ke,s),u(F,e,s),m(e,_e,s),m(e,l,s),u(P,l,null),t(l,ze),t(l,G),t(l,Be),t(l,Y),t(l,Ie),t(l,$),u(H,$,null),t($,qe),t($,O),t(l,Ce),t(l,w),u(Q,w,null),t(w,Ue),t(w,K),t(w,Ee),t(w,ee),t(l,je),t(l,x),u(X,x,null),t(x,Re),t(x,te),t(l,Ve),t(l,J),u(D,J,null),t(J,Le),t(J,ne),t(l,Ze),t(l,M),u(W,M,null),t(M,Fe),t(M,se),t(l,Pe),t(l,z),u(N,z,null),t(z,He),t(z,oe),t(l,Qe),t(l,B),u(S,B,null),t(B,Xe),t(B,re),m(e,be,s),m(e,ie,s),we=!0},p(e,[s]){const d={};s&2&&(d.$$scope={dirty:s,ctx:e}),y.$set(d)},i(e){we||(h(C.$$.fragment,e),h(U.$$.fragment,e),h(L.$$.fragment,e),h(Z.$$.fragment,e),h(y.$$.fragment,e),h(F.$$.fragment,e),h(P.$$.fragment,e),h(H.$$.fragment,e),h(Q.$$.fragment,e),h(X.$$.fragment,e),h(D.$$.fragment,e),h(W.$$.fragment,e),h(N.$$.fragment,e),h(S.$$.fragment,e),we=!0)},o(e){k(C.$$.fragment,e),k(U.$$.fragment,e),k(L.$$.fragment,e),k(Z.$$.fragment,e),k(y.$$.fragment,e),k(F.$$.fragment,e),k(P.$$.fragment,e),k(H.$$.fragment,e),k(Q.$$.fragment,e),k(X.$$.fragment,e),k(D.$$.fragment,e),k(W.$$.fragment,e),k(N.$$.fragment,e),k(S.$$.fragment,e),we=!1},d(e){e&&(n(q),n(b),n(A),n(de),n(me),n(E),n(ce),n(j),n(pe),n(R),n(fe),n(V),n(ge),n(ue),n(he),n(ke),n(_e),n(l),n(be),n(ie)),n(c),_(C,e),_(U,e),_(L,e),_(Z,e),_(y,e),_(F,e),_(P),_(H),_(Q),_(X),_(D),_(W),_(N),_(S)}}}const ut='{"title":"BERTweet","local":"bertweet","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"BertweetTokenizer","local":"transformers.BertweetTokenizer","sections":[],"depth":2}],"depth":1}';function ht(le){return at(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class yt extends lt{constructor(c){super(),dt(this,c,ht,gt,rt,{})}}export{yt as component};
