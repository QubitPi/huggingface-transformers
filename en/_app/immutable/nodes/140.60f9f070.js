import{s as Rs,f as Xs,o as Ns,n as H}from"../chunks/scheduler.9bc65507.js";import{S as Es,i as Ls,g as m,s as a,r as f,A as Ps,h as p,f as o,c as r,j as F,u as g,x as u,k as I,y as d,a as l,v as _,d as M,t as y,w as b}from"../chunks/index.707bf1b6.js";import{T as ls}from"../chunks/Tip.c2ecdbf4.js";import{D as P}from"../chunks/Docstring.17db21ae.js";import{C as We}from"../chunks/CodeBlock.54a9f38d.js";import{E as ke}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as q}from"../chunks/Heading.342b1fa6.js";function qs(v){let s,w="Example:",c,i,h;return i=new We({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdpdFZpc2lvbkNvbmZpZyUyQyUyMEdpdFZpc2lvbk1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEdpdFZpc2lvbkNvbmZpZyUyMHdpdGglMjBtaWNyb3NvZnQlMkZnaXQtYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBHaXRWaXNpb25Db25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBHaXRWaXNpb25Nb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwbWljcm9zb2Z0JTJGZ2l0LWJhc2UlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEdpdFZpc2lvbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GitVisionConfig, GitVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GitVisionConfig with microsoft/git-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = GitVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GitVisionModel (with random weights) from the microsoft/git-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GitVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){s=m("p"),s.textContent=w,c=a(),f(i.$$.fragment)},l(t){s=p(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-11lpom8"&&(s.textContent=w),c=r(t),g(i.$$.fragment,t)},m(t,T){l(t,s,T),l(t,c,T),_(i,t,T),h=!0},p:H,i(t){h||(M(i.$$.fragment,t),h=!0)},o(t){y(i.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(i,t)}}}function Hs(v){let s,w=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=m("p"),s.innerHTML=w},l(c){s=p(c,"P",{"data-svelte-h":!0}),u(s)!=="svelte-fincs2"&&(s.innerHTML=w)},m(c,i){l(c,s,i)},p:H,d(c){c&&o(s)}}}function Qs(v){let s,w="Examples:",c,i,h;return i=new We({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEdpdFZpc2lvbk1vZGVsJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGZ2l0LWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBHaXRWaXNpb25Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGZ2l0LWJhc2UlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, GitVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GitVisionModel.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state`,wrap:!1}}),{c(){s=m("p"),s.textContent=w,c=a(),f(i.$$.fragment)},l(t){s=p(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-kvfsh7"&&(s.textContent=w),c=r(t),g(i.$$.fragment,t)},m(t,T){l(t,s,T),l(t,c,T),_(i,t,T),h=!0},p:H,i(t){h||(M(i.$$.fragment,t),h=!0)},o(t){y(i.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(i,t)}}}function Ys(v){let s,w="Examples:",c,i,h;return i=new We({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdpdENvbmZpZyUyQyUyMEdpdE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEdJVCUyMG1pY3Jvc29mdCUyRmdpdC1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEdpdENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBtaWNyb3NvZnQlMkZnaXQtYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwR2l0TW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GitConfig, GitModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GIT microsoft/git-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = GitConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the microsoft/git-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GitModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){s=m("p"),s.textContent=w,c=a(),f(i.$$.fragment)},l(t){s=p(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-kvfsh7"&&(s.textContent=w),c=r(t),g(i.$$.fragment,t)},m(t,T){l(t,s,T),l(t,c,T),_(i,t,T),h=!0},p:H,i(t){h||(M(i.$$.fragment,t),h=!0)},o(t){y(i.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(i,t)}}}function Ss(v){let s,w=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=m("p"),s.innerHTML=w},l(c){s=p(c,"P",{"data-svelte-h":!0}),u(s)!=="svelte-fincs2"&&(s.innerHTML=w)},m(c,i){l(c,s,i)},p:H,d(c){c&&o(s)}}}function As(v){let s,w="Examples:",c,i,h;return i=new We({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWwlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmdpdC1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZnaXQtYmFzZSUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBdGV4dCUyMCUzRCUyMCUyMnRoaXMlMjBpcyUyMGFuJTIwaW1hZ2UlMjBvZiUyMHR3byUyMGNhdHMlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IodGV4dCUyQyUyMGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGUlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;this is an image of two cats&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state`,wrap:!1}}),{c(){s=m("p"),s.textContent=w,c=a(),f(i.$$.fragment)},l(t){s=p(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-kvfsh7"&&(s.textContent=w),c=r(t),g(i.$$.fragment,t)},m(t,T){l(t,s,T),l(t,c,T),_(i,t,T),h=!0},p:H,i(t){h||(M(i.$$.fragment,t),h=!0)},o(t){y(i.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(i,t)}}}function Ds(v){let s,w=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=m("p"),s.innerHTML=w},l(c){s=p(c,"P",{"data-svelte-h":!0}),u(s)!=="svelte-fincs2"&&(s.innerHTML=w)},m(c,i){l(c,s,i)},p:H,d(c){c&&o(s)}}}function Os(v){let s,w="Image captioning example:",c,i,h;return i=new We({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGZ2l0LWJhc2UtY29jbyUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZnaXQtYmFzZS1jb2NvJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFwaXhlbF92YWx1ZXMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5waXhlbF92YWx1ZXMlMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUocGl4ZWxfdmFsdWVzJTNEcGl4ZWxfdmFsdWVzJTJDJTIwbWF4X2xlbmd0aCUzRDUwKSUwQWdlbmVyYXRlZF9jYXB0aW9uJTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCUwQXByaW50KGdlbmVyYXRlZF9jYXB0aW9uKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-coco&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-coco&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(pixel_values=pixel_values, max_length=<span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_caption)
two cats sleeping on a pink blanket <span class="hljs-built_in">next</span> to remotes.`,wrap:!1}}),{c(){s=m("p"),s.textContent=w,c=a(),f(i.$$.fragment)},l(t){s=p(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-jwd2q3"&&(s.textContent=w),c=r(t),g(i.$$.fragment,t)},m(t,T){l(t,s,T),l(t,c,T),_(i,t,T),h=!0},p:H,i(t){h||(M(i.$$.fragment,t),h=!0)},o(t){y(i.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(i,t)}}}function Ks(v){let s,w="Visual question answering (VQA) example:",c,i,h;return i=new We({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQWZyb20lMjBodWdnaW5nZmFjZV9odWIlMjBpbXBvcnQlMjBoZl9odWJfZG93bmxvYWQlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZnaXQtYmFzZS10ZXh0dnFhJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmdpdC1iYXNlLXRleHR2cWElMjIpJTBBJTBBZmlsZV9wYXRoJTIwJTNEJTIwaGZfaHViX2Rvd25sb2FkKHJlcG9faWQlM0QlMjJuaWVsc3IlMkZ0ZXh0dnFhLXNhbXBsZSUyMiUyQyUyMGZpbGVuYW1lJTNEJTIyYnVzLnBuZyUyMiUyQyUyMHJlcG9fdHlwZSUzRCUyMmRhdGFzZXQlMjIpJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKGZpbGVfcGF0aCkuY29udmVydCglMjJSR0IlMjIpJTBBJTBBcGl4ZWxfdmFsdWVzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikucGl4ZWxfdmFsdWVzJTBBJTBBcXVlc3Rpb24lMjAlM0QlMjAlMjJ3aGF0JTIwZG9lcyUyMHRoZSUyMGZyb250JTIwb2YlMjB0aGUlMjBidXMlMjBzYXklMjBhdCUyMHRoZSUyMHRvcCUzRiUyMiUwQSUwQWlucHV0X2lkcyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEcXVlc3Rpb24lMkMlMjBhZGRfc3BlY2lhbF90b2tlbnMlM0RGYWxzZSkuaW5wdXRfaWRzJTBBaW5wdXRfaWRzJTIwJTNEJTIwJTVCcHJvY2Vzc29yLnRva2VuaXplci5jbHNfdG9rZW5faWQlNUQlMjAlMkIlMjBpbnB1dF9pZHMlMEFpbnB1dF9pZHMlMjAlM0QlMjB0b3JjaC50ZW5zb3IoaW5wdXRfaWRzKS51bnNxdWVlemUoMCklMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUocGl4ZWxfdmFsdWVzJTNEcGl4ZWxfdmFsdWVzJTJDJTIwaW5wdXRfaWRzJTNEaW5wdXRfaWRzJTJDJTIwbWF4X2xlbmd0aCUzRDUwKSUwQXByaW50KHByb2Nlc3Nvci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModelForCausalLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-textvqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-textvqa&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>file_path = hf_hub_download(repo_id=<span class="hljs-string">&quot;nielsr/textvqa-sample&quot;</span>, filename=<span class="hljs-string">&quot;bus.png&quot;</span>, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(file_path).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values

<span class="hljs-meta">&gt;&gt;&gt; </span>question = <span class="hljs-string">&quot;what does the front of the bus say at the top?&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = processor(text=question, add_special_tokens=<span class="hljs-literal">False</span>).input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = [processor.tokenizer.cls_token_id] + input_ids
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor(input_ids).unsqueeze(<span class="hljs-number">0</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_length=<span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>))
[<span class="hljs-string">&#x27;what does the front of the bus say at the top? special&#x27;</span>]`,wrap:!1}}),{c(){s=m("p"),s.textContent=w,c=a(),f(i.$$.fragment)},l(t){s=p(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-1yp8bv1"&&(s.textContent=w),c=r(t),g(i.$$.fragment,t)},m(t,T){l(t,s,T),l(t,c,T),_(i,t,T),h=!0},p:H,i(t){h||(M(i.$$.fragment,t),h=!0)},o(t){y(i.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(i,t)}}}function en(v){let s,w="Video captioning example:",c,i,h;return i=new We({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> av
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-vatex&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;microsoft/git-base-vatex&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># set seed for reproducability</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>np.random.seed(<span class="hljs-number">45</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">read_video_pyav</span>(<span class="hljs-params">container, indices</span>):
<span class="hljs-meta">... </span>    <span class="hljs-string">&#x27;&#x27;&#x27;
<span class="hljs-meta">... </span>    Decode the video with PyAV decoder.
<span class="hljs-meta">... </span>    Args:
<span class="hljs-meta">... </span>        container (\`av.container.input.InputContainer\`): PyAV container.
<span class="hljs-meta">... </span>        indices (\`List[int]\`): List of frame indices to decode.
<span class="hljs-meta">... </span>    Returns:
<span class="hljs-meta">... </span>        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).
<span class="hljs-meta">... </span>    &#x27;&#x27;&#x27;</span>
<span class="hljs-meta">... </span>    frames = []
<span class="hljs-meta">... </span>    container.seek(<span class="hljs-number">0</span>)
<span class="hljs-meta">... </span>    start_index = indices[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    end_index = indices[-<span class="hljs-number">1</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> i, frame <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(container.decode(video=<span class="hljs-number">0</span>)):
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i &gt; end_index:
<span class="hljs-meta">... </span>            <span class="hljs-keyword">break</span>
<span class="hljs-meta">... </span>        <span class="hljs-keyword">if</span> i &gt;= start_index <span class="hljs-keyword">and</span> i <span class="hljs-keyword">in</span> indices:
<span class="hljs-meta">... </span>            frames.append(frame)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> np.stack([x.to_ndarray(<span class="hljs-built_in">format</span>=<span class="hljs-string">&quot;rgb24&quot;</span>) <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> frames])


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">sample_frame_indices</span>(<span class="hljs-params">clip_len, frame_sample_rate, seg_len</span>):
<span class="hljs-meta">... </span>    <span class="hljs-string">&#x27;&#x27;&#x27;
<span class="hljs-meta">... </span>    Sample a given number of frame indices from the video.
<span class="hljs-meta">... </span>    Args:
<span class="hljs-meta">... </span>        clip_len (\`int\`): Total number of frames to sample.
<span class="hljs-meta">... </span>        frame_sample_rate (\`int\`): Sample every n-th frame.
<span class="hljs-meta">... </span>        seg_len (\`int\`): Maximum allowed index of sample&#x27;s last frame.
<span class="hljs-meta">... </span>    Returns:
<span class="hljs-meta">... </span>        indices (\`List[int]\`): List of sampled frame indices
<span class="hljs-meta">... </span>    &#x27;&#x27;&#x27;</span>
<span class="hljs-meta">... </span>    converted_len = <span class="hljs-built_in">int</span>(clip_len * frame_sample_rate)
<span class="hljs-meta">... </span>    end_idx = np.random.randint(converted_len, seg_len)
<span class="hljs-meta">... </span>    start_idx = end_idx - converted_len
<span class="hljs-meta">... </span>    indices = np.linspace(start_idx, end_idx, num=clip_len)
<span class="hljs-meta">... </span>    indices = np.clip(indices, start_idx, end_idx - <span class="hljs-number">1</span>).astype(np.int64)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> indices


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># load video</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>file_path = hf_hub_download(
<span class="hljs-meta">... </span>    repo_id=<span class="hljs-string">&quot;nielsr/video-demo&quot;</span>, filename=<span class="hljs-string">&quot;eating_spaghetti.mp4&quot;</span>, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>container = av.<span class="hljs-built_in">open</span>(file_path)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># sample frames</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_frames = model.config.num_image_with_embedding
<span class="hljs-meta">&gt;&gt;&gt; </span>indices = sample_frame_indices(
<span class="hljs-meta">... </span>    clip_len=num_frames, frame_sample_rate=<span class="hljs-number">4</span>, seg_len=container.streams.video[<span class="hljs-number">0</span>].frames
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>frames = read_video_pyav(container, indices)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = processor(images=<span class="hljs-built_in">list</span>(frames), return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(pixel_values=pixel_values, max_length=<span class="hljs-number">50</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Generated caption:&quot;</span>, processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>))
Generated caption: [<span class="hljs-string">&#x27;a woman is sitting at a table and she is talking about the food she is holding.&#x27;</span>]`,wrap:!1}}),{c(){s=m("p"),s.textContent=w,c=a(),f(i.$$.fragment)},l(t){s=p(t,"P",{"data-svelte-h":!0}),u(s)!=="svelte-sx5and"&&(s.textContent=w),c=r(t),g(i.$$.fragment,t)},m(t,T){l(t,s,T),l(t,c,T),_(i,t,T),h=!0},p:H,i(t){h||(M(i.$$.fragment,t),h=!0)},o(t){y(i.$$.fragment,t),h=!1},d(t){t&&(o(s),o(c)),b(i,t)}}}function tn(v){let s,w,c,i,h,t,T,st,ae,ds=`The GIT model was proposed in <a href="https://arxiv.org/abs/2205.14100" rel="nofollow">GIT: A Generative Image-to-text Transformer for Vision and Language</a> by
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang. GIT is a decoder-only Transformer
that leverages <a href="clip">CLIP</a>’s vision encoder to condition the model on vision inputs besides text. The model obtains state-of-the-art results on
image captioning and visual question answering benchmarks.`,nt,re,cs="The abstract from the paper is the following:",ot,ie,ms="<em>In this paper, we design and train a Generative Image-to-text Transformer, GIT, to unify vision-language tasks such as image/video captioning and question answering. While generative models provide a consistent network architecture between pre-training and fine-tuning, existing work typically contains complex structures (uni/multi-modal encoder/decoder) and depends on external modules such as object detectors/taggers and optical character recognition (OCR). In GIT, we simplify the architecture as one image encoder and one text decoder under a single language modeling task. We also scale up the pre-training data and the model size to boost the model performance. Without bells and whistles, our GIT establishes new state of the arts on 12 challenging benchmarks with a large margin. For instance, our model surpasses the human performance for the first time on TextCaps (138.2 vs. 125.5 in CIDEr). Furthermore, we present a new scheme of generation-based image classification and scene text recognition, achieving decent performance on standard benchmarks.</em>",at,Q,ps,rt,le,hs='GIT architecture. Taken from the <a href="https://arxiv.org/abs/2205.14100" target="_blank">original paper</a>.',it,de,us=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/microsoft/GenerativeImage2Text" rel="nofollow">here</a>.`,lt,ce,dt,me,fs="<li>GIT is implemented in a very similar way to GPT-2, the only difference being that the model is also conditioned on <code>pixel_values</code>.</li>",ct,pe,mt,he,gs="A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with GIT.",pt,ue,_s='<li>Demo notebooks regarding inference + fine-tuning GIT on custom data can be found <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/GIT" rel="nofollow">here</a>.</li> <li>See also: <a href="../tasks/language_modeling">Causal language modeling task guide</a></li>',ht,fe,Ms=`If you’re interested in submitting a resource to be included here, please feel free to open a Pull Request and we will review it.
The resource should ideally demonstrate something new instead of duplicating an existing resource.`,ut,ge,ft,Z,_e,Ut,Be,ys=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/git#transformers.GitVisionModel">GitVisionModel</a>. It is used to instantiate a GIT
vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the vision encoder of the GIT
<a href="https://huggingface.co/microsoft/git-base" rel="nofollow">microsoft/git-base</a> architecture.`,It,ze,bs=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Zt,Y,gt,Me,_t,G,ye,$t,Ve,Ts="The vision model from CLIP, used in GIT, without any head or projection on top.",xt,Fe,ws=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,kt,Re,vs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Wt,z,be,Bt,Xe,Js='The <a href="/docs/transformers/main/en/model_doc/git#transformers.GitVisionModel">GitVisionModel</a> forward method, overrides the <code>__call__</code> special method.',zt,S,Vt,A,Mt,Te,yt,$,we,Ft,Ne,js=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/git#transformers.GitModel">GitModel</a>. It is used to instantiate a GIT model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the GIT
<a href="https://huggingface.co/microsoft/git-base" rel="nofollow">microsoft/git-base</a> architecture.`,Rt,Ee,Gs=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Xt,D,bt,ve,Tt,x,Je,Nt,Le,Cs="Constructs a GIT processor which wraps a CLIP image processor and a BERT tokenizer into a single processor.",Et,Pe,Us=`<a href="/docs/transformers/main/en/model_doc/git#transformers.GitProcessor">GitProcessor</a> offers all the functionalities of <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> and <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>. See the
<a href="/docs/transformers/main/en/model_doc/git#transformers.GitProcessor.__call__"><strong>call</strong>()</a> and <code>decode()</code> for more information.`,Lt,O,je,Pt,qe,Is=`Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the <code>text</code>
and <code>kwargs</code> arguments to BertTokenizerFast’s <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__"><strong>call</strong>()</a> if <code>text</code> is not <code>None</code> to encode
the text. To prepare the image(s), this method forwards the <code>images</code> and <code>kwrags</code> arguments to
CLIPImageProcessor’s <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__"><strong>call</strong>()</a> if <code>images</code> is not <code>None</code>. Please refer to the doctsring
of the above two methods for more information.`,wt,Ge,vt,C,Ce,qt,He,Zs="The bare GIT Model transformer consisting of a CLIP image encoder and text decoder outputting raw hidden-states without any specific head on top.",Ht,Qe,$s=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Qt,Ye,xs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Yt,V,Ue,St,Se,ks='The <a href="/docs/transformers/main/en/model_doc/git#transformers.GitModel">GitModel</a> forward method, overrides the <code>__call__</code> special method.',At,K,Dt,ee,Jt,Ie,jt,U,Ze,Ot,Ae,Ws="GIT Model with a <code>language modeling</code> head on top for autoregressive language modeling.",Kt,De,Bs=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,es,Oe,zs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ts,J,$e,ss,Ke,Vs='The <a href="/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',ns,te,os,et,Fs="Examples:",as,se,rs,ne,is,oe,Gt,tt,Ct;return h=new q({props:{title:"GIT",local:"git",headingTag:"h1"}}),T=new q({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ce=new q({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),pe=new q({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ge=new q({props:{title:"GitVisionConfig",local:"transformers.GitVisionConfig",headingTag:"h2"}}),_e=new P({props:{name:"class transformers.GitVisionConfig",anchor:"transformers.GitVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GitVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GitVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.GitVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.GitVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.GitVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.GitVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.GitVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.GitVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.GitVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.GitVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/git/configuration_git.py#L30"}}),Y=new ke({props:{anchor:"transformers.GitVisionConfig.example",$$slots:{default:[qs]},$$scope:{ctx:v}}}),Me=new q({props:{title:"GitVisionModel",local:"transformers.GitVisionModel",headingTag:"h2"}}),ye=new P({props:{name:"class transformers.GitVisionModel",anchor:"transformers.GitVisionModel",parameters:[{name:"config",val:": GitVisionConfig"}],parametersDescription:[{anchor:"transformers.GitVisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/git#transformers.GitConfig">GitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/git/modeling_git.py#L962"}}),be=new P({props:{name:"forward",anchor:"transformers.GitVisionModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.GitVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.GitVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GitVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GitVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/git/modeling_git.py#L980",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.git.configuration_git.GitVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),S=new ls({props:{$$slots:{default:[Hs]},$$scope:{ctx:v}}}),A=new ke({props:{anchor:"transformers.GitVisionModel.forward.example",$$slots:{default:[Qs]},$$scope:{ctx:v}}}),Te=new q({props:{title:"GitConfig",local:"transformers.GitConfig",headingTag:"h2"}}),we=new P({props:{name:"class transformers.GitConfig",anchor:"transformers.GitConfig",parameters:[{name:"vision_config",val:" = None"},{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 6"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 1024"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"use_cache",val:" = True"},{name:"tie_word_embeddings",val:" = False"},{name:"bos_token_id",val:" = 101"},{name:"eos_token_id",val:" = 102"},{name:"num_image_with_embedding",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GitConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/git#transformers.GitVisionConfig">GitVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.GitConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the GIT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/git#transformers.GitModel">GitModel</a>.`,name:"vocab_size"},{anchor:"transformers.GitConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GitConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.GitConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.GitConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.GitConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.GitConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.GitConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.GitConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.GitConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.GitConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.GitConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.GitConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.GitConfig.num_image_with_embedding",description:`<strong>num_image_with_embedding</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of temporal embeddings to add, in case the model is used for video captioning/VQA.`,name:"num_image_with_embedding"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/git/configuration_git.py#L128"}}),D=new ke({props:{anchor:"transformers.GitConfig.example",$$slots:{default:[Ys]},$$scope:{ctx:v}}}),ve=new q({props:{title:"GitProcessor",local:"transformers.GitProcessor",headingTag:"h2"}}),Je=new P({props:{name:"class transformers.GitProcessor",anchor:"transformers.GitProcessor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.GitProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.GitProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/git/processing_git.py#L23"}}),je=new P({props:{name:"__call__",anchor:"transformers.GitProcessor.__call__",parameters:[{name:"text",val:" = None"},{name:"images",val:" = None"},{name:"return_tensors",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GitProcessor.__call__.text",description:`<strong>text</strong> (<code>str</code>, <code>List[str]</code>, <code>List[List[str]]</code>) &#x2014;
The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).`,name:"text"},{anchor:"transformers.GitProcessor.__call__.images",description:`<strong>images</strong> (<code>PIL.Image.Image</code>, <code>np.ndarray</code>, <code>torch.Tensor</code>, <code>List[PIL.Image.Image]</code>, <code>List[np.ndarray]</code>, <code>List[torch.Tensor]</code>) &#x2014;
The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch
tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a
number of channels, H and W are image height and width.`,name:"images"},{anchor:"transformers.GitProcessor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
<li><code>&apos;jax&apos;</code>: Return JAX <code>jnp.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/git/processing_git.py#L45",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a> with the following fields:</p>
<ul>
<li><strong>input_ids</strong> — List of token ids to be fed to a model. Returned when <code>text</code> is not <code>None</code>.</li>
<li><strong>attention_mask</strong> — List of indices specifying which tokens should be attended to by the model (when
<code>return_attention_mask=True</code> or if <em>“attention_mask”</em> is in <code>self.model_input_names</code> and if <code>text</code> is not
<code>None</code>).</li>
<li><strong>pixel_values</strong> — Pixel values to be fed to a model. Returned when <code>images</code> is not <code>None</code>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a></p>
`}}),Ge=new q({props:{title:"GitModel",local:"transformers.GitModel",headingTag:"h2"}}),Ce=new P({props:{name:"class transformers.GitModel",anchor:"transformers.GitModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.GitModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/git#transformers.GitConfig">GitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/git/modeling_git.py#L1033"}}),Ue=new P({props:{name:"forward",anchor:"transformers.GitModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.GitModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GitModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GitModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GitModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.GitModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GitModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GitModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GitModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GitModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GitModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GitModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/git/modeling_git.py#L1128",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/git#transformers.GitConfig"
>GitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),K=new ls({props:{$$slots:{default:[Ss]},$$scope:{ctx:v}}}),ee=new ke({props:{anchor:"transformers.GitModel.forward.example",$$slots:{default:[As]},$$scope:{ctx:v}}}),Ie=new q({props:{title:"GitForCausalLM",local:"transformers.GitForCausalLM",headingTag:"h2"}}),Ze=new P({props:{name:"class transformers.GitForCausalLM",anchor:"transformers.GitForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.GitForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/git#transformers.GitConfig">GitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/git/modeling_git.py#L1297"}}),$e=new P({props:{name:"forward",anchor:"transformers.GitForCausalLM.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.GitForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.GitForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GitForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.GitForCausalLM.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.GitForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GitForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GitForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GitForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GitForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GitForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels n <code>[0, ..., config.vocab_size]</code>`,name:"labels"},{anchor:"transformers.GitForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GitForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/git/modeling_git.py#L1318",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/git#transformers.GitConfig"
>GitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),te=new ls({props:{$$slots:{default:[Ds]},$$scope:{ctx:v}}}),se=new ke({props:{anchor:"transformers.GitForCausalLM.forward.example",$$slots:{default:[Os]},$$scope:{ctx:v}}}),ne=new ke({props:{anchor:"transformers.GitForCausalLM.forward.example-2",$$slots:{default:[Ks]},$$scope:{ctx:v}}}),oe=new ke({props:{anchor:"transformers.GitForCausalLM.forward.example-3",$$slots:{default:[en]},$$scope:{ctx:v}}}),{c(){s=m("meta"),w=a(),c=m("p"),i=a(),f(h.$$.fragment),t=a(),f(T.$$.fragment),st=a(),ae=m("p"),ae.innerHTML=ds,nt=a(),re=m("p"),re.textContent=cs,ot=a(),ie=m("p"),ie.innerHTML=ms,at=a(),Q=m("img"),rt=a(),le=m("small"),le.innerHTML=hs,it=a(),de=m("p"),de.innerHTML=us,lt=a(),f(ce.$$.fragment),dt=a(),me=m("ul"),me.innerHTML=fs,ct=a(),f(pe.$$.fragment),mt=a(),he=m("p"),he.textContent=gs,pt=a(),ue=m("ul"),ue.innerHTML=_s,ht=a(),fe=m("p"),fe.textContent=Ms,ut=a(),f(ge.$$.fragment),ft=a(),Z=m("div"),f(_e.$$.fragment),Ut=a(),Be=m("p"),Be.innerHTML=ys,It=a(),ze=m("p"),ze.innerHTML=bs,Zt=a(),f(Y.$$.fragment),gt=a(),f(Me.$$.fragment),_t=a(),G=m("div"),f(ye.$$.fragment),$t=a(),Ve=m("p"),Ve.textContent=Ts,xt=a(),Fe=m("p"),Fe.innerHTML=ws,kt=a(),Re=m("p"),Re.innerHTML=vs,Wt=a(),z=m("div"),f(be.$$.fragment),Bt=a(),Xe=m("p"),Xe.innerHTML=Js,zt=a(),f(S.$$.fragment),Vt=a(),f(A.$$.fragment),Mt=a(),f(Te.$$.fragment),yt=a(),$=m("div"),f(we.$$.fragment),Ft=a(),Ne=m("p"),Ne.innerHTML=js,Rt=a(),Ee=m("p"),Ee.innerHTML=Gs,Xt=a(),f(D.$$.fragment),bt=a(),f(ve.$$.fragment),Tt=a(),x=m("div"),f(Je.$$.fragment),Nt=a(),Le=m("p"),Le.textContent=Cs,Et=a(),Pe=m("p"),Pe.innerHTML=Us,Lt=a(),O=m("div"),f(je.$$.fragment),Pt=a(),qe=m("p"),qe.innerHTML=Is,wt=a(),f(Ge.$$.fragment),vt=a(),C=m("div"),f(Ce.$$.fragment),qt=a(),He=m("p"),He.textContent=Zs,Ht=a(),Qe=m("p"),Qe.innerHTML=$s,Qt=a(),Ye=m("p"),Ye.innerHTML=xs,Yt=a(),V=m("div"),f(Ue.$$.fragment),St=a(),Se=m("p"),Se.innerHTML=ks,At=a(),f(K.$$.fragment),Dt=a(),f(ee.$$.fragment),Jt=a(),f(Ie.$$.fragment),jt=a(),U=m("div"),f(Ze.$$.fragment),Ot=a(),Ae=m("p"),Ae.innerHTML=Ws,Kt=a(),De=m("p"),De.innerHTML=Bs,es=a(),Oe=m("p"),Oe.innerHTML=zs,ts=a(),J=m("div"),f($e.$$.fragment),ss=a(),Ke=m("p"),Ke.innerHTML=Vs,ns=a(),f(te.$$.fragment),os=a(),et=m("p"),et.textContent=Fs,as=a(),f(se.$$.fragment),rs=a(),f(ne.$$.fragment),is=a(),f(oe.$$.fragment),Gt=a(),tt=m("p"),this.h()},l(e){const n=Ps("svelte-u9bgzb",document.head);s=p(n,"META",{name:!0,content:!0}),n.forEach(o),w=r(e),c=p(e,"P",{}),F(c).forEach(o),i=r(e),g(h.$$.fragment,e),t=r(e),g(T.$$.fragment,e),st=r(e),ae=p(e,"P",{"data-svelte-h":!0}),u(ae)!=="svelte-2ujlop"&&(ae.innerHTML=ds),nt=r(e),re=p(e,"P",{"data-svelte-h":!0}),u(re)!=="svelte-vfdo9a"&&(re.textContent=cs),ot=r(e),ie=p(e,"P",{"data-svelte-h":!0}),u(ie)!=="svelte-1vsq93p"&&(ie.innerHTML=ms),at=r(e),Q=p(e,"IMG",{src:!0,alt:!0,width:!0}),rt=r(e),le=p(e,"SMALL",{"data-svelte-h":!0}),u(le)!=="svelte-1ebqlq"&&(le.innerHTML=hs),it=r(e),de=p(e,"P",{"data-svelte-h":!0}),u(de)!=="svelte-euav7t"&&(de.innerHTML=us),lt=r(e),g(ce.$$.fragment,e),dt=r(e),me=p(e,"UL",{"data-svelte-h":!0}),u(me)!=="svelte-d8zlsq"&&(me.innerHTML=fs),ct=r(e),g(pe.$$.fragment,e),mt=r(e),he=p(e,"P",{"data-svelte-h":!0}),u(he)!=="svelte-15k8xp3"&&(he.textContent=gs),pt=r(e),ue=p(e,"UL",{"data-svelte-h":!0}),u(ue)!=="svelte-tex1qj"&&(ue.innerHTML=_s),ht=r(e),fe=p(e,"P",{"data-svelte-h":!0}),u(fe)!=="svelte-1rggs50"&&(fe.textContent=Ms),ut=r(e),g(ge.$$.fragment,e),ft=r(e),Z=p(e,"DIV",{class:!0});var R=F(Z);g(_e.$$.fragment,R),Ut=r(R),Be=p(R,"P",{"data-svelte-h":!0}),u(Be)!=="svelte-1vm6zyw"&&(Be.innerHTML=ys),It=r(R),ze=p(R,"P",{"data-svelte-h":!0}),u(ze)!=="svelte-o55m63"&&(ze.innerHTML=bs),Zt=r(R),g(Y.$$.fragment,R),R.forEach(o),gt=r(e),g(Me.$$.fragment,e),_t=r(e),G=p(e,"DIV",{class:!0});var k=F(G);g(ye.$$.fragment,k),$t=r(k),Ve=p(k,"P",{"data-svelte-h":!0}),u(Ve)!=="svelte-183x34s"&&(Ve.textContent=Ts),xt=r(k),Fe=p(k,"P",{"data-svelte-h":!0}),u(Fe)!=="svelte-6pahdo"&&(Fe.innerHTML=ws),kt=r(k),Re=p(k,"P",{"data-svelte-h":!0}),u(Re)!=="svelte-hswkmf"&&(Re.innerHTML=vs),Wt=r(k),z=p(k,"DIV",{class:!0});var X=F(z);g(be.$$.fragment,X),Bt=r(X),Xe=p(X,"P",{"data-svelte-h":!0}),u(Xe)!=="svelte-zyu9nd"&&(Xe.innerHTML=Js),zt=r(X),g(S.$$.fragment,X),Vt=r(X),g(A.$$.fragment,X),X.forEach(o),k.forEach(o),Mt=r(e),g(Te.$$.fragment,e),yt=r(e),$=p(e,"DIV",{class:!0});var N=F($);g(we.$$.fragment,N),Ft=r(N),Ne=p(N,"P",{"data-svelte-h":!0}),u(Ne)!=="svelte-268s0t"&&(Ne.innerHTML=js),Rt=r(N),Ee=p(N,"P",{"data-svelte-h":!0}),u(Ee)!=="svelte-o55m63"&&(Ee.innerHTML=Gs),Xt=r(N),g(D.$$.fragment,N),N.forEach(o),bt=r(e),g(ve.$$.fragment,e),Tt=r(e),x=p(e,"DIV",{class:!0});var E=F(x);g(Je.$$.fragment,E),Nt=r(E),Le=p(E,"P",{"data-svelte-h":!0}),u(Le)!=="svelte-ihjj5e"&&(Le.textContent=Cs),Et=r(E),Pe=p(E,"P",{"data-svelte-h":!0}),u(Pe)!=="svelte-1pr6x7q"&&(Pe.innerHTML=Us),Lt=r(E),O=p(E,"DIV",{class:!0});var xe=F(O);g(je.$$.fragment,xe),Pt=r(xe),qe=p(xe,"P",{"data-svelte-h":!0}),u(qe)!=="svelte-kcv8c4"&&(qe.innerHTML=Is),xe.forEach(o),E.forEach(o),wt=r(e),g(Ge.$$.fragment,e),vt=r(e),C=p(e,"DIV",{class:!0});var W=F(C);g(Ce.$$.fragment,W),qt=r(W),He=p(W,"P",{"data-svelte-h":!0}),u(He)!=="svelte-1rw2l1f"&&(He.textContent=Zs),Ht=r(W),Qe=p(W,"P",{"data-svelte-h":!0}),u(Qe)!=="svelte-6pahdo"&&(Qe.innerHTML=$s),Qt=r(W),Ye=p(W,"P",{"data-svelte-h":!0}),u(Ye)!=="svelte-hswkmf"&&(Ye.innerHTML=xs),Yt=r(W),V=p(W,"DIV",{class:!0});var L=F(V);g(Ue.$$.fragment,L),St=r(L),Se=p(L,"P",{"data-svelte-h":!0}),u(Se)!=="svelte-dz132d"&&(Se.innerHTML=ks),At=r(L),g(K.$$.fragment,L),Dt=r(L),g(ee.$$.fragment,L),L.forEach(o),W.forEach(o),Jt=r(e),g(Ie.$$.fragment,e),jt=r(e),U=p(e,"DIV",{class:!0});var B=F(U);g(Ze.$$.fragment,B),Ot=r(B),Ae=p(B,"P",{"data-svelte-h":!0}),u(Ae)!=="svelte-n4hc9w"&&(Ae.innerHTML=Ws),Kt=r(B),De=p(B,"P",{"data-svelte-h":!0}),u(De)!=="svelte-6pahdo"&&(De.innerHTML=Bs),es=r(B),Oe=p(B,"P",{"data-svelte-h":!0}),u(Oe)!=="svelte-hswkmf"&&(Oe.innerHTML=zs),ts=r(B),J=p(B,"DIV",{class:!0});var j=F(J);g($e.$$.fragment,j),ss=r(j),Ke=p(j,"P",{"data-svelte-h":!0}),u(Ke)!=="svelte-d6d735"&&(Ke.innerHTML=Vs),ns=r(j),g(te.$$.fragment,j),os=r(j),et=p(j,"P",{"data-svelte-h":!0}),u(et)!=="svelte-kvfsh7"&&(et.textContent=Fs),as=r(j),g(se.$$.fragment,j),rs=r(j),g(ne.$$.fragment,j),is=r(j),g(oe.$$.fragment,j),j.forEach(o),B.forEach(o),Gt=r(e),tt=p(e,"P",{}),F(tt).forEach(o),this.h()},h(){I(s,"name","hf:doc:metadata"),I(s,"content",sn),Xs(Q.src,ps="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/git_architecture.jpg")||I(Q,"src",ps),I(Q,"alt","drawing"),I(Q,"width","600"),I(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){d(document.head,s),l(e,w,n),l(e,c,n),l(e,i,n),_(h,e,n),l(e,t,n),_(T,e,n),l(e,st,n),l(e,ae,n),l(e,nt,n),l(e,re,n),l(e,ot,n),l(e,ie,n),l(e,at,n),l(e,Q,n),l(e,rt,n),l(e,le,n),l(e,it,n),l(e,de,n),l(e,lt,n),_(ce,e,n),l(e,dt,n),l(e,me,n),l(e,ct,n),_(pe,e,n),l(e,mt,n),l(e,he,n),l(e,pt,n),l(e,ue,n),l(e,ht,n),l(e,fe,n),l(e,ut,n),_(ge,e,n),l(e,ft,n),l(e,Z,n),_(_e,Z,null),d(Z,Ut),d(Z,Be),d(Z,It),d(Z,ze),d(Z,Zt),_(Y,Z,null),l(e,gt,n),_(Me,e,n),l(e,_t,n),l(e,G,n),_(ye,G,null),d(G,$t),d(G,Ve),d(G,xt),d(G,Fe),d(G,kt),d(G,Re),d(G,Wt),d(G,z),_(be,z,null),d(z,Bt),d(z,Xe),d(z,zt),_(S,z,null),d(z,Vt),_(A,z,null),l(e,Mt,n),_(Te,e,n),l(e,yt,n),l(e,$,n),_(we,$,null),d($,Ft),d($,Ne),d($,Rt),d($,Ee),d($,Xt),_(D,$,null),l(e,bt,n),_(ve,e,n),l(e,Tt,n),l(e,x,n),_(Je,x,null),d(x,Nt),d(x,Le),d(x,Et),d(x,Pe),d(x,Lt),d(x,O),_(je,O,null),d(O,Pt),d(O,qe),l(e,wt,n),_(Ge,e,n),l(e,vt,n),l(e,C,n),_(Ce,C,null),d(C,qt),d(C,He),d(C,Ht),d(C,Qe),d(C,Qt),d(C,Ye),d(C,Yt),d(C,V),_(Ue,V,null),d(V,St),d(V,Se),d(V,At),_(K,V,null),d(V,Dt),_(ee,V,null),l(e,Jt,n),_(Ie,e,n),l(e,jt,n),l(e,U,n),_(Ze,U,null),d(U,Ot),d(U,Ae),d(U,Kt),d(U,De),d(U,es),d(U,Oe),d(U,ts),d(U,J),_($e,J,null),d(J,ss),d(J,Ke),d(J,ns),_(te,J,null),d(J,os),d(J,et),d(J,as),_(se,J,null),d(J,rs),_(ne,J,null),d(J,is),_(oe,J,null),l(e,Gt,n),l(e,tt,n),Ct=!0},p(e,[n]){const R={};n&2&&(R.$$scope={dirty:n,ctx:e}),Y.$set(R);const k={};n&2&&(k.$$scope={dirty:n,ctx:e}),S.$set(k);const X={};n&2&&(X.$$scope={dirty:n,ctx:e}),A.$set(X);const N={};n&2&&(N.$$scope={dirty:n,ctx:e}),D.$set(N);const E={};n&2&&(E.$$scope={dirty:n,ctx:e}),K.$set(E);const xe={};n&2&&(xe.$$scope={dirty:n,ctx:e}),ee.$set(xe);const W={};n&2&&(W.$$scope={dirty:n,ctx:e}),te.$set(W);const L={};n&2&&(L.$$scope={dirty:n,ctx:e}),se.$set(L);const B={};n&2&&(B.$$scope={dirty:n,ctx:e}),ne.$set(B);const j={};n&2&&(j.$$scope={dirty:n,ctx:e}),oe.$set(j)},i(e){Ct||(M(h.$$.fragment,e),M(T.$$.fragment,e),M(ce.$$.fragment,e),M(pe.$$.fragment,e),M(ge.$$.fragment,e),M(_e.$$.fragment,e),M(Y.$$.fragment,e),M(Me.$$.fragment,e),M(ye.$$.fragment,e),M(be.$$.fragment,e),M(S.$$.fragment,e),M(A.$$.fragment,e),M(Te.$$.fragment,e),M(we.$$.fragment,e),M(D.$$.fragment,e),M(ve.$$.fragment,e),M(Je.$$.fragment,e),M(je.$$.fragment,e),M(Ge.$$.fragment,e),M(Ce.$$.fragment,e),M(Ue.$$.fragment,e),M(K.$$.fragment,e),M(ee.$$.fragment,e),M(Ie.$$.fragment,e),M(Ze.$$.fragment,e),M($e.$$.fragment,e),M(te.$$.fragment,e),M(se.$$.fragment,e),M(ne.$$.fragment,e),M(oe.$$.fragment,e),Ct=!0)},o(e){y(h.$$.fragment,e),y(T.$$.fragment,e),y(ce.$$.fragment,e),y(pe.$$.fragment,e),y(ge.$$.fragment,e),y(_e.$$.fragment,e),y(Y.$$.fragment,e),y(Me.$$.fragment,e),y(ye.$$.fragment,e),y(be.$$.fragment,e),y(S.$$.fragment,e),y(A.$$.fragment,e),y(Te.$$.fragment,e),y(we.$$.fragment,e),y(D.$$.fragment,e),y(ve.$$.fragment,e),y(Je.$$.fragment,e),y(je.$$.fragment,e),y(Ge.$$.fragment,e),y(Ce.$$.fragment,e),y(Ue.$$.fragment,e),y(K.$$.fragment,e),y(ee.$$.fragment,e),y(Ie.$$.fragment,e),y(Ze.$$.fragment,e),y($e.$$.fragment,e),y(te.$$.fragment,e),y(se.$$.fragment,e),y(ne.$$.fragment,e),y(oe.$$.fragment,e),Ct=!1},d(e){e&&(o(w),o(c),o(i),o(t),o(st),o(ae),o(nt),o(re),o(ot),o(ie),o(at),o(Q),o(rt),o(le),o(it),o(de),o(lt),o(dt),o(me),o(ct),o(mt),o(he),o(pt),o(ue),o(ht),o(fe),o(ut),o(ft),o(Z),o(gt),o(_t),o(G),o(Mt),o(yt),o($),o(bt),o(Tt),o(x),o(wt),o(vt),o(C),o(Jt),o(jt),o(U),o(Gt),o(tt)),o(s),b(h,e),b(T,e),b(ce,e),b(pe,e),b(ge,e),b(_e),b(Y),b(Me,e),b(ye),b(be),b(S),b(A),b(Te,e),b(we),b(D),b(ve,e),b(Je),b(je),b(Ge,e),b(Ce),b(Ue),b(K),b(ee),b(Ie,e),b(Ze),b($e),b(te),b(se),b(ne),b(oe)}}}const sn='{"title":"GIT","local":"git","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"GitVisionConfig","local":"transformers.GitVisionConfig","sections":[],"depth":2},{"title":"GitVisionModel","local":"transformers.GitVisionModel","sections":[],"depth":2},{"title":"GitConfig","local":"transformers.GitConfig","sections":[],"depth":2},{"title":"GitProcessor","local":"transformers.GitProcessor","sections":[],"depth":2},{"title":"GitModel","local":"transformers.GitModel","sections":[],"depth":2},{"title":"GitForCausalLM","local":"transformers.GitForCausalLM","sections":[],"depth":2}],"depth":1}';function nn(v){return Ns(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pn extends Es{constructor(s){super(),Ls(this,s,nn,tn,Rs,{})}}export{pn as component};
