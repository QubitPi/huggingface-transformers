import{s as So,f as xo,o as Lo,n as L}from"../chunks/scheduler.9bc65507.js";import{S as jo,i as Jo,g as m,s as i,r as g,A as Zo,h as p,f as s,c as l,j as v,u as f,x as T,k as $,y as c,a as d,v as u,d as h,t as _,w as b}from"../chunks/index.707bf1b6.js";import{T as xt}from"../chunks/Tip.c2ecdbf4.js";import{D as w}from"../chunks/Docstring.17db21ae.js";import{C as ee}from"../chunks/CodeBlock.54a9f38d.js";import{E as K}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as ko}from"../chunks/PipelineTag.44585822.js";import{H as Q}from"../chunks/Heading.342b1fa6.js";function Uo(I){let n,C="Example:",r,a,M;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdDb25maWclMkMlMjBDTElQU2VnTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ0NvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ01vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ01vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZyUwQSUwQSUyMyUyMFdlJTIwY2FuJTIwYWxzbyUyMGluaXRpYWxpemUlMjBhJTIwQ0xJUFNlZ0NvbmZpZyUyMGZyb20lMjBhJTIwQ0xJUFNlZ1RleHRDb25maWclMjBhbmQlMjBhJTIwQ0xJUFNlZ1Zpc2lvbkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQU2VnVGV4dCUyMGFuZCUyMENMSVBTZWdWaXNpb24lMjBjb25maWd1cmF0aW9uJTBBY29uZmlnX3RleHQlMjAlM0QlMjBDTElQU2VnVGV4dENvbmZpZygpJTBBY29uZmlnX3Zpc2lvbiUyMCUzRCUyMENMSVBTZWdWaXNpb25Db25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMENMSVBTZWdDb25maWcuZnJvbV90ZXh0X3Zpc2lvbl9jb25maWdzKGNvbmZpZ190ZXh0JTJDJTIwY29uZmlnX3Zpc2lvbik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegConfig, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a CLIPSegConfig from a CLIPSegTextConfig and a CLIPSegVisionConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegText and CLIPSegVision configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = CLIPSegTextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = CLIPSegVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-11lpom8"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,y){d(t,n,y),d(t,r,y),u(a,t,y),M=!0},p:L,i(t){M||(h(a.$$.fragment,t),M=!0)},o(t){_(a.$$.fragment,t),M=!1},d(t){t&&(s(n),s(r)),b(a,t)}}}function Wo(I){let n,C="Example:",r,a,M;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdUZXh0Q29uZmlnJTJDJTIwQ0xJUFNlZ1RleHRNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQU2VnVGV4dENvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnVGV4dENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMENMSVBTZWdUZXh0TW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMENJREFTJTJGY2xpcHNlZy1yZDY0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnVGV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegTextConfig, CLIPSegTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegTextConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegTextModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-11lpom8"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,y){d(t,n,y),d(t,r,y),u(a,t,y),M=!0},p:L,i(t){M||(h(a.$$.fragment,t),M=!0)},o(t){_(a.$$.fragment,t),M=!1},d(t){t&&(s(n),s(r)),b(a,t)}}}function zo(I){let n,C="Example:",r,a,M;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdWaXNpb25Db25maWclMkMlMjBDTElQU2VnVmlzaW9uTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ1Zpc2lvbkNvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnVmlzaW9uQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ1Zpc2lvbk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ1Zpc2lvbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegVisionConfig, CLIPSegVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegVisionConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegVisionModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-11lpom8"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,y){d(t,n,y),d(t,r,y),u(a,t,y),M=!0},p:L,i(t){M||(h(a.$$.fragment,t),M=!0)},o(t){_(a.$$.fragment,t),M=!1},d(t){t&&(s(n),s(r)),b(a,t)}}}function Bo(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),T(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:L,d(r){r&&s(n)}}}function Vo(I){let n,C="Examples:",r,a,M;return a=new ee({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdNb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMEElMjAlMjAlMjAlMjB0ZXh0JTNEJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcGFkZGluZyUzRFRydWUlMEEpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0c19wZXJfaW1hZ2UlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0c19wZXJfaW1hZ2UlMjAlMjAlMjMlMjB0aGlzJTIwaXMlMjB0aGUlMjBpbWFnZS10ZXh0JTIwc2ltaWxhcml0eSUyMHNjb3JlJTBBcHJvYnMlMjAlM0QlMjBsb2dpdHNfcGVyX2ltYWdlLnNvZnRtYXgoZGltJTNEMSklMjAlMjAlMjMlMjB3ZSUyMGNhbiUyMHRha2UlMjB0aGUlMjBzb2Z0bWF4JTIwdG8lMjBnZXQlMjB0aGUlMjBsYWJlbCUyMHByb2JhYmlsaXRpZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,y){d(t,n,y),d(t,r,y),u(a,t,y),M=!0},p:L,i(t){M||(h(a.$$.fragment,t),M=!0)},o(t){_(a.$$.fragment,t),M=!1},d(t){t&&(s(n),s(r)),b(a,t)}}}function No(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),T(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:L,d(r){r&&s(n)}}}function Fo(I){let n,C="Examples:",r,a,M;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQU2VnTW9kZWwlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJDSURBUyUyRmNsaXBzZWctcmQ2NC1yZWZpbmVkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJDSURBUyUyRmNsaXBzZWctcmQ2NC1yZWZpbmVkJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBdGV4dF9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF90ZXh0X2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,y){d(t,n,y),d(t,r,y),u(a,t,y),M=!0},p:L,i(t){M||(h(a.$$.fragment,t),M=!0)},o(t){_(a.$$.fragment,t),M=!1},d(t){t&&(s(n),s(r)),b(a,t)}}}function Eo(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),T(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:L,d(r){r&&s(n)}}}function Qo(I){let n,C="Examples:",r,a,M;return a=new ee({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdNb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBaW1hZ2VfZmVhdHVyZXMlMjAlM0QlMjBtb2RlbC5nZXRfaW1hZ2VfZmVhdHVyZXMoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,y){d(t,n,y),d(t,r,y),u(a,t,y),M=!0},p:L,i(t){M||(h(a.$$.fragment,t),M=!0)},o(t){_(a.$$.fragment,t),M=!1},d(t){t&&(s(n),s(r)),b(a,t)}}}function qo(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),T(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:L,d(r){r&&s(n)}}}function Ho(I){let n,C="Examples:",r,a,M;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQU2VnVGV4dE1vZGVsJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyQ0lEQVMlMkZjbGlwc2VnLXJkNjQtcmVmaW5lZCUyMiklMEFtb2RlbCUyMCUzRCUyMENMSVBTZWdUZXh0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGUlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBcG9vbGVkX291dHB1dCUyMCUzRCUyMG91dHB1dHMucG9vbGVyX291dHB1dCUyMCUyMCUyMyUyMHBvb2xlZCUyMChFT1MlMjB0b2tlbiklMjBzdGF0ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPSegTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegTextModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,y){d(t,n,y),d(t,r,y),u(a,t,y),M=!0},p:L,i(t){M||(h(a.$$.fragment,t),M=!0)},o(t){_(a.$$.fragment,t),M=!1},d(t){t&&(s(n),s(r)),b(a,t)}}}function Xo(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),T(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:L,d(r){r&&s(n)}}}function Ro(I){let n,C="Examples:",r,a,M;return a=new ee({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdWaXNpb25Nb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnVmlzaW9uTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQXBvb2xlZF9vdXRwdXQlMjAlM0QlMjBvdXRwdXRzLnBvb2xlcl9vdXRwdXQlMjAlMjAlMjMlMjBwb29sZWQlMjBDTFMlMjBzdGF0ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegVisionModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,y){d(t,n,y),d(t,r,y),u(a,t,y),M=!0},p:L,i(t){M||(h(a.$$.fragment,t),M=!0)},o(t){_(a.$$.fragment,t),M=!1},d(t){t&&(s(n),s(r)),b(a,t)}}}function Go(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),T(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:L,d(r){r&&s(n)}}}function Do(I){let n,C="Examples:",r,a,M;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBDTElQU2VnRm9ySW1hZ2VTZWdtZW50YXRpb24lMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnRm9ySW1hZ2VTZWdtZW50YXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQXRleHRzJTIwJTNEJTIwJTVCJTIyYSUyMGNhdCUyMiUyQyUyMCUyMmElMjByZW1vdGUlMjIlMkMlMjAlMjJhJTIwYmxhbmtldCUyMiU1RCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEdGV4dHMlMkMlMjBpbWFnZXMlM0QlNUJpbWFnZSU1RCUyMColMjBsZW4odGV4dHMpJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cyUwQXByaW50KGxvZ2l0cy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegForImageSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;a cat&quot;</span>, <span class="hljs-string">&quot;a remote&quot;</span>, <span class="hljs-string">&quot;a blanket&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=[image] * <span class="hljs-built_in">len</span>(texts), padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(logits.shape)
torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">352</span>, <span class="hljs-number">352</span>])`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),T(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,y){d(t,n,y),d(t,r,y),u(a,t,y),M=!0},p:L,i(t){M||(h(a.$$.fragment,t),M=!0)},o(t){_(a.$$.fragment,t),M=!1},d(t){t&&(s(n),s(r)),b(a,t)}}}function Ao(I){let n,C,r,a,M,t,y,jt,Te,On=`The CLIPSeg model was proposed in <a href="https://arxiv.org/abs/2112.10003" rel="nofollow">Image Segmentation Using Text and Image Prompts</a> by Timo LÃ¼ddecke
and Alexander Ecker. CLIPSeg adds a minimal decoder on top of a frozen <a href="clip">CLIP</a> model for zero- and one-shot image segmentation.`,Jt,Ie,Kn="The abstract from the paper is the following:",Zt,$e,eo=`<em>Image segmentation is usually addressed by training a
model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive
as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system
that can generate image segmentations based on arbitrary
prompts at test time. A prompt can be either a text or an
image. This approach enables us to create a unified model
(trained once) for three common segmentation tasks, which
come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation.
We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense
prediction. After training on an extended version of the
PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on
an additional image expressing the query. We analyze different variants of the latter image-based prompts in detail.
This novel hybrid input allows for dynamic adaptation not
only to the three segmentation tasks mentioned above, but
to any binary segmentation task where a text or image query
can be formulated. Finally, we find our system to adapt well
to generalized queries involving affordances or properties</em>`,kt,te,to,Ut,ve,no='CLIPSeg overview. Taken from the <a href="https://arxiv.org/abs/2112.10003">original paper.</a>',Wt,we,oo=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/timojl/clipseg" rel="nofollow">here</a>.`,zt,Pe,Bt,Se,so=`<li><a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> adds a decoder on top of <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. The latter is identical to <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a>.</li> <li><a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text
(provided to the model as <code>input_ids</code>) or an image (provided to the model as <code>conditional_pixel_values</code>). One can also provide custom
conditional embeddings (provided to the model as <code>conditional_embeddings</code>).</li>`,Vt,xe,Nt,Le,ao="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with CLIPSeg. If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",Ft,je,Et,Je,ro='<li>A notebook that illustrates <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb" rel="nofollow">zero-shot image segmentation with CLIPSeg</a>.</li>',Qt,Ze,qt,P,ke,ln,dt,io=`<a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to
instantiate a CLIPSeg model according to the specified arguments, defining the text model and vision model configs.
Instantiating a configuration with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,dn,ct,lo=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,cn,ne,mn,oe,Ue,pn,mt,co=`Instantiate a <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> (or a derived class) from clipseg text model configuration and clipseg vision
model configuration.`,Ht,We,Xt,j,ze,gn,pt,mo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,fn,gt,po=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,un,se,Rt,Be,Gt,J,Ve,hn,ft,go=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,_n,ut,fo=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,bn,ae,Dt,Ne,At,S,Fe,Mn,ht,uo="Constructs a CLIPSeg processor which wraps a CLIPSeg image processor and a CLIP tokenizer into a single processor.",Cn,_t,ho=`<a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> offers all the functionalities of <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> and <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegProcessor.decode">decode()</a> for more information.`,yn,re,Ee,Tn,bt,_o=`This method forwards all its arguments to CLIPTokenizerFastâ€™s <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.batch_decode">batch_decode()</a>. Please
refer to the docstring of this method for more information.`,In,ie,Qe,$n,Mt,bo=`This method forwards all its arguments to CLIPTokenizerFastâ€™s <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Tokenizer.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,Yt,qe,Ot,x,He,vn,Ct,Mo=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,wn,z,Xe,Pn,yt,Co='The <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> forward method, overrides the <code>__call__</code> special method.',Sn,le,xn,de,Ln,B,Re,jn,Tt,yo='The <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> forward method, overrides the <code>__call__</code> special method.',Jn,ce,Zn,me,kn,V,Ge,Un,It,To='The <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> forward method, overrides the <code>__call__</code> special method.',Wn,pe,zn,ge,Kt,De,en,Y,Ae,Bn,N,Ye,Vn,$t,Io='The <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegTextModel">CLIPSegTextModel</a> forward method, overrides the <code>__call__</code> special method.',Nn,fe,Fn,ue,tn,Oe,nn,O,Ke,En,F,et,Qn,vt,$o='The <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionModel">CLIPSegVisionModel</a> forward method, overrides the <code>__call__</code> special method.',qn,he,Hn,_e,on,tt,sn,Z,nt,Xn,wt,vo="CLIPSeg model with a Transformer-based decoder on top for zero-shot and one-shot image segmentation.",Rn,Pt,wo=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Gn,E,ot,Dn,St,Po='The <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> forward method, overrides the <code>__call__</code> special method.',An,be,Yn,Me,an,Lt,rn;return M=new Q({props:{title:"CLIPSeg",local:"clipseg",headingTag:"h1"}}),y=new Q({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Pe=new Q({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),xe=new Q({props:{title:"Resources",local:"resources",headingTag:"h2"}}),je=new ko({props:{pipeline:"image-segmentation"}}),Ze=new Q({props:{title:"CLIPSegConfig",local:"transformers.CLIPSegConfig",headingTag:"h2"}}),ke=new w({props:{name:"class transformers.CLIPSegConfig",anchor:"transformers.CLIPSegConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"extract_layers",val:" = [3, 6, 9]"},{name:"reduce_dim",val:" = 64"},{name:"decoder_num_attention_heads",val:" = 4"},{name:"decoder_attention_dropout",val:" = 0.0"},{name:"decoder_hidden_act",val:" = 'quick_gelu'"},{name:"decoder_intermediate_size",val:" = 2048"},{name:"conditional_layer",val:" = 0"},{name:"use_complex_transposed_convolution",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegTextConfig">CLIPSegTextConfig</a>.`,name:"text_config"},{anchor:"transformers.CLIPSegConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionConfig">CLIPSegVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.CLIPSegConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.CLIPSegConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original CLIPSeg implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.CLIPSegConfig.extract_layers",description:`<strong>extract_layers</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 6, 9]</code>) &#x2014;
Layers to extract when forwarding the query image through the frozen visual backbone of CLIP.`,name:"extract_layers"},{anchor:"transformers.CLIPSegConfig.reduce_dim",description:`<strong>reduce_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality to reduce the CLIP vision embedding.`,name:"reduce_dim"},{anchor:"transformers.CLIPSegConfig.decoder_num_attention_heads",description:`<strong>decoder_num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of attention heads in the decoder of CLIPSeg.`,name:"decoder_num_attention_heads"},{anchor:"transformers.CLIPSegConfig.decoder_attention_dropout",description:`<strong>decoder_attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"decoder_attention_dropout"},{anchor:"transformers.CLIPSegConfig.decoder_hidden_act",description:`<strong>decoder_hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"decoder_hidden_act"},{anchor:"transformers.CLIPSegConfig.decoder_intermediate_size",description:`<strong>decoder_intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layers in the Transformer decoder.`,name:"decoder_intermediate_size"},{anchor:"transformers.CLIPSegConfig.conditional_layer",description:`<strong>conditional_layer</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The layer to use of the Transformer encoder whose activations will be combined with the condition
embeddings using FiLM (Feature-wise Linear Modulation). If 0, the last layer is used.`,name:"conditional_layer"},{anchor:"transformers.CLIPSegConfig.use_complex_transposed_convolution",description:`<strong>use_complex_transposed_convolution</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a more complex transposed convolution in the decoder, enabling more fine-grained
segmentation.`,name:"use_complex_transposed_convolution"},{anchor:"transformers.CLIPSegConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L248"}}),ne=new K({props:{anchor:"transformers.CLIPSegConfig.example",$$slots:{default:[Uo]},$$scope:{ctx:I}}}),Ue=new w({props:{name:"from_text_vision_configs",anchor:"transformers.CLIPSegConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": CLIPSegTextConfig"},{name:"vision_config",val:": CLIPSegVisionConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L423",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig"
>CLIPSegConfig</a></p>
`}}),We=new Q({props:{title:"CLIPSegTextConfig",local:"transformers.CLIPSegTextConfig",headingTag:"h2"}}),ze=new w({props:{name:"class transformers.CLIPSegTextConfig",anchor:"transformers.CLIPSegTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 77"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the CLIPSeg text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>.`,name:"vocab_size"},{anchor:"transformers.CLIPSegTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPSegTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPSegTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPSegTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPSegTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 77) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.CLIPSegTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.CLIPSegTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.CLIPSegTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPSegTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPSegTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.CLIPSegTextConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.CLIPSegTextConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49406) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.CLIPSegTextConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49407) &#x2014;
End of stream token id.`,name:"eos_token_id"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L31"}}),se=new K({props:{anchor:"transformers.CLIPSegTextConfig.example",$$slots:{default:[Wo]},$$scope:{ctx:I}}}),Be=new Q({props:{title:"CLIPSegVisionConfig",local:"transformers.CLIPSegVisionConfig",headingTag:"h2"}}),Ve=new w({props:{name:"class transformers.CLIPSegVisionConfig",anchor:"transformers.CLIPSegVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPSegVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPSegVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPSegVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPSegVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.CLIPSegVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.CLIPSegVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.CLIPSegVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.CLIPSegVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.CLIPSegVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPSegVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPSegVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L143"}}),ae=new K({props:{anchor:"transformers.CLIPSegVisionConfig.example",$$slots:{default:[zo]},$$scope:{ctx:I}}}),Ne=new Q({props:{title:"CLIPSegProcessor",local:"transformers.CLIPSegProcessor",headingTag:"h2"}}),Fe=new w({props:{name:"class transformers.CLIPSegProcessor",anchor:"transformers.CLIPSegProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a>, <em>optional</em>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.CLIPSegProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L25"}}),Ee=new w({props:{name:"batch_decode",anchor:"transformers.CLIPSegProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L134"}}),Qe=new w({props:{name:"decode",anchor:"transformers.CLIPSegProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L141"}}),qe=new Q({props:{title:"CLIPSegModel",local:"transformers.CLIPSegModel",headingTag:"h2"}}),He=new w({props:{name:"class transformers.CLIPSegModel",anchor:"transformers.CLIPSegModel",parameters:[{name:"config",val:": CLIPSegConfig"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L924"}}),Xe=new w({props:{name:"forward",anchor:"transformers.CLIPSegModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"return_loss",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.CLIPSegModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1056",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) â€” Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) â€” The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) â€” The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) â€” The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>image_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) â€” The image embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
<li><strong>text_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>vision_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),le=new xt({props:{$$slots:{default:[Bo]},$$scope:{ctx:I}}}),de=new K({props:{anchor:"transformers.CLIPSegModel.forward.example",$$slots:{default:[Vo]},$$scope:{ctx:I}}}),Re=new w({props:{name:"get_text_features",anchor:"transformers.CLIPSegModel.get_text_features",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L960",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),ce=new xt({props:{$$slots:{default:[No]},$$scope:{ctx:I}}}),me=new K({props:{anchor:"transformers.CLIPSegModel.get_text_features.example",$$slots:{default:[Fo]},$$scope:{ctx:I}}}),Ge=new w({props:{name:"get_image_features",anchor:"transformers.CLIPSegModel.get_image_features",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1007",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),pe=new xt({props:{$$slots:{default:[Eo]},$$scope:{ctx:I}}}),ge=new K({props:{anchor:"transformers.CLIPSegModel.get_image_features.example",$$slots:{default:[Qo]},$$scope:{ctx:I}}}),De=new Q({props:{title:"CLIPSegTextModel",local:"transformers.CLIPSegTextModel",headingTag:"h2"}}),Ae=new w({props:{name:"class transformers.CLIPSegTextModel",anchor:"transformers.CLIPSegTextModel",parameters:[{name:"config",val:": CLIPSegTextConfig"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L757"}}),Ye=new w({props:{name:"forward",anchor:"transformers.CLIPSegTextModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L774",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new xt({props:{$$slots:{default:[qo]},$$scope:{ctx:I}}}),ue=new K({props:{anchor:"transformers.CLIPSegTextModel.forward.example",$$slots:{default:[Ho]},$$scope:{ctx:I}}}),Oe=new Q({props:{title:"CLIPSegVisionModel",local:"transformers.CLIPSegVisionModel",headingTag:"h2"}}),Ke=new w({props:{name:"class transformers.CLIPSegVisionModel",anchor:"transformers.CLIPSegVisionModel",parameters:[{name:"config",val:": CLIPSegVisionConfig"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L872"}}),et=new w({props:{name:"forward",anchor:"transformers.CLIPSegVisionModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L885",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),he=new xt({props:{$$slots:{default:[Xo]},$$scope:{ctx:I}}}),_e=new K({props:{anchor:"transformers.CLIPSegVisionModel.forward.example",$$slots:{default:[Ro]},$$scope:{ctx:I}}}),tt=new Q({props:{title:"CLIPSegForImageSegmentation",local:"transformers.CLIPSegForImageSegmentation",headingTag:"h2"}}),nt=new w({props:{name:"class transformers.CLIPSegForImageSegmentation",anchor:"transformers.CLIPSegForImageSegmentation",parameters:[{name:"config",val:": CLIPSegConfig"}],parametersDescription:[{anchor:"transformers.CLIPSegForImageSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1307"}}),ot=new w({props:{name:"forward",anchor:"transformers.CLIPSegForImageSegmentation.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"conditional_pixel_values",val:": Optional = None"},{name:"conditional_embeddings",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegForImageSegmentation.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1358",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) â€” Contrastive loss for image-text similarity.
â€¦</li>
<li><strong>vision_model_output</strong> (<code>BaseModelOutputWithPooling</code>) â€” The output of the <a
  href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),be=new xt({props:{$$slots:{default:[Go]},$$scope:{ctx:I}}}),Me=new K({props:{anchor:"transformers.CLIPSegForImageSegmentation.forward.example",$$slots:{default:[Do]},$$scope:{ctx:I}}}),{c(){n=m("meta"),C=i(),r=m("p"),a=i(),g(M.$$.fragment),t=i(),g(y.$$.fragment),jt=i(),Te=m("p"),Te.innerHTML=On,Jt=i(),Ie=m("p"),Ie.textContent=Kn,Zt=i(),$e=m("p"),$e.innerHTML=eo,kt=i(),te=m("img"),Ut=i(),ve=m("small"),ve.innerHTML=no,Wt=i(),we=m("p"),we.innerHTML=oo,zt=i(),g(Pe.$$.fragment),Bt=i(),Se=m("ul"),Se.innerHTML=so,Vt=i(),g(xe.$$.fragment),Nt=i(),Le=m("p"),Le.textContent=ao,Ft=i(),g(je.$$.fragment),Et=i(),Je=m("ul"),Je.innerHTML=ro,Qt=i(),g(Ze.$$.fragment),qt=i(),P=m("div"),g(ke.$$.fragment),ln=i(),dt=m("p"),dt.innerHTML=io,dn=i(),ct=m("p"),ct.innerHTML=lo,cn=i(),g(ne.$$.fragment),mn=i(),oe=m("div"),g(Ue.$$.fragment),pn=i(),mt=m("p"),mt.innerHTML=co,Ht=i(),g(We.$$.fragment),Xt=i(),j=m("div"),g(ze.$$.fragment),gn=i(),pt=m("p"),pt.innerHTML=mo,fn=i(),gt=m("p"),gt.innerHTML=po,un=i(),g(se.$$.fragment),Rt=i(),g(Be.$$.fragment),Gt=i(),J=m("div"),g(Ve.$$.fragment),hn=i(),ft=m("p"),ft.innerHTML=go,_n=i(),ut=m("p"),ut.innerHTML=fo,bn=i(),g(ae.$$.fragment),Dt=i(),g(Ne.$$.fragment),At=i(),S=m("div"),g(Fe.$$.fragment),Mn=i(),ht=m("p"),ht.textContent=uo,Cn=i(),_t=m("p"),_t.innerHTML=ho,yn=i(),re=m("div"),g(Ee.$$.fragment),Tn=i(),bt=m("p"),bt.innerHTML=_o,In=i(),ie=m("div"),g(Qe.$$.fragment),$n=i(),Mt=m("p"),Mt.innerHTML=bo,Yt=i(),g(qe.$$.fragment),Ot=i(),x=m("div"),g(He.$$.fragment),vn=i(),Ct=m("p"),Ct.innerHTML=Mo,wn=i(),z=m("div"),g(Xe.$$.fragment),Pn=i(),yt=m("p"),yt.innerHTML=Co,Sn=i(),g(le.$$.fragment),xn=i(),g(de.$$.fragment),Ln=i(),B=m("div"),g(Re.$$.fragment),jn=i(),Tt=m("p"),Tt.innerHTML=yo,Jn=i(),g(ce.$$.fragment),Zn=i(),g(me.$$.fragment),kn=i(),V=m("div"),g(Ge.$$.fragment),Un=i(),It=m("p"),It.innerHTML=To,Wn=i(),g(pe.$$.fragment),zn=i(),g(ge.$$.fragment),Kt=i(),g(De.$$.fragment),en=i(),Y=m("div"),g(Ae.$$.fragment),Bn=i(),N=m("div"),g(Ye.$$.fragment),Vn=i(),$t=m("p"),$t.innerHTML=Io,Nn=i(),g(fe.$$.fragment),Fn=i(),g(ue.$$.fragment),tn=i(),g(Oe.$$.fragment),nn=i(),O=m("div"),g(Ke.$$.fragment),En=i(),F=m("div"),g(et.$$.fragment),Qn=i(),vt=m("p"),vt.innerHTML=$o,qn=i(),g(he.$$.fragment),Hn=i(),g(_e.$$.fragment),on=i(),g(tt.$$.fragment),sn=i(),Z=m("div"),g(nt.$$.fragment),Xn=i(),wt=m("p"),wt.textContent=vo,Rn=i(),Pt=m("p"),Pt.innerHTML=wo,Gn=i(),E=m("div"),g(ot.$$.fragment),Dn=i(),St=m("p"),St.innerHTML=Po,An=i(),g(be.$$.fragment),Yn=i(),g(Me.$$.fragment),an=i(),Lt=m("p"),this.h()},l(e){const o=Zo("svelte-u9bgzb",document.head);n=p(o,"META",{name:!0,content:!0}),o.forEach(s),C=l(e),r=p(e,"P",{}),v(r).forEach(s),a=l(e),f(M.$$.fragment,e),t=l(e),f(y.$$.fragment,e),jt=l(e),Te=p(e,"P",{"data-svelte-h":!0}),T(Te)!=="svelte-1m7a0jl"&&(Te.innerHTML=On),Jt=l(e),Ie=p(e,"P",{"data-svelte-h":!0}),T(Ie)!=="svelte-vfdo9a"&&(Ie.textContent=Kn),Zt=l(e),$e=p(e,"P",{"data-svelte-h":!0}),T($e)!=="svelte-2767bu"&&($e.innerHTML=eo),kt=l(e),te=p(e,"IMG",{src:!0,alt:!0,width:!0}),Ut=l(e),ve=p(e,"SMALL",{"data-svelte-h":!0}),T(ve)!=="svelte-r8s5wt"&&(ve.innerHTML=no),Wt=l(e),we=p(e,"P",{"data-svelte-h":!0}),T(we)!=="svelte-14bl8yt"&&(we.innerHTML=oo),zt=l(e),f(Pe.$$.fragment,e),Bt=l(e),Se=p(e,"UL",{"data-svelte-h":!0}),T(Se)!=="svelte-1i9xdxd"&&(Se.innerHTML=so),Vt=l(e),f(xe.$$.fragment,e),Nt=l(e),Le=p(e,"P",{"data-svelte-h":!0}),T(Le)!=="svelte-i5etz8"&&(Le.textContent=ao),Ft=l(e),f(je.$$.fragment,e),Et=l(e),Je=p(e,"UL",{"data-svelte-h":!0}),T(Je)!=="svelte-ksa1e9"&&(Je.innerHTML=ro),Qt=l(e),f(Ze.$$.fragment,e),qt=l(e),P=p(e,"DIV",{class:!0});var k=v(P);f(ke.$$.fragment,k),ln=l(k),dt=p(k,"P",{"data-svelte-h":!0}),T(dt)!=="svelte-n9fv2j"&&(dt.innerHTML=io),dn=l(k),ct=p(k,"P",{"data-svelte-h":!0}),T(ct)!=="svelte-o55m63"&&(ct.innerHTML=lo),cn=l(k),f(ne.$$.fragment,k),mn=l(k),oe=p(k,"DIV",{class:!0});var st=v(oe);f(Ue.$$.fragment,st),pn=l(st),mt=p(st,"P",{"data-svelte-h":!0}),T(mt)!=="svelte-1jbt4n9"&&(mt.innerHTML=co),st.forEach(s),k.forEach(s),Ht=l(e),f(We.$$.fragment,e),Xt=l(e),j=p(e,"DIV",{class:!0});var q=v(j);f(ze.$$.fragment,q),gn=l(q),pt=p(q,"P",{"data-svelte-h":!0}),T(pt)!=="svelte-dj3u6s"&&(pt.innerHTML=mo),fn=l(q),gt=p(q,"P",{"data-svelte-h":!0}),T(gt)!=="svelte-o55m63"&&(gt.innerHTML=po),un=l(q),f(se.$$.fragment,q),q.forEach(s),Rt=l(e),f(Be.$$.fragment,e),Gt=l(e),J=p(e,"DIV",{class:!0});var H=v(J);f(Ve.$$.fragment,H),hn=l(H),ft=p(H,"P",{"data-svelte-h":!0}),T(ft)!=="svelte-dj3u6s"&&(ft.innerHTML=go),_n=l(H),ut=p(H,"P",{"data-svelte-h":!0}),T(ut)!=="svelte-o55m63"&&(ut.innerHTML=fo),bn=l(H),f(ae.$$.fragment,H),H.forEach(s),Dt=l(e),f(Ne.$$.fragment,e),At=l(e),S=p(e,"DIV",{class:!0});var U=v(S);f(Fe.$$.fragment,U),Mn=l(U),ht=p(U,"P",{"data-svelte-h":!0}),T(ht)!=="svelte-9hszhx"&&(ht.textContent=uo),Cn=l(U),_t=p(U,"P",{"data-svelte-h":!0}),T(_t)!=="svelte-hb9pu0"&&(_t.innerHTML=ho),yn=l(U),re=p(U,"DIV",{class:!0});var at=v(re);f(Ee.$$.fragment,at),Tn=l(at),bt=p(at,"P",{"data-svelte-h":!0}),T(bt)!=="svelte-1bns3ah"&&(bt.innerHTML=_o),at.forEach(s),In=l(U),ie=p(U,"DIV",{class:!0});var rt=v(ie);f(Qe.$$.fragment,rt),$n=l(rt),Mt=p(rt,"P",{"data-svelte-h":!0}),T(Mt)!=="svelte-853ynk"&&(Mt.innerHTML=bo),rt.forEach(s),U.forEach(s),Yt=l(e),f(qe.$$.fragment,e),Ot=l(e),x=p(e,"DIV",{class:!0});var W=v(x);f(He.$$.fragment,W),vn=l(W),Ct=p(W,"P",{"data-svelte-h":!0}),T(Ct)!=="svelte-1gjh92c"&&(Ct.innerHTML=Mo),wn=l(W),z=p(W,"DIV",{class:!0});var X=v(z);f(Xe.$$.fragment,X),Pn=l(X),yt=p(X,"P",{"data-svelte-h":!0}),T(yt)!=="svelte-t3a68g"&&(yt.innerHTML=Co),Sn=l(X),f(le.$$.fragment,X),xn=l(X),f(de.$$.fragment,X),X.forEach(s),Ln=l(W),B=p(W,"DIV",{class:!0});var R=v(B);f(Re.$$.fragment,R),jn=l(R),Tt=p(R,"P",{"data-svelte-h":!0}),T(Tt)!=="svelte-t3a68g"&&(Tt.innerHTML=yo),Jn=l(R),f(ce.$$.fragment,R),Zn=l(R),f(me.$$.fragment,R),R.forEach(s),kn=l(W),V=p(W,"DIV",{class:!0});var G=v(V);f(Ge.$$.fragment,G),Un=l(G),It=p(G,"P",{"data-svelte-h":!0}),T(It)!=="svelte-t3a68g"&&(It.innerHTML=To),Wn=l(G),f(pe.$$.fragment,G),zn=l(G),f(ge.$$.fragment,G),G.forEach(s),W.forEach(s),Kt=l(e),f(De.$$.fragment,e),en=l(e),Y=p(e,"DIV",{class:!0});var it=v(Y);f(Ae.$$.fragment,it),Bn=l(it),N=p(it,"DIV",{class:!0});var D=v(N);f(Ye.$$.fragment,D),Vn=l(D),$t=p(D,"P",{"data-svelte-h":!0}),T($t)!=="svelte-1cmt72u"&&($t.innerHTML=Io),Nn=l(D),f(fe.$$.fragment,D),Fn=l(D),f(ue.$$.fragment,D),D.forEach(s),it.forEach(s),tn=l(e),f(Oe.$$.fragment,e),nn=l(e),O=p(e,"DIV",{class:!0});var lt=v(O);f(Ke.$$.fragment,lt),En=l(lt),F=p(lt,"DIV",{class:!0});var A=v(F);f(et.$$.fragment,A),Qn=l(A),vt=p(A,"P",{"data-svelte-h":!0}),T(vt)!=="svelte-1ipxvuo"&&(vt.innerHTML=$o),qn=l(A),f(he.$$.fragment,A),Hn=l(A),f(_e.$$.fragment,A),A.forEach(s),lt.forEach(s),on=l(e),f(tt.$$.fragment,e),sn=l(e),Z=p(e,"DIV",{class:!0});var Ce=v(Z);f(nt.$$.fragment,Ce),Xn=l(Ce),wt=p(Ce,"P",{"data-svelte-h":!0}),T(wt)!=="svelte-qndran"&&(wt.textContent=vo),Rn=l(Ce),Pt=p(Ce,"P",{"data-svelte-h":!0}),T(Pt)!=="svelte-1gjh92c"&&(Pt.innerHTML=wo),Gn=l(Ce),E=p(Ce,"DIV",{class:!0});var ye=v(E);f(ot.$$.fragment,ye),Dn=l(ye),St=p(ye,"P",{"data-svelte-h":!0}),T(St)!=="svelte-3xvx5w"&&(St.innerHTML=Po),An=l(ye),f(be.$$.fragment,ye),Yn=l(ye),f(Me.$$.fragment,ye),ye.forEach(s),Ce.forEach(s),an=l(e),Lt=p(e,"P",{}),v(Lt).forEach(s),this.h()},h(){$(n,"name","hf:doc:metadata"),$(n,"content",Yo),xo(te.src,to="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png")||$(te,"src",to),$(te,"alt","drawing"),$(te,"width","600"),$(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){c(document.head,n),d(e,C,o),d(e,r,o),d(e,a,o),u(M,e,o),d(e,t,o),u(y,e,o),d(e,jt,o),d(e,Te,o),d(e,Jt,o),d(e,Ie,o),d(e,Zt,o),d(e,$e,o),d(e,kt,o),d(e,te,o),d(e,Ut,o),d(e,ve,o),d(e,Wt,o),d(e,we,o),d(e,zt,o),u(Pe,e,o),d(e,Bt,o),d(e,Se,o),d(e,Vt,o),u(xe,e,o),d(e,Nt,o),d(e,Le,o),d(e,Ft,o),u(je,e,o),d(e,Et,o),d(e,Je,o),d(e,Qt,o),u(Ze,e,o),d(e,qt,o),d(e,P,o),u(ke,P,null),c(P,ln),c(P,dt),c(P,dn),c(P,ct),c(P,cn),u(ne,P,null),c(P,mn),c(P,oe),u(Ue,oe,null),c(oe,pn),c(oe,mt),d(e,Ht,o),u(We,e,o),d(e,Xt,o),d(e,j,o),u(ze,j,null),c(j,gn),c(j,pt),c(j,fn),c(j,gt),c(j,un),u(se,j,null),d(e,Rt,o),u(Be,e,o),d(e,Gt,o),d(e,J,o),u(Ve,J,null),c(J,hn),c(J,ft),c(J,_n),c(J,ut),c(J,bn),u(ae,J,null),d(e,Dt,o),u(Ne,e,o),d(e,At,o),d(e,S,o),u(Fe,S,null),c(S,Mn),c(S,ht),c(S,Cn),c(S,_t),c(S,yn),c(S,re),u(Ee,re,null),c(re,Tn),c(re,bt),c(S,In),c(S,ie),u(Qe,ie,null),c(ie,$n),c(ie,Mt),d(e,Yt,o),u(qe,e,o),d(e,Ot,o),d(e,x,o),u(He,x,null),c(x,vn),c(x,Ct),c(x,wn),c(x,z),u(Xe,z,null),c(z,Pn),c(z,yt),c(z,Sn),u(le,z,null),c(z,xn),u(de,z,null),c(x,Ln),c(x,B),u(Re,B,null),c(B,jn),c(B,Tt),c(B,Jn),u(ce,B,null),c(B,Zn),u(me,B,null),c(x,kn),c(x,V),u(Ge,V,null),c(V,Un),c(V,It),c(V,Wn),u(pe,V,null),c(V,zn),u(ge,V,null),d(e,Kt,o),u(De,e,o),d(e,en,o),d(e,Y,o),u(Ae,Y,null),c(Y,Bn),c(Y,N),u(Ye,N,null),c(N,Vn),c(N,$t),c(N,Nn),u(fe,N,null),c(N,Fn),u(ue,N,null),d(e,tn,o),u(Oe,e,o),d(e,nn,o),d(e,O,o),u(Ke,O,null),c(O,En),c(O,F),u(et,F,null),c(F,Qn),c(F,vt),c(F,qn),u(he,F,null),c(F,Hn),u(_e,F,null),d(e,on,o),u(tt,e,o),d(e,sn,o),d(e,Z,o),u(nt,Z,null),c(Z,Xn),c(Z,wt),c(Z,Rn),c(Z,Pt),c(Z,Gn),c(Z,E),u(ot,E,null),c(E,Dn),c(E,St),c(E,An),u(be,E,null),c(E,Yn),u(Me,E,null),d(e,an,o),d(e,Lt,o),rn=!0},p(e,[o]){const k={};o&2&&(k.$$scope={dirty:o,ctx:e}),ne.$set(k);const st={};o&2&&(st.$$scope={dirty:o,ctx:e}),se.$set(st);const q={};o&2&&(q.$$scope={dirty:o,ctx:e}),ae.$set(q);const H={};o&2&&(H.$$scope={dirty:o,ctx:e}),le.$set(H);const U={};o&2&&(U.$$scope={dirty:o,ctx:e}),de.$set(U);const at={};o&2&&(at.$$scope={dirty:o,ctx:e}),ce.$set(at);const rt={};o&2&&(rt.$$scope={dirty:o,ctx:e}),me.$set(rt);const W={};o&2&&(W.$$scope={dirty:o,ctx:e}),pe.$set(W);const X={};o&2&&(X.$$scope={dirty:o,ctx:e}),ge.$set(X);const R={};o&2&&(R.$$scope={dirty:o,ctx:e}),fe.$set(R);const G={};o&2&&(G.$$scope={dirty:o,ctx:e}),ue.$set(G);const it={};o&2&&(it.$$scope={dirty:o,ctx:e}),he.$set(it);const D={};o&2&&(D.$$scope={dirty:o,ctx:e}),_e.$set(D);const lt={};o&2&&(lt.$$scope={dirty:o,ctx:e}),be.$set(lt);const A={};o&2&&(A.$$scope={dirty:o,ctx:e}),Me.$set(A)},i(e){rn||(h(M.$$.fragment,e),h(y.$$.fragment,e),h(Pe.$$.fragment,e),h(xe.$$.fragment,e),h(je.$$.fragment,e),h(Ze.$$.fragment,e),h(ke.$$.fragment,e),h(ne.$$.fragment,e),h(Ue.$$.fragment,e),h(We.$$.fragment,e),h(ze.$$.fragment,e),h(se.$$.fragment,e),h(Be.$$.fragment,e),h(Ve.$$.fragment,e),h(ae.$$.fragment,e),h(Ne.$$.fragment,e),h(Fe.$$.fragment,e),h(Ee.$$.fragment,e),h(Qe.$$.fragment,e),h(qe.$$.fragment,e),h(He.$$.fragment,e),h(Xe.$$.fragment,e),h(le.$$.fragment,e),h(de.$$.fragment,e),h(Re.$$.fragment,e),h(ce.$$.fragment,e),h(me.$$.fragment,e),h(Ge.$$.fragment,e),h(pe.$$.fragment,e),h(ge.$$.fragment,e),h(De.$$.fragment,e),h(Ae.$$.fragment,e),h(Ye.$$.fragment,e),h(fe.$$.fragment,e),h(ue.$$.fragment,e),h(Oe.$$.fragment,e),h(Ke.$$.fragment,e),h(et.$$.fragment,e),h(he.$$.fragment,e),h(_e.$$.fragment,e),h(tt.$$.fragment,e),h(nt.$$.fragment,e),h(ot.$$.fragment,e),h(be.$$.fragment,e),h(Me.$$.fragment,e),rn=!0)},o(e){_(M.$$.fragment,e),_(y.$$.fragment,e),_(Pe.$$.fragment,e),_(xe.$$.fragment,e),_(je.$$.fragment,e),_(Ze.$$.fragment,e),_(ke.$$.fragment,e),_(ne.$$.fragment,e),_(Ue.$$.fragment,e),_(We.$$.fragment,e),_(ze.$$.fragment,e),_(se.$$.fragment,e),_(Be.$$.fragment,e),_(Ve.$$.fragment,e),_(ae.$$.fragment,e),_(Ne.$$.fragment,e),_(Fe.$$.fragment,e),_(Ee.$$.fragment,e),_(Qe.$$.fragment,e),_(qe.$$.fragment,e),_(He.$$.fragment,e),_(Xe.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(Re.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(Ge.$$.fragment,e),_(pe.$$.fragment,e),_(ge.$$.fragment,e),_(De.$$.fragment,e),_(Ae.$$.fragment,e),_(Ye.$$.fragment,e),_(fe.$$.fragment,e),_(ue.$$.fragment,e),_(Oe.$$.fragment,e),_(Ke.$$.fragment,e),_(et.$$.fragment,e),_(he.$$.fragment,e),_(_e.$$.fragment,e),_(tt.$$.fragment,e),_(nt.$$.fragment,e),_(ot.$$.fragment,e),_(be.$$.fragment,e),_(Me.$$.fragment,e),rn=!1},d(e){e&&(s(C),s(r),s(a),s(t),s(jt),s(Te),s(Jt),s(Ie),s(Zt),s($e),s(kt),s(te),s(Ut),s(ve),s(Wt),s(we),s(zt),s(Bt),s(Se),s(Vt),s(Nt),s(Le),s(Ft),s(Et),s(Je),s(Qt),s(qt),s(P),s(Ht),s(Xt),s(j),s(Rt),s(Gt),s(J),s(Dt),s(At),s(S),s(Yt),s(Ot),s(x),s(Kt),s(en),s(Y),s(tn),s(nn),s(O),s(on),s(sn),s(Z),s(an),s(Lt)),s(n),b(M,e),b(y,e),b(Pe,e),b(xe,e),b(je,e),b(Ze,e),b(ke),b(ne),b(Ue),b(We,e),b(ze),b(se),b(Be,e),b(Ve),b(ae),b(Ne,e),b(Fe),b(Ee),b(Qe),b(qe,e),b(He),b(Xe),b(le),b(de),b(Re),b(ce),b(me),b(Ge),b(pe),b(ge),b(De,e),b(Ae),b(Ye),b(fe),b(ue),b(Oe,e),b(Ke),b(et),b(he),b(_e),b(tt,e),b(nt),b(ot),b(be),b(Me)}}}const Yo='{"title":"CLIPSeg","local":"clipseg","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"CLIPSegConfig","local":"transformers.CLIPSegConfig","sections":[],"depth":2},{"title":"CLIPSegTextConfig","local":"transformers.CLIPSegTextConfig","sections":[],"depth":2},{"title":"CLIPSegVisionConfig","local":"transformers.CLIPSegVisionConfig","sections":[],"depth":2},{"title":"CLIPSegProcessor","local":"transformers.CLIPSegProcessor","sections":[],"depth":2},{"title":"CLIPSegModel","local":"transformers.CLIPSegModel","sections":[],"depth":2},{"title":"CLIPSegTextModel","local":"transformers.CLIPSegTextModel","sections":[],"depth":2},{"title":"CLIPSegVisionModel","local":"transformers.CLIPSegVisionModel","sections":[],"depth":2},{"title":"CLIPSegForImageSegmentation","local":"transformers.CLIPSegForImageSegmentation","sections":[],"depth":2}],"depth":1}';function Oo(I){return Lo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class is extends jo{constructor(n){super(),Jo(this,n,Oo,Ao,So,{})}}export{is as component};
