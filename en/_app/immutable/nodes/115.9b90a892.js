import{s as vt,o as yt,n as be}from"../chunks/scheduler.9bc65507.js";import{S as Mt,i as wt,g as c,s as r,r as h,A as Tt,h as m,f as o,c as i,j as re,u as f,x as y,k as ie,y as p,a,v as g,d as _,t as b,w as v}from"../chunks/index.707bf1b6.js";import{T as bt}from"../chunks/Tip.c2ecdbf4.js";import{D as _e}from"../chunks/Docstring.17db21ae.js";import{C as Ge}from"../chunks/CodeBlock.54a9f38d.js";import{E as tt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as $t}from"../chunks/PipelineTag.44585822.js";import{H as le}from"../chunks/Heading.342b1fa6.js";function Ct(C){let n,M="Example:",l,d,u;return d=new Ge({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERpbm92MkNvbmZpZyUyQyUyMERpbm92Mk1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMERpbm92MiUyMGRpbm92Mi1iYXNlLXBhdGNoMTYtMjI0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMERpbm92MkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBkaW5vdjItYmFzZS1wYXRjaDE2LTIyNCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwRGlub3YyTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Dinov2Config, Dinov2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Dinov2 dinov2-base-patch16-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Dinov2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the dinov2-base-patch16-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Dinov2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=c("p"),n.textContent=M,l=r(),h(d.$$.fragment)},l(s){n=m(s,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=M),l=i(s),f(d.$$.fragment,s)},m(s,w){a(s,n,w),a(s,l,w),g(d,s,w),u=!0},p:be,i(s){u||(_(d.$$.fragment,s),u=!0)},o(s){b(d.$$.fragment,s),u=!1},d(s){s&&(o(n),o(l)),v(d,s)}}}function Jt(C){let n,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=c("p"),n.innerHTML=M},l(l){n=m(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=M)},m(l,d){a(l,n,d)},p:be,d(l){l&&o(n)}}}function jt(C){let n,M="Example:",l,d,u;return d=new Ge({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMERpbm92Mk1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmh1Z2dpbmdmYWNlJTJGY2F0cy1pbWFnZSUyMiklMEFpbWFnZSUyMCUzRCUyMGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTVCJTIyaW1hZ2UlMjIlNUQlNUIwJTVEJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmRpbm92Mi1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwRGlub3YyTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZGlub3YyLWJhc2UlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, Dinov2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/dinov2-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Dinov2Model.from_pretrained(<span class="hljs-string">&quot;facebook/dinov2-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">257</span>, <span class="hljs-number">768</span>]`,wrap:!1}}),{c(){n=c("p"),n.textContent=M,l=r(),h(d.$$.fragment)},l(s){n=m(s,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=M),l=i(s),f(d.$$.fragment,s)},m(s,w){a(s,n,w),a(s,l,w),g(d,s,w),u=!0},p:be,i(s){u||(_(d.$$.fragment,s),u=!0)},o(s){b(d.$$.fragment,s),u=!1},d(s){s&&(o(n),o(l)),v(d,s)}}}function It(C){let n,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=c("p"),n.innerHTML=M},l(l){n=m(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=M)},m(l,d){a(l,n,d)},p:be,d(l){l&&o(n)}}}function kt(C){let n,M="Example:",l,d,u;return d=new Ge({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMERpbm92MkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZGlub3YyLXNtYWxsLWltYWdlbmV0MWstMS1sYXllciUyMiklMEFtb2RlbCUyMCUzRCUyMERpbm92MkZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZGlub3YyLXNtYWxsLWltYWdlbmV0MWstMS1sYXllciUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, Dinov2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/dinov2-small-imagenet1k-1-layer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Dinov2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/dinov2-small-imagenet1k-1-layer&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){n=c("p"),n.textContent=M,l=r(),h(d.$$.fragment)},l(s){n=m(s,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=M),l=i(s),f(d.$$.fragment,s)},m(s,w){a(s,n,w),a(s,l,w),g(d,s,w),u=!0},p:be,i(s){u||(_(d.$$.fragment,s),u=!0)},o(s){b(d.$$.fragment,s),u=!1},d(s){s&&(o(n),o(l)),v(d,s)}}}function xt(C){let n,M,l,d,u,s,w,ve,R,ot=`The DINOv2 model was proposed in <a href="https://arxiv.org/abs/2304.07193" rel="nofollow">DINOv2: Learning Robust Visual Features without Supervision</a> by
Maxime Oquab, TimothÃ©e Darcet, ThÃ©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, HervÃ© Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, Piotr Bojanowski.
DINOv2 is an upgrade of <a href="https://arxiv.org/abs/2104.14294" rel="nofollow">DINO</a>, a self-supervised method applied on <a href="vit">Vision Transformers</a>. This method enables all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning.`,ye,G,nt="The abstract from the paper is the following:",Me,V,st="<em>The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.</em>",we,Y,at=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/facebookresearch/dinov2" rel="nofollow">here</a>.`,Te,E,$e,H,rt="The model can be traced using <code>torch.jit.trace</code> which leverages JIT compilation to optimize the model making it faster to run. Note this still produces some mis-matched elements and the difference between the original model and the traced model is of the order of 1e-4.",Ce,X,Je,P,je,q,it="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with DPT.",Ie,L,lt='<li>Demo notebooks for DINOv2 can be found <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DINOv2" rel="nofollow">here</a>. ðŸŒŽ</li>',ke,S,xe,Q,dt='<li><a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2ForImageClassification">Dinov2ForImageClassification</a> is supported by this <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">example script</a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">notebook</a>.</li> <li>See also: <a href="../tasks/image_classification">Image classification task guide</a></li>',Ue,A,ct="If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",We,O,Ze,T,K,Ve,de,mt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Model">Dinov2Model</a>. It is used to instantiate an
Dinov2 model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Dinov2
<a href="https://huggingface.co/google/dinov2-base-patch16-224" rel="nofollow">google/dinov2-base-patch16-224</a> architecture.`,Ye,ce,pt=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ee,D,De,ee,Fe,I,te,He,me,ut=`The bare DINOv2 Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Xe,J,oe,Pe,pe,ht='The <a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Model">Dinov2Model</a> forward method, overrides the <code>__call__</code> special method.',qe,F,Le,z,ze,ne,Be,$,se,Se,ue,ft=`Dinov2 Model transformer with an image classification head on top (a linear layer on top of the final hidden state
of the [CLS] token) e.g. for ImageNet.`,Qe,he,gt=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Ae,j,ae,Oe,fe,_t='The <a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2ForImageClassification">Dinov2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',Ke,B,et,N,Ne,ge,Re;return u=new le({props:{title:"DINOv2",local:"dinov2",headingTag:"h1"}}),w=new le({props:{title:"Overview",local:"overview",headingTag:"h2"}}),E=new le({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),X=new Ge({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b0ltYWdlUHJvY2Vzc29yJTJDJTIwQXV0b01vZGVsJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAnaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyclMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCdmYWNlYm9vayUyRmRpbm92Mi1iYXNlJyklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJ2ZhY2Vib29rJTJGZGlub3YyLWJhc2UnKSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMlNUIwJTVEJTBBJTBBJTIzJTIwV2UlMjBoYXZlJTIwdG8lMjBmb3JjZSUyMHJldHVybl9kaWN0JTNERmFsc2UlMjBmb3IlMjB0cmFjaW5nJTBBbW9kZWwuY29uZmlnLnJldHVybl9kaWN0JTIwJTNEJTIwRmFsc2UlMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwdHJhY2VkX21vZGVsJTIwJTNEJTIwdG9yY2guaml0LnRyYWNlKG1vZGVsJTJDJTIwJTVCaW5wdXRzLnBpeGVsX3ZhbHVlcyU1RCklMEElMjAlMjAlMjAlMjB0cmFjZWRfb3V0cHV0cyUyMCUzRCUyMHRyYWNlZF9tb2RlbChpbnB1dHMucGl4ZWxfdmFsdWVzKSUwQSUwQXByaW50KChsYXN0X2hpZGRlbl9zdGF0ZXMlMjAtJTIwdHJhY2VkX291dHB1dHMlNUIwJTVEKS5hYnMoKS5tYXgoKSk=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModel
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

url = <span class="hljs-string">&#x27;http://images.cocodataset.org/val2017/000000039769.jpg&#x27;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&#x27;facebook/dinov2-base&#x27;</span>)
model = AutoModel.from_pretrained(<span class="hljs-string">&#x27;facebook/dinov2-base&#x27;</span>)

inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
outputs = model(**inputs)
last_hidden_states = outputs[<span class="hljs-number">0</span>]

<span class="hljs-comment"># We have to force return_dict=False for tracing</span>
model.config.return_dict = <span class="hljs-literal">False</span>

<span class="hljs-keyword">with</span> torch.no_grad():
    traced_model = torch.jit.trace(model, [inputs.pixel_values])
    traced_outputs = traced_model(inputs.pixel_values)

<span class="hljs-built_in">print</span>((last_hidden_states - traced_outputs[<span class="hljs-number">0</span>]).<span class="hljs-built_in">abs</span>().<span class="hljs-built_in">max</span>())`,wrap:!1}}),P=new le({props:{title:"Resources",local:"resources",headingTag:"h2"}}),S=new $t({props:{pipeline:"image-classification"}}),O=new le({props:{title:"Dinov2Config",local:"transformers.Dinov2Config",headingTag:"h2"}}),K=new _e({props:{name:"class transformers.Dinov2Config",anchor:"transformers.Dinov2Config",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"mlp_ratio",val:" = 4"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"layerscale_value",val:" = 1.0"},{name:"drop_path_rate",val:" = 0.0"},{name:"use_swiglu_ffn",val:" = False"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"apply_layernorm",val:" = True"},{name:"reshape_hidden_states",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Dinov2Config.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.Dinov2Config.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.Dinov2Config.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.Dinov2Config.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Ratio of the hidden size of the MLPs relative to the <code>hidden_size</code>.`,name:"mlp_ratio"},{anchor:"transformers.Dinov2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.Dinov2Config.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.Dinov2Config.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.Dinov2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Dinov2Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.Dinov2Config.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.Dinov2Config.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.Dinov2Config.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.Dinov2Config.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.Dinov2Config.layerscale_value",description:`<strong>layerscale_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Initial value to use for layer scale.`,name:"layerscale_value"},{anchor:"transformers.Dinov2Config.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Stochastic depth rate per sample (when applied in the main path of residual layers).`,name:"drop_path_rate"},{anchor:"transformers.Dinov2Config.use_swiglu_ffn",description:`<strong>use_swiglu_ffn</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use the SwiGLU feedforward neural network.`,name:"use_swiglu_ffn"},{anchor:"transformers.Dinov2Config.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.Dinov2Config.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"},{anchor:"transformers.Dinov2Config.apply_layernorm",description:`<strong>apply_layernorm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to apply layer normalization to the feature maps in case the model is used as backbone.`,name:"apply_layernorm"},{anchor:"transformers.Dinov2Config.reshape_hidden_states",description:`<strong>reshape_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to reshape the feature maps to 4D tensors of shape <code>(batch_size, hidden_size, height, width)</code> in
case the model is used as backbone. If <code>False</code>, the feature maps will be 3D tensors of shape <code>(batch_size, seq_len, hidden_size)</code>.`,name:"reshape_hidden_states"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dinov2/configuration_dinov2.py#L35"}}),D=new tt({props:{anchor:"transformers.Dinov2Config.example",$$slots:{default:[Ct]},$$scope:{ctx:C}}}),ee=new le({props:{title:"Dinov2Model",local:"transformers.Dinov2Model",headingTag:"h2"}}),te=new _e({props:{name:"class transformers.Dinov2Model",anchor:"transformers.Dinov2Model",parameters:[{name:"config",val:": Dinov2Config"}],parametersDescription:[{anchor:"transformers.Dinov2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config">Dinov2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dinov2/modeling_dinov2.py#L576"}}),oe=new _e({props:{name:"forward",anchor:"transformers.Dinov2Model.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Dinov2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessor.preprocess">BitImageProcessor.preprocess()</a> for details.`,name:"pixel_values"},{anchor:"transformers.Dinov2Model.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0). Only relevant for
pre-training.`,name:"bool_masked_pos"},{anchor:"transformers.Dinov2Model.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Dinov2Model.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Dinov2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Dinov2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dinov2/modeling_dinov2.py#L604",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config"
>Dinov2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),F=new bt({props:{$$slots:{default:[Jt]},$$scope:{ctx:C}}}),z=new tt({props:{anchor:"transformers.Dinov2Model.forward.example",$$slots:{default:[jt]},$$scope:{ctx:C}}}),ne=new le({props:{title:"Dinov2ForImageClassification",local:"transformers.Dinov2ForImageClassification",headingTag:"h2"}}),se=new _e({props:{name:"class transformers.Dinov2ForImageClassification",anchor:"transformers.Dinov2ForImageClassification",parameters:[{name:"config",val:": Dinov2Config"}],parametersDescription:[{anchor:"transformers.Dinov2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config">Dinov2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dinov2/modeling_dinov2.py#L662"}}),ae=new _e({props:{name:"forward",anchor:"transformers.Dinov2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Dinov2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessor.preprocess">BitImageProcessor.preprocess()</a> for details.`,name:"pixel_values"},{anchor:"transformers.Dinov2ForImageClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Dinov2ForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Dinov2ForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Dinov2ForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Dinov2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dinov2/modeling_dinov2.py#L684",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config"
>Dinov2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),B=new bt({props:{$$slots:{default:[It]},$$scope:{ctx:C}}}),N=new tt({props:{anchor:"transformers.Dinov2ForImageClassification.forward.example",$$slots:{default:[kt]},$$scope:{ctx:C}}}),{c(){n=c("meta"),M=r(),l=c("p"),d=r(),h(u.$$.fragment),s=r(),h(w.$$.fragment),ve=r(),R=c("p"),R.innerHTML=ot,ye=r(),G=c("p"),G.textContent=nt,Me=r(),V=c("p"),V.innerHTML=st,we=r(),Y=c("p"),Y.innerHTML=at,Te=r(),h(E.$$.fragment),$e=r(),H=c("p"),H.innerHTML=rt,Ce=r(),h(X.$$.fragment),Je=r(),h(P.$$.fragment),je=r(),q=c("p"),q.textContent=it,Ie=r(),L=c("ul"),L.innerHTML=lt,ke=r(),h(S.$$.fragment),xe=r(),Q=c("ul"),Q.innerHTML=dt,Ue=r(),A=c("p"),A.textContent=ct,We=r(),h(O.$$.fragment),Ze=r(),T=c("div"),h(K.$$.fragment),Ve=r(),de=c("p"),de.innerHTML=mt,Ye=r(),ce=c("p"),ce.innerHTML=pt,Ee=r(),h(D.$$.fragment),De=r(),h(ee.$$.fragment),Fe=r(),I=c("div"),h(te.$$.fragment),He=r(),me=c("p"),me.innerHTML=ut,Xe=r(),J=c("div"),h(oe.$$.fragment),Pe=r(),pe=c("p"),pe.innerHTML=ht,qe=r(),h(F.$$.fragment),Le=r(),h(z.$$.fragment),ze=r(),h(ne.$$.fragment),Be=r(),$=c("div"),h(se.$$.fragment),Se=r(),ue=c("p"),ue.textContent=ft,Qe=r(),he=c("p"),he.innerHTML=gt,Ae=r(),j=c("div"),h(ae.$$.fragment),Oe=r(),fe=c("p"),fe.innerHTML=_t,Ke=r(),h(B.$$.fragment),et=r(),h(N.$$.fragment),Ne=r(),ge=c("p"),this.h()},l(e){const t=Tt("svelte-u9bgzb",document.head);n=m(t,"META",{name:!0,content:!0}),t.forEach(o),M=i(e),l=m(e,"P",{}),re(l).forEach(o),d=i(e),f(u.$$.fragment,e),s=i(e),f(w.$$.fragment,e),ve=i(e),R=m(e,"P",{"data-svelte-h":!0}),y(R)!=="svelte-189nb70"&&(R.innerHTML=ot),ye=i(e),G=m(e,"P",{"data-svelte-h":!0}),y(G)!=="svelte-vfdo9a"&&(G.textContent=nt),Me=i(e),V=m(e,"P",{"data-svelte-h":!0}),y(V)!=="svelte-1p4dq7"&&(V.innerHTML=st),we=i(e),Y=m(e,"P",{"data-svelte-h":!0}),y(Y)!=="svelte-1exxkia"&&(Y.innerHTML=at),Te=i(e),f(E.$$.fragment,e),$e=i(e),H=m(e,"P",{"data-svelte-h":!0}),y(H)!=="svelte-1bw89ap"&&(H.innerHTML=rt),Ce=i(e),f(X.$$.fragment,e),Je=i(e),f(P.$$.fragment,e),je=i(e),q=m(e,"P",{"data-svelte-h":!0}),y(q)!=="svelte-1b0rfkj"&&(q.textContent=it),Ie=i(e),L=m(e,"UL",{"data-svelte-h":!0}),y(L)!=="svelte-1vxq6gu"&&(L.innerHTML=lt),ke=i(e),f(S.$$.fragment,e),xe=i(e),Q=m(e,"UL",{"data-svelte-h":!0}),y(Q)!=="svelte-1fr57ih"&&(Q.innerHTML=dt),Ue=i(e),A=m(e,"P",{"data-svelte-h":!0}),y(A)!=="svelte-1xesile"&&(A.textContent=ct),We=i(e),f(O.$$.fragment,e),Ze=i(e),T=m(e,"DIV",{class:!0});var k=re(T);f(K.$$.fragment,k),Ve=i(k),de=m(k,"P",{"data-svelte-h":!0}),y(de)!=="svelte-x3ratf"&&(de.innerHTML=mt),Ye=i(k),ce=m(k,"P",{"data-svelte-h":!0}),y(ce)!=="svelte-o55m63"&&(ce.innerHTML=pt),Ee=i(k),f(D.$$.fragment,k),k.forEach(o),De=i(e),f(ee.$$.fragment,e),Fe=i(e),I=m(e,"DIV",{class:!0});var Z=re(I);f(te.$$.fragment,Z),He=i(Z),me=m(Z,"P",{"data-svelte-h":!0}),y(me)!=="svelte-1jmeimp"&&(me.innerHTML=ut),Xe=i(Z),J=m(Z,"DIV",{class:!0});var x=re(J);f(oe.$$.fragment,x),Pe=i(x),pe=m(x,"P",{"data-svelte-h":!0}),y(pe)!=="svelte-g9ij7n"&&(pe.innerHTML=ht),qe=i(x),f(F.$$.fragment,x),Le=i(x),f(z.$$.fragment,x),x.forEach(o),Z.forEach(o),ze=i(e),f(ne.$$.fragment,e),Be=i(e),$=m(e,"DIV",{class:!0});var U=re($);f(se.$$.fragment,U),Se=i(U),ue=m(U,"P",{"data-svelte-h":!0}),y(ue)!=="svelte-1i57dtz"&&(ue.textContent=ft),Qe=i(U),he=m(U,"P",{"data-svelte-h":!0}),y(he)!=="svelte-1gjh92c"&&(he.innerHTML=gt),Ae=i(U),j=m(U,"DIV",{class:!0});var W=re(j);f(ae.$$.fragment,W),Oe=i(W),fe=m(W,"P",{"data-svelte-h":!0}),y(fe)!=="svelte-1olmd1j"&&(fe.innerHTML=_t),Ke=i(W),f(B.$$.fragment,W),et=i(W),f(N.$$.fragment,W),W.forEach(o),U.forEach(o),Ne=i(e),ge=m(e,"P",{}),re(ge).forEach(o),this.h()},h(){ie(n,"name","hf:doc:metadata"),ie(n,"content",Ut),ie(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ie(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ie(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ie(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),ie($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){p(document.head,n),a(e,M,t),a(e,l,t),a(e,d,t),g(u,e,t),a(e,s,t),g(w,e,t),a(e,ve,t),a(e,R,t),a(e,ye,t),a(e,G,t),a(e,Me,t),a(e,V,t),a(e,we,t),a(e,Y,t),a(e,Te,t),g(E,e,t),a(e,$e,t),a(e,H,t),a(e,Ce,t),g(X,e,t),a(e,Je,t),g(P,e,t),a(e,je,t),a(e,q,t),a(e,Ie,t),a(e,L,t),a(e,ke,t),g(S,e,t),a(e,xe,t),a(e,Q,t),a(e,Ue,t),a(e,A,t),a(e,We,t),g(O,e,t),a(e,Ze,t),a(e,T,t),g(K,T,null),p(T,Ve),p(T,de),p(T,Ye),p(T,ce),p(T,Ee),g(D,T,null),a(e,De,t),g(ee,e,t),a(e,Fe,t),a(e,I,t),g(te,I,null),p(I,He),p(I,me),p(I,Xe),p(I,J),g(oe,J,null),p(J,Pe),p(J,pe),p(J,qe),g(F,J,null),p(J,Le),g(z,J,null),a(e,ze,t),g(ne,e,t),a(e,Be,t),a(e,$,t),g(se,$,null),p($,Se),p($,ue),p($,Qe),p($,he),p($,Ae),p($,j),g(ae,j,null),p(j,Oe),p(j,fe),p(j,Ke),g(B,j,null),p(j,et),g(N,j,null),a(e,Ne,t),a(e,ge,t),Re=!0},p(e,[t]){const k={};t&2&&(k.$$scope={dirty:t,ctx:e}),D.$set(k);const Z={};t&2&&(Z.$$scope={dirty:t,ctx:e}),F.$set(Z);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),z.$set(x);const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),B.$set(U);const W={};t&2&&(W.$$scope={dirty:t,ctx:e}),N.$set(W)},i(e){Re||(_(u.$$.fragment,e),_(w.$$.fragment,e),_(E.$$.fragment,e),_(X.$$.fragment,e),_(P.$$.fragment,e),_(S.$$.fragment,e),_(O.$$.fragment,e),_(K.$$.fragment,e),_(D.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(oe.$$.fragment,e),_(F.$$.fragment,e),_(z.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(ae.$$.fragment,e),_(B.$$.fragment,e),_(N.$$.fragment,e),Re=!0)},o(e){b(u.$$.fragment,e),b(w.$$.fragment,e),b(E.$$.fragment,e),b(X.$$.fragment,e),b(P.$$.fragment,e),b(S.$$.fragment,e),b(O.$$.fragment,e),b(K.$$.fragment,e),b(D.$$.fragment,e),b(ee.$$.fragment,e),b(te.$$.fragment,e),b(oe.$$.fragment,e),b(F.$$.fragment,e),b(z.$$.fragment,e),b(ne.$$.fragment,e),b(se.$$.fragment,e),b(ae.$$.fragment,e),b(B.$$.fragment,e),b(N.$$.fragment,e),Re=!1},d(e){e&&(o(M),o(l),o(d),o(s),o(ve),o(R),o(ye),o(G),o(Me),o(V),o(we),o(Y),o(Te),o($e),o(H),o(Ce),o(Je),o(je),o(q),o(Ie),o(L),o(ke),o(xe),o(Q),o(Ue),o(A),o(We),o(Ze),o(T),o(De),o(Fe),o(I),o(ze),o(Be),o($),o(Ne),o(ge)),o(n),v(u,e),v(w,e),v(E,e),v(X,e),v(P,e),v(S,e),v(O,e),v(K),v(D),v(ee,e),v(te),v(oe),v(F),v(z),v(ne,e),v(se),v(ae),v(B),v(N)}}}const Ut='{"title":"DINOv2","local":"dinov2","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"Dinov2Config","local":"transformers.Dinov2Config","sections":[],"depth":2},{"title":"Dinov2Model","local":"transformers.Dinov2Model","sections":[],"depth":2},{"title":"Dinov2ForImageClassification","local":"transformers.Dinov2ForImageClassification","sections":[],"depth":2}],"depth":1}';function Wt(C){return yt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Vt extends Mt{constructor(n){super(),wt(this,n,Wt,xt,vt,{})}}export{Vt as component};
