import{s as Ze,o as Ne,n as Se}from"../chunks/scheduler.9bc65507.js";import{S as We,i as Xe,g as a,s,r as h,A as Ae,h as i,f as n,c as r,j as P,u as f,x as m,k as J,y as o,a as d,v as g,d as u,t as _,w as k}from"../chunks/index.707bf1b6.js";import{T as Ye}from"../chunks/Tip.c2ecdbf4.js";import{D as ee}from"../chunks/Docstring.17db21ae.js";import{C as Oe}from"../chunks/CodeBlock.54a9f38d.js";import{H as be}from"../chunks/Heading.342b1fa6.js";function Ke(ne){let p,M=`PhoBERT implementation is the same as BERT, except for tokenization. Refer to <a href="bert">EART documentation</a> for information on
configuration classes and their parameters. PhoBERT-specific tokenizer is documented below.`;return{c(){p=a("p"),p.innerHTML=M},l(b){p=i(b,"P",{"data-svelte-h":!0}),m(p)!=="svelte-v9qnng"&&(p.innerHTML=M)},m(b,H){d(b,p,H)},p:Se,d(b){b&&n(p)}}}function et(ne){let p,M,b,H,q,oe,U,se,z,Ve='The PhoBERT model was proposed in <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.92.pdf" rel="nofollow">PhoBERT: Pre-trained language models for Vietnamese</a> by Dat Quoc Nguyen, Anh Tuan Nguyen.',re,C,Ee="The abstract from the paper is the following:",ae,V,Ie=`<em>We present PhoBERT with two versions, PhoBERT-base and PhoBERT-large, the first public large-scale monolingual
language models pre-trained for Vietnamese. Experimental results show that PhoBERT consistently outperforms the recent
best pre-trained multilingual model XLM-R (Conneau et al., 2020) and improves the state-of-the-art in multiple
Vietnamese-specific NLP tasks including Part-of-speech tagging, Dependency parsing, Named-entity recognition and
Natural language inference.</em>`,ie,E,Be='This model was contributed by <a href="https://huggingface.co/dqnguyen" rel="nofollow">dqnguyen</a>. The original code can be found <a href="https://github.com/VinAIResearch/PhoBERT" rel="nofollow">here</a>.',le,I,de,B,ce,v,pe,L,me,l,R,Te,Z,Le="Construct a PhoBERT tokenizer. Based on Byte-Pair-Encoding.",ve,N,Re=`This tokenizer inherits from <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,we,w,j,ye,S,je="Loads a pre-existing dictionary from a text file and adds its symbols to this instance.",$e,T,D,xe,W,De=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A PhoBERT sequence has the following format:`,Pe,X,Qe="<li>single sequence: <code>&lt;s&gt; X &lt;/s&gt;</code></li> <li>pair of sequences: <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code></li>",Je,y,Q,Me,A,Ge="Converts a sequence of tokens (string) in a single string.",qe,$,G,Ue,Y,Fe=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. PhoBERT does not
make use of token type ids, therefore a list of zeros is returned.`,ze,x,F,Ce,O,He=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,he,te,fe;return q=new be({props:{title:"PhoBERT",local:"phobert",headingTag:"h1"}}),U=new be({props:{title:"Overview",local:"overview",headingTag:"h2"}}),I=new be({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),B=new Oe({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXBob2JlcnQlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnZpbmFpJTJGcGhvYmVydC1iYXNlJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMnZpbmFpJTJGcGhvYmVydC1iYXNlJTIyKSUwQSUwQSUyMyUyMElOUFVUJTIwVEVYVCUyME1VU1QlMjBCRSUyMEFMUkVBRFklMjBXT1JELVNFR01FTlRFRCElMEFsaW5lJTIwJTNEJTIwJTIyVCVDMyVCNGklMjBsJUMzJUEwJTIwc2luaF92aSVDMyVBQW4lMjB0ciVDNiVCMCVFMSVCQiU5RG5nJTIwJUM0JTkxJUUxJUJBJUExaV9oJUUxJUJCJThEYyUyMEMlQzMlQjRuZ19uZ2glRTElQkIlODclMjAuJTIyJTBBJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QnRva2VuaXplci5lbmNvZGUobGluZSklNUQpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGZlYXR1cmVzJTIwJTNEJTIwcGhvYmVydChpbnB1dF9pZHMpJTIwJTIwJTIzJTIwTW9kZWxzJTIwb3V0cHV0cyUyMGFyZSUyMG5vdyUyMHR1cGxlcyUwQSUwQSUyMyUyMFdpdGglMjBUZW5zb3JGbG93JTIwMi4wJTJCJTNBJTBBJTIzJTIwZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsJTBBJTIzJTIwcGhvYmVydCUyMCUzRCUyMFRGQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJ2aW5haSUyRnBob2JlcnQtYmFzZSUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>phobert = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/phobert-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># INPUT TEXT MUST BE ALREADY WORD-SEGMENTED!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;Tôi là sinh_viên trường đại_học Công_nghệ .&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(line)])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    features = phobert(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With TensorFlow 2.0+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># from transformers import TFAutoModel</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># phobert = TFAutoModel.from_pretrained(&quot;vinai/phobert-base&quot;)</span>`,wrap:!1}}),v=new Ye({props:{$$slots:{default:[Ke]},$$scope:{ctx:ne}}}),L=new be({props:{title:"PhobertTokenizer",local:"transformers.PhobertTokenizer",headingTag:"h2"}}),R=new ee({props:{name:"class transformers.PhobertTokenizer",anchor:"transformers.PhobertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.PhobertTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.PhobertTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>st</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.PhobertTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.PhobertTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.PhobertTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.PhobertTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.PhobertTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.PhobertTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/phobert/tokenization_phobert.py#L68"}}),j=new ee({props:{name:"add_from_file",anchor:"transformers.PhobertTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/phobert/tokenization_phobert.py#L346"}}),D=new ee({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/phobert/tokenization_phobert.py#L165",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),Q=new ee({props:{name:"convert_tokens_to_string",anchor:"transformers.PhobertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/phobert/tokenization_phobert.py#L312"}}),G=new ee({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/phobert/tokenization_phobert.py#L219",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),F=new ee({props:{name:"get_special_tokens_mask",anchor:"transformers.PhobertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.PhobertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/phobert/tokenization_phobert.py#L191",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),{c(){p=a("meta"),M=s(),b=a("p"),H=s(),h(q.$$.fragment),oe=s(),h(U.$$.fragment),se=s(),z=a("p"),z.innerHTML=Ve,re=s(),C=a("p"),C.textContent=Ee,ae=s(),V=a("p"),V.innerHTML=Ie,ie=s(),E=a("p"),E.innerHTML=Be,le=s(),h(I.$$.fragment),de=s(),h(B.$$.fragment),ce=s(),h(v.$$.fragment),pe=s(),h(L.$$.fragment),me=s(),l=a("div"),h(R.$$.fragment),Te=s(),Z=a("p"),Z.textContent=Le,ve=s(),N=a("p"),N.innerHTML=Re,we=s(),w=a("div"),h(j.$$.fragment),ye=s(),S=a("p"),S.textContent=je,$e=s(),T=a("div"),h(D.$$.fragment),xe=s(),W=a("p"),W.textContent=De,Pe=s(),X=a("ul"),X.innerHTML=Qe,Je=s(),y=a("div"),h(Q.$$.fragment),Me=s(),A=a("p"),A.textContent=Ge,qe=s(),$=a("div"),h(G.$$.fragment),Ue=s(),Y=a("p"),Y.textContent=Fe,ze=s(),x=a("div"),h(F.$$.fragment),Ce=s(),O=a("p"),O.innerHTML=He,he=s(),te=a("p"),this.h()},l(e){const t=Ae("svelte-u9bgzb",document.head);p=i(t,"META",{name:!0,content:!0}),t.forEach(n),M=r(e),b=i(e,"P",{}),P(b).forEach(n),H=r(e),f(q.$$.fragment,e),oe=r(e),f(U.$$.fragment,e),se=r(e),z=i(e,"P",{"data-svelte-h":!0}),m(z)!=="svelte-1ao5fqi"&&(z.innerHTML=Ve),re=r(e),C=i(e,"P",{"data-svelte-h":!0}),m(C)!=="svelte-vfdo9a"&&(C.textContent=Ee),ae=r(e),V=i(e,"P",{"data-svelte-h":!0}),m(V)!=="svelte-pnlemp"&&(V.innerHTML=Ie),ie=r(e),E=i(e,"P",{"data-svelte-h":!0}),m(E)!=="svelte-cbf0gj"&&(E.innerHTML=Be),le=r(e),f(I.$$.fragment,e),de=r(e),f(B.$$.fragment,e),ce=r(e),f(v.$$.fragment,e),pe=r(e),f(L.$$.fragment,e),me=r(e),l=i(e,"DIV",{class:!0});var c=P(l);f(R.$$.fragment,c),Te=r(c),Z=i(c,"P",{"data-svelte-h":!0}),m(Z)!=="svelte-f1kmpq"&&(Z.textContent=Le),ve=r(c),N=i(c,"P",{"data-svelte-h":!0}),m(N)!=="svelte-1c3t5ty"&&(N.innerHTML=Re),we=r(c),w=i(c,"DIV",{class:!0});var ge=P(w);f(j.$$.fragment,ge),ye=r(ge),S=i(ge,"P",{"data-svelte-h":!0}),m(S)!=="svelte-ooaeix"&&(S.textContent=je),ge.forEach(n),$e=r(c),T=i(c,"DIV",{class:!0});var K=P(T);f(D.$$.fragment,K),xe=r(K),W=i(K,"P",{"data-svelte-h":!0}),m(W)!=="svelte-dhtv7"&&(W.textContent=De),Pe=r(K),X=i(K,"UL",{"data-svelte-h":!0}),m(X)!=="svelte-rq8uot"&&(X.innerHTML=Qe),K.forEach(n),Je=r(c),y=i(c,"DIV",{class:!0});var ue=P(y);f(Q.$$.fragment,ue),Me=r(ue),A=i(ue,"P",{"data-svelte-h":!0}),m(A)!=="svelte-b3k2yi"&&(A.textContent=Ge),ue.forEach(n),qe=r(c),$=i(c,"DIV",{class:!0});var _e=P($);f(G.$$.fragment,_e),Ue=r(_e),Y=i(_e,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-qg6q9n"&&(Y.textContent=Fe),_e.forEach(n),ze=r(c),x=i(c,"DIV",{class:!0});var ke=P(x);f(F.$$.fragment,ke),Ce=r(ke),O=i(ke,"P",{"data-svelte-h":!0}),m(O)!=="svelte-1f4f5kp"&&(O.innerHTML=He),ke.forEach(n),c.forEach(n),he=r(e),te=i(e,"P",{}),P(te).forEach(n),this.h()},h(){J(p,"name","hf:doc:metadata"),J(p,"content",tt),J(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(l,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){o(document.head,p),d(e,M,t),d(e,b,t),d(e,H,t),g(q,e,t),d(e,oe,t),g(U,e,t),d(e,se,t),d(e,z,t),d(e,re,t),d(e,C,t),d(e,ae,t),d(e,V,t),d(e,ie,t),d(e,E,t),d(e,le,t),g(I,e,t),d(e,de,t),g(B,e,t),d(e,ce,t),g(v,e,t),d(e,pe,t),g(L,e,t),d(e,me,t),d(e,l,t),g(R,l,null),o(l,Te),o(l,Z),o(l,ve),o(l,N),o(l,we),o(l,w),g(j,w,null),o(w,ye),o(w,S),o(l,$e),o(l,T),g(D,T,null),o(T,xe),o(T,W),o(T,Pe),o(T,X),o(l,Je),o(l,y),g(Q,y,null),o(y,Me),o(y,A),o(l,qe),o(l,$),g(G,$,null),o($,Ue),o($,Y),o(l,ze),o(l,x),g(F,x,null),o(x,Ce),o(x,O),d(e,he,t),d(e,te,t),fe=!0},p(e,[t]){const c={};t&2&&(c.$$scope={dirty:t,ctx:e}),v.$set(c)},i(e){fe||(u(q.$$.fragment,e),u(U.$$.fragment,e),u(I.$$.fragment,e),u(B.$$.fragment,e),u(v.$$.fragment,e),u(L.$$.fragment,e),u(R.$$.fragment,e),u(j.$$.fragment,e),u(D.$$.fragment,e),u(Q.$$.fragment,e),u(G.$$.fragment,e),u(F.$$.fragment,e),fe=!0)},o(e){_(q.$$.fragment,e),_(U.$$.fragment,e),_(I.$$.fragment,e),_(B.$$.fragment,e),_(v.$$.fragment,e),_(L.$$.fragment,e),_(R.$$.fragment,e),_(j.$$.fragment,e),_(D.$$.fragment,e),_(Q.$$.fragment,e),_(G.$$.fragment,e),_(F.$$.fragment,e),fe=!1},d(e){e&&(n(M),n(b),n(H),n(oe),n(se),n(z),n(re),n(C),n(ae),n(V),n(ie),n(E),n(le),n(de),n(ce),n(pe),n(me),n(l),n(he),n(te)),n(p),k(q,e),k(U,e),k(I,e),k(B,e),k(v,e),k(L,e),k(R),k(j),k(D),k(Q),k(G),k(F)}}}const tt='{"title":"PhoBERT","local":"phobert","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"PhobertTokenizer","local":"transformers.PhobertTokenizer","sections":[],"depth":2}],"depth":1}';function nt(ne){return Ne(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class dt extends We{constructor(p){super(),Xe(this,p,nt,et,Ze,{})}}export{dt as component};
