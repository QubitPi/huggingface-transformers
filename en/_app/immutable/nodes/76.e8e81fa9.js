import{s as zt,o as Ut,n as Ce}from"../chunks/scheduler.9bc65507.js";import{S as Pt,i as Jt,g as d,s as r,r as f,A as Ft,h as m,f as o,c as i,j as F,u as g,x as v,k as R,y as p,a,v as h,d as u,t as _,w as b}from"../chunks/index.707bf1b6.js";import{T as jt}from"../chunks/Tip.c2ecdbf4.js";import{D as ge}from"../chunks/Docstring.17db21ae.js";import{C as mt}from"../chunks/CodeBlock.54a9f38d.js";import{E as dt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as Rt}from"../chunks/PipelineTag.44585822.js";import{H as Q}from"../chunks/Heading.342b1fa6.js";function Nt(C){let n,$="Example:",l,c,y;return c=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJpdENvbmZpZyUyQyUyMEJpdE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJpVCUyMGJpdC01MCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCaXRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwYml0LTUwJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBCaXRNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitConfig, BitModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BiT bit-50 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BitConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the bit-50 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BitModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=d("p"),n.textContent=$,l=r(),f(c.$$.fragment)},l(s){n=m(s,"P",{"data-svelte-h":!0}),v(n)!=="svelte-11lpom8"&&(n.textContent=$),l=i(s),g(c.$$.fragment,s)},m(s,w){a(s,n,w),a(s,l,w),h(c,s,w),y=!0},p:Ce,i(s){y||(u(c.$$.fragment,s),y=!0)},o(s){_(c.$$.fragment,s),y=!1},d(s){s&&(o(n),o(l)),b(c,s)}}}function Wt(C){let n,$=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=d("p"),n.innerHTML=$},l(l){n=m(l,"P",{"data-svelte-h":!0}),v(n)!=="svelte-fincs2"&&(n.innerHTML=$)},m(l,c){a(l,n,c)},p:Ce,d(l){l&&o(n)}}}function Zt(C){let n,$="Example:",l,c,y;return c=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJpdE1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmh1Z2dpbmdmYWNlJTJGY2F0cy1pbWFnZSUyMiklMEFpbWFnZSUyMCUzRCUyMGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTVCJTIyaW1hZ2UlMjIlNUQlNUIwJTVEJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZiaXQtNTAlMjIpJTBBbW9kZWwlMjAlM0QlMjBCaXRNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGYml0LTUwJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BitModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google/bit-50&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BitModel.from_pretrained(<span class="hljs-string">&quot;google/bit-50&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">2048</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`,wrap:!1}}),{c(){n=d("p"),n.textContent=$,l=r(),f(c.$$.fragment)},l(s){n=m(s,"P",{"data-svelte-h":!0}),v(n)!=="svelte-11lpom8"&&(n.textContent=$),l=i(s),g(c.$$.fragment,s)},m(s,w){a(s,n,w),a(s,l,w),h(c,s,w),y=!0},p:Ce,i(s){y||(u(c.$$.fragment,s),y=!0)},o(s){_(c.$$.fragment,s),y=!1},d(s){s&&(o(n),o(l)),b(c,s)}}}function kt(C){let n,$=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=d("p"),n.innerHTML=$},l(l){n=m(l,"P",{"data-svelte-h":!0}),v(n)!=="svelte-fincs2"&&(n.innerHTML=$)},m(l,c){a(l,n,c)},p:Ce,d(l){l&&o(n)}}}function Lt(C){let n,$="Example:",l,c,y;return c=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJpdEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRmJpdC01MCUyMiklMEFtb2RlbCUyMCUzRCUyMEJpdEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRmJpdC01MCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BitForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google/bit-50&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BitForImageClassification.from_pretrained(<span class="hljs-string">&quot;google/bit-50&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tiger cat`,wrap:!1}}),{c(){n=d("p"),n.textContent=$,l=r(),f(c.$$.fragment)},l(s){n=m(s,"P",{"data-svelte-h":!0}),v(n)!=="svelte-11lpom8"&&(n.textContent=$),l=i(s),g(c.$$.fragment,s)},m(s,w){a(s,n,w),a(s,l,w),h(c,s,w),y=!0},p:Ce,i(s){y||(u(c.$$.fragment,s),y=!0)},o(s){_(c.$$.fragment,s),y=!1},d(s){s&&(o(n),o(l)),b(c,s)}}}function qt(C){let n,$,l,c,y,s,w,Be,S,pt=`The BiT model was proposed in <a href="https://arxiv.org/abs/1912.11370" rel="nofollow">Big Transfer (BiT): General Visual Representation Learning</a> by Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby.
BiT is a simple recipe for scaling up pre-training of <a href="resnet">ResNet</a>-like architectures (specifically, ResNetv2). The method results in significant improvements for transfer learning.`,xe,H,ft="The abstract from the paper is the following:",Ie,X,gt="<em>Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes — from 1 example per class to 1M total examples. BiT achieves 87.5% top-1 accuracy on ILSVRC-2012, 99.4% on CIFAR-10, and 76.3% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8% on ILSVRC-2012 with 10 examples per class, and 97.0% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.</em>",je,V,ht=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/google-research/big_transfer" rel="nofollow">here</a>.`,ze,A,Ue,Y,ut=`<li>BiT models are equivalent to ResNetv2 in terms of architecture, except that: 1) all batch normalization layers are replaced by <a href="https://arxiv.org/abs/1803.08494" rel="nofollow">group normalization</a>,
2) <a href="https://arxiv.org/abs/1903.10520" rel="nofollow">weight standardization</a> is used for convolutional layers. The authors show that the combination of both is useful for training with large batch sizes, and has a significant
impact on transfer learning.</li>`,Pe,D,Je,O,_t="A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BiT.",Fe,K,Re,ee,bt='<li><a href="/docs/transformers/main/en/model_doc/bit#transformers.BitForImageClassification">BitForImageClassification</a> is supported by this <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">example script</a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">notebook</a>.</li> <li>See also: <a href="../tasks/image_classification">Image classification task guide</a></li>',Ne,te,yt="If you’re interested in submitting a resource to be included here, please feel free to open a Pull Request and we’ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",We,oe,Ze,T,ne,Xe,he,vt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitModel">BitModel</a>. It is used to instantiate an BiT
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the BiT
<a href="https://huggingface.co/google/bit-50" rel="nofollow">google/bit-50</a> architecture.`,Ve,ue,$t=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ae,N,ke,se,Le,I,ae,Ye,_e,wt="Constructs a BiT image processor.",De,W,re,Oe,be,Tt="Preprocess an image or batch of images.",qe,ie,Ee,j,le,Ke,ye,Mt=`The bare BiT model outputting raw features without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,et,B,ce,tt,ve,Ct='The <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitModel">BitModel</a> forward method, overrides the <code>__call__</code> special method.',ot,Z,nt,k,Ge,de,Qe,M,me,st,$e,Bt=`BiT Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,at,we,xt=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,rt,x,pe,it,Te,It='The <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitForImageClassification">BitForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',lt,L,ct,q,Se,Me,He;return y=new Q({props:{title:"Big Transfer (BiT)",local:"big-transfer-bit",headingTag:"h1"}}),w=new Q({props:{title:"Overview",local:"overview",headingTag:"h2"}}),A=new Q({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),D=new Q({props:{title:"Resources",local:"resources",headingTag:"h2"}}),K=new Rt({props:{pipeline:"image-classification"}}),oe=new Q({props:{title:"BitConfig",local:"transformers.BitConfig",headingTag:"h2"}}),ne=new ge({props:{name:"class transformers.BitConfig",anchor:"transformers.BitConfig",parameters:[{name:"num_channels",val:" = 3"},{name:"embedding_size",val:" = 64"},{name:"hidden_sizes",val:" = [256, 512, 1024, 2048]"},{name:"depths",val:" = [3, 4, 6, 3]"},{name:"layer_type",val:" = 'preactivation'"},{name:"hidden_act",val:" = 'relu'"},{name:"global_padding",val:" = None"},{name:"num_groups",val:" = 32"},{name:"drop_path_rate",val:" = 0.0"},{name:"embedding_dynamic_padding",val:" = False"},{name:"output_stride",val:" = 32"},{name:"width_factor",val:" = 1"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BitConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.BitConfig.embedding_size",description:`<strong>embedding_size</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality (hidden size) for the embedding layer.`,name:"embedding_size"},{anchor:"transformers.BitConfig.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[256, 512, 1024, 2048]</code>) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.BitConfig.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 4, 6, 3]</code>) &#x2014;
Depth (number of layers) for each stage.`,name:"depths"},{anchor:"transformers.BitConfig.layer_type",description:`<strong>layer_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;preactivation&quot;</code>) &#x2014;
The layer to use, it can be either <code>&quot;preactivation&quot;</code> or <code>&quot;bottleneck&quot;</code>.`,name:"layer_type"},{anchor:"transformers.BitConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;relu&quot;</code>) &#x2014;
The non-linear activation function in each block. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code>
are supported.`,name:"hidden_act"},{anchor:"transformers.BitConfig.global_padding",description:`<strong>global_padding</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Padding strategy to use for the convolutional layers. Can be either <code>&quot;valid&quot;</code>, <code>&quot;same&quot;</code>, or <code>None</code>.`,name:"global_padding"},{anchor:"transformers.BitConfig.num_groups",description:`<strong>num_groups</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of groups used for the <code>BitGroupNormActivation</code> layers.`,name:"num_groups"},{anchor:"transformers.BitConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The drop path rate for the stochastic depth.`,name:"drop_path_rate"},{anchor:"transformers.BitConfig.embedding_dynamic_padding",description:`<strong>embedding_dynamic_padding</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to make use of dynamic padding for the embedding layer.`,name:"embedding_dynamic_padding"},{anchor:"transformers.BitConfig.output_stride",description:`<strong>output_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The output stride of the model.`,name:"output_stride"},{anchor:"transformers.BitConfig.width_factor",description:`<strong>width_factor</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The width factor for the model.`,name:"width_factor"},{anchor:"transformers.BitConfig.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.BitConfig.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bit/configuration_bit.py#L29"}}),N=new dt({props:{anchor:"transformers.BitConfig.example",$$slots:{default:[Nt]},$$scope:{ctx:C}}}),se=new Q({props:{title:"BitImageProcessor",local:"transformers.BitImageProcessor",headingTag:"h2"}}),ae=new ge({props:{name:"class transformers.BitImageProcessor",anchor:"transformers.BitImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": Dict = None"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": Union = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"do_convert_rgb",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BitImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by
<code>do_resize</code> in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.BitImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;shortest_edge&quot; -- 224}</code>):
Size of the image after resizing. The shortest edge of the image is resized to size[&#x201C;shortest_edge&#x201D;], with
the longest edge resized to keep the input aspect ratio. Can be overridden by <code>size</code> in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.BitImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by <code>resample</code> in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.BitImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the image to the specified <code>crop_size</code>. Can be overridden by <code>do_center_crop</code> in the
<code>preprocess</code> method.`,name:"do_center_crop"},{anchor:"transformers.BitImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to 224) &#x2014;
Size of the output image after applying <code>center_crop</code>. Can be overridden by <code>crop_size</code> in the <code>preprocess</code>
method.`,name:"crop_size"},{anchor:"transformers.BitImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by <code>do_rescale</code> in
the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.BitImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by <code>rescale_factor</code> in the <code>preprocess</code>
method.
do_normalize &#x2014;
Whether to normalize the image. Can be overridden by <code>do_normalize</code> in the <code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.BitImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>OPENAI_CLIP_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.BitImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>OPENAI_CLIP_MEAN</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.
Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.BitImageProcessor.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bit/image_processing_bit.py#L50"}}),re=new ge({props:{name:"preprocess",anchor:"transformers.BitImageProcessor.preprocess",parameters:[{name:"images",val:": Union"},{name:"do_resize",val:": bool = None"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = None"},{name:"do_center_crop",val:": bool = None"},{name:"crop_size",val:": int = None"},{name:"do_rescale",val:": bool = None"},{name:"rescale_factor",val:": float = None"},{name:"do_normalize",val:": bool = None"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"do_convert_rgb",val:": bool = None"},{name:"return_tensors",val:": Union = None"},{name:"data_format",val:": Optional = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BitImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.BitImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.BitImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image after resizing. Shortest edge of the image is resized to size[&#x201C;shortest_edge&#x201D;], with
the longest edge resized to keep the input aspect ratio.`,name:"size"},{anchor:"transformers.BitImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.BitImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.BitImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the center crop. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.BitImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image.`,name:"do_rescale"},{anchor:"transformers.BitImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.BitImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.BitImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean to use for normalization. Only has an effect if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.BitImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation to use for normalization. Only has an effect if <code>do_normalize</code> is set to
<code>True</code>.`,name:"image_std"},{anchor:"transformers.BitImageProcessor.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_convert_rgb</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.BitImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.BitImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.BitImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bit/image_processing_bit.py#L174"}}),ie=new Q({props:{title:"BitModel",local:"transformers.BitModel",headingTag:"h2"}}),le=new ge({props:{name:"class transformers.BitModel",anchor:"transformers.BitModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BitModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/bit#transformers.BitConfig">BitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bit/modeling_bit.py#L697"}}),ce=new ge({props:{name:"forward",anchor:"transformers.BitModel.forward",parameters:[{name:"pixel_values",val:": Tensor"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BitModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">BitImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.BitModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BitModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bit/modeling_bit.py#L719",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/bit#transformers.BitConfig"
>BitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Z=new jt({props:{$$slots:{default:[Wt]},$$scope:{ctx:C}}}),k=new dt({props:{anchor:"transformers.BitModel.forward.example",$$slots:{default:[Zt]},$$scope:{ctx:C}}}),de=new Q({props:{title:"BitForImageClassification",local:"transformers.BitForImageClassification",headingTag:"h2"}}),me=new ge({props:{name:"class transformers.BitForImageClassification",anchor:"transformers.BitForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BitForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/bit#transformers.BitConfig">BitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bit/modeling_bit.py#L757"}}),pe=new ge({props:{name:"forward",anchor:"transformers.BitForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BitForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">BitImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.BitForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BitForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BitForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bit/modeling_bit.py#L777",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/bit#transformers.BitConfig"
>BitConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),L=new jt({props:{$$slots:{default:[kt]},$$scope:{ctx:C}}}),q=new dt({props:{anchor:"transformers.BitForImageClassification.forward.example",$$slots:{default:[Lt]},$$scope:{ctx:C}}}),{c(){n=d("meta"),$=r(),l=d("p"),c=r(),f(y.$$.fragment),s=r(),f(w.$$.fragment),Be=r(),S=d("p"),S.innerHTML=pt,xe=r(),H=d("p"),H.textContent=ft,Ie=r(),X=d("p"),X.innerHTML=gt,je=r(),V=d("p"),V.innerHTML=ht,ze=r(),f(A.$$.fragment),Ue=r(),Y=d("ul"),Y.innerHTML=ut,Pe=r(),f(D.$$.fragment),Je=r(),O=d("p"),O.textContent=_t,Fe=r(),f(K.$$.fragment),Re=r(),ee=d("ul"),ee.innerHTML=bt,Ne=r(),te=d("p"),te.textContent=yt,We=r(),f(oe.$$.fragment),Ze=r(),T=d("div"),f(ne.$$.fragment),Xe=r(),he=d("p"),he.innerHTML=vt,Ve=r(),ue=d("p"),ue.innerHTML=$t,Ae=r(),f(N.$$.fragment),ke=r(),f(se.$$.fragment),Le=r(),I=d("div"),f(ae.$$.fragment),Ye=r(),_e=d("p"),_e.textContent=wt,De=r(),W=d("div"),f(re.$$.fragment),Oe=r(),be=d("p"),be.textContent=Tt,qe=r(),f(ie.$$.fragment),Ee=r(),j=d("div"),f(le.$$.fragment),Ke=r(),ye=d("p"),ye.innerHTML=Mt,et=r(),B=d("div"),f(ce.$$.fragment),tt=r(),ve=d("p"),ve.innerHTML=Ct,ot=r(),f(Z.$$.fragment),nt=r(),f(k.$$.fragment),Ge=r(),f(de.$$.fragment),Qe=r(),M=d("div"),f(me.$$.fragment),st=r(),$e=d("p"),$e.textContent=Bt,at=r(),we=d("p"),we.innerHTML=xt,rt=r(),x=d("div"),f(pe.$$.fragment),it=r(),Te=d("p"),Te.innerHTML=It,lt=r(),f(L.$$.fragment),ct=r(),f(q.$$.fragment),Se=r(),Me=d("p"),this.h()},l(e){const t=Ft("svelte-u9bgzb",document.head);n=m(t,"META",{name:!0,content:!0}),t.forEach(o),$=i(e),l=m(e,"P",{}),F(l).forEach(o),c=i(e),g(y.$$.fragment,e),s=i(e),g(w.$$.fragment,e),Be=i(e),S=m(e,"P",{"data-svelte-h":!0}),v(S)!=="svelte-2l65fw"&&(S.innerHTML=pt),xe=i(e),H=m(e,"P",{"data-svelte-h":!0}),v(H)!=="svelte-vfdo9a"&&(H.textContent=ft),Ie=i(e),X=m(e,"P",{"data-svelte-h":!0}),v(X)!=="svelte-ea79tk"&&(X.innerHTML=gt),je=i(e),V=m(e,"P",{"data-svelte-h":!0}),v(V)!=="svelte-114mxy8"&&(V.innerHTML=ht),ze=i(e),g(A.$$.fragment,e),Ue=i(e),Y=m(e,"UL",{"data-svelte-h":!0}),v(Y)!=="svelte-7gnaj2"&&(Y.innerHTML=ut),Pe=i(e),g(D.$$.fragment,e),Je=i(e),O=m(e,"P",{"data-svelte-h":!0}),v(O)!=="svelte-jm9112"&&(O.textContent=_t),Fe=i(e),g(K.$$.fragment,e),Re=i(e),ee=m(e,"UL",{"data-svelte-h":!0}),v(ee)!=="svelte-usb3o0"&&(ee.innerHTML=bt),Ne=i(e),te=m(e,"P",{"data-svelte-h":!0}),v(te)!=="svelte-1xesile"&&(te.textContent=yt),We=i(e),g(oe.$$.fragment,e),Ze=i(e),T=m(e,"DIV",{class:!0});var z=F(T);g(ne.$$.fragment,z),Xe=i(z),he=m(z,"P",{"data-svelte-h":!0}),v(he)!=="svelte-1svubdi"&&(he.innerHTML=vt),Ve=i(z),ue=m(z,"P",{"data-svelte-h":!0}),v(ue)!=="svelte-o55m63"&&(ue.innerHTML=$t),Ae=i(z),g(N.$$.fragment,z),z.forEach(o),ke=i(e),g(se.$$.fragment,e),Le=i(e),I=m(e,"DIV",{class:!0});var P=F(I);g(ae.$$.fragment,P),Ye=i(P),_e=m(P,"P",{"data-svelte-h":!0}),v(_e)!=="svelte-zncq1j"&&(_e.textContent=wt),De=i(P),W=m(P,"DIV",{class:!0});var fe=F(W);g(re.$$.fragment,fe),Oe=i(fe),be=m(fe,"P",{"data-svelte-h":!0}),v(be)!=="svelte-1x3yxsa"&&(be.textContent=Tt),fe.forEach(o),P.forEach(o),qe=i(e),g(ie.$$.fragment,e),Ee=i(e),j=m(e,"DIV",{class:!0});var J=F(j);g(le.$$.fragment,J),Ke=i(J),ye=m(J,"P",{"data-svelte-h":!0}),v(ye)!=="svelte-1fxsgvt"&&(ye.innerHTML=Mt),et=i(J),B=m(J,"DIV",{class:!0});var U=F(B);g(ce.$$.fragment,U),tt=i(U),ve=m(U,"P",{"data-svelte-h":!0}),v(ve)!=="svelte-1j8iwzk"&&(ve.innerHTML=Ct),ot=i(U),g(Z.$$.fragment,U),nt=i(U),g(k.$$.fragment,U),U.forEach(o),J.forEach(o),Ge=i(e),g(de.$$.fragment,e),Qe=i(e),M=m(e,"DIV",{class:!0});var E=F(M);g(me.$$.fragment,E),st=i(E),$e=m(E,"P",{"data-svelte-h":!0}),v($e)!=="svelte-a01xtz"&&($e.textContent=Bt),at=i(E),we=m(E,"P",{"data-svelte-h":!0}),v(we)!=="svelte-1gjh92c"&&(we.innerHTML=xt),rt=i(E),x=m(E,"DIV",{class:!0});var G=F(x);g(pe.$$.fragment,G),it=i(G),Te=m(G,"P",{"data-svelte-h":!0}),v(Te)!=="svelte-8ugfb4"&&(Te.innerHTML=It),lt=i(G),g(L.$$.fragment,G),ct=i(G),g(q.$$.fragment,G),G.forEach(o),E.forEach(o),Se=i(e),Me=m(e,"P",{}),F(Me).forEach(o),this.h()},h(){R(n,"name","hf:doc:metadata"),R(n,"content",Et),R(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),R(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){p(document.head,n),a(e,$,t),a(e,l,t),a(e,c,t),h(y,e,t),a(e,s,t),h(w,e,t),a(e,Be,t),a(e,S,t),a(e,xe,t),a(e,H,t),a(e,Ie,t),a(e,X,t),a(e,je,t),a(e,V,t),a(e,ze,t),h(A,e,t),a(e,Ue,t),a(e,Y,t),a(e,Pe,t),h(D,e,t),a(e,Je,t),a(e,O,t),a(e,Fe,t),h(K,e,t),a(e,Re,t),a(e,ee,t),a(e,Ne,t),a(e,te,t),a(e,We,t),h(oe,e,t),a(e,Ze,t),a(e,T,t),h(ne,T,null),p(T,Xe),p(T,he),p(T,Ve),p(T,ue),p(T,Ae),h(N,T,null),a(e,ke,t),h(se,e,t),a(e,Le,t),a(e,I,t),h(ae,I,null),p(I,Ye),p(I,_e),p(I,De),p(I,W),h(re,W,null),p(W,Oe),p(W,be),a(e,qe,t),h(ie,e,t),a(e,Ee,t),a(e,j,t),h(le,j,null),p(j,Ke),p(j,ye),p(j,et),p(j,B),h(ce,B,null),p(B,tt),p(B,ve),p(B,ot),h(Z,B,null),p(B,nt),h(k,B,null),a(e,Ge,t),h(de,e,t),a(e,Qe,t),a(e,M,t),h(me,M,null),p(M,st),p(M,$e),p(M,at),p(M,we),p(M,rt),p(M,x),h(pe,x,null),p(x,it),p(x,Te),p(x,lt),h(L,x,null),p(x,ct),h(q,x,null),a(e,Se,t),a(e,Me,t),He=!0},p(e,[t]){const z={};t&2&&(z.$$scope={dirty:t,ctx:e}),N.$set(z);const P={};t&2&&(P.$$scope={dirty:t,ctx:e}),Z.$set(P);const fe={};t&2&&(fe.$$scope={dirty:t,ctx:e}),k.$set(fe);const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),L.$set(J);const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),q.$set(U)},i(e){He||(u(y.$$.fragment,e),u(w.$$.fragment,e),u(A.$$.fragment,e),u(D.$$.fragment,e),u(K.$$.fragment,e),u(oe.$$.fragment,e),u(ne.$$.fragment,e),u(N.$$.fragment,e),u(se.$$.fragment,e),u(ae.$$.fragment,e),u(re.$$.fragment,e),u(ie.$$.fragment,e),u(le.$$.fragment,e),u(ce.$$.fragment,e),u(Z.$$.fragment,e),u(k.$$.fragment,e),u(de.$$.fragment,e),u(me.$$.fragment,e),u(pe.$$.fragment,e),u(L.$$.fragment,e),u(q.$$.fragment,e),He=!0)},o(e){_(y.$$.fragment,e),_(w.$$.fragment,e),_(A.$$.fragment,e),_(D.$$.fragment,e),_(K.$$.fragment,e),_(oe.$$.fragment,e),_(ne.$$.fragment,e),_(N.$$.fragment,e),_(se.$$.fragment,e),_(ae.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),_(ce.$$.fragment,e),_(Z.$$.fragment,e),_(k.$$.fragment,e),_(de.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(L.$$.fragment,e),_(q.$$.fragment,e),He=!1},d(e){e&&(o($),o(l),o(c),o(s),o(Be),o(S),o(xe),o(H),o(Ie),o(X),o(je),o(V),o(ze),o(Ue),o(Y),o(Pe),o(Je),o(O),o(Fe),o(Re),o(ee),o(Ne),o(te),o(We),o(Ze),o(T),o(ke),o(Le),o(I),o(qe),o(Ee),o(j),o(Ge),o(Qe),o(M),o(Se),o(Me)),o(n),b(y,e),b(w,e),b(A,e),b(D,e),b(K,e),b(oe,e),b(ne),b(N),b(se,e),b(ae),b(re),b(ie,e),b(le),b(ce),b(Z),b(k),b(de,e),b(me),b(pe),b(L),b(q)}}}const Et='{"title":"Big Transfer (BiT)","local":"big-transfer-bit","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"BitConfig","local":"transformers.BitConfig","sections":[],"depth":2},{"title":"BitImageProcessor","local":"transformers.BitImageProcessor","sections":[],"depth":2},{"title":"BitModel","local":"transformers.BitModel","sections":[],"depth":2},{"title":"BitForImageClassification","local":"transformers.BitForImageClassification","sections":[],"depth":2}],"depth":1}';function Gt(C){return Ut(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ot extends Pt{constructor(n){super(),Jt(this,n,Gt,qt,zt,{})}}export{Ot as component};
