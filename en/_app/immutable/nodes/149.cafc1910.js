import{s as Gn,o as Sn,n as Z}from"../chunks/scheduler.9bc65507.js";import{S as Rn,i as Fn,g as c,s as l,r as h,A as Nn,h as m,f as s,c as r,j as N,u as f,x as J,k as P,y as p,a as i,v as g,d as T,t as _,w as M}from"../chunks/index.707bf1b6.js";import{T as Vn}from"../chunks/Tip.c2ecdbf4.js";import{D as Y}from"../chunks/Docstring.17db21ae.js";import{C as K}from"../chunks/CodeBlock.54a9f38d.js";import{E as ve}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as S}from"../chunks/Heading.342b1fa6.js";function Pn(C){let t,u="Example:",a,d,y;return d=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVFNhbkphcGFuZXNlVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwR1BUU2FuSmFwYW5lc2VUb2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMlRhbnJlaSUyRkdQVFNBTi1qYXBhbmVzZSUyMiklMEElMjMlMjBZb3UlMjBjYW4lMjBjb25maXJtJTIwYm90aCUyMCVFNiU4NSVCNiVFNSVCRiU5QyUyMGFuZCUyMCVFNiU4NSVCNiVFNiU4NyU4OSUyMGFyZSUyMGVuY29kZWQlMjB0byUyMDE3NzUwJTBBdG9rZW5pemVyKCUyMiVFNSU5MCVCRSVFOCVCQyVBOSVFMyU4MSVBRiVFNyU4QyVBQiVFMyU4MSVBNyVFMyU4MSU4MiVFMyU4MiU4QiVGMCU5RiU5MCVBRiVFMyU4MCU4MiVFNSVBRSU5RiVFMyU4MSVBRiVFNiU4NSVCNiVFNSVCRiU5QyglRTYlODUlQjYlRTYlODclODkpJUU1JUE0JUE3JUU1JUFEJUE2JUU1JTg3JUJBJUU4JUJBJUFCJTIyKSU1QiUyMmlucHV0X2lkcyUyMiU1RCUwQSUwQSUyMyUyMEJvdGglMjAlRTYlODUlQjYlRTUlQkYlOUMlMjBhbmQlMjAlRTYlODUlQjYlRTYlODclODklMjBhcmUlMjBkZWNvZGVkJTIwdG8lMjAlRTYlODUlQjYlRTUlQkYlOUMlMEF0b2tlbml6ZXIuZGVjb2RlKHRva2VuaXplciglMjIlRTUlOTAlQkUlRTglQkMlQTklRTMlODElQUYlRTclOEMlQUIlRTMlODElQTclRTMlODElODIlRTMlODIlOEIlRjAlOUYlOTAlQUYlRTMlODAlODIlRTUlQUUlOUYlRTMlODElQUYlRTYlODUlQjYlRTUlQkYlOUMoJUU2JTg1JUI2JUU2JTg3JTg5KSVFNSVBNCVBNyVFNSVBRCVBNiVFNSU4NyVCQSVFOCVCQSVBQiUyMiklNUIlMjJpbnB1dF9pZHMlMjIlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTSanJapaneseTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTSanJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># You can confirm both ÊÖ∂Âøú and ÊÖ∂Êáâ are encoded to 17750</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer(<span class="hljs-string">&quot;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
[<span class="hljs-number">35993</span>, <span class="hljs-number">35998</span>, <span class="hljs-number">34347</span>, <span class="hljs-number">31459</span>, <span class="hljs-number">30647</span>, <span class="hljs-number">31448</span>, <span class="hljs-number">25</span>, <span class="hljs-number">30659</span>, <span class="hljs-number">35729</span>, <span class="hljs-number">35676</span>, <span class="hljs-number">32417</span>, <span class="hljs-number">30647</span>, <span class="hljs-number">17750</span>, <span class="hljs-number">35589</span>, <span class="hljs-number">17750</span>, <span class="hljs-number">35590</span>, <span class="hljs-number">321</span>, <span class="hljs-number">1281</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Both ÊÖ∂Âøú and ÊÖ∂Êáâ are decoded to ÊÖ∂Âøú</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(tokenizer(<span class="hljs-string">&quot;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&#x27;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Âøú)Â§ßÂ≠¶Âá∫Ë∫´&#x27;</span>`,wrap:!1}}),{c(){t=c("p"),t.textContent=u,a=l(),h(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),J(t)!=="svelte-11lpom8"&&(t.textContent=u),a=r(o),f(d.$$.fragment,o)},m(o,b){i(o,t,b),i(o,a,b),g(d,o,b),y=!0},p:Z,i(o){y||(T(d.$$.fragment,o),y=!0)},o(o){_(d.$$.fragment,o),y=!1},d(o){o&&(s(t),s(a)),M(d,o)}}}function In(C){let t,u="Example for Prefix-LM:",a,d,y;return d=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVFNhbkphcGFuZXNlVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwR1BUU2FuSmFwYW5lc2VUb2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMlRhbnJlaSUyRkdQVFNBTi1qYXBhbmVzZSUyMiklMEF0b2tlbml6ZXIoJTIyJUU1JUFFJTlGJUUzJTgxJUFGJUU2JTg1JUI2JUU1JUJGJTlDKCVFNiU4NSVCNiVFNiU4NyU4OSklRTUlQTQlQTclRTUlQUQlQTYlRTUlODclQkElRTglQkElQUIlMjIlMkMlMjBwcmVmaXhfdGV4dCUzRCUyMiVFNSU5MCVCRSVFOCVCQyVBOSVFMyU4MSVBRiVFNyU4QyVBQiVFMyU4MSVBNyVFMyU4MSU4MiVFMyU4MiU4QiVGMCU5RiU5MCVBRiVFMyU4MCU4MiUyMiklNUIlMjJpbnB1dF9pZHMlMjIlNUQlMEElMEElMjMlMjBNYXNrJTIwZm9yJTIwUHJlZml4LUxNJTIwaW5wdXRzJTBBdG9rZW5pemVyKCUyMiVFNSVBRSU5RiVFMyU4MSVBRiVFNiU4NSVCNiVFNSVCRiU5QyglRTYlODUlQjYlRTYlODclODkpJUU1JUE0JUE3JUU1JUFEJUE2JUU1JTg3JUJBJUU4JUJBJUFCJTIyJTJDJTIwcHJlZml4X3RleHQlM0QlMjIlRTUlOTAlQkUlRTglQkMlQTklRTMlODElQUYlRTclOEMlQUIlRTMlODElQTclRTMlODElODIlRTMlODIlOEIlRjAlOUYlOTAlQUYlRTMlODAlODIlMjIpJTVCJTIydG9rZW5fdHlwZV9pZHMlMjIlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTSanJapaneseTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTSanJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer(<span class="hljs-string">&quot;ÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´&quot;</span>, prefix_text=<span class="hljs-string">&quot;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇ&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
[<span class="hljs-number">35993</span>, <span class="hljs-number">34347</span>, <span class="hljs-number">31459</span>, <span class="hljs-number">30647</span>, <span class="hljs-number">31448</span>, <span class="hljs-number">25</span>, <span class="hljs-number">30659</span>, <span class="hljs-number">35729</span>, <span class="hljs-number">35676</span>, <span class="hljs-number">35998</span>, <span class="hljs-number">32417</span>, <span class="hljs-number">30647</span>, <span class="hljs-number">17750</span>, <span class="hljs-number">35589</span>, <span class="hljs-number">17750</span>, <span class="hljs-number">35590</span>, <span class="hljs-number">321</span>, <span class="hljs-number">1281</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Mask for Prefix-LM inputs</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer(<span class="hljs-string">&quot;ÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´&quot;</span>, prefix_text=<span class="hljs-string">&quot;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇ&quot;</span>)[<span class="hljs-string">&quot;token_type_ids&quot;</span>]
[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]`,wrap:!1}}),{c(){t=c("p"),t.textContent=u,a=l(),h(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),J(t)!=="svelte-sklucv"&&(t.textContent=u),a=r(o),f(d.$$.fragment,o)},m(o,b){i(o,t,b),i(o,a,b),g(d,o,b),y=!0},p:Z,i(o){y||(T(d.$$.fragment,o),y=!0)},o(o){_(d.$$.fragment,o),y=!1},d(o){o&&(s(t),s(a)),M(d,o)}}}function Qn(C){let t,u="Example for batch encode:",a,d,y;return d=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVFNhbkphcGFuZXNlVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwR1BUU2FuSmFwYW5lc2VUb2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMlRhbnJlaSUyRkdQVFNBTi1qYXBhbmVzZSUyMiklMEF0b2tlbml6ZXIoJTVCJTVCJTIyJUU2JUFEJUE2JUU3JTk0JUIwJUU0JUJGJUExJUU3JThFJTg0JTIyJTJDJTIwJTIyJUUzJTgxJUFGJUUzJTgwJTgxJTIyJTVEJTJDJTIwJTVCJTIyJUU3JUI5JTk0JUU3JTk0JUIwJUU0JUJGJUExJUU5JTk1JUI3JTIyJTJDJTIwJTIyJUUzJTgxJUFFJUU5JTg1JThEJUU0JUI4JThCJUUzJTgxJUFFJUUzJTgwJTgxJTIyJTVEJTVEJTJDJTIwcGFkZGluZyUzRFRydWUpJTVCJTIyaW5wdXRfaWRzJTIyJTVEJTBBJTBBJTIzJTIwTWFzayUyMGZvciUyMFByZWZpeC1MTSUyMGlucHV0cyUwQXRva2VuaXplciglNUIlNUIlMjIlRTYlQUQlQTYlRTclOTQlQjAlRTQlQkYlQTElRTclOEUlODQlMjIlMkMlMjAlMjIlRTMlODElQUYlRTMlODAlODElMjIlNUQlMkMlMjAlNUIlMjIlRTclQjklOTQlRTclOTQlQjAlRTQlQkYlQTElRTklOTUlQjclMjIlMkMlMjAlMjIlRTMlODElQUUlRTklODUlOEQlRTQlQjglOEIlRTMlODElQUUlRTMlODAlODElMjIlNUQlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSklNUIlMjJ0b2tlbl90eXBlX2lkcyUyMiU1RCUwQSUwQSUyMyUyME1hc2slMjBmb3IlMjBwYWRkaW5nJTBBdG9rZW5pemVyKCU1QiU1QiUyMiVFNiVBRCVBNiVFNyU5NCVCMCVFNCVCRiVBMSVFNyU4RSU4NCUyMiUyQyUyMCUyMiVFMyU4MSVBRiVFMyU4MCU4MSUyMiU1RCUyQyUyMCU1QiUyMiVFNyVCOSU5NCVFNyU5NCVCMCVFNCVCRiVBMSVFOSU5NSVCNyUyMiUyQyUyMCUyMiVFMyU4MSVBRSVFOSU4NSU4RCVFNCVCOCU4QiVFMyU4MSVBRSVFMyU4MCU4MSUyMiU1RCU1RCUyQyUyMHBhZGRpbmclM0RUcnVlKSU1QiUyMmF0dGVudGlvbl9tYXNrJTIyJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTSanJapaneseTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTSanJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer([[<span class="hljs-string">&quot;Ê≠¶Áî∞‰ø°ÁéÑ&quot;</span>, <span class="hljs-string">&quot;„ÅØ„ÄÅ&quot;</span>], [<span class="hljs-string">&quot;ÁπîÁî∞‰ø°Èï∑&quot;</span>, <span class="hljs-string">&quot;„ÅÆÈÖç‰∏ã„ÅÆ„ÄÅ&quot;</span>]], padding=<span class="hljs-literal">True</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
[[<span class="hljs-number">35993</span>, <span class="hljs-number">8640</span>, <span class="hljs-number">25948</span>, <span class="hljs-number">35998</span>, <span class="hljs-number">30647</span>, <span class="hljs-number">35675</span>, <span class="hljs-number">35999</span>, <span class="hljs-number">35999</span>], [<span class="hljs-number">35993</span>, <span class="hljs-number">10382</span>, <span class="hljs-number">9868</span>, <span class="hljs-number">35998</span>, <span class="hljs-number">30646</span>, <span class="hljs-number">9459</span>, <span class="hljs-number">30646</span>, <span class="hljs-number">35675</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Mask for Prefix-LM inputs</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer([[<span class="hljs-string">&quot;Ê≠¶Áî∞‰ø°ÁéÑ&quot;</span>, <span class="hljs-string">&quot;„ÅØ„ÄÅ&quot;</span>], [<span class="hljs-string">&quot;ÁπîÁî∞‰ø°Èï∑&quot;</span>, <span class="hljs-string">&quot;„ÅÆÈÖç‰∏ã„ÅÆ„ÄÅ&quot;</span>]], padding=<span class="hljs-literal">True</span>)[<span class="hljs-string">&quot;token_type_ids&quot;</span>]
[[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Mask for padding</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer([[<span class="hljs-string">&quot;Ê≠¶Áî∞‰ø°ÁéÑ&quot;</span>, <span class="hljs-string">&quot;„ÅØ„ÄÅ&quot;</span>], [<span class="hljs-string">&quot;ÁπîÁî∞‰ø°Èï∑&quot;</span>, <span class="hljs-string">&quot;„ÅÆÈÖç‰∏ã„ÅÆ„ÄÅ&quot;</span>]], padding=<span class="hljs-literal">True</span>)[<span class="hljs-string">&quot;attention_mask&quot;</span>]
[[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]]`,wrap:!1}}),{c(){t=c("p"),t.textContent=u,a=l(),h(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),J(t)!=="svelte-1t2nozh"&&(t.textContent=u),a=r(o),f(d.$$.fragment,o)},m(o,b){i(o,t,b),i(o,a,b),g(d,o,b),y=!0},p:Z,i(o){y||(T(d.$$.fragment,o),y=!0)},o(o){_(d.$$.fragment,o),y=!1},d(o){o&&(s(t),s(a)),M(d,o)}}}function zn(C){let t,u="Example:",a,d,y;return d=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVFNhbkphcGFuZXNlVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwR1BUU2FuSmFwYW5lc2VUb2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMlRhbnJlaSUyRkdQVFNBTi1qYXBhbmVzZSUyMiklMEF4X3Rva2VuJTIwJTNEJTIwdG9rZW5pemVyKCUyMiVFRiVCRCVCMSVFRiVCRCVCMiVFRiVCRCVCMyVFRiVCRCVCNCUyMiklMEElMjMlMjBpbnB1dF9pZHMlM0ElMjAlMjAlMjAlMjAlMjAlMjAlN0MlMjBTT1QlMjAlN0MlMjBTRUclMjAlN0MlMjAlRUYlQkQlQjElMjAlN0MlMjAlRUYlQkQlQjIlMjAlN0MlMjAlRUYlQkQlQjMlMjAlN0MlMjAlRUYlQkQlQjQlMjAlN0MlMEElMjMlMjB0b2tlbl90eXBlX2lkcyUzQSUyMCU3QyUyMDElMjAlMjAlMjAlN0MlMjAwJTIwJTIwJTIwJTdDJTIwMCUyMCU3QyUyMDAlMjAlN0MlMjAwJTIwJTdDJTIwMCUyMCU3QyUwQSUwQXhfdG9rZW4lMjAlM0QlMjB0b2tlbml6ZXIoJTIyJTIyJTJDJTIwcHJlZml4X3RleHQlM0QlMjIlRUYlQkQlQjElRUYlQkQlQjIlRUYlQkQlQjMlRUYlQkQlQjQlMjIpJTBBJTIzJTIwaW5wdXRfaWRzJTNBJTIwJTIwJTIwJTIwJTIwJTIwJTdDJTIwU09UJTIwJTdDJTIwJUVGJUJEJUIxJTIwJTdDJTIwJUVGJUJEJUIyJTIwJTdDJTIwJUVGJUJEJUIzJTIwJTdDJTIwJUVGJUJEJUI0JTIwJTdDJTIwU0VHJTIwJTdDJTBBJTIzJTIwdG9rZW5fdHlwZV9pZHMlM0ElMjAlN0MlMjAxJTIwJTIwJTIwJTdDJTIwMSUyMCU3QyUyMDElMjAlN0MlMjAxJTIwJTdDJTIwMSUyMCU3QyUyMDAlMjAlMjAlN0MlMEElMEF4X3Rva2VuJTIwJTNEJTIwdG9rZW5pemVyKCUyMiVFRiVCRCVCMyVFRiVCRCVCNCUyMiUyQyUyMHByZWZpeF90ZXh0JTNEJTIyJUVGJUJEJUIxJUVGJUJEJUIyJTIyKSUwQSUyMyUyMGlucHV0X2lkcyUzQSUyMCUyMCUyMCUyMCUyMCUyMCU3QyUyMFNPVCUyMCU3QyUyMCVFRiVCRCVCMSUyMCU3QyUyMCVFRiVCRCVCMiUyMCU3QyUyMFNFRyUyMCU3QyUyMCVFRiVCRCVCMyUyMCU3QyUyMCVFRiVCRCVCNCUyMCU3QyUwQSUyMyUyMHRva2VuX3R5cGVfaWRzJTNBJTIwJTdDJTIwMSUyMCUyMCUyMCU3QyUyMDElMjAlN0MlMjAxJTIwJTdDJTIwMCUyMCUyMCUyMCU3QyUyMDAlMjAlN0MlMjAwJTIwJTdD",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTSanJapaneseTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTSanJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>x_token = tokenizer(<span class="hljs-string">&quot;ÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># input_ids:      | SOT | SEG | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ |</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>x_token = tokenizer(<span class="hljs-string">&quot;&quot;</span>, prefix_text=<span class="hljs-string">&quot;ÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># input_ids:      | SOT | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ | SEG |</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>x_token = tokenizer(<span class="hljs-string">&quot;ÔΩ≥ÔΩ¥&quot;</span>, prefix_text=<span class="hljs-string">&quot;ÔΩ±ÔΩ≤&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># input_ids:      | SOT | ÔΩ± | ÔΩ≤ | SEG | ÔΩ≥ | ÔΩ¥ |</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |</span>`,wrap:!1}}),{c(){t=c("p"),t.textContent=u,a=l(),h(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),J(t)!=="svelte-11lpom8"&&(t.textContent=u),a=r(o),f(d.$$.fragment,o)},m(o,b){i(o,t,b),i(o,a,b),g(d,o,b),y=!0},p:Z,i(o){y||(T(d.$$.fragment,o),y=!0)},o(o){_(d.$$.fragment,o),y=!1},d(o){o&&(s(t),s(a)),M(d,o)}}}function Zn(C){let t,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=u},l(a){t=m(a,"P",{"data-svelte-h":!0}),J(t)!=="svelte-fincs2"&&(t.innerHTML=u)},m(a,d){i(a,t,d)},p:Z,d(a){a&&s(t)}}}function qn(C){let t,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=u},l(a){t=m(a,"P",{"data-svelte-h":!0}),J(t)!=="svelte-fincs2"&&(t.innerHTML=u)},m(a,d){i(a,t,d)},p:Z,d(a){a&&s(t)}}}function En(C){let t,u;return t=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUyQyUyMEF1dG9Ub2tlbml6ZXIlMkMlMjB0cmFpbmVyX3V0aWxzJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJUYW5yZWklMkZHUFRTQU4tamFwYW5lc2UlMjIpLnRvKGRldmljZSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJUYW5yZWklMkZHUFRTQU4tamFwYW5lc2UlMjIpJTBBeF90b2tlbiUyMCUzRCUyMHRva2VuaXplciglMjIlRTclQjklOTQlRTclOTQlQjAlRTQlQkYlQTElRTklOTUlQjclRTMlODElQUYlRTMlODAlODElMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXRyYWluZXJfdXRpbHMuc2V0X3NlZWQoMzApJTBBaW5wdXRfaWRzJTIwJTNEJTIweF90b2tlbi5pbnB1dF9pZHMudG8oZGV2aWNlKSUwQWdlbl90b2tlbiUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKGlucHV0X2lkcyUyQyUyMG1heF9uZXdfdG9rZW5zJTNENTApJTBBdG9rZW5pemVyLmRlY29kZShnZW5fdG9rZW4lNUIwJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer, trainer_utils

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>).to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>x_token = tokenizer(<span class="hljs-string">&quot;ÁπîÁî∞‰ø°Èï∑„ÅØ„ÄÅ&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer_utils.set_seed(<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = x_token.input_ids.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_token = model.generate(input_ids, max_new_tokens=<span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(gen_token[<span class="hljs-number">0</span>])
<span class="hljs-string">&quot;ÁπîÁî∞‰ø°Èï∑„ÅØ„ÄÅÊîøÊ≤ª„ÉªËªç‰∫ã„ÅÆ‰∏≠Êû¢„Åæ„ÅßÊéåÊè°„Åó„ÅüÊîøÊ≤ªÂÆ∂„Åß„ÅÇ„Çä„ÄÅÊó•Êú¨Âè≤‰∏äÈ°û„ÇíË¶ã„Å™„ÅÑÈ©öÁï∞ÁöÑ„Å™Ëªç‰∫ã‰æµÊîª„ÇíÁ∂ö„Åë...&quot;</span>`,wrap:!1}}),{c(){h(t.$$.fragment)},l(a){f(t.$$.fragment,a)},m(a,d){g(t,a,d),u=!0},p:Z,i(a){u||(T(t.$$.fragment,a),u=!0)},o(a){_(t.$$.fragment,a),u=!1},d(a){M(t,a)}}}function Bn(C){let t,u;return t=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUyQyUyMEF1dG9Ub2tlbml6ZXIlMkMlMjB0cmFpbmVyX3V0aWxzJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJUYW5yZWklMkZHUFRTQU4tamFwYW5lc2UlMjIpLnRvKGRldmljZSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJUYW5yZWklMkZHUFRTQU4tamFwYW5lc2UlMjIpJTBBeF90b2tlbiUyMCUzRCUyMHRva2VuaXplciglMjIlMjIlMkMlMjBwcmVmaXhfdGV4dCUzRCUyMiVFNyVCOSU5NCVFNyU5NCVCMCVFNCVCRiVBMSVFOSU5NSVCNyVFMyU4MSVBRiVFMyU4MCU4MSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBdHJhaW5lcl91dGlscy5zZXRfc2VlZCgzMCklMEFpbnB1dF9pZHMlMjAlM0QlMjB4X3Rva2VuLmlucHV0X2lkcy50byhkZXZpY2UpJTBBdG9rZW5fdHlwZV9pZHMlMjAlM0QlMjB4X3Rva2VuLnRva2VuX3R5cGVfaWRzLnRvKGRldmljZSklMEFnZW5fdG9rZW4lMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZShpbnB1dF9pZHMlMkMlMjB0b2tlbl90eXBlX2lkcyUzRHRva2VuX3R5cGVfaWRzJTJDJTIwbWF4X25ld190b2tlbnMlM0Q1MCklMEF0b2tlbml6ZXIuZGVjb2RlKGdlbl90b2tlbiU1QjAlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer, trainer_utils

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>).to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>x_token = tokenizer(<span class="hljs-string">&quot;&quot;</span>, prefix_text=<span class="hljs-string">&quot;ÁπîÁî∞‰ø°Èï∑„ÅØ„ÄÅ&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer_utils.set_seed(<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = x_token.input_ids.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>token_type_ids = x_token.token_type_ids.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=<span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(gen_token[<span class="hljs-number">0</span>])
<span class="hljs-string">&quot;ÁπîÁî∞‰ø°Èï∑„ÅØ„ÄÅÊîøÊ≤ª„ÉªÂ§ñ‰∫§„ÅßÊï∞„ÄÖ„ÅÆÊà¶Êûú„Çí‰∏ä„Åí„Çã„Åå„ÄÅ1568Âπ¥„Åã„Çâ„ÅØ„ÄÅ„ÅÑ„Çè„ÇÜ„ÇãÊú¨ËÉΩÂØ∫„ÅÆÂ§â„ÅßÁ¥∞Â∑ùÊô¥ÂÖÉ„Å´ÊöóÊÆ∫„Åï„Çå„Çã...&quot;</span>`,wrap:!1}}),{c(){h(t.$$.fragment)},l(a){f(t.$$.fragment,a)},m(a,d){g(t,a,d),u=!0},p:Z,i(a){u||(T(t.$$.fragment,a),u=!0)},o(a){_(t.$$.fragment,a),u=!1},d(a){M(t,a)}}}function Wn(C){let t,u;return t=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUyQyUyMEF1dG9Ub2tlbml6ZXIlMkMlMjB0cmFpbmVyX3V0aWxzJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJUYW5yZWklMkZHUFRTQU4tamFwYW5lc2UlMjIpLnRvKGRldmljZSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJUYW5yZWklMkZHUFRTQU4tamFwYW5lc2UlMjIpJTBBbWFza2VkX3NlbnRlbmNlJTIwJTNEJTIwJTIyJUU2JUFEJUE2JUU3JTk0JUIwJUU0JUJGJUExJUU3JThFJTg0JUUzJTgxJUFGJUUzJTgwJTgxJTNDJTdDaW5wdXRtYXNrJTdDJTNFJUU2JTk5JTgyJUU0JUJCJUEzJUUzJTgzJTk1JUUzJTgyJUExJUUzJTgzJUIzJUUzJTgxJUFBJUUzJTgyJTg5JUUzJTgxJTlDJUUzJTgxJUIyJUU2JThBJUJDJUUzJTgxJTk1JUUzJTgxJTg4JTNDJTdDaW5wdXRtYXNrJTdDJTNFJUUzJTgxJThEJUUzJTgxJTlGJUUzJTgxJTg0JUU1JTkwJThEJUU1JUIwJTg2JUUzJTgxJUFFJUU0JUI4JTgwJUU0JUJBJUJBJUUzJTgwJTgyJTIyJTBBeF90b2tlbiUyMCUzRCUyMHRva2VuaXplciglMjIlMjIlMkMlMjBwcmVmaXhfdGV4dCUzRG1hc2tlZF9zZW50ZW5jZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBdHJhaW5lcl91dGlscy5zZXRfc2VlZCgzMCklMEFpbnB1dF9pZHMlMjAlM0QlMjB4X3Rva2VuLmlucHV0X2lkcy50byhkZXZpY2UpJTBBdG9rZW5fdHlwZV9pZHMlMjAlM0QlMjB4X3Rva2VuLnRva2VuX3R5cGVfaWRzLnRvKGRldmljZSklMEFvdXRfbG1fdG9rZW4lMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZShpbnB1dF9pZHMlMkMlMjB0b2tlbl90eXBlX2lkcyUzRHRva2VuX3R5cGVfaWRzJTJDJTIwbWF4X25ld190b2tlbnMlM0Q1MCklMEFvdXRfbWxtX3Rva2VuJTIwJTNEJTIwbW9kZWwoaW5wdXRfaWRzJTJDJTIwdG9rZW5fdHlwZV9pZHMlM0R0b2tlbl90eXBlX2lkcykubG9naXRzLmFyZ21heChheGlzJTNELTEpJTBBdG9rZW5pemVyLmRlY29kZShvdXRfbWxtX3Rva2VuJTVCMCU1RCklMEElMEF0b2tlbml6ZXIuZGVjb2RlKG91dF9sbV90b2tlbiU1QjAlNUQlNUJpbnB1dF9pZHMuc2hhcGUlNUIxJTVEJTIwJTNBJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer, trainer_utils

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>).to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>masked_sentence = <span class="hljs-string">&quot;Ê≠¶Áî∞‰ø°ÁéÑ„ÅØ„ÄÅ&lt;|inputmask|&gt;ÊôÇ‰ª£„Éï„Ç°„É≥„Å™„Çâ„Åú„Å≤Êäº„Åï„Åà&lt;|inputmask|&gt;„Åç„Åü„ÅÑÂêçÂ∞Ü„ÅÆ‰∏Ä‰∫∫„ÄÇ&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>x_token = tokenizer(<span class="hljs-string">&quot;&quot;</span>, prefix_text=masked_sentence, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer_utils.set_seed(<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = x_token.input_ids.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>token_type_ids = x_token.token_type_ids.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>out_lm_token = model.generate(input_ids, token_type_ids=token_type_ids, max_new_tokens=<span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>out_mlm_token = model(input_ids, token_type_ids=token_type_ids).logits.argmax(axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(out_mlm_token[<span class="hljs-number">0</span>])
<span class="hljs-string">&quot;Ê≠¶Áî∞‰ø°ÁéÑ„ÅØ„ÄÅÊà¶ÂõΩÊôÇ‰ª£„Éï„Ç°„É≥„Å™„Çâ„Åú„Å≤Êäº„Åï„Åà„Å¶„Åä„Åç„Åü„ÅÑÂêçÂ∞Ü„ÅÆ‰∏Ä‰∫∫„ÄÇ&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(out_lm_token[<span class="hljs-number">0</span>][input_ids.shape[<span class="hljs-number">1</span>] :])
<span class="hljs-string">&quot;Ê≠¶Áî∞Ê∞è„ÅÆ‰∏â‰ª£„Å´Ê∏°„Å£„ÅüÊ≠¶Áî∞ÂÆ∂„ÅÆ„Å≤„Å®„Çä\\nÁî≤ÊñêÂ∏Ç„Å´‰Ωè„ÇÄ„ÄÅÊó•Êú¨Âè≤‰∏äÊúÄÂ§ß„ÅÆÊà¶ÂõΩÂ§ßÂêç„ÄÇ...&quot;</span>`,wrap:!1}}),{c(){h(t.$$.fragment)},l(a){f(t.$$.fragment,a)},m(a,d){g(t,a,d),u=!0},p:Z,i(a){u||(T(t.$$.fragment,a),u=!0)},o(a){_(t.$$.fragment,a),u=!1},d(a){M(t,a)}}}function An(C){let t,u,a,d,y,o,b,De,ee,en="The GPTSAN-japanese model was released in the repository by Toshiyuki Sakamoto (tanreinama).",Le,te,tn=`GPTSAN is a Japanese language model using Switch Transformer. It has the same structure as the model introduced as Prefix LM
in the T5 paper, and support both Text Generation and Masked Language Modeling tasks. These basic tasks similarly can
fine-tune for translation or summarization.`,Ye,ne,Ke,se,nn="The <code>generate()</code> method can be used to generate text using GPTSAN-Japanese model.",et,ae,tt,oe,nt,le,sn=`GPTSAN has some unique features. It has a model structure of Prefix-LM. It works as a shifted Masked Language Model for Prefix Input tokens. Un-prefixed inputs behave like normal generative models.
The Spout vector is a GPTSAN specific input. Spout is pre-trained with random inputs, but you can specify a class of text or an arbitrary vector during fine-tuning. This allows you to indicate the tendency of the generated text.
GPTSAN has a sparse Feed Forward based on Switch-Transformer. You can also add other layers and train them partially. See the original GPTSAN repository for details.`,st,re,at,ie,an=`GPTSAN has the structure of the model named Prefix-LM in the <code>T5</code> paper. (The original GPTSAN repository calls it <code>hybrid</code>)
In GPTSAN, the <code>Prefix</code> part of Prefix-LM, that is, the input position that can be referenced by both tokens, can be specified with any length.
Arbitrary lengths can also be specified differently for each batch.
This length applies to the text entered in <code>prefix_text</code> for the tokenizer.
The tokenizer returns the mask of the <code>Prefix</code> part of Prefix-LM as <code>token_type_ids</code>.
The model treats the part where <code>token_type_ids</code> is 1 as a <code>Prefix</code> part, that is, the input can refer to both tokens before and after.`,ot,pe,lt,de,on=`Specifying the Prefix part is done with a mask passed to self-attention.
When token_type_ids=None or all zero, it is equivalent to regular causal mask`,rt,ce,ln="for example:",it,me,rn=`<blockquote><blockquote><p>x_token = tokenizer(‚ÄúÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥‚Äù)
input_ids:      | SOT | SEG | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ |
token_type_ids: | 1   | 0   | 0 | 0 | 0 | 0 |
prefix_lm_mask:
SOT | 1 0 0 0 0 0 |
SEG | 1 1 0 0 0 0 |
ÔΩ±   | 1 1 1 0 0 0 |
ÔΩ≤   | 1 1 1 1 0 0 |
ÔΩ≥   | 1 1 1 1 1 0 |
ÔΩ¥   | 1 1 1 1 1 1 |</p></blockquote></blockquote>`,pt,ue,pn=`<blockquote><blockquote><p>x_token = tokenizer(&quot;&quot;, prefix_text=‚ÄúÔΩ±ÔΩ≤ÔΩ≥ÔΩ¥‚Äù)
input_ids:      | SOT | ÔΩ± | ÔΩ≤ | ÔΩ≥ | ÔΩ¥ | SEG |
token_type_ids: | 1   | 1 | 1 | 1 | 1 | 0  |
prefix_lm_mask:
SOT | 1 1 1 1 1 0 |
ÔΩ±   | 1 1 1 1 1 0 |
ÔΩ≤   | 1 1 1 1 1 0 |
ÔΩ≥   | 1 1 1 1 1 0 |
ÔΩ¥   | 1 1 1 1 1 0 |
SEG | 1 1 1 1 1 1 |</p></blockquote></blockquote>`,dt,he,dn=`<blockquote><blockquote><p>x_token = tokenizer(‚ÄúÔΩ≥ÔΩ¥‚Äù, prefix_text=‚ÄúÔΩ±ÔΩ≤‚Äù)
input_ids:      | SOT | ÔΩ± | ÔΩ≤ | SEG | ÔΩ≥ | ÔΩ¥ |
token_type_ids: | 1   | 1 | 1 | 0   | 0 | 0 |
prefix_lm_mask:
SOT | 1 1 1 0 0 0 |
ÔΩ±   | 1 1 1 0 0 0 |
ÔΩ≤   | 1 1 1 0 0 0 |
SEG | 1 1 1 1 0 0 |
ÔΩ≥   | 1 1 1 1 1 0 |
ÔΩ¥   | 1 1 1 1 1 1 |</p></blockquote></blockquote>`,ct,fe,mt,ge,cn=`A Spout Vector is a special vector for controlling text generation.
This vector is treated as the first embedding in self-attention to bring extraneous attention to the generated tokens.
In the pre-trained model published from <code>Tanrei/GPTSAN-japanese</code>, the Spout Vector is a 128-dimensional vector that passes through 8 fully connected layers in the model and is projected into the space acting as external attention.
The Spout Vector projected by the fully connected layer is split to be passed to all self-attentions.`,ut,Te,ht,G,_e,bt,Ve,mn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel">GPTSanJapaneseModel</a>. It is used to instantiate
a GPTSANJapanese model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the GPTSANJapanese
<a href="https://huggingface.co/Tanrei/GPTSAN-japanese" rel="nofollow">Tanrei/GPTSAN-japanese</a> architecture.`,kt,Ge,un=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ft,Me,gt,j,Je,jt,Se,hn="This tokenizer is based on GPTNeoXJapaneseTokenizer and has the following modifications",Ct,Re,fn=`<li>Decoding byte0~byte255 tokens correctly</li> <li>Added bagofword token handling</li> <li>Return token_type_ids for Prefix-LM model
The bagofword token represents a repetition of the previous token and is converted to 3 consecutive tokens when
decoding In addition, the original Japanese special Sub-Word-Encoding has been released in this repository
(<a href="https://github.com/tanreinama/Japanese-BPEEncoder_V2" rel="nofollow">https://github.com/tanreinama/Japanese-BPEEncoder_V2</a>). The token_type_ids is a mask indicating the prefix input
position of the Prefix-LM model. To specify a prefix position, specify a prefix input for prefix_text, or specify a
sentence of the prefix part and the part after it as a text pair of batch input.</li>`,xt,q,$t,E,wt,B,vt,W,ye,Vt,Fe,gn="Converts a sequence of tokens (string) in a single string.",Gt,R,Ue,St,Ne,Tn=`The tokenizer returns token_type_ids as separators between the Prefix part and the rest.
token_type_ids is 1 for the Prefix part and 0 for the rest of the token.`,Rt,A,Tt,be,_t,$,ke,Ft,Pe,_n="The bare GPTSAN-japanese Model transformer outputting raw hidden-states without any specific head on top.",Nt,Ie,Mn=`The <a href="https://github.com/tanreinama/GPTSAN" rel="nofollow">GPTSAN-japanese</a> model was proposed in General-purpose Swich transformer
based Japanese language model`,Pt,Qe,Jn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,It,F,je,Qt,ze,yn='The <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel">GPTSanJapaneseModel</a> forward method, overrides the <code>__call__</code> special method.',zt,O,Mt,Ce,Jt,w,xe,Zt,Ze,Un="The bare GPTSAN-japanese Model with a language modeling head.",qt,qe,bn=`The <a href="https://github.com/tanreinama/GPTSAN" rel="nofollow">GPTSAN-japanese</a> model was proposed in General-purpose Swich transformer
based Japanese language model`,Et,Ee,kn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Bt,U,$e,Wt,Be,jn='The <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',At,X,Ot,We,Cn="Example:",Xt,Ae,xn="Text Generation with regular LM Model",Ht,H,Dt,Oe,$n="Text Generation with Prefix-LM Model",Lt,D,Yt,Xe,wn="Simultaneously Text Generation And Masked Language Model",Kt,L,yt,He,Ut;return y=new S({props:{title:"GPTSAN-japanese",local:"gptsan-japanese",headingTag:"h1"}}),b=new S({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ne=new S({props:{title:"Usage example",local:"usage-example",headingTag:"h3"}}),ae=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUyQyUyMEF1dG9Ub2tlbml6ZXIlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMlRhbnJlaSUyRkdQVFNBTi1qYXBhbmVzZSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyVGFucmVpJTJGR1BUU0FOLWphcGFuZXNlJTIyKS5jdWRhKCklMEF4X3RvayUyMCUzRCUyMHRva2VuaXplciglMjIlRTMlODElQUYlRTMlODAlODElMjIlMkMlMjBwcmVmaXhfdGV4dCUzRCUyMiVFNyVCOSU5NCVFNyU5NCVCMCVFNCVCRiVBMSVFOSU5NSVCNyUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBdG9yY2gubWFudWFsX3NlZWQoMCklMEFnZW5fdG9rJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoeF90b2suaW5wdXRfaWRzLmN1ZGEoKSUyQyUyMHRva2VuX3R5cGVfaWRzJTNEeF90b2sudG9rZW5fdHlwZV9pZHMuY3VkYSgpJTJDJTIwbWF4X25ld190b2tlbnMlM0QyMCklMEF0b2tlbml6ZXIuZGVjb2RlKGdlbl90b2slNUIwJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;Tanrei/GPTSAN-japanese&quot;</span>).cuda()
<span class="hljs-meta">&gt;&gt;&gt; </span>x_tok = tokenizer(<span class="hljs-string">&quot;„ÅØ„ÄÅ&quot;</span>, prefix_text=<span class="hljs-string">&quot;ÁπîÁî∞‰ø°Èï∑&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">0</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_tok = model.generate(x_tok.input_ids.cuda(), token_type_ids=x_tok.token_type_ids.cuda(), max_new_tokens=<span class="hljs-number">20</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(gen_tok[<span class="hljs-number">0</span>])
<span class="hljs-string">&#x27;ÁπîÁî∞‰ø°Èï∑„ÅØ„ÄÅ2004Âπ¥„Å´„ÄéÊà¶ÂõΩBASARA„Äè„ÅÆ„Åü„ÇÅ„Å´„ÄÅË±äËá£ÁßÄÂêâ&#x27;</span>`,wrap:!1}}),oe=new S({props:{title:"GPTSAN Features",local:"gptsan-features",headingTag:"h2"}}),re=new S({props:{title:"Prefix-LM Model",local:"prefix-lm-model",headingTag:"h3"}}),pe=new S({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),fe=new S({props:{title:"Spout Vector",local:"spout-vector",headingTag:"h3"}}),Te=new S({props:{title:"GPTSanJapaneseConfig",local:"transformers.GPTSanJapaneseConfig",headingTag:"h2"}}),_e=new Y({props:{name:"class transformers.GPTSanJapaneseConfig",anchor:"transformers.GPTSanJapaneseConfig",parameters:[{name:"vocab_size",val:" = 36000"},{name:"max_position_embeddings",val:" = 1280"},{name:"d_model",val:" = 1024"},{name:"d_ff",val:" = 8192"},{name:"d_ext",val:" = 4096"},{name:"d_spout",val:" = 128"},{name:"num_switch_layers",val:" = 10"},{name:"num_ext_layers",val:" = 0"},{name:"num_heads",val:" = 16"},{name:"num_experts",val:" = 16"},{name:"expert_capacity",val:" = 128"},{name:"dropout_rate",val:" = 0.0"},{name:"layer_norm_epsilon",val:" = 1e-05"},{name:"router_bias",val:" = False"},{name:"router_jitter_noise",val:" = 0.0"},{name:"router_dtype",val:" = 'float32'"},{name:"router_ignore_padding_tokens",val:" = False"},{name:"output_hidden_states",val:" = False"},{name:"output_attentions",val:" = False"},{name:"initializer_factor",val:" = 0.002"},{name:"output_router_logits",val:" = False"},{name:"use_cache",val:" = True"},{name:"separator_token_id",val:" = 35998"},{name:"pad_token_id",val:" = 35995"},{name:"eos_token_id",val:" = 35999"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTSanJapaneseConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 36000) &#x2014;
Vocabulary size of the GPTSANJapanese model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseModel">GPTSanJapaneseModel</a>.`,name:"vocab_size"},{anchor:"transformers.GPTSanJapaneseConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 1280) &#x2014;
The maximum sequence length that this model might ever be used with. Defaults set this to 1280.`,name:"max_position_embeddings"},{anchor:"transformers.GPTSanJapaneseConfig.d_model",description:`<strong>d_model</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Size of the encoder layers and the pooler layer.`,name:"d_model"},{anchor:"transformers.GPTSanJapaneseConfig.d_ff",description:`<strong>d_ff</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Size of the intermediate feed forward layer in each <code>SwitchTransformersBlock</code>.`,name:"d_ff"},{anchor:"transformers.GPTSanJapaneseConfig.d_ext",description:`<strong>d_ext</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Size of the intermediate feed forward layer in each Extra-layers.`,name:"d_ext"},{anchor:"transformers.GPTSanJapaneseConfig.d_spout",description:`<strong>d_spout</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Size of the <code>spout</code> vector.`,name:"d_spout"},{anchor:"transformers.GPTSanJapaneseConfig.num_switch_layers",description:`<strong>num_switch_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Number of layers in the Switch Transformer layer.`,name:"num_switch_layers"},{anchor:"transformers.GPTSanJapaneseConfig.num_ext_layers",description:`<strong>num_ext_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Number of layers in the Extra-layers.`,name:"num_ext_layers"},{anchor:"transformers.GPTSanJapaneseConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.GPTSanJapaneseConfig.num_experts",description:`<strong>num_experts</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of experts for each SwitchTransformer layer.`,name:"num_experts"},{anchor:"transformers.GPTSanJapaneseConfig.expert_capacity",description:`<strong>expert_capacity</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Number of tokens that can be stored in each expert. If set to 1, the model will behave like a regular
Transformer.`,name:"expert_capacity"},{anchor:"transformers.GPTSanJapaneseConfig.dropout_rate",description:`<strong>dropout_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The ratio for all dropout layers.`,name:"dropout_rate"},{anchor:"transformers.GPTSanJapaneseConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.GPTSanJapaneseConfig.router_bias",description:`<strong>router_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to add a bias to the router.`,name:"router_bias"},{anchor:"transformers.GPTSanJapaneseConfig.router_jitter_noise",description:`<strong>router_jitter_noise</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Amount of noise to add to the router. Set it to 0.0 during prediction or set small value (usually 1e-2)
during training.`,name:"router_jitter_noise"},{anchor:"transformers.GPTSanJapaneseConfig.router_dtype",description:`<strong>router_dtype</strong> (<code>str</code>, <em>optional</em>, default to <code>&quot;float32&quot;</code>) &#x2014;
The <code>dtype</code> used for the routers. It is preferable to keep the <code>dtype</code> to <code>&quot;float32&quot;</code> as specified in the
<em>selective precision</em> discussion in <a href="https://arxiv.org/abs/2101.03961" rel="nofollow">the paper</a>.`,name:"router_dtype"},{anchor:"transformers.GPTSanJapaneseConfig.router_ignore_padding_tokens",description:`<strong>router_ignore_padding_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to ignore padding tokens when routing.`,name:"router_ignore_padding_tokens"},{anchor:"transformers.GPTSanJapaneseConfig.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>, default to <code>False</code>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTSanJapaneseConfig.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to return the attentions tensors of all attention layers.`,name:"output_attentions"},{anchor:"transformers.GPTSanJapaneseConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 0.002) &#x2014;
A factor for initializing all weight matrices.`,name:"initializer_factor"},{anchor:"transformers.GPTSanJapaneseConfig.output_router_logits",description:`<strong>output_router_logits</strong> (<code>bool</code>, <em>optional</em>, default to <code>False</code>) &#x2014;
Whether or not to return the router logits of all experts.`,name:"output_router_logits"},{anchor:"transformers.GPTSanJapaneseConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models)`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gptsan_japanese/configuration_gptsan_japanese.py#L29"}}),Me=new S({props:{title:"GPTSanJapaneseTokenizer",local:"transformers.GPTSanJapaneseTokenizer",headingTag:"h2"}}),Je=new Y({props:{name:"class transformers.GPTSanJapaneseTokenizer",anchor:"transformers.GPTSanJapaneseTokenizer",parameters:[{name:"vocab_file",val:""},{name:"emoji_file",val:""},{name:"unk_token",val:" = '<|nottoken|>'"},{name:"pad_token",val:" = '<|separator|>'"},{name:"bos_token",val:" = '<|startoftext|>'"},{name:"eos_token",val:" = '<|endoftext|>'"},{name:"sep_token",val:" = '<|segmenter|>'"},{name:"do_clean_text",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTSanJapaneseTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.GPTSanJapaneseTokenizer.emoji_file",description:`<strong>emoji_file</strong> (<code>str</code>) &#x2014;
File containing the emoji.`,name:"emoji_file"},{anchor:"transformers.GPTSanJapaneseTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|nottoken|&gt;&quot;</code>) &#x2014;
The token used for unknown charactor`,name:"unk_token"},{anchor:"transformers.GPTSanJapaneseTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|separator|&gt;&quot;</code>) &#x2014;
The token used for padding`,name:"pad_token"},{anchor:"transformers.GPTSanJapaneseTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|startoftext|&gt;&quot;</code>) &#x2014;
The beginning of sequence token.`,name:"bos_token"},{anchor:"transformers.GPTSanJapaneseTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.GPTSanJapaneseTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|segmenter|&gt;&quot;</code>) &#x2014;
A special token to separate token to prefix part and general input part.`,name:"sep_token"},{anchor:"transformers.GPTSanJapaneseTokenizer.do_clean_text",description:`<strong>do_clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to clean text for URL, EMAIL, TEL, Japanese DATE and Japanese PRICE.`,name:"do_clean_text"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L74"}}),q=new ve({props:{anchor:"transformers.GPTSanJapaneseTokenizer.example",$$slots:{default:[Pn]},$$scope:{ctx:C}}}),E=new ve({props:{anchor:"transformers.GPTSanJapaneseTokenizer.example-2",$$slots:{default:[In]},$$scope:{ctx:C}}}),B=new ve({props:{anchor:"transformers.GPTSanJapaneseTokenizer.example-3",$$slots:{default:[Qn]},$$scope:{ctx:C}}}),ye=new Y({props:{name:"convert_tokens_to_string",anchor:"transformers.GPTSanJapaneseTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L219"}}),Ue=new Y({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.GPTSanJapaneseTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gptsan_japanese/tokenization_gptsan_japanese.py#L308"}}),A=new ve({props:{anchor:"transformers.GPTSanJapaneseTokenizer.create_token_type_ids_from_sequences.example",$$slots:{default:[zn]},$$scope:{ctx:C}}}),be=new S({props:{title:"GPTSanJapaneseModel",local:"transformers.GPTSanJapaneseModel",headingTag:"h2"}}),ke=new Y({props:{name:"class transformers.GPTSanJapaneseModel",anchor:"transformers.GPTSanJapaneseModel",parameters:[{name:"config",val:": GPTSanJapaneseConfig"}],parametersDescription:[{anchor:"transformers.GPTSanJapaneseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig">GPTSanJapaneseConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L855"}}),je=new Y({props:{name:"forward",anchor:"transformers.GPTSanJapaneseModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"spout",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"use_cache",val:": Optional = False"},{name:"inputs_embeds",val:": Optional = None"},{name:"decoder_inputs_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"},{name:"output_router_logits",val:": Optional = None"},{name:"num_precontext",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.GPTSanJapaneseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. GPTSAN-japanese is a model that generates sentence
continuations or predicts tokens at mask positions. Special tokens required for inputs to the model are
automatically appended.`,name:"input_ids"},{anchor:"transformers.GPTSanJapaneseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTSanJapaneseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
An input that masks the Prefix part in the Prefix-LM input. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>prefix</strong> input,</li>
<li>0 for tokens that are <strong>not-prefix</strong> input.</li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.GPTSanJapaneseModel.forward.spout",description:`<strong>spout</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, config.d_spout)</code>) &#x2014;
This vector is transformed through an 8-layer FFN and can be used instead of <code>past_key_values</code>.`,name:"spout"},{anchor:"transformers.GPTSanJapaneseModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GPTSanJapaneseModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:`,name:"head_mask"},{anchor:"transformers.GPTSanJapaneseModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTSanJapaneseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GPTSanJapaneseModel.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. If <code>past_key_values</code> is used, optionally only the last <code>decoder_inputs_embeds</code> have to be
input (see <code>past_key_values</code>). This is useful if you want more control over how to convert
<code>decoder_input_ids</code> indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.GPTSanJapaneseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTSanJapaneseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTSanJapaneseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTSanJapaneseModel.forward.router_logits",description:`<strong>router_logits</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_router_logits=True</code> is passed or when <code>config.add_router_probs=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, sequence_length, num_experts)</code>.
Router logits of the decoder model, useful to compute the auxiliary loss for Mixture of Experts models.`,name:"router_logits"},{anchor:"transformers.GPTSanJapaneseModel.forward.num_precontext",description:`<strong>num_precontext</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,1)</code>) &#x2014;
length of <code>hybrid</code> input tokens in the input. Tokens up to this length refer to both front and back like
BERT, tokens after that refer only to front like GPT. see also:
<a href="https://github.com/tanreinama/GPTSAN/blob/main/report/model.md" rel="nofollow">https://github.com/tanreinama/GPTSAN/blob/main/report/model.md</a>`,name:"num_precontext"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L893",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>MoEModelOutputWithPastAndCrossAttentions</code> or <code>tuple</code> if <code>return_dict</code> returns
MoEModelOutputWithPastAndCrossAttentions insted of tuple</p>
`}}),O=new Vn({props:{$$slots:{default:[Zn]},$$scope:{ctx:C}}}),Ce=new S({props:{title:"GPTSanJapaneseForConditionalGeneration",local:"transformers.GPTSanJapaneseForConditionalGeneration",headingTag:"h2"}}),xe=new Y({props:{name:"class transformers.GPTSanJapaneseForConditionalGeneration",anchor:"transformers.GPTSanJapaneseForConditionalGeneration",parameters:[{name:"config",val:": GPTSanJapaneseConfig"}],parametersDescription:[{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig">GPTSanJapaneseConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1100"}}),$e=new Y({props:{name:"forward",anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"spout",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"use_cache",val:": Optional = False"},{name:"inputs_embeds",val:": Optional = None"},{name:"decoder_inputs_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"},{name:"output_router_logits",val:": Optional = None"},{name:"labels",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. GPTSAN-japanese is a model that generates sentence
continuations or predicts tokens at mask positions. Special tokens required for inputs to the model are
automatically appended.`,name:"input_ids"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
An input that masks the Prefix part in the Prefix-LM input. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>prefix</strong> input,</li>
<li>0 for tokens that are <strong>not-prefix</strong> input.</li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.spout",description:`<strong>spout</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, config.d_spout)</code>) &#x2014;
This vector is transformed through an 8-layer FFN and can be used instead of <code>past_key_values</code>.`,name:"spout"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:`,name:"head_mask"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.decoder_inputs_embeds",description:`<strong>decoder_inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, target_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>decoder_input_ids</code> you can choose to directly pass an embedded
representation. If <code>past_key_values</code> is used, optionally only the last <code>decoder_inputs_embeds</code> have to be
input (see <code>past_key_values</code>). This is useful if you want more control over how to convert
<code>decoder_input_ids</code> indices into associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"decoder_inputs_embeds"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.router_logits",description:`<strong>router_logits</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_router_logits=True</code> is passed or when <code>config.add_router_probs=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, sequence_length, num_experts)</code>.
Router logits of the decoder model, useful to compute the auxiliary loss for Mixture of Experts models.`,name:"router_logits"},{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification loss. Indices should be in <code>[-100, 0, ..., config.vocab_size - 1]</code>. All labels set to <code>-100</code> are ignored (masked), the loss is only computed for
labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py#L1115",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>MoECausalLMOutputWithPast</code> or <code>tuple</code> if <code>return_dict</code> returns MoECausalLMOutputWithPast insted of tuple</p>
`}}),X=new Vn({props:{$$slots:{default:[qn]},$$scope:{ctx:C}}}),H=new ve({props:{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.example",$$slots:{default:[En]},$$scope:{ctx:C}}}),D=new ve({props:{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.example-2",$$slots:{default:[Bn]},$$scope:{ctx:C}}}),L=new ve({props:{anchor:"transformers.GPTSanJapaneseForConditionalGeneration.forward.example-3",$$slots:{default:[Wn]},$$scope:{ctx:C}}}),{c(){t=c("meta"),u=l(),a=c("p"),d=l(),h(y.$$.fragment),o=l(),h(b.$$.fragment),De=l(),ee=c("p"),ee.textContent=en,Le=l(),te=c("p"),te.textContent=tn,Ye=l(),h(ne.$$.fragment),Ke=l(),se=c("p"),se.innerHTML=nn,et=l(),h(ae.$$.fragment),tt=l(),h(oe.$$.fragment),nt=l(),le=c("p"),le.textContent=sn,st=l(),h(re.$$.fragment),at=l(),ie=c("p"),ie.innerHTML=an,ot=l(),h(pe.$$.fragment),lt=l(),de=c("p"),de.textContent=on,rt=l(),ce=c("p"),ce.textContent=ln,it=l(),me=c("blockquote"),me.innerHTML=rn,pt=l(),ue=c("blockquote"),ue.innerHTML=pn,dt=l(),he=c("blockquote"),he.innerHTML=dn,ct=l(),h(fe.$$.fragment),mt=l(),ge=c("p"),ge.innerHTML=cn,ut=l(),h(Te.$$.fragment),ht=l(),G=c("div"),h(_e.$$.fragment),bt=l(),Ve=c("p"),Ve.innerHTML=mn,kt=l(),Ge=c("p"),Ge.innerHTML=un,ft=l(),h(Me.$$.fragment),gt=l(),j=c("div"),h(Je.$$.fragment),jt=l(),Se=c("p"),Se.textContent=hn,Ct=l(),Re=c("ul"),Re.innerHTML=fn,xt=l(),h(q.$$.fragment),$t=l(),h(E.$$.fragment),wt=l(),h(B.$$.fragment),vt=l(),W=c("div"),h(ye.$$.fragment),Vt=l(),Fe=c("p"),Fe.textContent=gn,Gt=l(),R=c("div"),h(Ue.$$.fragment),St=l(),Ne=c("p"),Ne.textContent=Tn,Rt=l(),h(A.$$.fragment),Tt=l(),h(be.$$.fragment),_t=l(),$=c("div"),h(ke.$$.fragment),Ft=l(),Pe=c("p"),Pe.textContent=_n,Nt=l(),Ie=c("p"),Ie.innerHTML=Mn,Pt=l(),Qe=c("p"),Qe.innerHTML=Jn,It=l(),F=c("div"),h(je.$$.fragment),Qt=l(),ze=c("p"),ze.innerHTML=yn,zt=l(),h(O.$$.fragment),Mt=l(),h(Ce.$$.fragment),Jt=l(),w=c("div"),h(xe.$$.fragment),Zt=l(),Ze=c("p"),Ze.textContent=Un,qt=l(),qe=c("p"),qe.innerHTML=bn,Et=l(),Ee=c("p"),Ee.innerHTML=kn,Bt=l(),U=c("div"),h($e.$$.fragment),Wt=l(),Be=c("p"),Be.innerHTML=jn,At=l(),h(X.$$.fragment),Ot=l(),We=c("p"),We.textContent=Cn,Xt=l(),Ae=c("p"),Ae.textContent=xn,Ht=l(),h(H.$$.fragment),Dt=l(),Oe=c("p"),Oe.textContent=$n,Lt=l(),h(D.$$.fragment),Yt=l(),Xe=c("p"),Xe.textContent=wn,Kt=l(),h(L.$$.fragment),yt=l(),He=c("p"),this.h()},l(e){const n=Nn("svelte-u9bgzb",document.head);t=m(n,"META",{name:!0,content:!0}),n.forEach(s),u=r(e),a=m(e,"P",{}),N(a).forEach(s),d=r(e),f(y.$$.fragment,e),o=r(e),f(b.$$.fragment,e),De=r(e),ee=m(e,"P",{"data-svelte-h":!0}),J(ee)!=="svelte-tknax7"&&(ee.textContent=en),Le=r(e),te=m(e,"P",{"data-svelte-h":!0}),J(te)!=="svelte-ydpvl8"&&(te.textContent=tn),Ye=r(e),f(ne.$$.fragment,e),Ke=r(e),se=m(e,"P",{"data-svelte-h":!0}),J(se)!=="svelte-1hxg3na"&&(se.innerHTML=nn),et=r(e),f(ae.$$.fragment,e),tt=r(e),f(oe.$$.fragment,e),nt=r(e),le=m(e,"P",{"data-svelte-h":!0}),J(le)!=="svelte-ertpqo"&&(le.textContent=sn),st=r(e),f(re.$$.fragment,e),at=r(e),ie=m(e,"P",{"data-svelte-h":!0}),J(ie)!=="svelte-1rqeshc"&&(ie.innerHTML=an),ot=r(e),f(pe.$$.fragment,e),lt=r(e),de=m(e,"P",{"data-svelte-h":!0}),J(de)!=="svelte-1xag9tb"&&(de.textContent=on),rt=r(e),ce=m(e,"P",{"data-svelte-h":!0}),J(ce)!=="svelte-1gl9mvz"&&(ce.textContent=ln),it=r(e),me=m(e,"BLOCKQUOTE",{"data-svelte-h":!0}),J(me)!=="svelte-4r9qjo"&&(me.innerHTML=rn),pt=r(e),ue=m(e,"BLOCKQUOTE",{"data-svelte-h":!0}),J(ue)!=="svelte-1qp1s1n"&&(ue.innerHTML=pn),dt=r(e),he=m(e,"BLOCKQUOTE",{"data-svelte-h":!0}),J(he)!=="svelte-1use4j"&&(he.innerHTML=dn),ct=r(e),f(fe.$$.fragment,e),mt=r(e),ge=m(e,"P",{"data-svelte-h":!0}),J(ge)!=="svelte-4kiza2"&&(ge.innerHTML=cn),ut=r(e),f(Te.$$.fragment,e),ht=r(e),G=m(e,"DIV",{class:!0});var I=N(G);f(_e.$$.fragment,I),bt=r(I),Ve=m(I,"P",{"data-svelte-h":!0}),J(Ve)!=="svelte-1xaa77o"&&(Ve.innerHTML=mn),kt=r(I),Ge=m(I,"P",{"data-svelte-h":!0}),J(Ge)!=="svelte-o55m63"&&(Ge.innerHTML=un),I.forEach(s),ft=r(e),f(Me.$$.fragment,e),gt=r(e),j=m(e,"DIV",{class:!0});var x=N(j);f(Je.$$.fragment,x),jt=r(x),Se=m(x,"P",{"data-svelte-h":!0}),J(Se)!=="svelte-1pkozbi"&&(Se.textContent=hn),Ct=r(x),Re=m(x,"UL",{"data-svelte-h":!0}),J(Re)!=="svelte-12rov6i"&&(Re.innerHTML=fn),xt=r(x),f(q.$$.fragment,x),$t=r(x),f(E.$$.fragment,x),wt=r(x),f(B.$$.fragment,x),vt=r(x),W=m(x,"DIV",{class:!0});var we=N(W);f(ye.$$.fragment,we),Vt=r(we),Fe=m(we,"P",{"data-svelte-h":!0}),J(Fe)!=="svelte-b3k2yi"&&(Fe.textContent=gn),we.forEach(s),Gt=r(x),R=m(x,"DIV",{class:!0});var Q=N(R);f(Ue.$$.fragment,Q),St=r(Q),Ne=m(Q,"P",{"data-svelte-h":!0}),J(Ne)!=="svelte-btxjfp"&&(Ne.textContent=Tn),Rt=r(Q),f(A.$$.fragment,Q),Q.forEach(s),x.forEach(s),Tt=r(e),f(be.$$.fragment,e),_t=r(e),$=m(e,"DIV",{class:!0});var v=N($);f(ke.$$.fragment,v),Ft=r(v),Pe=m(v,"P",{"data-svelte-h":!0}),J(Pe)!=="svelte-1jq56ze"&&(Pe.textContent=_n),Nt=r(v),Ie=m(v,"P",{"data-svelte-h":!0}),J(Ie)!=="svelte-1sm4xoj"&&(Ie.innerHTML=Mn),Pt=r(v),Qe=m(v,"P",{"data-svelte-h":!0}),J(Qe)!=="svelte-hswkmf"&&(Qe.innerHTML=Jn),It=r(v),F=m(v,"DIV",{class:!0});var z=N(F);f(je.$$.fragment,z),Qt=r(z),ze=m(z,"P",{"data-svelte-h":!0}),J(ze)!=="svelte-19ya390"&&(ze.innerHTML=yn),zt=r(z),f(O.$$.fragment,z),z.forEach(s),v.forEach(s),Mt=r(e),f(Ce.$$.fragment,e),Jt=r(e),w=m(e,"DIV",{class:!0});var V=N(w);f(xe.$$.fragment,V),Zt=r(V),Ze=m(V,"P",{"data-svelte-h":!0}),J(Ze)!=="svelte-mgq95n"&&(Ze.textContent=Un),qt=r(V),qe=m(V,"P",{"data-svelte-h":!0}),J(qe)!=="svelte-1sm4xoj"&&(qe.innerHTML=bn),Et=r(V),Ee=m(V,"P",{"data-svelte-h":!0}),J(Ee)!=="svelte-hswkmf"&&(Ee.innerHTML=kn),Bt=r(V),U=m(V,"DIV",{class:!0});var k=N(U);f($e.$$.fragment,k),Wt=r(k),Be=m(k,"P",{"data-svelte-h":!0}),J(Be)!=="svelte-1d362me"&&(Be.innerHTML=jn),At=r(k),f(X.$$.fragment,k),Ot=r(k),We=m(k,"P",{"data-svelte-h":!0}),J(We)!=="svelte-11lpom8"&&(We.textContent=Cn),Xt=r(k),Ae=m(k,"P",{"data-svelte-h":!0}),J(Ae)!=="svelte-14d7hu9"&&(Ae.textContent=xn),Ht=r(k),f(H.$$.fragment,k),Dt=r(k),Oe=m(k,"P",{"data-svelte-h":!0}),J(Oe)!=="svelte-11j4hh0"&&(Oe.textContent=$n),Lt=r(k),f(D.$$.fragment,k),Yt=r(k),Xe=m(k,"P",{"data-svelte-h":!0}),J(Xe)!=="svelte-t76udw"&&(Xe.textContent=wn),Kt=r(k),f(L.$$.fragment,k),k.forEach(s),V.forEach(s),yt=r(e),He=m(e,"P",{}),N(He).forEach(s),this.h()},h(){P(t,"name","hf:doc:metadata"),P(t,"content",On),P(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),P(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){p(document.head,t),i(e,u,n),i(e,a,n),i(e,d,n),g(y,e,n),i(e,o,n),g(b,e,n),i(e,De,n),i(e,ee,n),i(e,Le,n),i(e,te,n),i(e,Ye,n),g(ne,e,n),i(e,Ke,n),i(e,se,n),i(e,et,n),g(ae,e,n),i(e,tt,n),g(oe,e,n),i(e,nt,n),i(e,le,n),i(e,st,n),g(re,e,n),i(e,at,n),i(e,ie,n),i(e,ot,n),g(pe,e,n),i(e,lt,n),i(e,de,n),i(e,rt,n),i(e,ce,n),i(e,it,n),i(e,me,n),i(e,pt,n),i(e,ue,n),i(e,dt,n),i(e,he,n),i(e,ct,n),g(fe,e,n),i(e,mt,n),i(e,ge,n),i(e,ut,n),g(Te,e,n),i(e,ht,n),i(e,G,n),g(_e,G,null),p(G,bt),p(G,Ve),p(G,kt),p(G,Ge),i(e,ft,n),g(Me,e,n),i(e,gt,n),i(e,j,n),g(Je,j,null),p(j,jt),p(j,Se),p(j,Ct),p(j,Re),p(j,xt),g(q,j,null),p(j,$t),g(E,j,null),p(j,wt),g(B,j,null),p(j,vt),p(j,W),g(ye,W,null),p(W,Vt),p(W,Fe),p(j,Gt),p(j,R),g(Ue,R,null),p(R,St),p(R,Ne),p(R,Rt),g(A,R,null),i(e,Tt,n),g(be,e,n),i(e,_t,n),i(e,$,n),g(ke,$,null),p($,Ft),p($,Pe),p($,Nt),p($,Ie),p($,Pt),p($,Qe),p($,It),p($,F),g(je,F,null),p(F,Qt),p(F,ze),p(F,zt),g(O,F,null),i(e,Mt,n),g(Ce,e,n),i(e,Jt,n),i(e,w,n),g(xe,w,null),p(w,Zt),p(w,Ze),p(w,qt),p(w,qe),p(w,Et),p(w,Ee),p(w,Bt),p(w,U),g($e,U,null),p(U,Wt),p(U,Be),p(U,At),g(X,U,null),p(U,Ot),p(U,We),p(U,Xt),p(U,Ae),p(U,Ht),g(H,U,null),p(U,Dt),p(U,Oe),p(U,Lt),g(D,U,null),p(U,Yt),p(U,Xe),p(U,Kt),g(L,U,null),i(e,yt,n),i(e,He,n),Ut=!0},p(e,[n]){const I={};n&2&&(I.$$scope={dirty:n,ctx:e}),q.$set(I);const x={};n&2&&(x.$$scope={dirty:n,ctx:e}),E.$set(x);const we={};n&2&&(we.$$scope={dirty:n,ctx:e}),B.$set(we);const Q={};n&2&&(Q.$$scope={dirty:n,ctx:e}),A.$set(Q);const v={};n&2&&(v.$$scope={dirty:n,ctx:e}),O.$set(v);const z={};n&2&&(z.$$scope={dirty:n,ctx:e}),X.$set(z);const V={};n&2&&(V.$$scope={dirty:n,ctx:e}),H.$set(V);const k={};n&2&&(k.$$scope={dirty:n,ctx:e}),D.$set(k);const vn={};n&2&&(vn.$$scope={dirty:n,ctx:e}),L.$set(vn)},i(e){Ut||(T(y.$$.fragment,e),T(b.$$.fragment,e),T(ne.$$.fragment,e),T(ae.$$.fragment,e),T(oe.$$.fragment,e),T(re.$$.fragment,e),T(pe.$$.fragment,e),T(fe.$$.fragment,e),T(Te.$$.fragment,e),T(_e.$$.fragment,e),T(Me.$$.fragment,e),T(Je.$$.fragment,e),T(q.$$.fragment,e),T(E.$$.fragment,e),T(B.$$.fragment,e),T(ye.$$.fragment,e),T(Ue.$$.fragment,e),T(A.$$.fragment,e),T(be.$$.fragment,e),T(ke.$$.fragment,e),T(je.$$.fragment,e),T(O.$$.fragment,e),T(Ce.$$.fragment,e),T(xe.$$.fragment,e),T($e.$$.fragment,e),T(X.$$.fragment,e),T(H.$$.fragment,e),T(D.$$.fragment,e),T(L.$$.fragment,e),Ut=!0)},o(e){_(y.$$.fragment,e),_(b.$$.fragment,e),_(ne.$$.fragment,e),_(ae.$$.fragment,e),_(oe.$$.fragment,e),_(re.$$.fragment,e),_(pe.$$.fragment,e),_(fe.$$.fragment,e),_(Te.$$.fragment,e),_(_e.$$.fragment,e),_(Me.$$.fragment,e),_(Je.$$.fragment,e),_(q.$$.fragment,e),_(E.$$.fragment,e),_(B.$$.fragment,e),_(ye.$$.fragment,e),_(Ue.$$.fragment,e),_(A.$$.fragment,e),_(be.$$.fragment,e),_(ke.$$.fragment,e),_(je.$$.fragment,e),_(O.$$.fragment,e),_(Ce.$$.fragment,e),_(xe.$$.fragment,e),_($e.$$.fragment,e),_(X.$$.fragment,e),_(H.$$.fragment,e),_(D.$$.fragment,e),_(L.$$.fragment,e),Ut=!1},d(e){e&&(s(u),s(a),s(d),s(o),s(De),s(ee),s(Le),s(te),s(Ye),s(Ke),s(se),s(et),s(tt),s(nt),s(le),s(st),s(at),s(ie),s(ot),s(lt),s(de),s(rt),s(ce),s(it),s(me),s(pt),s(ue),s(dt),s(he),s(ct),s(mt),s(ge),s(ut),s(ht),s(G),s(ft),s(gt),s(j),s(Tt),s(_t),s($),s(Mt),s(Jt),s(w),s(yt),s(He)),s(t),M(y,e),M(b,e),M(ne,e),M(ae,e),M(oe,e),M(re,e),M(pe,e),M(fe,e),M(Te,e),M(_e),M(Me,e),M(Je),M(q),M(E),M(B),M(ye),M(Ue),M(A),M(be,e),M(ke),M(je),M(O),M(Ce,e),M(xe),M($e),M(X),M(H),M(D),M(L)}}}const On='{"title":"GPTSAN-japanese","local":"gptsan-japanese","sections":[{"title":"Overview","local":"overview","sections":[{"title":"Usage example","local":"usage-example","sections":[],"depth":3}],"depth":2},{"title":"GPTSAN Features","local":"gptsan-features","sections":[{"title":"Prefix-LM Model","local":"prefix-lm-model","sections":[],"depth":3}],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[{"title":"Spout Vector","local":"spout-vector","sections":[],"depth":3}],"depth":2},{"title":"GPTSanJapaneseConfig","local":"transformers.GPTSanJapaneseConfig","sections":[],"depth":2},{"title":"GPTSanJapaneseTokenizer","local":"transformers.GPTSanJapaneseTokenizer","sections":[],"depth":2},{"title":"GPTSanJapaneseModel","local":"transformers.GPTSanJapaneseModel","sections":[],"depth":2},{"title":"GPTSanJapaneseForConditionalGeneration","local":"transformers.GPTSanJapaneseForConditionalGeneration","sections":[],"depth":2}],"depth":1}';function Xn(C){return Sn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ns extends Rn{constructor(t){super(),Fn(this,t,Xn,An,Gn,{})}}export{ns as component};
