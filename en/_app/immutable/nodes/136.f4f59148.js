import{s as Tt,o as Nt,n as L}from"../chunks/scheduler.9bc65507.js";import{S as jt,i as Ct,g as m,s as l,r as u,A as xt,h as p,f as s,c as i,j as R,u as g,x as F,k as V,y as d,a as c,v as _,d as b,t as y,w as M}from"../chunks/index.707bf1b6.js";import{T as Re}from"../chunks/Tip.c2ecdbf4.js";import{D as de}from"../chunks/Docstring.17db21ae.js";import{C as Xe}from"../chunks/CodeBlock.54a9f38d.js";import{E as Ve}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as Fe}from"../chunks/Heading.342b1fa6.js";function Ut(v){let t,h="Example:",n,r,f;return r=new Xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEZvY2FsTmV0Q29uZmlnJTJDJTIwRm9jYWxOZXRNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBGb2NhbE5ldCUyMG1pY3Jvc29mdCUyRmZvY2FsbmV0LXRpbnklMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwRm9jYWxOZXRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwbWljcm9zb2Z0JTJGZm9jYWxuZXQtdGlueSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwRm9jYWxOZXRNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FocalNetConfig, FocalNetModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FocalNet microsoft/focalnet-tiny style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FocalNetConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the microsoft/focalnet-tiny style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FocalNetModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=l(),u(r.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),F(t)!=="svelte-11lpom8"&&(t.textContent=h),n=i(o),g(r.$$.fragment,o)},m(o,w){c(o,t,w),c(o,n,w),_(r,o,w),f=!0},p:L,i(o){f||(b(r.$$.fragment,o),f=!0)},o(o){y(r.$$.fragment,o),f=!1},d(o){o&&(s(t),s(n)),M(r,o)}}}function Zt(v){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=h},l(n){t=p(n,"P",{"data-svelte-h":!0}),F(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(n,r){c(n,t,r)},p:L,d(n){n&&s(t)}}}function It(v){let t,h="Example:",n,r,f;return r=new Xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEZvY2FsTmV0TW9kZWwlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmZvY2FsbmV0LXRpbnklMjIpJTBBbW9kZWwlMjAlM0QlMjBGb2NhbE5ldE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZmb2NhbG5ldC10aW55JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, FocalNetModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/focalnet-tiny&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FocalNetModel.from_pretrained(<span class="hljs-string">&quot;microsoft/focalnet-tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">49</span>, <span class="hljs-number">768</span>]`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=l(),u(r.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),F(t)!=="svelte-11lpom8"&&(t.textContent=h),n=i(o),g(r.$$.fragment,o)},m(o,w){c(o,t,w),c(o,n,w),_(r,o,w),f=!0},p:L,i(o){f||(b(r.$$.fragment,o),f=!0)},o(o){y(r.$$.fragment,o),f=!1},d(o){o&&(s(t),s(n)),M(r,o)}}}function kt(v){let t,h=`Note that we provide a script to pre-train this model on custom data in our <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining" rel="nofollow">examples
directory</a>.`;return{c(){t=m("p"),t.innerHTML=h},l(n){t=p(n,"P",{"data-svelte-h":!0}),F(t)!=="svelte-7i3y9o"&&(t.innerHTML=h)},m(n,r){c(n,t,r)},p:L,d(n){n&&s(t)}}}function Wt(v){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=h},l(n){t=p(n,"P",{"data-svelte-h":!0}),F(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(n,r){c(n,t,r)},p:L,d(n){n&&s(t)}}}function zt(v){let t,h="Examples:",n,r,f;return r=new Xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEZvY2FsTmV0Q29uZmlnJTJDJTIwRm9jYWxOZXRGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmZvY2FsbmV0LWJhc2Utc2ltbWltLXdpbmRvdzYtMTkyJTIyKSUwQWNvbmZpZyUyMCUzRCUyMEZvY2FsTmV0Q29uZmlnKCklMEFtb2RlbCUyMCUzRCUyMEZvY2FsTmV0Rm9yTWFza2VkSW1hZ2VNb2RlbGluZyhjb25maWcpJTBBJTBBbnVtX3BhdGNoZXMlMjAlM0QlMjAobW9kZWwuY29uZmlnLmltYWdlX3NpemUlMjAlMkYlMkYlMjBtb2RlbC5jb25maWcucGF0Y2hfc2l6ZSklMjAqKiUyMDIlMEFwaXhlbF92YWx1ZXMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5waXhlbF92YWx1ZXMlMEElMjMlMjBjcmVhdGUlMjByYW5kb20lMjBib29sZWFuJTIwbWFzayUyMG9mJTIwc2hhcGUlMjAoYmF0Y2hfc2l6ZSUyQyUyMG51bV9wYXRjaGVzKSUwQWJvb2xfbWFza2VkX3BvcyUyMCUzRCUyMHRvcmNoLnJhbmRpbnQobG93JTNEMCUyQyUyMGhpZ2glM0QyJTJDJTIwc2l6ZSUzRCgxJTJDJTIwbnVtX3BhdGNoZXMpKS5ib29sKCklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwocGl4ZWxfdmFsdWVzJTJDJTIwYm9vbF9tYXNrZWRfcG9zJTNEYm9vbF9tYXNrZWRfcG9zKSUwQWxvc3MlMkMlMjByZWNvbnN0cnVjdGVkX3BpeGVsX3ZhbHVlcyUyMCUzRCUyMG91dHB1dHMubG9zcyUyQyUyMG91dHB1dHMubG9naXRzJTBBbGlzdChyZWNvbnN0cnVjdGVkX3BpeGVsX3ZhbHVlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, FocalNetConfig, FocalNetForMaskedImageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/focalnet-base-simmim-window6-192&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config = FocalNetConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FocalNetForMaskedImageModeling(config)

<span class="hljs-meta">&gt;&gt;&gt; </span>num_patches = (model.config.image_size // model.config.patch_size) ** <span class="hljs-number">2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create random boolean mask of shape (batch_size, num_patches)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>bool_masked_pos = torch.randint(low=<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=(<span class="hljs-number">1</span>, num_patches)).<span class="hljs-built_in">bool</span>()

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, reconstructed_pixel_values = outputs.loss, outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(reconstructed_pixel_values.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">192</span>, <span class="hljs-number">192</span>]`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=l(),u(r.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),F(t)!=="svelte-kvfsh7"&&(t.textContent=h),n=i(o),g(r.$$.fragment,o)},m(o,w){c(o,t,w),c(o,n,w),_(r,o,w),f=!0},p:L,i(o){f||(b(r.$$.fragment,o),f=!0)},o(o){y(r.$$.fragment,o),f=!1},d(o){o&&(s(t),s(n)),M(r,o)}}}function Jt(v){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=m("p"),t.innerHTML=h},l(n){t=p(n,"P",{"data-svelte-h":!0}),F(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(n,r){c(n,t,r)},p:L,d(n){n&&s(t)}}}function Gt(v){let t,h="Example:",n,r,f;return r=new Xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEZvY2FsTmV0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGZm9jYWxuZXQtdGlueSUyMiklMEFtb2RlbCUyMCUzRCUyMEZvY2FsTmV0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGZm9jYWxuZXQtdGlueSUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, FocalNetForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/focalnet-tiny&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FocalNetForImageClassification.from_pretrained(<span class="hljs-string">&quot;microsoft/focalnet-tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){t=m("p"),t.textContent=h,n=l(),u(r.$$.fragment)},l(o){t=p(o,"P",{"data-svelte-h":!0}),F(t)!=="svelte-11lpom8"&&(t.textContent=h),n=i(o),g(r.$$.fragment,o)},m(o,w){c(o,t,w),c(o,n,w),_(r,o,w),f=!0},p:L,i(o){f||(b(r.$$.fragment,o),f=!0)},o(o){y(r.$$.fragment,o),f=!1},d(o){o&&(s(t),s(n)),M(r,o)}}}function Rt(v){let t,h,n,r,f,o,w,$e,q,ct=`The FocalNet model was proposed in <a href="https://arxiv.org/abs/2203.11926" rel="nofollow">Focal Modulation Networks</a> by Jianwei Yang, Chunyuan Li, Xiyang Dai, Lu Yuan, Jianfeng Gao.
FocalNets completely replace self-attention (used in models like <a href="vit">ViT</a> and <a href="swin">Swin</a>) by a focal modulation mechanism for modeling token interactions in vision.
The authors claim that FocalNets outperform self-attention based models with similar computational costs on the tasks of image classification, object detection, and segmentation.`,Te,A,dt="The abstract from the paper is the following:",Ne,D,mt=`<em>We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its
content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3.</em>`,je,O,pt=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/microsoft/FocalNet" rel="nofollow">here</a>.`,Ce,K,xe,N,ee,Be,me,ht=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetModel">FocalNetModel</a>. It is used to instantiate a
FocalNet model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the FocalNet
<a href="https://huggingface.co/microsoft/focalnet-tiny" rel="nofollow">microsoft/focalnet-tiny</a> architecture.`,Ye,pe,ft=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Qe,X,Ue,te,Ze,Z,oe,Ee,he,ut=`The bare FocalNet Model outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Pe,C,ae,Se,fe,gt='The <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetModel">FocalNetModel</a> forward method, overrides the <code>__call__</code> special method.',He,B,Le,Y,Ie,ne,ke,$,se,qe,ue,_t="FocalNet Model with a decoder on top for masked image modeling.",Ae,ge,bt='This follows the same implementation as in <a href="https://arxiv.org/abs/2111.09886" rel="nofollow">SimMIM</a>.',De,Q,Oe,_e,yt=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Ke,x,re,et,be,Mt='The <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForMaskedImageModeling">FocalNetForMaskedImageModeling</a> forward method, overrides the <code>__call__</code> special method.',tt,E,ot,P,We,le,ze,j,ie,at,ye,wt=`FocalNet Model with an image classification head on top (a linear layer on top of the pooled output) e.g. for
ImageNet.`,nt,Me,Ft=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,st,U,ce,rt,we,vt='The <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForImageClassification">FocalNetForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',lt,S,it,H,Je,ve,Ge;return f=new Fe({props:{title:"FocalNet",local:"focalnet",headingTag:"h1"}}),w=new Fe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),K=new Fe({props:{title:"FocalNetConfig",local:"transformers.FocalNetConfig",headingTag:"h2"}}),ee=new de({props:{name:"class transformers.FocalNetConfig",anchor:"transformers.FocalNetConfig",parameters:[{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 4"},{name:"num_channels",val:" = 3"},{name:"embed_dim",val:" = 96"},{name:"use_conv_embed",val:" = False"},{name:"hidden_sizes",val:" = [192, 384, 768, 768]"},{name:"depths",val:" = [2, 2, 6, 2]"},{name:"focal_levels",val:" = [2, 2, 2, 2]"},{name:"focal_windows",val:" = [3, 3, 3, 3]"},{name:"hidden_act",val:" = 'gelu'"},{name:"mlp_ratio",val:" = 4.0"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"drop_path_rate",val:" = 0.1"},{name:"use_layerscale",val:" = False"},{name:"layerscale_value",val:" = 0.0001"},{name:"use_post_layernorm",val:" = False"},{name:"use_post_layernorm_in_modulation",val:" = False"},{name:"normalize_modulator",val:" = False"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"encoder_stride",val:" = 32"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FocalNetConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FocalNetConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The size (resolution) of each patch in the embeddings layer.`,name:"patch_size"},{anchor:"transformers.FocalNetConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FocalNetConfig.embed_dim",description:`<strong>embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 96) &#x2014;
Dimensionality of patch embedding.`,name:"embed_dim"},{anchor:"transformers.FocalNetConfig.use_conv_embed",description:`<strong>use_conv_embed</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use convolutional embedding. The authors noted that using convolutional embedding usually
improve the performance, but it&#x2019;s not used by default.`,name:"use_conv_embed"},{anchor:"transformers.FocalNetConfig.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[192, 384, 768, 768]</code>) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.FocalNetConfig.depths",description:`<strong>depths</strong> (<code>list(int)</code>, <em>optional</em>, defaults to <code>[2, 2, 6, 2]</code>) &#x2014;
Depth (number of layers) of each stage in the encoder.`,name:"depths"},{anchor:"transformers.FocalNetConfig.focal_levels",description:`<strong>focal_levels</strong> (<code>list(int)</code>, <em>optional</em>, defaults to <code>[2, 2, 2, 2]</code>) &#x2014;
Number of focal levels in each layer of the respective stages in the encoder.`,name:"focal_levels"},{anchor:"transformers.FocalNetConfig.focal_windows",description:`<strong>focal_windows</strong> (<code>list(int)</code>, <em>optional</em>, defaults to <code>[3, 3, 3, 3]</code>) &#x2014;
Focal window size in each layer of the respective stages in the encoder.`,name:"focal_windows"},{anchor:"transformers.FocalNetConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FocalNetConfig.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
Ratio of MLP hidden dimensionality to embedding dimensionality.`,name:"mlp_ratio"},{anchor:"transformers.FocalNetConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings and encoder.`,name:"hidden_dropout_prob"},{anchor:"transformers.FocalNetConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Stochastic depth rate.`,name:"drop_path_rate"},{anchor:"transformers.FocalNetConfig.use_layerscale",description:`<strong>use_layerscale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use layer scale in the encoder.`,name:"use_layerscale"},{anchor:"transformers.FocalNetConfig.layerscale_value",description:`<strong>layerscale_value</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0001) &#x2014;
The initial value of the layer scale.`,name:"layerscale_value"},{anchor:"transformers.FocalNetConfig.use_post_layernorm",description:`<strong>use_post_layernorm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use post layer normalization in the encoder.`,name:"use_post_layernorm"},{anchor:"transformers.FocalNetConfig.use_post_layernorm_in_modulation",description:`<strong>use_post_layernorm_in_modulation</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use post layer normalization in the modulation layer.`,name:"use_post_layernorm_in_modulation"},{anchor:"transformers.FocalNetConfig.normalize_modulator",description:`<strong>normalize_modulator</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to normalize the modulator.`,name:"normalize_modulator"},{anchor:"transformers.FocalNetConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FocalNetConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FocalNetConfig.encoder_stride",description:`<strong>encoder_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Factor to increase the spatial resolution by in the decoder head for masked image modeling.`,name:"encoder_stride"},{anchor:"transformers.FocalNetConfig.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.FocalNetConfig.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/focalnet/configuration_focalnet.py#L29"}}),X=new Ve({props:{anchor:"transformers.FocalNetConfig.example",$$slots:{default:[Ut]},$$scope:{ctx:v}}}),te=new Fe({props:{title:"FocalNetModel",local:"transformers.FocalNetModel",headingTag:"h2"}}),oe=new de({props:{name:"class transformers.FocalNetModel",anchor:"transformers.FocalNetModel",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"},{name:"use_mask_token",val:" = False"}],parametersDescription:[{anchor:"transformers.FocalNetModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/focalnet/modeling_focalnet.py#L681"}}),ae=new de({props:{name:"forward",anchor:"transformers.FocalNetModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FocalNetModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<code>AutoImageProcessor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FocalNetModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FocalNetModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FocalNetModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/focalnet/modeling_focalnet.py#L704",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.focalnet.modeling_focalnet.FocalNetModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig"
>FocalNetConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>, <em>optional</em>, returned when <code>add_pooling_layer=True</code> is passed) — Average pooling of the last layer hidden-state.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.focalnet.modeling_focalnet.FocalNetModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),B=new Re({props:{$$slots:{default:[Zt]},$$scope:{ctx:v}}}),Y=new Ve({props:{anchor:"transformers.FocalNetModel.forward.example",$$slots:{default:[It]},$$scope:{ctx:v}}}),ne=new Fe({props:{title:"FocalNetForMaskedImageModeling",local:"transformers.FocalNetForMaskedImageModeling",headingTag:"h2"}}),se=new de({props:{name:"class transformers.FocalNetForMaskedImageModeling",anchor:"transformers.FocalNetForMaskedImageModeling",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.FocalNetForMaskedImageModeling.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/focalnet/modeling_focalnet.py#L761"}}),Q=new Re({props:{$$slots:{default:[kt]},$$scope:{ctx:v}}}),re=new de({props:{name:"forward",anchor:"transformers.FocalNetForMaskedImageModeling.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FocalNetForMaskedImageModeling.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<code>AutoImageProcessor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FocalNetForMaskedImageModeling.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FocalNetForMaskedImageModeling.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FocalNetForMaskedImageModeling.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/focalnet/modeling_focalnet.py#L793",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.focalnet.modeling_focalnet.FocalNetMaskedImageModelingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig"
>FocalNetConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>bool_masked_pos</code> is provided) — Masked image modeling (MLM) loss.</p>
</li>
<li>
<p><strong>reconstruction</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Reconstructed pixel values.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.focalnet.modeling_focalnet.FocalNetMaskedImageModelingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),E=new Re({props:{$$slots:{default:[Wt]},$$scope:{ctx:v}}}),P=new Ve({props:{anchor:"transformers.FocalNetForMaskedImageModeling.forward.example",$$slots:{default:[zt]},$$scope:{ctx:v}}}),le=new Fe({props:{title:"FocalNetForImageClassification",local:"transformers.FocalNetForImageClassification",headingTag:"h2"}}),ie=new de({props:{name:"class transformers.FocalNetForImageClassification",anchor:"transformers.FocalNetForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.FocalNetForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/focalnet/modeling_focalnet.py#L876"}}),ce=new de({props:{name:"forward",anchor:"transformers.FocalNetForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FocalNetForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<code>AutoImageProcessor.__call__()</code> for details.`,name:"pixel_values"},{anchor:"transformers.FocalNetForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FocalNetForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FocalNetForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/focalnet/modeling_focalnet.py#L899",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.focalnet.modeling_focalnet.FocalNetImageClassifierOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig"
>FocalNetConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.focalnet.modeling_focalnet.FocalNetImageClassifierOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),S=new Re({props:{$$slots:{default:[Jt]},$$scope:{ctx:v}}}),H=new Ve({props:{anchor:"transformers.FocalNetForImageClassification.forward.example",$$slots:{default:[Gt]},$$scope:{ctx:v}}}),{c(){t=m("meta"),h=l(),n=m("p"),r=l(),u(f.$$.fragment),o=l(),u(w.$$.fragment),$e=l(),q=m("p"),q.innerHTML=ct,Te=l(),A=m("p"),A.textContent=dt,Ne=l(),D=m("p"),D.innerHTML=mt,je=l(),O=m("p"),O.innerHTML=pt,Ce=l(),u(K.$$.fragment),xe=l(),N=m("div"),u(ee.$$.fragment),Be=l(),me=m("p"),me.innerHTML=ht,Ye=l(),pe=m("p"),pe.innerHTML=ft,Qe=l(),u(X.$$.fragment),Ue=l(),u(te.$$.fragment),Ze=l(),Z=m("div"),u(oe.$$.fragment),Ee=l(),he=m("p"),he.innerHTML=ut,Pe=l(),C=m("div"),u(ae.$$.fragment),Se=l(),fe=m("p"),fe.innerHTML=gt,He=l(),u(B.$$.fragment),Le=l(),u(Y.$$.fragment),Ie=l(),u(ne.$$.fragment),ke=l(),$=m("div"),u(se.$$.fragment),qe=l(),ue=m("p"),ue.textContent=_t,Ae=l(),ge=m("p"),ge.innerHTML=bt,De=l(),u(Q.$$.fragment),Oe=l(),_e=m("p"),_e.innerHTML=yt,Ke=l(),x=m("div"),u(re.$$.fragment),et=l(),be=m("p"),be.innerHTML=Mt,tt=l(),u(E.$$.fragment),ot=l(),u(P.$$.fragment),We=l(),u(le.$$.fragment),ze=l(),j=m("div"),u(ie.$$.fragment),at=l(),ye=m("p"),ye.textContent=wt,nt=l(),Me=m("p"),Me.innerHTML=Ft,st=l(),U=m("div"),u(ce.$$.fragment),rt=l(),we=m("p"),we.innerHTML=vt,lt=l(),u(S.$$.fragment),it=l(),u(H.$$.fragment),Je=l(),ve=m("p"),this.h()},l(e){const a=xt("svelte-u9bgzb",document.head);t=p(a,"META",{name:!0,content:!0}),a.forEach(s),h=i(e),n=p(e,"P",{}),R(n).forEach(s),r=i(e),g(f.$$.fragment,e),o=i(e),g(w.$$.fragment,e),$e=i(e),q=p(e,"P",{"data-svelte-h":!0}),F(q)!=="svelte-84c53e"&&(q.innerHTML=ct),Te=i(e),A=p(e,"P",{"data-svelte-h":!0}),F(A)!=="svelte-vfdo9a"&&(A.textContent=dt),Ne=i(e),D=p(e,"P",{"data-svelte-h":!0}),F(D)!=="svelte-1uz33aa"&&(D.innerHTML=mt),je=i(e),O=p(e,"P",{"data-svelte-h":!0}),F(O)!=="svelte-dxae3d"&&(O.innerHTML=pt),Ce=i(e),g(K.$$.fragment,e),xe=i(e),N=p(e,"DIV",{class:!0});var I=R(N);g(ee.$$.fragment,I),Be=i(I),me=p(I,"P",{"data-svelte-h":!0}),F(me)!=="svelte-1pfic19"&&(me.innerHTML=ht),Ye=i(I),pe=p(I,"P",{"data-svelte-h":!0}),F(pe)!=="svelte-o55m63"&&(pe.innerHTML=ft),Qe=i(I),g(X.$$.fragment,I),I.forEach(s),Ue=i(e),g(te.$$.fragment,e),Ze=i(e),Z=p(e,"DIV",{class:!0});var G=R(Z);g(oe.$$.fragment,G),Ee=i(G),he=p(G,"P",{"data-svelte-h":!0}),F(he)!=="svelte-9um4dr"&&(he.innerHTML=ut),Pe=i(G),C=p(G,"DIV",{class:!0});var k=R(C);g(ae.$$.fragment,k),Se=i(k),fe=p(k,"P",{"data-svelte-h":!0}),F(fe)!=="svelte-7tya37"&&(fe.innerHTML=gt),He=i(k),g(B.$$.fragment,k),Le=i(k),g(Y.$$.fragment,k),k.forEach(s),G.forEach(s),Ie=i(e),g(ne.$$.fragment,e),ke=i(e),$=p(e,"DIV",{class:!0});var T=R($);g(se.$$.fragment,T),qe=i(T),ue=p(T,"P",{"data-svelte-h":!0}),F(ue)!=="svelte-1082wlw"&&(ue.textContent=_t),Ae=i(T),ge=p(T,"P",{"data-svelte-h":!0}),F(ge)!=="svelte-muou64"&&(ge.innerHTML=bt),De=i(T),g(Q.$$.fragment,T),Oe=i(T),_e=p(T,"P",{"data-svelte-h":!0}),F(_e)!=="svelte-68lg8f"&&(_e.innerHTML=yt),Ke=i(T),x=p(T,"DIV",{class:!0});var W=R(x);g(re.$$.fragment,W),et=i(W),be=p(W,"P",{"data-svelte-h":!0}),F(be)!=="svelte-zo6p8f"&&(be.innerHTML=Mt),tt=i(W),g(E.$$.fragment,W),ot=i(W),g(P.$$.fragment,W),W.forEach(s),T.forEach(s),We=i(e),g(le.$$.fragment,e),ze=i(e),j=p(e,"DIV",{class:!0});var z=R(j);g(ie.$$.fragment,z),at=i(z),ye=p(z,"P",{"data-svelte-h":!0}),F(ye)!=="svelte-mhgt84"&&(ye.textContent=wt),nt=i(z),Me=p(z,"P",{"data-svelte-h":!0}),F(Me)!=="svelte-68lg8f"&&(Me.innerHTML=Ft),st=i(z),U=p(z,"DIV",{class:!0});var J=R(U);g(ce.$$.fragment,J),rt=i(J),we=p(J,"P",{"data-svelte-h":!0}),F(we)!=="svelte-3q1uz3"&&(we.innerHTML=vt),lt=i(J),g(S.$$.fragment,J),it=i(J),g(H.$$.fragment,J),J.forEach(s),z.forEach(s),Je=i(e),ve=p(e,"P",{}),R(ve).forEach(s),this.h()},h(){V(t,"name","hf:doc:metadata"),V(t,"content",Vt),V(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,a){d(document.head,t),c(e,h,a),c(e,n,a),c(e,r,a),_(f,e,a),c(e,o,a),_(w,e,a),c(e,$e,a),c(e,q,a),c(e,Te,a),c(e,A,a),c(e,Ne,a),c(e,D,a),c(e,je,a),c(e,O,a),c(e,Ce,a),_(K,e,a),c(e,xe,a),c(e,N,a),_(ee,N,null),d(N,Be),d(N,me),d(N,Ye),d(N,pe),d(N,Qe),_(X,N,null),c(e,Ue,a),_(te,e,a),c(e,Ze,a),c(e,Z,a),_(oe,Z,null),d(Z,Ee),d(Z,he),d(Z,Pe),d(Z,C),_(ae,C,null),d(C,Se),d(C,fe),d(C,He),_(B,C,null),d(C,Le),_(Y,C,null),c(e,Ie,a),_(ne,e,a),c(e,ke,a),c(e,$,a),_(se,$,null),d($,qe),d($,ue),d($,Ae),d($,ge),d($,De),_(Q,$,null),d($,Oe),d($,_e),d($,Ke),d($,x),_(re,x,null),d(x,et),d(x,be),d(x,tt),_(E,x,null),d(x,ot),_(P,x,null),c(e,We,a),_(le,e,a),c(e,ze,a),c(e,j,a),_(ie,j,null),d(j,at),d(j,ye),d(j,nt),d(j,Me),d(j,st),d(j,U),_(ce,U,null),d(U,rt),d(U,we),d(U,lt),_(S,U,null),d(U,it),_(H,U,null),c(e,Je,a),c(e,ve,a),Ge=!0},p(e,[a]){const I={};a&2&&(I.$$scope={dirty:a,ctx:e}),X.$set(I);const G={};a&2&&(G.$$scope={dirty:a,ctx:e}),B.$set(G);const k={};a&2&&(k.$$scope={dirty:a,ctx:e}),Y.$set(k);const T={};a&2&&(T.$$scope={dirty:a,ctx:e}),Q.$set(T);const W={};a&2&&(W.$$scope={dirty:a,ctx:e}),E.$set(W);const z={};a&2&&(z.$$scope={dirty:a,ctx:e}),P.$set(z);const J={};a&2&&(J.$$scope={dirty:a,ctx:e}),S.$set(J);const $t={};a&2&&($t.$$scope={dirty:a,ctx:e}),H.$set($t)},i(e){Ge||(b(f.$$.fragment,e),b(w.$$.fragment,e),b(K.$$.fragment,e),b(ee.$$.fragment,e),b(X.$$.fragment,e),b(te.$$.fragment,e),b(oe.$$.fragment,e),b(ae.$$.fragment,e),b(B.$$.fragment,e),b(Y.$$.fragment,e),b(ne.$$.fragment,e),b(se.$$.fragment,e),b(Q.$$.fragment,e),b(re.$$.fragment,e),b(E.$$.fragment,e),b(P.$$.fragment,e),b(le.$$.fragment,e),b(ie.$$.fragment,e),b(ce.$$.fragment,e),b(S.$$.fragment,e),b(H.$$.fragment,e),Ge=!0)},o(e){y(f.$$.fragment,e),y(w.$$.fragment,e),y(K.$$.fragment,e),y(ee.$$.fragment,e),y(X.$$.fragment,e),y(te.$$.fragment,e),y(oe.$$.fragment,e),y(ae.$$.fragment,e),y(B.$$.fragment,e),y(Y.$$.fragment,e),y(ne.$$.fragment,e),y(se.$$.fragment,e),y(Q.$$.fragment,e),y(re.$$.fragment,e),y(E.$$.fragment,e),y(P.$$.fragment,e),y(le.$$.fragment,e),y(ie.$$.fragment,e),y(ce.$$.fragment,e),y(S.$$.fragment,e),y(H.$$.fragment,e),Ge=!1},d(e){e&&(s(h),s(n),s(r),s(o),s($e),s(q),s(Te),s(A),s(Ne),s(D),s(je),s(O),s(Ce),s(xe),s(N),s(Ue),s(Ze),s(Z),s(Ie),s(ke),s($),s(We),s(ze),s(j),s(Je),s(ve)),s(t),M(f,e),M(w,e),M(K,e),M(ee),M(X),M(te,e),M(oe),M(ae),M(B),M(Y),M(ne,e),M(se),M(Q),M(re),M(E),M(P),M(le,e),M(ie),M(ce),M(S),M(H)}}}const Vt='{"title":"FocalNet","local":"focalnet","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"FocalNetConfig","local":"transformers.FocalNetConfig","sections":[],"depth":2},{"title":"FocalNetModel","local":"transformers.FocalNetModel","sections":[],"depth":2},{"title":"FocalNetForMaskedImageModeling","local":"transformers.FocalNetForMaskedImageModeling","sections":[],"depth":2},{"title":"FocalNetForImageClassification","local":"transformers.FocalNetForImageClassification","sections":[],"depth":2}],"depth":1}';function Xt(v){return Nt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Lt extends jt{constructor(t){super(),Ct(this,t,Xt,Rt,Tt,{})}}export{Lt as component};
