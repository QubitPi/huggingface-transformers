import{s as Mn,o as Cn,n as de}from"../chunks/scheduler.9bc65507.js";import{S as On,i as zn,g as l,s,r as f,A as xn,h as c,f as a,c as r,j,u as g,x as h,k as J,y as i,a as d,v as _,d as b,t as v,w as y}from"../chunks/index.707bf1b6.js";import{T as ke}from"../chunks/Tip.c2ecdbf4.js";import{D as ie}from"../chunks/Docstring.17db21ae.js";import{C as Ln}from"../chunks/CodeBlock.54a9f38d.js";import{E as kn}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as we}from"../chunks/Heading.342b1fa6.js";function Fn($){let n,u="This model is in maintenance mode only, we donâ€™t accept any new PRs changing its code.",o,m,L=`If you run into any issues running this model, please reinstall the last version that supported this model: v4.31.0.
You can do so by running the following command: <code>pip install -U transformers==4.31.0</code>.`;return{c(){n=l("p"),n.textContent=u,o=s(),m=l("p"),m.innerHTML=L},l(p){n=c(p,"P",{"data-svelte-h":!0}),h(n)!=="svelte-1dwyvn5"&&(n.textContent=u),o=r(p),m=c(p,"P",{"data-svelte-h":!0}),h(m)!=="svelte-1o63f7c"&&(m.innerHTML=L)},m(p,k){d(p,n,k),d(p,o,k),d(p,m,k)},p:de,d(p){p&&(a(n),a(o),a(m))}}}function Pn($){let n,u='This model differs from the <a href="https://huggingface.co/models?search=openllama" rel="nofollow">OpenLLaMA models</a> on the Hugging Face Hub, which primarily use the <a href="llama">LLaMA</a> architecture.';return{c(){n=l("p"),n.innerHTML=u},l(o){n=c(o,"P",{"data-svelte-h":!0}),h(n)!=="svelte-1kc01gl"&&(n.innerHTML=u)},m(o,m){d(o,n,m)},p:de,d(o){o&&a(n)}}}function qn($){let n,u;return n=new Ln({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME9wZW5MbGFtYU1vZGVsJTJDJTIwT3BlbkxsYW1hQ29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyME9wZW4tTGxhbWElMjBvcGVuX2xsYW1hLTdiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyME9wZW5MbGFtYUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMG9wZW5fbGxhbWEtN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyME9wZW5MbGFtYU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> OpenLlamaModel, OpenLlamaConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Open-Llama open_llama-7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = OpenLlamaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the open_llama-7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenLlamaModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){f(n.$$.fragment)},l(o){g(n.$$.fragment,o)},m(o,m){_(n,o,m),u=!0},p:de,i(o){u||(b(n.$$.fragment,o),u=!0)},o(o){v(n.$$.fragment,o),u=!1},d(o){y(n,o)}}}function In($){let n,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=l("p"),n.innerHTML=u},l(o){n=c(o,"P",{"data-svelte-h":!0}),h(n)!=="svelte-fincs2"&&(n.innerHTML=u)},m(o,m){d(o,n,m)},p:de,d(o){o&&a(n)}}}function Wn($){let n,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=l("p"),n.innerHTML=u},l(o){n=c(o,"P",{"data-svelte-h":!0}),h(n)!=="svelte-fincs2"&&(n.innerHTML=u)},m(o,m){d(o,n,m)},p:de,d(o){o&&a(n)}}}function Sn($){let n,u="Example:",o,m,L;return m=new Ln({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBPcGVuTGxhbWFGb3JDYXVzYWxMTSUwQSUwQW1vZGVsJTIwJTNEJTIwT3BlbkxsYW1hRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5sbS1yZXNlYXJjaCUyRm9wZW5fbGxhbWFfN2IlMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmxtLXJlc2VhcmNoJTJGb3Blbl9sbGFtYV83YiUyMiklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJIZXklMkMlMjBhcmUlMjB5b3UlMjBjb25zY2lvdXMlM0YlMjBDYW4lMjB5b3UlMjB0YWxrJTIwdG8lMjBtZSUzRiUyMiUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQSUyMyUyMEdlbmVyYXRlJTBBZ2VuZXJhdGVfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoaW5wdXRzLmlucHV0X2lkcyUyQyUyMG1heF9sZW5ndGglM0QzMCklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlJTJDJTIwY2xlYW5fdXBfdG9rZW5pemF0aW9uX3NwYWNlcyUzREZhbHNlKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, OpenLlamaForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = OpenLlamaForCausalLM.from_pretrained(<span class="hljs-string">&quot;openlm-research/open_llama_7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openlm-research/open_llama_7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?\\nI&#x27;m not conscious, but I can talk to you.&quot;</span>`,wrap:!1}}),{c(){n=l("p"),n.textContent=u,o=s(),f(m.$$.fragment)},l(p){n=c(p,"P",{"data-svelte-h":!0}),h(n)!=="svelte-11lpom8"&&(n.textContent=u),o=r(p),g(m.$$.fragment,p)},m(p,k){d(p,n,k),d(p,o,k),_(m,p,k),L=!0},p:de,i(p){L||(b(m.$$.fragment,p),L=!0)},o(p){v(m.$$.fragment,p),L=!1},d(p){p&&(a(n),a(o)),y(m,p)}}}function jn($){let n,u=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=l("p"),n.innerHTML=u},l(o){n=c(o,"P",{"data-svelte-h":!0}),h(n)!=="svelte-fincs2"&&(n.innerHTML=u)},m(o,m){d(o,n,m)},p:de,d(o){o&&a(n)}}}function Jn($){let n,u,o,m,L,p,k,Le,U,Me,V,Ce,E,dn="The Open-Llama model was proposed in the open source Open-Llama project by community developer s-JoL.",Oe,A,ln=`The model is mainly based on LLaMA with some modifications, incorporating memory-efficient attention from Xformers, stable embedding from Bloom, and shared input-output embedding from PaLM.
And the model is pre-trained on both Chinese and English, which gives it better performance on Chinese language tasks.`,ze,Y,cn=`This model was contributed by <a href="https://huggingface.co/s-JoL" rel="nofollow">s-JoL</a>.
The original code was released on GitHub by <a href="https://github.com/s-JoL" rel="nofollow">s-JoL</a>, but is now removed.`,xe,D,Fe,C,X,He,le,mn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaModel">OpenLlamaModel</a>. It is used to instantiate an
Open-Llama model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the
<a href="https://huggingface.co/s-JoL/Open-Llama-V1" rel="nofollow">s-JoL/Open-Llama-V1</a>.`,Ze,ce,pn=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ne,H,Pe,R,qe,M,Q,Ge,me,un=`The bare Open-Llama Model outputting raw hidden-states without any specific head on top.
This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Be,pe,hn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ve,ue,fn="Transformer decoder consisting of <em>config.num_hidden_layers</em> layers. Each layer is a <code>OpenLlamaDecoderLayer</code>",Ee,P,K,Ae,he,gn='The <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaModel">OpenLlamaModel</a> forward method, overrides the <code>__call__</code> special method.',Ye,Z,Ie,ee,We,I,ne,De,z,te,Xe,fe,_n='The <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaForCausalLM">OpenLlamaForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',Re,N,Qe,G,Se,oe,je,T,ae,Ke,ge,bn="The LLaMa Model transformer with a sequence classification head on top (linear layer).",en,_e,vn=`<a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaForSequenceClassification">OpenLlamaForSequenceClassification</a> uses the last token in order to do the classification, as other causal
models (e.g. GPT-2) do.`,nn,be,yn=`Since it does classification on the last token, it requires to know the position of the last token. If a
<code>pad_token_id</code> is defined in the configuration, it finds the last token that is not a padding token in each row. If
no <code>pad_token_id</code> is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when <code>inputs_embeds</code> are passed instead of <code>input_ids</code>, it does the same (take the last value in
each row of the batch).`,tn,ve,Tn=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,on,ye,wn=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,an,q,se,sn,Te,$n='The <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaForSequenceClassification">OpenLlamaForSequenceClassification</a> forward method, overrides the <code>__call__</code> special method.',rn,B,Je,$e,Ue;return L=new we({props:{title:"Open-Llama",local:"open-llama",headingTag:"h1"}}),k=new ke({props:{warning:!0,$$slots:{default:[Fn]},$$scope:{ctx:$}}}),U=new ke({props:{warning:!0,$$slots:{default:[Pn]},$$scope:{ctx:$}}}),V=new we({props:{title:"Overview",local:"overview",headingTag:"h2"}}),D=new we({props:{title:"OpenLlamaConfig",local:"transformers.OpenLlamaConfig",headingTag:"h2"}}),X=new ie({props:{name:"class transformers.OpenLlamaConfig",anchor:"transformers.OpenLlamaConfig",parameters:[{name:"vocab_size",val:" = 100000"},{name:"hidden_size",val:" = 4096"},{name:"intermediate_size",val:" = 11008"},{name:"num_hidden_layers",val:" = 32"},{name:"num_attention_heads",val:" = 32"},{name:"hidden_act",val:" = 'silu'"},{name:"max_position_embeddings",val:" = 2048"},{name:"initializer_range",val:" = 0.02"},{name:"rms_norm_eps",val:" = 1e-06"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = 0"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"tie_word_embeddings",val:" = False"},{name:"use_memory_efficient_attention",val:" = True"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_dropout_prob",val:" = 0.1"},{name:"use_stable_embedding",val:" = True"},{name:"shared_input_output_embedding",val:" = True"},{name:"rope_scaling",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.OpenLlamaConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
Vocabulary size of the Open-Llama model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaModel">OpenLlamaModel</a>`,name:"vocab_size"},{anchor:"transformers.OpenLlamaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimension of the hidden representations.`,name:"hidden_size"},{anchor:"transformers.OpenLlamaConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 11008) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.OpenLlamaConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.OpenLlamaConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.OpenLlamaConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the decoder.`,name:"hidden_act"},{anchor:"transformers.OpenLlamaConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.OpenLlamaConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.OpenLlamaConfig.rms_norm_eps",description:`<strong>rms_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the rms normalization layers.`,name:"rms_norm_eps"},{anchor:"transformers.OpenLlamaConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.OpenLlamaConfig.tie_word_embeddings(bool,",description:`<strong>tie_word_embeddings(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to tie weight embeddings`,name:"tie_word_embeddings(bool,"},{anchor:"transformers.OpenLlamaConfig.rope_scaling",description:`<strong>rope_scaling</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling
strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is
<code>{&quot;type&quot;: strategy name, &quot;factor&quot;: scaling factor}</code>. When using this flag, don&#x2019;t update
<code>max_position_embeddings</code> to the expected new maximum. See the following thread for more information on how
these scaling strategies behave:
<a href="https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/" rel="nofollow">https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/</a>. This is an
experimental feature, subject to breaking API changes in future versions.</p>
<p>Example &#x2014;`,name:"rope_scaling"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/open_llama/configuration_open_llama.py#L33"}}),H=new kn({props:{anchor:"transformers.OpenLlamaConfig.example",$$slots:{default:[qn]},$$scope:{ctx:$}}}),R=new we({props:{title:"OpenLlamaModel",local:"transformers.OpenLlamaModel",headingTag:"h2"}}),Q=new ie({props:{name:"class transformers.OpenLlamaModel",anchor:"transformers.OpenLlamaModel",parameters:[{name:"config",val:": OpenLlamaConfig"}],parametersDescription:[{anchor:"transformers.OpenLlamaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.
config &#x2014; OpenLlamaConfig`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L512"}}),K=new ie({props:{name:"forward",anchor:"transformers.OpenLlamaModel.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.OpenLlamaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.OpenLlamaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OpenLlamaModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.OpenLlamaModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.OpenLlamaModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.OpenLlamaModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.OpenLlamaModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OpenLlamaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OpenLlamaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L547"}}),Z=new ke({props:{$$slots:{default:[In]},$$scope:{ctx:$}}}),ee=new we({props:{title:"OpenLlamaForCausalLM",local:"transformers.OpenLlamaForCausalLM",headingTag:"h2"}}),ne=new ie({props:{name:"class transformers.OpenLlamaForCausalLM",anchor:"transformers.OpenLlamaForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L674"}}),te=new ie({props:{name:"forward",anchor:"transformers.OpenLlamaForCausalLM.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.OpenLlamaForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.OpenLlamaForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OpenLlamaForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.OpenLlamaForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.OpenLlamaForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.OpenLlamaForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.OpenLlamaForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OpenLlamaForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OpenLlamaForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.</p>
<p>Args &#x2014;
labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L704",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaConfig"
>OpenLlamaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) â€” Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) â€” Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),N=new ke({props:{$$slots:{default:[Wn]},$$scope:{ctx:$}}}),G=new kn({props:{anchor:"transformers.OpenLlamaForCausalLM.forward.example",$$slots:{default:[Sn]},$$scope:{ctx:$}}}),oe=new we({props:{title:"OpenLlamaForSequenceClassification",local:"transformers.OpenLlamaForSequenceClassification",headingTag:"h2"}}),ae=new ie({props:{name:"class transformers.OpenLlamaForSequenceClassification",anchor:"transformers.OpenLlamaForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.OpenLlamaForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L848"}}),se=new ie({props:{name:"forward",anchor:"transformers.OpenLlamaForSequenceClassification.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.OpenLlamaForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.OpenLlamaForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/open_llama/modeling_open_llama.py#L879"}}),B=new ke({props:{$$slots:{default:[jn]},$$scope:{ctx:$}}}),{c(){n=l("meta"),u=s(),o=l("p"),m=s(),f(L.$$.fragment),p=s(),f(k.$$.fragment),Le=s(),f(U.$$.fragment),Me=s(),f(V.$$.fragment),Ce=s(),E=l("p"),E.textContent=dn,Oe=s(),A=l("p"),A.textContent=ln,ze=s(),Y=l("p"),Y.innerHTML=cn,xe=s(),f(D.$$.fragment),Fe=s(),C=l("div"),f(X.$$.fragment),He=s(),le=l("p"),le.innerHTML=mn,Ze=s(),ce=l("p"),ce.innerHTML=pn,Ne=s(),f(H.$$.fragment),Pe=s(),f(R.$$.fragment),qe=s(),M=l("div"),f(Q.$$.fragment),Ge=s(),me=l("p"),me.innerHTML=un,Be=s(),pe=l("p"),pe.innerHTML=hn,Ve=s(),ue=l("p"),ue.innerHTML=fn,Ee=s(),P=l("div"),f(K.$$.fragment),Ae=s(),he=l("p"),he.innerHTML=gn,Ye=s(),f(Z.$$.fragment),Ie=s(),f(ee.$$.fragment),We=s(),I=l("div"),f(ne.$$.fragment),De=s(),z=l("div"),f(te.$$.fragment),Xe=s(),fe=l("p"),fe.innerHTML=_n,Re=s(),f(N.$$.fragment),Qe=s(),f(G.$$.fragment),Se=s(),f(oe.$$.fragment),je=s(),T=l("div"),f(ae.$$.fragment),Ke=s(),ge=l("p"),ge.textContent=bn,en=s(),_e=l("p"),_e.innerHTML=vn,nn=s(),be=l("p"),be.innerHTML=yn,tn=s(),ve=l("p"),ve.innerHTML=Tn,on=s(),ye=l("p"),ye.innerHTML=wn,an=s(),q=l("div"),f(se.$$.fragment),sn=s(),Te=l("p"),Te.innerHTML=$n,rn=s(),f(B.$$.fragment),Je=s(),$e=l("p"),this.h()},l(e){const t=xn("svelte-u9bgzb",document.head);n=c(t,"META",{name:!0,content:!0}),t.forEach(a),u=r(e),o=c(e,"P",{}),j(o).forEach(a),m=r(e),g(L.$$.fragment,e),p=r(e),g(k.$$.fragment,e),Le=r(e),g(U.$$.fragment,e),Me=r(e),g(V.$$.fragment,e),Ce=r(e),E=c(e,"P",{"data-svelte-h":!0}),h(E)!=="svelte-ycxpq0"&&(E.textContent=dn),Oe=r(e),A=c(e,"P",{"data-svelte-h":!0}),h(A)!=="svelte-1snb5sc"&&(A.textContent=ln),ze=r(e),Y=c(e,"P",{"data-svelte-h":!0}),h(Y)!=="svelte-glizh3"&&(Y.innerHTML=cn),xe=r(e),g(D.$$.fragment,e),Fe=r(e),C=c(e,"DIV",{class:!0});var x=j(C);g(X.$$.fragment,x),He=r(x),le=c(x,"P",{"data-svelte-h":!0}),h(le)!=="svelte-1e6w5uw"&&(le.innerHTML=mn),Ze=r(x),ce=c(x,"P",{"data-svelte-h":!0}),h(ce)!=="svelte-o55m63"&&(ce.innerHTML=pn),Ne=r(x),g(H.$$.fragment,x),x.forEach(a),Pe=r(e),g(R.$$.fragment,e),qe=r(e),M=c(e,"DIV",{class:!0});var O=j(M);g(Q.$$.fragment,O),Ge=r(O),me=c(O,"P",{"data-svelte-h":!0}),h(me)!=="svelte-1ap6vay"&&(me.innerHTML=un),Be=r(O),pe=c(O,"P",{"data-svelte-h":!0}),h(pe)!=="svelte-hswkmf"&&(pe.innerHTML=hn),Ve=r(O),ue=c(O,"P",{"data-svelte-h":!0}),h(ue)!=="svelte-aj729g"&&(ue.innerHTML=fn),Ee=r(O),P=c(O,"DIV",{class:!0});var W=j(P);g(K.$$.fragment,W),Ae=r(W),he=c(W,"P",{"data-svelte-h":!0}),h(he)!=="svelte-ijxxc7"&&(he.innerHTML=gn),Ye=r(W),g(Z.$$.fragment,W),W.forEach(a),O.forEach(a),Ie=r(e),g(ee.$$.fragment,e),We=r(e),I=c(e,"DIV",{class:!0});var re=j(I);g(ne.$$.fragment,re),De=r(re),z=c(re,"DIV",{class:!0});var F=j(z);g(te.$$.fragment,F),Xe=r(F),fe=c(F,"P",{"data-svelte-h":!0}),h(fe)!=="svelte-1azw5v3"&&(fe.innerHTML=_n),Re=r(F),g(N.$$.fragment,F),Qe=r(F),g(G.$$.fragment,F),F.forEach(a),re.forEach(a),Se=r(e),g(oe.$$.fragment,e),je=r(e),T=c(e,"DIV",{class:!0});var w=j(T);g(ae.$$.fragment,w),Ke=r(w),ge=c(w,"P",{"data-svelte-h":!0}),h(ge)!=="svelte-62must"&&(ge.textContent=bn),en=r(w),_e=c(w,"P",{"data-svelte-h":!0}),h(_e)!=="svelte-1v4v25c"&&(_e.innerHTML=vn),nn=r(w),be=c(w,"P",{"data-svelte-h":!0}),h(be)!=="svelte-10ugs3m"&&(be.innerHTML=yn),tn=r(w),ve=c(w,"P",{"data-svelte-h":!0}),h(ve)!=="svelte-6pahdo"&&(ve.innerHTML=Tn),on=r(w),ye=c(w,"P",{"data-svelte-h":!0}),h(ye)!=="svelte-hswkmf"&&(ye.innerHTML=wn),an=r(w),q=c(w,"DIV",{class:!0});var S=j(q);g(se.$$.fragment,S),sn=r(S),Te=c(S,"P",{"data-svelte-h":!0}),h(Te)!=="svelte-ma5ip1"&&(Te.innerHTML=$n),rn=r(S),g(B.$$.fragment,S),S.forEach(a),w.forEach(a),Je=r(e),$e=c(e,"P",{}),j($e).forEach(a),this.h()},h(){J(n,"name","hf:doc:metadata"),J(n,"content",Un),J(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){i(document.head,n),d(e,u,t),d(e,o,t),d(e,m,t),_(L,e,t),d(e,p,t),_(k,e,t),d(e,Le,t),_(U,e,t),d(e,Me,t),_(V,e,t),d(e,Ce,t),d(e,E,t),d(e,Oe,t),d(e,A,t),d(e,ze,t),d(e,Y,t),d(e,xe,t),_(D,e,t),d(e,Fe,t),d(e,C,t),_(X,C,null),i(C,He),i(C,le),i(C,Ze),i(C,ce),i(C,Ne),_(H,C,null),d(e,Pe,t),_(R,e,t),d(e,qe,t),d(e,M,t),_(Q,M,null),i(M,Ge),i(M,me),i(M,Be),i(M,pe),i(M,Ve),i(M,ue),i(M,Ee),i(M,P),_(K,P,null),i(P,Ae),i(P,he),i(P,Ye),_(Z,P,null),d(e,Ie,t),_(ee,e,t),d(e,We,t),d(e,I,t),_(ne,I,null),i(I,De),i(I,z),_(te,z,null),i(z,Xe),i(z,fe),i(z,Re),_(N,z,null),i(z,Qe),_(G,z,null),d(e,Se,t),_(oe,e,t),d(e,je,t),d(e,T,t),_(ae,T,null),i(T,Ke),i(T,ge),i(T,en),i(T,_e),i(T,nn),i(T,be),i(T,tn),i(T,ve),i(T,on),i(T,ye),i(T,an),i(T,q),_(se,q,null),i(q,sn),i(q,Te),i(q,rn),_(B,q,null),d(e,Je,t),d(e,$e,t),Ue=!0},p(e,[t]){const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),k.$set(x);const O={};t&2&&(O.$$scope={dirty:t,ctx:e}),U.$set(O);const W={};t&2&&(W.$$scope={dirty:t,ctx:e}),H.$set(W);const re={};t&2&&(re.$$scope={dirty:t,ctx:e}),Z.$set(re);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),N.$set(F);const w={};t&2&&(w.$$scope={dirty:t,ctx:e}),G.$set(w);const S={};t&2&&(S.$$scope={dirty:t,ctx:e}),B.$set(S)},i(e){Ue||(b(L.$$.fragment,e),b(k.$$.fragment,e),b(U.$$.fragment,e),b(V.$$.fragment,e),b(D.$$.fragment,e),b(X.$$.fragment,e),b(H.$$.fragment,e),b(R.$$.fragment,e),b(Q.$$.fragment,e),b(K.$$.fragment,e),b(Z.$$.fragment,e),b(ee.$$.fragment,e),b(ne.$$.fragment,e),b(te.$$.fragment,e),b(N.$$.fragment,e),b(G.$$.fragment,e),b(oe.$$.fragment,e),b(ae.$$.fragment,e),b(se.$$.fragment,e),b(B.$$.fragment,e),Ue=!0)},o(e){v(L.$$.fragment,e),v(k.$$.fragment,e),v(U.$$.fragment,e),v(V.$$.fragment,e),v(D.$$.fragment,e),v(X.$$.fragment,e),v(H.$$.fragment,e),v(R.$$.fragment,e),v(Q.$$.fragment,e),v(K.$$.fragment,e),v(Z.$$.fragment,e),v(ee.$$.fragment,e),v(ne.$$.fragment,e),v(te.$$.fragment,e),v(N.$$.fragment,e),v(G.$$.fragment,e),v(oe.$$.fragment,e),v(ae.$$.fragment,e),v(se.$$.fragment,e),v(B.$$.fragment,e),Ue=!1},d(e){e&&(a(u),a(o),a(m),a(p),a(Le),a(Me),a(Ce),a(E),a(Oe),a(A),a(ze),a(Y),a(xe),a(Fe),a(C),a(Pe),a(qe),a(M),a(Ie),a(We),a(I),a(Se),a(je),a(T),a(Je),a($e)),a(n),y(L,e),y(k,e),y(U,e),y(V,e),y(D,e),y(X),y(H),y(R,e),y(Q),y(K),y(Z),y(ee,e),y(ne),y(te),y(N),y(G),y(oe,e),y(ae),y(se),y(B)}}}const Un='{"title":"Open-Llama","local":"open-llama","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"OpenLlamaConfig","local":"transformers.OpenLlamaConfig","sections":[],"depth":2},{"title":"OpenLlamaModel","local":"transformers.OpenLlamaModel","sections":[],"depth":2},{"title":"OpenLlamaForCausalLM","local":"transformers.OpenLlamaForCausalLM","sections":[],"depth":2},{"title":"OpenLlamaForSequenceClassification","local":"transformers.OpenLlamaForSequenceClassification","sections":[],"depth":2}],"depth":1}';function Hn($){return Cn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Yn extends On{constructor(n){super(),zn(this,n,Hn,Jn,Mn,{})}}export{Yn as component};
