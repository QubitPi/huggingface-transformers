import{s as Et,o as zt,n as re}from"../chunks/scheduler.9bc65507.js";import{S as Ft,i as Qt,g as $,s as i,r as d,A as Lt,h as w,f as s,c as p,j as Bt,u as h,x as _,k as Yt,y as qt,a as l,v as u,d as y,t as g,w as b,m as Ht,n as Nt}from"../chunks/index.707bf1b6.js";import{T as ze}from"../chunks/Tip.c2ecdbf4.js";import{Y as St}from"../chunks/Youtube.e1129c6f.js";import{C}from"../chunks/CodeBlock.54a9f38d.js";import{D as At}from"../chunks/DocNotebookDropdown.41f65cb5.js";import{F as $t,M as He}from"../chunks/Markdown.fef84341.js";import{H as Ye}from"../chunks/Heading.342b1fa6.js";function Pt(k){let t,m,a='<a href="../model_doc/albert">ALBERT</a>, <a href="../model_doc/bart">BART</a>, <a href="../model_doc/bert">BERT</a>, <a href="../model_doc/big_bird">BigBird</a>, <a href="../model_doc/bigbird_pegasus">BigBird-Pegasus</a>, <a href="../model_doc/biogpt">BioGpt</a>, <a href="../model_doc/bloom">BLOOM</a>, <a href="../model_doc/camembert">CamemBERT</a>, <a href="../model_doc/canine">CANINE</a>, <a href="../model_doc/code_llama">CodeLlama</a>, <a href="../model_doc/convbert">ConvBERT</a>, <a href="../model_doc/ctrl">CTRL</a>, <a href="../model_doc/data2vec-text">Data2VecText</a>, <a href="../model_doc/deberta">DeBERTa</a>, <a href="../model_doc/deberta-v2">DeBERTa-v2</a>, <a href="../model_doc/distilbert">DistilBERT</a>, <a href="../model_doc/electra">ELECTRA</a>, <a href="../model_doc/ernie">ERNIE</a>, <a href="../model_doc/ernie_m">ErnieM</a>, <a href="../model_doc/esm">ESM</a>, <a href="../model_doc/falcon">Falcon</a>, <a href="../model_doc/flaubert">FlauBERT</a>, <a href="../model_doc/fnet">FNet</a>, <a href="../model_doc/funnel">Funnel Transformer</a>, <a href="../model_doc/gpt-sw3">GPT-Sw3</a>, <a href="../model_doc/gpt2">OpenAI GPT-2</a>, <a href="../model_doc/gpt_bigcode">GPTBigCode</a>, <a href="../model_doc/gpt_neo">GPT Neo</a>, <a href="../model_doc/gpt_neox">GPT NeoX</a>, <a href="../model_doc/gptj">GPT-J</a>, <a href="../model_doc/ibert">I-BERT</a>, <a href="../model_doc/layoutlm">LayoutLM</a>, <a href="../model_doc/layoutlmv2">LayoutLMv2</a>, <a href="../model_doc/layoutlmv3">LayoutLMv3</a>, <a href="../model_doc/led">LED</a>, <a href="../model_doc/lilt">LiLT</a>, <a href="../model_doc/llama">LLaMA</a>, <a href="../model_doc/longformer">Longformer</a>, <a href="../model_doc/luke">LUKE</a>, <a href="../model_doc/markuplm">MarkupLM</a>, <a href="../model_doc/mbart">mBART</a>, <a href="../model_doc/mega">MEGA</a>, <a href="../model_doc/megatron-bert">Megatron-BERT</a>, <a href="../model_doc/mistral">Mistral</a>, <a href="../model_doc/mixtral">Mixtral</a>, <a href="../model_doc/mobilebert">MobileBERT</a>, <a href="../model_doc/mpnet">MPNet</a>, <a href="../model_doc/mpt">MPT</a>, <a href="../model_doc/mra">MRA</a>, <a href="../model_doc/mt5">MT5</a>, <a href="../model_doc/mvp">MVP</a>, <a href="../model_doc/nezha">Nezha</a>, <a href="../model_doc/nystromformer">Nyströmformer</a>, <a href="../model_doc/open-llama">OpenLlama</a>, <a href="../model_doc/openai-gpt">OpenAI GPT</a>, <a href="../model_doc/opt">OPT</a>, <a href="../model_doc/perceiver">Perceiver</a>, <a href="../model_doc/persimmon">Persimmon</a>, <a href="../model_doc/phi">Phi</a>, <a href="../model_doc/plbart">PLBart</a>, <a href="../model_doc/qdqbert">QDQBert</a>, <a href="../model_doc/qwen2">Qwen2</a>, <a href="../model_doc/reformer">Reformer</a>, <a href="../model_doc/rembert">RemBERT</a>, <a href="../model_doc/roberta">RoBERTa</a>, <a href="../model_doc/roberta-prelayernorm">RoBERTa-PreLayerNorm</a>, <a href="../model_doc/roc_bert">RoCBert</a>, <a href="../model_doc/roformer">RoFormer</a>, <a href="../model_doc/squeezebert">SqueezeBERT</a>, <a href="../model_doc/stablelm">StableLm</a>, <a href="../model_doc/t5">T5</a>, <a href="../model_doc/tapas">TAPAS</a>, <a href="../model_doc/transfo-xl">Transformer-XL</a>, <a href="../model_doc/umt5">UMT5</a>, <a href="../model_doc/xlm">XLM</a>, <a href="../model_doc/xlm-roberta">XLM-RoBERTa</a>, <a href="../model_doc/xlm-roberta-xl">XLM-RoBERTa-XL</a>, <a href="../model_doc/xlnet">XLNet</a>, <a href="../model_doc/xmod">X-MOD</a>, <a href="../model_doc/yoso">YOSO</a>';return{c(){t=Ht(`The task illustrated in this tutorial is supported by the following model architectures:

`),m=$("p"),m.innerHTML=a},l(c){t=Nt(c,`The task illustrated in this tutorial is supported by the following model architectures:

`),m=w(c,"P",{"data-svelte-h":!0}),_(m)!=="svelte-1jhr4zc"&&(m.innerHTML=a)},m(c,M){l(c,t,M),l(c,m,M)},p:re,d(c){c&&(s(t),s(m))}}}function Dt(k){let t,m;return t=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nJTBBJTBBZGF0YV9jb2xsYXRvciUyMCUzRCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nKHRva2VuaXplciUzRHRva2VuaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,wrap:!1}}),{c(){d(t.$$.fragment)},l(a){h(t.$$.fragment,a)},m(a,c){u(t,a,c),m=!0},p:re,i(a){m||(y(t.$$.fragment,a),m=!0)},o(a){g(t.$$.fragment,a),m=!1},d(a){b(t,a)}}}function Kt(k){let t,m;return t=new He({props:{$$slots:{default:[Dt]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(a){h(t.$$.fragment,a)},m(a,c){u(t,a,c),m=!0},p(a,c){const M={};c&2&&(M.$$scope={dirty:c,ctx:a}),t.$set(M)},i(a){m||(y(t.$$.fragment,a),m=!0)},o(a){g(t.$$.fragment,a),m=!1},d(a){b(t,a)}}}function Ot(k){let t,m;return t=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nJTBBJTBBZGF0YV9jb2xsYXRvciUyMCUzRCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nKHRva2VuaXplciUzRHRva2VuaXplciUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIydGYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),{c(){d(t.$$.fragment)},l(a){h(t.$$.fragment,a)},m(a,c){u(t,a,c),m=!0},p:re,i(a){m||(y(t.$$.fragment,a),m=!0)},o(a){g(t.$$.fragment,a),m=!1},d(a){b(t,a)}}}function ea(k){let t,m;return t=new He({props:{$$slots:{default:[Ot]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(a){h(t.$$.fragment,a)},m(a,c){u(t,a,c),m=!0},p(a,c){const M={};c&2&&(M.$$scope={dirty:c,ctx:a}),t.$set(M)},i(a){m||(y(t.$$.fragment,a),m=!0)},o(a){g(t.$$.fragment,a),m=!1},d(a){b(t,a)}}}function ta(k){let t,m='If you aren’t familiar with finetuning a model with the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a>, take a look at the basic tutorial <a href="../training#train-with-pytorch-trainer">here</a>!';return{c(){t=$("p"),t.innerHTML=m},l(a){t=w(a,"P",{"data-svelte-h":!0}),_(t)!=="svelte-15s4um0"&&(t.innerHTML=m)},m(a,c){l(a,t,c)},p:re,d(a){a&&s(t)}}}function aa(k){let t,m='<a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> applies dynamic padding by default when you pass <code>tokenizer</code> to it. In this case, you don’t need to specify a data collator explicitly.';return{c(){t=$("p"),t.innerHTML=m},l(a){t=w(a,"P",{"data-svelte-h":!0}),_(t)!=="svelte-19w37zq"&&(t.innerHTML=m)},m(a,c){l(a,t,c)},p:re,d(a){a&&s(t)}}}function sa(k){let t,m,a,c='You’re ready to start training your model now! Load DistilBERT with <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification">AutoModelForSequenceClassification</a> along with the number of expected labels, and the label mappings:',M,v,V,W,Z="At this point, only three steps remain:",G,U,H='<li>Define your training hyperparameters in <a href="/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>. The only required parameter is <code>output_dir</code> which specifies where to save your model. You’ll push this model to the Hub by setting <code>push_to_hub=True</code> (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> will evaluate the accuracy and save the training checkpoint.</li> <li>Pass the training arguments to <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> along with the model, dataset, tokenizer, data collator, and <code>compute_metrics</code> function.</li> <li>Call <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train">train()</a> to finetune your model.</li>',R,J,X,o,j,B,F='Once training is completed, share your model to the Hub with the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub">push_to_hub()</a> method so everyone can use your model:',I,N,Y;return t=new ze({props:{$$slots:{default:[ta]},$$scope:{ctx:k}}}),v=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMkMlMjBUcmFpbmluZ0FyZ3VtZW50cyUyQyUyMFRyYWluZXIlMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZCUyMiUyQyUyMG51bV9sYWJlbHMlM0QyJTJDJTIwaWQybGFiZWwlM0RpZDJsYWJlbCUyQyUyMGxhYmVsMmlkJTNEbGFiZWwyaWQlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>, id2label=id2label, label2id=label2id
<span class="hljs-meta">... </span>)`,wrap:!1}}),J=new C({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJteV9hd2Vzb21lX21vZGVsJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNEMiUyQyUwQSUyMCUyMCUyMCUyMHdlaWdodF9kZWNheSUzRDAuMDElMkMlMEElMjAlMjAlMjAlMjBldmFsdWF0aW9uX3N0cmF0ZWd5JTNEJTIyZXBvY2glMjIlMkMlMEElMjAlMjAlMjAlMjBzYXZlX3N0cmF0ZWd5JTNEJTIyZXBvY2glMjIlMkMlMEElMjAlMjAlMjAlMjBsb2FkX2Jlc3RfbW9kZWxfYXRfZW5kJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHB1c2hfdG9faHViJTNEVHJ1ZSUyQyUwQSklMEElMEF0cmFpbmVyJTIwJTNEJTIwVHJhaW5lciglMEElMjAlMjAlMjAlMjBtb2RlbCUzRG1vZGVsJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEdG9rZW5pemVkX2ltZGIlNUIlMjJ0cmFpbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfZGF0YXNldCUzRHRva2VuaXplZF9pbWRiJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRHRva2VuaXplciUyQyUwQSUyMCUyMCUyMCUyMGRhdGFfY29sbGF0b3IlM0RkYXRhX2NvbGxhdG9yJTJDJTBBJTIwJTIwJTIwJTIwY29tcHV0ZV9tZXRyaWNzJTNEY29tcHV0ZV9tZXRyaWNzJTJDJTBBKSUwQSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_model&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    load_best_model_at_end=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),o=new ze({props:{$$slots:{default:[aa]},$$scope:{ctx:k}}}),N=new C({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),{c(){d(t.$$.fragment),m=i(),a=$("p"),a.innerHTML=c,M=i(),d(v.$$.fragment),V=i(),W=$("p"),W.textContent=Z,G=i(),U=$("ol"),U.innerHTML=H,R=i(),d(J.$$.fragment),X=i(),d(o.$$.fragment),j=i(),B=$("p"),B.innerHTML=F,I=i(),d(N.$$.fragment)},l(f){h(t.$$.fragment,f),m=p(f),a=w(f,"P",{"data-svelte-h":!0}),_(a)!=="svelte-1pebxcc"&&(a.innerHTML=c),M=p(f),h(v.$$.fragment,f),V=p(f),W=w(f,"P",{"data-svelte-h":!0}),_(W)!=="svelte-l42k0i"&&(W.textContent=Z),G=p(f),U=w(f,"OL",{"data-svelte-h":!0}),_(U)!=="svelte-18vvra9"&&(U.innerHTML=H),R=p(f),h(J.$$.fragment,f),X=p(f),h(o.$$.fragment,f),j=p(f),B=w(f,"P",{"data-svelte-h":!0}),_(B)!=="svelte-1v13hlo"&&(B.innerHTML=F),I=p(f),h(N.$$.fragment,f)},m(f,x){u(t,f,x),l(f,m,x),l(f,a,x),l(f,M,x),u(v,f,x),l(f,V,x),l(f,W,x),l(f,G,x),l(f,U,x),l(f,R,x),u(J,f,x),l(f,X,x),u(o,f,x),l(f,j,x),l(f,B,x),l(f,I,x),u(N,f,x),Y=!0},p(f,x){const E={};x&2&&(E.$$scope={dirty:x,ctx:f}),t.$set(E);const z={};x&2&&(z.$$scope={dirty:x,ctx:f}),o.$set(z)},i(f){Y||(y(t.$$.fragment,f),y(v.$$.fragment,f),y(J.$$.fragment,f),y(o.$$.fragment,f),y(N.$$.fragment,f),Y=!0)},o(f){g(t.$$.fragment,f),g(v.$$.fragment,f),g(J.$$.fragment,f),g(o.$$.fragment,f),g(N.$$.fragment,f),Y=!1},d(f){f&&(s(m),s(a),s(M),s(V),s(W),s(G),s(U),s(R),s(X),s(j),s(B),s(I)),b(t,f),b(v,f),b(J,f),b(o,f),b(N,f)}}}function la(k){let t,m;return t=new He({props:{$$slots:{default:[sa]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(a){h(t.$$.fragment,a)},m(a,c){u(t,a,c),m=!0},p(a,c){const M={};c&2&&(M.$$scope={dirty:c,ctx:a}),t.$set(M)},i(a){m||(y(t.$$.fragment,a),m=!0)},o(a){g(t.$$.fragment,a),m=!1},d(a){b(t,a)}}}function na(k){let t,m='If you aren’t familiar with finetuning a model with Keras, take a look at the basic tutorial <a href="../training#train-a-tensorflow-model-with-keras">here</a>!';return{c(){t=$("p"),t.innerHTML=m},l(a){t=w(a,"P",{"data-svelte-h":!0}),_(t)!=="svelte-1rd4nl8"&&(t.innerHTML=m)},m(a,c){l(a,t,c)},p:re,d(a){a&&s(t)}}}function ra(k){let t,m,a,c,M,v='Then you can load DistilBERT with <a href="/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification">TFAutoModelForSequenceClassification</a> along with the number of expected labels, and the label mappings:',V,W,Z,G,U='Convert your datasets to the <code>tf.data.Dataset</code> format with <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset">prepare_tf_dataset()</a>:',H,R,J,X,o='Configure the model for training with <a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a>. Note that Transformers models all have a default task-relevant loss function, so you don’t need to specify one unless you want to:',j,B,F,I,N='The last two things to setup before you start training is to compute the accuracy from the predictions, and provide a way to push your model to the Hub. Both are done by using <a href="../main_classes/keras_callbacks">Keras callbacks</a>.',Y,f,x='Pass your <code>compute_metrics</code> function to <a href="/docs/transformers/main/en/main_classes/keras_callbacks#transformers.KerasMetricCallback">KerasMetricCallback</a>:',E,z,Q,ae,ce='Specify where to push your model and tokenizer in the <a href="/docs/transformers/main/en/main_classes/keras_callbacks#transformers.PushToHubCallback">PushToHubCallback</a>:',L,q,S,A,se="Then bundle your callbacks together:",fe,P,D,K,le='Finally, you’re ready to start training your model! Call <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a> with your training and validation datasets, the number of epochs, and your callbacks to finetune the model:',de,O,ee,te,ne="Once training is completed, your model is automatically uploaded to the Hub so everyone can use it!",he;return t=new ze({props:{$$slots:{default:[na]},$$scope:{ctx:k}}}),a=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGNyZWF0ZV9vcHRpbWl6ZXIlMEFpbXBvcnQlMjB0ZW5zb3JmbG93JTIwYXMlMjB0ZiUwQSUwQWJhdGNoX3NpemUlMjAlM0QlMjAxNiUwQW51bV9lcG9jaHMlMjAlM0QlMjA1JTBBYmF0Y2hlc19wZXJfZXBvY2glMjAlM0QlMjBsZW4odG9rZW5pemVkX2ltZGIlNUIlMjJ0cmFpbiUyMiU1RCklMjAlMkYlMkYlMjBiYXRjaF9zaXplJTBBdG90YWxfdHJhaW5fc3RlcHMlMjAlM0QlMjBpbnQoYmF0Y2hlc19wZXJfZXBvY2glMjAqJTIwbnVtX2Vwb2NocyklMEFvcHRpbWl6ZXIlMkMlMjBzY2hlZHVsZSUyMCUzRCUyMGNyZWF0ZV9vcHRpbWl6ZXIoaW5pdF9sciUzRDJlLTUlMkMlMjBudW1fd2FybXVwX3N0ZXBzJTNEMCUyQyUyMG51bV90cmFpbl9zdGVwcyUzRHRvdGFsX3RyYWluX3N0ZXBzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
<span class="hljs-meta">&gt;&gt;&gt; </span>total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`,wrap:!1}}),W=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIlMkMlMjBudW1fbGFiZWxzJTNEMiUyQyUyMGlkMmxhYmVsJTNEaWQybGFiZWwlMkMlMjBsYWJlbDJpZCUzRGxhYmVsMmlkJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>, id2label=id2label, label2id=label2id
<span class="hljs-meta">... </span>)`,wrap:!1}}),R=new C({props:{code:"dGZfdHJhaW5fc2V0JTIwJTNEJTIwbW9kZWwucHJlcGFyZV90Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMHRva2VuaXplZF9pbWRiJTVCJTIydHJhaW4lMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBzaHVmZmxlJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGJhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMGNvbGxhdGVfZm4lM0RkYXRhX2NvbGxhdG9yJTJDJTBBKSUwQSUwQXRmX3ZhbGlkYXRpb25fc2V0JTIwJTNEJTIwbW9kZWwucHJlcGFyZV90Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMHRva2VuaXplZF9pbWRiJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHNodWZmbGUlM0RGYWxzZSUyQyUwQSUyMCUyMCUyMCUyMGJhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMGNvbGxhdGVfZm4lM0RkYXRhX2NvbGxhdG9yJTJDJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_validation_set = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`,wrap:!1}}),B=new C({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzRG9wdGltaXplciklMjAlMjAlMjMlMjBObyUyMGxvc3MlMjBhcmd1bWVudCE=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)  <span class="hljs-comment"># No loss argument!</span>`,wrap:!1}}),z=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBLZXJhc01ldHJpY0NhbGxiYWNrJTBBJTBBbWV0cmljX2NhbGxiYWNrJTIwJTNEJTIwS2VyYXNNZXRyaWNDYWxsYmFjayhtZXRyaWNfZm4lM0Rjb21wdXRlX21ldHJpY3MlMkMlMjBldmFsX2RhdGFzZXQlM0R0Zl92YWxpZGF0aW9uX3NldCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> KerasMetricCallback

<span class="hljs-meta">&gt;&gt;&gt; </span>metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)`,wrap:!1}}),q=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBQdXNoVG9IdWJDYWxsYmFjayUwQSUwQXB1c2hfdG9faHViX2NhbGxiYWNrJTIwJTNEJTIwUHVzaFRvSHViQ2FsbGJhY2soJTBBJTIwJTIwJTIwJTIwb3V0cHV0X2RpciUzRCUyMm15X2F3ZXNvbWVfbW9kZWwlMjIlMkMlMEElMjAlMjAlMjAlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

<span class="hljs-meta">&gt;&gt;&gt; </span>push_to_hub_callback = PushToHubCallback(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_model&quot;</span>,
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>)`,wrap:!1}}),P=new C({props:{code:"Y2FsbGJhY2tzJTIwJTNEJTIwJTVCbWV0cmljX2NhbGxiYWNrJTJDJTIwcHVzaF90b19odWJfY2FsbGJhY2slNUQ=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>callbacks = [metric_callback, push_to_hub_callback]',wrap:!1}}),O=new C({props:{code:"bW9kZWwuZml0KHglM0R0Zl90cmFpbl9zZXQlMkMlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl92YWxpZGF0aW9uX3NldCUyQyUyMGVwb2NocyUzRDMlMkMlMjBjYWxsYmFja3MlM0RjYWxsYmFja3Mp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=<span class="hljs-number">3</span>, callbacks=callbacks)',wrap:!1}}),{c(){d(t.$$.fragment),m=Ht(`
To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:

	`),d(a.$$.fragment),c=i(),M=$("p"),M.innerHTML=v,V=i(),d(W.$$.fragment),Z=i(),G=$("p"),G.innerHTML=U,H=i(),d(R.$$.fragment),J=i(),X=$("p"),X.innerHTML=o,j=i(),d(B.$$.fragment),F=i(),I=$("p"),I.innerHTML=N,Y=i(),f=$("p"),f.innerHTML=x,E=i(),d(z.$$.fragment),Q=i(),ae=$("p"),ae.innerHTML=ce,L=i(),d(q.$$.fragment),S=i(),A=$("p"),A.textContent=se,fe=i(),d(P.$$.fragment),D=i(),K=$("p"),K.innerHTML=le,de=i(),d(O.$$.fragment),ee=i(),te=$("p"),te.textContent=ne},l(r){h(t.$$.fragment,r),m=Nt(r,`
To finetune a model in TensorFlow, start by setting up an optimizer function, learning rate schedule, and some training hyperparameters:

	`),h(a.$$.fragment,r),c=p(r),M=w(r,"P",{"data-svelte-h":!0}),_(M)!=="svelte-1ggexyy"&&(M.innerHTML=v),V=p(r),h(W.$$.fragment,r),Z=p(r),G=w(r,"P",{"data-svelte-h":!0}),_(G)!=="svelte-9ymftz"&&(G.innerHTML=U),H=p(r),h(R.$$.fragment,r),J=p(r),X=w(r,"P",{"data-svelte-h":!0}),_(X)!=="svelte-17cxx5e"&&(X.innerHTML=o),j=p(r),h(B.$$.fragment,r),F=p(r),I=w(r,"P",{"data-svelte-h":!0}),_(I)!=="svelte-6l1wkp"&&(I.innerHTML=N),Y=p(r),f=w(r,"P",{"data-svelte-h":!0}),_(f)!=="svelte-bi2rpv"&&(f.innerHTML=x),E=p(r),h(z.$$.fragment,r),Q=p(r),ae=w(r,"P",{"data-svelte-h":!0}),_(ae)!=="svelte-1b3skyn"&&(ae.innerHTML=ce),L=p(r),h(q.$$.fragment,r),S=p(r),A=w(r,"P",{"data-svelte-h":!0}),_(A)!=="svelte-1lw9xm8"&&(A.textContent=se),fe=p(r),h(P.$$.fragment,r),D=p(r),K=w(r,"P",{"data-svelte-h":!0}),_(K)!=="svelte-1hrpv1v"&&(K.innerHTML=le),de=p(r),h(O.$$.fragment,r),ee=p(r),te=w(r,"P",{"data-svelte-h":!0}),_(te)!=="svelte-2s71om"&&(te.textContent=ne)},m(r,T){u(t,r,T),l(r,m,T),u(a,r,T),l(r,c,T),l(r,M,T),l(r,V,T),u(W,r,T),l(r,Z,T),l(r,G,T),l(r,H,T),u(R,r,T),l(r,J,T),l(r,X,T),l(r,j,T),u(B,r,T),l(r,F,T),l(r,I,T),l(r,Y,T),l(r,f,T),l(r,E,T),u(z,r,T),l(r,Q,T),l(r,ae,T),l(r,L,T),u(q,r,T),l(r,S,T),l(r,A,T),l(r,fe,T),u(P,r,T),l(r,D,T),l(r,K,T),l(r,de,T),u(O,r,T),l(r,ee,T),l(r,te,T),he=!0},p(r,T){const Ne={};T&2&&(Ne.$$scope={dirty:T,ctx:r}),t.$set(Ne)},i(r){he||(y(t.$$.fragment,r),y(a.$$.fragment,r),y(W.$$.fragment,r),y(R.$$.fragment,r),y(B.$$.fragment,r),y(z.$$.fragment,r),y(q.$$.fragment,r),y(P.$$.fragment,r),y(O.$$.fragment,r),he=!0)},o(r){g(t.$$.fragment,r),g(a.$$.fragment,r),g(W.$$.fragment,r),g(R.$$.fragment,r),g(B.$$.fragment,r),g(z.$$.fragment,r),g(q.$$.fragment,r),g(P.$$.fragment,r),g(O.$$.fragment,r),he=!1},d(r){r&&(s(m),s(c),s(M),s(V),s(Z),s(G),s(H),s(J),s(X),s(j),s(F),s(I),s(Y),s(f),s(E),s(Q),s(ae),s(L),s(S),s(A),s(fe),s(D),s(K),s(de),s(ee),s(te)),b(t,r),b(a,r),b(W,r),b(R,r),b(B,r),b(z,r),b(q,r),b(P,r),b(O,r)}}}function oa(k){let t,m;return t=new He({props:{$$slots:{default:[ra]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(a){h(t.$$.fragment,a)},m(a,c){u(t,a,c),m=!0},p(a,c){const M={};c&2&&(M.$$scope={dirty:c,ctx:a}),t.$set(M)},i(a){m||(y(t.$$.fragment,a),m=!0)},o(a){g(t.$$.fragment,a),m=!1},d(a){b(t,a)}}}function ia(k){let t,m=`For a more in-depth example of how to finetune a model for text classification, take a look at the corresponding
<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb" rel="nofollow">PyTorch notebook</a>
or <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb" rel="nofollow">TensorFlow notebook</a>.`;return{c(){t=$("p"),t.innerHTML=m},l(a){t=w(a,"P",{"data-svelte-h":!0}),_(t)!=="svelte-m6bho8"&&(t.innerHTML=m)},m(a,c){l(a,t,c)},p:re,d(a){a&&s(t)}}}function pa(k){let t,m="Tokenize the text and return PyTorch tensors:",a,c,M,v,V="Pass your inputs to the model and return the <code>logits</code>:",W,Z,G,U,H="Get the class with the highest probability, and use the model’s <code>id2label</code> mapping to convert it to a text label:",R,J,X;return c=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),Z=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMnN0ZXZobGl1JTJGbXlfYXdlc29tZV9tb2RlbCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits`,wrap:!1}}),J=new C({props:{code:"cHJlZGljdGVkX2NsYXNzX2lkJTIwJTNEJTIwbG9naXRzLmFyZ21heCgpLml0ZW0oKSUwQW1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9jbGFzc19pZCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;POSITIVE&#x27;</span>`,wrap:!1}}),{c(){t=$("p"),t.textContent=m,a=i(),d(c.$$.fragment),M=i(),v=$("p"),v.innerHTML=V,W=i(),d(Z.$$.fragment),G=i(),U=$("p"),U.innerHTML=H,R=i(),d(J.$$.fragment)},l(o){t=w(o,"P",{"data-svelte-h":!0}),_(t)!=="svelte-1qcz1wr"&&(t.textContent=m),a=p(o),h(c.$$.fragment,o),M=p(o),v=w(o,"P",{"data-svelte-h":!0}),_(v)!=="svelte-f3g043"&&(v.innerHTML=V),W=p(o),h(Z.$$.fragment,o),G=p(o),U=w(o,"P",{"data-svelte-h":!0}),_(U)!=="svelte-6mgrol"&&(U.innerHTML=H),R=p(o),h(J.$$.fragment,o)},m(o,j){l(o,t,j),l(o,a,j),u(c,o,j),l(o,M,j),l(o,v,j),l(o,W,j),u(Z,o,j),l(o,G,j),l(o,U,j),l(o,R,j),u(J,o,j),X=!0},p:re,i(o){X||(y(c.$$.fragment,o),y(Z.$$.fragment,o),y(J.$$.fragment,o),X=!0)},o(o){g(c.$$.fragment,o),g(Z.$$.fragment,o),g(J.$$.fragment,o),X=!1},d(o){o&&(s(t),s(a),s(M),s(v),s(W),s(G),s(U),s(R)),b(c,o),b(Z,o),b(J,o)}}}function ma(k){let t,m;return t=new He({props:{$$slots:{default:[pa]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(a){h(t.$$.fragment,a)},m(a,c){u(t,a,c),m=!0},p(a,c){const M={};c&2&&(M.$$scope={dirty:c,ctx:a}),t.$set(M)},i(a){m||(y(t.$$.fragment,a),m=!0)},o(a){g(t.$$.fragment,a),m=!1},d(a){b(t,a)}}}function ca(k){let t,m="Tokenize the text and return TensorFlow tensors:",a,c,M,v,V="Pass your inputs to the model and return the <code>logits</code>:",W,Z,G,U,H="Get the class with the highest probability, and use the model’s <code>id2label</code> mapping to convert it to a text label:",R,J,X;return c=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),Z=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfbW9kZWwlMjIpJTBBbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`,wrap:!1}}),J=new C({props:{code:"cHJlZGljdGVkX2NsYXNzX2lkJTIwJTNEJTIwaW50KHRmLm1hdGguYXJnbWF4KGxvZ2l0cyUyQyUyMGF4aXMlM0QtMSklNUIwJTVEKSUwQW1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9jbGFzc19pZCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;POSITIVE&#x27;</span>`,wrap:!1}}),{c(){t=$("p"),t.textContent=m,a=i(),d(c.$$.fragment),M=i(),v=$("p"),v.innerHTML=V,W=i(),d(Z.$$.fragment),G=i(),U=$("p"),U.innerHTML=H,R=i(),d(J.$$.fragment)},l(o){t=w(o,"P",{"data-svelte-h":!0}),_(t)!=="svelte-s1qr7b"&&(t.textContent=m),a=p(o),h(c.$$.fragment,o),M=p(o),v=w(o,"P",{"data-svelte-h":!0}),_(v)!=="svelte-f3g043"&&(v.innerHTML=V),W=p(o),h(Z.$$.fragment,o),G=p(o),U=w(o,"P",{"data-svelte-h":!0}),_(U)!=="svelte-6mgrol"&&(U.innerHTML=H),R=p(o),h(J.$$.fragment,o)},m(o,j){l(o,t,j),l(o,a,j),u(c,o,j),l(o,M,j),l(o,v,j),l(o,W,j),u(Z,o,j),l(o,G,j),l(o,U,j),l(o,R,j),u(J,o,j),X=!0},p:re,i(o){X||(y(c.$$.fragment,o),y(Z.$$.fragment,o),y(J.$$.fragment,o),X=!0)},o(o){g(c.$$.fragment,o),g(Z.$$.fragment,o),g(J.$$.fragment,o),X=!1},d(o){o&&(s(t),s(a),s(M),s(v),s(W),s(G),s(U),s(R)),b(c,o),b(Z,o),b(J,o)}}}function fa(k){let t,m;return t=new He({props:{$$slots:{default:[ca]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(a){h(t.$$.fragment,a)},m(a,c){u(t,a,c),m=!0},p(a,c){const M={};c&2&&(M.$$scope={dirty:c,ctx:a}),t.$set(M)},i(a){m||(y(t.$$.fragment,a),m=!0)},o(a){g(t.$$.fragment,a),m=!1},d(a){b(t,a)}}}function da(k){let t,m,a,c,M,v,V,W,Z,G,U,H="Text classification is a common NLP task that assigns a label or class to text. Some of the largest companies run text classification in production for a wide range of practical applications. One of the most popular forms of text classification is sentiment analysis, which assigns a label like 🙂 positive, 🙁 negative, or 😐 neutral to a sequence of text.",R,J,X="This guide will show you how to:",o,j,B='<li>Finetune <a href="https://huggingface.co/distilbert/distilbert-base-uncased" rel="nofollow">DistilBERT</a> on the <a href="https://huggingface.co/datasets/imdb" rel="nofollow">IMDb</a> dataset to determine whether a movie review is positive or negative.</li> <li>Use your finetuned model for inference.</li>',F,I,N,Y,f="Before you begin, make sure you have all the necessary libraries installed:",x,E,z,Q,ae="We encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:",ce,L,q,S,A,se,fe="Start by loading the IMDb dataset from the 🤗 Datasets library:",P,D,K,le,de="Then take a look at an example:",O,ee,te,ne,he="There are two fields in this dataset:",r,T,Ne="<li><code>text</code>: the movie review text.</li> <li><code>label</code>: a value that is either <code>0</code> for a negative review or <code>1</code> for a positive review.</li>",Fe,ue,Qe,ye,wt="The next step is to load a DistilBERT tokenizer to preprocess the <code>text</code> field:",Le,ge,qe,be,Tt="Create a preprocessing function to tokenize <code>text</code> and truncate sequences to be no longer than DistilBERT’s maximum input length:",Se,Me,Ae,$e,_t='To apply the preprocessing function over the entire dataset, use 🤗 Datasets <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map" rel="nofollow">map</a> function. You can speed up <code>map</code> by setting <code>batched=True</code> to process multiple elements of the dataset at once:',Pe,we,De,Te,jt='Now create a batch of examples using <a href="/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding">DataCollatorWithPadding</a>. It’s more efficient to <em>dynamically pad</em> the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.',Ke,oe,Oe,_e,et,je,Jt='Including a metric during training is often helpful for evaluating your model’s performance. You can quickly load a evaluation method with the 🤗 <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> library. For this task, load the <a href="https://huggingface.co/spaces/evaluate-metric/accuracy" rel="nofollow">accuracy</a> metric (see the 🤗 Evaluate <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">quick tour</a> to learn more about how to load and compute a metric):',tt,Je,at,Ue,Ut='Then create a function that passes your predictions and labels to <a href="https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute" rel="nofollow">compute</a> to calculate the accuracy:',st,ke,lt,ve,kt="Your <code>compute_metrics</code> function is ready to go now, and you’ll return to it when you setup your training.",nt,Ze,rt,Ce,vt="Before you start training your model, create a map of the expected ids to their labels with <code>id2label</code> and <code>label2id</code>:",ot,We,it,ie,pt,pe,mt,xe,ct,Ge,Zt="Great, now that you’ve finetuned a model, you can use it for inference!",ft,Re,Ct="Grab some text you’d like to run inference on:",dt,Xe,ht,Ve,Wt='The simplest way to try out your finetuned model for inference is to use it in a <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a>. Instantiate a <code>pipeline</code> for sentiment analysis with your model, and pass your text to it:',ut,Ie,yt,Be,xt="You can also manually replicate the results of the <code>pipeline</code> if you’d like:",gt,me,bt,Ee,Mt;return M=new Ye({props:{title:"Text classification",local:"text-classification",headingTag:"h1"}}),V=new At({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/sequence_classification.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/sequence_classification.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/sequence_classification.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/sequence_classification.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/sequence_classification.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/sequence_classification.ipynb"}]}}),Z=new St({props:{id:"leNG9fN9FQU"}}),I=new ze({props:{$$slots:{default:[Pt]},$$scope:{ctx:k}}}),E=new C({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGUlMjBhY2NlbGVyYXRl",highlighted:"pip install transformers datasets evaluate accelerate",wrap:!1}}),L=new C({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),S=new Ye({props:{title:"Load IMDb dataset",local:"load-imdb-dataset",headingTag:"h2"}}),D=new C({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBaW1kYiUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJpbWRiJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>imdb = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)`,wrap:!1}}),ee=new C({props:{code:"aW1kYiU1QiUyMnRlc3QlMjIlNUQlNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>imdb[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-number">0</span>]
{
    <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn&#x27;t match the background, and painfully one-dimensional characters cannot be overcome with a &#x27;sci-fi&#x27; setting. (I&#x27;m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It&#x27;s not. It&#x27;s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It&#x27;s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it&#x27;s rubbish as they have to always say \\&quot;Gene Roddenberry&#x27;s Earth...\\&quot; otherwise people would not continue watching. Roddenberry&#x27;s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.&quot;</span>,
}`,wrap:!1}}),ue=new Ye({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),ge=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),Me=new C({props:{code:"ZGVmJTIwcHJlcHJvY2Vzc19mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjByZXR1cm4lMjB0b2tlbml6ZXIoZXhhbXBsZXMlNUIlMjJ0ZXh0JTIyJTVEJTJDJTIwdHJ1bmNhdGlvbiUzRFRydWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], truncation=<span class="hljs-literal">True</span>)`,wrap:!1}}),we=new C({props:{code:"dG9rZW5pemVkX2ltZGIlMjAlM0QlMjBpbWRiLm1hcChwcmVwcm9jZXNzX2Z1bmN0aW9uJTJDJTIwYmF0Y2hlZCUzRFRydWUp",highlighted:'tokenized_imdb = imdb.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)',wrap:!1}}),oe=new $t({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ea],pytorch:[Kt]},$$scope:{ctx:k}}}),_e=new Ye({props:{title:"Evaluate",local:"evaluate",headingTag:"h2"}}),Je=new C({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFhY2N1cmFjeSUyMCUzRCUyMGV2YWx1YXRlLmxvYWQoJTIyYWNjdXJhY3klMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`,wrap:!1}}),ke=new C({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUyQyUyMGxhYmVscyUyMCUzRCUyMGV2YWxfcHJlZCUwQSUyMCUyMCUyMCUyMHByZWRpY3Rpb25zJTIwJTNEJTIwbnAuYXJnbWF4KHByZWRpY3Rpb25zJTJDJTIwYXhpcyUzRDEpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwYWNjdXJhY3kuY29tcHV0ZShwcmVkaWN0aW9ucyUzRHByZWRpY3Rpb25zJTJDJTIwcmVmZXJlbmNlcyUzRGxhYmVscyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    predictions, labels = eval_pred
<span class="hljs-meta">... </span>    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> accuracy.compute(predictions=predictions, references=labels)`,wrap:!1}}),Ze=new Ye({props:{title:"Train",local:"train",headingTag:"h2"}}),We=new C({props:{code:"aWQybGFiZWwlMjAlM0QlMjAlN0IwJTNBJTIwJTIyTkVHQVRJVkUlMjIlMkMlMjAxJTNBJTIwJTIyUE9TSVRJVkUlMjIlN0QlMEFsYWJlbDJpZCUyMCUzRCUyMCU3QiUyMk5FR0FUSVZFJTIyJTNBJTIwMCUyQyUyMCUyMlBPU0lUSVZFJTIyJTNBJTIwMSU3RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>id2label = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {<span class="hljs-string">&quot;NEGATIVE&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;POSITIVE&quot;</span>: <span class="hljs-number">1</span>}`,wrap:!1}}),ie=new $t({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[oa],pytorch:[la]},$$scope:{ctx:k}}}),pe=new ze({props:{$$slots:{default:[ia]},$$scope:{ctx:k}}}),xe=new Ye({props:{title:"Inference",local:"inference",headingTag:"h2"}}),Xe=new C({props:{code:"dGV4dCUyMCUzRCUyMCUyMlRoaXMlMjB3YXMlMjBhJTIwbWFzdGVycGllY2UuJTIwTm90JTIwY29tcGxldGVseSUyMGZhaXRoZnVsJTIwdG8lMjB0aGUlMjBib29rcyUyQyUyMGJ1dCUyMGVudGhyYWxsaW5nJTIwZnJvbSUyMGJlZ2lubmluZyUyMHRvJTIwZW5kLiUyME1pZ2h0JTIwYmUlMjBteSUyMGZhdm9yaXRlJTIwb2YlMjB0aGUlMjB0aHJlZS4lMjI=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.&quot;</span>',wrap:!1}}),Ie=new C({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMnNlbnRpbWVudC1hbmFseXNpcyUyMiUyQyUyMG1vZGVsJTNEJTIyc3RldmhsaXUlMkZteV9hd2Vzb21lX21vZGVsJTIyKSUwQWNsYXNzaWZpZXIodGV4dCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(text)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9994940757751465</span>}]`,wrap:!1}}),me=new $t({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[fa],pytorch:[ma]},$$scope:{ctx:k}}}),{c(){t=$("meta"),m=i(),a=$("p"),c=i(),d(M.$$.fragment),v=i(),d(V.$$.fragment),W=i(),d(Z.$$.fragment),G=i(),U=$("p"),U.textContent=H,R=i(),J=$("p"),J.textContent=X,o=i(),j=$("ol"),j.innerHTML=B,F=i(),d(I.$$.fragment),N=i(),Y=$("p"),Y.textContent=f,x=i(),d(E.$$.fragment),z=i(),Q=$("p"),Q.textContent=ae,ce=i(),d(L.$$.fragment),q=i(),d(S.$$.fragment),A=i(),se=$("p"),se.textContent=fe,P=i(),d(D.$$.fragment),K=i(),le=$("p"),le.textContent=de,O=i(),d(ee.$$.fragment),te=i(),ne=$("p"),ne.textContent=he,r=i(),T=$("ul"),T.innerHTML=Ne,Fe=i(),d(ue.$$.fragment),Qe=i(),ye=$("p"),ye.innerHTML=wt,Le=i(),d(ge.$$.fragment),qe=i(),be=$("p"),be.innerHTML=Tt,Se=i(),d(Me.$$.fragment),Ae=i(),$e=$("p"),$e.innerHTML=_t,Pe=i(),d(we.$$.fragment),De=i(),Te=$("p"),Te.innerHTML=jt,Ke=i(),d(oe.$$.fragment),Oe=i(),d(_e.$$.fragment),et=i(),je=$("p"),je.innerHTML=Jt,tt=i(),d(Je.$$.fragment),at=i(),Ue=$("p"),Ue.innerHTML=Ut,st=i(),d(ke.$$.fragment),lt=i(),ve=$("p"),ve.innerHTML=kt,nt=i(),d(Ze.$$.fragment),rt=i(),Ce=$("p"),Ce.innerHTML=vt,ot=i(),d(We.$$.fragment),it=i(),d(ie.$$.fragment),pt=i(),d(pe.$$.fragment),mt=i(),d(xe.$$.fragment),ct=i(),Ge=$("p"),Ge.textContent=Zt,ft=i(),Re=$("p"),Re.textContent=Ct,dt=i(),d(Xe.$$.fragment),ht=i(),Ve=$("p"),Ve.innerHTML=Wt,ut=i(),d(Ie.$$.fragment),yt=i(),Be=$("p"),Be.innerHTML=xt,gt=i(),d(me.$$.fragment),bt=i(),Ee=$("p"),this.h()},l(e){const n=Lt("svelte-u9bgzb",document.head);t=w(n,"META",{name:!0,content:!0}),n.forEach(s),m=p(e),a=w(e,"P",{}),Bt(a).forEach(s),c=p(e),h(M.$$.fragment,e),v=p(e),h(V.$$.fragment,e),W=p(e),h(Z.$$.fragment,e),G=p(e),U=w(e,"P",{"data-svelte-h":!0}),_(U)!=="svelte-a44la3"&&(U.textContent=H),R=p(e),J=w(e,"P",{"data-svelte-h":!0}),_(J)!=="svelte-1aff4p7"&&(J.textContent=X),o=p(e),j=w(e,"OL",{"data-svelte-h":!0}),_(j)!=="svelte-g478nj"&&(j.innerHTML=B),F=p(e),h(I.$$.fragment,e),N=p(e),Y=w(e,"P",{"data-svelte-h":!0}),_(Y)!=="svelte-1c9nexd"&&(Y.textContent=f),x=p(e),h(E.$$.fragment,e),z=p(e),Q=w(e,"P",{"data-svelte-h":!0}),_(Q)!=="svelte-k76o1m"&&(Q.textContent=ae),ce=p(e),h(L.$$.fragment,e),q=p(e),h(S.$$.fragment,e),A=p(e),se=w(e,"P",{"data-svelte-h":!0}),_(se)!=="svelte-cx4bj0"&&(se.textContent=fe),P=p(e),h(D.$$.fragment,e),K=p(e),le=w(e,"P",{"data-svelte-h":!0}),_(le)!=="svelte-1m91ua0"&&(le.textContent=de),O=p(e),h(ee.$$.fragment,e),te=p(e),ne=w(e,"P",{"data-svelte-h":!0}),_(ne)!=="svelte-q802b4"&&(ne.textContent=he),r=p(e),T=w(e,"UL",{"data-svelte-h":!0}),_(T)!=="svelte-4d5l79"&&(T.innerHTML=Ne),Fe=p(e),h(ue.$$.fragment,e),Qe=p(e),ye=w(e,"P",{"data-svelte-h":!0}),_(ye)!=="svelte-1gepr51"&&(ye.innerHTML=wt),Le=p(e),h(ge.$$.fragment,e),qe=p(e),be=w(e,"P",{"data-svelte-h":!0}),_(be)!=="svelte-1xsc197"&&(be.innerHTML=Tt),Se=p(e),h(Me.$$.fragment,e),Ae=p(e),$e=w(e,"P",{"data-svelte-h":!0}),_($e)!=="svelte-sjtxhg"&&($e.innerHTML=_t),Pe=p(e),h(we.$$.fragment,e),De=p(e),Te=w(e,"P",{"data-svelte-h":!0}),_(Te)!=="svelte-pjl5y5"&&(Te.innerHTML=jt),Ke=p(e),h(oe.$$.fragment,e),Oe=p(e),h(_e.$$.fragment,e),et=p(e),je=w(e,"P",{"data-svelte-h":!0}),_(je)!=="svelte-j1ipe9"&&(je.innerHTML=Jt),tt=p(e),h(Je.$$.fragment,e),at=p(e),Ue=w(e,"P",{"data-svelte-h":!0}),_(Ue)!=="svelte-14irt3v"&&(Ue.innerHTML=Ut),st=p(e),h(ke.$$.fragment,e),lt=p(e),ve=w(e,"P",{"data-svelte-h":!0}),_(ve)!=="svelte-183aynn"&&(ve.innerHTML=kt),nt=p(e),h(Ze.$$.fragment,e),rt=p(e),Ce=w(e,"P",{"data-svelte-h":!0}),_(Ce)!=="svelte-18c6io4"&&(Ce.innerHTML=vt),ot=p(e),h(We.$$.fragment,e),it=p(e),h(ie.$$.fragment,e),pt=p(e),h(pe.$$.fragment,e),mt=p(e),h(xe.$$.fragment,e),ct=p(e),Ge=w(e,"P",{"data-svelte-h":!0}),_(Ge)!=="svelte-633ppb"&&(Ge.textContent=Zt),ft=p(e),Re=w(e,"P",{"data-svelte-h":!0}),_(Re)!=="svelte-o1jbfg"&&(Re.textContent=Ct),dt=p(e),h(Xe.$$.fragment,e),ht=p(e),Ve=w(e,"P",{"data-svelte-h":!0}),_(Ve)!=="svelte-m4fb1l"&&(Ve.innerHTML=Wt),ut=p(e),h(Ie.$$.fragment,e),yt=p(e),Be=w(e,"P",{"data-svelte-h":!0}),_(Be)!=="svelte-1njl8vm"&&(Be.innerHTML=xt),gt=p(e),h(me.$$.fragment,e),bt=p(e),Ee=w(e,"P",{}),Bt(Ee).forEach(s),this.h()},h(){Yt(t,"name","hf:doc:metadata"),Yt(t,"content",ha)},m(e,n){qt(document.head,t),l(e,m,n),l(e,a,n),l(e,c,n),u(M,e,n),l(e,v,n),u(V,e,n),l(e,W,n),u(Z,e,n),l(e,G,n),l(e,U,n),l(e,R,n),l(e,J,n),l(e,o,n),l(e,j,n),l(e,F,n),u(I,e,n),l(e,N,n),l(e,Y,n),l(e,x,n),u(E,e,n),l(e,z,n),l(e,Q,n),l(e,ce,n),u(L,e,n),l(e,q,n),u(S,e,n),l(e,A,n),l(e,se,n),l(e,P,n),u(D,e,n),l(e,K,n),l(e,le,n),l(e,O,n),u(ee,e,n),l(e,te,n),l(e,ne,n),l(e,r,n),l(e,T,n),l(e,Fe,n),u(ue,e,n),l(e,Qe,n),l(e,ye,n),l(e,Le,n),u(ge,e,n),l(e,qe,n),l(e,be,n),l(e,Se,n),u(Me,e,n),l(e,Ae,n),l(e,$e,n),l(e,Pe,n),u(we,e,n),l(e,De,n),l(e,Te,n),l(e,Ke,n),u(oe,e,n),l(e,Oe,n),u(_e,e,n),l(e,et,n),l(e,je,n),l(e,tt,n),u(Je,e,n),l(e,at,n),l(e,Ue,n),l(e,st,n),u(ke,e,n),l(e,lt,n),l(e,ve,n),l(e,nt,n),u(Ze,e,n),l(e,rt,n),l(e,Ce,n),l(e,ot,n),u(We,e,n),l(e,it,n),u(ie,e,n),l(e,pt,n),u(pe,e,n),l(e,mt,n),u(xe,e,n),l(e,ct,n),l(e,Ge,n),l(e,ft,n),l(e,Re,n),l(e,dt,n),u(Xe,e,n),l(e,ht,n),l(e,Ve,n),l(e,ut,n),u(Ie,e,n),l(e,yt,n),l(e,Be,n),l(e,gt,n),u(me,e,n),l(e,bt,n),l(e,Ee,n),Mt=!0},p(e,[n]){const Gt={};n&2&&(Gt.$$scope={dirty:n,ctx:e}),I.$set(Gt);const Rt={};n&2&&(Rt.$$scope={dirty:n,ctx:e}),oe.$set(Rt);const Xt={};n&2&&(Xt.$$scope={dirty:n,ctx:e}),ie.$set(Xt);const Vt={};n&2&&(Vt.$$scope={dirty:n,ctx:e}),pe.$set(Vt);const It={};n&2&&(It.$$scope={dirty:n,ctx:e}),me.$set(It)},i(e){Mt||(y(M.$$.fragment,e),y(V.$$.fragment,e),y(Z.$$.fragment,e),y(I.$$.fragment,e),y(E.$$.fragment,e),y(L.$$.fragment,e),y(S.$$.fragment,e),y(D.$$.fragment,e),y(ee.$$.fragment,e),y(ue.$$.fragment,e),y(ge.$$.fragment,e),y(Me.$$.fragment,e),y(we.$$.fragment,e),y(oe.$$.fragment,e),y(_e.$$.fragment,e),y(Je.$$.fragment,e),y(ke.$$.fragment,e),y(Ze.$$.fragment,e),y(We.$$.fragment,e),y(ie.$$.fragment,e),y(pe.$$.fragment,e),y(xe.$$.fragment,e),y(Xe.$$.fragment,e),y(Ie.$$.fragment,e),y(me.$$.fragment,e),Mt=!0)},o(e){g(M.$$.fragment,e),g(V.$$.fragment,e),g(Z.$$.fragment,e),g(I.$$.fragment,e),g(E.$$.fragment,e),g(L.$$.fragment,e),g(S.$$.fragment,e),g(D.$$.fragment,e),g(ee.$$.fragment,e),g(ue.$$.fragment,e),g(ge.$$.fragment,e),g(Me.$$.fragment,e),g(we.$$.fragment,e),g(oe.$$.fragment,e),g(_e.$$.fragment,e),g(Je.$$.fragment,e),g(ke.$$.fragment,e),g(Ze.$$.fragment,e),g(We.$$.fragment,e),g(ie.$$.fragment,e),g(pe.$$.fragment,e),g(xe.$$.fragment,e),g(Xe.$$.fragment,e),g(Ie.$$.fragment,e),g(me.$$.fragment,e),Mt=!1},d(e){e&&(s(m),s(a),s(c),s(v),s(W),s(G),s(U),s(R),s(J),s(o),s(j),s(F),s(N),s(Y),s(x),s(z),s(Q),s(ce),s(q),s(A),s(se),s(P),s(K),s(le),s(O),s(te),s(ne),s(r),s(T),s(Fe),s(Qe),s(ye),s(Le),s(qe),s(be),s(Se),s(Ae),s($e),s(Pe),s(De),s(Te),s(Ke),s(Oe),s(et),s(je),s(tt),s(at),s(Ue),s(st),s(lt),s(ve),s(nt),s(rt),s(Ce),s(ot),s(it),s(pt),s(mt),s(ct),s(Ge),s(ft),s(Re),s(dt),s(ht),s(Ve),s(ut),s(yt),s(Be),s(gt),s(bt),s(Ee)),s(t),b(M,e),b(V,e),b(Z,e),b(I,e),b(E,e),b(L,e),b(S,e),b(D,e),b(ee,e),b(ue,e),b(ge,e),b(Me,e),b(we,e),b(oe,e),b(_e,e),b(Je,e),b(ke,e),b(Ze,e),b(We,e),b(ie,e),b(pe,e),b(xe,e),b(Xe,e),b(Ie,e),b(me,e)}}}const ha='{"title":"Text classification","local":"text-classification","sections":[{"title":"Load IMDb dataset","local":"load-imdb-dataset","sections":[],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[],"depth":2},{"title":"Evaluate","local":"evaluate","sections":[],"depth":2},{"title":"Train","local":"train","sections":[],"depth":2},{"title":"Inference","local":"inference","sections":[],"depth":2}],"depth":1}';function ua(k){return zt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ja extends Ft{constructor(t){super(),Qt(this,t,ua,da,Et,{})}}export{ja as component};
