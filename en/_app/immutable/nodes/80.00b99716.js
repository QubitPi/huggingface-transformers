import{s as $s,f as Ts,o as ws,n as I}from"../chunks/scheduler.9bc65507.js";import{S as Bs,i as ks,g as c,s as r,r as u,A as Js,h as p,f as l,c as i,j as x,u as g,x as y,k as B,y as a,a as m,v as h,d as _,t as b,w as M}from"../chunks/index.707bf1b6.js";import{T as _t}from"../chunks/Tip.c2ecdbf4.js";import{D as Z}from"../chunks/Docstring.17db21ae.js";import{C as K}from"../chunks/CodeBlock.54a9f38d.js";import{E as O}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as R}from"../chunks/Heading.342b1fa6.js";function Cs(T){let t,f="Example:",o,d,v;return d=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMCglMEElMjAlMjAlMjAlMjBCbGlwMlZpc2lvbkNvbmZpZyUyQyUwQSUyMCUyMCUyMCUyMEJsaXAyUUZvcm1lckNvbmZpZyUyQyUwQSUyMCUyMCUyMCUyME9QVENvbmZpZyUyQyUwQSUyMCUyMCUyMCUyMEJsaXAyQ29uZmlnJTJDJTBBJTIwJTIwJTIwJTIwQmxpcDJGb3JDb25kaXRpb25hbEdlbmVyYXRpb24lMkMlMEEpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJsaXAyQ29uZmlnJTIwd2l0aCUyMFNhbGVzZm9yY2UlMkZibGlwMi1vcHQtMi43YiUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCbGlwMkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJsaXAyRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEJsaXAyRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZyUwQSUwQSUyMyUyMFdlJTIwY2FuJTIwYWxzbyUyMGluaXRpYWxpemUlMjBhJTIwQmxpcDJDb25maWclMjBmcm9tJTIwYSUyMEJsaXAyVmlzaW9uQ29uZmlnJTJDJTIwQmxpcDJRRm9ybWVyQ29uZmlnJTIwYW5kJTIwYW55JTIwUHJldHJhaW5lZENvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMEJMSVAtMiUyMHZpc2lvbiUyQyUyMEJMSVAtMiUyMFEtRm9ybWVyJTIwYW5kJTIwbGFuZ3VhZ2UlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb25zJTBBdmlzaW9uX2NvbmZpZyUyMCUzRCUyMEJsaXAyVmlzaW9uQ29uZmlnKCklMEFxZm9ybWVyX2NvbmZpZyUyMCUzRCUyMEJsaXAyUUZvcm1lckNvbmZpZygpJTBBdGV4dF9jb25maWclMjAlM0QlMjBPUFRDb25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMEJsaXAyQ29uZmlnLmZyb21fdGV4dF92aXNpb25fY29uZmlncyh2aXNpb25fY29uZmlnJTJDJTIwcWZvcm1lcl9jb25maWclMkMlMjB0ZXh0X2NvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    Blip2VisionConfig,
<span class="hljs-meta">... </span>    Blip2QFormerConfig,
<span class="hljs-meta">... </span>    OPTConfig,
<span class="hljs-meta">... </span>    Blip2Config,
<span class="hljs-meta">... </span>    Blip2ForConditionalGeneration,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Blip2Config with Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Blip2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Blip2ForConditionalGeneration (with random weights) from the Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2ForConditionalGeneration(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a Blip2Config from a Blip2VisionConfig, Blip2QFormerConfig and any PretrainedConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing BLIP-2 vision, BLIP-2 Q-Former and language model configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vision_config = Blip2VisionConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>qformer_config = Blip2QFormerConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>text_config = OPTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = Blip2Config.from_text_vision_configs(vision_config, qformer_config, text_config)`,wrap:!1}}),{c(){t=c("p"),t.textContent=f,o=r(),u(d.$$.fragment)},l(n){t=p(n,"P",{"data-svelte-h":!0}),y(t)!=="svelte-11lpom8"&&(t.textContent=f),o=i(n),g(d.$$.fragment,n)},m(n,$){m(n,t,$),m(n,o,$),h(d,n,$),v=!0},p:I,i(n){v||(_(d.$$.fragment,n),v=!0)},o(n){b(d.$$.fragment,n),v=!1},d(n){n&&(l(t),l(o)),M(d,n)}}}function xs(T){let t,f="Example:",o,d,v;return d=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJsaXAyVmlzaW9uQ29uZmlnJTJDJTIwQmxpcDJWaXNpb25Nb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCbGlwMlZpc2lvbkNvbmZpZyUyMHdpdGglMjBTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQmxpcDJWaXNpb25Db25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCbGlwMlZpc2lvbk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEJsaXAyVmlzaW9uTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Blip2VisionConfig, Blip2VisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Blip2VisionConfig with Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Blip2VisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Blip2VisionModel (with random weights) from the Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2VisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=c("p"),t.textContent=f,o=r(),u(d.$$.fragment)},l(n){t=p(n,"P",{"data-svelte-h":!0}),y(t)!=="svelte-11lpom8"&&(t.textContent=f),o=i(n),g(d.$$.fragment,n)},m(n,$){m(n,t,$),m(n,o,$),h(d,n,$),v=!0},p:I,i(n){v||(_(d.$$.fragment,n),v=!0)},o(n){b(d.$$.fragment,n),v=!1},d(n){n&&(l(t),l(o)),M(d,n)}}}function js(T){let t,f="Examples:",o,d,v;return d=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJsaXAyUUZvcm1lckNvbmZpZyUyQyUyMEJsaXAyUUZvcm1lck1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJMSVAtMiUyMFNhbGVzZm9yY2UlMkZibGlwMi1vcHQtMi43YiUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCbGlwMlFGb3JtZXJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBCbGlwMlFGb3JtZXJNb2RlbChjb25maWd1cmF0aW9uKSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Blip2QFormerConfig, Blip2QFormerModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BLIP-2 Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Blip2QFormerConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the Salesforce/blip2-opt-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2QFormerModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=c("p"),t.textContent=f,o=r(),u(d.$$.fragment)},l(n){t=p(n,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=f),o=i(n),g(d.$$.fragment,n)},m(n,$){m(n,t,$),m(n,o,$),h(d,n,$),v=!0},p:I,i(n){v||(_(d.$$.fragment,n),v=!0)},o(n){b(d.$$.fragment,n),v=!1},d(n){n&&(l(t),l(o)),M(d,n)}}}function Us(T){let t,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=f},l(o){t=p(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=f)},m(o,d){m(o,t,d)},p:I,d(o){o&&l(t)}}}function Zs(T){let t,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=f},l(o){t=p(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=f)},m(o,d){m(o,t,d)},p:I,d(o){o&&l(t)}}}function Is(T){let t,f="Examples:",o,d,v;return d=new K({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQmxpcDJQcm9jZXNzb3IlMkMlMjBCbGlwMk1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEElMEFkZXZpY2UlMjAlM0QlMjAlMjJjdWRhJTIyJTIwaWYlMjB0b3JjaC5jdWRhLmlzX2F2YWlsYWJsZSgpJTIwZWxzZSUyMCUyMmNwdSUyMiUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEJsaXAyUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjIpJTBBbW9kZWwlMjAlM0QlMjBCbGlwMk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYpJTBBbW9kZWwudG8oZGV2aWNlKSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJRdWVzdGlvbiUzQSUyMGhvdyUyMG1hbnklMjBjYXRzJTIwYXJlJTIwdGhlcmUlM0YlMjBBbnN3ZXIlM0ElMjIlMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjB0ZXh0JTNEcHJvbXB0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlJTJDJTIwdG9yY2guZmxvYXQxNiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Blip2Processor, Blip2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Blip2Processor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2Model.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>, torch_dtype=torch.float16)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Question: how many cats are there? Answer:&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device, torch.float16)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)`,wrap:!1}}),{c(){t=c("p"),t.textContent=f,o=r(),u(d.$$.fragment)},l(n){t=p(n,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=f),o=i(n),g(d.$$.fragment,n)},m(n,$){m(n,t,$),m(n,o,$),h(d,n,$),v=!0},p:I,i(n){v||(_(d.$$.fragment,n),v=!0)},o(n){b(d.$$.fragment,n),v=!1},d(n){n&&(l(t),l(o)),M(d,n)}}}function zs(T){let t,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=f},l(o){t=p(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=f)},m(o,d){m(o,t,d)},p:I,d(o){o&&l(t)}}}function Ws(T){let t,f="Examples:",o,d,v;return d=new K({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Rva2VuaXplciUyQyUyMEJsaXAyTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEJsaXAyTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMlNhbGVzZm9yY2UlMkZibGlwMi1vcHQtMi43YiUyMiklMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJTYWxlc2ZvcmNlJTJGYmxpcDItb3B0LTIuN2IlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF0ZXh0X2ZlYXR1cmVzJTIwJTNEJTIwbW9kZWwuZ2V0X3RleHRfZmVhdHVyZXMoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, Blip2Model

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2Model.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){t=c("p"),t.textContent=f,o=r(),u(d.$$.fragment)},l(n){t=p(n,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=f),o=i(n),g(d.$$.fragment,n)},m(n,$){m(n,t,$),m(n,o,$),h(d,n,$),v=!0},p:I,i(n){v||(_(d.$$.fragment,n),v=!0)},o(n){b(d.$$.fragment,n),v=!1},d(n){n&&(l(t),l(o)),M(d,n)}}}function Fs(T){let t,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=f},l(o){t=p(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=f)},m(o,d){m(o,t,d)},p:I,d(o){o&&l(t)}}}function Ps(T){let t,f="Examples:",o,d,v;return d=new K({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvUHJvY2Vzc29yJTJDJTIwQmxpcDJNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQmxpcDJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIyKSUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMlNhbGVzZm9yY2UlMkZibGlwMi1vcHQtMi43YiUyMiklMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFpbWFnZV9vdXRwdXRzJTIwJTNEJTIwbW9kZWwuZ2V0X2ltYWdlX2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Blip2Model

<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2Model.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_outputs = model.get_image_features(**inputs)`,wrap:!1}}),{c(){t=c("p"),t.textContent=f,o=r(),u(d.$$.fragment)},l(n){t=p(n,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=f),o=i(n),g(d.$$.fragment,n)},m(n,$){m(n,t,$),m(n,o,$),h(d,n,$),v=!0},p:I,i(n){v||(_(d.$$.fragment,n),v=!0)},o(n){b(d.$$.fragment,n),v=!1},d(n){n&&(l(t),l(o)),M(d,n)}}}function Vs(T){let t,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=f},l(o){t=p(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=f)},m(o,d){m(o,t,d)},p:I,d(o){o&&l(t)}}}function Qs(T){let t,f="Examples:",o,d,v;return d=new K({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBCbGlwMlByb2Nlc3NvciUyQyUyMEJsaXAyTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBCbGlwMlByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIyKSUwQW1vZGVsJTIwJTNEJTIwQmxpcDJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXFmb3JtZXJfb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdldF9xZm9ybWVyX2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Blip2Processor, Blip2Model

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Blip2Processor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2Model.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>qformer_outputs = model.get_qformer_features(**inputs)`,wrap:!1}}),{c(){t=c("p"),t.textContent=f,o=r(),u(d.$$.fragment)},l(n){t=p(n,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=f),o=i(n),g(d.$$.fragment,n)},m(n,$){m(n,t,$),m(n,o,$),h(d,n,$),v=!0},p:I,i(n){v||(_(d.$$.fragment,n),v=!0)},o(n){b(d.$$.fragment,n),v=!1},d(n){n&&(l(t),l(o)),M(d,n)}}}function Ns(T){let t,f="Note that Flan-T5 checkpoints cannot be cast to float16. They are pre-trained using bfloat16.";return{c(){t=c("p"),t.textContent=f},l(o){t=p(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-1cxmmi2"&&(t.textContent=f)},m(o,d){m(o,t,d)},p:I,d(o){o&&l(t)}}}function Gs(T){let t,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=f},l(o){t=p(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=f)},m(o,d){m(o,t,d)},p:I,d(o){o&&l(t)}}}function qs(T){let t,f;return t=new K({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQmxpcDJQcm9jZXNzb3IlMkMlMjBCbGlwMkZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMGlmJTIwdG9yY2guY3VkYS5pc19hdmFpbGFibGUoKSUyMGVsc2UlMjAlMjJjcHUlMjIlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBCbGlwMlByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIyKSUwQW1vZGVsJTIwJTNEJTIwQmxpcDJGb3JDb25kaXRpb25hbEdlbmVyYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMlNhbGVzZm9yY2UlMkZibGlwMi1vcHQtMi43YiUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUlMkMlMjBkZXZpY2VfbWFwJTNEJTdCJTIyJTIyJTNBJTIwMCU3RCUyQyUyMHRvcmNoX2R0eXBlJTNEdG9yY2guZmxvYXQxNiUwQSklMjAlMjAlMjMlMjBkb2N0ZXN0JTNBJTIwJTJCSUdOT1JFX1JFU1VMVCUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Blip2Processor, Blip2ForConditionalGeneration
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = Blip2Processor.from_pretrained(<span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2ForConditionalGeneration.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, device_map={<span class="hljs-string">&quot;&quot;</span>: <span class="hljs-number">0</span>}, torch_dtype=torch.float16
<span class="hljs-meta">... </span>)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)`,wrap:!1}}),{c(){u(t.$$.fragment)},l(o){g(t.$$.fragment,o)},m(o,d){h(t,o,d),f=!0},p:I,i(o){f||(_(t.$$.fragment,o),f=!0)},o(o){b(t.$$.fragment,o),f=!1},d(o){M(t,o)}}}function Ls(T){let t,f="Image captioning (without providing a text prompt):",o,d,v;return d=new K({props:{code:"aW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlJTJDJTIwdG9yY2guZmxvYXQxNiklMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMpJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVELnN0cmlwKCklMEFwcmludChnZW5lcmF0ZWRfdGV4dCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device, torch.float16)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>].strip()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text)
two cats laying on a couch`,wrap:!1}}),{c(){t=c("p"),t.textContent=f,o=r(),u(d.$$.fragment)},l(n){t=p(n,"P",{"data-svelte-h":!0}),y(t)!=="svelte-1xz2grm"&&(t.textContent=f),o=i(n),g(d.$$.fragment,n)},m(n,$){m(n,t,$),m(n,o,$),h(d,n,$),v=!0},p:I,i(n){v||(_(d.$$.fragment,n),v=!0)},o(n){b(d.$$.fragment,n),v=!1},d(n){n&&(l(t),l(o)),M(d,n)}}}function Rs(T){let t,f="Visual question answering (prompt = question):",o,d,v;return d=new K({props:{code:"cHJvbXB0JTIwJTNEJTIwJTIyUXVlc3Rpb24lM0ElMjBob3clMjBtYW55JTIwY2F0cyUyMGFyZSUyMHRoZXJlJTNGJTIwQW5zd2VyJTNBJTIyJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwdGV4dCUzRHByb21wdCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKGRldmljZSUzRCUyMmN1ZGElMjIlMkMlMjBkdHlwZSUzRHRvcmNoLmZsb2F0MTYpJTBBJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQWdlbmVyYXRlZF90ZXh0JTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RC5zdHJpcCgpJTBBcHJpbnQoZ2VuZXJhdGVkX3RleHQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Question: how many cats are there? Answer:&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device=<span class="hljs-string">&quot;cuda&quot;</span>, dtype=torch.float16)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>].strip()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text)
two`,wrap:!1}}),{c(){t=c("p"),t.textContent=f,o=r(),u(d.$$.fragment)},l(n){t=p(n,"P",{"data-svelte-h":!0}),y(t)!=="svelte-dkukz2"&&(t.textContent=f),o=i(n),g(d.$$.fragment,n)},m(n,$){m(n,t,$),m(n,o,$),h(d,n,$),v=!0},p:I,i(n){v||(_(d.$$.fragment,n),v=!0)},o(n){b(d.$$.fragment,n),v=!1},d(n){n&&(l(t),l(o)),M(d,n)}}}function Xs(T){let t,f;return t=new K({props:{code:"bW9kZWwlMjAlM0QlMjBCbGlwMkZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyU2FsZXNmb3JjZSUyRmJsaXAyLW9wdC0yLjdiJTIyJTJDJTIwbG9hZF9pbl84Yml0JTNEVHJ1ZSUyQyUyMGRldmljZV9tYXAlM0QlN0IlMjIlMjIlM0ElMjAwJTdEJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiUwQSklMjAlMjAlMjMlMjBkb2N0ZXN0JTNBJTIwJTJCSUdOT1JFX1JFU1VMVCUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHRleHQlM0Rwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhkZXZpY2UlM0QlMjJjdWRhJTIyJTJDJTIwZHR5cGUlM0R0b3JjaC5iZmxvYXQxNiklMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMpJTBBZ2VuZXJhdGVkX3RleHQlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVELnN0cmlwKCklMEFwcmludChnZW5lcmF0ZWRfdGV4dCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>model = Blip2ForConditionalGeneration.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;Salesforce/blip2-opt-2.7b&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, device_map={<span class="hljs-string">&quot;&quot;</span>: <span class="hljs-number">0</span>}, torch_dtype=torch.bfloat16
<span class="hljs-meta">... </span>)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, text=prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device=<span class="hljs-string">&quot;cuda&quot;</span>, dtype=torch.bfloat16)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_text = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>].strip()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(generated_text)
two`,wrap:!1}}),{c(){u(t.$$.fragment)},l(o){g(t.$$.fragment,o)},m(o,d){h(t,o,d),f=!0},p:I,i(o){f||(_(t.$$.fragment,o),f=!0)},o(o){b(t.$$.fragment,o),f=!1},d(o){M(t,o)}}}function Hs(T){let t,f,o,d,v,n,$,At,xe,Wo=`The BLIP-2 model was proposed in <a href="https://arxiv.org/abs/2301.12597" rel="nofollow">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a> by
Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer
encoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon <a href="https://arxiv.org/abs/2204.14198" rel="nofollow">Flamingo</a>, an 80 billion parameter model, by 8.7%
on zero-shot VQAv2 with 54x fewer trainable parameters.`,Dt,je,Fo="The abstract from the paper is the following:",Ot,Ue,Po="<em>The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model’s emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.</em>",Kt,re,Vo,en,Ze,Qo='BLIP-2 architecture. Taken from the <a href="https://arxiv.org/abs/2301.12597">original paper.</a>',tn,Ie,No=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207" rel="nofollow">here</a>.`,nn,ze,on,We,Go='<li>BLIP-2 can be used for conditional text generation given an image and an optional text prompt. At inference time, it’s recommended to use the <code>generate</code> method.</li> <li>One can use <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> to prepare images for the model, and decode the predicted tokens ID’s back to text.</li>',sn,Fe,an,Pe,qo="A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BLIP-2.",rn,Ve,Lo='<li>Demo notebooks for BLIP-2 for image captioning, visual question answering (VQA) and chat-like conversations can be found <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BLIP-2" rel="nofollow">here</a>.</li>',ln,Qe,Ro="If you’re interested in submitting a resource to be included here, please feel free to open a Pull Request and we’ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",dn,Ne,cn,z,Ge,Cn,bt,Xo=`<a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a>. It is
used to instantiate a BLIP-2 model according to the specified arguments, defining the vision model, Q-Former model
and language model configs. Instantiating a configuration with the defaults will yield a similar configuration to
that of the BLIP-2 <a href="https://huggingface.co/Salesforce/blip2-opt-2.7b" rel="nofollow">Salesforce/blip2-opt-2.7b</a> architecture.`,xn,Mt,Ho=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,jn,ie,Un,le,qe,Zn,yt,Eo=`Instantiate a <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> (or a derived class) from a BLIP-2 vision model, Q-Former and language model
configurations.`,pn,Le,mn,F,Re,In,vt,So=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2VisionModel">Blip2VisionModel</a>. It is used to instantiate a
BLIP-2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration defaults will yield a similar configuration to that of the BLIP-2
<a href="https://huggingface.co/Salesforce/blip2-opt-2.7b" rel="nofollow">Salesforce/blip2-opt-2.7b</a> architecture.`,zn,$t,Yo=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Wn,de,fn,Xe,un,P,He,Fn,Tt,Ao=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2QFormerModel">Blip2QFormerModel</a>. It is used to instantiate a
BLIP-2 Querying Transformer (Q-Former) model according to the specified arguments, defining the model architecture.
Instantiating a configuration with the defaults will yield a similar configuration to that of the BLIP-2
<a href="https://huggingface.co/Salesforce/blip2-opt-2.7b" rel="nofollow">Salesforce/blip2-opt-2.7b</a> architecture. Configuration objects
inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the documentation from
<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Pn,wt,Do='Note that <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2QFormerModel">Blip2QFormerModel</a> is very similar to <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> with interleaved cross-attention.',Vn,ce,gn,Ee,hn,W,Se,Qn,Bt,Oo="Constructs a BLIP-2 processor which wraps a BLIP image processor and an OPT/T5 tokenizer into a single processor.",Nn,kt,Ko=`<a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipProcessor">BlipProcessor</a> offers all the functionalities of <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> and <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See the docstring
of <code>__call__()</code> and <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipProcessor.decode">decode()</a> for more information.`,Gn,pe,Ye,qn,Jt,es=`This method forwards all its arguments to PreTrainedTokenizer’s <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.batch_decode">batch_decode()</a>. Please
refer to the docstring of this method for more information.`,Ln,me,Ae,Rn,Ct,ts=`This method forwards all its arguments to PreTrainedTokenizer’s <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Tokenizer.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,_n,De,bn,ne,Oe,Xn,ee,Ke,Hn,xt,ns='The <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2VisionModel">Blip2VisionModel</a> forward method, overrides the <code>__call__</code> special method.',En,fe,Mn,et,yn,X,tt,Sn,jt,os="Querying Transformer (Q-Former), used in BLIP-2.",Yn,te,nt,An,Ut,ss=`encoder_hidden_states  (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <code>optional</code>):
Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if
the model is configured as a decoder.
encoder_attention_mask (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <code>optional</code>):
Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in
the cross-attention if the model is configured as a decoder. Mask values selected in <code>[0, 1]</code>:`,Dn,Zt,as=`<li>1 for tokens that are <strong>not masked</strong>,</li> <li>0 for tokens that are <strong>masked</strong>.
past_key_values (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of:
shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>): Contains precomputed key and
value hidden states of the attention blocks. Can be used to speed up decoding. If <code>past_key_values</code> are
used, the user can optionally input only the last <code>decoder_input_ids</code> (those that don’t have their past key
value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>decoder_input_ids</code> of shape
<code>(batch_size, sequence_length)</code>.
use_cache (<code>bool</code>, <code>optional</code>):
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).</li>`,vn,ot,$n,J,st,On,It,rs=`BLIP-2 Model for generating text and image features. The model consists of a vision encoder, Querying Transformer
(Q-Former) and a language model.`,Kn,zt,is=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,eo,Wt,ls=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,to,N,at,no,Ft,ds='The <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> forward method, overrides the <code>__call__</code> special method.',oo,ue,so,ge,ao,G,rt,ro,Pt,cs='The <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> forward method, overrides the <code>__call__</code> special method.',io,he,lo,_e,co,q,it,po,Vt,ps='The <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> forward method, overrides the <code>__call__</code> special method.',mo,be,fo,Me,uo,L,lt,go,Qt,ms='The <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> forward method, overrides the <code>__call__</code> special method.',ho,ye,_o,ve,Tn,dt,wn,C,ct,bo,Nt,fs=`BLIP-2 Model for generating text given an image and an optional text prompt. The model consists of a vision
encoder, Querying Transformer (Q-Former) and a language model.`,Mo,Gt,us=`One can optionally pass <code>input_ids</code> to the model, which serve as a text prompt, to make the language model continue
the prompt. Otherwise, the language model starts generating text from the [BOS] (beginning-of-sequence) token.`,yo,$e,vo,qt,gs=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,$o,Lt,hs=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,To,w,pt,wo,Rt,_s='The <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',Bo,Te,ko,Xt,bs="Examples:",Jo,Ht,Ms="Prepare processor, model and image input",Co,we,xo,Be,jo,ke,Uo,Et,ys=`Note that int8 inference is also supported through <a href="https://github.com/TimDettmers/bitsandbytes" rel="nofollow">bitsandbytes</a>.
This greatly reduces the amount of memory used by the model while maintaining the same performance.`,Zo,Je,Io,Ce,mt,zo,St,vs="Overrides <code>generate</code> function to be able to use the model as a conditional generator.",Bn,Yt,kn;return v=new R({props:{title:"BLIP-2",local:"blip-2",headingTag:"h1"}}),$=new R({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ze=new R({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),Fe=new R({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Ne=new R({props:{title:"Blip2Config",local:"transformers.Blip2Config",headingTag:"h2"}}),Ge=new Z({props:{name:"class transformers.Blip2Config",anchor:"transformers.Blip2Config",parameters:[{name:"vision_config",val:" = None"},{name:"qformer_config",val:" = None"},{name:"text_config",val:" = None"},{name:"num_query_tokens",val:" = 32"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Blip2Config.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2VisionConfig">Blip2VisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.Blip2Config.qformer_config",description:`<strong>qformer_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2QFormerConfig">Blip2QFormerConfig</a>.`,name:"qformer_config"},{anchor:"transformers.Blip2Config.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize any <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>.`,name:"text_config"},{anchor:"transformers.Blip2Config.num_query_tokens",description:`<strong>num_query_tokens</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The number of query tokens passed through the Transformer.`,name:"num_query_tokens"},{anchor:"transformers.Blip2Config.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/configuration_blip_2.py#L251"}}),ie=new O({props:{anchor:"transformers.Blip2Config.example",$$slots:{default:[Cs]},$$scope:{ctx:T}}}),qe=new Z({props:{name:"from_vision_qformer_text_configs",anchor:"transformers.Blip2Config.from_vision_qformer_text_configs",parameters:[{name:"vision_config",val:": Blip2VisionConfig"},{name:"qformer_config",val:": Blip2QFormerConfig"},{name:"text_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/configuration_blip_2.py#L335",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config"
>Blip2Config</a></p>
`}}),Le=new R({props:{title:"Blip2VisionConfig",local:"transformers.Blip2VisionConfig",headingTag:"h2"}}),Re=new Z({props:{name:"class transformers.Blip2VisionConfig",anchor:"transformers.Blip2VisionConfig",parameters:[{name:"hidden_size",val:" = 1408"},{name:"intermediate_size",val:" = 6144"},{name:"num_hidden_layers",val:" = 39"},{name:"num_attention_heads",val:" = 16"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 14"},{name:"hidden_act",val:" = 'gelu'"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 1e-10"},{name:"qkv_bias",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Blip2VisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1408) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.Blip2VisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 6144) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.Blip2VisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 39) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.Blip2VisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.Blip2VisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.Blip2VisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.Blip2VisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;gelu&quot;</code> are supported. layer_norm_eps (<code>float</code>, <em>optional</em>, defaults
to 1e-5): The epsilon used by the layer normalization layers.`,name:"hidden_act"},{anchor:"transformers.Blip2VisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.Blip2VisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Blip2VisionConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries and values in the self-attention layers.`,name:"qkv_bias"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/configuration_blip_2.py#L33"}}),de=new O({props:{anchor:"transformers.Blip2VisionConfig.example",$$slots:{default:[xs]},$$scope:{ctx:T}}}),Xe=new R({props:{title:"Blip2QFormerConfig",local:"transformers.Blip2QFormerConfig",headingTag:"h2"}}),He=new Z({props:{name:"class transformers.Blip2QFormerConfig",anchor:"transformers.Blip2QFormerConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"cross_attention_frequency",val:" = 2"},{name:"encoder_hidden_size",val:" = 1408"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Blip2QFormerConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Q-Former model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling the model.`,name:"vocab_size"},{anchor:"transformers.Blip2QFormerConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.Blip2QFormerConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.Blip2QFormerConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.Blip2QFormerConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.Blip2QFormerConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.Blip2QFormerConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.Blip2QFormerConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.Blip2QFormerConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.Blip2QFormerConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Blip2QFormerConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.Blip2QFormerConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.Blip2QFormerConfig.cross_attention_frequency",description:`<strong>cross_attention_frequency</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The frequency of adding cross-attention to the Transformer layers.`,name:"cross_attention_frequency"},{anchor:"transformers.Blip2QFormerConfig.encoder_hidden_size",description:`<strong>encoder_hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1408) &#x2014;
The hidden size of the hidden states for cross-attention.`,name:"encoder_hidden_size"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/configuration_blip_2.py#L132"}}),ce=new O({props:{anchor:"transformers.Blip2QFormerConfig.example",$$slots:{default:[js]},$$scope:{ctx:T}}}),Ee=new R({props:{title:"Blip2Processor",local:"transformers.Blip2Processor",headingTag:"h2"}}),Se=new Z({props:{name:"class transformers.Blip2Processor",anchor:"transformers.Blip2Processor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.Blip2Processor.image_processor",description:`<strong>image_processor</strong> (<code>BlipImageProcessor</code>) &#x2014;
An instance of <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a>. The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.Blip2Processor.tokenizer",description:"<strong>tokenizer</strong> (<code>AutoTokenizer</code>) &#x2014;\nAn instance of [&#x2018;PreTrainedTokenizer`]. The tokenizer is a required input.",name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/processing_blip_2.py#L27"}}),Ye=new Z({props:{name:"batch_decode",anchor:"transformers.Blip2Processor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/processing_blip_2.py#L135"}}),Ae=new Z({props:{name:"decode",anchor:"transformers.Blip2Processor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/processing_blip_2.py#L143"}}),De=new R({props:{title:"Blip2VisionModel",local:"transformers.Blip2VisionModel",headingTag:"h2"}}),Oe=new Z({props:{name:"class transformers.Blip2VisionModel",anchor:"transformers.Blip2VisionModel",parameters:[{name:"config",val:": Blip2VisionConfig"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L501"}}),Ke=new Z({props:{name:"forward",anchor:"transformers.Blip2VisionModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Blip2VisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for
details.`,name:"pixel_values"},{anchor:"transformers.Blip2VisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2VisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2VisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L516",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new _t({props:{$$slots:{default:[Us]},$$scope:{ctx:T}}}),et=new R({props:{title:"Blip2QFormerModel",local:"transformers.Blip2QFormerModel",headingTag:"h2"}}),tt=new Z({props:{name:"class transformers.Blip2QFormerModel",anchor:"transformers.Blip2QFormerModel",parameters:[{name:"config",val:": Blip2QFormerConfig"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L989"}}),nt=new Z({props:{name:"forward",anchor:"transformers.Blip2QFormerModel.forward",parameters:[{name:"query_embeds",val:": FloatTensor"},{name:"attention_mask",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"encoder_hidden_states",val:": Optional = None"},{name:"encoder_attention_mask",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L1064"}}),ot=new R({props:{title:"Blip2Model",local:"transformers.Blip2Model",headingTag:"h2"}}),st=new Z({props:{name:"class transformers.Blip2Model",anchor:"transformers.Blip2Model",parameters:[{name:"config",val:": Blip2Config"}],parametersDescription:[{anchor:"transformers.Blip2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L1178"}}),at=new Z({props:{name:"forward",anchor:"transformers.Blip2Model.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": FloatTensor"},{name:"attention_mask",val:": Optional = None"},{name:"decoder_input_ids",val:": Optional = None"},{name:"decoder_attention_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Blip2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for
details.`,name:"pixel_values"},{anchor:"transformers.Blip2Model.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be
provided to serve as text prompt, which the language model can continue.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Blip2Model.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Blip2Model.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an
encoder-decoder language model (like T5) is used.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#decoder-input-ids">What are decoder input IDs?</a>`,name:"decoder_input_ids"},{anchor:"transformers.Blip2Model.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.</p>
<p>Only relevant in case an encoder-decoder language model (like T5) is used.`,name:"decoder_attention_mask"},{anchor:"transformers.Blip2Model.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L1397",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) — Language modeling loss from the language model.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head of the language model.</li>
<li><strong>vision_outputs</strong> (<code>BaseModelOutputWithPooling</code>) — Outputs of the vision encoder.</li>
<li><strong>qformer_outputs</strong> (<code>BaseModelOutputWithPoolingAndCrossAttentions</code>) — Outputs of the Q-Former (Querying Transformer).</li>
<li><strong>language_model_outputs</strong> (<code>CausalLMOutputWithPast</code> or <code>Seq2SeqLMOutput</code>) — Outputs of the language model.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ue=new _t({props:{$$slots:{default:[Zs]},$$scope:{ctx:T}}}),ge=new O({props:{anchor:"transformers.Blip2Model.forward.example",$$slots:{default:[Is]},$$scope:{ctx:T}}}),rt=new Z({props:{name:"get_text_features",anchor:"transformers.Blip2Model.get_text_features",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"decoder_input_ids",val:": Optional = None"},{name:"decoder_attention_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Blip2Model.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Blip2Model.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.Blip2Model.get_text_features.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#decoder-input-ids">What are decoder input IDs?</a></p>
<p>T5 uses the <code>pad_token_id</code> as the starting token for <code>decoder_input_ids</code> generation. If <code>past_key_values</code>
is used, optionally only the last <code>decoder_input_ids</code> have to be input (see <code>past_key_values</code>).</p>
<p>To know more on how to prepare <code>decoder_input_ids</code> for pretraining take a look at <a href="./t5#training">T5
Training</a>.`,name:"decoder_input_ids"},{anchor:"transformers.Blip2Model.get_text_features.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.`,name:"decoder_attention_mask"},{anchor:"transformers.Blip2Model.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2Model.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2Model.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L1235",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The language model outputs. If <code>return_dict=True</code>, the output is a <code>CausalLMOutputWithPast</code> that
contains the language model logits, the past key values and the hidden states if
<code>output_hidden_states=True</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_outputs (<code>CausalLMOutputWithPast</code>, or <code>tuple(torch.FloatTensor)</code> if <code>return_dict=False</code>)</p>
`}}),he=new _t({props:{$$slots:{default:[zs]},$$scope:{ctx:T}}}),_e=new O({props:{anchor:"transformers.Blip2Model.get_text_features.example",$$slots:{default:[Ws]},$$scope:{ctx:T}}}),it=new Z({props:{name:"get_image_features",anchor:"transformers.Blip2Model.get_image_features",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Blip2Model.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for
details.`,name:"pixel_values"},{anchor:"transformers.Blip2Model.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2Model.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2Model.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L1294",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The vision model outputs. If <code>return_dict=True</code>, the output is a <code>BaseModelOutputWithPooling</code> that
contains the image features, the pooled image features and the hidden states if
<code>output_hidden_states=True</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>vision_outputs (<code>BaseModelOutputWithPooling</code> or tuple of <code>torch.FloatTensor</code>)</p>
`}}),be=new _t({props:{$$slots:{default:[Fs]},$$scope:{ctx:T}}}),Me=new O({props:{anchor:"transformers.Blip2Model.get_image_features.example",$$slots:{default:[Ps]},$$scope:{ctx:T}}}),lt=new Z({props:{name:"get_qformer_features",anchor:"transformers.Blip2Model.get_qformer_features",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Blip2Model.get_qformer_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for
details.`,name:"pixel_values"},{anchor:"transformers.Blip2Model.get_qformer_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be
provided to serve as text prompt, which the language model can continue.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Blip2Model.get_qformer_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Blip2Model.get_qformer_features.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an
encoder-decoder language model (like T5) is used.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#decoder-input-ids">What are decoder input IDs?</a>`,name:"decoder_input_ids"},{anchor:"transformers.Blip2Model.get_qformer_features.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.</p>
<p>Only relevant in case an encoder-decoder language model (like T5) is used.`,name:"decoder_attention_mask"},{anchor:"transformers.Blip2Model.get_qformer_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2Model.get_qformer_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2Model.get_qformer_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L1338",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The vision model outputs. If <code>return_dict=True</code>, the output is a <code>BaseModelOutputWithPooling</code> that
contains the image features, the pooled image features and the hidden states if
<code>output_hidden_states=True</code>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>vision_outputs (<code>BaseModelOutputWithPooling</code> or tuple of <code>torch.FloatTensor</code>)</p>
`}}),ye=new _t({props:{$$slots:{default:[Vs]},$$scope:{ctx:T}}}),ve=new O({props:{anchor:"transformers.Blip2Model.get_qformer_features.example",$$slots:{default:[Qs]},$$scope:{ctx:T}}}),dt=new R({props:{title:"Blip2ForConditionalGeneration",local:"transformers.Blip2ForConditionalGeneration",headingTag:"h2"}}),ct=new Z({props:{name:"class transformers.Blip2ForConditionalGeneration",anchor:"transformers.Blip2ForConditionalGeneration",parameters:[{name:"config",val:": Blip2Config"}],parametersDescription:[{anchor:"transformers.Blip2ForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L1524"}}),$e=new _t({props:{$$slots:{default:[Ns]},$$scope:{ctx:T}}}),pt=new Z({props:{name:"forward",anchor:"transformers.Blip2ForConditionalGeneration.forward",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": FloatTensor"},{name:"attention_mask",val:": Optional = None"},{name:"decoder_input_ids",val:": Optional = None"},{name:"decoder_attention_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Blip2ForConditionalGeneration.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for
details.`,name:"pixel_values"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be
provided to serve as text prompt, which the language model can continue.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a>. See <code>Blip2Processor.__call__()</code> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.decoder_input_ids",description:`<strong>decoder_input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an
encoder-decoder language model (like T5) is used.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#decoder-input-ids">What are decoder input IDs?</a>`,name:"decoder_input_ids"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.decoder_attention_mask",description:`<strong>decoder_attention_mask</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, target_sequence_length)</code>, <em>optional</em>) &#x2014;
Default behavior: generate a tensor that ignores pad tokens in <code>decoder_input_ids</code>. Causal mask will also
be used by default.</p>
<p>Only relevant in case an encoder-decoder language model (like T5) is used.`,name:"decoder_attention_mask"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Blip2ForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L1610",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.blip_2.configuration_blip_2.Blip2VisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>labels</code> is provided, <code>torch.FloatTensor</code> of shape <code>(1,)</code>) — Language modeling loss from the language model.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head of the language model.</li>
<li><strong>vision_outputs</strong> (<code>BaseModelOutputWithPooling</code>) — Outputs of the vision encoder.</li>
<li><strong>qformer_outputs</strong> (<code>BaseModelOutputWithPoolingAndCrossAttentions</code>) — Outputs of the Q-Former (Querying Transformer).</li>
<li><strong>language_model_outputs</strong> (<code>CausalLMOutputWithPast</code> or <code>Seq2SeqLMOutput</code>) — Outputs of the language model.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Te=new _t({props:{$$slots:{default:[Gs]},$$scope:{ctx:T}}}),we=new O({props:{anchor:"transformers.Blip2ForConditionalGeneration.forward.example",$$slots:{default:[qs]},$$scope:{ctx:T}}}),Be=new O({props:{anchor:"transformers.Blip2ForConditionalGeneration.forward.example-2",$$slots:{default:[Ls]},$$scope:{ctx:T}}}),ke=new O({props:{anchor:"transformers.Blip2ForConditionalGeneration.forward.example-3",$$slots:{default:[Rs]},$$scope:{ctx:T}}}),Je=new O({props:{anchor:"transformers.Blip2ForConditionalGeneration.forward.example-4",$$slots:{default:[Xs]},$$scope:{ctx:T}}}),mt=new Z({props:{name:"generate",anchor:"transformers.Blip2ForConditionalGeneration.generate",parameters:[{name:"pixel_values",val:": FloatTensor"},{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"**generate_kwargs",val:""}],parametersDescription:[{anchor:"transformers.Blip2ForConditionalGeneration.generate.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape (batch_size, num_channels, height, width)) &#x2014;
Input images to be processed.`,name:"pixel_values"},{anchor:"transformers.Blip2ForConditionalGeneration.generate.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape (batch_size, sequence_length), <em>optional</em>) &#x2014;
The sequence used as a prompt for the generation.`,name:"input_ids"},{anchor:"transformers.Blip2ForConditionalGeneration.generate.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.LongTensor</code> of shape (batch_size, sequence_length), <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices`,name:"attention_mask"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/blip_2/modeling_blip_2.py#L1773",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of strings of length batch_size * num_captions.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>captions (list)</p>
`}}),{c(){t=c("meta"),f=r(),o=c("p"),d=r(),u(v.$$.fragment),n=r(),u($.$$.fragment),At=r(),xe=c("p"),xe.innerHTML=Wo,Dt=r(),je=c("p"),je.textContent=Fo,Ot=r(),Ue=c("p"),Ue.innerHTML=Po,Kt=r(),re=c("img"),en=r(),Ze=c("small"),Ze.innerHTML=Qo,tn=r(),Ie=c("p"),Ie.innerHTML=No,nn=r(),u(ze.$$.fragment),on=r(),We=c("ul"),We.innerHTML=Go,sn=r(),u(Fe.$$.fragment),an=r(),Pe=c("p"),Pe.textContent=qo,rn=r(),Ve=c("ul"),Ve.innerHTML=Lo,ln=r(),Qe=c("p"),Qe.textContent=Ro,dn=r(),u(Ne.$$.fragment),cn=r(),z=c("div"),u(Ge.$$.fragment),Cn=r(),bt=c("p"),bt.innerHTML=Xo,xn=r(),Mt=c("p"),Mt.innerHTML=Ho,jn=r(),u(ie.$$.fragment),Un=r(),le=c("div"),u(qe.$$.fragment),Zn=r(),yt=c("p"),yt.innerHTML=Eo,pn=r(),u(Le.$$.fragment),mn=r(),F=c("div"),u(Re.$$.fragment),In=r(),vt=c("p"),vt.innerHTML=So,zn=r(),$t=c("p"),$t.innerHTML=Yo,Wn=r(),u(de.$$.fragment),fn=r(),u(Xe.$$.fragment),un=r(),P=c("div"),u(He.$$.fragment),Fn=r(),Tt=c("p"),Tt.innerHTML=Ao,Pn=r(),wt=c("p"),wt.innerHTML=Do,Vn=r(),u(ce.$$.fragment),gn=r(),u(Ee.$$.fragment),hn=r(),W=c("div"),u(Se.$$.fragment),Qn=r(),Bt=c("p"),Bt.textContent=Oo,Nn=r(),kt=c("p"),kt.innerHTML=Ko,Gn=r(),pe=c("div"),u(Ye.$$.fragment),qn=r(),Jt=c("p"),Jt.innerHTML=es,Ln=r(),me=c("div"),u(Ae.$$.fragment),Rn=r(),Ct=c("p"),Ct.innerHTML=ts,_n=r(),u(De.$$.fragment),bn=r(),ne=c("div"),u(Oe.$$.fragment),Xn=r(),ee=c("div"),u(Ke.$$.fragment),Hn=r(),xt=c("p"),xt.innerHTML=ns,En=r(),u(fe.$$.fragment),Mn=r(),u(et.$$.fragment),yn=r(),X=c("div"),u(tt.$$.fragment),Sn=r(),jt=c("p"),jt.textContent=os,Yn=r(),te=c("div"),u(nt.$$.fragment),An=r(),Ut=c("p"),Ut.innerHTML=ss,Dn=r(),Zt=c("ul"),Zt.innerHTML=as,vn=r(),u(ot.$$.fragment),$n=r(),J=c("div"),u(st.$$.fragment),On=r(),It=c("p"),It.textContent=rs,Kn=r(),zt=c("p"),zt.innerHTML=is,eo=r(),Wt=c("p"),Wt.innerHTML=ls,to=r(),N=c("div"),u(at.$$.fragment),no=r(),Ft=c("p"),Ft.innerHTML=ds,oo=r(),u(ue.$$.fragment),so=r(),u(ge.$$.fragment),ao=r(),G=c("div"),u(rt.$$.fragment),ro=r(),Pt=c("p"),Pt.innerHTML=cs,io=r(),u(he.$$.fragment),lo=r(),u(_e.$$.fragment),co=r(),q=c("div"),u(it.$$.fragment),po=r(),Vt=c("p"),Vt.innerHTML=ps,mo=r(),u(be.$$.fragment),fo=r(),u(Me.$$.fragment),uo=r(),L=c("div"),u(lt.$$.fragment),go=r(),Qt=c("p"),Qt.innerHTML=ms,ho=r(),u(ye.$$.fragment),_o=r(),u(ve.$$.fragment),Tn=r(),u(dt.$$.fragment),wn=r(),C=c("div"),u(ct.$$.fragment),bo=r(),Nt=c("p"),Nt.textContent=fs,Mo=r(),Gt=c("p"),Gt.innerHTML=us,yo=r(),u($e.$$.fragment),vo=r(),qt=c("p"),qt.innerHTML=gs,$o=r(),Lt=c("p"),Lt.innerHTML=hs,To=r(),w=c("div"),u(pt.$$.fragment),wo=r(),Rt=c("p"),Rt.innerHTML=_s,Bo=r(),u(Te.$$.fragment),ko=r(),Xt=c("p"),Xt.textContent=bs,Jo=r(),Ht=c("p"),Ht.textContent=Ms,Co=r(),u(we.$$.fragment),xo=r(),u(Be.$$.fragment),jo=r(),u(ke.$$.fragment),Uo=r(),Et=c("p"),Et.innerHTML=ys,Zo=r(),u(Je.$$.fragment),Io=r(),Ce=c("div"),u(mt.$$.fragment),zo=r(),St=c("p"),St.innerHTML=vs,Bn=r(),Yt=c("p"),this.h()},l(e){const s=Js("svelte-u9bgzb",document.head);t=p(s,"META",{name:!0,content:!0}),s.forEach(l),f=i(e),o=p(e,"P",{}),x(o).forEach(l),d=i(e),g(v.$$.fragment,e),n=i(e),g($.$$.fragment,e),At=i(e),xe=p(e,"P",{"data-svelte-h":!0}),y(xe)!=="svelte-1tx2g06"&&(xe.innerHTML=Wo),Dt=i(e),je=p(e,"P",{"data-svelte-h":!0}),y(je)!=="svelte-vfdo9a"&&(je.textContent=Fo),Ot=i(e),Ue=p(e,"P",{"data-svelte-h":!0}),y(Ue)!=="svelte-k65nu1"&&(Ue.innerHTML=Po),Kt=i(e),re=p(e,"IMG",{src:!0,alt:!0,width:!0}),en=i(e),Ze=p(e,"SMALL",{"data-svelte-h":!0}),y(Ze)!=="svelte-1f3k680"&&(Ze.innerHTML=Qo),tn=i(e),Ie=p(e,"P",{"data-svelte-h":!0}),y(Ie)!=="svelte-1xpociz"&&(Ie.innerHTML=No),nn=i(e),g(ze.$$.fragment,e),on=i(e),We=p(e,"UL",{"data-svelte-h":!0}),y(We)!=="svelte-160q6p0"&&(We.innerHTML=Go),sn=i(e),g(Fe.$$.fragment,e),an=i(e),Pe=p(e,"P",{"data-svelte-h":!0}),y(Pe)!=="svelte-11jcle7"&&(Pe.textContent=qo),rn=i(e),Ve=p(e,"UL",{"data-svelte-h":!0}),y(Ve)!=="svelte-1tw7src"&&(Ve.innerHTML=Lo),ln=i(e),Qe=p(e,"P",{"data-svelte-h":!0}),y(Qe)!=="svelte-1xesile"&&(Qe.textContent=Ro),dn=i(e),g(Ne.$$.fragment,e),cn=i(e),z=p(e,"DIV",{class:!0});var V=x(z);g(Ge.$$.fragment,V),Cn=i(V),bt=p(V,"P",{"data-svelte-h":!0}),y(bt)!=="svelte-19zzypg"&&(bt.innerHTML=Xo),xn=i(V),Mt=p(V,"P",{"data-svelte-h":!0}),y(Mt)!=="svelte-o55m63"&&(Mt.innerHTML=Ho),jn=i(V),g(ie.$$.fragment,V),Un=i(V),le=p(V,"DIV",{class:!0});var ft=x(le);g(qe.$$.fragment,ft),Zn=i(ft),yt=p(ft,"P",{"data-svelte-h":!0}),y(yt)!=="svelte-13ivq3o"&&(yt.innerHTML=Eo),ft.forEach(l),V.forEach(l),pn=i(e),g(Le.$$.fragment,e),mn=i(e),F=p(e,"DIV",{class:!0});var H=x(F);g(Re.$$.fragment,H),In=i(H),vt=p(H,"P",{"data-svelte-h":!0}),y(vt)!=="svelte-8lkgtn"&&(vt.innerHTML=So),zn=i(H),$t=p(H,"P",{"data-svelte-h":!0}),y($t)!=="svelte-o55m63"&&($t.innerHTML=Yo),Wn=i(H),g(de.$$.fragment,H),H.forEach(l),fn=i(e),g(Xe.$$.fragment,e),un=i(e),P=p(e,"DIV",{class:!0});var E=x(P);g(He.$$.fragment,E),Fn=i(E),Tt=p(E,"P",{"data-svelte-h":!0}),y(Tt)!=="svelte-t733gv"&&(Tt.innerHTML=Ao),Pn=i(E),wt=p(E,"P",{"data-svelte-h":!0}),y(wt)!=="svelte-fju7n4"&&(wt.innerHTML=Do),Vn=i(E),g(ce.$$.fragment,E),E.forEach(l),gn=i(e),g(Ee.$$.fragment,e),hn=i(e),W=p(e,"DIV",{class:!0});var Q=x(W);g(Se.$$.fragment,Q),Qn=i(Q),Bt=p(Q,"P",{"data-svelte-h":!0}),y(Bt)!=="svelte-qq98xh"&&(Bt.textContent=Oo),Nn=i(Q),kt=p(Q,"P",{"data-svelte-h":!0}),y(kt)!=="svelte-es6nc1"&&(kt.innerHTML=Ko),Gn=i(Q),pe=p(Q,"DIV",{class:!0});var ut=x(pe);g(Ye.$$.fragment,ut),qn=i(ut),Jt=p(ut,"P",{"data-svelte-h":!0}),y(Jt)!=="svelte-taj591"&&(Jt.innerHTML=es),ut.forEach(l),Ln=i(Q),me=p(Q,"DIV",{class:!0});var gt=x(me);g(Ae.$$.fragment,gt),Rn=i(gt),Ct=p(gt,"P",{"data-svelte-h":!0}),y(Ct)!=="svelte-mvw0hw"&&(Ct.innerHTML=ts),gt.forEach(l),Q.forEach(l),_n=i(e),g(De.$$.fragment,e),bn=i(e),ne=p(e,"DIV",{class:!0});var ht=x(ne);g(Oe.$$.fragment,ht),Xn=i(ht),ee=p(ht,"DIV",{class:!0});var oe=x(ee);g(Ke.$$.fragment,oe),Hn=i(oe),xt=p(oe,"P",{"data-svelte-h":!0}),y(xt)!=="svelte-1s0uc0f"&&(xt.innerHTML=ns),En=i(oe),g(fe.$$.fragment,oe),oe.forEach(l),ht.forEach(l),Mn=i(e),g(et.$$.fragment,e),yn=i(e),X=p(e,"DIV",{class:!0});var se=x(X);g(tt.$$.fragment,se),Sn=i(se),jt=p(se,"P",{"data-svelte-h":!0}),y(jt)!=="svelte-d9cja9"&&(jt.textContent=os),Yn=i(se),te=p(se,"DIV",{class:!0});var ae=x(te);g(nt.$$.fragment,ae),An=i(ae),Ut=p(ae,"P",{"data-svelte-h":!0}),y(Ut)!=="svelte-sj4g2x"&&(Ut.innerHTML=ss),Dn=i(ae),Zt=p(ae,"UL",{"data-svelte-h":!0}),y(Zt)!=="svelte-1mo0n1o"&&(Zt.innerHTML=as),ae.forEach(l),se.forEach(l),vn=i(e),g(ot.$$.fragment,e),$n=i(e),J=p(e,"DIV",{class:!0});var j=x(J);g(st.$$.fragment,j),On=i(j),It=p(j,"P",{"data-svelte-h":!0}),y(It)!=="svelte-1ib14ae"&&(It.textContent=rs),Kn=i(j),zt=p(j,"P",{"data-svelte-h":!0}),y(zt)!=="svelte-6pahdo"&&(zt.innerHTML=is),eo=i(j),Wt=p(j,"P",{"data-svelte-h":!0}),y(Wt)!=="svelte-hswkmf"&&(Wt.innerHTML=ls),to=i(j),N=p(j,"DIV",{class:!0});var S=x(N);g(at.$$.fragment,S),no=i(S),Ft=p(S,"P",{"data-svelte-h":!0}),y(Ft)!=="svelte-ypzrz"&&(Ft.innerHTML=ds),oo=i(S),g(ue.$$.fragment,S),so=i(S),g(ge.$$.fragment,S),S.forEach(l),ao=i(j),G=p(j,"DIV",{class:!0});var Y=x(G);g(rt.$$.fragment,Y),ro=i(Y),Pt=p(Y,"P",{"data-svelte-h":!0}),y(Pt)!=="svelte-ypzrz"&&(Pt.innerHTML=cs),io=i(Y),g(he.$$.fragment,Y),lo=i(Y),g(_e.$$.fragment,Y),Y.forEach(l),co=i(j),q=p(j,"DIV",{class:!0});var A=x(q);g(it.$$.fragment,A),po=i(A),Vt=p(A,"P",{"data-svelte-h":!0}),y(Vt)!=="svelte-ypzrz"&&(Vt.innerHTML=ps),mo=i(A),g(be.$$.fragment,A),fo=i(A),g(Me.$$.fragment,A),A.forEach(l),uo=i(j),L=p(j,"DIV",{class:!0});var D=x(L);g(lt.$$.fragment,D),go=i(D),Qt=p(D,"P",{"data-svelte-h":!0}),y(Qt)!=="svelte-ypzrz"&&(Qt.innerHTML=ms),ho=i(D),g(ye.$$.fragment,D),_o=i(D),g(ve.$$.fragment,D),D.forEach(l),j.forEach(l),Tn=i(e),g(dt.$$.fragment,e),wn=i(e),C=p(e,"DIV",{class:!0});var U=x(C);g(ct.$$.fragment,U),bo=i(U),Nt=p(U,"P",{"data-svelte-h":!0}),y(Nt)!=="svelte-1fsmwyj"&&(Nt.textContent=fs),Mo=i(U),Gt=p(U,"P",{"data-svelte-h":!0}),y(Gt)!=="svelte-1ks26sg"&&(Gt.innerHTML=us),yo=i(U),g($e.$$.fragment,U),vo=i(U),qt=p(U,"P",{"data-svelte-h":!0}),y(qt)!=="svelte-6pahdo"&&(qt.innerHTML=gs),$o=i(U),Lt=p(U,"P",{"data-svelte-h":!0}),y(Lt)!=="svelte-hswkmf"&&(Lt.innerHTML=hs),To=i(U),w=p(U,"DIV",{class:!0});var k=x(w);g(pt.$$.fragment,k),wo=i(k),Rt=p(k,"P",{"data-svelte-h":!0}),y(Rt)!=="svelte-1p9ou3n"&&(Rt.innerHTML=_s),Bo=i(k),g(Te.$$.fragment,k),ko=i(k),Xt=p(k,"P",{"data-svelte-h":!0}),y(Xt)!=="svelte-kvfsh7"&&(Xt.textContent=bs),Jo=i(k),Ht=p(k,"P",{"data-svelte-h":!0}),y(Ht)!=="svelte-1c4jfk4"&&(Ht.textContent=Ms),Co=i(k),g(we.$$.fragment,k),xo=i(k),g(Be.$$.fragment,k),jo=i(k),g(ke.$$.fragment,k),Uo=i(k),Et=p(k,"P",{"data-svelte-h":!0}),y(Et)!=="svelte-gy1q6u"&&(Et.innerHTML=ys),Zo=i(k),g(Je.$$.fragment,k),k.forEach(l),Io=i(U),Ce=p(U,"DIV",{class:!0});var Jn=x(Ce);g(mt.$$.fragment,Jn),zo=i(Jn),St=p(Jn,"P",{"data-svelte-h":!0}),y(St)!=="svelte-eq620n"&&(St.innerHTML=vs),Jn.forEach(l),U.forEach(l),Bn=i(e),Yt=p(e,"P",{}),x(Yt).forEach(l),this.h()},h(){B(t,"name","hf:doc:metadata"),B(t,"content",Es),Ts(re.src,Vo="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/blip2_architecture.jpg")||B(re,"src",Vo),B(re,"alt","drawing"),B(re,"width","600"),B(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){a(document.head,t),m(e,f,s),m(e,o,s),m(e,d,s),h(v,e,s),m(e,n,s),h($,e,s),m(e,At,s),m(e,xe,s),m(e,Dt,s),m(e,je,s),m(e,Ot,s),m(e,Ue,s),m(e,Kt,s),m(e,re,s),m(e,en,s),m(e,Ze,s),m(e,tn,s),m(e,Ie,s),m(e,nn,s),h(ze,e,s),m(e,on,s),m(e,We,s),m(e,sn,s),h(Fe,e,s),m(e,an,s),m(e,Pe,s),m(e,rn,s),m(e,Ve,s),m(e,ln,s),m(e,Qe,s),m(e,dn,s),h(Ne,e,s),m(e,cn,s),m(e,z,s),h(Ge,z,null),a(z,Cn),a(z,bt),a(z,xn),a(z,Mt),a(z,jn),h(ie,z,null),a(z,Un),a(z,le),h(qe,le,null),a(le,Zn),a(le,yt),m(e,pn,s),h(Le,e,s),m(e,mn,s),m(e,F,s),h(Re,F,null),a(F,In),a(F,vt),a(F,zn),a(F,$t),a(F,Wn),h(de,F,null),m(e,fn,s),h(Xe,e,s),m(e,un,s),m(e,P,s),h(He,P,null),a(P,Fn),a(P,Tt),a(P,Pn),a(P,wt),a(P,Vn),h(ce,P,null),m(e,gn,s),h(Ee,e,s),m(e,hn,s),m(e,W,s),h(Se,W,null),a(W,Qn),a(W,Bt),a(W,Nn),a(W,kt),a(W,Gn),a(W,pe),h(Ye,pe,null),a(pe,qn),a(pe,Jt),a(W,Ln),a(W,me),h(Ae,me,null),a(me,Rn),a(me,Ct),m(e,_n,s),h(De,e,s),m(e,bn,s),m(e,ne,s),h(Oe,ne,null),a(ne,Xn),a(ne,ee),h(Ke,ee,null),a(ee,Hn),a(ee,xt),a(ee,En),h(fe,ee,null),m(e,Mn,s),h(et,e,s),m(e,yn,s),m(e,X,s),h(tt,X,null),a(X,Sn),a(X,jt),a(X,Yn),a(X,te),h(nt,te,null),a(te,An),a(te,Ut),a(te,Dn),a(te,Zt),m(e,vn,s),h(ot,e,s),m(e,$n,s),m(e,J,s),h(st,J,null),a(J,On),a(J,It),a(J,Kn),a(J,zt),a(J,eo),a(J,Wt),a(J,to),a(J,N),h(at,N,null),a(N,no),a(N,Ft),a(N,oo),h(ue,N,null),a(N,so),h(ge,N,null),a(J,ao),a(J,G),h(rt,G,null),a(G,ro),a(G,Pt),a(G,io),h(he,G,null),a(G,lo),h(_e,G,null),a(J,co),a(J,q),h(it,q,null),a(q,po),a(q,Vt),a(q,mo),h(be,q,null),a(q,fo),h(Me,q,null),a(J,uo),a(J,L),h(lt,L,null),a(L,go),a(L,Qt),a(L,ho),h(ye,L,null),a(L,_o),h(ve,L,null),m(e,Tn,s),h(dt,e,s),m(e,wn,s),m(e,C,s),h(ct,C,null),a(C,bo),a(C,Nt),a(C,Mo),a(C,Gt),a(C,yo),h($e,C,null),a(C,vo),a(C,qt),a(C,$o),a(C,Lt),a(C,To),a(C,w),h(pt,w,null),a(w,wo),a(w,Rt),a(w,Bo),h(Te,w,null),a(w,ko),a(w,Xt),a(w,Jo),a(w,Ht),a(w,Co),h(we,w,null),a(w,xo),h(Be,w,null),a(w,jo),h(ke,w,null),a(w,Uo),a(w,Et),a(w,Zo),h(Je,w,null),a(C,Io),a(C,Ce),h(mt,Ce,null),a(Ce,zo),a(Ce,St),m(e,Bn,s),m(e,Yt,s),kn=!0},p(e,[s]){const V={};s&2&&(V.$$scope={dirty:s,ctx:e}),ie.$set(V);const ft={};s&2&&(ft.$$scope={dirty:s,ctx:e}),de.$set(ft);const H={};s&2&&(H.$$scope={dirty:s,ctx:e}),ce.$set(H);const E={};s&2&&(E.$$scope={dirty:s,ctx:e}),fe.$set(E);const Q={};s&2&&(Q.$$scope={dirty:s,ctx:e}),ue.$set(Q);const ut={};s&2&&(ut.$$scope={dirty:s,ctx:e}),ge.$set(ut);const gt={};s&2&&(gt.$$scope={dirty:s,ctx:e}),he.$set(gt);const ht={};s&2&&(ht.$$scope={dirty:s,ctx:e}),_e.$set(ht);const oe={};s&2&&(oe.$$scope={dirty:s,ctx:e}),be.$set(oe);const se={};s&2&&(se.$$scope={dirty:s,ctx:e}),Me.$set(se);const ae={};s&2&&(ae.$$scope={dirty:s,ctx:e}),ye.$set(ae);const j={};s&2&&(j.$$scope={dirty:s,ctx:e}),ve.$set(j);const S={};s&2&&(S.$$scope={dirty:s,ctx:e}),$e.$set(S);const Y={};s&2&&(Y.$$scope={dirty:s,ctx:e}),Te.$set(Y);const A={};s&2&&(A.$$scope={dirty:s,ctx:e}),we.$set(A);const D={};s&2&&(D.$$scope={dirty:s,ctx:e}),Be.$set(D);const U={};s&2&&(U.$$scope={dirty:s,ctx:e}),ke.$set(U);const k={};s&2&&(k.$$scope={dirty:s,ctx:e}),Je.$set(k)},i(e){kn||(_(v.$$.fragment,e),_($.$$.fragment,e),_(ze.$$.fragment,e),_(Fe.$$.fragment,e),_(Ne.$$.fragment,e),_(Ge.$$.fragment,e),_(ie.$$.fragment,e),_(qe.$$.fragment,e),_(Le.$$.fragment,e),_(Re.$$.fragment,e),_(de.$$.fragment,e),_(Xe.$$.fragment,e),_(He.$$.fragment,e),_(ce.$$.fragment,e),_(Ee.$$.fragment,e),_(Se.$$.fragment,e),_(Ye.$$.fragment,e),_(Ae.$$.fragment,e),_(De.$$.fragment,e),_(Oe.$$.fragment,e),_(Ke.$$.fragment,e),_(fe.$$.fragment,e),_(et.$$.fragment,e),_(tt.$$.fragment,e),_(nt.$$.fragment,e),_(ot.$$.fragment,e),_(st.$$.fragment,e),_(at.$$.fragment,e),_(ue.$$.fragment,e),_(ge.$$.fragment,e),_(rt.$$.fragment,e),_(he.$$.fragment,e),_(_e.$$.fragment,e),_(it.$$.fragment,e),_(be.$$.fragment,e),_(Me.$$.fragment,e),_(lt.$$.fragment,e),_(ye.$$.fragment,e),_(ve.$$.fragment,e),_(dt.$$.fragment,e),_(ct.$$.fragment,e),_($e.$$.fragment,e),_(pt.$$.fragment,e),_(Te.$$.fragment,e),_(we.$$.fragment,e),_(Be.$$.fragment,e),_(ke.$$.fragment,e),_(Je.$$.fragment,e),_(mt.$$.fragment,e),kn=!0)},o(e){b(v.$$.fragment,e),b($.$$.fragment,e),b(ze.$$.fragment,e),b(Fe.$$.fragment,e),b(Ne.$$.fragment,e),b(Ge.$$.fragment,e),b(ie.$$.fragment,e),b(qe.$$.fragment,e),b(Le.$$.fragment,e),b(Re.$$.fragment,e),b(de.$$.fragment,e),b(Xe.$$.fragment,e),b(He.$$.fragment,e),b(ce.$$.fragment,e),b(Ee.$$.fragment,e),b(Se.$$.fragment,e),b(Ye.$$.fragment,e),b(Ae.$$.fragment,e),b(De.$$.fragment,e),b(Oe.$$.fragment,e),b(Ke.$$.fragment,e),b(fe.$$.fragment,e),b(et.$$.fragment,e),b(tt.$$.fragment,e),b(nt.$$.fragment,e),b(ot.$$.fragment,e),b(st.$$.fragment,e),b(at.$$.fragment,e),b(ue.$$.fragment,e),b(ge.$$.fragment,e),b(rt.$$.fragment,e),b(he.$$.fragment,e),b(_e.$$.fragment,e),b(it.$$.fragment,e),b(be.$$.fragment,e),b(Me.$$.fragment,e),b(lt.$$.fragment,e),b(ye.$$.fragment,e),b(ve.$$.fragment,e),b(dt.$$.fragment,e),b(ct.$$.fragment,e),b($e.$$.fragment,e),b(pt.$$.fragment,e),b(Te.$$.fragment,e),b(we.$$.fragment,e),b(Be.$$.fragment,e),b(ke.$$.fragment,e),b(Je.$$.fragment,e),b(mt.$$.fragment,e),kn=!1},d(e){e&&(l(f),l(o),l(d),l(n),l(At),l(xe),l(Dt),l(je),l(Ot),l(Ue),l(Kt),l(re),l(en),l(Ze),l(tn),l(Ie),l(nn),l(on),l(We),l(sn),l(an),l(Pe),l(rn),l(Ve),l(ln),l(Qe),l(dn),l(cn),l(z),l(pn),l(mn),l(F),l(fn),l(un),l(P),l(gn),l(hn),l(W),l(_n),l(bn),l(ne),l(Mn),l(yn),l(X),l(vn),l($n),l(J),l(Tn),l(wn),l(C),l(Bn),l(Yt)),l(t),M(v,e),M($,e),M(ze,e),M(Fe,e),M(Ne,e),M(Ge),M(ie),M(qe),M(Le,e),M(Re),M(de),M(Xe,e),M(He),M(ce),M(Ee,e),M(Se),M(Ye),M(Ae),M(De,e),M(Oe),M(Ke),M(fe),M(et,e),M(tt),M(nt),M(ot,e),M(st),M(at),M(ue),M(ge),M(rt),M(he),M(_e),M(it),M(be),M(Me),M(lt),M(ye),M(ve),M(dt,e),M(ct),M($e),M(pt),M(Te),M(we),M(Be),M(ke),M(Je),M(mt)}}}const Es='{"title":"BLIP-2","local":"blip-2","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"Blip2Config","local":"transformers.Blip2Config","sections":[],"depth":2},{"title":"Blip2VisionConfig","local":"transformers.Blip2VisionConfig","sections":[],"depth":2},{"title":"Blip2QFormerConfig","local":"transformers.Blip2QFormerConfig","sections":[],"depth":2},{"title":"Blip2Processor","local":"transformers.Blip2Processor","sections":[],"depth":2},{"title":"Blip2VisionModel","local":"transformers.Blip2VisionModel","sections":[],"depth":2},{"title":"Blip2QFormerModel","local":"transformers.Blip2QFormerModel","sections":[],"depth":2},{"title":"Blip2Model","local":"transformers.Blip2Model","sections":[],"depth":2},{"title":"Blip2ForConditionalGeneration","local":"transformers.Blip2ForConditionalGeneration","sections":[],"depth":2}],"depth":1}';function Ss(T){return ws(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class na extends Bs{constructor(t){super(),ks(this,t,Ss,Hs,$s,{})}}export{na as component};
