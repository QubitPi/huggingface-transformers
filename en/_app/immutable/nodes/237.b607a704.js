import{s as Zt,o as en,n as zt}from"../chunks/scheduler.9bc65507.js";import{S as tn,i as nn,g as i,s as n,r as f,A as on,h as a,f as r,c as o,j as B,u as h,x as p,k as q,y as t,a as l,v as u,d as _,t as g,w as k}from"../chunks/index.707bf1b6.js";import{T as rn}from"../chunks/Tip.c2ecdbf4.js";import{D as P}from"../chunks/Docstring.17db21ae.js";import{C as Gt}from"../chunks/CodeBlock.54a9f38d.js";import{E as Yt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as Pe}from"../chunks/Heading.342b1fa6.js";function sn(F){let c,z="This model is in maintenance mode only, so we won’t accept any new PRs changing its code.",T,m,v=`If you run into any issues running this model, please reinstall the last version that supported this model: v4.30.0.
You can do so by running the following command: <code>pip install -U transformers==4.30.0</code>.`;return{c(){c=i("p"),c.textContent=z,T=n(),m=i("p"),m.innerHTML=v},l(s){c=a(s,"P",{"data-svelte-h":!0}),p(c)!=="svelte-lwu440"&&(c.textContent=z),T=o(s),m=a(s,"P",{"data-svelte-h":!0}),p(m)!=="svelte-4042uy"&&(m.innerHTML=v)},m(s,$){l(s,c,$),l(s,T,$),l(s,m,$)},p:zt,d(s){s&&(r(c),r(T),r(m))}}}function an(F){let c,z="pair mask has the following format:",T,m,v;return m=new Gt({props:{code:"MCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMEElN0MlMjBmaXJzdCUyMHNlcXVlbmNlJTIwJTIwJTIwJTIwJTdDJTIwc2Vjb25kJTIwc2VxdWVuY2UlMjAlN0M=",highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`,wrap:!1}}),{c(){c=i("p"),c.textContent=z,T=n(),f(m.$$.fragment)},l(s){c=a(s,"P",{"data-svelte-h":!0}),p(c)!=="svelte-qjgeij"&&(c.textContent=z),T=o(s),h(m.$$.fragment,s)},m(s,$){l(s,c,$),l(s,T,$),u(m,s,$),v=!0},p:zt,i(s){v||(_(m.$$.fragment,s),v=!0)},o(s){g(m.$$.fragment,s),v=!1},d(s){s&&(r(c),r(T)),k(m,s)}}}function dn(F){let c,z="pair mask has the following format:",T,m,v;return m=new Gt({props:{code:"MCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMEElN0MlMjBmaXJzdCUyMHNlcXVlbmNlJTIwJTIwJTIwJTIwJTdDJTIwc2Vjb25kJTIwc2VxdWVuY2UlMjAlN0M=",highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`,wrap:!1}}),{c(){c=i("p"),c.textContent=z,T=n(),f(m.$$.fragment)},l(s){c=a(s,"P",{"data-svelte-h":!0}),p(c)!=="svelte-qjgeij"&&(c.textContent=z),T=o(s),h(m.$$.fragment,s)},m(s,$){l(s,c,$),l(s,T,$),u(m,s,$),v=!0},p:zt,i(s){v||(_(m.$$.fragment,s),v=!0)},o(s){g(m.$$.fragment,s),v=!1},d(s){s&&(r(c),r(T)),k(m,s)}}}function cn(F){let c,z,T,m,v,s,$,Ie,O,je,W,Bt=`The RetriBERT model was proposed in the blog post <a href="https://yjernite.github.io/lfqa.html" rel="nofollow">Explain Anything Like I’m Five: A Model for Open Domain Long Form
Question Answering</a>. RetriBERT is a small model that uses either a single or
pair of BERT encoders with lower-dimension projection for dense semantic indexing of text.`,Ee,Q,qt=`This model was contributed by <a href="https://huggingface.co/yjernite" rel="nofollow">yjernite</a>. Code to train and use the model can be
found <a href="https://github.com/huggingface/transformers/tree/main/examples/research-projects/distillation" rel="nofollow">here</a>.`,Ae,K,Fe,L,X,Xe,le,Rt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a>. It is used to instantiate a
RetriBertModel model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the RetriBERT
<a href="https://huggingface.co/yjernite/retribert-base-uncased" rel="nofollow">yjernite/retribert-base-uncased</a> architecture.`,Ye,me,Mt=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,He,Y,Se,b,G,Ge,pe,Ct="Constructs a RetriBERT tokenizer.",Ze,fe,Lt=`<a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertTokenizer">RetriBertTokenizer</a> is identical to <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> and runs end-to-end tokenization: punctuation splitting
and wordpiece.`,et,he,Pt=`This tokenizer inherits from <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer
to: this superclass for more information regarding those methods.`,tt,D,Z,nt,ue,Dt=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`,ot,_e,It="<li>single sequence: <code>[CLS] X [SEP]</code></li> <li>pair of sequences: <code>[CLS] A [SEP] B [SEP]</code></li>",rt,H,ee,st,ge,jt="Converts a sequence of tokens (string) in a single string.",it,R,te,at,ke,Et="Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence",dt,S,ct,be,At="If <code>token_ids_1</code> is <code>None</code>, this method only returns the first portion of the mask (0s).",lt,J,ne,mt,Te,Ft=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Je,oe,Ue,x,re,pt,ve,Ht="Construct a “fast” RetriBERT tokenizer (backed by HuggingFace’s <em>tokenizers</em> library).",ft,$e,St=`<a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertTokenizerFast">RetriBertTokenizerFast</a> is identical to <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> and runs end-to-end tokenization: punctuation
splitting and wordpiece.`,ht,we,Jt=`This tokenizer inherits from <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`,ut,I,se,_t,xe,Ut=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`,gt,ye,Nt="<li>single sequence: <code>[CLS] X [SEP]</code></li> <li>pair of sequences: <code>[CLS] A [SEP] B [SEP]</code></li>",kt,M,ie,bt,ze,Vt="Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence",Tt,U,vt,Be,Ot="If <code>token_ids_1</code> is <code>None</code>, this method only returns the first portion of the mask (0s).",Ne,ae,Ve,y,de,$t,qe,Wt="Bert Based model to embed queries or document for document retrieval.",wt,Re,Qt=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,xt,Me,Kt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,yt,Ce,ce,Oe,De,We;return v=new Pe({props:{title:"RetriBERT",local:"retribert",headingTag:"h1"}}),$=new rn({props:{warning:!0,$$slots:{default:[sn]},$$scope:{ctx:F}}}),O=new Pe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),K=new Pe({props:{title:"RetriBertConfig",local:"transformers.RetriBertConfig",headingTag:"h2"}}),X=new P({props:{name:"class transformers.RetriBertConfig",anchor:"transformers.RetriBertConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 8"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"share_encoders",val:" = True"},{name:"projection_dim",val:" = 128"},{name:"pad_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RetriBertConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the RetriBERT model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a>`,name:"vocab_size"},{anchor:"transformers.RetriBertConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.RetriBertConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.RetriBertConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.RetriBertConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.RetriBertConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.RetriBertConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.RetriBertConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.RetriBertConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.RetriBertConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <em>token_type_ids</em> passed into <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertModel">BertModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.RetriBertConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.RetriBertConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.RetriBertConfig.share_encoders",description:`<strong>share_encoders</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use the same Bert-type encoder for the queries and document`,name:"share_encoders"},{anchor:"transformers.RetriBertConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Final dimension of the query and document representation after projection`,name:"projection_dim"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/configuration_retribert.py#L31"}}),Y=new Pe({props:{title:"RetriBertTokenizer",local:"transformers.RetriBertTokenizer",headingTag:"h2"}}),G=new P({props:{name:"class transformers.RetriBertTokenizer",anchor:"transformers.RetriBertTokenizer",parameters:[{name:"vocab_file",val:""},{name:"do_lower_case",val:" = True"},{name:"do_basic_tokenize",val:" = True"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RetriBertTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RetriBertTokenizer.do_basic_tokenize",description:`<strong>do_basic_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to do basic tokenization before WordPiece.`,name:"do_basic_tokenize"},{anchor:"transformers.RetriBertTokenizer.never_split",description:`<strong>never_split</strong> (<code>Iterable</code>, <em>optional</em>) &#x2014;
Collection of tokens which will never be split during tokenization. Only has an effect when
<code>do_basic_tokenize=True</code>`,name:"never_split"},{anchor:"transformers.RetriBertTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RetriBertTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RetriBertTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RetriBertTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RetriBertTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RetriBertTokenizer.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see this
<a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RetriBertTokenizer.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L70"}}),Z=new P({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RetriBertTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L214",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),ee=new P({props:{name:"convert_tokens_to_string",anchor:"transformers.RetriBertTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L208"}}),te=new P({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.RetriBertTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L269",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),S=new Yt({props:{anchor:"transformers.RetriBertTokenizer.create_token_type_ids_from_sequences.example",$$slots:{default:[an]},$$scope:{ctx:F}}}),ne=new P({props:{name:"get_special_tokens_mask",anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.RetriBertTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/tokenization_retribert.py#L240",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),oe=new Pe({props:{title:"RetriBertTokenizerFast",local:"transformers.RetriBertTokenizerFast",headingTag:"h2"}}),re=new P({props:{name:"class transformers.RetriBertTokenizerFast",anchor:"transformers.RetriBertTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"do_lower_case",val:" = True"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"tokenize_chinese_chars",val:" = True"},{name:"strip_accents",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.RetriBertTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.RetriBertTokenizerFast.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to lowercase the input when tokenizing.`,name:"do_lower_case"},{anchor:"transformers.RetriBertTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[UNK]&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.RetriBertTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[SEP]&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.RetriBertTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[PAD]&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.RetriBertTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[CLS]&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.RetriBertTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;[MASK]&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.RetriBertTokenizerFast.clean_text",description:`<strong>clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to clean the text before tokenization by removing any control characters and replacing all
whitespaces by the classic one.`,name:"clean_text"},{anchor:"transformers.RetriBertTokenizerFast.tokenize_chinese_chars",description:`<strong>tokenize_chinese_chars</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to tokenize Chinese characters. This should likely be deactivated for Japanese (see <a href="https://github.com/huggingface/transformers/issues/328" rel="nofollow">this
issue</a>).`,name:"tokenize_chinese_chars"},{anchor:"transformers.RetriBertTokenizerFast.strip_accents",description:`<strong>strip_accents</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to strip all accents. If this option is not specified, then it will be determined by the
value for <code>lowercase</code> (as in the original BERT).`,name:"strip_accents"},{anchor:"transformers.RetriBertTokenizerFast.wordpieces_prefix",description:`<strong>wordpieces_prefix</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;##&quot;</code>) &#x2014;
The prefix for subwords.`,name:"wordpieces_prefix"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py#L54"}}),se=new P({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.RetriBertTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:""},{name:"token_ids_1",val:" = None"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py#L148",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),ie=new P({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.RetriBertTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.RetriBertTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.RetriBertTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/tokenization_retribert_fast.py#L173",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),U=new Yt({props:{anchor:"transformers.RetriBertTokenizerFast.create_token_type_ids_from_sequences.example",$$slots:{default:[dn]},$$scope:{ctx:F}}}),ae=new Pe({props:{title:"RetriBertModel",local:"transformers.RetriBertModel",headingTag:"h2"}}),de=new P({props:{name:"class transformers.RetriBertModel",anchor:"transformers.RetriBertModel",parameters:[{name:"config",val:": RetriBertConfig"}],parametersDescription:[{anchor:"transformers.RetriBertModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/modeling_retribert.py#L84"}}),ce=new P({props:{name:"forward",anchor:"transformers.RetriBertModel.forward",parameters:[{name:"input_ids_query",val:": LongTensor"},{name:"attention_mask_query",val:": Optional"},{name:"input_ids_doc",val:": LongTensor"},{name:"attention_mask_doc",val:": Optional"},{name:"checkpoint_batch_size",val:": int = -1"}],parametersDescription:[{anchor:"transformers.RetriBertModel.forward.input_ids_query",description:`<strong>input_ids_query</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary for the queries in a batch.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_query"},{anchor:"transformers.RetriBertModel.forward.attention_mask_query",description:`<strong>attention_mask_query</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask_query"},{anchor:"transformers.RetriBertModel.forward.input_ids_doc",description:`<strong>input_ids_doc</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary for the documents in a batch.`,name:"input_ids_doc"},{anchor:"transformers.RetriBertModel.forward.attention_mask_doc",description:`<strong>attention_mask_doc</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on documents padding token indices.`,name:"attention_mask_doc"},{anchor:"transformers.RetriBertModel.forward.checkpoint_batch_size",description:`<strong>checkpoint_batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to <code>-1</code>) &#x2014;
If greater than 0, uses gradient checkpointing to only compute sequence representation on
<code>checkpoint_batch_size</code> examples at a time on the GPU. All query representations are still compared to
all document representations in the batch.`,name:"checkpoint_batch_size"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deprecated/retribert/modeling_retribert.py#L176",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The bidirectional cross-entropy loss obtained while trying to match each query to its
corresponding document and each document to its corresponding query in the batch</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>\`torch.FloatTensor“</p>
`}}),{c(){c=i("meta"),z=n(),T=i("p"),m=n(),f(v.$$.fragment),s=n(),f($.$$.fragment),Ie=n(),f(O.$$.fragment),je=n(),W=i("p"),W.innerHTML=Bt,Ee=n(),Q=i("p"),Q.innerHTML=qt,Ae=n(),f(K.$$.fragment),Fe=n(),L=i("div"),f(X.$$.fragment),Xe=n(),le=i("p"),le.innerHTML=Rt,Ye=n(),me=i("p"),me.innerHTML=Mt,He=n(),f(Y.$$.fragment),Se=n(),b=i("div"),f(G.$$.fragment),Ge=n(),pe=i("p"),pe.textContent=Ct,Ze=n(),fe=i("p"),fe.innerHTML=Lt,et=n(),he=i("p"),he.innerHTML=Pt,tt=n(),D=i("div"),f(Z.$$.fragment),nt=n(),ue=i("p"),ue.textContent=Dt,ot=n(),_e=i("ul"),_e.innerHTML=It,rt=n(),H=i("div"),f(ee.$$.fragment),st=n(),ge=i("p"),ge.textContent=jt,it=n(),R=i("div"),f(te.$$.fragment),at=n(),ke=i("p"),ke.textContent=Et,dt=n(),f(S.$$.fragment),ct=n(),be=i("p"),be.innerHTML=At,lt=n(),J=i("div"),f(ne.$$.fragment),mt=n(),Te=i("p"),Te.innerHTML=Ft,Je=n(),f(oe.$$.fragment),Ue=n(),x=i("div"),f(re.$$.fragment),pt=n(),ve=i("p"),ve.innerHTML=Ht,ft=n(),$e=i("p"),$e.innerHTML=St,ht=n(),we=i("p"),we.innerHTML=Jt,ut=n(),I=i("div"),f(se.$$.fragment),_t=n(),xe=i("p"),xe.textContent=Ut,gt=n(),ye=i("ul"),ye.innerHTML=Nt,kt=n(),M=i("div"),f(ie.$$.fragment),bt=n(),ze=i("p"),ze.textContent=Vt,Tt=n(),f(U.$$.fragment),vt=n(),Be=i("p"),Be.innerHTML=Ot,Ne=n(),f(ae.$$.fragment),Ve=n(),y=i("div"),f(de.$$.fragment),$t=n(),qe=i("p"),qe.textContent=Wt,wt=n(),Re=i("p"),Re.innerHTML=Qt,xt=n(),Me=i("p"),Me.innerHTML=Kt,yt=n(),Ce=i("div"),f(ce.$$.fragment),Oe=n(),De=i("p"),this.h()},l(e){const d=on("svelte-u9bgzb",document.head);c=a(d,"META",{name:!0,content:!0}),d.forEach(r),z=o(e),T=a(e,"P",{}),B(T).forEach(r),m=o(e),h(v.$$.fragment,e),s=o(e),h($.$$.fragment,e),Ie=o(e),h(O.$$.fragment,e),je=o(e),W=a(e,"P",{"data-svelte-h":!0}),p(W)!=="svelte-td4eob"&&(W.innerHTML=Bt),Ee=o(e),Q=a(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-1hctczn"&&(Q.innerHTML=qt),Ae=o(e),h(K.$$.fragment,e),Fe=o(e),L=a(e,"DIV",{class:!0});var E=B(L);h(X.$$.fragment,E),Xe=o(E),le=a(E,"P",{"data-svelte-h":!0}),p(le)!=="svelte-bmnr81"&&(le.innerHTML=Rt),Ye=o(E),me=a(E,"P",{"data-svelte-h":!0}),p(me)!=="svelte-o55m63"&&(me.innerHTML=Mt),E.forEach(r),He=o(e),h(Y.$$.fragment,e),Se=o(e),b=a(e,"DIV",{class:!0});var w=B(b);h(G.$$.fragment,w),Ge=o(w),pe=a(w,"P",{"data-svelte-h":!0}),p(pe)!=="svelte-otns3l"&&(pe.textContent=Ct),Ze=o(w),fe=a(w,"P",{"data-svelte-h":!0}),p(fe)!=="svelte-ow77y0"&&(fe.innerHTML=Lt),et=o(w),he=a(w,"P",{"data-svelte-h":!0}),p(he)!=="svelte-1ss6yiy"&&(he.innerHTML=Pt),tt=o(w),D=a(w,"DIV",{class:!0});var A=B(D);h(Z.$$.fragment,A),nt=o(A),ue=a(A,"P",{"data-svelte-h":!0}),p(ue)!=="svelte-t7qurq"&&(ue.textContent=Dt),ot=o(A),_e=a(A,"UL",{"data-svelte-h":!0}),p(_e)!=="svelte-xi6653"&&(_e.innerHTML=It),A.forEach(r),rt=o(w),H=a(w,"DIV",{class:!0});var Qe=B(H);h(ee.$$.fragment,Qe),st=o(Qe),ge=a(Qe,"P",{"data-svelte-h":!0}),p(ge)!=="svelte-b3k2yi"&&(ge.textContent=jt),Qe.forEach(r),it=o(w),R=a(w,"DIV",{class:!0});var N=B(R);h(te.$$.fragment,N),at=o(N),ke=a(N,"P",{"data-svelte-h":!0}),p(ke)!=="svelte-gn6wi7"&&(ke.textContent=Et),dt=o(N),h(S.$$.fragment,N),ct=o(N),be=a(N,"P",{"data-svelte-h":!0}),p(be)!=="svelte-owoxgn"&&(be.innerHTML=At),N.forEach(r),lt=o(w),J=a(w,"DIV",{class:!0});var Ke=B(J);h(ne.$$.fragment,Ke),mt=o(Ke),Te=a(Ke,"P",{"data-svelte-h":!0}),p(Te)!=="svelte-1f4f5kp"&&(Te.innerHTML=Ft),Ke.forEach(r),w.forEach(r),Je=o(e),h(oe.$$.fragment,e),Ue=o(e),x=a(e,"DIV",{class:!0});var C=B(x);h(re.$$.fragment,C),pt=o(C),ve=a(C,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-1g6s8hh"&&(ve.innerHTML=Ht),ft=o(C),$e=a(C,"P",{"data-svelte-h":!0}),p($e)!=="svelte-1vg4yyw"&&($e.innerHTML=St),ht=o(C),we=a(C,"P",{"data-svelte-h":!0}),p(we)!=="svelte-fh0aq"&&(we.innerHTML=Jt),ut=o(C),I=a(C,"DIV",{class:!0});var Le=B(I);h(se.$$.fragment,Le),_t=o(Le),xe=a(Le,"P",{"data-svelte-h":!0}),p(xe)!=="svelte-t7qurq"&&(xe.textContent=Ut),gt=o(Le),ye=a(Le,"UL",{"data-svelte-h":!0}),p(ye)!=="svelte-xi6653"&&(ye.innerHTML=Nt),Le.forEach(r),kt=o(C),M=a(C,"DIV",{class:!0});var V=B(M);h(ie.$$.fragment,V),bt=o(V),ze=a(V,"P",{"data-svelte-h":!0}),p(ze)!=="svelte-gn6wi7"&&(ze.textContent=Vt),Tt=o(V),h(U.$$.fragment,V),vt=o(V),Be=a(V,"P",{"data-svelte-h":!0}),p(Be)!=="svelte-owoxgn"&&(Be.innerHTML=Ot),V.forEach(r),C.forEach(r),Ne=o(e),h(ae.$$.fragment,e),Ve=o(e),y=a(e,"DIV",{class:!0});var j=B(y);h(de.$$.fragment,j),$t=o(j),qe=a(j,"P",{"data-svelte-h":!0}),p(qe)!=="svelte-1c8tt6f"&&(qe.textContent=Wt),wt=o(j),Re=a(j,"P",{"data-svelte-h":!0}),p(Re)!=="svelte-6pahdo"&&(Re.innerHTML=Qt),xt=o(j),Me=a(j,"P",{"data-svelte-h":!0}),p(Me)!=="svelte-hswkmf"&&(Me.innerHTML=Kt),yt=o(j),Ce=a(j,"DIV",{class:!0});var Xt=B(Ce);h(ce.$$.fragment,Xt),Xt.forEach(r),j.forEach(r),Oe=o(e),De=a(e,"P",{}),B(De).forEach(r),this.h()},h(){q(c,"name","hf:doc:metadata"),q(c,"content",ln),q(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,d){t(document.head,c),l(e,z,d),l(e,T,d),l(e,m,d),u(v,e,d),l(e,s,d),u($,e,d),l(e,Ie,d),u(O,e,d),l(e,je,d),l(e,W,d),l(e,Ee,d),l(e,Q,d),l(e,Ae,d),u(K,e,d),l(e,Fe,d),l(e,L,d),u(X,L,null),t(L,Xe),t(L,le),t(L,Ye),t(L,me),l(e,He,d),u(Y,e,d),l(e,Se,d),l(e,b,d),u(G,b,null),t(b,Ge),t(b,pe),t(b,Ze),t(b,fe),t(b,et),t(b,he),t(b,tt),t(b,D),u(Z,D,null),t(D,nt),t(D,ue),t(D,ot),t(D,_e),t(b,rt),t(b,H),u(ee,H,null),t(H,st),t(H,ge),t(b,it),t(b,R),u(te,R,null),t(R,at),t(R,ke),t(R,dt),u(S,R,null),t(R,ct),t(R,be),t(b,lt),t(b,J),u(ne,J,null),t(J,mt),t(J,Te),l(e,Je,d),u(oe,e,d),l(e,Ue,d),l(e,x,d),u(re,x,null),t(x,pt),t(x,ve),t(x,ft),t(x,$e),t(x,ht),t(x,we),t(x,ut),t(x,I),u(se,I,null),t(I,_t),t(I,xe),t(I,gt),t(I,ye),t(x,kt),t(x,M),u(ie,M,null),t(M,bt),t(M,ze),t(M,Tt),u(U,M,null),t(M,vt),t(M,Be),l(e,Ne,d),u(ae,e,d),l(e,Ve,d),l(e,y,d),u(de,y,null),t(y,$t),t(y,qe),t(y,wt),t(y,Re),t(y,xt),t(y,Me),t(y,yt),t(y,Ce),u(ce,Ce,null),l(e,Oe,d),l(e,De,d),We=!0},p(e,[d]){const E={};d&2&&(E.$$scope={dirty:d,ctx:e}),$.$set(E);const w={};d&2&&(w.$$scope={dirty:d,ctx:e}),S.$set(w);const A={};d&2&&(A.$$scope={dirty:d,ctx:e}),U.$set(A)},i(e){We||(_(v.$$.fragment,e),_($.$$.fragment,e),_(O.$$.fragment,e),_(K.$$.fragment,e),_(X.$$.fragment,e),_(Y.$$.fragment,e),_(G.$$.fragment,e),_(Z.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(S.$$.fragment,e),_(ne.$$.fragment,e),_(oe.$$.fragment,e),_(re.$$.fragment,e),_(se.$$.fragment,e),_(ie.$$.fragment,e),_(U.$$.fragment,e),_(ae.$$.fragment,e),_(de.$$.fragment,e),_(ce.$$.fragment,e),We=!0)},o(e){g(v.$$.fragment,e),g($.$$.fragment,e),g(O.$$.fragment,e),g(K.$$.fragment,e),g(X.$$.fragment,e),g(Y.$$.fragment,e),g(G.$$.fragment,e),g(Z.$$.fragment,e),g(ee.$$.fragment,e),g(te.$$.fragment,e),g(S.$$.fragment,e),g(ne.$$.fragment,e),g(oe.$$.fragment,e),g(re.$$.fragment,e),g(se.$$.fragment,e),g(ie.$$.fragment,e),g(U.$$.fragment,e),g(ae.$$.fragment,e),g(de.$$.fragment,e),g(ce.$$.fragment,e),We=!1},d(e){e&&(r(z),r(T),r(m),r(s),r(Ie),r(je),r(W),r(Ee),r(Q),r(Ae),r(Fe),r(L),r(He),r(Se),r(b),r(Je),r(Ue),r(x),r(Ne),r(Ve),r(y),r(Oe),r(De)),r(c),k(v,e),k($,e),k(O,e),k(K,e),k(X),k(Y,e),k(G),k(Z),k(ee),k(te),k(S),k(ne),k(oe,e),k(re),k(se),k(ie),k(U),k(ae,e),k(de),k(ce)}}}const ln='{"title":"RetriBERT","local":"retribert","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"RetriBertConfig","local":"transformers.RetriBertConfig","sections":[],"depth":2},{"title":"RetriBertTokenizer","local":"transformers.RetriBertTokenizer","sections":[],"depth":2},{"title":"RetriBertTokenizerFast","local":"transformers.RetriBertTokenizerFast","sections":[],"depth":2},{"title":"RetriBertModel","local":"transformers.RetriBertModel","sections":[],"depth":2}],"depth":1}';function mn(F){return en(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bn extends tn{constructor(c){super(),nn(this,c,mn,cn,Zt,{})}}export{bn as component};
