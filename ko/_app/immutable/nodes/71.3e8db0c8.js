import{s as fn,o as mn,n as nt}from"../chunks/scheduler.56730f09.js";import{S as pn,i as rn,g as a,s as n,r as $,A as on,h as f,f as l,c as s,j as an,u,x as m,k as d,y as $n,a as i,v as T,d as v,t as _,w as c}from"../chunks/index.1f144517.js";import{T as it}from"../chunks/Tip.41e845e5.js";import{H as C}from"../chunks/Heading.57d46534.js";function un(L){let p,o='더 나아가기 전에, 기존 Transformer 아키텍처에 대한 기본적인 지식을 숙지하는 것이 좋습니다. 인코더, 디코더 및 어텐션의 작동 방식을 알면 다양한 Transformer 모델이 어떻게 작동하는지 이해하는 데 도움이 됩니다. 시작 단계거나 복습이 필요한 경우, 더 많은 정보를 위해 <a href="https://huggingface.co/course/chapter1/4?fw=pt" rel="nofollow">코스</a>를 확인하세요!';return{c(){p=a("p"),p.innerHTML=o},l(r){p=f(r,"P",{"data-svelte-h":!0}),m(p)!=="svelte-1njfzc1"&&(p.innerHTML=o)},m(r,x){i(r,p,x)},p:nt,d(r){r&&l(p)}}}function Tn(L){let p,o='세 번째 방법은 Transformer와 합성곱(예를 들어, <a href="model_doc/cvt">Convolutional Vision Transformer</a> 또는 <a href="model_doc/levit">LeViT</a>)을 결합하는 것입니다. 우리는 살펴볼 두 가지 방법만 결합하기 때문에 여기서 이 방법을 다루지 않습니다.';return{c(){p=a("p"),p.innerHTML=o},l(r){p=f(r,"P",{"data-svelte-h":!0}),m(p)!=="svelte-yjmb0n"&&(p.innerHTML=o)},m(r,x){i(r,p,x)},p:nt,d(r){r&&l(p)}}}function vn(L){let p,o='이 섹션에서는 합성곱에 대해 간략하게 설명합니다. 그러나 이미지의 모양과 크기가 어떻게 변화하는지에 대한 사전 이해가 있다면 도움이 될 것입니다. 합성곱에 익숙하지 않은 경우, fastai book의 <a href="https://github.com/fastai/fastbook/blob/master/13_convolutions.ipynb" rel="nofollow">합성곱 신경망 챕터</a>를 확인하세요!';return{c(){p=a("p"),p.innerHTML=o},l(r){p=f(r,"P",{"data-svelte-h":!0}),m(p)!=="svelte-15kmis9"&&(p.innerHTML=o)},m(r,x){i(r,p,x)},p:nt,d(r){r&&l(p)}}}function _n(L){let p,o="💡 사전훈련된 BERT를 다양한 작업에 사용하는 것이 얼마나 쉬운지 주목하세요. 사전훈련된 모델에 특정 헤드를 추가하기만 하면 은닉 상태를 원하는 출력으로 조작할 수 있습니다!";return{c(){p=a("p"),p.textContent=o},l(r){p=f(r,"P",{"data-svelte-h":!0}),m(p)!=="svelte-1xuoxrx"&&(p.textContent=o)},m(r,x){i(r,p,x)},p:nt,d(r){r&&l(p)}}}function cn(L){let p,o='텍스트 생성에 대한 자세한 내용은 <a href="generation_strategies">텍스트 생성 전략</a> 가이드를 확인하세요!';return{c(){p=a("p"),p.innerHTML=o},l(r){p=f(r,"P",{"data-svelte-h":!0}),m(p)!=="svelte-1mp4ipn"&&(p.innerHTML=o)},m(r,x){i(r,p,x)},p:nt,d(r){r&&l(p)}}}function xn(L){let p,o='텍스트 생성에 대한 자세한 내용은 <a href="generation_strategies">텍스트 생성 전략</a> 가이드를 확인하세요!';return{c(){p=a("p"),p.innerHTML=o},l(r){p=f(r,"P",{"data-svelte-h":!0}),m(p)!=="svelte-1mp4ipn"&&(p.innerHTML=o)},m(r,x){i(r,p,x)},p:nt,d(r){r&&l(p)}}}function Cn(L){let p,o='텍스트 생성에 대한 자세한 내용은 <a href="generation_strategies">텍스트 생성 전략</a> 가이드를 확인하세요!';return{c(){p=a("p"),p.innerHTML=o},l(r){p=f(r,"P",{"data-svelte-h":!0}),m(p)!=="svelte-1mp4ipn"&&(p.innerHTML=o)},m(r,x){i(r,p,x)},p:nt,d(r){r&&l(p)}}}function Ln(L){let p,o,r,x,z,at,q,Sl='<a href="task_summary">🤗 Transformers로 할 수 있는 작업</a>에서 자연어 처리(NLP), 음성 및 오디오, 컴퓨터 비전 작업 등의 중요한 응용을 배웠습니다. 이 페이지에서는 모델이 이러한 작업을 어떻게 해결하는지 자세히 살펴보고 내부에서 어떤 일이 일어나는지 설명합니다. 주어진 작업을 해결하는 많은 방법이 있으며, 일부 모델은 특정 기술을 구현하거나 심지어 새로운 방식으로 작업에 접근할 수도 있지만, Transformer 모델의 경우 일반적인 아이디어는 동일합니다. 유연한 아키텍처 덕분에 대부분의 모델은 인코더, 디코더 또는 인코더-디코더 구조의 변형입니다. Transformer 모델뿐만 아니라 우리의 라이브러리에는 오늘날 컴퓨터 비전 작업에 사용되는 몇 가지 합성곱 신경망(CNNs)도 있습니다. 또한, 우리는 현대 CNN의 작동 방식에 대해 설명할 것입니다.',ft,A,Gl="작업이 어떻게 해결되는지 설명하기 위해, 유용한 예측을 출력하고자 모델 내부에서 어떤 일이 일어나는지 살펴봅니다.",mt,F,Il='<li>오디오 분류 및 자동 음성 인식(ASR)을 위한 <a href="model_doc/wav2vec2">Wav2Vec2</a></li> <li>이미지 분류를 위한 <a href="model_doc/vit">Vision Transformer (ViT)</a> 및 <a href="model_doc/convnext">ConvNeXT</a></li> <li>객체 탐지를 위한 <a href="model_doc/detr">DETR</a></li> <li>이미지 분할을 위한 <a href="model_doc/mask2former">Mask2Former</a></li> <li>깊이 추정을 위한 <a href="model_doc/glpn">GLPN</a></li> <li>인코더를 사용하는 텍스트 분류, 토큰 분류 및 질의응답과 같은 NLP 작업을 위한 <a href="model_doc/bert">BERT</a></li> <li>디코더를 사용하는 텍스트 생성과 같은 NLP 작업을 위한 <a href="model_doc/gpt2">GPT2</a></li> <li>인코더-디코더를 사용하는 요약 및 번역과 같은 NLP 작업을 위한 <a href="model_doc/bart">BART</a></li>',pt,g,rt,S,ot,G,Ol='<a href="model_doc/wav2vec2">Wav2Vec2</a>는 레이블이 지정되지 않은 음성 데이터에 대해 사전훈련된 모델로, 오디오 분류 및 자동 음성 인식을 위해 레이블이 지정된 데이터로 미세 조정합니다.',$t,M,Xl='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/wav2vec2_architecture.png"/>',ut,I,Wl="이 모델에는 4가지 주요 구성 요소가 있습니다:",Tt,O,Ul="<li><p><em>특징 인코더(feature encoder)</em>는 원시 오디오 파형(raw audio waveform)을 가져와서 제로 평균 및 단위 분산으로 표준화하고, 각각 20ms 길이의 특징 벡터의 시퀀스로 변환합니다.</p></li> <li><p>오디오 파형은 본질적으로 연속적이기 때문에, 텍스트 시퀀스를 단어로 나누는 것과 같이 분할할 수 없습니다. 그래서 <em>양자화 모듈(quantization module)</em>로 전달되는 특징 벡터는 이산형 음성 단위를 학습하기 위한 것입니다. 음성 단위는 <em>코드북(codebook)</em>(어휘집이라고 생각할 수 있습니다)이라는 코드단어(codewords) 콜렉션에서 선택됩니다. 코드북에서 연속적인 오디오 입력을 가장 잘 나타내는 벡터 또는 음성 단위가 선택되어 모델을 통과합니다.</p></li> <li><p>특징 벡터의 절반은 무작위로 마스크가 적용되며, 마스크된 특징 벡터는 <em>상대적 위치 임베딩</em>을 추가하는 Transformer 인코더인 <em>문맥 네트워크(context network)</em>로 전달됩니다.</p></li> <li><p>문맥 네트워크의 사전훈련 목표는 <em>대조적 작업(contrastive task)</em>입니다. 모델은 잘못된 예측 시퀀스에서 마스크된 예측의 실제 양자화된 음성 표현을 예측하며, 모델이 가장 유사한 컨텍스트 벡터와 양자화된 음성 단위(타겟 레이블)를 찾도록 권장합니다.</p></li>",vt,X,Jl="이제 wav2vec2가 사전훈련되었으므로, 오디오 분류 또는 자동 음성 인식을 위해 데이터에 맞춰 미세 조정할 수 있습니다!",_t,W,ct,U,Kl="사전훈련된 모델을 오디오 분류에 사용하려면, 기본 Wav2Vec2 모델 상단에 시퀀스 분류 헤드를 추가하면 됩니다. 분류 헤드는 인코더의 은닉 상태(hidden states)를 받는 선형 레이어입니다. 은닉 상태는 각각 길이가 다른 오디오 프레임에서 학습된 특징을 나타냅니다. 고정 길이의 벡터 하나를 만들기 위해, 은닉 상태는 먼저 풀링되고, 클래스 레이블에 대한 로짓으로 변환됩니다. 가장 가능성이 높은 클래스를 찾기 위해 로짓과 타겟 사이의 교차 엔트로피 손실이 계산됩니다.",xt,J,Ql='오디오 분류에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/audio_classification">오디오 분류 가이드</a>를 확인하여 Wav2Vec2를 미세 조정하고 추론에 사용하는 방법을 학습하세요!',Ct,K,Lt,Q,Yl='사전훈련된 모델을 자동 음성 인식에 사용하려면, <a href="glossary#connectionist-temporal-classification-ctc">연결주의적 시간 분류(CTC, Connectionist Temporal Classification)</a>를 위해 기본 Wav2Vec2 모델 상단에 언어 모델링 헤드를 추가합니다. 언어 모델링 헤드는 인코더의 은닉 상태를 받아서 로짓으로 변환합니다. 각 로짓은 토큰 클래스(토큰 수는 작업의 어휘에서 나타납니다)를 나타냅니다. CTC 손실은 텍스트로 디코딩된 토큰에서 가장 가능성이 높은 토큰 시퀀스를 찾기 위해 로짓과 타겟 사이에서 계산됩니다.',dt,Y,Zl='자동 음성 인식에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/asr">자동 음성 인식 가이드</a>를 확인하여 Wav2Vec2를 미세 조정하고 추론에 사용하는 방법을 학습하세요!',gt,Z,Mt,ee,ei="컴퓨터 비전 작업에 접근하는 2가지 방법이 있습니다:",Ht,te,ti='<li>이미지를 패치 시퀀스로 분리하고 Transformer로 병렬 처리합니다.</li> <li><a href="model_doc/convnext">ConvNeXT</a>와 같은 현대 CNN을 사용합니다. 이는 합성곱 레이어를 기반으로 하지만 현대 네트워크 설계를 적용합니다.</li>',ht,H,Pt,le,li="ViT와 ConvNeXT는 일반적으로 이미지 분류에서 사용되지만, 물체 감지, 분할, 깊이 추정과 같은 다른 비전 작업에는 각각 DETR, Mask2Former, GLPN이 더 적합하므로 이러한 모델을 살펴보겠습니다.",kt,ie,wt,ne,ii="ViT와 ConvNeXT 모두 이미지 분류에 사용될 수 있지만, ViT는 어텐션 메커니즘을, ConvNeXT는 합성곱을 사용하는 것이 주된 차이입니다.",Nt,se,bt,ae,ni='<a href="model_doc/vit">ViT</a>은 합성곱을 전적으로 순수 Transformer 아키텍처로 대체합니다. 기존 Transformer에 익숙하다면, ViT를 이해하는 방법의 대부분을 이미 파악했다고 볼 수 있습니다.',Rt,h,si='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg"/>',Et,fe,ai="ViT가 도입한 주요 변경 사항은 이미지가 Transformer로 어떻게 전달되는지에 있습니다:",Bt,me,fi="<li><p>이미지는 서로 중첩되지 않는 정사각형 패치로 분할되고, 각 패치는 벡터 또는 <em>패치 임베딩(patch embedding)</em>으로 변환됩니다. 패치 임베딩은 적절한 입력 차원을 만드는 2D 합성곱 계층에서 생성됩니다(기본 Transformer의 경우 각 패치의 임베딩마다 768개의 값이 필요합니다). 224x224 픽셀 이미지가 있다면, 16x16 이미지 패치 196개로 분할할 수 있습니다. 텍스트가 단어로 토큰화되는 것처럼, 이미지도 패치 시퀀스로 “토큰화”됩니다.</p></li> <li><p><em>학습 가능한 임베딩(learnable embedding)</em>(특수한 <code>[CLS]</code> 토큰)이 BERT와 같이 패치 임베딩의 시작 부분에 추가됩니다. <code>[CLS]</code> 토큰의 마지막 은닉 상태는 부착된 분류 헤드의 입력으로 사용되고, 다른 출력은 무시됩니다. 이 토큰은 모델이 이미지의 표현을 인코딩하는 방법을 학습하는 데 도움이 됩니다.</p></li> <li><p>패치와 학습 가능한 임베딩에 마지막으로 추가할 것은 <em>위치 임베딩</em>입니다. 왜냐하면 모델은 이미지 패치의 순서를 모르기 때문입니다. 위치 임베딩도 학습 가능하며, 패치 임베딩과 동일한 크기를 가집니다. 최종적으로, 모든 임베딩이 Transformer 인코더에 전달됩니다.</p></li> <li><p><code>[CLS]</code> 토큰을 포함한 출력은 다층 퍼셉트론 헤드(MLP)에 전달됩니다. ViT의 사전훈련 목표는 단순히 분류입니다. 다른 분류 헤드와 같이, MLP 헤드는 출력을 클래스 레이블에 대해 로짓으로 변환하고 교차 엔트로피 손실을 계산하여 가장 가능성이 높은 클래스를 찾습니다.</p></li>",jt,pe,mi='이미지 분류에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/image_classification">이미지 분류 가이드</a>를 확인하여 ViT를 미세 조정하고 추론에 사용하는 방법을 학습하세요!',Vt,re,yt,P,Dt,oe,pi='<a href="model_doc/convnext">ConvNeXT</a>는 성능을 높이기 위해 새로운 현대 네트워크 설계를 적용한 CNN 구조입니다. 그러나 합성곱은 여전히 모델의 핵심입니다. 높은 수준의 관점에서 볼 때, <a href="glossary#convolution">합성곱</a>은 작은 행렬(<em>커널</em>)에 이미지 픽셀의 작은 윈도우를 곱하는 연산입니다. 이는 특정 텍스쳐(texture)이나 선의 곡률과 같은 일부 특징을 계산합니다. 그러고 다음 픽셀 윈도우로 넘어가는데, 여기서 합성곱이 이동하는 거리를 <em>보폭(stride)</em>이라고 합니다.',zt,k,ri='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convolution.gif"/>',qt,$e,oi='패딩이나 보폭이 없는 기본 합성곱, <a href="https://arxiv.org/abs/1603.07285">딥러닝을 위한 합성곱 연산 가이드</a>',At,ue,$i="이 출력을 다른 합성곱 레이어에 전달할 수 있으며, 각 연속적인 레이어를 통해 네트워크는 핫도그나 로켓과 같이 더 복잡하고 추상적인 것을 학습합니다. 합성곱 레이어 사이에 풀링 레이어를 추가하여 차원을 줄이고 특징의 위치 변화에 대해 모델을 더 견고하게 만드는 것이 일반적입니다.",Ft,w,ui='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.png"/>',St,Te,Ti="ConvNeXT는 CNN을 5가지 방식으로 현대화합니다:",Gt,ve,vi="<li><p>각 단계의 블록 수를 변경하고 더 큰 보폭과 그에 대응하는 커널 크기로 이미지를 “패치화(patchify)“합니다. 겹치지 않는 슬라이딩 윈도우는 ViT가 이미지를 패치로 분할하는 방법과 유사하게 이 패치화 전략을 만듭니다.</p></li> <li><p><em>병목(bottleneck)</em> 레이어는 채널 수를 줄였다가 다시 복원합니다. 왜냐하면 1x1 합성곱을 수행하는 것이 더 빠르고, 깊이를 늘릴 수 있기 때문입니다. 역 병목(inverted bottlenect)은 채널 수를 확장하고 축소함으로써 그 반대로 수행하므로, 메모리 효율이 더 높습니다.</p></li> <li><p>병목 레이어의 일반적인 3x3 합성곱 레이어를 각 입력 채널에 개별적으로 합성곱을 적용한 다음 마지막에 쌓는 <em>깊이별 합성곱(depthwise convolution)</em>으로 대체합니다. 이는 네트워크 폭이 넓혀 성능이 향상됩니다.</p></li> <li><p>ViT는 어텐션 메커니즘 덕분에 한 번에 더 많은 이미지를 볼 수 있는 전역 수신 필드를 가지고 있습니다. ConvNeXT는 커널 크기를 7x7로 늘려 이 효과를 재현하려고 시도합니다.</p></li> <li><p>또한 ConvNeXT는 Transformer 모델을 모방하는 몇 가지 레이어 설계를 변경합니다. 활성화 및 정규화 레이어가 더 적고, 활성화 함수가 ReLU 대신 GELU로 전환되고, BatchNorm 대신 LayerNorm을 사용합니다.</p></li>",It,_e,_i="합성곱 블록의 출력은 분류 헤드로 전달되며, 분류 헤드는 출력을 로짓으로 변환하고 교차 엔트로피 손실을 계산하여 가장 가능성이 높은 레이블을 찾습니다.",Ot,ce,Xt,xe,ci='<a href="model_doc/detr">DETR</a>, <em>DEtection TRansformer</em>는 CNN과 Transformer 인코더-디코더를 결합한 종단간(end-to-end) 객체 탐지 모델입니다.',Wt,N,xi='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/detr_architecture.png"/>',Ut,Ce,Ci="<li><p>사전훈련된 CNN <em>백본(backbone)</em>은 픽셀 값으로 나타낸 이미지를 가져와 저해상도 특징 맵을 만듭니다. 특징 맵에 대해 1x1 합성곱을 적용하여 차원을 줄이고, 고수준 이미지 표현을 가진 새로운 특징 맵을 생성합니다. Transformer는 시퀀스 모델이기 때문에 특징 맵을 위치 임베딩과 결합된 특징 벡터의 시퀀스로 평탄화합니다.</p></li> <li><p>특징 벡터는 어텐션 레이어를 사용하여 이미지 표현을 학습하는 인코더에 전달됩니다. 다음으로, 인코더의 은닉 상태는 디코더에서 <em>객체 쿼리</em>와 결합됩니다. 객체 쿼리는 이미지의 다른 영역에 초점을 맞춘 학습된 임베딩으로 학습되고, 각 어텐션 레이어를 진행하면서 갱신됩니다. 디코더의 은닉 상태는 각 객체 쿼리에 대한 바운딩 박스 좌표와 클래스 레이블을 예측하는 순방향 네트워크에 전달되며, 객체가 없는 경우 <code>no object</code>가 출력됩니다.</p> <p>DETR은 각 객체 쿼리를 병렬로 디코딩하여 <em>N</em> 개의 최종 예측을 출력합니다. 여기서 <em>N</em>은 쿼리 수입니다. 한 번에 하나의 요소를 예측하는 일반적인 자기회귀 모델과 달리, 객체 탐지는 한 번에 <em>N</em> 개의 예측을 수행하는 집합 예측 작업(<code>바운딩 박스</code>, <code>클래스 레이블</code>)입니다.</p></li> <li><p>DETR은 훈련 중 <em>이분 매칭 손실(bipartite matching loss)</em>을 사용하여 고정된 수의 예측과 고정된 실제 정답 레이블(ground truth labels) 세트를 비교합니다. <em>N</em>개의 레이블 세트에 실제 정답 레이블보다 적은 경우, <code>no object</code> 클래스로 패딩됩니다. 이 손실 함수는 DETR이 예측과 실제 정답 레이블 간 1:1 대응을 찾도록 권장합니다. 바운딩 박스 또는 클래스 레이블 중 하나라도 잘못된 경우, 손실이 발생합니다. 마찬가지로, 존재하지 않는 객체를 예측하는 경우, 패널티를 받습니다. 이로 인해 DETR은 이미지에서 눈에 잘 띄는 물체 하나에 집중하는 대신, 다른 객체를 찾도록 권장됩니다.</p></li>",Jt,Le,Li="객체 탐지 헤드가 DETR 상단에 추가되어 클래스 레이블과 바운딩 박스의 좌표를 찾습니다. 객체 탐지 헤드에는 두 가지 구성 요소가 있습니다: 디코더 은닉 상태를 클래스 레이블의 로짓으로 변환하는 선형 레이어 및 바운딩 박스를 예측하는 MLP",Kt,de,di='객체 탐지에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/object_detection">객체 탐지 가이드</a>를 확인하여 DETR을 미세 조정하고 추론에 사용하는 방법을 학습하세요!',Qt,ge,Yt,Me,gi='<a href="model_doc/mask2former">Mask2Former</a>는 모든 유형의 이미지 분할 작업을 해결하는 범용 아키텍처입니다. 전통적인 분할 모델은 일반적으로 시멘틱(semantic) 또는 파놉틱(panoptic) 분할과 같은 이미지 분할의 특정 하위 작업에 맞춰 조정됩니다. Mask2Former는 모든 작업을 <em>마스크 분류</em> 문제로 구성합니다. 마스크 분류는 픽셀을 <em>N</em>개 세그먼트로 그룹화하고, 주어진 이미지에 대해 <em>N</em>개의 마스크와 그에 대응하는 클래스 레이블을 예측합니다. 이 섹션에서 Mask2Former의 작동 방법을 설명한 다음, 마지막에 SegFormer를 미세 조정해볼 수 있습니다.',Zt,b,Mi='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/mask2former_architecture.png"/>',el,He,Hi="Mask2Former에는 3가지 주요 구성 요소가 있습니다:",tl,he,hi='<li><p><a href="model_doc/swin">Swin</a> 백본이 이미지를 받아 3개의 연속된 3x3 합성곱에서 저해상도 이미지 특징 맵을 생성합니다.</p></li> <li><p>특징 맵은 <em>픽셀 디코더</em>에 전달됩니다. 이 디코더는 저해상도 특징을 고해상도 픽셀 임베딩으로 점진적으로 업샘플링합니다. 픽셀 디코더는 실제로 원본 이미지의 1/32, 1/16, 1/8 해상도의 다중 스케일 특징(저해상도 및 고해상도 특징 모두 포함)을 생성합니다.</p></li> <li><p>이러한 서로 다른 크기의 특징 맵은 고해상도 특징에서 작은 객체를 포착하기 위해 한 번에 하나의 Transformer 디코더 레이어에 연속적으로 공급됩니다. Mask2Former의 핵심은 디코더의 <em>마스크 어텐션</em> 메커니즘입니다. 전체 이미지를 참조할 수 있는 크로스 어텐션(cross-attention)과 달리, 마스크 어텐션은 이미지의 특정 영역에만 집중합니다. 이는 이미지의 지역적 특징만으로 모델이 충분히 학습할 수 있기 때문에 더 빠르고 성능이 우수합니다.</p></li> <li><p><a href="tasks_explained#object-detection">DETR</a>과 같이, Mask2Former는 학습된 객체 쿼리를 사용하고 이를 픽셀 디코더에서의 이미지 특징과 결합하여 예측 집합(<code>클래스 레이블</code>, <code>마스크 예측</code>)을 생성합니다. 디코더의 은닉 상태는 선형 레이어로 전달되어 클래스 레이블에 대한 로짓으로 변환됩니다. 로짓과 클래스 레이블 사이의 교차 엔트로피 손실을 계산하여 가장 가능성이 높은 것을 찾습니다.</p> <p>마스크 예측은 픽셀 임베딩과 최종 디코더 은닉 상태를 결합하여 생성됩니다. 시그모이드 교차 엔트로피 및 Dice 손실은 로짓과 실제 정답 마스크(ground truth mask) 사이에서 계산되어 가장 가능성이 높은 마스크를 찾습니다.</p></li>',ll,Pe,Pi='이미지 분할에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/semantic_segmentation">이미지 분할 가이드</a>를 확인하여 SegFormer를 미세 조정하고 추론에 사용하는 방법을 학습하세요!',il,ke,nl,we,ki='<a href="model_doc/glpn">GLPN</a>, <em>Global-Local Path Network</em>는 <a href="model_doc/segformer">SegFormer</a> 인코더와 경량 디코더를 결합한 깊이 추정을 위한 Transformer입니다.',sl,R,wi='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/glpn_architecture.jpg"/>',al,Ne,Ni='<li><p>ViT와 같이, 이미지는 패치 시퀀스로 분할되지만, 이미지 패치가 더 작다는 점이 다릅니다. 이는 세그멘테이션이나 깊이 추정과 같은 밀도 예측 작업에 더 적합합니다. 이미지 패치는 패치 임베딩으로 변환되어(패치 임베딩이 생성되는 방법은 <a href="#image-classification">이미지 분류</a> 섹션을 참조하세요), 인코더로 전달됩니다.</p></li> <li><p>인코더는 패치 임베딩을 받아, 여러 인코더 블록에 전달합니다. 각 블록은 어텐션 및 Mix-FFN 레이어로 구성됩니다. 후자의 목적은 위치 정보를 제공하는 것입니다. 각 인코더 블록의 끝에는 계층적 표현을 생성하기 위한 <em>패치 병합(patch merging)</em> 레이어가 있습니다. 각 인접한 패치 그룹의 특징은 연결되고, 연결된 특징에 선형 레이어가 적용되어 패치 수를 1/4의 해상도로 줄입니다. 이는 다음 인코더 블록의 입력이 되며, 이러한 전체 프로세스는 1/8, 1/16, 1/32 해상도의 이미지 특징을 가질 때까지 반복됩니다.</p></li> <li><p>경량 디코더는 인코더에서 마지막 특징 맵(1/32 크기)을 가져와 1/16 크기로 업샘플링합니다. 여기서, 특징은 <em>선택적 특징 융합(SFF, Selective Feature Fusion)</em> 모듈로 전달됩니다. 이 모듈은 각 특징에 대해 어텐션 맵에서 로컬 및 전역 특징을 선택하고 결합한 다음, 1/8로 업샘플링합니다. 이 프로세스는 디코딩된 특성이 원본 이미지와 동일한 크기가 될 때까지 반복됩니다. 출력은 두 개의 합성곱 레이어를 거친 다음, 시그모이드 활성화가 적용되어 각 픽셀의 깊이를 예측합니다.</p></li>',fl,be,ml,Re,bi="Transformer는 초기에 기계 번역을 위해 설계되었고, 그 이후로는 사실상 모든 NLP 작업을 해결하기 위한 기본 아키텍처가 되었습니다. 어떤 작업은 Transformer의 인코더 구조에 적합하며, 다른 작업은 디코더에 더 적합합니다. 또 다른 작업은 Transformer의 인코더-디코더 구조를 모두 활용합니다.",pl,Ee,rl,Be,Ri='<a href="model_doc/bert">BERT</a>는 인코더 전용 모델이며, 텍스트의 풍부한 표현을 학습하기 위해 양방향의 단어에 주목함으로써 심층 양방향성(deep bidirectionality)을 효과적으로 구현한 최초의 모델입니다.',ol,je,Ei='<li><p>BERT는 <a href="tokenizer_summary#wordpiece">WordPiece</a> 토큰화를 사용하여 문장의 토큰 임베딩을 생성합니다. 단일 문장과 한 쌍의 문장을 구분하기 위해 특수한 <code>[SEP]</code> 토큰이 추가됩니다. 모든 텍스트 시퀀스의 시작 부분에는 특수한 <code>[CLS]</code> 토큰이 추가됩니다. <code>[CLS]</code> 토큰이 있는 최종 출력은 분류 작업을 위한 분류 헤드로 입력에 사용됩니다. BERT는 또한 한 쌍의 문장에서 각 토큰이 첫 번째 문장인지 두 번째 문장에 속하는지 나타내는 세그먼트 임베딩(segment embedding)을 추가합니다.</p></li> <li><p>BERT는 마스크드 언어 모델링과 다음 문장 예측, 두 가지 목적으로 사전훈련됩니다. 마스크드 언어 모델링에서는 입력 토큰의 일부가 무작위로 마스킹되고, 모델은 이를 예측해야 합니다. 이는 모델이 모든 단어를 보고 다음 단어를 “예측”할 수 있는 양방향성 문제를 해결합니다. 예측된 마스크 토큰의 최종 은닉 상태는 어휘에 대한 소프트맥스가 있는 순방향 네트워크로 전달되어 마스크된 단어를 예측합니다.</p> <p>두 번째 사전훈련 대상은 다음 문장 예측입니다. 모델은 문장 B가 문장 A 다음에 오는지 예측해야 합니다. 문장 B가 다음 문장인 경우와 무작위 문장인 경우 각각 50%의 확률로 발생합니다. 다음 문장인지 아닌지에 대한 예측은 두 개의 클래스(<code>IsNext</code> 및 <code>NotNext</code>)에 대한 소프트맥스가 있는 순방향 네트워크로 전달됩니다.</p></li> <li><p>입력 임베딩은 여러 인코더 레이어를 거쳐서 최종 은닉 상태를 출력합니다.</p></li>',$l,Ve,Bi="사전훈련된 모델을 텍스트 분류에 사용하려면, 기본 BERT 모델 상단에 시퀀스 분류 헤드를 추가합니다. 시퀀스 분류 헤드는 최종 은닉 상태를 받는 선형 레이어이며, 로짓으로 변환하기 위해 선형 변환을 수행합니다. 교차 엔트로피 손실은 로짓과 타겟 간에 계산되어 가장 가능성이 높은 레이블을 찾습니다.",ul,ye,ji='텍스트 분류에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/sequence_classification">텍스트 분류 가이드</a>를 확인하여 DistilBERT를 미세 조정하고 추론에 사용하는 방법을 학습하세요!',Tl,De,vl,ze,Vi="개체명 인식(Named Entity Recognition, NER)과 같은 토큰 분류 작업에 BERT를 사용하려면, 기본 BERT 모델 상단에 토큰 분류 헤드를 추가합니다. 토큰 분류 헤드는 최종 은닉 상태를 받는 선형 레이어이며, 로짓으로 변환하기 위해 선형 변환을 수행합니다. 교차 엔트로피 손실은 로짓과 각 토큰 간에 계산되어 가장 가능성이 높은 레이블을 찾습니다.",_l,qe,yi='토큰 분류에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/token_classification">토큰 분류 가이드</a>를 확인하여 DistilBERT를 미세 조정하고 추론에 사용하는 방법을 학습하세요!',cl,Ae,xl,Fe,Di="질의응답에 BERT를 사용하려면, 기본 BERT 모델 위에 스팬(span) 분류 헤드를 추가합니다. 이 선형 레이어는 최종 은닉 상태를 받고, 답변에 대응하는 <code>스팬</code>의 시작과 끝 로그를 계산하기 위해 선형 변환을 수행합니다. 교차 엔트로피 손실은 로짓과 각 레이블 위치 간에 계산되어 답변에 대응하는 가장 가능성이 높은 텍스트의 스팬을 찾습니다.",Cl,Se,zi='질의응답에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/question_answering">질의응답 가이드</a>를 확인하여 DistilBERT를 미세 조정하고 추론에 사용하는 방법을 학습하세요!',Ll,E,dl,Ge,gl,Ie,qi='<a href="model_doc/gpt2">GPT-2</a>는 대량의 텍스트에 대해 사전훈련된 디코딩 전용 모델입니다. 프롬프트를 주어지면 설득력 있는 (항상 사실은 아니지만!) 텍스트를 생성하고 명시적으로 훈련되지 않았음에도 불구하고 질의응답과 같은 다른 NLP 작업을 완수할 수 있습니다.',Ml,B,Ai='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gpt2_architecture.png"/>',Hl,Oe,Fi='<li><p>GPT-2는 단어를 토큰화하고 토큰 임베딩을 생성하기 위해 <a href="tokenizer_summary#bytepair-encoding-bpe">바이트 페어 인코딩(BPE, byte pair encoding)</a>을 사용합니다. 위치 인코딩은 시퀀스에서 각 토큰의 위치를 나타내기 위해 토큰 임베딩에 추가됩니다. 입력 임베딩은 여러 디코더 블록을 거쳐 일부 최종 은닉 상태를 출력합니다. 각 디코더 블록 내에서 GPT-2는 <em>마스크드 셀프 어텐션(masked self-attention)</em> 레이어를 사용합니다. 이는 GPT-2가 이후 토큰(future tokens)에 주의를 기울일 수 없도록 합니다. 왼쪽에 있는 토큰에만 주의를 기울일 수 있습니다. 마스크드 셀프 어텐션에서는 어텐션 마스크를 사용하여 이후 토큰에 대한 점수(score)를 <code>0</code>으로 설정하기 때문에 BERT의 <code>mask</code> 토큰과 다릅니다.</p></li> <li><p>디코더의 출력은 언어 모델링 헤드에 전달되며, 언어 모델링 헤드는 은닉 상태를 로짓으로 선형 변환을 수행합니다. 레이블은 시퀀스의 다음 토큰으로, 로짓을 오른쪽으로 하나씩 이동하여 생성됩니다. 교차 엔트로피 손실은 이동된 로짓과 레이블 간에 계산되어 가장 가능성이 높은 다음 토큰을 출력합니다.</p></li>',hl,Xe,Si='GPT-2의 사전훈련 목적은 전적으로 <a href="glossary#causal-language-modeling">인과적 언어 모델링</a>에 기반하여, 시퀀스에서 다음 단어를 예측하는 것입니다. 이는 GPT-2가 텍스트 생성에 관련된 작업에 특히 우수하도록 합니다.',Pl,We,Gi='텍스트 생성에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/language_modeling#causal-language-modeling">인과적 언어 모델링 가이드</a>를 확인하여 DistilGPT-2를 미세 조정하고 추론에 사용하는 방법을 학습하세요!',kl,j,wl,Ue,Nl,Je,Ii='<a href="model_doc/bart">BART</a> 및 <a href="model_doc/t5">T5</a>와 같은 인코더-디코더 모델은 요약 작업의 시퀀스-투-시퀀스 패턴을 위해 설계되었습니다. 이 섹션에서 BART의 작동 방법을 설명한 다음, 마지막에 T5를 미세 조정해볼 수 있습니다.',bl,V,Oi='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bart_architecture.png"/>',Rl,Ke,Xi="<li><p>BART의 인코더 아키텍처는 BERT와 매우 유사하며 텍스트의 토큰 및 위치 임베딩을 받습니다. BART는 입력을 변형시키고 디코더로 재구성하여 사전훈련됩니다. 특정 변형 기법이 있는 다른 인코더와는 달리, BART는 모든 유형의 변형을 적용할 수 있습니다. 그러나 <em>text infilling</em> 변형 기법이 가장 잘 작동합니다. Text Infiling에서는 여러 텍스트 스팬을 <strong>단일</strong> <code>mask</code> 토큰으로 대체합니다. 이는 모델이 마스크된 토큰을 예측해야 하고, 모델에 누락된 토큰의 수를 예측하도록 가르치기 때문에 중요합니다. 입력 임베딩과 마스크된 스팬이 인코더를 거쳐 최종 은닉 상태를 출력하지만, BERT와 달리 BART는 마지막에 단어를 예측하는 순방향 네트워크를 추가하지 않습니다.</p></li> <li><p>인코더의 출력은 디코더로 전달되며, 디코더는 인코더의 출력에서 마스크 토큰과 변형되지 않은 토큰을 예측해야 합니다. 이는 디코더가 원본 텍스트를 복원하는 데 도움이 되는 추가적인 문맥을 얻도록 합니다. 디코더의 출력은 언어 모델링 헤드에 전달되며, 언어 모델링 헤드는 은닉 상태를 로짓으로 선형 변환을 수행합니다. 교차 엔트로피 손실은 로짓과 토큰이 오른쪽으로 이동된 레이블 간에 계산됩니다.</p></li>",El,Qe,Wi='요약에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/summarization">요약 가이드</a>를 확인하여 T5를 미세 조정하고 추론에 사용하는 방법을 학습하세요!',Bl,y,jl,Ye,Vl,Ze,Ui='번역은 시퀀스-투-시퀀스 작업의 또 다른 예로, <a href="model_doc/bart">BART</a> 또는 <a href="model_doc/t5">T5</a>와 같은 인코더-디코더 모델을 사용할 수 있습니다. 이 섹션에서 BART의 작동 방법을 설명한 다음, 마지막에 T5를 미세 조정해볼 수 있습니다.',yl,et,Ji="BART는 원천 언어를 타겟 언어로 디코딩할 수 있는 입력에 매핑하기 위해 무작위로 초기화된 별도의 인코더를 추가하여 번역에 적용합니다. 이 새로운 인코더의 임베딩은 원본 단어 임베딩 대신 사전훈련된 인코더로 전달됩니다. 원천 인코더는 모델 출력의 교차 엔트로피 손실로부터 원천 인코더, 위치 임베딩, 입력 임베딩을 갱신하여 훈련됩니다. 첫 번째 단계에서는 모델 파라미터가 고정되고, 두 번째 단계에서는 모든 모델 파라미터가 함께 훈련됩니다.",Dl,tt,Ki="BART는 이후 번역을 위해 다양한 언어로 사전훈련된 다국어 버전의 mBART로 확장되었습니다.",zl,lt,Qi='번역에 직접 도전할 준비가 되셨나요? 완전한 <a href="tasks/summarization">번역 가이드</a>를 확인하여 T5를 미세 조정하고 추론에 사용하는 방법을 학습하세요!',ql,D,Al,st,Fl;return z=new C({props:{title:"🤗 Transformers로 작업을 해결하는 방법",local:"how-transformers-solve-tasks",headingTag:"h1"}}),g=new it({props:{$$slots:{default:[un]},$$scope:{ctx:L}}}),S=new C({props:{title:"음성 및 오디오",local:"speech-and-audio",headingTag:"h2"}}),W=new C({props:{title:"오디오 분류",local:"audio-classification",headingTag:"h3"}}),K=new C({props:{title:"자동 음성 인식",local:"automatic-speech-recognition",headingTag:"h3"}}),Z=new C({props:{title:"컴퓨터 비전",local:"computer-vision",headingTag:"h2"}}),H=new it({props:{$$slots:{default:[Tn]},$$scope:{ctx:L}}}),ie=new C({props:{title:"이미지 분류",local:"image-classification",headingTag:"h3"}}),se=new C({props:{title:"Transformer",local:"transformer",headingTag:"h4"}}),re=new C({props:{title:"CNN",local:"cnn",headingTag:"h4"}}),P=new it({props:{$$slots:{default:[vn]},$$scope:{ctx:L}}}),ce=new C({props:{title:"객체 탐지",local:"object-detection",headingTag:"h3"}}),ge=new C({props:{title:"이미지 분할",local:"image-segmentation",headingTag:"h3"}}),ke=new C({props:{title:"깊이 추정",local:"depth-estimation",headingTag:"h3"}}),be=new C({props:{title:"자연어처리",local:"natural-language-processing",headingTag:"h2"}}),Ee=new C({props:{title:"텍스트 분류",local:"text-classification",headingTag:"h3"}}),De=new C({props:{title:"토큰 분류",local:"token-classification",headingTag:"h3"}}),Ae=new C({props:{title:"질의응답",local:"question-answering",headingTag:"h3"}}),E=new it({props:{$$slots:{default:[_n]},$$scope:{ctx:L}}}),Ge=new C({props:{title:"텍스트 생성",local:"text-generation",headingTag:"h3"}}),j=new it({props:{$$slots:{default:[cn]},$$scope:{ctx:L}}}),Ue=new C({props:{title:"요약",local:"summarization",headingTag:"h3"}}),y=new it({props:{$$slots:{default:[xn]},$$scope:{ctx:L}}}),Ye=new C({props:{title:"번역",local:"translation",headingTag:"h3"}}),D=new it({props:{$$slots:{default:[Cn]},$$scope:{ctx:L}}}),{c(){p=a("meta"),o=n(),r=a("p"),x=n(),$(z.$$.fragment),at=n(),q=a("p"),q.innerHTML=Sl,ft=n(),A=a("p"),A.textContent=Gl,mt=n(),F=a("ul"),F.innerHTML=Il,pt=n(),$(g.$$.fragment),rt=n(),$(S.$$.fragment),ot=n(),G=a("p"),G.innerHTML=Ol,$t=n(),M=a("div"),M.innerHTML=Xl,ut=n(),I=a("p"),I.textContent=Wl,Tt=n(),O=a("ol"),O.innerHTML=Ul,vt=n(),X=a("p"),X.textContent=Jl,_t=n(),$(W.$$.fragment),ct=n(),U=a("p"),U.textContent=Kl,xt=n(),J=a("p"),J.innerHTML=Ql,Ct=n(),$(K.$$.fragment),Lt=n(),Q=a("p"),Q.innerHTML=Yl,dt=n(),Y=a("p"),Y.innerHTML=Zl,gt=n(),$(Z.$$.fragment),Mt=n(),ee=a("p"),ee.textContent=ei,Ht=n(),te=a("ol"),te.innerHTML=ti,ht=n(),$(H.$$.fragment),Pt=n(),le=a("p"),le.textContent=li,kt=n(),$(ie.$$.fragment),wt=n(),ne=a("p"),ne.textContent=ii,Nt=n(),$(se.$$.fragment),bt=n(),ae=a("p"),ae.innerHTML=ni,Rt=n(),h=a("div"),h.innerHTML=si,Et=n(),fe=a("p"),fe.textContent=ai,Bt=n(),me=a("ol"),me.innerHTML=fi,jt=n(),pe=a("p"),pe.innerHTML=mi,Vt=n(),$(re.$$.fragment),yt=n(),$(P.$$.fragment),Dt=n(),oe=a("p"),oe.innerHTML=pi,zt=n(),k=a("div"),k.innerHTML=ri,qt=n(),$e=a("small"),$e.innerHTML=oi,At=n(),ue=a("p"),ue.textContent=$i,Ft=n(),w=a("div"),w.innerHTML=ui,St=n(),Te=a("p"),Te.textContent=Ti,Gt=n(),ve=a("ol"),ve.innerHTML=vi,It=n(),_e=a("p"),_e.textContent=_i,Ot=n(),$(ce.$$.fragment),Xt=n(),xe=a("p"),xe.innerHTML=ci,Wt=n(),N=a("div"),N.innerHTML=xi,Ut=n(),Ce=a("ol"),Ce.innerHTML=Ci,Jt=n(),Le=a("p"),Le.textContent=Li,Kt=n(),de=a("p"),de.innerHTML=di,Qt=n(),$(ge.$$.fragment),Yt=n(),Me=a("p"),Me.innerHTML=gi,Zt=n(),b=a("div"),b.innerHTML=Mi,el=n(),He=a("p"),He.textContent=Hi,tl=n(),he=a("ol"),he.innerHTML=hi,ll=n(),Pe=a("p"),Pe.innerHTML=Pi,il=n(),$(ke.$$.fragment),nl=n(),we=a("p"),we.innerHTML=ki,sl=n(),R=a("div"),R.innerHTML=wi,al=n(),Ne=a("ol"),Ne.innerHTML=Ni,fl=n(),$(be.$$.fragment),ml=n(),Re=a("p"),Re.textContent=bi,pl=n(),$(Ee.$$.fragment),rl=n(),Be=a("p"),Be.innerHTML=Ri,ol=n(),je=a("ol"),je.innerHTML=Ei,$l=n(),Ve=a("p"),Ve.textContent=Bi,ul=n(),ye=a("p"),ye.innerHTML=ji,Tl=n(),$(De.$$.fragment),vl=n(),ze=a("p"),ze.textContent=Vi,_l=n(),qe=a("p"),qe.innerHTML=yi,cl=n(),$(Ae.$$.fragment),xl=n(),Fe=a("p"),Fe.innerHTML=Di,Cl=n(),Se=a("p"),Se.innerHTML=zi,Ll=n(),$(E.$$.fragment),dl=n(),$(Ge.$$.fragment),gl=n(),Ie=a("p"),Ie.innerHTML=qi,Ml=n(),B=a("div"),B.innerHTML=Ai,Hl=n(),Oe=a("ol"),Oe.innerHTML=Fi,hl=n(),Xe=a("p"),Xe.innerHTML=Si,Pl=n(),We=a("p"),We.innerHTML=Gi,kl=n(),$(j.$$.fragment),wl=n(),$(Ue.$$.fragment),Nl=n(),Je=a("p"),Je.innerHTML=Ii,bl=n(),V=a("div"),V.innerHTML=Oi,Rl=n(),Ke=a("ol"),Ke.innerHTML=Xi,El=n(),Qe=a("p"),Qe.innerHTML=Wi,Bl=n(),$(y.$$.fragment),jl=n(),$(Ye.$$.fragment),Vl=n(),Ze=a("p"),Ze.innerHTML=Ui,yl=n(),et=a("p"),et.textContent=Ji,Dl=n(),tt=a("p"),tt.textContent=Ki,zl=n(),lt=a("p"),lt.innerHTML=Qi,ql=n(),$(D.$$.fragment),Al=n(),st=a("p"),this.h()},l(e){const t=on("svelte-u9bgzb",document.head);p=f(t,"META",{name:!0,content:!0}),t.forEach(l),o=s(e),r=f(e,"P",{}),an(r).forEach(l),x=s(e),u(z.$$.fragment,e),at=s(e),q=f(e,"P",{"data-svelte-h":!0}),m(q)!=="svelte-x0zli4"&&(q.innerHTML=Sl),ft=s(e),A=f(e,"P",{"data-svelte-h":!0}),m(A)!=="svelte-psllts"&&(A.textContent=Gl),mt=s(e),F=f(e,"UL",{"data-svelte-h":!0}),m(F)!=="svelte-1q8vrgb"&&(F.innerHTML=Il),pt=s(e),u(g.$$.fragment,e),rt=s(e),u(S.$$.fragment,e),ot=s(e),G=f(e,"P",{"data-svelte-h":!0}),m(G)!=="svelte-h4i7z7"&&(G.innerHTML=Ol),$t=s(e),M=f(e,"DIV",{class:!0,"data-svelte-h":!0}),m(M)!=="svelte-xcocqf"&&(M.innerHTML=Xl),ut=s(e),I=f(e,"P",{"data-svelte-h":!0}),m(I)!=="svelte-1ryvk34"&&(I.textContent=Wl),Tt=s(e),O=f(e,"OL",{"data-svelte-h":!0}),m(O)!=="svelte-iv3f70"&&(O.innerHTML=Ul),vt=s(e),X=f(e,"P",{"data-svelte-h":!0}),m(X)!=="svelte-1gqfmpa"&&(X.textContent=Jl),_t=s(e),u(W.$$.fragment,e),ct=s(e),U=f(e,"P",{"data-svelte-h":!0}),m(U)!=="svelte-17pu8hl"&&(U.textContent=Kl),xt=s(e),J=f(e,"P",{"data-svelte-h":!0}),m(J)!=="svelte-ge0lbe"&&(J.innerHTML=Ql),Ct=s(e),u(K.$$.fragment,e),Lt=s(e),Q=f(e,"P",{"data-svelte-h":!0}),m(Q)!=="svelte-1axm4sn"&&(Q.innerHTML=Yl),dt=s(e),Y=f(e,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-1ddxryn"&&(Y.innerHTML=Zl),gt=s(e),u(Z.$$.fragment,e),Mt=s(e),ee=f(e,"P",{"data-svelte-h":!0}),m(ee)!=="svelte-1ly5gy4"&&(ee.textContent=ei),Ht=s(e),te=f(e,"OL",{"data-svelte-h":!0}),m(te)!=="svelte-qtplg3"&&(te.innerHTML=ti),ht=s(e),u(H.$$.fragment,e),Pt=s(e),le=f(e,"P",{"data-svelte-h":!0}),m(le)!=="svelte-1u6qfk0"&&(le.textContent=li),kt=s(e),u(ie.$$.fragment,e),wt=s(e),ne=f(e,"P",{"data-svelte-h":!0}),m(ne)!=="svelte-4jtjhr"&&(ne.textContent=ii),Nt=s(e),u(se.$$.fragment,e),bt=s(e),ae=f(e,"P",{"data-svelte-h":!0}),m(ae)!=="svelte-10wuyn7"&&(ae.innerHTML=ni),Rt=s(e),h=f(e,"DIV",{class:!0,"data-svelte-h":!0}),m(h)!=="svelte-1cij8g4"&&(h.innerHTML=si),Et=s(e),fe=f(e,"P",{"data-svelte-h":!0}),m(fe)!=="svelte-1ll22i7"&&(fe.textContent=ai),Bt=s(e),me=f(e,"OL",{"data-svelte-h":!0}),m(me)!=="svelte-1k71o8x"&&(me.innerHTML=fi),jt=s(e),pe=f(e,"P",{"data-svelte-h":!0}),m(pe)!=="svelte-1447o6m"&&(pe.innerHTML=mi),Vt=s(e),u(re.$$.fragment,e),yt=s(e),u(P.$$.fragment,e),Dt=s(e),oe=f(e,"P",{"data-svelte-h":!0}),m(oe)!=="svelte-1hyo8tc"&&(oe.innerHTML=pi),zt=s(e),k=f(e,"DIV",{class:!0,"data-svelte-h":!0}),m(k)!=="svelte-7yb5mw"&&(k.innerHTML=ri),qt=s(e),$e=f(e,"SMALL",{"data-svelte-h":!0}),m($e)!=="svelte-sz5r20"&&($e.innerHTML=oi),At=s(e),ue=f(e,"P",{"data-svelte-h":!0}),m(ue)!=="svelte-ll0lqy"&&(ue.textContent=$i),Ft=s(e),w=f(e,"DIV",{class:!0,"data-svelte-h":!0}),m(w)!=="svelte-jm5gk8"&&(w.innerHTML=ui),St=s(e),Te=f(e,"P",{"data-svelte-h":!0}),m(Te)!=="svelte-br77n0"&&(Te.textContent=Ti),Gt=s(e),ve=f(e,"OL",{"data-svelte-h":!0}),m(ve)!=="svelte-1jvvs9t"&&(ve.innerHTML=vi),It=s(e),_e=f(e,"P",{"data-svelte-h":!0}),m(_e)!=="svelte-18b5mk3"&&(_e.textContent=_i),Ot=s(e),u(ce.$$.fragment,e),Xt=s(e),xe=f(e,"P",{"data-svelte-h":!0}),m(xe)!=="svelte-m2p8iq"&&(xe.innerHTML=ci),Wt=s(e),N=f(e,"DIV",{class:!0,"data-svelte-h":!0}),m(N)!=="svelte-1h5i4u2"&&(N.innerHTML=xi),Ut=s(e),Ce=f(e,"OL",{"data-svelte-h":!0}),m(Ce)!=="svelte-1r4xi30"&&(Ce.innerHTML=Ci),Jt=s(e),Le=f(e,"P",{"data-svelte-h":!0}),m(Le)!=="svelte-f3t41q"&&(Le.textContent=Li),Kt=s(e),de=f(e,"P",{"data-svelte-h":!0}),m(de)!=="svelte-pblr8h"&&(de.innerHTML=di),Qt=s(e),u(ge.$$.fragment,e),Yt=s(e),Me=f(e,"P",{"data-svelte-h":!0}),m(Me)!=="svelte-g778em"&&(Me.innerHTML=gi),Zt=s(e),b=f(e,"DIV",{class:!0,"data-svelte-h":!0}),m(b)!=="svelte-1sj8vq2"&&(b.innerHTML=Mi),el=s(e),He=f(e,"P",{"data-svelte-h":!0}),m(He)!=="svelte-aiyuje"&&(He.textContent=Hi),tl=s(e),he=f(e,"OL",{"data-svelte-h":!0}),m(he)!=="svelte-uozixt"&&(he.innerHTML=hi),ll=s(e),Pe=f(e,"P",{"data-svelte-h":!0}),m(Pe)!=="svelte-1g9k3om"&&(Pe.innerHTML=Pi),il=s(e),u(ke.$$.fragment,e),nl=s(e),we=f(e,"P",{"data-svelte-h":!0}),m(we)!=="svelte-1eco0tv"&&(we.innerHTML=ki),sl=s(e),R=f(e,"DIV",{class:!0,"data-svelte-h":!0}),m(R)!=="svelte-ou0pxu"&&(R.innerHTML=wi),al=s(e),Ne=f(e,"OL",{"data-svelte-h":!0}),m(Ne)!=="svelte-175o4hy"&&(Ne.innerHTML=Ni),fl=s(e),u(be.$$.fragment,e),ml=s(e),Re=f(e,"P",{"data-svelte-h":!0}),m(Re)!=="svelte-pexwu3"&&(Re.textContent=bi),pl=s(e),u(Ee.$$.fragment,e),rl=s(e),Be=f(e,"P",{"data-svelte-h":!0}),m(Be)!=="svelte-19affru"&&(Be.innerHTML=Ri),ol=s(e),je=f(e,"OL",{"data-svelte-h":!0}),m(je)!=="svelte-18y16ju"&&(je.innerHTML=Ei),$l=s(e),Ve=f(e,"P",{"data-svelte-h":!0}),m(Ve)!=="svelte-jcg9dy"&&(Ve.textContent=Bi),ul=s(e),ye=f(e,"P",{"data-svelte-h":!0}),m(ye)!=="svelte-18sqty5"&&(ye.innerHTML=ji),Tl=s(e),u(De.$$.fragment,e),vl=s(e),ze=f(e,"P",{"data-svelte-h":!0}),m(ze)!=="svelte-1sn0k2k"&&(ze.textContent=Vi),_l=s(e),qe=f(e,"P",{"data-svelte-h":!0}),m(qe)!=="svelte-oo9dd5"&&(qe.innerHTML=yi),cl=s(e),u(Ae.$$.fragment,e),xl=s(e),Fe=f(e,"P",{"data-svelte-h":!0}),m(Fe)!=="svelte-g2ae0z"&&(Fe.innerHTML=Di),Cl=s(e),Se=f(e,"P",{"data-svelte-h":!0}),m(Se)!=="svelte-9nrz52"&&(Se.innerHTML=zi),Ll=s(e),u(E.$$.fragment,e),dl=s(e),u(Ge.$$.fragment,e),gl=s(e),Ie=f(e,"P",{"data-svelte-h":!0}),m(Ie)!=="svelte-hkck8t"&&(Ie.innerHTML=qi),Ml=s(e),B=f(e,"DIV",{class:!0,"data-svelte-h":!0}),m(B)!=="svelte-8822hi"&&(B.innerHTML=Ai),Hl=s(e),Oe=f(e,"OL",{"data-svelte-h":!0}),m(Oe)!=="svelte-1fi5dlu"&&(Oe.innerHTML=Fi),hl=s(e),Xe=f(e,"P",{"data-svelte-h":!0}),m(Xe)!=="svelte-pb62h4"&&(Xe.innerHTML=Si),Pl=s(e),We=f(e,"P",{"data-svelte-h":!0}),m(We)!=="svelte-1txe2eu"&&(We.innerHTML=Gi),kl=s(e),u(j.$$.fragment,e),wl=s(e),u(Ue.$$.fragment,e),Nl=s(e),Je=f(e,"P",{"data-svelte-h":!0}),m(Je)!=="svelte-1ohcbzg"&&(Je.innerHTML=Ii),bl=s(e),V=f(e,"DIV",{class:!0,"data-svelte-h":!0}),m(V)!=="svelte-1vl5qj0"&&(V.innerHTML=Oi),Rl=s(e),Ke=f(e,"OL",{"data-svelte-h":!0}),m(Ke)!=="svelte-15i85rs"&&(Ke.innerHTML=Xi),El=s(e),Qe=f(e,"P",{"data-svelte-h":!0}),m(Qe)!=="svelte-7ci0fn"&&(Qe.innerHTML=Wi),Bl=s(e),u(y.$$.fragment,e),jl=s(e),u(Ye.$$.fragment,e),Vl=s(e),Ze=f(e,"P",{"data-svelte-h":!0}),m(Ze)!=="svelte-7a2z8c"&&(Ze.innerHTML=Ui),yl=s(e),et=f(e,"P",{"data-svelte-h":!0}),m(et)!=="svelte-ga0hng"&&(et.textContent=Ji),Dl=s(e),tt=f(e,"P",{"data-svelte-h":!0}),m(tt)!=="svelte-wdjwk3"&&(tt.textContent=Ki),zl=s(e),lt=f(e,"P",{"data-svelte-h":!0}),m(lt)!=="svelte-17o39yz"&&(lt.innerHTML=Qi),ql=s(e),u(D.$$.fragment,e),Al=s(e),st=f(e,"P",{}),an(st).forEach(l),this.h()},h(){d(p,"name","hf:doc:metadata"),d(p,"content",dn),d(M,"class","flex justify-center"),d(h,"class","flex justify-center"),d(k,"class","flex justify-center"),d(w,"class","flex justify-center"),d(N,"class","flex justify-center"),d(b,"class","flex justify-center"),d(R,"class","flex justify-center"),d(B,"class","flex justify-center"),d(V,"class","flex justify-center")},m(e,t){$n(document.head,p),i(e,o,t),i(e,r,t),i(e,x,t),T(z,e,t),i(e,at,t),i(e,q,t),i(e,ft,t),i(e,A,t),i(e,mt,t),i(e,F,t),i(e,pt,t),T(g,e,t),i(e,rt,t),T(S,e,t),i(e,ot,t),i(e,G,t),i(e,$t,t),i(e,M,t),i(e,ut,t),i(e,I,t),i(e,Tt,t),i(e,O,t),i(e,vt,t),i(e,X,t),i(e,_t,t),T(W,e,t),i(e,ct,t),i(e,U,t),i(e,xt,t),i(e,J,t),i(e,Ct,t),T(K,e,t),i(e,Lt,t),i(e,Q,t),i(e,dt,t),i(e,Y,t),i(e,gt,t),T(Z,e,t),i(e,Mt,t),i(e,ee,t),i(e,Ht,t),i(e,te,t),i(e,ht,t),T(H,e,t),i(e,Pt,t),i(e,le,t),i(e,kt,t),T(ie,e,t),i(e,wt,t),i(e,ne,t),i(e,Nt,t),T(se,e,t),i(e,bt,t),i(e,ae,t),i(e,Rt,t),i(e,h,t),i(e,Et,t),i(e,fe,t),i(e,Bt,t),i(e,me,t),i(e,jt,t),i(e,pe,t),i(e,Vt,t),T(re,e,t),i(e,yt,t),T(P,e,t),i(e,Dt,t),i(e,oe,t),i(e,zt,t),i(e,k,t),i(e,qt,t),i(e,$e,t),i(e,At,t),i(e,ue,t),i(e,Ft,t),i(e,w,t),i(e,St,t),i(e,Te,t),i(e,Gt,t),i(e,ve,t),i(e,It,t),i(e,_e,t),i(e,Ot,t),T(ce,e,t),i(e,Xt,t),i(e,xe,t),i(e,Wt,t),i(e,N,t),i(e,Ut,t),i(e,Ce,t),i(e,Jt,t),i(e,Le,t),i(e,Kt,t),i(e,de,t),i(e,Qt,t),T(ge,e,t),i(e,Yt,t),i(e,Me,t),i(e,Zt,t),i(e,b,t),i(e,el,t),i(e,He,t),i(e,tl,t),i(e,he,t),i(e,ll,t),i(e,Pe,t),i(e,il,t),T(ke,e,t),i(e,nl,t),i(e,we,t),i(e,sl,t),i(e,R,t),i(e,al,t),i(e,Ne,t),i(e,fl,t),T(be,e,t),i(e,ml,t),i(e,Re,t),i(e,pl,t),T(Ee,e,t),i(e,rl,t),i(e,Be,t),i(e,ol,t),i(e,je,t),i(e,$l,t),i(e,Ve,t),i(e,ul,t),i(e,ye,t),i(e,Tl,t),T(De,e,t),i(e,vl,t),i(e,ze,t),i(e,_l,t),i(e,qe,t),i(e,cl,t),T(Ae,e,t),i(e,xl,t),i(e,Fe,t),i(e,Cl,t),i(e,Se,t),i(e,Ll,t),T(E,e,t),i(e,dl,t),T(Ge,e,t),i(e,gl,t),i(e,Ie,t),i(e,Ml,t),i(e,B,t),i(e,Hl,t),i(e,Oe,t),i(e,hl,t),i(e,Xe,t),i(e,Pl,t),i(e,We,t),i(e,kl,t),T(j,e,t),i(e,wl,t),T(Ue,e,t),i(e,Nl,t),i(e,Je,t),i(e,bl,t),i(e,V,t),i(e,Rl,t),i(e,Ke,t),i(e,El,t),i(e,Qe,t),i(e,Bl,t),T(y,e,t),i(e,jl,t),T(Ye,e,t),i(e,Vl,t),i(e,Ze,t),i(e,yl,t),i(e,et,t),i(e,Dl,t),i(e,tt,t),i(e,zl,t),i(e,lt,t),i(e,ql,t),T(D,e,t),i(e,Al,t),i(e,st,t),Fl=!0},p(e,[t]){const Yi={};t&2&&(Yi.$$scope={dirty:t,ctx:e}),g.$set(Yi);const Zi={};t&2&&(Zi.$$scope={dirty:t,ctx:e}),H.$set(Zi);const en={};t&2&&(en.$$scope={dirty:t,ctx:e}),P.$set(en);const tn={};t&2&&(tn.$$scope={dirty:t,ctx:e}),E.$set(tn);const ln={};t&2&&(ln.$$scope={dirty:t,ctx:e}),j.$set(ln);const nn={};t&2&&(nn.$$scope={dirty:t,ctx:e}),y.$set(nn);const sn={};t&2&&(sn.$$scope={dirty:t,ctx:e}),D.$set(sn)},i(e){Fl||(v(z.$$.fragment,e),v(g.$$.fragment,e),v(S.$$.fragment,e),v(W.$$.fragment,e),v(K.$$.fragment,e),v(Z.$$.fragment,e),v(H.$$.fragment,e),v(ie.$$.fragment,e),v(se.$$.fragment,e),v(re.$$.fragment,e),v(P.$$.fragment,e),v(ce.$$.fragment,e),v(ge.$$.fragment,e),v(ke.$$.fragment,e),v(be.$$.fragment,e),v(Ee.$$.fragment,e),v(De.$$.fragment,e),v(Ae.$$.fragment,e),v(E.$$.fragment,e),v(Ge.$$.fragment,e),v(j.$$.fragment,e),v(Ue.$$.fragment,e),v(y.$$.fragment,e),v(Ye.$$.fragment,e),v(D.$$.fragment,e),Fl=!0)},o(e){_(z.$$.fragment,e),_(g.$$.fragment,e),_(S.$$.fragment,e),_(W.$$.fragment,e),_(K.$$.fragment,e),_(Z.$$.fragment,e),_(H.$$.fragment,e),_(ie.$$.fragment,e),_(se.$$.fragment,e),_(re.$$.fragment,e),_(P.$$.fragment,e),_(ce.$$.fragment,e),_(ge.$$.fragment,e),_(ke.$$.fragment,e),_(be.$$.fragment,e),_(Ee.$$.fragment,e),_(De.$$.fragment,e),_(Ae.$$.fragment,e),_(E.$$.fragment,e),_(Ge.$$.fragment,e),_(j.$$.fragment,e),_(Ue.$$.fragment,e),_(y.$$.fragment,e),_(Ye.$$.fragment,e),_(D.$$.fragment,e),Fl=!1},d(e){e&&(l(o),l(r),l(x),l(at),l(q),l(ft),l(A),l(mt),l(F),l(pt),l(rt),l(ot),l(G),l($t),l(M),l(ut),l(I),l(Tt),l(O),l(vt),l(X),l(_t),l(ct),l(U),l(xt),l(J),l(Ct),l(Lt),l(Q),l(dt),l(Y),l(gt),l(Mt),l(ee),l(Ht),l(te),l(ht),l(Pt),l(le),l(kt),l(wt),l(ne),l(Nt),l(bt),l(ae),l(Rt),l(h),l(Et),l(fe),l(Bt),l(me),l(jt),l(pe),l(Vt),l(yt),l(Dt),l(oe),l(zt),l(k),l(qt),l($e),l(At),l(ue),l(Ft),l(w),l(St),l(Te),l(Gt),l(ve),l(It),l(_e),l(Ot),l(Xt),l(xe),l(Wt),l(N),l(Ut),l(Ce),l(Jt),l(Le),l(Kt),l(de),l(Qt),l(Yt),l(Me),l(Zt),l(b),l(el),l(He),l(tl),l(he),l(ll),l(Pe),l(il),l(nl),l(we),l(sl),l(R),l(al),l(Ne),l(fl),l(ml),l(Re),l(pl),l(rl),l(Be),l(ol),l(je),l($l),l(Ve),l(ul),l(ye),l(Tl),l(vl),l(ze),l(_l),l(qe),l(cl),l(xl),l(Fe),l(Cl),l(Se),l(Ll),l(dl),l(gl),l(Ie),l(Ml),l(B),l(Hl),l(Oe),l(hl),l(Xe),l(Pl),l(We),l(kl),l(wl),l(Nl),l(Je),l(bl),l(V),l(Rl),l(Ke),l(El),l(Qe),l(Bl),l(jl),l(Vl),l(Ze),l(yl),l(et),l(Dl),l(tt),l(zl),l(lt),l(ql),l(Al),l(st)),l(p),c(z,e),c(g,e),c(S,e),c(W,e),c(K,e),c(Z,e),c(H,e),c(ie,e),c(se,e),c(re,e),c(P,e),c(ce,e),c(ge,e),c(ke,e),c(be,e),c(Ee,e),c(De,e),c(Ae,e),c(E,e),c(Ge,e),c(j,e),c(Ue,e),c(y,e),c(Ye,e),c(D,e)}}}const dn='{"title":"🤗 Transformers로 작업을 해결하는 방법","local":"how-transformers-solve-tasks","sections":[{"title":"음성 및 오디오","local":"speech-and-audio","sections":[{"title":"오디오 분류","local":"audio-classification","sections":[],"depth":3},{"title":"자동 음성 인식","local":"automatic-speech-recognition","sections":[],"depth":3}],"depth":2},{"title":"컴퓨터 비전","local":"computer-vision","sections":[{"title":"이미지 분류","local":"image-classification","sections":[{"title":"Transformer","local":"transformer","sections":[],"depth":4},{"title":"CNN","local":"cnn","sections":[],"depth":4}],"depth":3},{"title":"객체 탐지","local":"object-detection","sections":[],"depth":3},{"title":"이미지 분할","local":"image-segmentation","sections":[],"depth":3},{"title":"깊이 추정","local":"depth-estimation","sections":[],"depth":3}],"depth":2},{"title":"자연어처리","local":"natural-language-processing","sections":[{"title":"텍스트 분류","local":"text-classification","sections":[],"depth":3},{"title":"토큰 분류","local":"token-classification","sections":[],"depth":3},{"title":"질의응답","local":"question-answering","sections":[],"depth":3},{"title":"텍스트 생성","local":"text-generation","sections":[],"depth":3},{"title":"요약","local":"summarization","sections":[],"depth":3},{"title":"번역","local":"translation","sections":[],"depth":3}],"depth":2}],"depth":1}';function gn(L){return mn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class kn extends pn{constructor(p){super(),rn(this,p,gn,Ln,fn,{})}}export{kn as component};
