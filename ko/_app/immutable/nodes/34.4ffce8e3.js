import{s as pl,o as ml,n as al}from"../chunks/scheduler.56730f09.js";import{S as ol,i as rl,g as a,s,r,A as fl,h as p,f as l,c as i,j as nl,u as f,x as m,k as sl,y as bl,a as n,v as b,d,t as u,w as c}from"../chunks/index.1f144517.js";import{T as il}from"../chunks/Tip.41e845e5.js";import{C as T}from"../chunks/CodeBlock.738eeccb.js";import{H as $}from"../chunks/Heading.57d46534.js";function dl(Ue){let o,y="이 기능은 다중 GPU 설정에서도 사용할 수 있습니다.";return{c(){o=a("p"),o.textContent=y},l(M){o=p(M,"P",{"data-svelte-h":!0}),m(o)!=="svelte-1wakscf"&&(o.textContent=y)},m(M,g){n(M,o,g)},p:al,d(M){M&&l(o)}}}function ul(Ue){let o,y="이 기능은 다중 GPU 설정에서도 사용할 수 있습니다.";return{c(){o=a("p"),o.textContent=y},l(M){o=p(M,"P",{"data-svelte-h":!0}),m(o)!=="svelte-1wakscf"&&(o.textContent=y)},m(M,g){n(M,o,g)},p:al,d(M){M&&l(o)}}}function cl(Ue){let o,y,M,g,_,_e,h,ht='이 가이드 외에도, <a href="perf_train_gpu_one">단일 GPU에서의 훈련 가이드</a>와 <a href="perf_infer_cpu">CPU에서의 추론 가이드</a>에서도 관련 정보를 찾을 수 있습니다.',he,C,Ce,w,Ct='PyTorch 네이티브 <a href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/" rel="nofollow"><code>nn.MultiHeadAttention</code></a> 어텐션 패스트패스인 BetterTransformer는 <a href="https://huggingface.co/docs/optimum/bettertransformer/overview" rel="nofollow">🤗 Optimum 라이브러리</a>의 통합을 통해 Transformers와 함께 사용할 수 있습니다.',we,J,wt='PyTorch의 어텐션 패스트패스는 커널 퓨전과 <a href="https://pytorch.org/docs/stable/nested.html" rel="nofollow">중첩된 텐서</a>의 사용을 통해 추론 속도를 높일 수 있습니다. 자세한 벤치마크는 <a href="https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2" rel="nofollow">이 블로그 글</a>에서 확인할 수 있습니다.',Je,x,Jt='<a href="https://github.com/huggingface/optimum" rel="nofollow"><code>optimum</code></a> 패키지를 설치한 후에는 추론 중 Better Transformer를 사용할 수 있도록 <code>to_bettertransformer()</code>를 호출하여 관련 내부 모듈을 대체합니다:',xe,G,Ge,W,xt="<code>reverse_bettertransformer()</code> 메소드는 정규화된 transformers 모델링을 사용하기 위해 모델을 저장하기 전 원래의 모델링으로 돌아갈 수 있도록 해줍니다:",We,j,je,Z,Gt='PyTorch 2.0부터는 어텐션 패스트패스가 인코더와 디코더 모두에서 지원됩니다. 지원되는 아키텍처 목록은 <a href="https://huggingface.co/docs/optimum/bettertransformer/overview#supported-models" rel="nofollow">여기</a>에서 확인할 수 있습니다.',Ze,F,Fe,P,Wt="<code>bitsandbytes</code>를 설치하면 GPU에서 손쉽게 모델을 압축할 수 있습니다. FP4 양자화를 사용하면 원래의 전체 정밀도 버전과 비교하여 모델 크기를 최대 8배 줄일 수 있습니다. 아래에서 시작하는 방법을 확인하세요.",Pe,U,ke,k,Ve,V,jt=`<li><p>최신 <code>bitsandbytes</code> 라이브러리
<code>pip install bitsandbytes&gt;=0.39.0</code></p></li> <li><p>최신 <code>accelerate</code>를 소스에서 설치
<code>pip install git+https://github.com/huggingface/accelerate.git</code></p></li> <li><p>최신 <code>transformers</code>를 소스에서 설치
<code>pip install git+https://github.com/huggingface/transformers.git</code></p></li>`,Re,R,Ie,I,Zt="다음 코드를 실행하여 단일 GPU에서 빠르게 FP4 모델을 실행할 수 있습니다.",Xe,X,He,H,Ft="<code>device_map</code>은 선택 사항입니다. 그러나 <code>device_map = &#39;auto&#39;</code>로 설정하는 것이 사용 가능한 리소스를 효율적으로 디스패치하기 때문에 추론에 있어 권장됩니다.",Le,L,Be,B,Pt="다중 GPU에서 혼합 4비트 모델을 가져오는 방법은 단일 GPU 설정과 동일합니다(동일한 명령어 사용):",qe,q,Ye,Y,kt="하지만 <code>accelerate</code>를 사용하여 각 GPU에 할당할 GPU RAM을 제어할 수 있습니다. 다음과 같이 <code>max_memory</code> 인수를 사용하세요:",Ee,E,Ne,N,Vt="이 예에서는 첫 번째 GPU가 600MB의 메모리를 사용하고 두 번째 GPU가 1GB를 사용합니다.",Qe,Q,ze,z,Rt='이 방법의 더 고급 사용법에 대해서는 <a href="main_classes/quantization">양자화</a> 문서 페이지를 참조하세요.',Ae,A,Se,v,Ke,S,It=`<a href="https://arxiv.org/abs/2208.07339" rel="nofollow"><code>LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale</code></a> 논문에서 우리는 몇 줄의 코드로 Hub의 모든 모델에 대한 Hugging Face 통합을 지원합니다.
이 방법은 <code>float16</code> 및 <code>bfloat16</code> 가중치에 대해 <code>nn.Linear</code> 크기를 2배로 줄이고, <code>float32</code> 가중치에 대해 4배로 줄입니다. 이는 절반 정밀도에서 이상치를 처리함으로써 품질에 거의 영향을 미치지 않습니다.`,Oe,K,Xt='<img src="https://cdn-uploads.huggingface.co/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png" alt="HFxbitsandbytes.png"/>',De,O,Ht=`Int8 혼합 정밀도 행렬 분해는 행렬 곱셈을 두 개의 스트림으로 분리합니다: (1) fp16로 곱해지는 체계적인 특이값 이상치 스트림 행렬(0.01%) 및 (2) int8 행렬 곱셈의 일반적인 스트림(99.9%). 이 방법을 사용하면 매우 큰 모델에 대해 예측 저하 없이 int8 추론이 가능합니다.
이 방법에 대한 자세한 내용은 <a href="https://arxiv.org/abs/2208.07339" rel="nofollow">논문</a>이나 <a href="https://huggingface.co/blog/hf-bitsandbytes-integration" rel="nofollow">통합에 관한 블로그 글</a>에서 확인할 수 있습니다.`,et,D,Lt='<img src="https://cdn-uploads.huggingface.co/production/uploads/1660567469965-62441d1d9fdefb55a0b7d12c.gif" alt="MixedInt8.gif"/>',tt,ee,Bt=`커널은 GPU 전용으로 컴파일되어 있기 때문에 혼합 8비트 모델을 실행하려면 GPU가 필요합니다. 이 기능을 사용하기 전에 모델의 1/4(또는 모델 가중치가 절반 정밀도인 경우 절반)을 저장할 충분한 GPU 메모리가 있는지 확인하세요.
이 모듈을 사용하는 데 도움이 되는 몇 가지 참고 사항이 아래에 나와 있습니다. 또는 <a href="#colab-demos">Google colab</a>에서 데모를 따라할 수도 있습니다.`,lt,te,nt,le,qt=`<li><code>bitsandbytes&lt;0.37.0</code>을 사용하는 경우, 8비트 텐서 코어(Turing, Ampere 또는 이후 아키텍처 - 예: T4, RTX20s RTX30s, A40-A100)를 지원하는 NVIDIA GPU에서 실행하는지 확인하세요. <code>bitsandbytes&gt;=0.37.0</code>을 사용하는 경우, 모든 GPU가 지원됩니다.</li> <li>올바른 버전의 <code>bitsandbytes</code>를 다음 명령으로 설치하세요:
<code>pip install bitsandbytes&gt;=0.31.5</code></li> <li><code>accelerate</code>를 설치하세요
<code>pip install accelerate&gt;=0.12.0</code></li>`,st,ne,it,se,Yt="필요한 라이브러리를 설치한 후 혼합 8비트 모델을 가져오는 방법은 다음과 같습니다:",at,ie,pt,ae,Et="텍스트 생성의 경우:",mt,pe,Nt="<li><code>pipeline()</code> 함수 대신 모델의 <code>generate()</code> 메소드를 사용하는 것을 권장합니다. <code>pipeline()</code> 함수로는 추론이 가능하지만, 혼합 8비트 모델에 최적화되지 않았기 때문에 <code>generate()</code> 메소드를 사용하는 것보다 느릴 수 있습니다. 또한, nucleus 샘플링과 같은 일부 샘플링 전략은 혼합 8비트 모델에 대해 <code>pipeline()</code> 함수에서 지원되지 않습니다.</li> <li>입력을 모델과 동일한 GPU에 배치하는 것이 좋습니다.</li>",ot,me,Qt="다음은 간단한 예입니다:",rt,oe,ft,re,bt,fe,zt="다중 GPU에서 혼합 8비트 모델을 로드하는 방법은 단일 GPU 설정과 동일합니다(동일한 명령어 사용):",dt,be,ut,de,At="하지만 <code>accelerate</code>를 사용하여 각 GPU에 할당할 GPU RAM을 제어할 수 있습니다. 다음과 같이 <code>max_memory</code> 인수를 사용하세요:",ct,ue,Mt,ce,St="이 예시에서는 첫 번째 GPU가 1GB의 메모리를 사용하고 두 번째 GPU가 2GB를 사용합니다.",$t,Me,yt,$e,Kt=`이 방법을 사용하면 이전에 Google Colab에서 추론할 수 없었던 모델에 대해 추론할 수 있습니다.
Google Colab에서 8비트 양자화를 사용하여 T5-11b(42GB in fp32)를 실행하는 데모를 확인하세요:`,Tt,ye,Ot='<a href="https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab: T5-11b demo"/></a>',gt,Te,Dt="또는 BLOOM-3B에 대한 데모를 확인하세요:",Ut,ge,el='<a href="https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab: BLOOM-3b demo"/></a>',vt,ve,_t;return _=new $({props:{title:"단일 GPU에서 효율적인 추론",local:"efficient-inference-on-a-single-gpu",headingTag:"h1"}}),C=new $({props:{title:"Better Transformer: PyTorch 네이티브 Transformer 패스트패스",local:"better-transformer-pytorchnative-transformer-fastpath",headingTag:"h2"}}),G=new T({props:{code:"bW9kZWwlMjAlM0QlMjBtb2RlbC50b19iZXR0ZXJ0cmFuc2Zvcm1lcigp",highlighted:"model = model.to_bettertransformer()",wrap:!1}}),j=new T({props:{code:"bW9kZWwlMjAlM0QlMjBtb2RlbC5yZXZlcnNlX2JldHRlcnRyYW5zZm9ybWVyKCklMEFtb2RlbC5zYXZlX3ByZXRyYWluZWQoJTIyc2F2ZWRfbW9kZWwlMjIp",highlighted:`model = model.reverse_bettertransformer()
model.save_pretrained(<span class="hljs-string">&quot;saved_model&quot;</span>)`,wrap:!1}}),F=new $({props:{title:"FP4 혼합 정밀도 추론을 위한 bitsandbytes 통합",local:"bitsandbytes-integration-for-fp4-mixedprecision-inference",headingTag:"h2"}}),U=new il({props:{$$slots:{default:[dl]},$$scope:{ctx:Ue}}}),k=new $({props:{title:"요구 사항",local:"requirements-for-fp4-mixedprecision-inference",headingTag:"h3"}}),R=new $({props:{title:"FP4 모델 실행 - 단일 GPU 설정 - 빠른 시작",local:"running-fp4-models-single-gpu-setup-quickstart",headingTag:"h3"}}),X=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfbmFtZSUyMCUzRCUyMCUyMmJpZ3NjaWVuY2UlMkZibG9vbS0yYjUlMjIlMEFtb2RlbF80Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fNGJpdCUzRFRydWUp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),L=new $({props:{title:"FP4 모델 실행 - 다중 GPU 설정",local:"running-fp4-models-multi-gpu-setup",headingTag:"h3"}}),q=new T({props:{code:"bW9kZWxfbmFtZSUyMCUzRCUyMCUyMmJpZ3NjaWVuY2UlMkZibG9vbS0yYjUlMjIlMEFtb2RlbF80Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fNGJpdCUzRFRydWUp",highlighted:`model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),E=new T({props:{code:"bWF4X21lbW9yeV9tYXBwaW5nJTIwJTNEJTIwJTdCMCUzQSUyMCUyMjYwME1CJTIyJTJDJTIwMSUzQSUyMCUyMjFHQiUyMiU3RCUwQW1vZGVsX25hbWUlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tM2IlMjIlMEFtb2RlbF80Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fNGJpdCUzRFRydWUlMkMlMjBtYXhfbWVtb3J5JTNEbWF4X21lbW9yeV9tYXBwaW5nJTBBKQ==",highlighted:`max_memory_mapping = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;600MB&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;1GB&quot;</span>}
model_name = <span class="hljs-string">&quot;bigscience/bloom-3b&quot;</span>
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>, max_memory=max_memory_mapping
)`,wrap:!1}}),Q=new $({props:{title:"고급 사용법",local:"advanced-usage",headingTag:"h3"}}),A=new $({props:{title:"Int8 혼합 정밀도 행렬 분해를 위한 bitsandbytes 통합",local:"bitsandbytes-integration-for-int8-mixedprecision-matrix-decomposition",headingTag:"h2"}}),v=new il({props:{$$slots:{default:[ul]},$$scope:{ctx:Ue}}}),te=new $({props:{title:"요구 사항",local:"requirements-for-int8-mixedprecision-matrix-decomposition",headingTag:"h3"}}),ne=new $({props:{title:"혼합 Int8 모델 실행 - 단일 GPU 설정",local:"running-mixedint8-models-single-gpu-setup",headingTag:"h3"}}),ie=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfbmFtZSUyMCUzRCUyMCUyMmJpZ3NjaWVuY2UlMkZibG9vbS0yYjUlMjIlMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),oe=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsX25hbWUlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMmI1JTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfbmFtZSklMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIySGVsbG8lMkMlMjBteSUyMGxsYW1hJTIwaXMlMjBjdXRlJTIyJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHByb21wdCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKCUyMmN1ZGElMjIpJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQW91dHB1dHMlMjAlM0QlMjB0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)

prompt = <span class="hljs-string">&quot;Hello, my llama is cute&quot;</span>
inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)`,wrap:!1}}),re=new $({props:{title:"혼합 Int8 모델 실행 - 다중 GPU 설정",local:"running-mixedint8-models-multi-gpu-setup",headingTag:"h3"}}),be=new T({props:{code:"bW9kZWxfbmFtZSUyMCUzRCUyMCUyMmJpZ3NjaWVuY2UlMkZibG9vbS0yYjUlMjIlMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUp",highlighted:`model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),ue=new T({props:{code:"bWF4X21lbW9yeV9tYXBwaW5nJTIwJTNEJTIwJTdCMCUzQSUyMCUyMjFHQiUyMiUyQyUyMDElM0ElMjAlMjIyR0IlMjIlN0QlMEFtb2RlbF9uYW1lJTIwJTNEJTIwJTIyYmlnc2NpZW5jZSUyRmJsb29tLTNiJTIyJTBBbW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjBtb2RlbF9uYW1lJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlJTJDJTIwbWF4X21lbW9yeSUzRG1heF9tZW1vcnlfbWFwcGluZyUwQSk=",highlighted:`max_memory_mapping = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;1GB&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;2GB&quot;</span>}
model_name = <span class="hljs-string">&quot;bigscience/bloom-3b&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, max_memory=max_memory_mapping
)`,wrap:!1}}),Me=new $({props:{title:"Colab 데모",local:"colab-demos",headingTag:"h3"}}),{c(){o=a("meta"),y=s(),M=a("p"),g=s(),r(_.$$.fragment),_e=s(),h=a("p"),h.innerHTML=ht,he=s(),r(C.$$.fragment),Ce=s(),w=a("p"),w.innerHTML=Ct,we=s(),J=a("p"),J.innerHTML=wt,Je=s(),x=a("p"),x.innerHTML=Jt,xe=s(),r(G.$$.fragment),Ge=s(),W=a("p"),W.innerHTML=xt,We=s(),r(j.$$.fragment),je=s(),Z=a("p"),Z.innerHTML=Gt,Ze=s(),r(F.$$.fragment),Fe=s(),P=a("p"),P.innerHTML=Wt,Pe=s(),r(U.$$.fragment),ke=s(),r(k.$$.fragment),Ve=s(),V=a("ul"),V.innerHTML=jt,Re=s(),r(R.$$.fragment),Ie=s(),I=a("p"),I.textContent=Zt,Xe=s(),r(X.$$.fragment),He=s(),H=a("p"),H.innerHTML=Ft,Le=s(),r(L.$$.fragment),Be=s(),B=a("p"),B.textContent=Pt,qe=s(),r(q.$$.fragment),Ye=s(),Y=a("p"),Y.innerHTML=kt,Ee=s(),r(E.$$.fragment),Ne=s(),N=a("p"),N.textContent=Vt,Qe=s(),r(Q.$$.fragment),ze=s(),z=a("p"),z.innerHTML=Rt,Ae=s(),r(A.$$.fragment),Se=s(),r(v.$$.fragment),Ke=s(),S=a("p"),S.innerHTML=It,Oe=s(),K=a("p"),K.innerHTML=Xt,De=s(),O=a("p"),O.innerHTML=Ht,et=s(),D=a("p"),D.innerHTML=Lt,tt=s(),ee=a("p"),ee.innerHTML=Bt,lt=s(),r(te.$$.fragment),nt=s(),le=a("ul"),le.innerHTML=qt,st=s(),r(ne.$$.fragment),it=s(),se=a("p"),se.textContent=Yt,at=s(),r(ie.$$.fragment),pt=s(),ae=a("p"),ae.textContent=Et,mt=s(),pe=a("ul"),pe.innerHTML=Nt,ot=s(),me=a("p"),me.textContent=Qt,rt=s(),r(oe.$$.fragment),ft=s(),r(re.$$.fragment),bt=s(),fe=a("p"),fe.textContent=zt,dt=s(),r(be.$$.fragment),ut=s(),de=a("p"),de.innerHTML=At,ct=s(),r(ue.$$.fragment),Mt=s(),ce=a("p"),ce.textContent=St,$t=s(),r(Me.$$.fragment),yt=s(),$e=a("p"),$e.textContent=Kt,Tt=s(),ye=a("p"),ye.innerHTML=Ot,gt=s(),Te=a("p"),Te.textContent=Dt,Ut=s(),ge=a("p"),ge.innerHTML=el,vt=s(),ve=a("p"),this.h()},l(e){const t=fl("svelte-u9bgzb",document.head);o=p(t,"META",{name:!0,content:!0}),t.forEach(l),y=i(e),M=p(e,"P",{}),nl(M).forEach(l),g=i(e),f(_.$$.fragment,e),_e=i(e),h=p(e,"P",{"data-svelte-h":!0}),m(h)!=="svelte-1m7ldpj"&&(h.innerHTML=ht),he=i(e),f(C.$$.fragment,e),Ce=i(e),w=p(e,"P",{"data-svelte-h":!0}),m(w)!=="svelte-ee52o1"&&(w.innerHTML=Ct),we=i(e),J=p(e,"P",{"data-svelte-h":!0}),m(J)!=="svelte-8k8zjr"&&(J.innerHTML=wt),Je=i(e),x=p(e,"P",{"data-svelte-h":!0}),m(x)!=="svelte-1rnbe8f"&&(x.innerHTML=Jt),xe=i(e),f(G.$$.fragment,e),Ge=i(e),W=p(e,"P",{"data-svelte-h":!0}),m(W)!=="svelte-1u5t675"&&(W.innerHTML=xt),We=i(e),f(j.$$.fragment,e),je=i(e),Z=p(e,"P",{"data-svelte-h":!0}),m(Z)!=="svelte-17375w3"&&(Z.innerHTML=Gt),Ze=i(e),f(F.$$.fragment,e),Fe=i(e),P=p(e,"P",{"data-svelte-h":!0}),m(P)!=="svelte-sxo64f"&&(P.innerHTML=Wt),Pe=i(e),f(U.$$.fragment,e),ke=i(e),f(k.$$.fragment,e),Ve=i(e),V=p(e,"UL",{"data-svelte-h":!0}),m(V)!=="svelte-dwg1v5"&&(V.innerHTML=jt),Re=i(e),f(R.$$.fragment,e),Ie=i(e),I=p(e,"P",{"data-svelte-h":!0}),m(I)!=="svelte-60dj5"&&(I.textContent=Zt),Xe=i(e),f(X.$$.fragment,e),He=i(e),H=p(e,"P",{"data-svelte-h":!0}),m(H)!=="svelte-1xe6z7w"&&(H.innerHTML=Ft),Le=i(e),f(L.$$.fragment,e),Be=i(e),B=p(e,"P",{"data-svelte-h":!0}),m(B)!=="svelte-11nsoog"&&(B.textContent=Pt),qe=i(e),f(q.$$.fragment,e),Ye=i(e),Y=p(e,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-7qj9fn"&&(Y.innerHTML=kt),Ee=i(e),f(E.$$.fragment,e),Ne=i(e),N=p(e,"P",{"data-svelte-h":!0}),m(N)!=="svelte-1wq62m5"&&(N.textContent=Vt),Qe=i(e),f(Q.$$.fragment,e),ze=i(e),z=p(e,"P",{"data-svelte-h":!0}),m(z)!=="svelte-19t36l7"&&(z.innerHTML=Rt),Ae=i(e),f(A.$$.fragment,e),Se=i(e),f(v.$$.fragment,e),Ke=i(e),S=p(e,"P",{"data-svelte-h":!0}),m(S)!=="svelte-1ynbgg5"&&(S.innerHTML=It),Oe=i(e),K=p(e,"P",{"data-svelte-h":!0}),m(K)!=="svelte-1tsrdqi"&&(K.innerHTML=Xt),De=i(e),O=p(e,"P",{"data-svelte-h":!0}),m(O)!=="svelte-sz8751"&&(O.innerHTML=Ht),et=i(e),D=p(e,"P",{"data-svelte-h":!0}),m(D)!=="svelte-y2mgdg"&&(D.innerHTML=Lt),tt=i(e),ee=p(e,"P",{"data-svelte-h":!0}),m(ee)!=="svelte-ewmhxc"&&(ee.innerHTML=Bt),lt=i(e),f(te.$$.fragment,e),nt=i(e),le=p(e,"UL",{"data-svelte-h":!0}),m(le)!=="svelte-r9nrya"&&(le.innerHTML=qt),st=i(e),f(ne.$$.fragment,e),it=i(e),se=p(e,"P",{"data-svelte-h":!0}),m(se)!=="svelte-1trjkap"&&(se.textContent=Yt),at=i(e),f(ie.$$.fragment,e),pt=i(e),ae=p(e,"P",{"data-svelte-h":!0}),m(ae)!=="svelte-bekc1k"&&(ae.textContent=Et),mt=i(e),pe=p(e,"UL",{"data-svelte-h":!0}),m(pe)!=="svelte-daelpd"&&(pe.innerHTML=Nt),ot=i(e),me=p(e,"P",{"data-svelte-h":!0}),m(me)!=="svelte-1md3fzz"&&(me.textContent=Qt),rt=i(e),f(oe.$$.fragment,e),ft=i(e),f(re.$$.fragment,e),bt=i(e),fe=p(e,"P",{"data-svelte-h":!0}),m(fe)!=="svelte-bnkuc8"&&(fe.textContent=zt),dt=i(e),f(be.$$.fragment,e),ut=i(e),de=p(e,"P",{"data-svelte-h":!0}),m(de)!=="svelte-7qj9fn"&&(de.innerHTML=At),ct=i(e),f(ue.$$.fragment,e),Mt=i(e),ce=p(e,"P",{"data-svelte-h":!0}),m(ce)!=="svelte-1bub4cv"&&(ce.textContent=St),$t=i(e),f(Me.$$.fragment,e),yt=i(e),$e=p(e,"P",{"data-svelte-h":!0}),m($e)!=="svelte-1at89fq"&&($e.textContent=Kt),Tt=i(e),ye=p(e,"P",{"data-svelte-h":!0}),m(ye)!=="svelte-1yb5ek4"&&(ye.innerHTML=Ot),gt=i(e),Te=p(e,"P",{"data-svelte-h":!0}),m(Te)!=="svelte-u2br1q"&&(Te.textContent=Dt),Ut=i(e),ge=p(e,"P",{"data-svelte-h":!0}),m(ge)!=="svelte-6z7881"&&(ge.innerHTML=el),vt=i(e),ve=p(e,"P",{}),nl(ve).forEach(l),this.h()},h(){sl(o,"name","hf:doc:metadata"),sl(o,"content",Ml)},m(e,t){bl(document.head,o),n(e,y,t),n(e,M,t),n(e,g,t),b(_,e,t),n(e,_e,t),n(e,h,t),n(e,he,t),b(C,e,t),n(e,Ce,t),n(e,w,t),n(e,we,t),n(e,J,t),n(e,Je,t),n(e,x,t),n(e,xe,t),b(G,e,t),n(e,Ge,t),n(e,W,t),n(e,We,t),b(j,e,t),n(e,je,t),n(e,Z,t),n(e,Ze,t),b(F,e,t),n(e,Fe,t),n(e,P,t),n(e,Pe,t),b(U,e,t),n(e,ke,t),b(k,e,t),n(e,Ve,t),n(e,V,t),n(e,Re,t),b(R,e,t),n(e,Ie,t),n(e,I,t),n(e,Xe,t),b(X,e,t),n(e,He,t),n(e,H,t),n(e,Le,t),b(L,e,t),n(e,Be,t),n(e,B,t),n(e,qe,t),b(q,e,t),n(e,Ye,t),n(e,Y,t),n(e,Ee,t),b(E,e,t),n(e,Ne,t),n(e,N,t),n(e,Qe,t),b(Q,e,t),n(e,ze,t),n(e,z,t),n(e,Ae,t),b(A,e,t),n(e,Se,t),b(v,e,t),n(e,Ke,t),n(e,S,t),n(e,Oe,t),n(e,K,t),n(e,De,t),n(e,O,t),n(e,et,t),n(e,D,t),n(e,tt,t),n(e,ee,t),n(e,lt,t),b(te,e,t),n(e,nt,t),n(e,le,t),n(e,st,t),b(ne,e,t),n(e,it,t),n(e,se,t),n(e,at,t),b(ie,e,t),n(e,pt,t),n(e,ae,t),n(e,mt,t),n(e,pe,t),n(e,ot,t),n(e,me,t),n(e,rt,t),b(oe,e,t),n(e,ft,t),b(re,e,t),n(e,bt,t),n(e,fe,t),n(e,dt,t),b(be,e,t),n(e,ut,t),n(e,de,t),n(e,ct,t),b(ue,e,t),n(e,Mt,t),n(e,ce,t),n(e,$t,t),b(Me,e,t),n(e,yt,t),n(e,$e,t),n(e,Tt,t),n(e,ye,t),n(e,gt,t),n(e,Te,t),n(e,Ut,t),n(e,ge,t),n(e,vt,t),n(e,ve,t),_t=!0},p(e,[t]){const tl={};t&2&&(tl.$$scope={dirty:t,ctx:e}),U.$set(tl);const ll={};t&2&&(ll.$$scope={dirty:t,ctx:e}),v.$set(ll)},i(e){_t||(d(_.$$.fragment,e),d(C.$$.fragment,e),d(G.$$.fragment,e),d(j.$$.fragment,e),d(F.$$.fragment,e),d(U.$$.fragment,e),d(k.$$.fragment,e),d(R.$$.fragment,e),d(X.$$.fragment,e),d(L.$$.fragment,e),d(q.$$.fragment,e),d(E.$$.fragment,e),d(Q.$$.fragment,e),d(A.$$.fragment,e),d(v.$$.fragment,e),d(te.$$.fragment,e),d(ne.$$.fragment,e),d(ie.$$.fragment,e),d(oe.$$.fragment,e),d(re.$$.fragment,e),d(be.$$.fragment,e),d(ue.$$.fragment,e),d(Me.$$.fragment,e),_t=!0)},o(e){u(_.$$.fragment,e),u(C.$$.fragment,e),u(G.$$.fragment,e),u(j.$$.fragment,e),u(F.$$.fragment,e),u(U.$$.fragment,e),u(k.$$.fragment,e),u(R.$$.fragment,e),u(X.$$.fragment,e),u(L.$$.fragment,e),u(q.$$.fragment,e),u(E.$$.fragment,e),u(Q.$$.fragment,e),u(A.$$.fragment,e),u(v.$$.fragment,e),u(te.$$.fragment,e),u(ne.$$.fragment,e),u(ie.$$.fragment,e),u(oe.$$.fragment,e),u(re.$$.fragment,e),u(be.$$.fragment,e),u(ue.$$.fragment,e),u(Me.$$.fragment,e),_t=!1},d(e){e&&(l(y),l(M),l(g),l(_e),l(h),l(he),l(Ce),l(w),l(we),l(J),l(Je),l(x),l(xe),l(Ge),l(W),l(We),l(je),l(Z),l(Ze),l(Fe),l(P),l(Pe),l(ke),l(Ve),l(V),l(Re),l(Ie),l(I),l(Xe),l(He),l(H),l(Le),l(Be),l(B),l(qe),l(Ye),l(Y),l(Ee),l(Ne),l(N),l(Qe),l(ze),l(z),l(Ae),l(Se),l(Ke),l(S),l(Oe),l(K),l(De),l(O),l(et),l(D),l(tt),l(ee),l(lt),l(nt),l(le),l(st),l(it),l(se),l(at),l(pt),l(ae),l(mt),l(pe),l(ot),l(me),l(rt),l(ft),l(bt),l(fe),l(dt),l(ut),l(de),l(ct),l(Mt),l(ce),l($t),l(yt),l($e),l(Tt),l(ye),l(gt),l(Te),l(Ut),l(ge),l(vt),l(ve)),l(o),c(_,e),c(C,e),c(G,e),c(j,e),c(F,e),c(U,e),c(k,e),c(R,e),c(X,e),c(L,e),c(q,e),c(E,e),c(Q,e),c(A,e),c(v,e),c(te,e),c(ne,e),c(ie,e),c(oe,e),c(re,e),c(be,e),c(ue,e),c(Me,e)}}}const Ml='{"title":"단일 GPU에서 효율적인 추론","local":"efficient-inference-on-a-single-gpu","sections":[{"title":"Better Transformer: PyTorch 네이티브 Transformer 패스트패스","local":"better-transformer-pytorchnative-transformer-fastpath","sections":[],"depth":2},{"title":"FP4 혼합 정밀도 추론을 위한 bitsandbytes 통합","local":"bitsandbytes-integration-for-fp4-mixedprecision-inference","sections":[{"title":"요구 사항","local":"requirements-for-fp4-mixedprecision-inference","sections":[],"depth":3},{"title":"FP4 모델 실행 - 단일 GPU 설정 - 빠른 시작","local":"running-fp4-models-single-gpu-setup-quickstart","sections":[],"depth":3},{"title":"FP4 모델 실행 - 다중 GPU 설정","local":"running-fp4-models-multi-gpu-setup","sections":[],"depth":3},{"title":"고급 사용법","local":"advanced-usage","sections":[],"depth":3}],"depth":2},{"title":"Int8 혼합 정밀도 행렬 분해를 위한 bitsandbytes 통합","local":"bitsandbytes-integration-for-int8-mixedprecision-matrix-decomposition","sections":[{"title":"요구 사항","local":"requirements-for-int8-mixedprecision-matrix-decomposition","sections":[],"depth":3},{"title":"혼합 Int8 모델 실행 - 단일 GPU 설정","local":"running-mixedint8-models-single-gpu-setup","sections":[],"depth":3},{"title":"혼합 Int8 모델 실행 - 다중 GPU 설정","local":"running-mixedint8-models-multi-gpu-setup","sections":[],"depth":3},{"title":"Colab 데모","local":"colab-demos","sections":[],"depth":3}],"depth":2}],"depth":1}';function $l(Ue){return ml(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _l extends ol{constructor(o){super(),rl(this,o,$l,cl,pl,{})}}export{_l as component};
