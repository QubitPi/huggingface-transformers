import{s as Ss,o as Qs,n as me}from"../chunks/scheduler.56730f09.js";import{S as qs,i as As,g,s as o,r as M,A as Ls,h as j,f as l,c as m,j as Ns,u as d,x as T,k as Fs,y as Ps,a,v as y,d as u,t as h,w as $,m as Es,n as Ys}from"../chunks/index.1f144517.js";import{T as gs}from"../chunks/Tip.41e845e5.js";import{Y as zs}from"../chunks/Youtube.62e0f062.js";import{C as R}from"../chunks/CodeBlock.738eeccb.js";import{D as Ds}from"../chunks/DocNotebookDropdown.243c3df7.js";import{F as js,M as ze}from"../chunks/Markdown.c541024b.js";import{H as Fe}from"../chunks/Heading.57d46534.js";function Ks(k){let t,c,s='<a href="../model_doc/bart">BART</a>, <a href="../model_doc/bigbird_pegasus">BigBird-Pegasus</a>, <a href="../model_doc/blenderbot">Blenderbot</a>, <a href="../model_doc/blenderbot-small">BlenderbotSmall</a>, <a href="../model_doc/encoder-decoder">Encoder decoder</a>, <a href="../model_doc/fsmt">FairSeq Machine-Translation</a>, <a href="../model_doc/gptsan-japanese">GPTSAN-japanese</a>, <a href="../model_doc/led">LED</a>, <a href="../model_doc/longt5">LongT5</a>, <a href="../model_doc/m2m_100">M2M100</a>, <a href="../model_doc/marian">Marian</a>, <a href="../model_doc/mbart">mBART</a>, <a href="../model_doc/mt5">MT5</a>, <a href="../model_doc/mvp">MVP</a>, <a href="../model_doc/nllb">NLLB</a>, <a href="../model_doc/nllb-moe">NLLB-MOE</a>, <a href="../model_doc/pegasus">Pegasus</a>, <a href="../model_doc/pegasus_x">PEGASUS-X</a>, <a href="../model_doc/plbart">PLBart</a>, <a href="../model_doc/prophetnet">ProphetNet</a>, <a href="../model_doc/switch_transformers">SwitchTransformers</a>, <a href="../model_doc/t5">T5</a>, <a href="../model_doc/xlm-prophetnet">XLM-ProphetNet</a>';return{c(){t=Es(`이 태스크 가이드는 아래 모델 아키텍처에도 응용할 수 있습니다.

`),c=g("p"),c.innerHTML=s},l(i){t=Ys(i,`이 태스크 가이드는 아래 모델 아키텍처에도 응용할 수 있습니다.

`),c=j(i,"P",{"data-svelte-h":!0}),T(c)!=="svelte-uls67l"&&(c.innerHTML=s)},m(i,b){a(i,t,b),a(i,c,b)},p:me,d(i){i&&(l(t),l(c))}}}function Os(k){let t,c;return t=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvclNlcTJTZXElMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGF0YUNvbGxhdG9yRm9yU2VxMlNlcSh0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMjBtb2RlbCUzRGNoZWNrcG9pbnQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)`,wrap:!1}}),{c(){M(t.$$.fragment)},l(s){d(t.$$.fragment,s)},m(s,i){y(t,s,i),c=!0},p:me,i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function et(k){let t,c;return t=new ze({props:{$$slots:{default:[Os]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){d(t.$$.fragment,s)},m(s,i){y(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function st(k){let t,c;return t=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvclNlcTJTZXElMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGF0YUNvbGxhdG9yRm9yU2VxMlNlcSh0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMjBtb2RlbCUzRGNoZWNrcG9pbnQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),{c(){M(t.$$.fragment)},l(s){d(t.$$.fragment,s)},m(s,i){y(t,s,i),c=!0},p:me,i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function tt(k){let t,c;return t=new ze({props:{$$slots:{default:[st]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){d(t.$$.fragment,s)},m(s,i){y(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function lt(k){let t,c='<code>Trainer</code>로 모델을 파인튜닝하는 방법에 익숙하지 않다면 <a href="../training#train-with-pytorch-trainer">여기</a>에서 기본 튜토리얼을 살펴보시기 바랍니다!';return{c(){t=g("p"),t.innerHTML=c},l(s){t=j(s,"P",{"data-svelte-h":!0}),T(t)!=="svelte-1ba0r5n"&&(t.innerHTML=c)},m(s,i){a(s,t,i)},p:me,d(s){s&&l(t)}}}function at(k){let t,c,s,i="모델을 훈련시킬 준비가 되었군요! <code>AutoModelForSeq2SeqLM</code>으로 T5를 로드하세요:",b,Z,W,X,C="이제 세 단계만 거치면 끝입니다:",B,_,I="<li><code>Seq2SeqTrainingArguments</code>에서 훈련 하이퍼파라미터를 정의하세요. 유일한 필수 매개변수는 모델을 저장할 위치인 <code>output_dir</code>입니다. 모델을 Hub에 푸시하기 위해 <code>push_to_hub=True</code>로 설정하세요. (모델을 업로드하려면 Hugging Face에 로그인해야 합니다.) <code>Trainer</code>는 에폭이 끝날때마다 SacreBLEU 메트릭을 평가하고 훈련 체크포인트를 저장합니다.</li> <li><code>Seq2SeqTrainer</code>에 훈련 인수를 전달하세요. 모델, 데이터 세트, 토크나이저, data collator 및 <code>compute_metrics</code> 함수도 덩달아 전달해야 합니다.</li> <li><code>train()</code>을 호출하여 모델을 파인튜닝하세요.</li>",V,U,G,r,J="학습이 완료되면 <code>push_to_hub()</code> 메서드로 모델을 Hub에 공유하세요. 이러면 누구나 모델을 사용할 수 있게 됩니다:",N,H,x;return t=new gs({props:{$$slots:{default:[lt]},$$scope:{ctx:k}}}),Z=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUyQyUyMFNlcTJTZXFUcmFpbmluZ0FyZ3VtZW50cyUyQyUyMFNlcTJTZXFUcmFpbmVyJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)`,wrap:!1}}),U=new R({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFNlcTJTZXFUcmFpbmluZ0FyZ3VtZW50cyglMEElMjAlMjAlMjAlMjBvdXRwdXRfZGlyJTNEJTIybXlfYXdlc29tZV9vcHVzX2Jvb2tzX21vZGVsJTIyJTJDJTBBJTIwJTIwJTIwJTIwZXZhbHVhdGlvbl9zdHJhdGVneSUzRCUyMmVwb2NoJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjB3ZWlnaHRfZGVjYXklM0QwLjAxJTJDJTBBJTIwJTIwJTIwJTIwc2F2ZV90b3RhbF9saW1pdCUzRDMlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNEMiUyQyUwQSUyMCUyMCUyMCUyMHByZWRpY3Rfd2l0aF9nZW5lcmF0ZSUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBmcDE2JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHB1c2hfdG9faHViJTNEVHJ1ZSUyQyUwQSklMEElMEF0cmFpbmVyJTIwJTNEJTIwU2VxMlNlcVRyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0Rtb2RlbCUyQyUwQSUyMCUyMCUyMCUyMGFyZ3MlM0R0cmFpbmluZ19hcmdzJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRHRva2VuaXplZF9ib29rcyU1QiUyMnRyYWluJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9kYXRhc2V0JTNEdG9rZW5pemVkX2Jvb2tzJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRHRva2VuaXplciUyQyUwQSUyMCUyMCUyMCUyMGRhdGFfY29sbGF0b3IlM0RkYXRhX2NvbGxhdG9yJTJDJTBBJTIwJTIwJTIwJTIwY29tcHV0ZV9tZXRyaWNzJTNEY29tcHV0ZV9tZXRyaWNzJTJDJTBBKSUwQSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = Seq2SeqTrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    predict_with_generate=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Seq2SeqTrainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_books[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_books[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),H=new R({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),{c(){M(t.$$.fragment),c=o(),s=g("p"),s.innerHTML=i,b=o(),M(Z.$$.fragment),W=o(),X=g("p"),X.textContent=C,B=o(),_=g("ol"),_.innerHTML=I,V=o(),M(U.$$.fragment),G=o(),r=g("p"),r.innerHTML=J,N=o(),M(H.$$.fragment)},l(f){d(t.$$.fragment,f),c=m(f),s=j(f,"P",{"data-svelte-h":!0}),T(s)!=="svelte-4h5y1d"&&(s.innerHTML=i),b=m(f),d(Z.$$.fragment,f),W=m(f),X=j(f,"P",{"data-svelte-h":!0}),T(X)!=="svelte-14zzcxs"&&(X.textContent=C),B=m(f),_=j(f,"OL",{"data-svelte-h":!0}),T(_)!=="svelte-df1avs"&&(_.innerHTML=I),V=m(f),d(U.$$.fragment,f),G=m(f),r=j(f,"P",{"data-svelte-h":!0}),T(r)!=="svelte-y88jp6"&&(r.innerHTML=J),N=m(f),d(H.$$.fragment,f)},m(f,v){y(t,f,v),a(f,c,v),a(f,s,v),a(f,b,v),y(Z,f,v),a(f,W,v),a(f,X,v),a(f,B,v),a(f,_,v),a(f,V,v),y(U,f,v),a(f,G,v),a(f,r,v),a(f,N,v),y(H,f,v),x=!0},p(f,v){const F={};v&2&&(F.$$scope={dirty:v,ctx:f}),t.$set(F)},i(f){x||(u(t.$$.fragment,f),u(Z.$$.fragment,f),u(U.$$.fragment,f),u(H.$$.fragment,f),x=!0)},o(f){h(t.$$.fragment,f),h(Z.$$.fragment,f),h(U.$$.fragment,f),h(H.$$.fragment,f),x=!1},d(f){f&&(l(c),l(s),l(b),l(W),l(X),l(B),l(_),l(V),l(G),l(r),l(N)),$(t,f),$(Z,f),$(U,f),$(H,f)}}}function nt(k){let t,c;return t=new ze({props:{$$slots:{default:[at]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){d(t.$$.fragment,s)},m(s,i){y(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function pt(k){let t,c='Keras로 모델을 파인튜닝하는 방법이 익숙하지 않다면, <a href="../training#train-a-tensorflow-model-with-keras">여기</a>에서 기본 튜토리얼을 살펴보시기 바랍니다!';return{c(){t=g("p"),t.innerHTML=c},l(s){t=j(s,"P",{"data-svelte-h":!0}),T(t)!=="svelte-5a866m"&&(t.innerHTML=c)},m(s,i){a(s,t,i)},p:me,d(s){s&&l(t)}}}function rt(k){let t,c,s,i,b,Z="이제 <code>TFAutoModelForSeq2SeqLM</code>로 T5를 가져오세요:",W,X,C,B,_="<code>prepare_tf_dataset()</code>로 데이터 세트를 <code>tf.data.Dataset</code> 형식으로 변환하세요:",I,V,U,G,r='훈련하기 위해 <a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a> 메서드로 모델을 구성하세요:',J,N,H,x,f='훈련을 시작하기 전에 예측값으로부터 SacreBLEU 메트릭을 계산하는 방법과 모델을 Hub에 업로드하는 방법 두 가지를 미리 설정해둬야 합니다. 둘 다 <a href="../main_classes/keras_callbacks">Keras callbacks</a>로 구현하세요.',v,F,ce="<code>KerasMetricCallback</code>에 <code>compute_metrics</code> 함수를 전달하세요.",z,E,Y,se,ie="모델과 토크나이저를 업로드할 위치를 <code>PushToHubCallback</code>에서 지정하세요:",S,Q,q,A,te="이제 콜백들을 한데로 묶어주세요:",fe,L,P,D,le='드디어 모델을 훈련시킬 모든 준비를 마쳤군요! 이제 훈련 및 검증 데이터 세트에 <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a> 메서드를 에폭 수와 만들어둔 콜백과 함께 호출하여 모델을 파인튜닝하세요:',Me,K,O,ee,ae="학습이 완료되면 모델이 자동으로 Hub에 업로드되고, 누구나 사용할 수 있게 됩니다!",de;return t=new gs({props:{$$slots:{default:[pt]},$$scope:{ctx:k}}}),s=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFkYW1XZWlnaHREZWNheSUwQSUwQW9wdGltaXplciUyMCUzRCUyMEFkYW1XZWlnaHREZWNheShsZWFybmluZ19yYXRlJTNEMmUtNSUyQyUyMHdlaWdodF9kZWNheV9yYXRlJTNEMC4wMSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`,wrap:!1}}),X=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxMlNlcUxNJTBBJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)`,wrap:!1}}),V=new R({props:{code:"dGZfdHJhaW5fc2V0JTIwJTNEJTIwbW9kZWwucHJlcGFyZV90Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMHRva2VuaXplZF9ib29rcyU1QiUyMnRyYWluJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwc2h1ZmZsZSUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBiYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBjb2xsYXRlX2ZuJTNEZGF0YV9jb2xsYXRvciUyQyUwQSklMEElMEF0Zl90ZXN0X3NldCUyMCUzRCUyMG1vZGVsLnByZXBhcmVfdGZfZGF0YXNldCglMEElMjAlMjAlMjAlMjB0b2tlbml6ZWRfYm9va3MlNUIlMjJ0ZXN0JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwc2h1ZmZsZSUzREZhbHNlJTJDJTBBJTIwJTIwJTIwJTIwYmF0Y2hfc2l6ZSUzRDE2JTJDJTBBJTIwJTIwJTIwJTIwY29sbGF0ZV9mbiUzRGRhdGFfY29sbGF0b3IlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    tokenized_books[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    tokenized_books[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`,wrap:!1}}),N=new R({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzRG9wdGltaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`,wrap:!1}}),E=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBLZXJhc01ldHJpY0NhbGxiYWNrJTBBJTBBbWV0cmljX2NhbGxiYWNrJTIwJTNEJTIwS2VyYXNNZXRyaWNDYWxsYmFjayhtZXRyaWNfZm4lM0Rjb21wdXRlX21ldHJpY3MlMkMlMjBldmFsX2RhdGFzZXQlM0R0Zl92YWxpZGF0aW9uX3NldCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> KerasMetricCallback

<span class="hljs-meta">&gt;&gt;&gt; </span>metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)`,wrap:!1}}),Q=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBQdXNoVG9IdWJDYWxsYmFjayUwQSUwQXB1c2hfdG9faHViX2NhbGxiYWNrJTIwJTNEJTIwUHVzaFRvSHViQ2FsbGJhY2soJTBBJTIwJTIwJTIwJTIwb3V0cHV0X2RpciUzRCUyMm15X2F3ZXNvbWVfb3B1c19ib29rc19tb2RlbCUyMiUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRHRva2VuaXplciUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

<span class="hljs-meta">&gt;&gt;&gt; </span>push_to_hub_callback = PushToHubCallback(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>,
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>)`,wrap:!1}}),L=new R({props:{code:"Y2FsbGJhY2tzJTIwJTNEJTIwJTVCbWV0cmljX2NhbGxiYWNrJTJDJTIwcHVzaF90b19odWJfY2FsbGJhY2slNUQ=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>callbacks = [metric_callback, push_to_hub_callback]',wrap:!1}}),K=new R({props:{code:"bW9kZWwuZml0KHglM0R0Zl90cmFpbl9zZXQlMkMlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl90ZXN0X3NldCUyQyUyMGVwb2NocyUzRDMlMkMlMjBjYWxsYmFja3MlM0RjYWxsYmFja3Mp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>, callbacks=callbacks)',wrap:!1}}),{c(){M(t.$$.fragment),c=Es(`
TensorFlow에서 모델을 파인튜닝하려면 우선 optimizer 함수, 학습률 스케줄 등의 훈련 하이퍼파라미터를 설정하세요:

	`),M(s.$$.fragment),i=o(),b=g("p"),b.innerHTML=Z,W=o(),M(X.$$.fragment),C=o(),B=g("p"),B.innerHTML=_,I=o(),M(V.$$.fragment),U=o(),G=g("p"),G.innerHTML=r,J=o(),M(N.$$.fragment),H=o(),x=g("p"),x.innerHTML=f,v=o(),F=g("p"),F.innerHTML=ce,z=o(),M(E.$$.fragment),Y=o(),se=g("p"),se.innerHTML=ie,S=o(),M(Q.$$.fragment),q=o(),A=g("p"),A.textContent=te,fe=o(),M(L.$$.fragment),P=o(),D=g("p"),D.innerHTML=le,Me=o(),M(K.$$.fragment),O=o(),ee=g("p"),ee.textContent=ae},l(p){d(t.$$.fragment,p),c=Ys(p,`
TensorFlow에서 모델을 파인튜닝하려면 우선 optimizer 함수, 학습률 스케줄 등의 훈련 하이퍼파라미터를 설정하세요:

	`),d(s.$$.fragment,p),i=m(p),b=j(p,"P",{"data-svelte-h":!0}),T(b)!=="svelte-187gdfo"&&(b.innerHTML=Z),W=m(p),d(X.$$.fragment,p),C=m(p),B=j(p,"P",{"data-svelte-h":!0}),T(B)!=="svelte-bc4nau"&&(B.innerHTML=_),I=m(p),d(V.$$.fragment,p),U=m(p),G=j(p,"P",{"data-svelte-h":!0}),T(G)!=="svelte-lcjrv5"&&(G.innerHTML=r),J=m(p),d(N.$$.fragment,p),H=m(p),x=j(p,"P",{"data-svelte-h":!0}),T(x)!=="svelte-1r18s5x"&&(x.innerHTML=f),v=m(p),F=j(p,"P",{"data-svelte-h":!0}),T(F)!=="svelte-1g3zl6s"&&(F.innerHTML=ce),z=m(p),d(E.$$.fragment,p),Y=m(p),se=j(p,"P",{"data-svelte-h":!0}),T(se)!=="svelte-1b0ip2a"&&(se.innerHTML=ie),S=m(p),d(Q.$$.fragment,p),q=m(p),A=j(p,"P",{"data-svelte-h":!0}),T(A)!=="svelte-1hl7vtj"&&(A.textContent=te),fe=m(p),d(L.$$.fragment,p),P=m(p),D=j(p,"P",{"data-svelte-h":!0}),T(D)!=="svelte-13m2vb0"&&(D.innerHTML=le),Me=m(p),d(K.$$.fragment,p),O=m(p),ee=j(p,"P",{"data-svelte-h":!0}),T(ee)!=="svelte-w4puhc"&&(ee.textContent=ae)},m(p,w){y(t,p,w),a(p,c,w),y(s,p,w),a(p,i,w),a(p,b,w),a(p,W,w),y(X,p,w),a(p,C,w),a(p,B,w),a(p,I,w),y(V,p,w),a(p,U,w),a(p,G,w),a(p,J,w),y(N,p,w),a(p,H,w),a(p,x,w),a(p,v,w),a(p,F,w),a(p,z,w),y(E,p,w),a(p,Y,w),a(p,se,w),a(p,S,w),y(Q,p,w),a(p,q,w),a(p,A,w),a(p,fe,w),y(L,p,w),a(p,P,w),a(p,D,w),a(p,Me,w),y(K,p,w),a(p,O,w),a(p,ee,w),de=!0},p(p,w){const ye={};w&2&&(ye.$$scope={dirty:w,ctx:p}),t.$set(ye)},i(p){de||(u(t.$$.fragment,p),u(s.$$.fragment,p),u(X.$$.fragment,p),u(V.$$.fragment,p),u(N.$$.fragment,p),u(E.$$.fragment,p),u(Q.$$.fragment,p),u(L.$$.fragment,p),u(K.$$.fragment,p),de=!0)},o(p){h(t.$$.fragment,p),h(s.$$.fragment,p),h(X.$$.fragment,p),h(V.$$.fragment,p),h(N.$$.fragment,p),h(E.$$.fragment,p),h(Q.$$.fragment,p),h(L.$$.fragment,p),h(K.$$.fragment,p),de=!1},d(p){p&&(l(c),l(i),l(b),l(W),l(C),l(B),l(I),l(U),l(G),l(J),l(H),l(x),l(v),l(F),l(z),l(Y),l(se),l(S),l(q),l(A),l(fe),l(P),l(D),l(Me),l(O),l(ee)),$(t,p),$(s,p),$(X,p),$(V,p),$(N,p),$(E,p),$(Q,p),$(L,p),$(K,p)}}}function ot(k){let t,c;return t=new ze({props:{$$slots:{default:[rt]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){d(t.$$.fragment,s)},m(s,i){y(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function mt(k){let t,c='번역을 위해 모델을 파인튜닝하는 방법에 대한 보다 자세한 예제는 해당 <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb" rel="nofollow">PyTorch 노트북</a> 또는 <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb" rel="nofollow">TensorFlow 노트북</a>을 참조하세요.';return{c(){t=g("p"),t.innerHTML=c},l(s){t=j(s,"P",{"data-svelte-h":!0}),T(t)!=="svelte-ar4lcc"&&(t.innerHTML=c)},m(s,i){a(s,t,i)},p:me,d(s){s&&l(t)}}}function ct(k){let t,c="텍스트를 토큰화하고 <code>input_ids</code>를 PyTorch 텐서로 반환하세요:",s,i,b,Z,W='<code>generate()</code> 메서드로 번역을 생성하세요. 다양한 텍스트 생성 전략 및 생성을 제어하기 위한 매개변수에 대한 자세한 내용은 <a href="../main_classes/text_generation">Text Generation</a> API를 살펴보시기 바랍니다.',X,C,B,_,I="생성된 토큰 ID들을 다시 텍스트로 디코딩하세요:",V,U,G;return i=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJteV9hd2Vzb21lX29wdXNfYm9va3NfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5pbnB1dF9pZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids`,wrap:!1}}),C=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMjJteV9hd2Vzb21lX29wdXNfYm9va3NfbW9kZWwlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKGlucHV0cyUyQyUyMG1heF9uZXdfdG9rZW5zJTNENDAlMkMlMjBkb19zYW1wbGUlM0RUcnVlJTJDJTIwdG9wX2slM0QzMCUyQyUyMHRvcF9wJTNEMC45NSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(inputs, max_new_tokens=<span class="hljs-number">40</span>, do_sample=<span class="hljs-literal">True</span>, top_k=<span class="hljs-number">30</span>, top_p=<span class="hljs-number">0.95</span>)`,wrap:!1}}),U=new R({props:{code:"dG9rZW5pemVyLmRlY29kZShvdXRwdXRzJTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Les lignées partagent des ressources avec des bactéries enfixant l&#x27;</span>azote.<span class="hljs-string">&#x27;</span>`,wrap:!1}}),{c(){t=g("p"),t.innerHTML=c,s=o(),M(i.$$.fragment),b=o(),Z=g("p"),Z.innerHTML=W,X=o(),M(C.$$.fragment),B=o(),_=g("p"),_.textContent=I,V=o(),M(U.$$.fragment)},l(r){t=j(r,"P",{"data-svelte-h":!0}),T(t)!=="svelte-1b4fw1g"&&(t.innerHTML=c),s=m(r),d(i.$$.fragment,r),b=m(r),Z=j(r,"P",{"data-svelte-h":!0}),T(Z)!=="svelte-14d7ihu"&&(Z.innerHTML=W),X=m(r),d(C.$$.fragment,r),B=m(r),_=j(r,"P",{"data-svelte-h":!0}),T(_)!=="svelte-1u14r4v"&&(_.textContent=I),V=m(r),d(U.$$.fragment,r)},m(r,J){a(r,t,J),a(r,s,J),y(i,r,J),a(r,b,J),a(r,Z,J),a(r,X,J),y(C,r,J),a(r,B,J),a(r,_,J),a(r,V,J),y(U,r,J),G=!0},p:me,i(r){G||(u(i.$$.fragment,r),u(C.$$.fragment,r),u(U.$$.fragment,r),G=!0)},o(r){h(i.$$.fragment,r),h(C.$$.fragment,r),h(U.$$.fragment,r),G=!1},d(r){r&&(l(t),l(s),l(b),l(Z),l(X),l(B),l(_),l(V)),$(i,r),$(C,r),$(U,r)}}}function it(k){let t,c;return t=new ze({props:{$$slots:{default:[ct]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){d(t.$$.fragment,s)},m(s,i){y(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function ft(k){let t,c="텍스트를 토큰화하고 <code>input_ids</code>를 TensorFlow 텐서로 반환하세요:",s,i,b,Z,W='<code>generate()</code> 메서드로 번역을 생성하세요. 다양한 텍스트 생성 전략 및 생성을 제어하기 위한 매개변수에 대한 자세한 내용은 <a href="../main_classes/text_generation">Text Generation</a> API를 살펴보시기 바랍니다.',X,C,B,_,I="생성된 토큰 ID들을 다시 텍스트로 디코딩하세요:",V,U,G;return i=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJteV9hd2Vzb21lX29wdXNfYm9va3NfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKS5pbnB1dF9pZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids`,wrap:!1}}),C=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxMlNlcUxNJTBBJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQoJTIybXlfYXdlc29tZV9vcHVzX2Jvb2tzX21vZGVsJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZShpbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDQwJTJDJTIwZG9fc2FtcGxlJTNEVHJ1ZSUyQyUyMHRvcF9rJTNEMzAlMkMlMjB0b3BfcCUzRDAuOTUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(inputs, max_new_tokens=<span class="hljs-number">40</span>, do_sample=<span class="hljs-literal">True</span>, top_k=<span class="hljs-number">30</span>, top_p=<span class="hljs-number">0.95</span>)`,wrap:!1}}),U=new R({props:{code:"dG9rZW5pemVyLmRlY29kZShvdXRwdXRzJTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Les lugumes partagent les ressources avec des bactéries fixatrices d&#x27;</span>azote.<span class="hljs-string">&#x27;</span>`,wrap:!1}}),{c(){t=g("p"),t.innerHTML=c,s=o(),M(i.$$.fragment),b=o(),Z=g("p"),Z.innerHTML=W,X=o(),M(C.$$.fragment),B=o(),_=g("p"),_.textContent=I,V=o(),M(U.$$.fragment)},l(r){t=j(r,"P",{"data-svelte-h":!0}),T(t)!=="svelte-jhzp8k"&&(t.innerHTML=c),s=m(r),d(i.$$.fragment,r),b=m(r),Z=j(r,"P",{"data-svelte-h":!0}),T(Z)!=="svelte-14d7ihu"&&(Z.innerHTML=W),X=m(r),d(C.$$.fragment,r),B=m(r),_=j(r,"P",{"data-svelte-h":!0}),T(_)!=="svelte-1u14r4v"&&(_.textContent=I),V=m(r),d(U.$$.fragment,r)},m(r,J){a(r,t,J),a(r,s,J),y(i,r,J),a(r,b,J),a(r,Z,J),a(r,X,J),y(C,r,J),a(r,B,J),a(r,_,J),a(r,V,J),y(U,r,J),G=!0},p:me,i(r){G||(u(i.$$.fragment,r),u(C.$$.fragment,r),u(U.$$.fragment,r),G=!0)},o(r){h(i.$$.fragment,r),h(C.$$.fragment,r),h(U.$$.fragment,r),G=!1},d(r){r&&(l(t),l(s),l(b),l(Z),l(X),l(B),l(_),l(V)),$(i,r),$(C,r),$(U,r)}}}function Mt(k){let t,c;return t=new ze({props:{$$slots:{default:[ft]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){d(t.$$.fragment,s)},m(s,i){y(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function dt(k){let t,c,s,i,b,Z,W,X,C,B,_,I="번역은 한 언어로 된 시퀀스를 다른 언어로 변환합니다. 번역이나 요약은 입력을 받아 일련의 출력을 반환하는 강력한 프레임워크인 시퀀스-투-시퀀스 문제로 구성할 수 있는 대표적인 태스크입니다. 번역 시스템은 일반적으로 다른 언어로 된 텍스트 간의 번역에 사용되지만, 음성 간의 통역이나 텍스트-음성 또는 음성-텍스트와 같은 조합에도 사용될 수 있습니다.",V,U,G="이 가이드에서 학습할 내용은:",r,J,N='<li>영어 텍스트를 프랑스어로 번역하기 위해 <a href="https://huggingface.co/google-t5/t5-small" rel="nofollow">T5</a> 모델을 OPUS Books 데이터세트의 영어-프랑스어 하위 집합으로 파인튜닝하는 방법과</li> <li>파인튜닝된 모델을 추론에 사용하는 방법입니다.</li>',H,x,f,v,F="시작하기 전에 필요한 라이브러리가 모두 설치되어 있는지 확인하세요:",ce,z,E,Y,se="모델을 업로드하고 커뮤니티와 공유할 수 있도록 Hugging Face 계정에 로그인하는 것이 좋습니다. 새로운 창이 표시되면 토큰을 입력하여 로그인하세요.",ie,S,Q,q,A,te,fe='먼저 🤗 Datasets 라이브러리에서 <a href="https://huggingface.co/datasets/opus_books" rel="nofollow">OPUS Books</a> 데이터세트의 영어-프랑스어 하위 집합을 가져오세요.',L,P,D,le,Me="데이터세트를 <code>train_test_split</code> 메서드를 사용하여 훈련 및 테스트 데이터로 분할하세요.",K,O,ee,ae,de="훈련 데이터에서 예시를 살펴볼까요?",p,w,ye,ue,ws="반환된 딕셔너리의 <code>translation</code> 키가 텍스트의 영어, 프랑스어 버전을 포함하고 있는 것을 볼 수 있습니다.",Ye,he,Se,$e,Qe,be,Ts="다음 단계로 영어-프랑스어 쌍을 처리하기 위해 T5 토크나이저를 가져오세요.",qe,ge,Ae,je,Js="만들 전처리 함수는 아래 요구사항을 충족해야 합니다:",Le,we,Us="<li>T5가 번역 태스크임을 인지할 수 있도록 입력 앞에 프롬프트를 추가하세요. 여러 NLP 태스크를 할 수 있는 모델 중 일부는 이렇게 태스크 프롬프트를 미리 줘야합니다.</li> <li>원어(영어)과 번역어(프랑스어)를 별도로 토큰화하세요. 영어 어휘로 사전 학습된 토크나이저로 프랑스어 텍스트를 토큰화할 수는 없기 때문입니다.</li> <li><code>max_length</code> 매개변수로 설정한 최대 길이보다 길지 않도록 시퀀스를 truncate하세요.</li>",Pe,Te,De,Je,_s="전체 데이터세트에 전처리 함수를 적용하려면 🤗 Datasets의 <code>map</code> 메서드를 사용하세요. <code>map</code> 함수의 속도를 높이려면 <code>batched=True</code>를 설정하여 데이터세트의 여러 요소를 한 번에 처리하는 방법이 있습니다.",Ke,Ue,Oe,_e,ks="이제 <code>DataCollatorForSeq2Seq</code>를 사용하여 예제 배치를 생성합니다. 데이터세트의 최대 길이로 전부를 padding하는 대신, 데이터 정렬 중 각 배치의 최대 길이로 문장을 <em>동적으로 padding</em>하는 것이 더 효율적입니다.",es,ne,ss,ke,ts,Ze,Zs='훈련 중에 메트릭을 포함하면 모델의 성능을 평가하는 데 도움이 됩니다. 🤗 <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> 라이브러리로 평가 방법(evaluation method)을 빠르게 가져올 수 있습니다. 현재 태스크에 적합한 SacreBLEU 메트릭을 가져오세요. (메트릭을 가져오고 계산하는 방법에 대해 자세히 알아보려면 🤗 Evaluate <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">둘러보기</a>를 참조하세요):',ls,Ce,as,ve,Cs="그런 다음 <code>compute</code>에 예측값과 레이블을 전달하여 SacreBLEU 점수를 계산하는 함수를 생성하세요:",ns,Re,ps,Xe,vs="이제 <code>compute_metrics</code> 함수는 준비되었고, 훈련 과정을 설정할 때 다시 살펴볼 예정입니다.",rs,Be,os,pe,ms,re,cs,Ve,is,Ge,Rs="좋아요, 이제 모델을 파인튜닝했으니 추론에 사용할 수 있습니다!",fs,We,Xs="다른 언어로 번역하고 싶은 텍스트를 써보세요. T5의 경우 원하는 태스크를 입력의 접두사로 추가해야 합니다. 예를 들어 영어에서 프랑스어로 번역하는 경우, 아래와 같은 접두사가 추가됩니다:",Ms,xe,ds,He,Bs="파인튜닝된 모델로 추론하기에 제일 간단한 방법은 <code>pipeline()</code>을 사용하는 것입니다. 해당 모델로 번역 <code>pipeline</code>을 만든 뒤, 텍스트를 전달하세요:",ys,Ie,us,Ne,Vs="원한다면 <code>pipeline</code>의 결과를 직접 복제할 수도 있습니다:",hs,oe,$s,Ee,bs;return b=new Fe({props:{title:"번역",local:"translation",headingTag:"h1"}}),W=new Ds({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/translation.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/translation.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/translation.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/translation.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/translation.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/translation.ipynb"}]}}),C=new zs({props:{id:"1JvfrvZgi6c"}}),x=new gs({props:{$$slots:{default:[Ks]},$$scope:{ctx:k}}}),z=new R({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGUlMjBzYWNyZWJsZXU=",highlighted:"pip install transformers datasets evaluate sacrebleu",wrap:!1}}),S=new R({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),q=new Fe({props:{title:"OPUS Books 데이터세트 가져오기",local:"load-opus-books-dataset",headingTag:"h2"}}),P=new R({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBYm9va3MlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyb3B1c19ib29rcyUyMiUyQyUyMCUyMmVuLWZyJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>books = load_dataset(<span class="hljs-string">&quot;opus_books&quot;</span>, <span class="hljs-string">&quot;en-fr&quot;</span>)`,wrap:!1}}),O=new R({props:{code:"Ym9va3MlMjAlM0QlMjBib29rcyU1QiUyMnRyYWluJTIyJTVELnRyYWluX3Rlc3Rfc3BsaXQodGVzdF9zaXplJTNEMC4yKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>books = books[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(test_size=<span class="hljs-number">0.2</span>)',wrap:!1}}),w=new R({props:{code:"Ym9va3MlNUIlMjJ0cmFpbiUyMiU1RCU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>books[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;90560&#x27;</span>,
 <span class="hljs-string">&#x27;translation&#x27;</span>: {<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-string">&#x27;But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.&#x27;</span>,
  <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-string">&#x27;Mais ce plateau élevé ne mesurait que quelques toises, et bientôt nous fûmes rentrés dans notre élément.&#x27;</span>}}`,wrap:!1}}),he=new Fe({props:{title:"전처리",local:"preprocess",headingTag:"h2"}}),$e=new zs({props:{id:"XAR8jnZZuUs"}}),ge=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEFjaGVja3BvaW50JTIwJTNEJTIwJTIyZ29vZ2xlLXQ1JTJGdDUtc21hbGwlMjIlMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChjaGVja3BvaW50KQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;google-t5/t5-small&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)`,wrap:!1}}),Te=new R({props:{code:"c291cmNlX2xhbmclMjAlM0QlMjAlMjJlbiUyMiUwQXRhcmdldF9sYW5nJTIwJTNEJTIwJTIyZnIlMjIlMEFwcmVmaXglMjAlM0QlMjAlMjJ0cmFuc2xhdGUlMjBFbmdsaXNoJTIwdG8lMjBGcmVuY2glM0ElMjAlMjIlMEElMEElMEFkZWYlMjBwcmVwcm9jZXNzX2Z1bmN0aW9uKGV4YW1wbGVzKSUzQSUwQSUyMCUyMCUyMCUyMGlucHV0cyUyMCUzRCUyMCU1QnByZWZpeCUyMCUyQiUyMGV4YW1wbGUlNUJzb3VyY2VfbGFuZyU1RCUyMGZvciUyMGV4YW1wbGUlMjBpbiUyMGV4YW1wbGVzJTVCJTIydHJhbnNsYXRpb24lMjIlNUQlNUQlMEElMjAlMjAlMjAlMjB0YXJnZXRzJTIwJTNEJTIwJTVCZXhhbXBsZSU1QnRhcmdldF9sYW5nJTVEJTIwZm9yJTIwZXhhbXBsZSUyMGluJTIwZXhhbXBsZXMlNUIlMjJ0cmFuc2xhdGlvbiUyMiU1RCU1RCUwQSUyMCUyMCUyMCUyMG1vZGVsX2lucHV0cyUyMCUzRCUyMHRva2VuaXplcihpbnB1dHMlMkMlMjB0ZXh0X3RhcmdldCUzRHRhcmdldHMlMkMlMjBtYXhfbGVuZ3RoJTNEMTI4JTJDJTIwdHJ1bmNhdGlvbiUzRFRydWUpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwbW9kZWxfaW5wdXRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>source_lang = <span class="hljs-string">&quot;en&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_lang = <span class="hljs-string">&quot;fr&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prefix = <span class="hljs-string">&quot;translate English to French: &quot;</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    inputs = [prefix + example[source_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    targets = [example[target_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    model_inputs = tokenizer(inputs, text_target=targets, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> model_inputs`,wrap:!1}}),Ue=new R({props:{code:"dG9rZW5pemVkX2Jvb2tzJTIwJTNEJTIwYm9va3MubWFwKHByZXByb2Nlc3NfZnVuY3Rpb24lMkMlMjBiYXRjaGVkJTNEVHJ1ZSk=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_books = books.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)',wrap:!1}}),ne=new js({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[tt],pytorch:[et]},$$scope:{ctx:k}}}),ke=new Fe({props:{title:"평가",local:"evalulate",headingTag:"h2"}}),Ce=new R({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFtZXRyaWMlMjAlM0QlMjBldmFsdWF0ZS5sb2FkKCUyMnNhY3JlYmxldSUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;sacrebleu&quot;</span>)`,wrap:!1}}),Re=new R({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwcG9zdHByb2Nlc3NfdGV4dChwcmVkcyUyQyUyMGxhYmVscyklM0ElMEElMjAlMjAlMjAlMjBwcmVkcyUyMCUzRCUyMCU1QnByZWQuc3RyaXAoKSUyMGZvciUyMHByZWQlMjBpbiUyMHByZWRzJTVEJTBBJTIwJTIwJTIwJTIwbGFiZWxzJTIwJTNEJTIwJTVCJTVCbGFiZWwuc3RyaXAoKSU1RCUyMGZvciUyMGxhYmVsJTIwaW4lMjBsYWJlbHMlNUQlMEElMEElMjAlMjAlMjAlMjByZXR1cm4lMjBwcmVkcyUyQyUyMGxhYmVscyUwQSUwQSUwQWRlZiUyMGNvbXB1dGVfbWV0cmljcyhldmFsX3ByZWRzKSUzQSUwQSUyMCUyMCUyMCUyMHByZWRzJTJDJTIwbGFiZWxzJTIwJTNEJTIwZXZhbF9wcmVkcyUwQSUyMCUyMCUyMCUyMGlmJTIwaXNpbnN0YW5jZShwcmVkcyUyQyUyMHR1cGxlKSUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHByZWRzJTIwJTNEJTIwcHJlZHMlNUIwJTVEJTBBJTIwJTIwJTIwJTIwZGVjb2RlZF9wcmVkcyUyMCUzRCUyMHRva2VuaXplci5iYXRjaF9kZWNvZGUocHJlZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEElMEElMjAlMjAlMjAlMjBsYWJlbHMlMjAlM0QlMjBucC53aGVyZShsYWJlbHMlMjAhJTNEJTIwLTEwMCUyQyUyMGxhYmVscyUyQyUyMHRva2VuaXplci5wYWRfdG9rZW5faWQpJTBBJTIwJTIwJTIwJTIwZGVjb2RlZF9sYWJlbHMlMjAlM0QlMjB0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGxhYmVscyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSUwQSUwQSUyMCUyMCUyMCUyMGRlY29kZWRfcHJlZHMlMkMlMjBkZWNvZGVkX2xhYmVscyUyMCUzRCUyMHBvc3Rwcm9jZXNzX3RleHQoZGVjb2RlZF9wcmVkcyUyQyUyMGRlY29kZWRfbGFiZWxzKSUwQSUwQSUyMCUyMCUyMCUyMHJlc3VsdCUyMCUzRCUyMG1ldHJpYy5jb21wdXRlKHByZWRpY3Rpb25zJTNEZGVjb2RlZF9wcmVkcyUyQyUyMHJlZmVyZW5jZXMlM0RkZWNvZGVkX2xhYmVscyklMEElMjAlMjAlMjAlMjByZXN1bHQlMjAlM0QlMjAlN0IlMjJibGV1JTIyJTNBJTIwcmVzdWx0JTVCJTIyc2NvcmUlMjIlNUQlN0QlMEElMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9uX2xlbnMlMjAlM0QlMjAlNUJucC5jb3VudF9ub256ZXJvKHByZWQlMjAhJTNEJTIwdG9rZW5pemVyLnBhZF90b2tlbl9pZCklMjBmb3IlMjBwcmVkJTIwaW4lMjBwcmVkcyU1RCUwQSUyMCUyMCUyMCUyMHJlc3VsdCU1QiUyMmdlbl9sZW4lMjIlNUQlMjAlM0QlMjBucC5tZWFuKHByZWRpY3Rpb25fbGVucyklMEElMjAlMjAlMjAlMjByZXN1bHQlMjAlM0QlMjAlN0JrJTNBJTIwcm91bmQodiUyQyUyMDQpJTIwZm9yJTIwayUyQyUyMHYlMjBpbiUyMHJlc3VsdC5pdGVtcygpJTdEJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwcmVzdWx0",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">postprocess_text</span>(<span class="hljs-params">preds, labels</span>):
<span class="hljs-meta">... </span>    preds = [pred.strip() <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
<span class="hljs-meta">... </span>    labels = [[label.strip()] <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]

<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> preds, labels


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_preds</span>):
<span class="hljs-meta">... </span>    preds, labels = eval_preds
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(preds, <span class="hljs-built_in">tuple</span>):
<span class="hljs-meta">... </span>        preds = preds[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
<span class="hljs-meta">... </span>    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

<span class="hljs-meta">... </span>    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
<span class="hljs-meta">... </span>    result = {<span class="hljs-string">&quot;bleu&quot;</span>: result[<span class="hljs-string">&quot;score&quot;</span>]}

<span class="hljs-meta">... </span>    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
<span class="hljs-meta">... </span>    result[<span class="hljs-string">&quot;gen_len&quot;</span>] = np.mean(prediction_lens)
<span class="hljs-meta">... </span>    result = {k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> result`,wrap:!1}}),Be=new Fe({props:{title:"훈련",local:"train",headingTag:"h2"}}),pe=new js({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ot],pytorch:[nt]},$$scope:{ctx:k}}}),re=new gs({props:{$$slots:{default:[mt]},$$scope:{ctx:k}}}),Ve=new Fe({props:{title:"추론",local:"inference",headingTag:"h2"}}),xe=new R({props:{code:"dGV4dCUyMCUzRCUyMCUyMnRyYW5zbGF0ZSUyMEVuZ2xpc2glMjB0byUyMEZyZW5jaCUzQSUyMExlZ3VtZXMlMjBzaGFyZSUyMHJlc291cmNlcyUyMHdpdGglMjBuaXRyb2dlbi1maXhpbmclMjBiYWN0ZXJpYS4lMjI=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;translate English to French: Legumes share resources with nitrogen-fixing bacteria.&quot;</span>',wrap:!1}}),Ie=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBdHJhbnNsYXRvciUyMCUzRCUyMHBpcGVsaW5lKCUyMnRyYW5zbGF0aW9uJTIyJTJDJTIwbW9kZWwlM0QlMjJteV9hd2Vzb21lX29wdXNfYm9va3NfbW9kZWwlMjIpJTBBdHJhbnNsYXRvcih0ZXh0KQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>translator = pipeline(<span class="hljs-string">&quot;translation&quot;</span>, model=<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>translator(text)
[{<span class="hljs-string">&#x27;translation_text&#x27;</span>: <span class="hljs-string">&#x27;Legumes partagent des ressources avec des bactéries azotantes.&#x27;</span>}]`,wrap:!1}}),oe=new js({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Mt],pytorch:[it]},$$scope:{ctx:k}}}),{c(){t=g("meta"),c=o(),s=g("p"),i=o(),M(b.$$.fragment),Z=o(),M(W.$$.fragment),X=o(),M(C.$$.fragment),B=o(),_=g("p"),_.textContent=I,V=o(),U=g("p"),U.textContent=G,r=o(),J=g("ol"),J.innerHTML=N,H=o(),M(x.$$.fragment),f=o(),v=g("p"),v.textContent=F,ce=o(),M(z.$$.fragment),E=o(),Y=g("p"),Y.textContent=se,ie=o(),M(S.$$.fragment),Q=o(),M(q.$$.fragment),A=o(),te=g("p"),te.innerHTML=fe,L=o(),M(P.$$.fragment),D=o(),le=g("p"),le.innerHTML=Me,K=o(),M(O.$$.fragment),ee=o(),ae=g("p"),ae.textContent=de,p=o(),M(w.$$.fragment),ye=o(),ue=g("p"),ue.innerHTML=ws,Ye=o(),M(he.$$.fragment),Se=o(),M($e.$$.fragment),Qe=o(),be=g("p"),be.textContent=Ts,qe=o(),M(ge.$$.fragment),Ae=o(),je=g("p"),je.textContent=Js,Le=o(),we=g("ol"),we.innerHTML=Us,Pe=o(),M(Te.$$.fragment),De=o(),Je=g("p"),Je.innerHTML=_s,Ke=o(),M(Ue.$$.fragment),Oe=o(),_e=g("p"),_e.innerHTML=ks,es=o(),M(ne.$$.fragment),ss=o(),M(ke.$$.fragment),ts=o(),Ze=g("p"),Ze.innerHTML=Zs,ls=o(),M(Ce.$$.fragment),as=o(),ve=g("p"),ve.innerHTML=Cs,ns=o(),M(Re.$$.fragment),ps=o(),Xe=g("p"),Xe.innerHTML=vs,rs=o(),M(Be.$$.fragment),os=o(),M(pe.$$.fragment),ms=o(),M(re.$$.fragment),cs=o(),M(Ve.$$.fragment),is=o(),Ge=g("p"),Ge.textContent=Rs,fs=o(),We=g("p"),We.textContent=Xs,Ms=o(),M(xe.$$.fragment),ds=o(),He=g("p"),He.innerHTML=Bs,ys=o(),M(Ie.$$.fragment),us=o(),Ne=g("p"),Ne.innerHTML=Vs,hs=o(),M(oe.$$.fragment),$s=o(),Ee=g("p"),this.h()},l(e){const n=Ls("svelte-u9bgzb",document.head);t=j(n,"META",{name:!0,content:!0}),n.forEach(l),c=m(e),s=j(e,"P",{}),Ns(s).forEach(l),i=m(e),d(b.$$.fragment,e),Z=m(e),d(W.$$.fragment,e),X=m(e),d(C.$$.fragment,e),B=m(e),_=j(e,"P",{"data-svelte-h":!0}),T(_)!=="svelte-1k3fp3p"&&(_.textContent=I),V=m(e),U=j(e,"P",{"data-svelte-h":!0}),T(U)!=="svelte-14hiaa1"&&(U.textContent=G),r=m(e),J=j(e,"OL",{"data-svelte-h":!0}),T(J)!=="svelte-4n42lt"&&(J.innerHTML=N),H=m(e),d(x.$$.fragment,e),f=m(e),v=j(e,"P",{"data-svelte-h":!0}),T(v)!=="svelte-1k0z9pm"&&(v.textContent=F),ce=m(e),d(z.$$.fragment,e),E=m(e),Y=j(e,"P",{"data-svelte-h":!0}),T(Y)!=="svelte-1jstx1u"&&(Y.textContent=se),ie=m(e),d(S.$$.fragment,e),Q=m(e),d(q.$$.fragment,e),A=m(e),te=j(e,"P",{"data-svelte-h":!0}),T(te)!=="svelte-adgncb"&&(te.innerHTML=fe),L=m(e),d(P.$$.fragment,e),D=m(e),le=j(e,"P",{"data-svelte-h":!0}),T(le)!=="svelte-19d5a0p"&&(le.innerHTML=Me),K=m(e),d(O.$$.fragment,e),ee=m(e),ae=j(e,"P",{"data-svelte-h":!0}),T(ae)!=="svelte-o7e31d"&&(ae.textContent=de),p=m(e),d(w.$$.fragment,e),ye=m(e),ue=j(e,"P",{"data-svelte-h":!0}),T(ue)!=="svelte-18ww8rs"&&(ue.innerHTML=ws),Ye=m(e),d(he.$$.fragment,e),Se=m(e),d($e.$$.fragment,e),Qe=m(e),be=j(e,"P",{"data-svelte-h":!0}),T(be)!=="svelte-1mrpk7j"&&(be.textContent=Ts),qe=m(e),d(ge.$$.fragment,e),Ae=m(e),je=j(e,"P",{"data-svelte-h":!0}),T(je)!=="svelte-cw39ls"&&(je.textContent=Js),Le=m(e),we=j(e,"OL",{"data-svelte-h":!0}),T(we)!=="svelte-1a06q10"&&(we.innerHTML=Us),Pe=m(e),d(Te.$$.fragment,e),De=m(e),Je=j(e,"P",{"data-svelte-h":!0}),T(Je)!=="svelte-1lmdyeh"&&(Je.innerHTML=_s),Ke=m(e),d(Ue.$$.fragment,e),Oe=m(e),_e=j(e,"P",{"data-svelte-h":!0}),T(_e)!=="svelte-1lgij5g"&&(_e.innerHTML=ks),es=m(e),d(ne.$$.fragment,e),ss=m(e),d(ke.$$.fragment,e),ts=m(e),Ze=j(e,"P",{"data-svelte-h":!0}),T(Ze)!=="svelte-1fiqbd1"&&(Ze.innerHTML=Zs),ls=m(e),d(Ce.$$.fragment,e),as=m(e),ve=j(e,"P",{"data-svelte-h":!0}),T(ve)!=="svelte-1925cbb"&&(ve.innerHTML=Cs),ns=m(e),d(Re.$$.fragment,e),ps=m(e),Xe=j(e,"P",{"data-svelte-h":!0}),T(Xe)!=="svelte-11m7xom"&&(Xe.innerHTML=vs),rs=m(e),d(Be.$$.fragment,e),os=m(e),d(pe.$$.fragment,e),ms=m(e),d(re.$$.fragment,e),cs=m(e),d(Ve.$$.fragment,e),is=m(e),Ge=j(e,"P",{"data-svelte-h":!0}),T(Ge)!=="svelte-1nv1rsn"&&(Ge.textContent=Rs),fs=m(e),We=j(e,"P",{"data-svelte-h":!0}),T(We)!=="svelte-1kbteb5"&&(We.textContent=Xs),Ms=m(e),d(xe.$$.fragment,e),ds=m(e),He=j(e,"P",{"data-svelte-h":!0}),T(He)!=="svelte-19gmubo"&&(He.innerHTML=Bs),ys=m(e),d(Ie.$$.fragment,e),us=m(e),Ne=j(e,"P",{"data-svelte-h":!0}),T(Ne)!=="svelte-ckakkh"&&(Ne.innerHTML=Vs),hs=m(e),d(oe.$$.fragment,e),$s=m(e),Ee=j(e,"P",{}),Ns(Ee).forEach(l),this.h()},h(){Fs(t,"name","hf:doc:metadata"),Fs(t,"content",yt)},m(e,n){Ps(document.head,t),a(e,c,n),a(e,s,n),a(e,i,n),y(b,e,n),a(e,Z,n),y(W,e,n),a(e,X,n),y(C,e,n),a(e,B,n),a(e,_,n),a(e,V,n),a(e,U,n),a(e,r,n),a(e,J,n),a(e,H,n),y(x,e,n),a(e,f,n),a(e,v,n),a(e,ce,n),y(z,e,n),a(e,E,n),a(e,Y,n),a(e,ie,n),y(S,e,n),a(e,Q,n),y(q,e,n),a(e,A,n),a(e,te,n),a(e,L,n),y(P,e,n),a(e,D,n),a(e,le,n),a(e,K,n),y(O,e,n),a(e,ee,n),a(e,ae,n),a(e,p,n),y(w,e,n),a(e,ye,n),a(e,ue,n),a(e,Ye,n),y(he,e,n),a(e,Se,n),y($e,e,n),a(e,Qe,n),a(e,be,n),a(e,qe,n),y(ge,e,n),a(e,Ae,n),a(e,je,n),a(e,Le,n),a(e,we,n),a(e,Pe,n),y(Te,e,n),a(e,De,n),a(e,Je,n),a(e,Ke,n),y(Ue,e,n),a(e,Oe,n),a(e,_e,n),a(e,es,n),y(ne,e,n),a(e,ss,n),y(ke,e,n),a(e,ts,n),a(e,Ze,n),a(e,ls,n),y(Ce,e,n),a(e,as,n),a(e,ve,n),a(e,ns,n),y(Re,e,n),a(e,ps,n),a(e,Xe,n),a(e,rs,n),y(Be,e,n),a(e,os,n),y(pe,e,n),a(e,ms,n),y(re,e,n),a(e,cs,n),y(Ve,e,n),a(e,is,n),a(e,Ge,n),a(e,fs,n),a(e,We,n),a(e,Ms,n),y(xe,e,n),a(e,ds,n),a(e,He,n),a(e,ys,n),y(Ie,e,n),a(e,us,n),a(e,Ne,n),a(e,hs,n),y(oe,e,n),a(e,$s,n),a(e,Ee,n),bs=!0},p(e,[n]){const Gs={};n&2&&(Gs.$$scope={dirty:n,ctx:e}),x.$set(Gs);const Ws={};n&2&&(Ws.$$scope={dirty:n,ctx:e}),ne.$set(Ws);const xs={};n&2&&(xs.$$scope={dirty:n,ctx:e}),pe.$set(xs);const Hs={};n&2&&(Hs.$$scope={dirty:n,ctx:e}),re.$set(Hs);const Is={};n&2&&(Is.$$scope={dirty:n,ctx:e}),oe.$set(Is)},i(e){bs||(u(b.$$.fragment,e),u(W.$$.fragment,e),u(C.$$.fragment,e),u(x.$$.fragment,e),u(z.$$.fragment,e),u(S.$$.fragment,e),u(q.$$.fragment,e),u(P.$$.fragment,e),u(O.$$.fragment,e),u(w.$$.fragment,e),u(he.$$.fragment,e),u($e.$$.fragment,e),u(ge.$$.fragment,e),u(Te.$$.fragment,e),u(Ue.$$.fragment,e),u(ne.$$.fragment,e),u(ke.$$.fragment,e),u(Ce.$$.fragment,e),u(Re.$$.fragment,e),u(Be.$$.fragment,e),u(pe.$$.fragment,e),u(re.$$.fragment,e),u(Ve.$$.fragment,e),u(xe.$$.fragment,e),u(Ie.$$.fragment,e),u(oe.$$.fragment,e),bs=!0)},o(e){h(b.$$.fragment,e),h(W.$$.fragment,e),h(C.$$.fragment,e),h(x.$$.fragment,e),h(z.$$.fragment,e),h(S.$$.fragment,e),h(q.$$.fragment,e),h(P.$$.fragment,e),h(O.$$.fragment,e),h(w.$$.fragment,e),h(he.$$.fragment,e),h($e.$$.fragment,e),h(ge.$$.fragment,e),h(Te.$$.fragment,e),h(Ue.$$.fragment,e),h(ne.$$.fragment,e),h(ke.$$.fragment,e),h(Ce.$$.fragment,e),h(Re.$$.fragment,e),h(Be.$$.fragment,e),h(pe.$$.fragment,e),h(re.$$.fragment,e),h(Ve.$$.fragment,e),h(xe.$$.fragment,e),h(Ie.$$.fragment,e),h(oe.$$.fragment,e),bs=!1},d(e){e&&(l(c),l(s),l(i),l(Z),l(X),l(B),l(_),l(V),l(U),l(r),l(J),l(H),l(f),l(v),l(ce),l(E),l(Y),l(ie),l(Q),l(A),l(te),l(L),l(D),l(le),l(K),l(ee),l(ae),l(p),l(ye),l(ue),l(Ye),l(Se),l(Qe),l(be),l(qe),l(Ae),l(je),l(Le),l(we),l(Pe),l(De),l(Je),l(Ke),l(Oe),l(_e),l(es),l(ss),l(ts),l(Ze),l(ls),l(as),l(ve),l(ns),l(ps),l(Xe),l(rs),l(os),l(ms),l(cs),l(is),l(Ge),l(fs),l(We),l(Ms),l(ds),l(He),l(ys),l(us),l(Ne),l(hs),l($s),l(Ee)),l(t),$(b,e),$(W,e),$(C,e),$(x,e),$(z,e),$(S,e),$(q,e),$(P,e),$(O,e),$(w,e),$(he,e),$($e,e),$(ge,e),$(Te,e),$(Ue,e),$(ne,e),$(ke,e),$(Ce,e),$(Re,e),$(Be,e),$(pe,e),$(re,e),$(Ve,e),$(xe,e),$(Ie,e),$(oe,e)}}}const yt='{"title":"번역","local":"translation","sections":[{"title":"OPUS Books 데이터세트 가져오기","local":"load-opus-books-dataset","sections":[],"depth":2},{"title":"전처리","local":"preprocess","sections":[],"depth":2},{"title":"평가","local":"evalulate","sections":[],"depth":2},{"title":"훈련","local":"train","sections":[],"depth":2},{"title":"추론","local":"inference","sections":[],"depth":2}],"depth":1}';function ut(k){return Qs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ut extends qs{constructor(t){super(),As(this,t,ut,dt,Ss,{})}}export{Ut as component};
