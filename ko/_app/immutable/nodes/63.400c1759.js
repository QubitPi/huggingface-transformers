import{s as Et,o as zt,n as pe}from"../chunks/scheduler.56730f09.js";import{S as Ft,i as Qt,g,s as o,r as d,A as Lt,h as j,f as l,c as i,j as Bt,u as h,x as T,k as Ht,y as qt,a,v as u,d as M,t as $,w as b,m as Yt,n as Nt}from"../chunks/index.1f144517.js";import{T as ze}from"../chunks/Tip.41e845e5.js";import{Y as St}from"../chunks/Youtube.62e0f062.js";import{C as v}from"../chunks/CodeBlock.738eeccb.js";import{D as At}from"../chunks/DocNotebookDropdown.243c3df7.js";import{F as gt,M as Ye}from"../chunks/Markdown.c541024b.js";import{H as He}from"../chunks/Heading.57d46534.js";function Pt(k){let t,m,s='<a href="../model_doc/albert">ALBERT</a>, <a href="../model_doc/bart">BART</a>, <a href="../model_doc/bert">BERT</a>, <a href="../model_doc/big_bird">BigBird</a>, <a href="../model_doc/bigbird_pegasus">BigBird-Pegasus</a>, <a href="../model_doc/bloom">BLOOM</a>, <a href="../model_doc/camembert">CamemBERT</a>, <a href="../model_doc/canine">CANINE</a>, <a href="../model_doc/convbert">ConvBERT</a>, <a href="../model_doc/ctrl">CTRL</a>, <a href="../model_doc/data2vec-text">Data2VecText</a>, <a href="../model_doc/deberta">DeBERTa</a>, <a href="../model_doc/deberta-v2">DeBERTa-v2</a>, <a href="../model_doc/distilbert">DistilBERT</a>, <a href="../model_doc/electra">ELECTRA</a>, <a href="../model_doc/ernie">ERNIE</a>, <a href="../model_doc/ernie_m">ErnieM</a>, <a href="../model_doc/esm">ESM</a>, <a href="../model_doc/flaubert">FlauBERT</a>, <a href="../model_doc/fnet">FNet</a>, <a href="../model_doc/funnel">Funnel Transformer</a>, <a href="../model_doc/gpt-sw3">GPT-Sw3</a>, <a href="../model_doc/gpt2">OpenAI GPT-2</a>, <a href="../model_doc/gpt_neo">GPT Neo</a>, <a href="../model_doc/gptj">GPT-J</a>, <a href="../model_doc/ibert">I-BERT</a>, <a href="../model_doc/layoutlm">LayoutLM</a>, <a href="../model_doc/layoutlmv2">LayoutLMv2</a>, <a href="../model_doc/layoutlmv3">LayoutLMv3</a>, <a href="../model_doc/led">LED</a>, <a href="../model_doc/lilt">LiLT</a>, <a href="../model_doc/llama">LLaMA</a>, <a href="../model_doc/longformer">Longformer</a>, <a href="../model_doc/luke">LUKE</a>, <a href="../model_doc/markuplm">MarkupLM</a>, <a href="../model_doc/mbart">mBART</a>, <a href="../model_doc/mega">MEGA</a>, <a href="../model_doc/megatron-bert">Megatron-BERT</a>, <a href="../model_doc/mobilebert">MobileBERT</a>, <a href="../model_doc/mpnet">MPNet</a>, <a href="../model_doc/mvp">MVP</a>, <a href="../model_doc/nezha">Nezha</a>, <a href="../model_doc/nystromformer">Nyströmformer</a>, <a href="../model_doc/openai-gpt">OpenAI GPT</a>, <a href="../model_doc/opt">OPT</a>, <a href="../model_doc/perceiver">Perceiver</a>, <a href="../model_doc/plbart">PLBart</a>, <a href="../model_doc/qdqbert">QDQBert</a>, <a href="../model_doc/reformer">Reformer</a>, <a href="../model_doc/rembert">RemBERT</a>, <a href="../model_doc/roberta">RoBERTa</a>, <a href="../model_doc/roberta-prelayernorm">RoBERTa-PreLayerNorm</a>, <a href="../model_doc/roc_bert">RoCBert</a>, <a href="../model_doc/roformer">RoFormer</a>, <a href="../model_doc/squeezebert">SqueezeBERT</a>, <a href="../model_doc/tapas">TAPAS</a>, <a href="../model_doc/transfo-xl">Transformer-XL</a>, <a href="../model_doc/xlm">XLM</a>, <a href="../model_doc/xlm-roberta">XLM-RoBERTa</a>, <a href="../model_doc/xlm-roberta-xl">XLM-RoBERTa-XL</a>, <a href="../model_doc/xlnet">XLNet</a>, <a href="../model_doc/xmod">X-MOD</a>, <a href="../model_doc/yoso">YOSO</a>';return{c(){t=Yt(`이 튜토리얼에서 설명하는 작업은 다음 모델 아키텍처에 의해 지원됩니다:

`),m=g("p"),m.innerHTML=s},l(c){t=Nt(c,`이 튜토리얼에서 설명하는 작업은 다음 모델 아키텍처에 의해 지원됩니다:

`),m=j(c,"P",{"data-svelte-h":!0}),T(m)!=="svelte-ueqi4z"&&(m.innerHTML=s)},m(c,y){a(c,t,y),a(c,m,y)},p:pe,d(c){c&&(l(t),l(m))}}}function Dt(k){let t,m;return t=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nJTBBJTBBZGF0YV9jb2xsYXRvciUyMCUzRCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nKHRva2VuaXplciUzRHRva2VuaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,wrap:!1}}),{c(){d(t.$$.fragment)},l(s){h(t.$$.fragment,s)},m(s,c){u(t,s,c),m=!0},p:pe,i(s){m||(M(t.$$.fragment,s),m=!0)},o(s){$(t.$$.fragment,s),m=!1},d(s){b(t,s)}}}function Kt(k){let t,m;return t=new Ye({props:{$$slots:{default:[Dt]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(s){h(t.$$.fragment,s)},m(s,c){u(t,s,c),m=!0},p(s,c){const y={};c&2&&(y.$$scope={dirty:c,ctx:s}),t.$set(y)},i(s){m||(M(t.$$.fragment,s),m=!0)},o(s){$(t.$$.fragment,s),m=!1},d(s){b(t,s)}}}function Ot(k){let t,m;return t=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nJTBBJTBBZGF0YV9jb2xsYXRvciUyMCUzRCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nKHRva2VuaXplciUzRHRva2VuaXplciUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIydGYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),{c(){d(t.$$.fragment)},l(s){h(t.$$.fragment,s)},m(s,c){u(t,s,c),m=!0},p:pe,i(s){m||(M(t.$$.fragment,s),m=!0)},o(s){$(t.$$.fragment,s),m=!1},d(s){b(t,s)}}}function es(k){let t,m;return t=new Ye({props:{$$slots:{default:[Ot]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(s){h(t.$$.fragment,s)},m(s,c){u(t,s,c),m=!0},p(s,c){const y={};c&2&&(y.$$scope={dirty:c,ctx:s}),t.$set(y)},i(s){m||(M(t.$$.fragment,s),m=!0)},o(s){$(t.$$.fragment,s),m=!1},d(s){b(t,s)}}}function ts(k){let t,m='<code>Trainer</code>를 사용하여 모델을 파인 튜닝하는 방법에 익숙하지 않은 경우, <a href="../training#train-with-pytorch-trainer">여기</a>의 기본 튜토리얼을 확인하세요!';return{c(){t=g("p"),t.innerHTML=m},l(s){t=j(s,"P",{"data-svelte-h":!0}),T(t)!=="svelte-1wclifr"&&(t.innerHTML=m)},m(s,c){a(s,t,c)},p:pe,d(s){s&&l(t)}}}function ss(k){let t,m="<code>Trainer</code>는 <code>tokenizer</code>를 전달하면 기본적으로 동적 매핑을 적용합니다. 이 경우, 명시적으로 데이터 수집기를 지정할 필요가 없습니다.";return{c(){t=g("p"),t.innerHTML=m},l(s){t=j(s,"P",{"data-svelte-h":!0}),T(t)!=="svelte-1wp2ucv"&&(t.innerHTML=m)},m(s,c){a(s,t,c)},p:pe,d(s){s&&l(t)}}}function ls(k){let t,m,s,c="이제 모델을 훈련시킬 준비가 되었습니다! <code>AutoModelForSequenceClassification</code>로 DistilBERT를 가쳐오고 예상되는 레이블 수와 레이블 매핑을 지정하세요:",y,Z,V,W,C="이제 세 단계만 거치면 끝입니다:",G,U,Y="<li><code>TrainingArguments</code>에서 하이퍼파라미터를 정의하세요. <code>output_dir</code>는 모델을 저장할 위치를 지정하는 유일한 파라미터입니다. 이 모델을 Hub에 업로드하기 위해 <code>push_to_hub=True</code>를 설정합니다. (모델을 업로드하기 위해 Hugging Face에 로그인해야합니다.) 각 에폭이 끝날 때마다, <code>Trainer</code>는 정확도를 평가하고 훈련 체크포인트를 저장합니다.</li> <li><code>Trainer</code>에 훈련 인수와 모델, 데이터셋, 토크나이저, 데이터 수집기 및 <code>compute_metrics</code> 함수를 전달하세요.</li> <li><code>train()</code>를 호출하여 모델은 파인 튜닝하세요.</li>",X,J,x,r,_,B,F="훈련이 완료되면, <code>push_to_hub()</code> 메소드를 사용하여 모델을 Hub에 공유할 수 있습니다.",I,N,H;return t=new ze({props:{$$slots:{default:[ts]},$$scope:{ctx:k}}}),Z=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMkMlMjBUcmFpbmluZ0FyZ3VtZW50cyUyQyUyMFRyYWluZXIlMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZCUyMiUyQyUyMG51bV9sYWJlbHMlM0QyJTJDJTIwaWQybGFiZWwlM0RpZDJsYWJlbCUyQyUyMGxhYmVsMmlkJTNEbGFiZWwyaWQlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>, id2label=id2label, label2id=label2id
<span class="hljs-meta">... </span>)`,wrap:!1}}),J=new v({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJteV9hd2Vzb21lX21vZGVsJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNEMiUyQyUwQSUyMCUyMCUyMCUyMHdlaWdodF9kZWNheSUzRDAuMDElMkMlMEElMjAlMjAlMjAlMjBldmFsdWF0aW9uX3N0cmF0ZWd5JTNEJTIyZXBvY2glMjIlMkMlMEElMjAlMjAlMjAlMjBzYXZlX3N0cmF0ZWd5JTNEJTIyZXBvY2glMjIlMkMlMEElMjAlMjAlMjAlMjBsb2FkX2Jlc3RfbW9kZWxfYXRfZW5kJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHB1c2hfdG9faHViJTNEVHJ1ZSUyQyUwQSklMEElMEF0cmFpbmVyJTIwJTNEJTIwVHJhaW5lciglMEElMjAlMjAlMjAlMjBtb2RlbCUzRG1vZGVsJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEdG9rZW5pemVkX2ltZGIlNUIlMjJ0cmFpbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfZGF0YXNldCUzRHRva2VuaXplZF9pbWRiJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRHRva2VuaXplciUyQyUwQSUyMCUyMCUyMCUyMGRhdGFfY29sbGF0b3IlM0RkYXRhX2NvbGxhdG9yJTJDJTBBJTIwJTIwJTIwJTIwY29tcHV0ZV9tZXRyaWNzJTNEY29tcHV0ZV9tZXRyaWNzJTJDJTBBKSUwQSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_model&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    load_best_model_at_end=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),r=new ze({props:{$$slots:{default:[ss]},$$scope:{ctx:k}}}),N=new v({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),{c(){d(t.$$.fragment),m=o(),s=g("p"),s.innerHTML=c,y=o(),d(Z.$$.fragment),V=o(),W=g("p"),W.textContent=C,G=o(),U=g("ol"),U.innerHTML=Y,X=o(),d(J.$$.fragment),x=o(),d(r.$$.fragment),_=o(),B=g("p"),B.innerHTML=F,I=o(),d(N.$$.fragment)},l(f){h(t.$$.fragment,f),m=i(f),s=j(f,"P",{"data-svelte-h":!0}),T(s)!=="svelte-1dpm8jo"&&(s.innerHTML=c),y=i(f),h(Z.$$.fragment,f),V=i(f),W=j(f,"P",{"data-svelte-h":!0}),T(W)!=="svelte-14zzcxs"&&(W.textContent=C),G=i(f),U=j(f,"OL",{"data-svelte-h":!0}),T(U)!=="svelte-637r6p"&&(U.innerHTML=Y),X=i(f),h(J.$$.fragment,f),x=i(f),h(r.$$.fragment,f),_=i(f),B=j(f,"P",{"data-svelte-h":!0}),T(B)!=="svelte-1aylhkm"&&(B.innerHTML=F),I=i(f),h(N.$$.fragment,f)},m(f,R){u(t,f,R),a(f,m,R),a(f,s,R),a(f,y,R),u(Z,f,R),a(f,V,R),a(f,W,R),a(f,G,R),a(f,U,R),a(f,X,R),u(J,f,R),a(f,x,R),u(r,f,R),a(f,_,R),a(f,B,R),a(f,I,R),u(N,f,R),H=!0},p(f,R){const E={};R&2&&(E.$$scope={dirty:R,ctx:f}),t.$set(E);const z={};R&2&&(z.$$scope={dirty:R,ctx:f}),r.$set(z)},i(f){H||(M(t.$$.fragment,f),M(Z.$$.fragment,f),M(J.$$.fragment,f),M(r.$$.fragment,f),M(N.$$.fragment,f),H=!0)},o(f){$(t.$$.fragment,f),$(Z.$$.fragment,f),$(J.$$.fragment,f),$(r.$$.fragment,f),$(N.$$.fragment,f),H=!1},d(f){f&&(l(m),l(s),l(y),l(V),l(W),l(G),l(U),l(X),l(x),l(_),l(B),l(I)),b(t,f),b(Z,f),b(J,f),b(r,f),b(N,f)}}}function as(k){let t,m;return t=new Ye({props:{$$slots:{default:[ls]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(s){h(t.$$.fragment,s)},m(s,c){u(t,s,c),m=!0},p(s,c){const y={};c&2&&(y.$$scope={dirty:c,ctx:s}),t.$set(y)},i(s){m||(M(t.$$.fragment,s),m=!0)},o(s){$(t.$$.fragment,s),m=!1},d(s){b(t,s)}}}function ns(k){let t,m='Keras를 사용하여 모델을 파인 튜닝하는 방법에 익숙하지 않은 경우, <a href="../training#train-a-tensorflow-model-with-keras">여기</a>의 기본 튜토리얼을 확인하세요!';return{c(){t=g("p"),t.innerHTML=m},l(s){t=j(s,"P",{"data-svelte-h":!0}),T(t)!=="svelte-nkj0lu"&&(t.innerHTML=m)},m(s,c){a(s,t,c)},p:pe,d(s){s&&l(t)}}}function ps(k){let t,m,s,c,y,Z="그런 다음 <code>TFAutoModelForSequenceClassification</code>을 사용하여 DistilBERT를 로드하고, 예상되는 레이블 수와 레이블 매핑을 로드할 수 있습니다:",V,W,C,G,U="<code>prepare_tf_dataset()</code>을 사용하여 데이터셋을 <code>tf.data.Dataset</code> 형식으로 변환합니다:",Y,X,J,x,r='<a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a>를 사용하여 훈련할 모델을 구성합니다:',_,B,F,I,N='훈련을 시작하기 전에 설정해야할 마지막 두 가지는 예측에서 정확도를 계산하고, 모델을 Hub에 업로드할 방법을 제공하는 것입니다. 모두 <a href="../main_classes/keras_callbacks">Keras callbacks</a>를 사용하여 수행됩니다.',H,f,R="<code>KerasMetricCallback</code>에 <code>compute_metrics</code>를 전달하여 정확도를 높입니다.",E,z,Q,se,ce="<code>PushToHubCallback</code>에서 모델과 토크나이저를 업로드할 위치를 지정합니다:",L,q,S,A,le="그런 다음 콜백을 함께 묶습니다:",fe,P,D,K,ae='드디어, 모델 훈련을 시작할 준비가 되었습니다! <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a>에 훈련 데이터셋, 검증 데이터셋, 에폭의 수 및 콜백을 전달하여 파인 튜닝합니다:',de,O,ee,te,ne="훈련이 완료되면, 모델이 자동으로 Hub에 업로드되어 모든 사람이 사용할 수 있습니다!",he;return t=new ze({props:{$$slots:{default:[ns]},$$scope:{ctx:k}}}),s=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGNyZWF0ZV9vcHRpbWl6ZXIlMEFpbXBvcnQlMjB0ZW5zb3JmbG93JTIwYXMlMjB0ZiUwQSUwQWJhdGNoX3NpemUlMjAlM0QlMjAxNiUwQW51bV9lcG9jaHMlMjAlM0QlMjA1JTBBYmF0Y2hlc19wZXJfZXBvY2glMjAlM0QlMjBsZW4odG9rZW5pemVkX2ltZGIlNUIlMjJ0cmFpbiUyMiU1RCklMjAlMkYlMkYlMjBiYXRjaF9zaXplJTBBdG90YWxfdHJhaW5fc3RlcHMlMjAlM0QlMjBpbnQoYmF0Y2hlc19wZXJfZXBvY2glMjAqJTIwbnVtX2Vwb2NocyklMEFvcHRpbWl6ZXIlMkMlMjBzY2hlZHVsZSUyMCUzRCUyMGNyZWF0ZV9vcHRpbWl6ZXIoaW5pdF9sciUzRDJlLTUlMkMlMjBudW1fd2FybXVwX3N0ZXBzJTNEMCUyQyUyMG51bV90cmFpbl9zdGVwcyUzRHRvdGFsX3RyYWluX3N0ZXBzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>batches_per_epoch = <span class="hljs-built_in">len</span>(tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>]) // batch_size
<span class="hljs-meta">&gt;&gt;&gt; </span>total_train_steps = <span class="hljs-built_in">int</span>(batches_per_epoch * num_epochs)
<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, schedule = create_optimizer(init_lr=<span class="hljs-number">2e-5</span>, num_warmup_steps=<span class="hljs-number">0</span>, num_train_steps=total_train_steps)`,wrap:!1}}),W=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIlMkMlMjBudW1fbGFiZWxzJTNEMiUyQyUyMGlkMmxhYmVsJTNEaWQybGFiZWwlMkMlMjBsYWJlbDJpZCUzRGxhYmVsMmlkJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>, num_labels=<span class="hljs-number">2</span>, id2label=id2label, label2id=label2id
<span class="hljs-meta">... </span>)`,wrap:!1}}),X=new v({props:{code:"dGZfdHJhaW5fc2V0JTIwJTNEJTIwbW9kZWwucHJlcGFyZV90Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMHRva2VuaXplZF9pbWRiJTVCJTIydHJhaW4lMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBzaHVmZmxlJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGJhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMGNvbGxhdGVfZm4lM0RkYXRhX2NvbGxhdG9yJTJDJTBBKSUwQSUwQXRmX3ZhbGlkYXRpb25fc2V0JTIwJTNEJTIwbW9kZWwucHJlcGFyZV90Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMHRva2VuaXplZF9pbWRiJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHNodWZmbGUlM0RGYWxzZSUyQyUwQSUyMCUyMCUyMCUyMGJhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMGNvbGxhdGVfZm4lM0RkYXRhX2NvbGxhdG9yJTJDJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    tokenized_imdb[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_validation_set = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    tokenized_imdb[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`,wrap:!1}}),B=new v({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzRG9wdGltaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)`,wrap:!1}}),z=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBLZXJhc01ldHJpY0NhbGxiYWNrJTBBJTBBbWV0cmljX2NhbGxiYWNrJTIwJTNEJTIwS2VyYXNNZXRyaWNDYWxsYmFjayhtZXRyaWNfZm4lM0Rjb21wdXRlX21ldHJpY3MlMkMlMjBldmFsX2RhdGFzZXQlM0R0Zl92YWxpZGF0aW9uX3NldCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> KerasMetricCallback

<span class="hljs-meta">&gt;&gt;&gt; </span>metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)`,wrap:!1}}),q=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBQdXNoVG9IdWJDYWxsYmFjayUwQSUwQXB1c2hfdG9faHViX2NhbGxiYWNrJTIwJTNEJTIwUHVzaFRvSHViQ2FsbGJhY2soJTBBJTIwJTIwJTIwJTIwb3V0cHV0X2RpciUzRCUyMm15X2F3ZXNvbWVfbW9kZWwlMjIlMkMlMEElMjAlMjAlMjAlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

<span class="hljs-meta">&gt;&gt;&gt; </span>push_to_hub_callback = PushToHubCallback(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_model&quot;</span>,
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>)`,wrap:!1}}),P=new v({props:{code:"Y2FsbGJhY2tzJTIwJTNEJTIwJTVCbWV0cmljX2NhbGxiYWNrJTJDJTIwcHVzaF90b19odWJfY2FsbGJhY2slNUQ=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>callbacks = [metric_callback, push_to_hub_callback]',wrap:!1}}),O=new v({props:{code:"bW9kZWwuZml0KHglM0R0Zl90cmFpbl9zZXQlMkMlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl92YWxpZGF0aW9uX3NldCUyQyUyMGVwb2NocyUzRDMlMkMlMjBjYWxsYmFja3MlM0RjYWxsYmFja3Mp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=<span class="hljs-number">3</span>, callbacks=callbacks)',wrap:!1}}),{c(){d(t.$$.fragment),m=Yt(`
TensorFlow에서 모델을 파인 튜닝하려면, 먼저 옵티마이저 함수와 학습률 스케쥴, 그리고 일부 훈련 하이퍼파라미터를 설정해야 합니다:

	`),d(s.$$.fragment),c=o(),y=g("p"),y.innerHTML=Z,V=o(),d(W.$$.fragment),C=o(),G=g("p"),G.innerHTML=U,Y=o(),d(X.$$.fragment),J=o(),x=g("p"),x.innerHTML=r,_=o(),d(B.$$.fragment),F=o(),I=g("p"),I.innerHTML=N,H=o(),f=g("p"),f.innerHTML=R,E=o(),d(z.$$.fragment),Q=o(),se=g("p"),se.innerHTML=ce,L=o(),d(q.$$.fragment),S=o(),A=g("p"),A.textContent=le,fe=o(),d(P.$$.fragment),D=o(),K=g("p"),K.innerHTML=ae,de=o(),d(O.$$.fragment),ee=o(),te=g("p"),te.textContent=ne},l(p){h(t.$$.fragment,p),m=Nt(p,`
TensorFlow에서 모델을 파인 튜닝하려면, 먼저 옵티마이저 함수와 학습률 스케쥴, 그리고 일부 훈련 하이퍼파라미터를 설정해야 합니다:

	`),h(s.$$.fragment,p),c=i(p),y=j(p,"P",{"data-svelte-h":!0}),T(y)!=="svelte-1lnro8f"&&(y.innerHTML=Z),V=i(p),h(W.$$.fragment,p),C=i(p),G=j(p,"P",{"data-svelte-h":!0}),T(G)!=="svelte-1hxq7ff"&&(G.innerHTML=U),Y=i(p),h(X.$$.fragment,p),J=i(p),x=j(p,"P",{"data-svelte-h":!0}),T(x)!=="svelte-qrlpiv"&&(x.innerHTML=r),_=i(p),h(B.$$.fragment,p),F=i(p),I=j(p,"P",{"data-svelte-h":!0}),T(I)!=="svelte-i81jhu"&&(I.innerHTML=N),H=i(p),f=j(p,"P",{"data-svelte-h":!0}),T(f)!=="svelte-6lp9q9"&&(f.innerHTML=R),E=i(p),h(z.$$.fragment,p),Q=i(p),se=j(p,"P",{"data-svelte-h":!0}),T(se)!=="svelte-1ilfuc9"&&(se.innerHTML=ce),L=i(p),h(q.$$.fragment,p),S=i(p),A=j(p,"P",{"data-svelte-h":!0}),T(A)!=="svelte-90s2we"&&(A.textContent=le),fe=i(p),h(P.$$.fragment,p),D=i(p),K=j(p,"P",{"data-svelte-h":!0}),T(K)!=="svelte-1ub0tax"&&(K.innerHTML=ae),de=i(p),h(O.$$.fragment,p),ee=i(p),te=j(p,"P",{"data-svelte-h":!0}),T(te)!=="svelte-14vf2pa"&&(te.textContent=ne)},m(p,w){u(t,p,w),a(p,m,w),u(s,p,w),a(p,c,w),a(p,y,w),a(p,V,w),u(W,p,w),a(p,C,w),a(p,G,w),a(p,Y,w),u(X,p,w),a(p,J,w),a(p,x,w),a(p,_,w),u(B,p,w),a(p,F,w),a(p,I,w),a(p,H,w),a(p,f,w),a(p,E,w),u(z,p,w),a(p,Q,w),a(p,se,w),a(p,L,w),u(q,p,w),a(p,S,w),a(p,A,w),a(p,fe,w),u(P,p,w),a(p,D,w),a(p,K,w),a(p,de,w),u(O,p,w),a(p,ee,w),a(p,te,w),he=!0},p(p,w){const Ne={};w&2&&(Ne.$$scope={dirty:w,ctx:p}),t.$set(Ne)},i(p){he||(M(t.$$.fragment,p),M(s.$$.fragment,p),M(W.$$.fragment,p),M(X.$$.fragment,p),M(B.$$.fragment,p),M(z.$$.fragment,p),M(q.$$.fragment,p),M(P.$$.fragment,p),M(O.$$.fragment,p),he=!0)},o(p){$(t.$$.fragment,p),$(s.$$.fragment,p),$(W.$$.fragment,p),$(X.$$.fragment,p),$(B.$$.fragment,p),$(z.$$.fragment,p),$(q.$$.fragment,p),$(P.$$.fragment,p),$(O.$$.fragment,p),he=!1},d(p){p&&(l(m),l(c),l(y),l(V),l(C),l(G),l(Y),l(J),l(x),l(_),l(F),l(I),l(H),l(f),l(E),l(Q),l(se),l(L),l(S),l(A),l(fe),l(D),l(K),l(de),l(ee),l(te)),b(t,p),b(s,p),b(W,p),b(X,p),b(B,p),b(z,p),b(q,p),b(P,p),b(O,p)}}}function rs(k){let t,m;return t=new Ye({props:{$$slots:{default:[ps]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(s){h(t.$$.fragment,s)},m(s,c){u(t,s,c),m=!0},p(s,c){const y={};c&2&&(y.$$scope={dirty:c,ctx:s}),t.$set(y)},i(s){m||(M(t.$$.fragment,s),m=!0)},o(s){$(t.$$.fragment,s),m=!1},d(s){b(t,s)}}}function os(k){let t,m='텍스트 분류를 위한 모델을 파인 튜닝하는 자세한 예제는 다음 <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb" rel="nofollow">PyTorch notebook</a> 또는 <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb" rel="nofollow">TensorFlow notebook</a>를 참조하세요.';return{c(){t=g("p"),t.innerHTML=m},l(s){t=j(s,"P",{"data-svelte-h":!0}),T(t)!=="svelte-105n05r"&&(t.innerHTML=m)},m(s,c){a(s,t,c)},p:pe,d(s){s&&l(t)}}}function is(k){let t,m="텍스트를 토큰화하고 PyTorch 텐서를 반환합니다.",s,c,y,Z,V="입력을 모델에 전달하고 <code>logits</code>을 반환합니다:",W,C,G,U,Y="가장 높은 확률을 가진 클래스를 모델의 <code>id2label</code> 매핑을 사용하여 텍스트 레이블로 변환합니다:",X,J,x;return c=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),C=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMnN0ZXZobGl1JTJGbXlfYXdlc29tZV9tb2RlbCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits`,wrap:!1}}),J=new v({props:{code:"cHJlZGljdGVkX2NsYXNzX2lkJTIwJTNEJTIwbG9naXRzLmFyZ21heCgpLml0ZW0oKSUwQW1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9jbGFzc19pZCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = logits.argmax().item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;POSITIVE&#x27;</span>`,wrap:!1}}),{c(){t=g("p"),t.textContent=m,s=o(),d(c.$$.fragment),y=o(),Z=g("p"),Z.innerHTML=V,W=o(),d(C.$$.fragment),G=o(),U=g("p"),U.innerHTML=Y,X=o(),d(J.$$.fragment)},l(r){t=j(r,"P",{"data-svelte-h":!0}),T(t)!=="svelte-1f989ch"&&(t.textContent=m),s=i(r),h(c.$$.fragment,r),y=i(r),Z=j(r,"P",{"data-svelte-h":!0}),T(Z)!=="svelte-1hjuppo"&&(Z.innerHTML=V),W=i(r),h(C.$$.fragment,r),G=i(r),U=j(r,"P",{"data-svelte-h":!0}),T(U)!=="svelte-1jbp04u"&&(U.innerHTML=Y),X=i(r),h(J.$$.fragment,r)},m(r,_){a(r,t,_),a(r,s,_),u(c,r,_),a(r,y,_),a(r,Z,_),a(r,W,_),u(C,r,_),a(r,G,_),a(r,U,_),a(r,X,_),u(J,r,_),x=!0},p:pe,i(r){x||(M(c.$$.fragment,r),M(C.$$.fragment,r),M(J.$$.fragment,r),x=!0)},o(r){$(c.$$.fragment,r),$(C.$$.fragment,r),$(J.$$.fragment,r),x=!1},d(r){r&&(l(t),l(s),l(y),l(Z),l(W),l(G),l(U),l(X)),b(c,r),b(C,r),b(J,r)}}}function ms(k){let t,m;return t=new Ye({props:{$$slots:{default:[is]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(s){h(t.$$.fragment,s)},m(s,c){u(t,s,c),m=!0},p(s,c){const y={};c&2&&(y.$$scope={dirty:c,ctx:s}),t.$set(y)},i(s){m||(M(t.$$.fragment,s),m=!0)},o(s){$(t.$$.fragment,s),m=!1},d(s){b(t,s)}}}function cs(k){let t,m="텍스트를 토큰화하고 TensorFlow 텐서를 반환합니다:",s,c,y,Z,V="입력값을 모델에 전달하고 <code>logits</code>을 반환합니다:",W,C,G,U,Y="가장 높은 확률을 가진 클래스를 모델의 <code>id2label</code> 매핑을 사용하여 텍스트 레이블로 변환합니다:",X,J,x;return c=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),C=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfbW9kZWwlMjIpJTBBbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`,wrap:!1}}),J=new v({props:{code:"cHJlZGljdGVkX2NsYXNzX2lkJTIwJTNEJTIwaW50KHRmLm1hdGguYXJnbWF4KGxvZ2l0cyUyQyUyMGF4aXMlM0QtMSklNUIwJTVEKSUwQW1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9jbGFzc19pZCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;POSITIVE&#x27;</span>`,wrap:!1}}),{c(){t=g("p"),t.textContent=m,s=o(),d(c.$$.fragment),y=o(),Z=g("p"),Z.innerHTML=V,W=o(),d(C.$$.fragment),G=o(),U=g("p"),U.innerHTML=Y,X=o(),d(J.$$.fragment)},l(r){t=j(r,"P",{"data-svelte-h":!0}),T(t)!=="svelte-1m12ipr"&&(t.textContent=m),s=i(r),h(c.$$.fragment,r),y=i(r),Z=j(r,"P",{"data-svelte-h":!0}),T(Z)!=="svelte-tcnp5y"&&(Z.innerHTML=V),W=i(r),h(C.$$.fragment,r),G=i(r),U=j(r,"P",{"data-svelte-h":!0}),T(U)!=="svelte-1jbp04u"&&(U.innerHTML=Y),X=i(r),h(J.$$.fragment,r)},m(r,_){a(r,t,_),a(r,s,_),u(c,r,_),a(r,y,_),a(r,Z,_),a(r,W,_),u(C,r,_),a(r,G,_),a(r,U,_),a(r,X,_),u(J,r,_),x=!0},p:pe,i(r){x||(M(c.$$.fragment,r),M(C.$$.fragment,r),M(J.$$.fragment,r),x=!0)},o(r){$(c.$$.fragment,r),$(C.$$.fragment,r),$(J.$$.fragment,r),x=!1},d(r){r&&(l(t),l(s),l(y),l(Z),l(W),l(G),l(U),l(X)),b(c,r),b(C,r),b(J,r)}}}function fs(k){let t,m;return t=new Ye({props:{$$slots:{default:[cs]},$$scope:{ctx:k}}}),{c(){d(t.$$.fragment)},l(s){h(t.$$.fragment,s)},m(s,c){u(t,s,c),m=!0},p(s,c){const y={};c&2&&(y.$$scope={dirty:c,ctx:s}),t.$set(y)},i(s){m||(M(t.$$.fragment,s),m=!0)},o(s){$(t.$$.fragment,s),m=!1},d(s){b(t,s)}}}function ds(k){let t,m,s,c,y,Z,V,W,C,G,U,Y="텍스트 분류는 자연어 처리의 일종으로, 텍스트에 레이블 또는 클래스를 지정하는 작업입니다. 많은 대기업이 다양한 실용적인 응용 분야에서 텍스트 분류를 운영하고 있습니다. 가장 인기 있는 텍스트 분류 형태 중 하나는 감성 분석으로, 텍스트 시퀀스에 🙂 긍정, 🙁 부정 또는 😐 중립과 같은 레이블을 지정합니다.",X,J,x="이 가이드에서 학습할 내용은:",r,_,B='<li><a href="https://huggingface.co/datasets/imdb" rel="nofollow">IMDb</a> 데이터셋에서 <a href="https://huggingface.co/distilbert/distilbert-base-uncased" rel="nofollow">DistilBERT</a>를 파인 튜닝하여 영화 리뷰가 긍정적인지 부정적인지 판단합니다.</li> <li>추론을 위해 파인 튜닝 모델을 사용합니다.</li>',F,I,N,H,f="시작하기 전에, 필요한 모든 라이브러리가 설치되어 있는지 확인하세요:",R,E,z,Q,se="Hugging Face 계정에 로그인하여 모델을 업로드하고 커뮤니티에 공유하는 것을 권장합니다. 메시지가 표시되면, 토큰을 입력하여 로그인하세요:",ce,L,q,S,A,le,fe="먼저 🤗 Datasets 라이브러리에서 IMDb 데이터셋을 가져옵니다:",P,D,K,ae,de="그런 다음 예시를 살펴봅시다:",O,ee,te,ne,he="이 데이터셋에는 두 가지 필드가 있습니다:",p,w,Ne="<li><code>text</code>: 영화 리뷰 텍스트</li> <li><code>label</code>: <code>0</code>은 부정적인 리뷰, <code>1</code>은 긍정적인 리뷰를 나타냅니다.</li>",Fe,ue,Qe,Me,jt="다음 단계는 DistilBERT 토크나이저를 가져와서 <code>text</code> 필드를 전처리하는 것입니다:",Le,$e,qe,be,wt="<code>text</code>를 토큰화하고 시퀀스가 DistilBERT의 최대 입력 길이보다 길지 않도록 자르기 위한 전처리 함수를 생성하세요:",Se,ye,Ae,ge,Tt="전체 데이터셋에 전처리 함수를 적용하려면, 🤗 Datasets <code>map</code> 함수를 사용하세요. 데이터셋의 여러 요소를 한 번에 처리하기 위해 <code>batched=True</code>로 설정함으로써 데이터셋 <code>map</code>를 더 빠르게 처리할 수 있습니다:",Pe,je,De,we,_t="이제 <code>DataCollatorWithPadding</code>를 사용하여 예제 배치를 만들어봅시다. 데이터셋 전체를 최대 길이로 패딩하는 대신, <em>동적 패딩</em>을 사용하여 배치에서 가장 긴 길이에 맞게 문장을 패딩하는 것이 효율적입니다.",Ke,re,Oe,Te,et,_e,Jt='훈련 중 모델의 성능을 평가하기 위해 메트릭을 포함하는 것이 유용합니다. 🤗 <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> 라이브러리를 사용하여 빠르게 평가 방법을 로드할 수 있습니다. 이 작업에서는 <a href="https://huggingface.co/spaces/evaluate-metric/accuracy" rel="nofollow">accuracy</a> 메트릭을 가져옵니다. (메트릭을 가져오고 계산하는 방법에 대해서는 🤗 Evaluate <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">quick tour</a>를 참조하세요):',tt,Je,st,Ue,Ut="그런 다음 <code>compute_metrics</code> 함수를 만들어서 예측과 레이블을 계산하여 정확도를 계산하도록 <code>compute</code>를 호출합니다:",lt,ke,at,Ze,kt="이제 <code>compute_metrics</code> 함수는 준비되었고, 훈련 과정을 설정할 때 다시 살펴볼 예정입니다.",nt,Ce,pt,ve,Zt="모델을 훈련하기 전에, <code>id2label</code>와 <code>label2id</code>를 사용하여 예상되는 id와 레이블의 맵을 생성하세요:",rt,We,ot,oe,it,ie,mt,Re,ct,Ge,Ct="좋아요, 이제 모델을 파인 튜닝했으니 추론에 사용할 수 있습니다!",ft,Xe,vt="추론을 수행하고자 하는 텍스트를 가져와봅시다:",dt,xe,ht,Ve,Wt="파인 튜닝된 모델로 추론을 시도하는 가장 간단한 방법은 <code>pipeline()</code>를 사용하는 것입니다. 모델로 감정 분석을 위한 <code>pipeline</code>을 인스턴스화하고, 텍스트를 전달해보세요:",ut,Ie,Mt,Be,Rt="원한다면, <code>pipeline</code>의 결과를 수동으로 복제할 수도 있습니다.",$t,me,bt,Ee,yt;return y=new He({props:{title:"텍스트 분류",local:"text-classification",headingTag:"h1"}}),V=new At({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/sequence_classification.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/sequence_classification.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/sequence_classification.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/sequence_classification.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/sequence_classification.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/sequence_classification.ipynb"}]}}),C=new St({props:{id:"leNG9fN9FQU"}}),I=new ze({props:{$$slots:{default:[Pt]},$$scope:{ctx:k}}}),E=new v({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGU=",highlighted:"pip install transformers datasets evaluate",wrap:!1}}),L=new v({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),S=new He({props:{title:"IMDb 데이터셋 가져오기",local:"load-imdb-dataset",headingTag:"h2"}}),D=new v({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBaW1kYiUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJpbWRiJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>imdb = load_dataset(<span class="hljs-string">&quot;imdb&quot;</span>)`,wrap:!1}}),ee=new v({props:{code:"aW1kYiU1QiUyMnRlc3QlMjIlNUQlNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>imdb[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-number">0</span>]
{
    <span class="hljs-string">&quot;label&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn&#x27;t match the background, and painfully one-dimensional characters cannot be overcome with a &#x27;sci-fi&#x27; setting. (I&#x27;m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It&#x27;s not. It&#x27;s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It&#x27;s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it&#x27;s rubbish as they have to always say \\&quot;Gene Roddenberry&#x27;s Earth...\\&quot; otherwise people would not continue watching. Roddenberry&#x27;s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.&quot;</span>,
}`,wrap:!1}}),ue=new He({props:{title:"전처리",local:"preprocess",headingTag:"h2"}}),$e=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),ye=new v({props:{code:"ZGVmJTIwcHJlcHJvY2Vzc19mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjByZXR1cm4lMjB0b2tlbml6ZXIoZXhhbXBsZXMlNUIlMjJ0ZXh0JTIyJTVEJTJDJTIwdHJ1bmNhdGlvbiUzRFRydWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], truncation=<span class="hljs-literal">True</span>)`,wrap:!1}}),je=new v({props:{code:"dG9rZW5pemVkX2ltZGIlMjAlM0QlMjBpbWRiLm1hcChwcmVwcm9jZXNzX2Z1bmN0aW9uJTJDJTIwYmF0Y2hlZCUzRFRydWUp",highlighted:'tokenized_imdb = imdb.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)',wrap:!1}}),re=new gt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[es],pytorch:[Kt]},$$scope:{ctx:k}}}),Te=new He({props:{title:"평가하기",local:"evaluate",headingTag:"h2"}}),Je=new v({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFhY2N1cmFjeSUyMCUzRCUyMGV2YWx1YXRlLmxvYWQoJTIyYWNjdXJhY3klMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`,wrap:!1}}),ke=new v({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUyQyUyMGxhYmVscyUyMCUzRCUyMGV2YWxfcHJlZCUwQSUyMCUyMCUyMCUyMHByZWRpY3Rpb25zJTIwJTNEJTIwbnAuYXJnbWF4KHByZWRpY3Rpb25zJTJDJTIwYXhpcyUzRDEpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwYWNjdXJhY3kuY29tcHV0ZShwcmVkaWN0aW9ucyUzRHByZWRpY3Rpb25zJTJDJTIwcmVmZXJlbmNlcyUzRGxhYmVscyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    predictions, labels = eval_pred
<span class="hljs-meta">... </span>    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> accuracy.compute(predictions=predictions, references=labels)`,wrap:!1}}),Ce=new He({props:{title:"훈련",local:"train",headingTag:"h2"}}),We=new v({props:{code:"aWQybGFiZWwlMjAlM0QlMjAlN0IwJTNBJTIwJTIyTkVHQVRJVkUlMjIlMkMlMjAxJTNBJTIwJTIyUE9TSVRJVkUlMjIlN0QlMEFsYWJlbDJpZCUyMCUzRCUyMCU3QiUyMk5FR0FUSVZFJTIyJTNBJTIwMCUyQyUyMCUyMlBPU0lUSVZFJTIyJTNBJTIwMSU3RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>id2label = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;NEGATIVE&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;POSITIVE&quot;</span>}
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {<span class="hljs-string">&quot;NEGATIVE&quot;</span>: <span class="hljs-number">0</span>, <span class="hljs-string">&quot;POSITIVE&quot;</span>: <span class="hljs-number">1</span>}`,wrap:!1}}),oe=new gt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[rs],pytorch:[as]},$$scope:{ctx:k}}}),ie=new ze({props:{$$slots:{default:[os]},$$scope:{ctx:k}}}),Re=new He({props:{title:"추론",local:"inference",headingTag:"h2"}}),xe=new v({props:{code:"dGV4dCUyMCUzRCUyMCUyMlRoaXMlMjB3YXMlMjBhJTIwbWFzdGVycGllY2UuJTIwTm90JTIwY29tcGxldGVseSUyMGZhaXRoZnVsJTIwdG8lMjB0aGUlMjBib29rcyUyQyUyMGJ1dCUyMGVudGhyYWxsaW5nJTIwZnJvbSUyMGJlZ2lubmluZyUyMHRvJTIwZW5kLiUyME1pZ2h0JTIwYmUlMjBteSUyMGZhdm9yaXRlJTIwb2YlMjB0aGUlMjB0aHJlZS4lMjI=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.&quot;</span>',wrap:!1}}),Ie=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMnNlbnRpbWVudC1hbmFseXNpcyUyMiUyQyUyMG1vZGVsJTNEJTIyc3RldmhsaXUlMkZteV9hd2Vzb21lX21vZGVsJTIyKSUwQWNsYXNzaWZpZXIodGV4dCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=<span class="hljs-string">&quot;stevhliu/my_awesome_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(text)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9994940757751465</span>}]`,wrap:!1}}),me=new gt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[fs],pytorch:[ms]},$$scope:{ctx:k}}}),{c(){t=g("meta"),m=o(),s=g("p"),c=o(),d(y.$$.fragment),Z=o(),d(V.$$.fragment),W=o(),d(C.$$.fragment),G=o(),U=g("p"),U.textContent=Y,X=o(),J=g("p"),J.textContent=x,r=o(),_=g("ol"),_.innerHTML=B,F=o(),d(I.$$.fragment),N=o(),H=g("p"),H.textContent=f,R=o(),d(E.$$.fragment),z=o(),Q=g("p"),Q.textContent=se,ce=o(),d(L.$$.fragment),q=o(),d(S.$$.fragment),A=o(),le=g("p"),le.textContent=fe,P=o(),d(D.$$.fragment),K=o(),ae=g("p"),ae.textContent=de,O=o(),d(ee.$$.fragment),te=o(),ne=g("p"),ne.textContent=he,p=o(),w=g("ul"),w.innerHTML=Ne,Fe=o(),d(ue.$$.fragment),Qe=o(),Me=g("p"),Me.innerHTML=jt,Le=o(),d($e.$$.fragment),qe=o(),be=g("p"),be.innerHTML=wt,Se=o(),d(ye.$$.fragment),Ae=o(),ge=g("p"),ge.innerHTML=Tt,Pe=o(),d(je.$$.fragment),De=o(),we=g("p"),we.innerHTML=_t,Ke=o(),d(re.$$.fragment),Oe=o(),d(Te.$$.fragment),et=o(),_e=g("p"),_e.innerHTML=Jt,tt=o(),d(Je.$$.fragment),st=o(),Ue=g("p"),Ue.innerHTML=Ut,lt=o(),d(ke.$$.fragment),at=o(),Ze=g("p"),Ze.innerHTML=kt,nt=o(),d(Ce.$$.fragment),pt=o(),ve=g("p"),ve.innerHTML=Zt,rt=o(),d(We.$$.fragment),ot=o(),d(oe.$$.fragment),it=o(),d(ie.$$.fragment),mt=o(),d(Re.$$.fragment),ct=o(),Ge=g("p"),Ge.textContent=Ct,ft=o(),Xe=g("p"),Xe.textContent=vt,dt=o(),d(xe.$$.fragment),ht=o(),Ve=g("p"),Ve.innerHTML=Wt,ut=o(),d(Ie.$$.fragment),Mt=o(),Be=g("p"),Be.innerHTML=Rt,$t=o(),d(me.$$.fragment),bt=o(),Ee=g("p"),this.h()},l(e){const n=Lt("svelte-u9bgzb",document.head);t=j(n,"META",{name:!0,content:!0}),n.forEach(l),m=i(e),s=j(e,"P",{}),Bt(s).forEach(l),c=i(e),h(y.$$.fragment,e),Z=i(e),h(V.$$.fragment,e),W=i(e),h(C.$$.fragment,e),G=i(e),U=j(e,"P",{"data-svelte-h":!0}),T(U)!=="svelte-u67xp1"&&(U.textContent=Y),X=i(e),J=j(e,"P",{"data-svelte-h":!0}),T(J)!=="svelte-14hiaa1"&&(J.textContent=x),r=i(e),_=j(e,"OL",{"data-svelte-h":!0}),T(_)!=="svelte-10cs0j"&&(_.innerHTML=B),F=i(e),h(I.$$.fragment,e),N=i(e),H=j(e,"P",{"data-svelte-h":!0}),T(H)!=="svelte-1bc8bfk"&&(H.textContent=f),R=i(e),h(E.$$.fragment,e),z=i(e),Q=j(e,"P",{"data-svelte-h":!0}),T(Q)!=="svelte-1hhyu5y"&&(Q.textContent=se),ce=i(e),h(L.$$.fragment,e),q=i(e),h(S.$$.fragment,e),A=i(e),le=j(e,"P",{"data-svelte-h":!0}),T(le)!=="svelte-71u4tm"&&(le.textContent=fe),P=i(e),h(D.$$.fragment,e),K=i(e),ae=j(e,"P",{"data-svelte-h":!0}),T(ae)!=="svelte-18cnd8l"&&(ae.textContent=de),O=i(e),h(ee.$$.fragment,e),te=i(e),ne=j(e,"P",{"data-svelte-h":!0}),T(ne)!=="svelte-nfqprk"&&(ne.textContent=he),p=i(e),w=j(e,"UL",{"data-svelte-h":!0}),T(w)!=="svelte-z448yg"&&(w.innerHTML=Ne),Fe=i(e),h(ue.$$.fragment,e),Qe=i(e),Me=j(e,"P",{"data-svelte-h":!0}),T(Me)!=="svelte-1t0711w"&&(Me.innerHTML=jt),Le=i(e),h($e.$$.fragment,e),qe=i(e),be=j(e,"P",{"data-svelte-h":!0}),T(be)!=="svelte-1lyhad1"&&(be.innerHTML=wt),Se=i(e),h(ye.$$.fragment,e),Ae=i(e),ge=j(e,"P",{"data-svelte-h":!0}),T(ge)!=="svelte-y65sjr"&&(ge.innerHTML=Tt),Pe=i(e),h(je.$$.fragment,e),De=i(e),we=j(e,"P",{"data-svelte-h":!0}),T(we)!=="svelte-1i6efmb"&&(we.innerHTML=_t),Ke=i(e),h(re.$$.fragment,e),Oe=i(e),h(Te.$$.fragment,e),et=i(e),_e=j(e,"P",{"data-svelte-h":!0}),T(_e)!=="svelte-16fyv40"&&(_e.innerHTML=Jt),tt=i(e),h(Je.$$.fragment,e),st=i(e),Ue=j(e,"P",{"data-svelte-h":!0}),T(Ue)!=="svelte-1wr52zz"&&(Ue.innerHTML=Ut),lt=i(e),h(ke.$$.fragment,e),at=i(e),Ze=j(e,"P",{"data-svelte-h":!0}),T(Ze)!=="svelte-11m7xom"&&(Ze.innerHTML=kt),nt=i(e),h(Ce.$$.fragment,e),pt=i(e),ve=j(e,"P",{"data-svelte-h":!0}),T(ve)!=="svelte-84b3vk"&&(ve.innerHTML=Zt),rt=i(e),h(We.$$.fragment,e),ot=i(e),h(oe.$$.fragment,e),it=i(e),h(ie.$$.fragment,e),mt=i(e),h(Re.$$.fragment,e),ct=i(e),Ge=j(e,"P",{"data-svelte-h":!0}),T(Ge)!=="svelte-1r6hgyn"&&(Ge.textContent=Ct),ft=i(e),Xe=j(e,"P",{"data-svelte-h":!0}),T(Xe)!=="svelte-ej1eir"&&(Xe.textContent=vt),dt=i(e),h(xe.$$.fragment,e),ht=i(e),Ve=j(e,"P",{"data-svelte-h":!0}),T(Ve)!=="svelte-hpekrl"&&(Ve.innerHTML=Wt),ut=i(e),h(Ie.$$.fragment,e),Mt=i(e),Be=j(e,"P",{"data-svelte-h":!0}),T(Be)!=="svelte-1kfpoxm"&&(Be.innerHTML=Rt),$t=i(e),h(me.$$.fragment,e),bt=i(e),Ee=j(e,"P",{}),Bt(Ee).forEach(l),this.h()},h(){Ht(t,"name","hf:doc:metadata"),Ht(t,"content",hs)},m(e,n){qt(document.head,t),a(e,m,n),a(e,s,n),a(e,c,n),u(y,e,n),a(e,Z,n),u(V,e,n),a(e,W,n),u(C,e,n),a(e,G,n),a(e,U,n),a(e,X,n),a(e,J,n),a(e,r,n),a(e,_,n),a(e,F,n),u(I,e,n),a(e,N,n),a(e,H,n),a(e,R,n),u(E,e,n),a(e,z,n),a(e,Q,n),a(e,ce,n),u(L,e,n),a(e,q,n),u(S,e,n),a(e,A,n),a(e,le,n),a(e,P,n),u(D,e,n),a(e,K,n),a(e,ae,n),a(e,O,n),u(ee,e,n),a(e,te,n),a(e,ne,n),a(e,p,n),a(e,w,n),a(e,Fe,n),u(ue,e,n),a(e,Qe,n),a(e,Me,n),a(e,Le,n),u($e,e,n),a(e,qe,n),a(e,be,n),a(e,Se,n),u(ye,e,n),a(e,Ae,n),a(e,ge,n),a(e,Pe,n),u(je,e,n),a(e,De,n),a(e,we,n),a(e,Ke,n),u(re,e,n),a(e,Oe,n),u(Te,e,n),a(e,et,n),a(e,_e,n),a(e,tt,n),u(Je,e,n),a(e,st,n),a(e,Ue,n),a(e,lt,n),u(ke,e,n),a(e,at,n),a(e,Ze,n),a(e,nt,n),u(Ce,e,n),a(e,pt,n),a(e,ve,n),a(e,rt,n),u(We,e,n),a(e,ot,n),u(oe,e,n),a(e,it,n),u(ie,e,n),a(e,mt,n),u(Re,e,n),a(e,ct,n),a(e,Ge,n),a(e,ft,n),a(e,Xe,n),a(e,dt,n),u(xe,e,n),a(e,ht,n),a(e,Ve,n),a(e,ut,n),u(Ie,e,n),a(e,Mt,n),a(e,Be,n),a(e,$t,n),u(me,e,n),a(e,bt,n),a(e,Ee,n),yt=!0},p(e,[n]){const Gt={};n&2&&(Gt.$$scope={dirty:n,ctx:e}),I.$set(Gt);const Xt={};n&2&&(Xt.$$scope={dirty:n,ctx:e}),re.$set(Xt);const xt={};n&2&&(xt.$$scope={dirty:n,ctx:e}),oe.$set(xt);const Vt={};n&2&&(Vt.$$scope={dirty:n,ctx:e}),ie.$set(Vt);const It={};n&2&&(It.$$scope={dirty:n,ctx:e}),me.$set(It)},i(e){yt||(M(y.$$.fragment,e),M(V.$$.fragment,e),M(C.$$.fragment,e),M(I.$$.fragment,e),M(E.$$.fragment,e),M(L.$$.fragment,e),M(S.$$.fragment,e),M(D.$$.fragment,e),M(ee.$$.fragment,e),M(ue.$$.fragment,e),M($e.$$.fragment,e),M(ye.$$.fragment,e),M(je.$$.fragment,e),M(re.$$.fragment,e),M(Te.$$.fragment,e),M(Je.$$.fragment,e),M(ke.$$.fragment,e),M(Ce.$$.fragment,e),M(We.$$.fragment,e),M(oe.$$.fragment,e),M(ie.$$.fragment,e),M(Re.$$.fragment,e),M(xe.$$.fragment,e),M(Ie.$$.fragment,e),M(me.$$.fragment,e),yt=!0)},o(e){$(y.$$.fragment,e),$(V.$$.fragment,e),$(C.$$.fragment,e),$(I.$$.fragment,e),$(E.$$.fragment,e),$(L.$$.fragment,e),$(S.$$.fragment,e),$(D.$$.fragment,e),$(ee.$$.fragment,e),$(ue.$$.fragment,e),$($e.$$.fragment,e),$(ye.$$.fragment,e),$(je.$$.fragment,e),$(re.$$.fragment,e),$(Te.$$.fragment,e),$(Je.$$.fragment,e),$(ke.$$.fragment,e),$(Ce.$$.fragment,e),$(We.$$.fragment,e),$(oe.$$.fragment,e),$(ie.$$.fragment,e),$(Re.$$.fragment,e),$(xe.$$.fragment,e),$(Ie.$$.fragment,e),$(me.$$.fragment,e),yt=!1},d(e){e&&(l(m),l(s),l(c),l(Z),l(W),l(G),l(U),l(X),l(J),l(r),l(_),l(F),l(N),l(H),l(R),l(z),l(Q),l(ce),l(q),l(A),l(le),l(P),l(K),l(ae),l(O),l(te),l(ne),l(p),l(w),l(Fe),l(Qe),l(Me),l(Le),l(qe),l(be),l(Se),l(Ae),l(ge),l(Pe),l(De),l(we),l(Ke),l(Oe),l(et),l(_e),l(tt),l(st),l(Ue),l(lt),l(at),l(Ze),l(nt),l(pt),l(ve),l(rt),l(ot),l(it),l(mt),l(ct),l(Ge),l(ft),l(Xe),l(dt),l(ht),l(Ve),l(ut),l(Mt),l(Be),l($t),l(bt),l(Ee)),l(t),b(y,e),b(V,e),b(C,e),b(I,e),b(E,e),b(L,e),b(S,e),b(D,e),b(ee,e),b(ue,e),b($e,e),b(ye,e),b(je,e),b(re,e),b(Te,e),b(Je,e),b(ke,e),b(Ce,e),b(We,e),b(oe,e),b(ie,e),b(Re,e),b(xe,e),b(Ie,e),b(me,e)}}}const hs='{"title":"텍스트 분류","local":"text-classification","sections":[{"title":"IMDb 데이터셋 가져오기","local":"load-imdb-dataset","sections":[],"depth":2},{"title":"전처리","local":"preprocess","sections":[],"depth":2},{"title":"평가하기","local":"evaluate","sections":[],"depth":2},{"title":"훈련","local":"train","sections":[],"depth":2},{"title":"추론","local":"inference","sections":[],"depth":2}],"depth":1}';function us(k){return zt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _s extends Ft{constructor(t){super(),Qt(this,t,us,ds,Et,{})}}export{_s as component};
