import{s as ma,o as ca,n as kt}from"../chunks/scheduler.56730f09.js";import{S as ra,i as oa,g as r,s as l,r as u,A as ua,h as o,f as a,c as n,j as la,u as h,x as $,k as na,y as ha,a as e,v as f,d,t as g,w as M,m as fa,n as da}from"../chunks/index.1f144517.js";import{T as vt}from"../chunks/Tip.41e845e5.js";import{Y as ga}from"../chunks/Youtube.62e0f062.js";import{C as _}from"../chunks/CodeBlock.738eeccb.js";import{D as Ma}from"../chunks/DocNotebookDropdown.243c3df7.js";import{F as pa,M as ia}from"../chunks/Markdown.c541024b.js";import{H as Cs}from"../chunks/Heading.57d46534.js";function ja(k){let p,b,i='<a href="../model_doc/audio-spectrogram-transformer">Audio Spectrogram Transformer</a>, <a href="../model_doc/data2vec-audio">Data2VecAudio</a>, <a href="../model_doc/hubert">Hubert</a>, <a href="../model_doc/sew">SEW</a>, <a href="../model_doc/sew-d">SEW-D</a>, <a href="../model_doc/unispeech">UniSpeech</a>, <a href="../model_doc/unispeech-sat">UniSpeechSat</a>, <a href="../model_doc/wav2vec2">Wav2Vec2</a>, <a href="../model_doc/wav2vec2-conformer">Wav2Vec2-Conformer</a>, <a href="../model_doc/wavlm">WavLM</a>, <a href="../model_doc/whisper">Whisper</a>';return{c(){p=fa(`이 튜토리얼에서 설명하는 작업은 아래의 모델 아키텍처에서 지원됩니다:

`),b=r("p"),b.innerHTML=i},l(j){p=da(j,`이 튜토리얼에서 설명하는 작업은 아래의 모델 아키텍처에서 지원됩니다:

`),b=o(j,"P",{"data-svelte-h":!0}),$(b)!=="svelte-1x7pfyj"&&(b.innerHTML=i)},m(j,y){e(j,p,y),e(j,b,y)},p:kt,d(j){j&&(a(p),a(b))}}}function ba(k){let p,b='<code>Trainer</code>로 모델을 미세 조정하는 데 익숙하지 않다면 기본 튜토리얼 <a href="../training#train-with-pytorch-trainer">여기</a>을 살펴보세요!';return{c(){p=r("p"),p.innerHTML=b},l(i){p=o(i,"P",{"data-svelte-h":!0}),$(p)!=="svelte-1ukxqs2"&&(p.innerHTML=b)},m(i,j){e(i,p,j)},p:kt,d(i){i&&a(p)}}}function $a(k){let p,b,i,j="이제 모델 훈련을 시작할 준비가 되었습니다! <code>AutoModelForAudioClassification</code>을 이용해서 Wav2Vec2를 불러옵니다. 예상되는 레이블 수와 레이블 매핑을 지정합니다:",y,U,v,Z,W="이제 세 단계만 남았습니다:",C,w,V="<li>훈련 하이퍼파라미터를 <code>TrainingArguments</code>에 정의합니다. 유일한 필수 매개변수는 모델을 저장할 위치를 지정하는 <code>output_dir</code>입니다. <code>push_to_hub = True</code>를 설정하여 이 모델을 허브로 푸시합니다(모델을 업로드하려면 허깅 페이스에 로그인해야 합니다). 각 에폭이 끝날 때마다 <code>Trainer</code>가 정확도를 평가하고 훈련 체크포인트를 저장합니다.</li> <li>모델, 데이터 세트, 토크나이저, 데이터 콜레이터, <code>compute_metrics</code> 함수와 함께 훈련 인자를 <code>Trainer</code>에 전달합니다.</li> <li><code>train()</code>을 호출하여 모델을 미세 조정합니다.</li>",I,T,R,c,x="훈련이 완료되면 모든 사람이 모델을 사용할 수 있도록 <code>push_to_hub()</code> 메소드를 사용하여 모델을 허브에 공유하세요:",B,G,X;return p=new vt({props:{$$slots:{default:[ba]},$$scope:{ctx:k}}}),U=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24lMkMlMjBUcmFpbmluZ0FyZ3VtZW50cyUyQyUyMFRyYWluZXIlMEElMEFudW1fbGFiZWxzJTIwJTNEJTIwbGVuKGlkMmxhYmVsKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9DbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlJTIyJTJDJTIwbnVtX2xhYmVscyUzRG51bV9sYWJlbHMlMkMlMjBsYWJlbDJpZCUzRGxhYmVsMmlkJTJDJTIwaWQybGFiZWwlM0RpZDJsYWJlbCUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForAudioClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>, num_labels=num_labels, label2id=label2id, id2label=id2label
<span class="hljs-meta">... </span>)`,wrap:!1}}),T=new _({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJteV9hd2Vzb21lX21pbmRfbW9kZWwlMjIlMkMlMEElMjAlMjAlMjAlMjBldmFsdWF0aW9uX3N0cmF0ZWd5JTNEJTIyZXBvY2glMjIlMkMlMEElMjAlMjAlMjAlMjBzYXZlX3N0cmF0ZWd5JTNEJTIyZXBvY2glMjIlMkMlMEElMjAlMjAlMjAlMjBsZWFybmluZ19yYXRlJTNEM2UtNSUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfdHJhaW5fYmF0Y2hfc2l6ZSUzRDMyJTJDJTBBJTIwJTIwJTIwJTIwZ3JhZGllbnRfYWNjdW11bGF0aW9uX3N0ZXBzJTNENCUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEMzIlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNEMTAlMkMlMEElMjAlMjAlMjAlMjB3YXJtdXBfcmF0aW8lM0QwLjElMkMlMEElMjAlMjAlMjAlMjBsb2dnaW5nX3N0ZXBzJTNEMTAlMkMlMEElMjAlMjAlMjAlMjBsb2FkX2Jlc3RfbW9kZWxfYXRfZW5kJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMG1ldHJpY19mb3JfYmVzdF9tb2RlbCUzRCUyMmFjY3VyYWN5JTIyJTJDJTBBJTIwJTIwJTIwJTIwcHVzaF90b19odWIlM0RUcnVlJTJDJTBBKSUwQSUwQXRyYWluZXIlMjAlM0QlMjBUcmFpbmVyKCUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEbW9kZWwlMkMlMEElMjAlMjAlMjAlMjBhcmdzJTNEdHJhaW5pbmdfYXJncyUyQyUwQSUyMCUyMCUyMCUyMHRyYWluX2RhdGFzZXQlM0RlbmNvZGVkX21pbmRzJTVCJTIydHJhaW4lMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBldmFsX2RhdGFzZXQlM0RlbmNvZGVkX21pbmRzJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRGZlYXR1cmVfZXh0cmFjdG9yJTJDJTBBJTIwJTIwJTIwJTIwY29tcHV0ZV9tZXRyaWNzJTNEY29tcHV0ZV9tZXRyaWNzJTJDJTBBKSUwQSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_mind_model&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">3e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">32</span>,
<span class="hljs-meta">... </span>    gradient_accumulation_steps=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">32</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">10</span>,
<span class="hljs-meta">... </span>    warmup_ratio=<span class="hljs-number">0.1</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">10</span>,
<span class="hljs-meta">... </span>    load_best_model_at_end=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    metric_for_best_model=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=encoded_minds[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=encoded_minds[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=feature_extractor,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),G=new _({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),{c(){u(p.$$.fragment),b=l(),i=r("p"),i.innerHTML=j,y=l(),u(U.$$.fragment),v=l(),Z=r("p"),Z.textContent=W,C=l(),w=r("ol"),w.innerHTML=V,I=l(),u(T.$$.fragment),R=l(),c=r("p"),c.innerHTML=x,B=l(),u(G.$$.fragment)},l(m){h(p.$$.fragment,m),b=n(m),i=o(m,"P",{"data-svelte-h":!0}),$(i)!=="svelte-mqhd57"&&(i.innerHTML=j),y=n(m),h(U.$$.fragment,m),v=n(m),Z=o(m,"P",{"data-svelte-h":!0}),$(Z)!=="svelte-1vwg7jz"&&(Z.textContent=W),C=n(m),w=o(m,"OL",{"data-svelte-h":!0}),$(w)!=="svelte-192msm8"&&(w.innerHTML=V),I=n(m),h(T.$$.fragment,m),R=n(m),c=o(m,"P",{"data-svelte-h":!0}),$(c)!=="svelte-yt6kqu"&&(c.innerHTML=x),B=n(m),h(G.$$.fragment,m)},m(m,J){f(p,m,J),e(m,b,J),e(m,i,J),e(m,y,J),f(U,m,J),e(m,v,J),e(m,Z,J),e(m,C,J),e(m,w,J),e(m,I,J),f(T,m,J),e(m,R,J),e(m,c,J),e(m,B,J),f(G,m,J),X=!0},p(m,J){const Is={};J&2&&(Is.$$scope={dirty:J,ctx:m}),p.$set(Is)},i(m){X||(d(p.$$.fragment,m),d(U.$$.fragment,m),d(T.$$.fragment,m),d(G.$$.fragment,m),X=!0)},o(m){g(p.$$.fragment,m),g(U.$$.fragment,m),g(T.$$.fragment,m),g(G.$$.fragment,m),X=!1},d(m){m&&(a(b),a(i),a(y),a(v),a(Z),a(C),a(w),a(I),a(R),a(c),a(B)),M(p,m),M(U,m),M(T,m),M(G,m)}}}function ya(k){let p,b;return p=new ia({props:{$$slots:{default:[$a]},$$scope:{ctx:k}}}),{c(){u(p.$$.fragment)},l(i){h(p.$$.fragment,i)},m(i,j){f(p,i,j),b=!0},p(i,j){const y={};j&2&&(y.$$scope={dirty:j,ctx:i}),p.$set(y)},i(i){b||(d(p.$$.fragment,i),b=!0)},o(i){g(p.$$.fragment,i),b=!1},d(i){M(p,i)}}}function Ja(k){let p,b='For a more in-depth example of how to finetune a model for audio classification, take a look at the corresponding <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb" rel="nofollow">PyTorch notebook</a>.';return{c(){p=r("p"),p.innerHTML=b},l(i){p=o(i,"P",{"data-svelte-h":!0}),$(p)!=="svelte-1zzytb"&&(p.innerHTML=b)},m(i,j){e(i,p,j)},p:kt,d(i){i&&a(p)}}}function wa(k){let p,b="특징 추출기를 가져와서 오디오 파일을 전처리하고 <code>입력</code>을 PyTorch 텐서로 반환합니다:",i,j,y,U,v="모델에 입력을 전달하고 로짓을 반환합니다:",Z,W,C,w,V="확률이 가장 높은 클래스를 가져온 다음 모델의 <code>id2label</code> 매핑을 사용하여 이를 레이블로 변환합니다:",I,T,R;return j=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyc3RldmhsaXUlMkZteV9hd2Vzb21lX21pbmRzX21vZGVsJTIyKSUwQWlucHV0cyUyMCUzRCUyMGZlYXR1cmVfZXh0cmFjdG9yKGRhdGFzZXQlNUIwJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUIlMjJhcnJheSUyMiU1RCUyQyUyMHNhbXBsaW5nX3JhdGUlM0RzYW1wbGluZ19yYXRlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_minds_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),W=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMnN0ZXZobGl1JTJGbXlfYXdlc29tZV9taW5kc19tb2RlbCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_minds_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits`,wrap:!1}}),T=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEElMEFwcmVkaWN0ZWRfY2xhc3NfaWRzJTIwJTNEJTIwdG9yY2guYXJnbWF4KGxvZ2l0cykuaXRlbSgpJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2NsYXNzX2lkcyU1RCUwQXByZWRpY3RlZF9sYWJlbA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_ids = torch.argmax(logits).item()
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = model.config.id2label[predicted_class_ids]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label
<span class="hljs-string">&#x27;cash_deposit&#x27;</span>`,wrap:!1}}),{c(){p=r("p"),p.innerHTML=b,i=l(),u(j.$$.fragment),y=l(),U=r("p"),U.textContent=v,Z=l(),u(W.$$.fragment),C=l(),w=r("p"),w.innerHTML=V,I=l(),u(T.$$.fragment)},l(c){p=o(c,"P",{"data-svelte-h":!0}),$(p)!=="svelte-17y820t"&&(p.innerHTML=b),i=n(c),h(j.$$.fragment,c),y=n(c),U=o(c,"P",{"data-svelte-h":!0}),$(U)!=="svelte-xk5bow"&&(U.textContent=v),Z=n(c),h(W.$$.fragment,c),C=n(c),w=o(c,"P",{"data-svelte-h":!0}),$(w)!=="svelte-qlgilp"&&(w.innerHTML=V),I=n(c),h(T.$$.fragment,c)},m(c,x){e(c,p,x),e(c,i,x),f(j,c,x),e(c,y,x),e(c,U,x),e(c,Z,x),f(W,c,x),e(c,C,x),e(c,w,x),e(c,I,x),f(T,c,x),R=!0},p:kt,i(c){R||(d(j.$$.fragment,c),d(W.$$.fragment,c),d(T.$$.fragment,c),R=!0)},o(c){g(j.$$.fragment,c),g(W.$$.fragment,c),g(T.$$.fragment,c),R=!1},d(c){c&&(a(p),a(i),a(y),a(U),a(Z),a(C),a(w),a(I)),M(j,c),M(W,c),M(T,c)}}}function Ta(k){let p,b;return p=new ia({props:{$$slots:{default:[wa]},$$scope:{ctx:k}}}),{c(){u(p.$$.fragment)},l(i){h(p.$$.fragment,i)},m(i,j){f(p,i,j),b=!0},p(i,j){const y={};j&2&&(y.$$scope={dirty:j,ctx:i}),p.$set(y)},i(i){b||(d(p.$$.fragment,i),b=!0)},o(i){g(p.$$.fragment,i),b=!1},d(i){M(p,i)}}}function _a(k){let p,b,i,j,y,U,v,Z,W,C,w,V="오디오 분류는 텍스트와 마찬가지로 입력 데이터에 클래스 레이블 출력을 할당합니다. 유일한 차이점은 텍스트 입력 대신 원시 오디오 파형이 있다는 것입니다. 오디오 분류의 실제 적용 분야에는 화자의 의도 파악, 언어 분류, 소리로 동물 종을 식별하는 것 등이 있습니다.",I,T,R="이 문서에서 방법을 알아보겠습니다:",c,x,B='<li><a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">MInDS-14</a> 데이터 세트를 <a href="https://huggingface.co/facebook/wav2vec2-base" rel="nofollow">Wav2Vec2</a>로 미세 조정하여 화자의 의도를 분류합니다.</li> <li>추론에 미세 조정된 모델을 사용하세요.</li>',G,X,m,J,Is="시작하기 전에 필요한 라이브러리가 모두 설치되어 있는지 확인하세요:",Rs,H,Gs,E,Ct="모델을 업로드하고 커뮤니티와 공유할 수 있도록 허깅페이스 계정에 로그인하는 것이 좋습니다. 메시지가 표시되면 토큰을 입력하여 로그인합니다:",Vs,Q,Ys,z,Fs,A,It="먼저 🤗 Datasets 라이브러리에서 MinDS-14 데이터 세트를 가져옵니다:",Ns,q,Bs,S,Xt="데이터 세트의 <code>train</code> 분할을 <code>train_test_split</code> 메소드를 사용하여 더 작은 훈련 및 테스트 집합으로 분할합니다. 이렇게 하면 전체 데이터 세트에 더 많은 시간을 소비하기 전에 모든 것이 작동하는지 실험하고 확인할 수 있습니다.",Hs,L,Es,P,Rt="이제 데이터 집합을 살펴볼게요:",Qs,D,zs,K,Gt="데이터 세트에는 <code>lang_id</code> 및 <code>english_transcription</code>과 같은 유용한 정보가 많이 포함되어 있지만 이 가이드에서는 <code>audio</code> 및 <code>intent_class</code>에 중점을 둘 것입니다. 다른 열은 <code>remove_columns</code> 메소드를 사용하여 제거합니다:",As,O,qs,ss,Vt="예시를 살펴보겠습니다:",Ss,ts,Ls,as,Yt="두 개의 필드가 있습니다:",Ps,es,Ft="<li><code>audio</code>: 오디오 파일을 가져오고 리샘플링하기 위해 호출해야 하는 음성 신호의 1차원 <code>배열</code>입니다.</li> <li><code>intent_class</code>: 화자의 의도에 대한 클래스 ID를 나타냅니다.</li>",Ds,ls,Nt="모델이 레이블 ID에서 레이블 이름을 쉽게 가져올 수 있도록 레이블 이름을 정수로 매핑하는 사전을 만들거나 그 반대로 매핑하는 사전을 만듭니다:",Ks,ns,Os,ps,Bt="이제 레이블 ID를 레이블 이름으로 변환할 수 있습니다:",st,is,tt,ms,at,cs,Ht="다음 단계는 오디오 신호를 처리하기 위해 Wav2Vec2 특징 추출기를 가져오는 것입니다:",et,rs,lt,os,Et='MinDS-14 데이터 세트의 샘플링 속도는 8000khz이므로(이 정보는 <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">데이터세트 카드</a>에서 확인할 수 있습니다), 사전 훈련된 Wav2Vec2 모델을 사용하려면 데이터 세트를 16000kHz로 리샘플링해야 합니다:',nt,us,pt,hs,Qt="이제 전처리 함수를 만듭니다:",it,fs,zt='<li>가져올 <code>오디오</code> 열을 호출하고 필요한 경우 오디오 파일을 리샘플링합니다.</li> <li>오디오 파일의 샘플링 속도가 모델에 사전 훈련된 오디오 데이터의 샘플링 속도와 일치하는지 확인합니다. 이 정보는 Wav2Vec2 <a href="https://huggingface.co/facebook/wav2vec2-base" rel="nofollow">모델 카드</a>에서 확인할 수 있습니다.</li> <li>긴 입력이 잘리지 않고 일괄 처리되도록 최대 입력 길이를 설정합니다.</li>',mt,ds,ct,gs,At="전체 데이터 세트에 전처리 기능을 적용하려면 🤗 Datasets <code>map</code> 함수를 사용합니다. <code>batched=True</code>를 설정하여 데이터 집합의 여러 요소를 한 번에 처리하면 <code>map</code>의 속도를 높일 수 있습니다. 필요하지 않은 열을 제거하고 <code>intent_class</code>의 이름을 모델이 예상하는 이름인 <code>label</code>로 변경합니다:",rt,Ms,ot,js,ut,bs,qt='훈련 중에 메트릭을 포함하면 모델의 성능을 평가하는 데 도움이 되는 경우가 많습니다. 🤗 <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> 라이브러리를 사용하여 평가 방법을 빠르게 가져올 수 있습니다. 이 작업에서는 <a href="https://huggingface.co/spaces/evaluate-metric/accuracy" rel="nofollow">accuracy(정확도)</a> 메트릭을 가져옵니다(메트릭을 가져오고 계산하는 방법에 대한 자세한 내용은 🤗 Evalutate <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">빠른 둘러보기</a> 참조하세요):',ht,$s,ft,ys,St="그런 다음 예측과 레이블을 <code>compute</code>에 전달하여 정확도를 계산하는 함수를 만듭니다:",dt,Js,gt,ws,Lt="이제 <code>compute_metrics</code> 함수를 사용할 준비가 되었으며, 트레이닝을 설정할 때 이 함수를 사용합니다.",Mt,Ts,jt,Y,bt,F,$t,_s,yt,xs,Pt="이제 모델을 미세 조정했으니 추론에 사용할 수 있습니다!",Jt,Us,Dt="추론을 실행할 오디오 파일을 가져옵니다. 필요한 경우 오디오 파일의 샘플링 속도를 모델의 샘플링 속도와 일치하도록 리샘플링하는 것을 잊지 마세요!",wt,Ws,Tt,Zs,Kt="추론을 위해 미세 조정한 모델을 시험해 보는 가장 간단한 방법은 <code>pipeline()</code>에서 사용하는 것입니다. 모델을 사용하여 오디오 분류를 위한 <code>pipeline</code>을 인스턴스화하고 오디오 파일을 전달합니다:",_t,ks,xt,vs,Ot="원하는 경우 <code>pipeline</code>의 결과를 수동으로 복제할 수도 있습니다:",Ut,N,Wt,Xs,Zt;return y=new Cs({props:{title:"오디오 분류",local:"audio_classification",headingTag:"h1"}}),v=new Ma({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/audio_classification.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/audio_classification.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/audio_classification.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/audio_classification.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/audio_classification.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/audio_classification.ipynb"}]}}),W=new ga({props:{id:"KWwzcmG98Ds"}}),X=new vt({props:{$$slots:{default:[ja]},$$scope:{ctx:k}}}),H=new _({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGU=",highlighted:"pip install transformers datasets evaluate",wrap:!1}}),Q=new _({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),z=new Cs({props:{title:"MInDS-14 데이터셋 불러오기",local:"load_minds_14_dataset",headingTag:"h2"}}),q=new _({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFtaW5kcyUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJQb2x5QUklMkZtaW5kczE0JTIyJTJDJTIwbmFtZSUzRCUyMmVuLVVTJTIyJTJDJTIwc3BsaXQlM0QlMjJ0cmFpbiUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>minds = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`,wrap:!1}}),L=new _({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy50cmFpbl90ZXN0X3NwbGl0KHRlc3Rfc2l6ZSUzRDAuMik=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.train_test_split(test_size=<span class="hljs-number">0.2</span>)',wrap:!1}}),D=new _({props:{code:"bWluZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds
DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;transcription&#x27;</span>, <span class="hljs-string">&#x27;english_transcription&#x27;</span>, <span class="hljs-string">&#x27;intent_class&#x27;</span>, <span class="hljs-string">&#x27;lang_id&#x27;</span>],
        num_rows: <span class="hljs-number">450</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;transcription&#x27;</span>, <span class="hljs-string">&#x27;english_transcription&#x27;</span>, <span class="hljs-string">&#x27;intent_class&#x27;</span>, <span class="hljs-string">&#x27;lang_id&#x27;</span>],
        num_rows: <span class="hljs-number">113</span>
    })
})`,wrap:!1}}),O=new _({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy5yZW1vdmVfY29sdW1ucyglNUIlMjJwYXRoJTIyJTJDJTIwJTIydHJhbnNjcmlwdGlvbiUyMiUyQyUyMCUyMmVuZ2xpc2hfdHJhbnNjcmlwdGlvbiUyMiUyQyUyMCUyMmxhbmdfaWQlMjIlNUQp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.remove_columns([<span class="hljs-string">&quot;path&quot;</span>, <span class="hljs-string">&quot;transcription&quot;</span>, <span class="hljs-string">&quot;english_transcription&quot;</span>, <span class="hljs-string">&quot;lang_id&quot;</span>])',wrap:!1}}),ts=new _({props:{code:"bWluZHMlNUIlMjJ0cmFpbiUyMiU1RCU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        , ..., -<span class="hljs-number">0.00048828</span>,
         -<span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>},
 <span class="hljs-string">&#x27;intent_class&#x27;</span>: <span class="hljs-number">2</span>}`,wrap:!1}}),ns=new _({props:{code:"bGFiZWxzJTIwJTNEJTIwbWluZHMlNUIlMjJ0cmFpbiUyMiU1RC5mZWF0dXJlcyU1QiUyMmludGVudF9jbGFzcyUyMiU1RC5uYW1lcyUwQWxhYmVsMmlkJTJDJTIwaWQybGFiZWwlMjAlM0QlMjBkaWN0KCklMkMlMjBkaWN0KCklMEFmb3IlMjBpJTJDJTIwbGFiZWwlMjBpbiUyMGVudW1lcmF0ZShsYWJlbHMpJTNBJTBBJTIwJTIwJTIwJTIwbGFiZWwyaWQlNUJsYWJlbCU1RCUyMCUzRCUyMHN0cihpKSUwQSUyMCUyMCUyMCUyMGlkMmxhYmVsJTVCc3RyKGkpJTVEJTIwJTNEJTIwbGFiZWw=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = minds[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">&quot;intent_class&quot;</span>].names
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id, id2label = <span class="hljs-built_in">dict</span>(), <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):
<span class="hljs-meta">... </span>    label2id[label] = <span class="hljs-built_in">str</span>(i)
<span class="hljs-meta">... </span>    id2label[<span class="hljs-built_in">str</span>(i)] = label`,wrap:!1}}),is=new _({props:{code:"aWQybGFiZWwlNUJzdHIoMiklNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>id2label[<span class="hljs-built_in">str</span>(<span class="hljs-number">2</span>)]
<span class="hljs-string">&#x27;app_error&#x27;</span>`,wrap:!1}}),ms=new Cs({props:{title:"전처리",local:"preprocess",headingTag:"h2"}}),rs=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`,wrap:!1}}),us=new _({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy5jYXN0X2NvbHVtbiglMjJhdWRpbyUyMiUyQyUyMEF1ZGlvKHNhbXBsaW5nX3JhdGUlM0QxNl8wMDApKSUwQW1pbmRzJTVCJTIydHJhaW4lMjIlNUQlNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>minds[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">2.2098757e-05</span>,  <span class="hljs-number">4.6582241e-05</span>, -<span class="hljs-number">2.2803260e-05</span>, ...,
         -<span class="hljs-number">2.8419291e-04</span>, -<span class="hljs-number">2.3305941e-04</span>, -<span class="hljs-number">1.1425107e-04</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>},
 <span class="hljs-string">&#x27;intent_class&#x27;</span>: <span class="hljs-number">2</span>}`,wrap:!1}}),ds=new _({props:{code:"ZGVmJTIwcHJlcHJvY2Vzc19mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjBhdWRpb19hcnJheXMlMjAlM0QlMjAlNUJ4JTVCJTIyYXJyYXklMjIlNUQlMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlcyU1QiUyMmF1ZGlvJTIyJTVEJTVEJTBBJTIwJTIwJTIwJTIwaW5wdXRzJTIwJTNEJTIwZmVhdHVyZV9leHRyYWN0b3IoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYXVkaW9fYXJyYXlzJTJDJTIwc2FtcGxpbmdfcmF0ZSUzRGZlYXR1cmVfZXh0cmFjdG9yLnNhbXBsaW5nX3JhdGUlMkMlMjBtYXhfbGVuZ3RoJTNEMTYwMDAlMkMlMjB0cnVuY2F0aW9uJTNEVHJ1ZSUwQSUyMCUyMCUyMCUyMCklMEElMjAlMjAlMjAlMjByZXR1cm4lMjBpbnB1dHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    audio_arrays = [x[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;audio&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = feature_extractor(
<span class="hljs-meta">... </span>        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=<span class="hljs-number">16000</span>, truncation=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`,wrap:!1}}),Ms=new _({props:{code:"ZW5jb2RlZF9taW5kcyUyMCUzRCUyMG1pbmRzLm1hcChwcmVwcm9jZXNzX2Z1bmN0aW9uJTJDJTIwcmVtb3ZlX2NvbHVtbnMlM0QlMjJhdWRpbyUyMiUyQyUyMGJhdGNoZWQlM0RUcnVlKSUwQWVuY29kZWRfbWluZHMlMjAlM0QlMjBlbmNvZGVkX21pbmRzLnJlbmFtZV9jb2x1bW4oJTIyaW50ZW50X2NsYXNzJTIyJTJDJTIwJTIybGFiZWwlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_minds = minds.<span class="hljs-built_in">map</span>(preprocess_function, remove_columns=<span class="hljs-string">&quot;audio&quot;</span>, batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_minds = encoded_minds.rename_column(<span class="hljs-string">&quot;intent_class&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>)`,wrap:!1}}),js=new Cs({props:{title:"평가하기",local:"evaluate",headingTag:"h2"}}),$s=new _({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFhY2N1cmFjeSUyMCUzRCUyMGV2YWx1YXRlLmxvYWQoJTIyYWNjdXJhY3klMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`,wrap:!1}}),Js=new _({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUyMCUzRCUyMG5wLmFyZ21heChldmFsX3ByZWQucHJlZGljdGlvbnMlMkMlMjBheGlzJTNEMSklMEElMjAlMjAlMjAlMjByZXR1cm4lMjBhY2N1cmFjeS5jb21wdXRlKHByZWRpY3Rpb25zJTNEcHJlZGljdGlvbnMlMkMlMjByZWZlcmVuY2VzJTNEZXZhbF9wcmVkLmxhYmVsX2lkcyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    predictions = np.argmax(eval_pred.predictions, axis=<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> accuracy.compute(predictions=predictions, references=eval_pred.label_ids)`,wrap:!1}}),Ts=new Cs({props:{title:"훈련",local:"train",headingTag:"h2"}}),Y=new pa({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[ya]},$$scope:{ctx:k}}}),F=new vt({props:{$$slots:{default:[Ja]},$$scope:{ctx:k}}}),_s=new Cs({props:{title:"추론",local:"inference",headingTag:"h2"}}),Ws=new _({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMlBvbHlBSSUyRm1pbmRzMTQlMjIlMkMlMjBuYW1lJTNEJTIyZW4tVVMlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LmNhc3RfY29sdW1uKCUyMmF1ZGlvJTIyJTJDJTIwQXVkaW8oc2FtcGxpbmdfcmF0ZSUzRDE2MDAwKSklMEFzYW1wbGluZ19yYXRlJTIwJTNEJTIwZGF0YXNldC5mZWF0dXJlcyU1QiUyMmF1ZGlvJTIyJTVELnNhbXBsaW5nX3JhdGUlMEFhdWRpb19maWxlJTIwJTNEJTIwZGF0YXNldCU1QjAlNUQlNUIlMjJhdWRpbyUyMiU1RCU1QiUyMnBhdGglMjIlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_file = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;path&quot;</span>]`,wrap:!1}}),ks=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMmF1ZGlvLWNsYXNzaWZpY2F0aW9uJTIyJTJDJTIwbW9kZWwlM0QlMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfbWluZHNfbW9kZWwlMjIpJTBBY2xhc3NpZmllcihhdWRpb19maWxlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;audio-classification&quot;</span>, model=<span class="hljs-string">&quot;stevhliu/my_awesome_minds_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(audio_file)
[
    {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.09766869246959686</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;cash_deposit&#x27;</span>},
    {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.07998877018690109</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;app_error&#x27;</span>},
    {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.0781070664525032</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;joint_account&#x27;</span>},
    {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.07667109370231628</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;pay_bill&#x27;</span>},
    {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.0755252093076706</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;balance&#x27;</span>}
]`,wrap:!1}}),N=new pa({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Ta]},$$scope:{ctx:k}}}),{c(){p=r("meta"),b=l(),i=r("p"),j=l(),u(y.$$.fragment),U=l(),u(v.$$.fragment),Z=l(),u(W.$$.fragment),C=l(),w=r("p"),w.textContent=V,I=l(),T=r("p"),T.textContent=R,c=l(),x=r("ol"),x.innerHTML=B,G=l(),u(X.$$.fragment),m=l(),J=r("p"),J.textContent=Is,Rs=l(),u(H.$$.fragment),Gs=l(),E=r("p"),E.textContent=Ct,Vs=l(),u(Q.$$.fragment),Ys=l(),u(z.$$.fragment),Fs=l(),A=r("p"),A.textContent=It,Ns=l(),u(q.$$.fragment),Bs=l(),S=r("p"),S.innerHTML=Xt,Hs=l(),u(L.$$.fragment),Es=l(),P=r("p"),P.textContent=Rt,Qs=l(),u(D.$$.fragment),zs=l(),K=r("p"),K.innerHTML=Gt,As=l(),u(O.$$.fragment),qs=l(),ss=r("p"),ss.textContent=Vt,Ss=l(),u(ts.$$.fragment),Ls=l(),as=r("p"),as.textContent=Yt,Ps=l(),es=r("ul"),es.innerHTML=Ft,Ds=l(),ls=r("p"),ls.textContent=Nt,Ks=l(),u(ns.$$.fragment),Os=l(),ps=r("p"),ps.textContent=Bt,st=l(),u(is.$$.fragment),tt=l(),u(ms.$$.fragment),at=l(),cs=r("p"),cs.textContent=Ht,et=l(),u(rs.$$.fragment),lt=l(),os=r("p"),os.innerHTML=Et,nt=l(),u(us.$$.fragment),pt=l(),hs=r("p"),hs.textContent=Qt,it=l(),fs=r("ol"),fs.innerHTML=zt,mt=l(),u(ds.$$.fragment),ct=l(),gs=r("p"),gs.innerHTML=At,rt=l(),u(Ms.$$.fragment),ot=l(),u(js.$$.fragment),ut=l(),bs=r("p"),bs.innerHTML=qt,ht=l(),u($s.$$.fragment),ft=l(),ys=r("p"),ys.innerHTML=St,dt=l(),u(Js.$$.fragment),gt=l(),ws=r("p"),ws.innerHTML=Lt,Mt=l(),u(Ts.$$.fragment),jt=l(),u(Y.$$.fragment),bt=l(),u(F.$$.fragment),$t=l(),u(_s.$$.fragment),yt=l(),xs=r("p"),xs.textContent=Pt,Jt=l(),Us=r("p"),Us.textContent=Dt,wt=l(),u(Ws.$$.fragment),Tt=l(),Zs=r("p"),Zs.innerHTML=Kt,_t=l(),u(ks.$$.fragment),xt=l(),vs=r("p"),vs.innerHTML=Ot,Ut=l(),u(N.$$.fragment),Wt=l(),Xs=r("p"),this.h()},l(s){const t=ua("svelte-u9bgzb",document.head);p=o(t,"META",{name:!0,content:!0}),t.forEach(a),b=n(s),i=o(s,"P",{}),la(i).forEach(a),j=n(s),h(y.$$.fragment,s),U=n(s),h(v.$$.fragment,s),Z=n(s),h(W.$$.fragment,s),C=n(s),w=o(s,"P",{"data-svelte-h":!0}),$(w)!=="svelte-1rid8x9"&&(w.textContent=V),I=n(s),T=o(s,"P",{"data-svelte-h":!0}),$(T)!=="svelte-18ym8yt"&&(T.textContent=R),c=n(s),x=o(s,"OL",{"data-svelte-h":!0}),$(x)!=="svelte-ff8wat"&&(x.innerHTML=B),G=n(s),h(X.$$.fragment,s),m=n(s),J=o(s,"P",{"data-svelte-h":!0}),$(J)!=="svelte-1k0z9pm"&&(J.textContent=Is),Rs=n(s),h(H.$$.fragment,s),Gs=n(s),E=o(s,"P",{"data-svelte-h":!0}),$(E)!=="svelte-c0su0x"&&(E.textContent=Ct),Vs=n(s),h(Q.$$.fragment,s),Ys=n(s),h(z.$$.fragment,s),Fs=n(s),A=o(s,"P",{"data-svelte-h":!0}),$(A)!=="svelte-hvgbt0"&&(A.textContent=It),Ns=n(s),h(q.$$.fragment,s),Bs=n(s),S=o(s,"P",{"data-svelte-h":!0}),$(S)!=="svelte-107kb56"&&(S.innerHTML=Xt),Hs=n(s),h(L.$$.fragment,s),Es=n(s),P=o(s,"P",{"data-svelte-h":!0}),$(P)!=="svelte-11m4j1g"&&(P.textContent=Rt),Qs=n(s),h(D.$$.fragment,s),zs=n(s),K=o(s,"P",{"data-svelte-h":!0}),$(K)!=="svelte-1qmbw2b"&&(K.innerHTML=Gt),As=n(s),h(O.$$.fragment,s),qs=n(s),ss=o(s,"P",{"data-svelte-h":!0}),$(ss)!=="svelte-194ktnh"&&(ss.textContent=Vt),Ss=n(s),h(ts.$$.fragment,s),Ls=n(s),as=o(s,"P",{"data-svelte-h":!0}),$(as)!=="svelte-6aiy91"&&(as.textContent=Yt),Ps=n(s),es=o(s,"UL",{"data-svelte-h":!0}),$(es)!=="svelte-a8ku1e"&&(es.innerHTML=Ft),Ds=n(s),ls=o(s,"P",{"data-svelte-h":!0}),$(ls)!=="svelte-1vm3gzf"&&(ls.textContent=Nt),Ks=n(s),h(ns.$$.fragment,s),Os=n(s),ps=o(s,"P",{"data-svelte-h":!0}),$(ps)!=="svelte-135k07i"&&(ps.textContent=Bt),st=n(s),h(is.$$.fragment,s),tt=n(s),h(ms.$$.fragment,s),at=n(s),cs=o(s,"P",{"data-svelte-h":!0}),$(cs)!=="svelte-1g5n3eo"&&(cs.textContent=Ht),et=n(s),h(rs.$$.fragment,s),lt=n(s),os=o(s,"P",{"data-svelte-h":!0}),$(os)!=="svelte-1w7dtwt"&&(os.innerHTML=Et),nt=n(s),h(us.$$.fragment,s),pt=n(s),hs=o(s,"P",{"data-svelte-h":!0}),$(hs)!=="svelte-1n50b1d"&&(hs.textContent=Qt),it=n(s),fs=o(s,"OL",{"data-svelte-h":!0}),$(fs)!=="svelte-ajo58h"&&(fs.innerHTML=zt),mt=n(s),h(ds.$$.fragment,s),ct=n(s),gs=o(s,"P",{"data-svelte-h":!0}),$(gs)!=="svelte-1rf4gyf"&&(gs.innerHTML=At),rt=n(s),h(Ms.$$.fragment,s),ot=n(s),h(js.$$.fragment,s),ut=n(s),bs=o(s,"P",{"data-svelte-h":!0}),$(bs)!=="svelte-n8ji9s"&&(bs.innerHTML=qt),ht=n(s),h($s.$$.fragment,s),ft=n(s),ys=o(s,"P",{"data-svelte-h":!0}),$(ys)!=="svelte-tpx6n8"&&(ys.innerHTML=St),dt=n(s),h(Js.$$.fragment,s),gt=n(s),ws=o(s,"P",{"data-svelte-h":!0}),$(ws)!=="svelte-1v7hg37"&&(ws.innerHTML=Lt),Mt=n(s),h(Ts.$$.fragment,s),jt=n(s),h(Y.$$.fragment,s),bt=n(s),h(F.$$.fragment,s),$t=n(s),h(_s.$$.fragment,s),yt=n(s),xs=o(s,"P",{"data-svelte-h":!0}),$(xs)!=="svelte-174wmmm"&&(xs.textContent=Pt),Jt=n(s),Us=o(s,"P",{"data-svelte-h":!0}),$(Us)!=="svelte-1uaek2x"&&(Us.textContent=Dt),wt=n(s),h(Ws.$$.fragment,s),Tt=n(s),Zs=o(s,"P",{"data-svelte-h":!0}),$(Zs)!=="svelte-5aredu"&&(Zs.innerHTML=Kt),_t=n(s),h(ks.$$.fragment,s),xt=n(s),vs=o(s,"P",{"data-svelte-h":!0}),$(vs)!=="svelte-186svr5"&&(vs.innerHTML=Ot),Ut=n(s),h(N.$$.fragment,s),Wt=n(s),Xs=o(s,"P",{}),la(Xs).forEach(a),this.h()},h(){na(p,"name","hf:doc:metadata"),na(p,"content",xa)},m(s,t){ha(document.head,p),e(s,b,t),e(s,i,t),e(s,j,t),f(y,s,t),e(s,U,t),f(v,s,t),e(s,Z,t),f(W,s,t),e(s,C,t),e(s,w,t),e(s,I,t),e(s,T,t),e(s,c,t),e(s,x,t),e(s,G,t),f(X,s,t),e(s,m,t),e(s,J,t),e(s,Rs,t),f(H,s,t),e(s,Gs,t),e(s,E,t),e(s,Vs,t),f(Q,s,t),e(s,Ys,t),f(z,s,t),e(s,Fs,t),e(s,A,t),e(s,Ns,t),f(q,s,t),e(s,Bs,t),e(s,S,t),e(s,Hs,t),f(L,s,t),e(s,Es,t),e(s,P,t),e(s,Qs,t),f(D,s,t),e(s,zs,t),e(s,K,t),e(s,As,t),f(O,s,t),e(s,qs,t),e(s,ss,t),e(s,Ss,t),f(ts,s,t),e(s,Ls,t),e(s,as,t),e(s,Ps,t),e(s,es,t),e(s,Ds,t),e(s,ls,t),e(s,Ks,t),f(ns,s,t),e(s,Os,t),e(s,ps,t),e(s,st,t),f(is,s,t),e(s,tt,t),f(ms,s,t),e(s,at,t),e(s,cs,t),e(s,et,t),f(rs,s,t),e(s,lt,t),e(s,os,t),e(s,nt,t),f(us,s,t),e(s,pt,t),e(s,hs,t),e(s,it,t),e(s,fs,t),e(s,mt,t),f(ds,s,t),e(s,ct,t),e(s,gs,t),e(s,rt,t),f(Ms,s,t),e(s,ot,t),f(js,s,t),e(s,ut,t),e(s,bs,t),e(s,ht,t),f($s,s,t),e(s,ft,t),e(s,ys,t),e(s,dt,t),f(Js,s,t),e(s,gt,t),e(s,ws,t),e(s,Mt,t),f(Ts,s,t),e(s,jt,t),f(Y,s,t),e(s,bt,t),f(F,s,t),e(s,$t,t),f(_s,s,t),e(s,yt,t),e(s,xs,t),e(s,Jt,t),e(s,Us,t),e(s,wt,t),f(Ws,s,t),e(s,Tt,t),e(s,Zs,t),e(s,_t,t),f(ks,s,t),e(s,xt,t),e(s,vs,t),e(s,Ut,t),f(N,s,t),e(s,Wt,t),e(s,Xs,t),Zt=!0},p(s,[t]){const sa={};t&2&&(sa.$$scope={dirty:t,ctx:s}),X.$set(sa);const ta={};t&2&&(ta.$$scope={dirty:t,ctx:s}),Y.$set(ta);const aa={};t&2&&(aa.$$scope={dirty:t,ctx:s}),F.$set(aa);const ea={};t&2&&(ea.$$scope={dirty:t,ctx:s}),N.$set(ea)},i(s){Zt||(d(y.$$.fragment,s),d(v.$$.fragment,s),d(W.$$.fragment,s),d(X.$$.fragment,s),d(H.$$.fragment,s),d(Q.$$.fragment,s),d(z.$$.fragment,s),d(q.$$.fragment,s),d(L.$$.fragment,s),d(D.$$.fragment,s),d(O.$$.fragment,s),d(ts.$$.fragment,s),d(ns.$$.fragment,s),d(is.$$.fragment,s),d(ms.$$.fragment,s),d(rs.$$.fragment,s),d(us.$$.fragment,s),d(ds.$$.fragment,s),d(Ms.$$.fragment,s),d(js.$$.fragment,s),d($s.$$.fragment,s),d(Js.$$.fragment,s),d(Ts.$$.fragment,s),d(Y.$$.fragment,s),d(F.$$.fragment,s),d(_s.$$.fragment,s),d(Ws.$$.fragment,s),d(ks.$$.fragment,s),d(N.$$.fragment,s),Zt=!0)},o(s){g(y.$$.fragment,s),g(v.$$.fragment,s),g(W.$$.fragment,s),g(X.$$.fragment,s),g(H.$$.fragment,s),g(Q.$$.fragment,s),g(z.$$.fragment,s),g(q.$$.fragment,s),g(L.$$.fragment,s),g(D.$$.fragment,s),g(O.$$.fragment,s),g(ts.$$.fragment,s),g(ns.$$.fragment,s),g(is.$$.fragment,s),g(ms.$$.fragment,s),g(rs.$$.fragment,s),g(us.$$.fragment,s),g(ds.$$.fragment,s),g(Ms.$$.fragment,s),g(js.$$.fragment,s),g($s.$$.fragment,s),g(Js.$$.fragment,s),g(Ts.$$.fragment,s),g(Y.$$.fragment,s),g(F.$$.fragment,s),g(_s.$$.fragment,s),g(Ws.$$.fragment,s),g(ks.$$.fragment,s),g(N.$$.fragment,s),Zt=!1},d(s){s&&(a(b),a(i),a(j),a(U),a(Z),a(C),a(w),a(I),a(T),a(c),a(x),a(G),a(m),a(J),a(Rs),a(Gs),a(E),a(Vs),a(Ys),a(Fs),a(A),a(Ns),a(Bs),a(S),a(Hs),a(Es),a(P),a(Qs),a(zs),a(K),a(As),a(qs),a(ss),a(Ss),a(Ls),a(as),a(Ps),a(es),a(Ds),a(ls),a(Ks),a(Os),a(ps),a(st),a(tt),a(at),a(cs),a(et),a(lt),a(os),a(nt),a(pt),a(hs),a(it),a(fs),a(mt),a(ct),a(gs),a(rt),a(ot),a(ut),a(bs),a(ht),a(ft),a(ys),a(dt),a(gt),a(ws),a(Mt),a(jt),a(bt),a($t),a(yt),a(xs),a(Jt),a(Us),a(wt),a(Tt),a(Zs),a(_t),a(xt),a(vs),a(Ut),a(Wt),a(Xs)),a(p),M(y,s),M(v,s),M(W,s),M(X,s),M(H,s),M(Q,s),M(z,s),M(q,s),M(L,s),M(D,s),M(O,s),M(ts,s),M(ns,s),M(is,s),M(ms,s),M(rs,s),M(us,s),M(ds,s),M(Ms,s),M(js,s),M($s,s),M(Js,s),M(Ts,s),M(Y,s),M(F,s),M(_s,s),M(Ws,s),M(ks,s),M(N,s)}}}const xa='{"title":"오디오 분류","local":"audio_classification","sections":[{"title":"MInDS-14 데이터셋 불러오기","local":"load_minds_14_dataset","sections":[],"depth":2},{"title":"전처리","local":"preprocess","sections":[],"depth":2},{"title":"평가하기","local":"evaluate","sections":[],"depth":2},{"title":"훈련","local":"train","sections":[],"depth":2},{"title":"추론","local":"inference","sections":[],"depth":2}],"depth":1}';function Ua(k){return ca(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ga extends ra{constructor(p){super(),oa(this,p,Ua,_a,ma,{})}}export{Ga as component};
