import{s as ha,o as fa,n as et}from"../chunks/scheduler.56730f09.js";import{S as ga,i as _a,g as i,s as n,r as p,A as ba,h as l,f as o,c as a,j as T,u,x as m,k as $,y as s,a as r,v as h,d as f,t as g,w as _}from"../chunks/index.1f144517.js";import{T as bn}from"../chunks/Tip.41e845e5.js";import{D as w,E as Mo}from"../chunks/ExampleCodeBlock.ee3463c7.js";import{C as jt}from"../chunks/CodeBlock.738eeccb.js";import{P as ua}from"../chunks/PipelineTag.82d6c31e.js";import{H as Z}from"../chunks/Heading.57d46534.js";function ka(C){let d,v;return d=new jt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMExsYW1hTW9kZWwlMkMlMjBMbGFtYUNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBMTGFNQSUyMGxsYW1hLTdiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMExsYW1hQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjBmcm9tJTIwdGhlJTIwbGxhbWEtN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMExsYW1hTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlamaModel, LlamaConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a LLaMA llama-7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = LlamaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the llama-7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = LlamaModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){p(d.$$.fragment)},l(c){u(d.$$.fragment,c)},m(c,k){h(d,c,k),v=!0},p:et,i(c){v||(f(d.$$.fragment,c),v=!0)},o(c){g(d.$$.fragment,c),v=!1},d(c){_(d,c)}}}function va(C){let d,v="sequence pair mask has the following format:",c,k,x;return k=new jt({props:{code:"MCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMEElN0MlMjBmaXJzdCUyMHNlcXVlbmNlJTIwJTIwJTIwJTIwJTdDJTIwc2Vjb25kJTIwc2VxdWVuY2UlMjAlN0M=",highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`,wrap:!1}}),{c(){d=i("p"),d.textContent=v,c=n(),p(k.$$.fragment)},l(b){d=l(b,"P",{"data-svelte-h":!0}),m(d)!=="svelte-16klr56"&&(d.textContent=v),c=a(b),u(k.$$.fragment,b)},m(b,F){r(b,d,F),r(b,c,F),h(k,b,F),x=!0},p:et,i(b){x||(f(k.$$.fragment,b),x=!0)},o(b){g(k.$$.fragment,b),x=!1},d(b){b&&(o(d),o(c)),_(k,b)}}}function ya(C){let d,v;return d=new jt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMExsYW1hVG9rZW5pemVyRmFzdCUwQSUwQXRva2VuaXplciUyMCUzRCUyMExsYW1hVG9rZW5pemVyRmFzdC5mcm9tX3ByZXRyYWluZWQoJTIyaGYtaW50ZXJuYWwtdGVzdGluZyUyRmxsYW1hLXRva2VuaXplciUyMiklMEF0b2tlbml6ZXIuZW5jb2RlKCUyMkhlbGxvJTIwdGhpcyUyMGlzJTIwYSUyMHRlc3QlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlamaTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = LlamaTokenizerFast.from_pretrained(<span class="hljs-string">&quot;hf-internal-testing/llama-tokenizer&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.encode(<span class="hljs-string">&quot;Hello this is a test&quot;</span>)
[<span class="hljs-number">1</span>, <span class="hljs-number">15043</span>, <span class="hljs-number">445</span>, <span class="hljs-number">338</span>, <span class="hljs-number">263</span>, <span class="hljs-number">1243</span>]`,wrap:!1}}),{c(){p(d.$$.fragment)},l(c){u(d.$$.fragment,c)},m(c,k){h(d,c,k),v=!0},p:et,i(c){v||(f(d.$$.fragment,c),v=!0)},o(c){g(d.$$.fragment,c),v=!1},d(c){_(d,c)}}}function La(C){let d,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=v},l(c){d=l(c,"P",{"data-svelte-h":!0}),m(d)!=="svelte-fincs2"&&(d.innerHTML=v)},m(c,k){r(c,d,k)},p:et,d(c){c&&o(d)}}}function Ta(C){let d,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=v},l(c){d=l(c,"P",{"data-svelte-h":!0}),m(d)!=="svelte-fincs2"&&(d.innerHTML=v)},m(c,k){r(c,d,k)},p:et,d(c){c&&o(d)}}}function $a(C){let d,v="Example:",c,k,x;return k=new jt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBMbGFtYUZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBMbGFtYUZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1oZiUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi03Yi1oZiUyMiklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJIZXklMkMlMjBhcmUlMjB5b3UlMjBjb25zY2lvdXMlM0YlMjBDYW4lMjB5b3UlMjB0YWxrJTIwdG8lMjBtZSUzRiUyMiUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQSUyMyUyMEdlbmVyYXRlJTBBZ2VuZXJhdGVfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoaW5wdXRzLmlucHV0X2lkcyUyQyUyMG1heF9sZW5ndGglM0QzMCklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlJTJDJTIwY2xlYW5fdXBfdG9rZW5pemF0aW9uX3NwYWNlcyUzREZhbHNlKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, LlamaForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = LlamaForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-7b-hf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?\\nI&#x27;m not conscious, but I can talk to you.&quot;</span>`,wrap:!1}}),{c(){d=i("p"),d.textContent=v,c=n(),p(k.$$.fragment)},l(b){d=l(b,"P",{"data-svelte-h":!0}),m(d)!=="svelte-11lpom8"&&(d.textContent=v),c=a(b),u(k.$$.fragment,b)},m(b,F){r(b,d,F),r(b,c,F),h(k,b,F),x=!0},p:et,i(b){x||(f(k.$$.fragment,b),x=!0)},o(b){g(k.$$.fragment,b),x=!1},d(b){b&&(o(d),o(c)),_(k,b)}}}function Ma(C){let d,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=v},l(c){d=l(c,"P",{"data-svelte-h":!0}),m(d)!=="svelte-fincs2"&&(d.innerHTML=v)},m(c,k){r(c,d,k)},p:et,d(c){c&&o(d)}}}function wa(C){let d,v,c,k,x,b,F,Ut,ne,kn='LLaMA 모델은 Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample에 의해 제안된 <a href="https://arxiv.org/abs/2302.13971" rel="nofollow">LLaMA: Open and Efficient Foundation Language Models</a>에서 소개되었습니다. 이 모델은 7B에서 65B개의 파라미터까지 다양한 크기의 기초 언어 모델을 모아놓은 것입니다.',Ht,ae,vn="논문의 초록은 다음과 같습니다:",At,se,yn="<em>“LLaMA는 7B에서 65B개의 파라미터 수를 가진 기초 언어 모델의 모음입니다. 우리는 수조 개의 토큰으로 모델을 훈련시켰고, 공개적으로 이용 가능한 데이터셋만을 사용하여 최고 수준의 모델을 훈련시킬 수 있음을 보여줍니다. 특히, LLaMA-13B 모델은 대부분의 벤치마크에서 GPT-3 (175B)를 능가하며, LLaMA-65B는 최고 수준의 모델인 Chinchilla-70B와 PaLM-540B에 버금가는 성능을 보입니다. 우리는 모든 모델을 연구 커뮤니티에 공개합니다.”</em>",Et,re,Ln="팁:",St,ie,Tn='<li>LLaMA 모델의 가중치는 <a href="https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form" rel="nofollow">이 양식</a>을 작성하여 얻을 수 있습니다.</li> <li>가중치를 다운로드한 후에는 이를 <a href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py" rel="nofollow">변환 스크립트</a>를 사용하여 Hugging Face Transformers 형식으로 변환해야합니다. 변환 스크립트를 실행하려면 아래의 예시 명령어를 참고하세요:</li>',Nt,le,Jt,de,$n="<li>변환을 하였다면 모델과 토크나이저는 다음과 같이 로드할 수 있습니다:</li>",Gt,ce,Zt,me,Mn="스크립트를 실행하기 위해서는 모델을 float16 정밀도로 전부 로드할 수 있을 만큼의 충분한 CPU RAM이 필요합니다. (가장 큰 버전의 모델이 여러 체크포인트로 나뉘어 있더라도, 각 체크포인트는 모델의 각 가중치의 일부를 포함하고 있기 때문에 모든 체크포인트를 RAM에 로드해야 합니다) 65B 모델의 경우, 총 130GB의 RAM이 필요합니다.",Dt,pe,wn='<li>LLaMA 토크나이저는 <a href="https://github.com/google/sentencepiece" rel="nofollow">sentencepiece</a>를 기반으로 하는 BPE 모델입니다. sentencepiece의 특징 중 하나는 시퀀스를 디코딩할 때 첫 토큰이 단어의 시작이라면 (예를 들어 “Banana”), 토크나이저는 문자열 앞에 공백을 추가하지 않는다는 것입니다.</li>',Bt,ue,xn='이 모델은 <a href="https://huggingface.co/BlackSamorez" rel="nofollow">BlackSamorez</a>의 기여와 함께, <a href="https://huggingface.co/zphang" rel="nofollow">zphang</a>에 의해 제공되었습니다. Hugging Face에서의 구현 코드는 GPT-NeoX를 기반으로 하며 <a href="https://github.com/EleutherAI/gpt-neox" rel="nofollow">여기</a>에서 찾을 수 있고, 저자의 코드 원본은 <a href="https://github.com/facebookresearch/llama" rel="nofollow">여기</a>에서 확인할 수 있습니다.',Rt,he,zn="원래 LLaMA 모델을 기반으로 Meta AI에서 몇 가지 후속 작업을 발표했습니다:",Vt,fe,Cn='<li><strong>Llama2</strong>: Llama2는 구조적인 몇 가지 수정(Grouped Query Attention)을 통해 개선된 버전이며, 2조 개의 토큰으로 사전 훈련이 되어 있습니다. Llama2에 대한 자세한 내용은 <a href="llama2">이 문서</a>를 참고하세요.</li>',Xt,ge,Qt,_e,Fn="LLaMA를 시작하는 데 도움이 될 Hugging Face 및 커뮤니티(🌎로 표시)의 공식 자료 목록입니다. 여기에 자료를 제출하고 싶다면 Pull Request를 올려주세요! 추가할 자료는 기존의 자료와 중복되지 않고 새로운 내용을 보여주는 것이 좋습니다.",Yt,be,Ot,ke,qn='<li>LLaMA 모델을 텍스트 분류 작업에 적용하기 위한 프롬프트 튜닝 방법에 대한 <a href="https://colab.research.google.com/github/bigscience-workshop/petals/blob/main/examples/prompt-tuning-sst2.ipynb#scrollTo=f04ba4d2" rel="nofollow">노트북</a> 🌎</li>',Kt,ve,eo,ye,Pn='<li><a href="https://stackexchange.com/" rel="nofollow">Stack Exchange</a>에서 질문에 답하는 LLaMA를 훈련하는 방법을 위한 <a href="https://huggingface.co/blog/stackllama#stackllama-a-hands-on-guide-to-train-llama-with-rlhf" rel="nofollow">StackLLaMA: RLHF로 LLaMA를 훈련하는 실전 가이드</a> 🌎</li>',to,Le,jn="⚗️ 최적화",oo,Te,In='<li>제한된 메모리를 가진 GPU에서 xturing 라이브러리를 사용하여 LLaMA 모델을 미세 조정하는 방법에 대한 <a href="https://colab.research.google.com/drive/1SQUXq1AMZPSLD4mk3A3swUIc6Y2dclme?usp=sharing" rel="nofollow">노트북</a> 🌎</li>',no,$e,Wn="⚡️ 추론",ao,Me,Un='<li>🤗 PEFT 라이브러리의 PeftModel을 사용하여 LLaMA 모델을 실행하는 방법에 대한 <a href="https://colab.research.google.com/github/DominguesM/alpaca-lora-ptbr-7b/blob/main/notebooks/02%20-%20Evaluate.ipynb" rel="nofollow">노트북</a> 🌎</li> <li>LangChain을 사용하여 PEFT 어댑터 LLaMA 모델을 로드하는 방법에 대한 <a href="https://colab.research.google.com/drive/1l2GiSSPbajVyp2Nk3CFT4t3uH6-5TiBe?usp=sharing" rel="nofollow">노트북</a> 🌎</li>',so,we,Hn="🚀 배포",ro,xe,An='<li>🤗 PEFT 라이브러리와 사용자 친화적인 UI로 LLaMA 모델을 미세 조정하는 방법에 대한 <a href="https://colab.research.google.com/github/lxe/simple-llama-finetuner/blob/master/Simple_LLaMA_FineTuner.ipynb#scrollTo=3PM_DilAZD8T" rel="nofollow">노트북</a> 🌎</li> <li>Amazon SageMaker에서 텍스트 생성을 위해 Open-LLaMA 모델을 배포하는 방법에 대한 <a href="https://github.com/aws/amazon-sagemaker-examples/blob/main/introduction_to_amazon_algorithms/jumpstart-foundation-models/text-generation-open-llama.ipynb" rel="nofollow">노트북</a> 🌎</li>',io,ze,lo,I,Ce,wo,tt,En=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ko/model_doc/llama2#transformers.LlamaModel">LlamaModel</a>. It is used to instantiate an LLaMA
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the LLaMA-7B.`,xo,ot,Sn=`Configuration objects inherit from <code>PretrainedConfig</code> and can be used to control the model outputs. Read the
documentation from <code>PretrainedConfig</code> for more information.`,zo,D,co,Fe,mo,z,qe,Co,nt,Nn=`Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding. The default padding token is unset as there is
no padding token in the original model.`,Fo,at,Pe,qo,B,je,Po,st,Jn=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,jo,W,Ie,Io,rt,Gn="Creates a mask from the two sequences passed to be used in a sequence-pair classification task. An ALBERT",Wo,R,Uo,it,Zn="if token_ids_1 is None, only returns the first portion of the mask (0s).",Ho,V,We,Ao,lt,Dn="Save the vocabulary and special tokens file to a directory.",po,Ue,uo,y,He,Eo,dt,Bn="Construct a Llama tokenizer. Based on byte-level Byte-Pair-Encoding.",So,ct,Rn="This uses notably ByteFallback and no normalization.",No,X,Jo,mt,Vn=`If you want to change the <code>bos_token</code> or the <code>eos_token</code>, make sure to specify them when initializing the model, or
call <code>tokenizer.update_post_processor()</code> to make sure that the post-processing is correctly done (otherwise the
values of the first token and final token of an encoded sequence will not be correct). For more details, checkout
[post-processors] (<a href="https://huggingface.co/docs/tokenizers/api/post-processors" rel="nofollow">https://huggingface.co/docs/tokenizers/api/post-processors</a>) documentation.`,Go,pt,Xn=`This tokenizer inherits from <code>PreTrainedTokenizerFast</code> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`,Zo,ut,Ae,Do,Q,Ee,Bo,ht,Qn=`Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> or <code>encode_plus</code> methods.`,Ro,E,Se,Vo,ft,Yn=`Create the token type IDs corresponding to the sequences passed. <a href="../glossary#token-type-ids">What are token type
IDs?</a>`,Xo,gt,On="Should be overridden in a subclass if the model has a special way of building those.",Qo,Y,Ne,Yo,_t,Kn="Updates the underlying post processor with the current <code>bos_token</code> and <code>eos_token</code>.",Oo,bt,Je,ho,Ge,fo,q,Ze,Ko,kt,ea=`The bare LLaMA Model outputting raw hidden-states without any specific head on top.
This model inherits from <code>PreTrainedModel</code>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,en,vt,ta=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,tn,yt,oa="Transformer decoder consisting of <em>config.num_hidden_layers</em> layers. Each layer is a <code>LlamaDecoderLayer</code>",on,S,De,nn,Lt,na='The <a href="/docs/transformers/main/ko/model_doc/llama2#transformers.LlamaModel">LlamaModel</a> forward method, overrides the <code>__call__</code> special method.',an,O,go,Be,_o,G,Re,sn,U,Ve,rn,Tt,aa='The <a href="/docs/transformers/main/ko/model_doc/llama2#transformers.LlamaForCausalLM">LlamaForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',ln,K,dn,ee,bo,Xe,ko,M,Qe,cn,$t,sa="The LLaMa Model transformer with a sequence classification head on top (linear layer).",mn,Mt,ra=`<a href="/docs/transformers/main/ko/model_doc/llama2#transformers.LlamaForSequenceClassification">LlamaForSequenceClassification</a> uses the last token in order to do the classification, as other causal models
(e.g. GPT-2) do.`,pn,wt,ia=`Since it does classification on the last token, it requires to know the position of the last token. If a
<code>pad_token_id</code> is defined in the configuration, it finds the last token that is not a padding token in each row. If
no <code>pad_token_id</code> is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when <code>inputs_embeds</code> are passed instead of <code>input_ids</code>, it does the same (take the last value in
each row of the batch).`,un,xt,la=`This model inherits from <code>PreTrainedModel</code>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,hn,zt,da=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,fn,N,Ye,gn,Ct,ca='The <a href="/docs/transformers/main/ko/model_doc/llama2#transformers.LlamaForSequenceClassification">LlamaForSequenceClassification</a> forward method, overrides the <code>__call__</code> special method.',_n,te,vo,It,yo;return x=new Z({props:{title:"LLaMA",local:"llama",headingTag:"h1"}}),F=new Z({props:{title:"개요",local:"overview",headingTag:"h2"}}),le=new jt({props:{code:"cHl0aG9uJTIwc3JjJTJGdHJhbnNmb3JtZXJzJTJGbW9kZWxzJTJGbGxhbWElMkZjb252ZXJ0X2xsYW1hX3dlaWdodHNfdG9faGYucHklMjAlNUMlMEElMjAlMjAlMjAlMjAtLWlucHV0X2RpciUyMCUyRnBhdGglMkZ0byUyRmRvd25sb2FkZWQlMkZsbGFtYSUyRndlaWdodHMlMjAtLW1vZGVsX3NpemUlMjA3QiUyMC0tb3V0cHV0X2RpciUyMCUyRm91dHB1dCUyRnBhdGg=",highlighted:`python src/transformers/models/llama/convert_llama_weights_to_hf.py \\
    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path`,wrap:!1}}),ce=new jt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMExsYW1hRm9yQ2F1c2FsTE0lMkMlMjBMbGFtYVRva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMExsYW1hVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjIlMkZvdXRwdXQlMkZwYXRoJTIyKSUwQW1vZGVsJTIwJTNEJTIwTGxhbWFGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyJTJGb3V0cHV0JTJGcGF0aCUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlamaForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained(<span class="hljs-string">&quot;/output/path&quot;</span>)
model = LlamaForCausalLM.from_pretrained(<span class="hljs-string">&quot;/output/path&quot;</span>)`,wrap:!1}}),ge=new Z({props:{title:"리소스",local:"resources",headingTag:"h2"}}),be=new ua({props:{pipeline:"text-classification"}}),ve=new ua({props:{pipeline:"question-answering"}}),ze=new Z({props:{title:"LlamaConfig",local:"llamaconfig ][ transformers.LlamaConfig",headingTag:"h2"}}),Ce=new w({props:{name:"class transformers.LlamaConfig",anchor:"transformers.LlamaConfig",parameters:[{name:"vocab_size",val:" = 32000"},{name:"hidden_size",val:" = 4096"},{name:"intermediate_size",val:" = 11008"},{name:"num_hidden_layers",val:" = 32"},{name:"num_attention_heads",val:" = 32"},{name:"num_key_value_heads",val:" = None"},{name:"hidden_act",val:" = 'silu'"},{name:"max_position_embeddings",val:" = 2048"},{name:"initializer_range",val:" = 0.02"},{name:"rms_norm_eps",val:" = 1e-06"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = None"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"pretraining_tp",val:" = 1"},{name:"tie_word_embeddings",val:" = False"},{name:"rope_theta",val:" = 10000.0"},{name:"rope_scaling",val:" = None"},{name:"attention_bias",val:" = False"},{name:"attention_dropout",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LlamaConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ko/model_doc/llama2#transformers.LlamaModel">LlamaModel</a>`,name:"vocab_size"},{anchor:"transformers.LlamaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimension of the hidden representations.`,name:"hidden_size"},{anchor:"transformers.LlamaConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 11008) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.LlamaConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the Transformer decoder.`,name:"num_hidden_layers"},{anchor:"transformers.LlamaConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads for each attention layer in the Transformer decoder.`,name:"num_attention_heads"},{anchor:"transformers.LlamaConfig.num_key_value_heads",description:`<strong>num_key_value_heads</strong> (<code>int</code>, <em>optional</em>) &#x2014;
This is the number of key_value heads that should be used to implement Grouped Query Attention. If
<code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if
<code>num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group. For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to </code>num_attention_heads\`.`,name:"num_key_value_heads"},{anchor:"transformers.LlamaConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the decoder.`,name:"hidden_act"},{anchor:"transformers.LlamaConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,
Llama 2 up to 4096, CodeLlama up to 16384.`,name:"max_position_embeddings"},{anchor:"transformers.LlamaConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.LlamaConfig.rms_norm_eps",description:`<strong>rms_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the rms normalization layers.`,name:"rms_norm_eps"},{anchor:"transformers.LlamaConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.LlamaConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.LlamaConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.LlamaConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
End of stream token id.`,name:"eos_token_id"},{anchor:"transformers.LlamaConfig.pretraining_tp",description:`<strong>pretraining_tp</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Experimental feature. Tensor parallelism rank used during pretraining. Please refer to <a href="https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism" rel="nofollow">this
document</a> to understand more about it. This value is
necessary to ensure exact reproducibility of the pretraining results. Please refer to <a href="https://github.com/pytorch/pytorch/issues/76232" rel="nofollow">this
issue</a>.`,name:"pretraining_tp"},{anchor:"transformers.LlamaConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to tie weight embeddings`,name:"tie_word_embeddings"},{anchor:"transformers.LlamaConfig.rope_theta",description:`<strong>rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 10000.0) &#x2014;
The base period of the RoPE embeddings.`,name:"rope_theta"},{anchor:"transformers.LlamaConfig.rope_scaling",description:`<strong>rope_scaling</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling
strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is
<code>{&quot;type&quot;: strategy name, &quot;factor&quot;: scaling factor}</code>. When using this flag, don&#x2019;t update
<code>max_position_embeddings</code> to the expected new maximum. See the following thread for more information on how
these scaling strategies behave:
<a href="https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/" rel="nofollow">https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/</a>. This is an
experimental feature, subject to breaking API changes in future versions.`,name:"rope_scaling"},{anchor:"transformers.LlamaConfig.attention_bias",description:`<strong>attention_bias</strong> (<code>bool</code>, defaults to <code>False</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a bias in the query, key, value and output projection layers during self-attention.`,name:"attention_bias"},{anchor:"transformers.LlamaConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/configuration_llama.py#L31"}}),D=new Mo({props:{anchor:"transformers.LlamaConfig.example",$$slots:{default:[ka]},$$scope:{ctx:C}}}),Fe=new Z({props:{title:"LlamaTokenizer",local:"llamatokenizer ][ transformers.LlamaTokenizer",headingTag:"h2"}}),qe=new w({props:{name:"class transformers.LlamaTokenizer",anchor:"transformers.LlamaTokenizer",parameters:[{name:"vocab_file",val:""},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"pad_token",val:" = None"},{name:"sp_model_kwargs",val:": Optional = None"},{name:"add_bos_token",val:" = True"},{name:"add_eos_token",val:" = False"},{name:"clean_up_tokenization_spaces",val:" = False"},{name:"use_default_system_prompt",val:" = False"},{name:"spaces_between_special_tokens",val:" = False"},{name:"legacy",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LlamaTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.LlamaTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.LlamaTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.`,name:"bos_token"},{anchor:"transformers.LlamaTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.LlamaTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>) &#x2014;
A special token used to make arrays of tokens the same size for batching purpose. Will then be ignored by
attention mechanisms or loss computation.`,name:"pad_token"},{anchor:"transformers.LlamaTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>Dict[str, Any]</code>, <code>Optional</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>SentencePieceProcessor.__init__()</code> method. The <a href="https://github.com/google/sentencepiece/tree/master/python" rel="nofollow">Python wrapper for
SentencePiece</a> can be used, among other things,
to set:</p>
<ul>
<li>
<p><code>enable_sampling</code>: Enable subword regularization.</p>
</li>
<li>
<p><code>nbest_size</code>: Sampling parameters for unigram. Invalid for BPE-Dropout.</p>
<ul>
<li><code>nbest_size = {0,1}</code>: No sampling is performed.</li>
<li><code>nbest_size &gt; 1</code>: samples from the nbest_size results.</li>
<li><code>nbest_size &lt; 0</code>: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
using forward-filtering-and-backward-sampling algorithm.</li>
</ul>
</li>
<li>
<p><code>alpha</code>: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
BPE-dropout.</p>
</li>
</ul>`,name:"sp_model_kwargs"},{anchor:"transformers.LlamaTokenizer.add_bos_token",description:`<strong>add_bos_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to add an <code>bos_token</code> at the start of sequences.`,name:"add_bos_token"},{anchor:"transformers.LlamaTokenizer.add_eos_token",description:`<strong>add_eos_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add an <code>eos_token</code> at the end of sequences.`,name:"add_eos_token"},{anchor:"transformers.LlamaTokenizer.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like
extra spaces.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.LlamaTokenizer.use_default_system_prompt",description:`<strong>use_default_system_prompt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the default system prompt for Llama should be used.`,name:"use_default_system_prompt"},{anchor:"transformers.LlamaTokenizer.spaces_between_special_tokens",description:`<strong>spaces_between_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add spaces between special tokens.`,name:"spaces_between_special_tokens"},{anchor:"transformers.LlamaTokenizer.legacy",description:`<strong>legacy</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not the <code>legacy</code> behavior of the tokenizer should be used. Legacy is before the merge of #24622
and #25224 which includes fixes to properly handle tokens that appear after special tokens. A simple
example:</p>
<ul>
<li><code>legacy=True</code>:</li>
</ul>`,name:"legacy"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L66"}}),Pe=new w({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.LlamaTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:""},{name:"token_ids_1",val:" = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L333"}}),je=new w({props:{name:"get_special_tokens_mask",anchor:"transformers.LlamaTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.LlamaTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.LlamaTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.LlamaTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L344",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),Ie=new w({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.LlamaTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of ids.`,name:"token_ids_0"},{anchor:"transformers.LlamaTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L381",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),R=new Mo({props:{anchor:"transformers.LlamaTokenizer.create_token_type_ids_from_sequences.example",$$slots:{default:[va]},$$scope:{ctx:C}}}),We=new w({props:{name:"save_vocabulary",anchor:"transformers.LlamaTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:""},{name:"filename_prefix",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaTokenizer.save_vocabulary.save_directory",description:`<strong>save_directory</strong> (<code>str</code>) &#x2014;
The directory in which to save the vocabulary.`,name:"save_directory"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama.py#L306",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Paths to the files saved.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Tuple(str)</code></p>
`}}),Ue=new Z({props:{title:"LlamaTokenizerFast",local:"llamatokenizerfast ][ transformers.LlamaTokenizerFast",headingTag:"h2"}}),He=new w({props:{name:"class transformers.LlamaTokenizerFast",anchor:"transformers.LlamaTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"clean_up_tokenization_spaces",val:" = False"},{name:"unk_token",val:" = '<unk>'"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"add_bos_token",val:" = True"},{name:"add_eos_token",val:" = False"},{name:"use_default_system_prompt",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LlamaTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a .model extension) that
contains the vocabulary necessary to instantiate a tokenizer.`,name:"vocab_file"},{anchor:"transformers.LlamaTokenizerFast.tokenizer_file",description:`<strong>tokenizer_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
<a href="https://github.com/huggingface/tokenizers" rel="nofollow">tokenizers</a> file (generally has a .json extension) that
contains everything needed to load the tokenizer.`,name:"tokenizer_file"},{anchor:"transformers.LlamaTokenizerFast.clean_up_tokenization_spaces",description:`<strong>clean_up_tokenization_spaces</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to cleanup spaces after decoding, cleanup consists in removing potential artifacts like
extra spaces.`,name:"clean_up_tokenization_spaces"},{anchor:"transformers.LlamaTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.LlamaTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.`,name:"bos_token"},{anchor:"transformers.LlamaTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code> or <code>tokenizers.AddedToken</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.LlamaTokenizerFast.add_bos_token",description:`<strong>add_bos_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to add an <code>bos_token</code> at the start of sequences.`,name:"add_bos_token"},{anchor:"transformers.LlamaTokenizerFast.add_eos_token",description:`<strong>add_eos_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add an <code>eos_token</code> at the end of sequences.`,name:"add_eos_token"},{anchor:"transformers.LlamaTokenizerFast.use_default_system_prompt",description:`<strong>use_default_system_prompt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the default system prompt for Llama should be used.`,name:"use_default_system_prompt"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama_fast.py#L57"}}),X=new Mo({props:{anchor:"transformers.LlamaTokenizerFast.example",$$slots:{default:[ya]},$$scope:{ctx:C}}}),Ae=new w({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.LlamaTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:""},{name:"token_ids_1",val:" = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama_fast.py#L272"}}),Ee=new w({props:{name:"get_special_tokens_mask",anchor:"transformers.LlamaTokenizerFast.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.LlamaTokenizerFast.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of ids of the first sequence.`,name:"token_ids_0"},{anchor:"transformers.LlamaTokenizerFast.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
List of ids of the second sequence.`,name:"token_ids_1"},{anchor:"transformers.LlamaTokenizerFast.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/tokenization_utils_base.py#L3798",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]</p>
`}}),Se=new w({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.LlamaTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:"<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014; The first tokenized sequence.",name:"token_ids_0"},{anchor:"transformers.LlamaTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:"<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014; The second tokenized sequence.",name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/tokenization_utils_base.py#L3328",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The token type ids.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),Ne=new w({props:{name:"update_post_processor",anchor:"transformers.LlamaTokenizerFast.update_post_processor",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama_fast.py#L146"}}),Je=new w({props:{name:"save_vocabulary",anchor:"transformers.LlamaTokenizerFast.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/tokenization_llama_fast.py#L190"}}),Ge=new Z({props:{title:"LlamaModel",local:"llamamodel ][ transformers.LlamaModel",headingTag:"h2"}}),Ze=new w({props:{name:"class transformers.LlamaModel",anchor:"transformers.LlamaModel",parameters:[{name:"config",val:": LlamaConfig"}],parametersDescription:[{anchor:"transformers.LlamaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ko/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<code>from_pretrained()</code> method to load the model weights.
config &#x2014; LlamaConfig`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L879"}}),De=new w({props:{name:"forward",anchor:"transformers.LlamaModel.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"},{name:"cache_position",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <code>AutoTokenizer</code>. See <code>PreTrainedTokenizer.encode()</code> and
<code>PreTrainedTokenizer.__call__()</code> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.LlamaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <code>AutoTokenizer</code>. See <code>PreTrainedTokenizer.encode()</code> and
<code>PreTrainedTokenizer.__call__()</code> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.LlamaModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.LlamaModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <code>Cache</code> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.LlamaModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.LlamaModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.LlamaModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.LlamaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.LlamaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <code>ModelOutput</code> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L915"}}),O=new bn({props:{$$slots:{default:[La]},$$scope:{ctx:C}}}),Be=new Z({props:{title:"LlamaForCausalLM",local:"llamaforcausallm ][ transformers.LlamaForCausalLM",headingTag:"h2"}}),Re=new w({props:{name:"class transformers.LlamaForCausalLM",anchor:"transformers.LlamaForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1070"}}),Ve=new w({props:{name:"forward",anchor:"transformers.LlamaForCausalLM.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"},{name:"cache_position",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <code>AutoTokenizer</code>. See <code>PreTrainedTokenizer.encode()</code> and
<code>PreTrainedTokenizer.__call__()</code> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.LlamaForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <code>AutoTokenizer</code>. See <code>PreTrainedTokenizer.encode()</code> and
<code>PreTrainedTokenizer.__call__()</code> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.LlamaForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.LlamaForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <code>Cache</code> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.LlamaForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.LlamaForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.LlamaForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.LlamaForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.LlamaForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <code>ModelOutput</code> instead of a plain tuple.</p>
<p>Args &#x2014;
labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1100",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.CausalLMOutputWithPast</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ko/model_doc/llama2#transformers.LlamaConfig"
>LlamaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.CausalLMOutputWithPast</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),K=new bn({props:{$$slots:{default:[Ta]},$$scope:{ctx:C}}}),ee=new Mo({props:{anchor:"transformers.LlamaForCausalLM.forward.example",$$slots:{default:[$a]},$$scope:{ctx:C}}}),Xe=new Z({props:{title:"LlamaForSequenceClassification",local:"llamaforsequenceclassification ][ transformers.LlamaForSequenceClassification",headingTag:"h2"}}),Qe=new w({props:{name:"class transformers.LlamaForSequenceClassification",anchor:"transformers.LlamaForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.LlamaForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ko/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<code>from_pretrained()</code> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1277"}}),Ye=new w({props:{name:"forward",anchor:"transformers.LlamaForSequenceClassification.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlamaForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <code>AutoTokenizer</code>. See <code>PreTrainedTokenizer.encode()</code> and
<code>PreTrainedTokenizer.__call__()</code> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.LlamaForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <code>AutoTokenizer</code>. See <code>PreTrainedTokenizer.encode()</code> and
<code>PreTrainedTokenizer.__call__()</code> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.LlamaForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.LlamaForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <code>Cache</code> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.LlamaForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.LlamaForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.LlamaForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.LlamaForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.LlamaForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <code>ModelOutput</code> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.LlamaForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L1308"}}),te=new bn({props:{$$slots:{default:[Ma]},$$scope:{ctx:C}}}),{c(){d=i("meta"),v=n(),c=i("p"),k=n(),p(x.$$.fragment),b=n(),p(F.$$.fragment),Ut=n(),ne=i("p"),ne.innerHTML=kn,Ht=n(),ae=i("p"),ae.textContent=vn,At=n(),se=i("p"),se.innerHTML=yn,Et=n(),re=i("p"),re.textContent=Ln,St=n(),ie=i("ul"),ie.innerHTML=Tn,Nt=n(),p(le.$$.fragment),Jt=n(),de=i("ul"),de.innerHTML=$n,Gt=n(),p(ce.$$.fragment),Zt=n(),me=i("p"),me.textContent=Mn,Dt=n(),pe=i("ul"),pe.innerHTML=wn,Bt=n(),ue=i("p"),ue.innerHTML=xn,Rt=n(),he=i("p"),he.textContent=zn,Vt=n(),fe=i("ul"),fe.innerHTML=Cn,Xt=n(),p(ge.$$.fragment),Qt=n(),_e=i("p"),_e.textContent=Fn,Yt=n(),p(be.$$.fragment),Ot=n(),ke=i("ul"),ke.innerHTML=qn,Kt=n(),p(ve.$$.fragment),eo=n(),ye=i("ul"),ye.innerHTML=Pn,to=n(),Le=i("p"),Le.textContent=jn,oo=n(),Te=i("ul"),Te.innerHTML=In,no=n(),$e=i("p"),$e.textContent=Wn,ao=n(),Me=i("ul"),Me.innerHTML=Un,so=n(),we=i("p"),we.textContent=Hn,ro=n(),xe=i("ul"),xe.innerHTML=An,io=n(),p(ze.$$.fragment),lo=n(),I=i("div"),p(Ce.$$.fragment),wo=n(),tt=i("p"),tt.innerHTML=En,xo=n(),ot=i("p"),ot.innerHTML=Sn,zo=n(),p(D.$$.fragment),co=n(),p(Fe.$$.fragment),mo=n(),z=i("div"),p(qe.$$.fragment),Co=n(),nt=i("p"),nt.textContent=Nn,Fo=n(),at=i("div"),p(Pe.$$.fragment),qo=n(),B=i("div"),p(je.$$.fragment),Po=n(),st=i("p"),st.innerHTML=Jn,jo=n(),W=i("div"),p(Ie.$$.fragment),Io=n(),rt=i("p"),rt.textContent=Gn,Wo=n(),p(R.$$.fragment),Uo=n(),it=i("p"),it.textContent=Zn,Ho=n(),V=i("div"),p(We.$$.fragment),Ao=n(),lt=i("p"),lt.textContent=Dn,po=n(),p(Ue.$$.fragment),uo=n(),y=i("div"),p(He.$$.fragment),Eo=n(),dt=i("p"),dt.textContent=Bn,So=n(),ct=i("p"),ct.textContent=Rn,No=n(),p(X.$$.fragment),Jo=n(),mt=i("p"),mt.innerHTML=Vn,Go=n(),pt=i("p"),pt.innerHTML=Xn,Zo=n(),ut=i("div"),p(Ae.$$.fragment),Do=n(),Q=i("div"),p(Ee.$$.fragment),Bo=n(),ht=i("p"),ht.innerHTML=Qn,Ro=n(),E=i("div"),p(Se.$$.fragment),Vo=n(),ft=i("p"),ft.innerHTML=Yn,Xo=n(),gt=i("p"),gt.textContent=On,Qo=n(),Y=i("div"),p(Ne.$$.fragment),Yo=n(),_t=i("p"),_t.innerHTML=Kn,Oo=n(),bt=i("div"),p(Je.$$.fragment),ho=n(),p(Ge.$$.fragment),fo=n(),q=i("div"),p(Ze.$$.fragment),Ko=n(),kt=i("p"),kt.innerHTML=ea,en=n(),vt=i("p"),vt.innerHTML=ta,tn=n(),yt=i("p"),yt.innerHTML=oa,on=n(),S=i("div"),p(De.$$.fragment),nn=n(),Lt=i("p"),Lt.innerHTML=na,an=n(),p(O.$$.fragment),go=n(),p(Be.$$.fragment),_o=n(),G=i("div"),p(Re.$$.fragment),sn=n(),U=i("div"),p(Ve.$$.fragment),rn=n(),Tt=i("p"),Tt.innerHTML=aa,ln=n(),p(K.$$.fragment),dn=n(),p(ee.$$.fragment),bo=n(),p(Xe.$$.fragment),ko=n(),M=i("div"),p(Qe.$$.fragment),cn=n(),$t=i("p"),$t.textContent=sa,mn=n(),Mt=i("p"),Mt.innerHTML=ra,pn=n(),wt=i("p"),wt.innerHTML=ia,un=n(),xt=i("p"),xt.innerHTML=la,hn=n(),zt=i("p"),zt.innerHTML=da,fn=n(),N=i("div"),p(Ye.$$.fragment),gn=n(),Ct=i("p"),Ct.innerHTML=ca,_n=n(),p(te.$$.fragment),vo=n(),It=i("p"),this.h()},l(e){const t=ba("svelte-u9bgzb",document.head);d=l(t,"META",{name:!0,content:!0}),t.forEach(o),v=a(e),c=l(e,"P",{}),T(c).forEach(o),k=a(e),u(x.$$.fragment,e),b=a(e),u(F.$$.fragment,e),Ut=a(e),ne=l(e,"P",{"data-svelte-h":!0}),m(ne)!=="svelte-a353de"&&(ne.innerHTML=kn),Ht=a(e),ae=l(e,"P",{"data-svelte-h":!0}),m(ae)!=="svelte-e5r8wp"&&(ae.textContent=vn),At=a(e),se=l(e,"P",{"data-svelte-h":!0}),m(se)!=="svelte-1w8ji9d"&&(se.innerHTML=yn),Et=a(e),re=l(e,"P",{"data-svelte-h":!0}),m(re)!=="svelte-k6v9m1"&&(re.textContent=Ln),St=a(e),ie=l(e,"UL",{"data-svelte-h":!0}),m(ie)!=="svelte-qgv262"&&(ie.innerHTML=Tn),Nt=a(e),u(le.$$.fragment,e),Jt=a(e),de=l(e,"UL",{"data-svelte-h":!0}),m(de)!=="svelte-1m2lh35"&&(de.innerHTML=$n),Gt=a(e),u(ce.$$.fragment,e),Zt=a(e),me=l(e,"P",{"data-svelte-h":!0}),m(me)!=="svelte-uib2dz"&&(me.textContent=Mn),Dt=a(e),pe=l(e,"UL",{"data-svelte-h":!0}),m(pe)!=="svelte-9ksrjp"&&(pe.innerHTML=wn),Bt=a(e),ue=l(e,"P",{"data-svelte-h":!0}),m(ue)!=="svelte-wjmudq"&&(ue.innerHTML=xn),Rt=a(e),he=l(e,"P",{"data-svelte-h":!0}),m(he)!=="svelte-f24v5"&&(he.textContent=zn),Vt=a(e),fe=l(e,"UL",{"data-svelte-h":!0}),m(fe)!=="svelte-1u3qfca"&&(fe.innerHTML=Cn),Xt=a(e),u(ge.$$.fragment,e),Qt=a(e),_e=l(e,"P",{"data-svelte-h":!0}),m(_e)!=="svelte-12agt6k"&&(_e.textContent=Fn),Yt=a(e),u(be.$$.fragment,e),Ot=a(e),ke=l(e,"UL",{"data-svelte-h":!0}),m(ke)!=="svelte-1hrgzzb"&&(ke.innerHTML=qn),Kt=a(e),u(ve.$$.fragment,e),eo=a(e),ye=l(e,"UL",{"data-svelte-h":!0}),m(ye)!=="svelte-1nqtlhx"&&(ye.innerHTML=Pn),to=a(e),Le=l(e,"P",{"data-svelte-h":!0}),m(Le)!=="svelte-14a1znp"&&(Le.textContent=jn),oo=a(e),Te=l(e,"UL",{"data-svelte-h":!0}),m(Te)!=="svelte-8brual"&&(Te.innerHTML=In),no=a(e),$e=l(e,"P",{"data-svelte-h":!0}),m($e)!=="svelte-1x58uo"&&($e.textContent=Wn),ao=a(e),Me=l(e,"UL",{"data-svelte-h":!0}),m(Me)!=="svelte-1dxsod4"&&(Me.innerHTML=Un),so=a(e),we=l(e,"P",{"data-svelte-h":!0}),m(we)!=="svelte-3z2x4b"&&(we.textContent=Hn),ro=a(e),xe=l(e,"UL",{"data-svelte-h":!0}),m(xe)!=="svelte-fjx670"&&(xe.innerHTML=An),io=a(e),u(ze.$$.fragment,e),lo=a(e),I=l(e,"DIV",{class:!0});var H=T(I);u(Ce.$$.fragment,H),wo=a(H),tt=l(H,"P",{"data-svelte-h":!0}),m(tt)!=="svelte-e7ysf1"&&(tt.innerHTML=En),xo=a(H),ot=l(H,"P",{"data-svelte-h":!0}),m(ot)!=="svelte-huu8ef"&&(ot.innerHTML=Sn),zo=a(H),u(D.$$.fragment,H),H.forEach(o),co=a(e),u(Fe.$$.fragment,e),mo=a(e),z=l(e,"DIV",{class:!0});var P=T(z);u(qe.$$.fragment,P),Co=a(P),nt=l(P,"P",{"data-svelte-h":!0}),m(nt)!=="svelte-qfiu5a"&&(nt.textContent=Nn),Fo=a(P),at=l(P,"DIV",{class:!0});var Wt=T(at);u(Pe.$$.fragment,Wt),Wt.forEach(o),qo=a(P),B=l(P,"DIV",{class:!0});var Oe=T(B);u(je.$$.fragment,Oe),Po=a(Oe),st=l(Oe,"P",{"data-svelte-h":!0}),m(st)!=="svelte-1f4f5kp"&&(st.innerHTML=Jn),Oe.forEach(o),jo=a(P),W=l(P,"DIV",{class:!0});var A=T(W);u(Ie.$$.fragment,A),Io=a(A),rt=l(A,"P",{"data-svelte-h":!0}),m(rt)!=="svelte-13bfd60"&&(rt.textContent=Gn),Wo=a(A),u(R.$$.fragment,A),Uo=a(A),it=l(A,"P",{"data-svelte-h":!0}),m(it)!=="svelte-wtrslu"&&(it.textContent=Zn),A.forEach(o),Ho=a(P),V=l(P,"DIV",{class:!0});var Ke=T(V);u(We.$$.fragment,Ke),Ao=a(Ke),lt=l(Ke,"P",{"data-svelte-h":!0}),m(lt)!=="svelte-1slb66l"&&(lt.textContent=Dn),Ke.forEach(o),P.forEach(o),po=a(e),u(Ue.$$.fragment,e),uo=a(e),y=l(e,"DIV",{class:!0});var L=T(y);u(He.$$.fragment,L),Eo=a(L),dt=l(L,"P",{"data-svelte-h":!0}),m(dt)!=="svelte-15tdcz8"&&(dt.textContent=Bn),So=a(L),ct=l(L,"P",{"data-svelte-h":!0}),m(ct)!=="svelte-llhmpa"&&(ct.textContent=Rn),No=a(L),u(X.$$.fragment,L),Jo=a(L),mt=l(L,"P",{"data-svelte-h":!0}),m(mt)!=="svelte-cnb6q1"&&(mt.innerHTML=Vn),Go=a(L),pt=l(L,"P",{"data-svelte-h":!0}),m(pt)!=="svelte-1ndfe3e"&&(pt.innerHTML=Xn),Zo=a(L),ut=l(L,"DIV",{class:!0});var ma=T(ut);u(Ae.$$.fragment,ma),ma.forEach(o),Do=a(L),Q=l(L,"DIV",{class:!0});var Lo=T(Q);u(Ee.$$.fragment,Lo),Bo=a(Lo),ht=l(Lo,"P",{"data-svelte-h":!0}),m(ht)!=="svelte-1wmjg8a"&&(ht.innerHTML=Qn),Lo.forEach(o),Ro=a(L),E=l(L,"DIV",{class:!0});var Ft=T(E);u(Se.$$.fragment,Ft),Vo=a(Ft),ft=l(Ft,"P",{"data-svelte-h":!0}),m(ft)!=="svelte-zj1vf1"&&(ft.innerHTML=Yn),Xo=a(Ft),gt=l(Ft,"P",{"data-svelte-h":!0}),m(gt)!=="svelte-9vptpw"&&(gt.textContent=On),Ft.forEach(o),Qo=a(L),Y=l(L,"DIV",{class:!0});var To=T(Y);u(Ne.$$.fragment,To),Yo=a(To),_t=l(To,"P",{"data-svelte-h":!0}),m(_t)!=="svelte-nfci2w"&&(_t.innerHTML=Kn),To.forEach(o),Oo=a(L),bt=l(L,"DIV",{class:!0});var pa=T(bt);u(Je.$$.fragment,pa),pa.forEach(o),L.forEach(o),ho=a(e),u(Ge.$$.fragment,e),fo=a(e),q=l(e,"DIV",{class:!0});var J=T(q);u(Ze.$$.fragment,J),Ko=a(J),kt=l(J,"P",{"data-svelte-h":!0}),m(kt)!=="svelte-16gi54p"&&(kt.innerHTML=ea),en=a(J),vt=l(J,"P",{"data-svelte-h":!0}),m(vt)!=="svelte-hswkmf"&&(vt.innerHTML=ta),tn=a(J),yt=l(J,"P",{"data-svelte-h":!0}),m(yt)!=="svelte-eom0yk"&&(yt.innerHTML=oa),on=a(J),S=l(J,"DIV",{class:!0});var qt=T(S);u(De.$$.fragment,qt),nn=a(qt),Lt=l(qt,"P",{"data-svelte-h":!0}),m(Lt)!=="svelte-1hj4k2x"&&(Lt.innerHTML=na),an=a(qt),u(O.$$.fragment,qt),qt.forEach(o),J.forEach(o),go=a(e),u(Be.$$.fragment,e),_o=a(e),G=l(e,"DIV",{class:!0});var $o=T(G);u(Re.$$.fragment,$o),sn=a($o),U=l($o,"DIV",{class:!0});var oe=T(U);u(Ve.$$.fragment,oe),rn=a(oe),Tt=l(oe,"P",{"data-svelte-h":!0}),m(Tt)!=="svelte-cwjvpl"&&(Tt.innerHTML=aa),ln=a(oe),u(K.$$.fragment,oe),dn=a(oe),u(ee.$$.fragment,oe),oe.forEach(o),$o.forEach(o),bo=a(e),u(Xe.$$.fragment,e),ko=a(e),M=l(e,"DIV",{class:!0});var j=T(M);u(Qe.$$.fragment,j),cn=a(j),$t=l(j,"P",{"data-svelte-h":!0}),m($t)!=="svelte-62must"&&($t.textContent=sa),mn=a(j),Mt=l(j,"P",{"data-svelte-h":!0}),m(Mt)!=="svelte-1v4exue"&&(Mt.innerHTML=ra),pn=a(j),wt=l(j,"P",{"data-svelte-h":!0}),m(wt)!=="svelte-10ugs3m"&&(wt.innerHTML=ia),un=a(j),xt=l(j,"P",{"data-svelte-h":!0}),m(xt)!=="svelte-ehy44e"&&(xt.innerHTML=la),hn=a(j),zt=l(j,"P",{"data-svelte-h":!0}),m(zt)!=="svelte-hswkmf"&&(zt.innerHTML=da),fn=a(j),N=l(j,"DIV",{class:!0});var Pt=T(N);u(Ye.$$.fragment,Pt),gn=a(Pt),Ct=l(Pt,"P",{"data-svelte-h":!0}),m(Ct)!=="svelte-tc4cob"&&(Ct.innerHTML=ca),_n=a(Pt),u(te.$$.fragment,Pt),Pt.forEach(o),j.forEach(o),vo=a(e),It=l(e,"P",{}),T(It).forEach(o),this.h()},h(){$(d,"name","hf:doc:metadata"),$(d,"content",xa),$(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(at,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){s(document.head,d),r(e,v,t),r(e,c,t),r(e,k,t),h(x,e,t),r(e,b,t),h(F,e,t),r(e,Ut,t),r(e,ne,t),r(e,Ht,t),r(e,ae,t),r(e,At,t),r(e,se,t),r(e,Et,t),r(e,re,t),r(e,St,t),r(e,ie,t),r(e,Nt,t),h(le,e,t),r(e,Jt,t),r(e,de,t),r(e,Gt,t),h(ce,e,t),r(e,Zt,t),r(e,me,t),r(e,Dt,t),r(e,pe,t),r(e,Bt,t),r(e,ue,t),r(e,Rt,t),r(e,he,t),r(e,Vt,t),r(e,fe,t),r(e,Xt,t),h(ge,e,t),r(e,Qt,t),r(e,_e,t),r(e,Yt,t),h(be,e,t),r(e,Ot,t),r(e,ke,t),r(e,Kt,t),h(ve,e,t),r(e,eo,t),r(e,ye,t),r(e,to,t),r(e,Le,t),r(e,oo,t),r(e,Te,t),r(e,no,t),r(e,$e,t),r(e,ao,t),r(e,Me,t),r(e,so,t),r(e,we,t),r(e,ro,t),r(e,xe,t),r(e,io,t),h(ze,e,t),r(e,lo,t),r(e,I,t),h(Ce,I,null),s(I,wo),s(I,tt),s(I,xo),s(I,ot),s(I,zo),h(D,I,null),r(e,co,t),h(Fe,e,t),r(e,mo,t),r(e,z,t),h(qe,z,null),s(z,Co),s(z,nt),s(z,Fo),s(z,at),h(Pe,at,null),s(z,qo),s(z,B),h(je,B,null),s(B,Po),s(B,st),s(z,jo),s(z,W),h(Ie,W,null),s(W,Io),s(W,rt),s(W,Wo),h(R,W,null),s(W,Uo),s(W,it),s(z,Ho),s(z,V),h(We,V,null),s(V,Ao),s(V,lt),r(e,po,t),h(Ue,e,t),r(e,uo,t),r(e,y,t),h(He,y,null),s(y,Eo),s(y,dt),s(y,So),s(y,ct),s(y,No),h(X,y,null),s(y,Jo),s(y,mt),s(y,Go),s(y,pt),s(y,Zo),s(y,ut),h(Ae,ut,null),s(y,Do),s(y,Q),h(Ee,Q,null),s(Q,Bo),s(Q,ht),s(y,Ro),s(y,E),h(Se,E,null),s(E,Vo),s(E,ft),s(E,Xo),s(E,gt),s(y,Qo),s(y,Y),h(Ne,Y,null),s(Y,Yo),s(Y,_t),s(y,Oo),s(y,bt),h(Je,bt,null),r(e,ho,t),h(Ge,e,t),r(e,fo,t),r(e,q,t),h(Ze,q,null),s(q,Ko),s(q,kt),s(q,en),s(q,vt),s(q,tn),s(q,yt),s(q,on),s(q,S),h(De,S,null),s(S,nn),s(S,Lt),s(S,an),h(O,S,null),r(e,go,t),h(Be,e,t),r(e,_o,t),r(e,G,t),h(Re,G,null),s(G,sn),s(G,U),h(Ve,U,null),s(U,rn),s(U,Tt),s(U,ln),h(K,U,null),s(U,dn),h(ee,U,null),r(e,bo,t),h(Xe,e,t),r(e,ko,t),r(e,M,t),h(Qe,M,null),s(M,cn),s(M,$t),s(M,mn),s(M,Mt),s(M,pn),s(M,wt),s(M,un),s(M,xt),s(M,hn),s(M,zt),s(M,fn),s(M,N),h(Ye,N,null),s(N,gn),s(N,Ct),s(N,_n),h(te,N,null),r(e,vo,t),r(e,It,t),yo=!0},p(e,[t]){const H={};t&2&&(H.$$scope={dirty:t,ctx:e}),D.$set(H);const P={};t&2&&(P.$$scope={dirty:t,ctx:e}),R.$set(P);const Wt={};t&2&&(Wt.$$scope={dirty:t,ctx:e}),X.$set(Wt);const Oe={};t&2&&(Oe.$$scope={dirty:t,ctx:e}),O.$set(Oe);const A={};t&2&&(A.$$scope={dirty:t,ctx:e}),K.$set(A);const Ke={};t&2&&(Ke.$$scope={dirty:t,ctx:e}),ee.$set(Ke);const L={};t&2&&(L.$$scope={dirty:t,ctx:e}),te.$set(L)},i(e){yo||(f(x.$$.fragment,e),f(F.$$.fragment,e),f(le.$$.fragment,e),f(ce.$$.fragment,e),f(ge.$$.fragment,e),f(be.$$.fragment,e),f(ve.$$.fragment,e),f(ze.$$.fragment,e),f(Ce.$$.fragment,e),f(D.$$.fragment,e),f(Fe.$$.fragment,e),f(qe.$$.fragment,e),f(Pe.$$.fragment,e),f(je.$$.fragment,e),f(Ie.$$.fragment,e),f(R.$$.fragment,e),f(We.$$.fragment,e),f(Ue.$$.fragment,e),f(He.$$.fragment,e),f(X.$$.fragment,e),f(Ae.$$.fragment,e),f(Ee.$$.fragment,e),f(Se.$$.fragment,e),f(Ne.$$.fragment,e),f(Je.$$.fragment,e),f(Ge.$$.fragment,e),f(Ze.$$.fragment,e),f(De.$$.fragment,e),f(O.$$.fragment,e),f(Be.$$.fragment,e),f(Re.$$.fragment,e),f(Ve.$$.fragment,e),f(K.$$.fragment,e),f(ee.$$.fragment,e),f(Xe.$$.fragment,e),f(Qe.$$.fragment,e),f(Ye.$$.fragment,e),f(te.$$.fragment,e),yo=!0)},o(e){g(x.$$.fragment,e),g(F.$$.fragment,e),g(le.$$.fragment,e),g(ce.$$.fragment,e),g(ge.$$.fragment,e),g(be.$$.fragment,e),g(ve.$$.fragment,e),g(ze.$$.fragment,e),g(Ce.$$.fragment,e),g(D.$$.fragment,e),g(Fe.$$.fragment,e),g(qe.$$.fragment,e),g(Pe.$$.fragment,e),g(je.$$.fragment,e),g(Ie.$$.fragment,e),g(R.$$.fragment,e),g(We.$$.fragment,e),g(Ue.$$.fragment,e),g(He.$$.fragment,e),g(X.$$.fragment,e),g(Ae.$$.fragment,e),g(Ee.$$.fragment,e),g(Se.$$.fragment,e),g(Ne.$$.fragment,e),g(Je.$$.fragment,e),g(Ge.$$.fragment,e),g(Ze.$$.fragment,e),g(De.$$.fragment,e),g(O.$$.fragment,e),g(Be.$$.fragment,e),g(Re.$$.fragment,e),g(Ve.$$.fragment,e),g(K.$$.fragment,e),g(ee.$$.fragment,e),g(Xe.$$.fragment,e),g(Qe.$$.fragment,e),g(Ye.$$.fragment,e),g(te.$$.fragment,e),yo=!1},d(e){e&&(o(v),o(c),o(k),o(b),o(Ut),o(ne),o(Ht),o(ae),o(At),o(se),o(Et),o(re),o(St),o(ie),o(Nt),o(Jt),o(de),o(Gt),o(Zt),o(me),o(Dt),o(pe),o(Bt),o(ue),o(Rt),o(he),o(Vt),o(fe),o(Xt),o(Qt),o(_e),o(Yt),o(Ot),o(ke),o(Kt),o(eo),o(ye),o(to),o(Le),o(oo),o(Te),o(no),o($e),o(ao),o(Me),o(so),o(we),o(ro),o(xe),o(io),o(lo),o(I),o(co),o(mo),o(z),o(po),o(uo),o(y),o(ho),o(fo),o(q),o(go),o(_o),o(G),o(bo),o(ko),o(M),o(vo),o(It)),o(d),_(x,e),_(F,e),_(le,e),_(ce,e),_(ge,e),_(be,e),_(ve,e),_(ze,e),_(Ce),_(D),_(Fe,e),_(qe),_(Pe),_(je),_(Ie),_(R),_(We),_(Ue,e),_(He),_(X),_(Ae),_(Ee),_(Se),_(Ne),_(Je),_(Ge,e),_(Ze),_(De),_(O),_(Be,e),_(Re),_(Ve),_(K),_(ee),_(Xe,e),_(Qe),_(Ye),_(te)}}}const xa='{"title":"LLaMA","local":"llama","sections":[{"title":"개요","local":"overview","sections":[],"depth":2},{"title":"리소스","local":"resources","sections":[],"depth":2},{"title":"LlamaConfig","local":"llamaconfig ][ transformers.LlamaConfig","sections":[],"depth":2},{"title":"LlamaTokenizer","local":"llamatokenizer ][ transformers.LlamaTokenizer","sections":[],"depth":2},{"title":"LlamaTokenizerFast","local":"llamatokenizerfast ][ transformers.LlamaTokenizerFast","sections":[],"depth":2},{"title":"LlamaModel","local":"llamamodel ][ transformers.LlamaModel","sections":[],"depth":2},{"title":"LlamaForCausalLM","local":"llamaforcausallm ][ transformers.LlamaForCausalLM","sections":[],"depth":2},{"title":"LlamaForSequenceClassification","local":"llamaforsequenceclassification ][ transformers.LlamaForSequenceClassification","sections":[],"depth":2}],"depth":1}';function za(C){return fa(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ua extends ga{constructor(d){super(),_a(this,d,za,wa,ha,{})}}export{Ua as component};
