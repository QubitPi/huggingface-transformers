import{s as Yt,o as Bt,n as ge}from"../chunks/scheduler.56730f09.js";import{S as Kt,i as Et,g as p,s as o,r as $,A as Qt,h as f,f as l,c as a,j as It,u,x as r,k as St,y as qt,a as n,v as T,d as g,t as d,w as c}from"../chunks/index.1f144517.js";import{T as Te}from"../chunks/Tip.41e845e5.js";import{C as mt}from"../chunks/CodeBlock.738eeccb.js";import{H as P}from"../chunks/Heading.57d46534.js";function Dt(U){let s,m='자세한 설명이 필요하지 않고 바로 TPU 샘플 코드를 시작하고 싶다면 <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb" rel="nofollow">우리의 TPU 예제 노트북!</a>을 확인하세요.';return{c(){s=p("p"),s.innerHTML=m},l(i){s=f(i,"P",{"data-svelte-h":!0}),r(s)!=="svelte-hfswm5"&&(s.innerHTML=m)},m(i,M){n(i,s,M)},p:ge,d(i){i&&l(s)}}}function Ot(U){let s,m="메모리에 있는 모든 데이터를 <code>np.ndarray</code> 또는 <code>tf.Tensor</code>로 맞출 수 있다면, Google Cloud Storage에 업로드할 필요 없이, Colab 또는 TPU 노드를 사용해서 해당 데이터에 <code>fit()</code> 할 수 있습니다.";return{c(){s=p("p"),s.innerHTML=m},l(i){s=f(i,"P",{"data-svelte-h":!0}),r(s)!=="svelte-1s60wds"&&(s.innerHTML=m)},m(i,M){n(i,s,M)},p:ge,d(i){i&&l(s)}}}function el(U){let s,m="<strong>🤗특수한 Hugging Face 팁🤗:</strong> TF 코드 예제에서 볼 수 있는 <code>Dataset.to_tf_dataset()</code> 메소드와 그 상위 래퍼(wrapper)인 <code>model.prepare_tf_dataset()</code>는 모두 TPU 노드에서 작동하지 않습니다. 그 이유는 <code>tf.data.Dataset</code>을 생성하더라도 “순수한” <code>tf.data</code> 파이프라인이 아니며 <code>tf.numpy_function</code> 또는 <code>Dataset.from_generator()</code>를 사용하여 기본 HuggingFace <code>Dataset</code>에서 데이터를 전송하기 때문입니다. 이 HuggingFace <code>Dataset</code>는 로컬 디스크에 있는 데이터로 지원되며 원격 TPU 노드가 읽을 수 없습니다.";return{c(){s=p("p"),s.innerHTML=m},l(i){s=f(i,"P",{"data-svelte-h":!0}),r(s)!=="svelte-1nz4vmg"&&(s.innerHTML=m)},m(i,M){n(i,s,M)},p:ge,d(i){i&&l(s)}}}function tl(U){let s,m="XLA로 컴파일된 코드는 대체로 더 빠릅니다. 따라서 TPU에서 실행할 계획이 없더라도, <code>jit_compile=True</code>를 추가하면 성능이 향상될 수 있습니다. 하지만 XLA 호환성에 대한 아래 주의 사항을 반드시 확인하세요!";return{c(){s=p("p"),s.innerHTML=m},l(i){s=f(i,"P",{"data-svelte-h":!0}),r(s)!=="svelte-10d6tus"&&(s.innerHTML=m)},m(i,M){n(i,s,M)},p:ge,d(i){i&&l(s)}}}function ll(U){let s,m="<strong>뼈아픈 경험에서 얻은 팁:</strong> <code>jit_compile=True</code>를 사용하면 속도를 높이고 CPU/GPU 코드가 XLA와 호환되는지 검증할 수 있는 좋은 방법이지만, 실제 TPU에서 훈련할 때 그대로 남겨두면 많은 문제를 초래할 수 있습니다. XLA 컴파일은 TPU에서 암시적으로 이뤄지므로, 실제 TPU에서 코드를 실행하기 전에 해당 줄을 제거하는 것을 잊지 마세요!";return{c(){s=p("p"),s.innerHTML=m},l(i){s=f(i,"P",{"data-svelte-h":!0}),r(s)!=="svelte-7ks6fl"&&(s.innerHTML=m)},m(i,M){n(i,s,M)},p:ge,d(i){i&&l(s)}}}function nl(U){let s,m="<strong>특수한 HuggingFace 팁🤗:</strong> 저희는 TensorFlow 모델과 손실 함수를 XLA와 호환되도록 재작성하는 데 많은 노력을 기울였습니다. 저희의 모델과 손실 함수는 대개 기본적으로 규칙 #1과 #2를 따르므로 <code>transformers</code> 모델을 사용하는 경우, 이를 건너뛸 수 있습니다. 하지만 자체 모델과 손실 함수를 작성할 때는 이러한 규칙을 잊지 마세요!";return{c(){s=p("p"),s.innerHTML=m},l(i){s=f(i,"P",{"data-svelte-h":!0}),r(s)!=="svelte-im51ua"&&(s.innerHTML=m)},m(i,M){n(i,s,M)},p:ge,d(i){i&&l(s)}}}function sl(U){let s,m="<strong>🤗특수한 HuggingFace 팁🤗:</strong> 토크나이저와 데이터 콜레이터에 도움이 될 수 있는 메소드가 있습니다. 토크나이저를 불러올 때 <code>padding=&quot;max_length&quot;</code> 또는 <code>padding=&quot;longest&quot;</code>를 사용하여 패딩된 데이터를 출력하도록 할 수 있습니다. 토크나이저와 데이터 콜레이터는 나타나는 고유한 입력 크기의 수를 줄이기 위해 사용할 수 있는 <code>pad_to_multiple_of</code> 인수도 있습니다!";return{c(){s=p("p"),s.innerHTML=m},l(i){s=f(i,"P",{"data-svelte-h":!0}),r(s)!=="svelte-1a7kovq"&&(s.innerHTML=m)},m(i,M){n(i,s,M)},p:ge,d(i){i&&l(s)}}}function ol(U){let s,m,i,M,x,ce,b,Me,y,Ue,H,$t="TPU는 <strong>텐서 처리 장치</strong>입니다. Google에서 설계한 하드웨어로, GPU처럼 신경망 내에서 텐서 연산을 더욱 빠르게 처리하기 위해 사용됩니다. 네트워크 훈련과 추론 모두에 사용할 수 있습니다. 일반적으로 Google의 클라우드 서비스를 통해 이용할 수 있지만, Google Colab과 Kaggle Kernel을 통해 소규모 TPU를 무료로 직접 이용할 수도 있습니다.",Pe,X,ut='<a href="https://huggingface.co/blog/tensorflow-philosophy" rel="nofollow">🤗 Transformers의 모든 Tensorflow 모델은 Keras 모델</a>이기 때문에, 이 문서에서 다루는 대부분의 메소드는 대체로 모든 Keras 모델을 위한 TPU 훈련에 적용할 수 있습니다! 하지만 Transformer와 데이터 세트의 HuggingFace 생태계(hug-o-system?)에 특화된 몇 가지 사항이 있으며, 해당 사항에 대해 설명할 때 반드시 언급하도록 하겠습니다.',be,A,_e,k,Tt="신규 사용자는 TPU의 범위와 다양한 이용 방법에 대해 매우 혼란스러워하는 경우가 많습니다. <strong>TPU 노드</strong>와 <strong>TPU VM</strong>의 차이점은 가장 먼저 이해해야 할 핵심적인 구분 사항입니다.",ve,j,gt="<strong>TPU 노드</strong>를 사용한다면, 실제로는 원격 TPU를 간접적으로 이용하는 것입니다. 네트워크와 데이터 파이프라인을 초기화한 다음, 이를 원격 노드로 전달할 별도의 VM이 필요합니다. Google Colab에서 TPU를 사용하는 경우, <strong>TPU 노드</strong> 방식으로 이용하게 됩니다.",we,z,dt="TPU 노드를 사용하는 것은 이를 사용하지 않는 사용자에게 예기치 않은 현상이 발생하기도 합니다! 특히, TPU는 파이썬 코드를 실행하는 기기(machine)와 물리적으로 다른 시스템에 있기 때문에 로컬 기기에 데이터를 저장할 수 없습니다. 즉, 컴퓨터의 내부 저장소에서 가져오는 데이터 파이프라인은 절대 작동하지 않습니다! 로컬 기기에 데이터를 저장하는 대신에, 데이터 파이프라인이 원격 TPU 노드에서 실행 중일 때에도 데이터 파이프라인이 계속 이용할 수 있는 Google Cloud Storage에 데이터를 저장해야 합니다.",he,_,Le,v,Ce,J,ct="TPU를 이용하는 두 번째 방법은 <strong>TPU VM</strong>을 사용하는 것입니다. TPU VM을 사용할 때, GPU VM에서 훈련하는 것과 같이 TPU가 장착된 기기에 직접 연결합니다. 특히 데이터 파이프라인과 관련하여, TPU VM은 대체로 작업하기 더 쉽습니다. 위의 모든 경고는 TPU VM에는 해당되지 않습니다!",xe,G,Mt='이 문서는 의견이 포함된 문서이며, 저희의 의견이 여기에 있습니다: <strong>가능하면 TPU 노드를 사용하지 마세요.</strong> TPU 노드는 TPU VM보다 더 복잡하고 디버깅하기가 더 어렵습니다. 또한 향후에는 지원되지 않을 가능성이 높습니다. Google의 최신 TPU인 TPUv4는 TPU VM으로만 이용할 수 있으므로, TPU 노드는 점점 더 “구식” 이용 방법이 될 것으로 전망됩니다. 그러나 TPU 노드를 사용하는 Colab과 Kaggle Kernel에서만 무료 TPU 이용이 가능한 것으로 확인되어, 필요한 경우 이를 다루는 방법을 설명해 드리겠습니다! 이에 대한 자세한 설명이 담긴 코드 샘플은 <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb" rel="nofollow">TPU 예제 노트북</a>에서 확인하시기 바랍니다.',ye,V,He,F,Ut="단일 TPU(v2-8/v3-8/v4-8)는 8개의 복제본(replicas)을 실행합니다. TPU는 수백 또는 수천 개의 복제본을 동시에 실행할 수 있는 <strong>pod</strong>로 존재합니다. 단일 TPU를 하나 이상 사용하지만 전체 Pod보다 적게 사용하는 경우(예를 들면, v3-32), TPU 구성을 <strong>pod 슬라이스</strong>라고 합니다.",Xe,R,Pt="Colab을 통해 무료 TPU에 이용하는 경우, 기본적으로 단일 v2-8 TPU를 제공받습니다.",Ae,Z,ke,N,bt="XLA는 최적화 컴파일러로, TensorFlow와 JAX에서 모두 사용됩니다. JAX에서는 유일한 컴파일러이지만, TensorFlow에서는 선택 사항입니다(하지만 TPU에서는 필수입니다!). Keras 모델을 훈련할 때 이를 활성화하는 가장 쉬운 방법은 <code>jit_compile=True</code> 인수를 <code>model.compile()</code>에 전달하는 것입니다. 오류가 없고 성능이 양호하다면, TPU로 전환할 준비가 되었다는 좋은 신호입니다!",je,W,_t="TPU에서 디버깅하는 것은 대개 CPU/GPU보다 조금 더 어렵기 때문에, TPU에서 시도하기 전에 먼저 XLA로 CPU/GPU에서 코드를 실행하는 것을 권장합니다. 물론 오래 학습할 필요는 없습니다. 즉, 모델과 데이터 파이프라인이 예상대로 작동하는지 확인하기 위해 몇 단계만 거치면 됩니다.",ze,w,Je,h,Ge,I,Ve,S,vt="대부분의 경우, 여러분의 코드는 이미 XLA와 호환될 것입니다! 그러나 표준 TensorFlow에서 작동하지만, XLA에서는 작동하지 않는 몇 가지 사항이 있습니다. 이를 아래 세 가지 핵심 규칙으로 간추렸습니다:",Fe,L,Re,Y,Ze,B,wt="어떤 <code>if</code>문도 <code>tf.Tensor</code> 내부의 값에 종속될 수 없다는 것을 의미합니다. 예를 들어, 이 코드 블록은 XLA로 컴파일할 수 없습니다!",Ne,K,We,E,ht='처음에는 매우 제한적으로 보일 수 있지만, 대부분의 신경망 코드에서는 이를 수행할 필요가 없습니다. <code>tf.cond</code>를 사용하거나(<a href="https://www.tensorflow.org/api_docs/python/tf/cond" rel="nofollow">여기</a> 문서를 참조), 다음과 같이 조건문을 제거하고 대신 지표 변수를 사용하는 영리한 수학 트릭을 찾아내어 이 제한을 우회할 수 있습니다:',Ie,Q,Se,q,Lt="이 코드는 위의 코드와 정확히 동일한 효과를 구현하지만, 조건문을 제거하여 문제 없이 XLA로 컴파일되도록 합니다!",Ye,D,Be,O,Ct="코드에서 모든 <code>tf.Tensor</code> 객체의 크기가 해당 값에 종속될 수 없다는 것을 의미합니다. 예를 들어, <code>tf.unique</code> 함수는 입력에서 각 고유 값의 인스턴스 하나를 포함하는 <code>tensor</code>를 반환하기 때문에 XLA로 컴파일할 수 없습니다. 이 출력의 크기는 입력 <code>Tensor</code>가 얼마나 반복적인지에 따라 분명히 달라질 것이므로, XLA는 이를 처리하지 못합니다!",Ke,ee,xt='일반적으로, 대부분의 신경망 코드는 기본값으로 규칙 2를 따릅니다. 그러나 문제가 되는 몇 가지 대표적인 사례가 있습니다. 가장 흔한 사례 중 하나는 <strong>레이블 마스킹</strong>을 사용하여 손실(loss)을 계산할 때, 해당 위치를 무시하도록 나타내기 위해 레이블을 음수 값으로 설정하는 경우입니다. 레이블 마스킹을 지원하는 NumPy나 PyTorch 손실 함수를 보면 <a href="https://numpy.org/doc/stable/user/basics.indexing.html#boolean-array-indexing" rel="nofollow">불 인덱싱</a>을 사용하는 다음과 같은 코드를 자주 접할 수 있습니다:',Ee,te,Qe,le,yt="이 코드는 NumPy나 PyTorch에서는 문제 없이 작동하지만, XLA에서는 손상됩니다! 왜 그럴까요? 얼마나 많은 위치가 마스킹되는지에 따라 <code>masked_outputs</code>와 <code>masked_labels</code>의 크기가 달라져서, <strong>데이터 종속 크기</strong>가 되기 때문입니다. 그러나 규칙 #1과 마찬가지로, 이 코드를 다시 작성하면 데이터 종속적 모양 크기가 정확히 동일한 출력을 산출할 수 있습니다.",qe,ne,De,se,Ht="여기서, 모든 위치에 대한 손실을 계산하지만, 평균을 계산할 때 분자와 분모 모두에서 마스크된 위치를 0으로 처리합니다. 이는 데이터 종속 크기를 방지하고 XLA 호환성을 유지하면서 첫 번째 블록과 정확히 동일한 결과를 산출합니다. 규칙 #1에서와 동일한 트릭을 사용하여 <code>tf.bool</code>을 <code>tf.float32</code>로 변환하고 이를 지표 변수로 사용합니다. 해당 트릭은 매우 유용하며, 자체 코드를 XLA로 변환해야 할 경우 기억해 두세요!",Oe,oe,et,ae,Xt="이것은 가장 큰 문제입니다. 입력 크기가 매우 가변적인 경우, XLA는 모델을 반복해서 다시 컴파일해야 하므로 성능에 큰 문제가 발생할 수 있습니다. 이 문제는 토큰화 후 입력 텍스트의 길이가 가변적인 NLP 모델에서 주로 발생합니다. 다른 모달리티에서는 정적 크기가 더 흔하며, 해당 규칙이 훨씬 덜 문제시 됩니다.",tt,ie,At="규칙 #3을 어떻게 우회할 수 있을까요? 핵심은 <strong>패딩</strong>입니다. 모든 입력을 동일한 길이로 패딩한 다음, <code>attention_mask</code>를 사용하면 어떤 XLA 문제도 없이 가변 크기에서 가져온 것과 동일한 결과를 가져올 수 있습니다. 그러나 과도한 패딩은 심각한 속도 저하를 야기할 수도 있습니다. 모든 샘플을 전체 데이터 세트의 최대 길이로 패딩하면, 무한한 패딩 토큰으로 구성된 배치가 생성되어 많은 연산과 메모리가 낭비될 수 있습니다!",lt,pe,kt="이 문제에 대한 완벽한 해결책은 없습니다. 하지만, 몇 가지 트릭을 시도해볼 수 있습니다. 한 가지 유용한 트릭은 <strong>샘플 배치를 32 또는 64 토큰과 같은 숫자의 배수까지 패딩하는 것입니다.</strong> 이는 토큰 수가 소폭 증가하지만, 모든 입력 크기가 32 또는 64의 배수여야 하기 때문에 고유한 입력 크기의 수가 대폭 줄어듭니다. 고유한 입력 크기가 적다는 것은 XLA 컴파일 횟수가 적어진다는 것을 의미합니다!",nt,C,st,fe,ot,re,jt='훈련이 XLA와 호환되고 (TPU 노드/Colab을 사용하는 경우) 데이터 세트가 적절하게 준비되었다면, TPU에서 실행하는 것은 놀랍도록 쉽습니다! 코드에서 몇 줄만 추가하여, TPU를 초기화하고 모델과 데이터 세트가 <code>TPUStrategy</code> 범위 내에 생성되도록 변경하면 됩니다. <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb" rel="nofollow">우리의 TPU 예제 노트북</a>을 참조하여 실제로 작동하는 모습을 확인해 보세요!',at,me,it,$e,zt="여기에 많은 내용이 포함되어 있으므로, TPU 훈련을 위한 모델을 준비할 때 따를 수 있는 간략한 체크리스트로 요약해 보겠습니다:",pt,ue,Jt='<li>코드가 XLA의 세 가지 규칙을 따르는지 확인합니다.</li> <li>CPU/GPU에서 <code>jit_compile=True</code>로 모델을 컴파일하고 XLA로 훈련할 수 있는지 확인합니다.</li> <li>데이터 세트를 메모리에 가져오거나 TPU 호환 데이터 세트를 가져오는 방식을 사용합니다(<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb" rel="nofollow">노트북</a> 참조)</li> <li>코드를 Colab(accelerator가 “TPU”로 설정됨) 또는 Google Cloud의 TPU VM으로 마이그레이션합니다.</li> <li>TPU 초기화 코드를 추가합니다(<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb" rel="nofollow">노트북</a> 참조)</li> <li><code>TPUStrategy</code>를 생성하고 데이터 세트를 가져오는 것과 모델 생성이 <code>strategy.scope()</code> 내에 있는지 확인합니다(<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb" rel="nofollow">노트북</a> 참조)</li> <li>TPU로 이동할 때 <code>jit_compile=True</code>를 다시 설정하는 것을 잊지 마세요!</li> <li>🙏🙏🙏🥺🥺🥺</li> <li>model.fit()을 불러옵니다.</li> <li>여러분이 해냈습니다!</li>',ft,de,rt;return x=new P({props:{title:"TensorFlow로 TPU에서 훈련하기",local:"training-on-tpu-with-tensorflow",headingTag:"h1"}}),b=new Te({props:{$$slots:{default:[Dt]},$$scope:{ctx:U}}}),y=new P({props:{title:"TPU가 무엇인가요?",local:"what-is-a-tpu",headingTag:"h3"}}),A=new P({props:{title:"어떤 종류의 TPU가 있나요?",local:"what-kinds-of-tpu-are-available",headingTag:"h3"}}),_=new Te({props:{$$slots:{default:[Ot]},$$scope:{ctx:U}}}),v=new Te({props:{$$slots:{default:[el]},$$scope:{ctx:U}}}),V=new P({props:{title:"어떤 크기의 TPU를 사용할 수 있나요?",local:"what-sizes-of-tpu-are-available",headingTag:"h3"}}),Z=new P({props:{title:"XLA에 대해 들어본 적이 있습니다. XLA란 무엇이고 TPU와 어떤 관련이 있나요?",local:"i-keep-hearing-about-this-xla-thing-whats-xla-and-how-does-it-relate-to-tpus",headingTag:"h3"}}),w=new Te({props:{$$slots:{default:[tl]},$$scope:{ctx:U}}}),h=new Te({props:{warning:!0,$$slots:{default:[ll]},$$scope:{ctx:U}}}),I=new P({props:{title:"제 XLA 모델과 호환하려면 어떻게 해야 하나요?",local:"how-do-i-make-my-model-xla-compatible",headingTag:"h3"}}),L=new Te({props:{$$slots:{default:[nl]},$$scope:{ctx:U}}}),Y=new P({props:{title:"XLA 규칙 #1: 코드에서 “데이터 종속 조건문”을 사용할 수 없습니다",local:"xla-rule-1-your-code-cannot-have-datadependent-conditionals",headingTag:"h4"}}),K=new mt({props:{code:"aWYlMjB0Zi5yZWR1Y2Vfc3VtKHRlbnNvciklMjAlM0UlMjAxMCUzQSUwQSUyMCUyMCUyMCUyMHRlbnNvciUyMCUzRCUyMHRlbnNvciUyMCUyRiUyMDIuMA==",highlighted:`<span class="hljs-keyword">if</span> tf.reduce_sum(tensor) &gt; <span class="hljs-number">10</span>:
    tensor = tensor / <span class="hljs-number">2.0</span>`,wrap:!1}}),Q=new mt({props:{code:"c3VtX292ZXJfMTAlMjAlM0QlMjB0Zi5jYXN0KHRmLnJlZHVjZV9zdW0odGVuc29yKSUyMCUzRSUyMDEwJTJDJTIwdGYuZmxvYXQzMiklMEF0ZW5zb3IlMjAlM0QlMjB0ZW5zb3IlMjAlMkYlMjAoMS4wJTIwJTJCJTIwc3VtX292ZXJfMTAp",highlighted:`sum_over_10 = tf.cast(tf.reduce_sum(tensor) &gt; <span class="hljs-number">10</span>, tf.float32)
tensor = tensor / (<span class="hljs-number">1.0</span> + sum_over_10)`,wrap:!1}}),D=new P({props:{title:"XLA 규칙 #2: 코드에서 “데이터 종속 크기”를 가질 수 없습니다",local:"xla-rule-2-your-code-cannot-have-datadependent-shapes",headingTag:"h4"}}),te=new mt({props:{code:"bGFiZWxfbWFzayUyMCUzRCUyMGxhYmVscyUyMCUzRSUzRCUyMDAlMEFtYXNrZWRfb3V0cHV0cyUyMCUzRCUyMG91dHB1dHMlNUJsYWJlbF9tYXNrJTVEJTBBbWFza2VkX2xhYmVscyUyMCUzRCUyMGxhYmVscyU1QmxhYmVsX21hc2slNUQlMEFsb3NzJTIwJTNEJTIwY29tcHV0ZV9sb3NzKG1hc2tlZF9vdXRwdXRzJTJDJTIwbWFza2VkX2xhYmVscyklMEFtZWFuX2xvc3MlMjAlM0QlMjB0b3JjaC5tZWFuKGxvc3Mp",highlighted:`label_mask = labels &gt;= <span class="hljs-number">0</span>
masked_outputs = outputs[label_mask]
masked_labels = labels[label_mask]
loss = compute_loss(masked_outputs, masked_labels)
mean_loss = torch.mean(loss)`,wrap:!1}}),ne=new mt({props:{code:"bGFiZWxfbWFzayUyMCUzRCUyMHRmLmNhc3QobGFiZWxzJTIwJTNFJTNEJTIwMCUyQyUyMHRmLmZsb2F0MzIpJTBBbG9zcyUyMCUzRCUyMGNvbXB1dGVfbG9zcyhvdXRwdXRzJTJDJTIwbGFiZWxzKSUwQWxvc3MlMjAlM0QlMjBsb3NzJTIwKiUyMGxhYmVsX21hc2slMjAlMjAlMjMlMjBTZXQlMjBuZWdhdGl2ZSUyMGxhYmVsJTIwcG9zaXRpb25zJTIwdG8lMjAwJTBBbWVhbl9sb3NzJTIwJTNEJTIwdGYucmVkdWNlX3N1bShsb3NzKSUyMCUyRiUyMHRmLnJlZHVjZV9zdW0obGFiZWxfbWFzayk=",highlighted:`label_mask = tf.cast(labels &gt;= <span class="hljs-number">0</span>, tf.float32)
loss = compute_loss(outputs, labels)
loss = loss * label_mask  <span class="hljs-comment"># Set negative label positions to 0</span>
mean_loss = tf.reduce_sum(loss) / tf.reduce_sum(label_mask)`,wrap:!1}}),oe=new P({props:{title:"XLA 규칙 #3: XLA는 각기 다른 입력 크기가 나타날 때마다 모델을 다시 컴파일해야 합니다",local:"xla-rule-3-xla-will-need-to-recompile-your-model-for-every-different-input-shape-it-sees",headingTag:"h4"}}),C=new Te({props:{$$slots:{default:[sl]},$$scope:{ctx:U}}}),fe=new P({props:{title:"실제 TPU로 모델을 훈련하려면 어떻게 해야 하나요?",local:"how-do-i-actually-train-my-model-on-tpu",headingTag:"h3"}}),me=new P({props:{title:"요약",local:"summary",headingTag:"h3"}}),{c(){s=p("meta"),m=o(),i=p("p"),M=o(),$(x.$$.fragment),ce=o(),$(b.$$.fragment),Me=o(),$(y.$$.fragment),Ue=o(),H=p("p"),H.innerHTML=$t,Pe=o(),X=p("p"),X.innerHTML=ut,be=o(),$(A.$$.fragment),_e=o(),k=p("p"),k.innerHTML=Tt,ve=o(),j=p("p"),j.innerHTML=gt,we=o(),z=p("p"),z.textContent=dt,he=o(),$(_.$$.fragment),Le=o(),$(v.$$.fragment),Ce=o(),J=p("p"),J.innerHTML=ct,xe=o(),G=p("p"),G.innerHTML=Mt,ye=o(),$(V.$$.fragment),He=o(),F=p("p"),F.innerHTML=Ut,Xe=o(),R=p("p"),R.textContent=Pt,Ae=o(),$(Z.$$.fragment),ke=o(),N=p("p"),N.innerHTML=bt,je=o(),W=p("p"),W.textContent=_t,ze=o(),$(w.$$.fragment),Je=o(),$(h.$$.fragment),Ge=o(),$(I.$$.fragment),Ve=o(),S=p("p"),S.textContent=vt,Fe=o(),$(L.$$.fragment),Re=o(),$(Y.$$.fragment),Ze=o(),B=p("p"),B.innerHTML=wt,Ne=o(),$(K.$$.fragment),We=o(),E=p("p"),E.innerHTML=ht,Ie=o(),$(Q.$$.fragment),Se=o(),q=p("p"),q.textContent=Lt,Ye=o(),$(D.$$.fragment),Be=o(),O=p("p"),O.innerHTML=Ct,Ke=o(),ee=p("p"),ee.innerHTML=xt,Ee=o(),$(te.$$.fragment),Qe=o(),le=p("p"),le.innerHTML=yt,qe=o(),$(ne.$$.fragment),De=o(),se=p("p"),se.innerHTML=Ht,Oe=o(),$(oe.$$.fragment),et=o(),ae=p("p"),ae.textContent=Xt,tt=o(),ie=p("p"),ie.innerHTML=At,lt=o(),pe=p("p"),pe.innerHTML=kt,nt=o(),$(C.$$.fragment),st=o(),$(fe.$$.fragment),ot=o(),re=p("p"),re.innerHTML=jt,at=o(),$(me.$$.fragment),it=o(),$e=p("p"),$e.textContent=zt,pt=o(),ue=p("ul"),ue.innerHTML=Jt,ft=o(),de=p("p"),this.h()},l(e){const t=Qt("svelte-u9bgzb",document.head);s=f(t,"META",{name:!0,content:!0}),t.forEach(l),m=a(e),i=f(e,"P",{}),It(i).forEach(l),M=a(e),u(x.$$.fragment,e),ce=a(e),u(b.$$.fragment,e),Me=a(e),u(y.$$.fragment,e),Ue=a(e),H=f(e,"P",{"data-svelte-h":!0}),r(H)!=="svelte-1vo7dgq"&&(H.innerHTML=$t),Pe=a(e),X=f(e,"P",{"data-svelte-h":!0}),r(X)!=="svelte-6vabgl"&&(X.innerHTML=ut),be=a(e),u(A.$$.fragment,e),_e=a(e),k=f(e,"P",{"data-svelte-h":!0}),r(k)!=="svelte-pk77g6"&&(k.innerHTML=Tt),ve=a(e),j=f(e,"P",{"data-svelte-h":!0}),r(j)!=="svelte-13pzrf0"&&(j.innerHTML=gt),we=a(e),z=f(e,"P",{"data-svelte-h":!0}),r(z)!=="svelte-tmg6qr"&&(z.textContent=dt),he=a(e),u(_.$$.fragment,e),Le=a(e),u(v.$$.fragment,e),Ce=a(e),J=f(e,"P",{"data-svelte-h":!0}),r(J)!=="svelte-6ukwfl"&&(J.innerHTML=ct),xe=a(e),G=f(e,"P",{"data-svelte-h":!0}),r(G)!=="svelte-1bbsy4d"&&(G.innerHTML=Mt),ye=a(e),u(V.$$.fragment,e),He=a(e),F=f(e,"P",{"data-svelte-h":!0}),r(F)!=="svelte-1dsamsh"&&(F.innerHTML=Ut),Xe=a(e),R=f(e,"P",{"data-svelte-h":!0}),r(R)!=="svelte-2l1qah"&&(R.textContent=Pt),Ae=a(e),u(Z.$$.fragment,e),ke=a(e),N=f(e,"P",{"data-svelte-h":!0}),r(N)!=="svelte-1faznrp"&&(N.innerHTML=bt),je=a(e),W=f(e,"P",{"data-svelte-h":!0}),r(W)!=="svelte-6jhj1o"&&(W.textContent=_t),ze=a(e),u(w.$$.fragment,e),Je=a(e),u(h.$$.fragment,e),Ge=a(e),u(I.$$.fragment,e),Ve=a(e),S=f(e,"P",{"data-svelte-h":!0}),r(S)!=="svelte-14wa7r5"&&(S.textContent=vt),Fe=a(e),u(L.$$.fragment,e),Re=a(e),u(Y.$$.fragment,e),Ze=a(e),B=f(e,"P",{"data-svelte-h":!0}),r(B)!=="svelte-11jc14q"&&(B.innerHTML=wt),Ne=a(e),u(K.$$.fragment,e),We=a(e),E=f(e,"P",{"data-svelte-h":!0}),r(E)!=="svelte-1wakr62"&&(E.innerHTML=ht),Ie=a(e),u(Q.$$.fragment,e),Se=a(e),q=f(e,"P",{"data-svelte-h":!0}),r(q)!=="svelte-y5kton"&&(q.textContent=Lt),Ye=a(e),u(D.$$.fragment,e),Be=a(e),O=f(e,"P",{"data-svelte-h":!0}),r(O)!=="svelte-1bfrl2m"&&(O.innerHTML=Ct),Ke=a(e),ee=f(e,"P",{"data-svelte-h":!0}),r(ee)!=="svelte-hfev1w"&&(ee.innerHTML=xt),Ee=a(e),u(te.$$.fragment,e),Qe=a(e),le=f(e,"P",{"data-svelte-h":!0}),r(le)!=="svelte-1kw9qi7"&&(le.innerHTML=yt),qe=a(e),u(ne.$$.fragment,e),De=a(e),se=f(e,"P",{"data-svelte-h":!0}),r(se)!=="svelte-1rul0av"&&(se.innerHTML=Ht),Oe=a(e),u(oe.$$.fragment,e),et=a(e),ae=f(e,"P",{"data-svelte-h":!0}),r(ae)!=="svelte-oc616w"&&(ae.textContent=Xt),tt=a(e),ie=f(e,"P",{"data-svelte-h":!0}),r(ie)!=="svelte-1loeg1m"&&(ie.innerHTML=At),lt=a(e),pe=f(e,"P",{"data-svelte-h":!0}),r(pe)!=="svelte-19vhsr6"&&(pe.innerHTML=kt),nt=a(e),u(C.$$.fragment,e),st=a(e),u(fe.$$.fragment,e),ot=a(e),re=f(e,"P",{"data-svelte-h":!0}),r(re)!=="svelte-euisut"&&(re.innerHTML=jt),at=a(e),u(me.$$.fragment,e),it=a(e),$e=f(e,"P",{"data-svelte-h":!0}),r($e)!=="svelte-skvmtp"&&($e.textContent=zt),pt=a(e),ue=f(e,"UL",{"data-svelte-h":!0}),r(ue)!=="svelte-uk1d8s"&&(ue.innerHTML=Jt),ft=a(e),de=f(e,"P",{}),It(de).forEach(l),this.h()},h(){St(s,"name","hf:doc:metadata"),St(s,"content",al)},m(e,t){qt(document.head,s),n(e,m,t),n(e,i,t),n(e,M,t),T(x,e,t),n(e,ce,t),T(b,e,t),n(e,Me,t),T(y,e,t),n(e,Ue,t),n(e,H,t),n(e,Pe,t),n(e,X,t),n(e,be,t),T(A,e,t),n(e,_e,t),n(e,k,t),n(e,ve,t),n(e,j,t),n(e,we,t),n(e,z,t),n(e,he,t),T(_,e,t),n(e,Le,t),T(v,e,t),n(e,Ce,t),n(e,J,t),n(e,xe,t),n(e,G,t),n(e,ye,t),T(V,e,t),n(e,He,t),n(e,F,t),n(e,Xe,t),n(e,R,t),n(e,Ae,t),T(Z,e,t),n(e,ke,t),n(e,N,t),n(e,je,t),n(e,W,t),n(e,ze,t),T(w,e,t),n(e,Je,t),T(h,e,t),n(e,Ge,t),T(I,e,t),n(e,Ve,t),n(e,S,t),n(e,Fe,t),T(L,e,t),n(e,Re,t),T(Y,e,t),n(e,Ze,t),n(e,B,t),n(e,Ne,t),T(K,e,t),n(e,We,t),n(e,E,t),n(e,Ie,t),T(Q,e,t),n(e,Se,t),n(e,q,t),n(e,Ye,t),T(D,e,t),n(e,Be,t),n(e,O,t),n(e,Ke,t),n(e,ee,t),n(e,Ee,t),T(te,e,t),n(e,Qe,t),n(e,le,t),n(e,qe,t),T(ne,e,t),n(e,De,t),n(e,se,t),n(e,Oe,t),T(oe,e,t),n(e,et,t),n(e,ae,t),n(e,tt,t),n(e,ie,t),n(e,lt,t),n(e,pe,t),n(e,nt,t),T(C,e,t),n(e,st,t),T(fe,e,t),n(e,ot,t),n(e,re,t),n(e,at,t),T(me,e,t),n(e,it,t),n(e,$e,t),n(e,pt,t),n(e,ue,t),n(e,ft,t),n(e,de,t),rt=!0},p(e,[t]){const Gt={};t&2&&(Gt.$$scope={dirty:t,ctx:e}),b.$set(Gt);const Vt={};t&2&&(Vt.$$scope={dirty:t,ctx:e}),_.$set(Vt);const Ft={};t&2&&(Ft.$$scope={dirty:t,ctx:e}),v.$set(Ft);const Rt={};t&2&&(Rt.$$scope={dirty:t,ctx:e}),w.$set(Rt);const Zt={};t&2&&(Zt.$$scope={dirty:t,ctx:e}),h.$set(Zt);const Nt={};t&2&&(Nt.$$scope={dirty:t,ctx:e}),L.$set(Nt);const Wt={};t&2&&(Wt.$$scope={dirty:t,ctx:e}),C.$set(Wt)},i(e){rt||(g(x.$$.fragment,e),g(b.$$.fragment,e),g(y.$$.fragment,e),g(A.$$.fragment,e),g(_.$$.fragment,e),g(v.$$.fragment,e),g(V.$$.fragment,e),g(Z.$$.fragment,e),g(w.$$.fragment,e),g(h.$$.fragment,e),g(I.$$.fragment,e),g(L.$$.fragment,e),g(Y.$$.fragment,e),g(K.$$.fragment,e),g(Q.$$.fragment,e),g(D.$$.fragment,e),g(te.$$.fragment,e),g(ne.$$.fragment,e),g(oe.$$.fragment,e),g(C.$$.fragment,e),g(fe.$$.fragment,e),g(me.$$.fragment,e),rt=!0)},o(e){d(x.$$.fragment,e),d(b.$$.fragment,e),d(y.$$.fragment,e),d(A.$$.fragment,e),d(_.$$.fragment,e),d(v.$$.fragment,e),d(V.$$.fragment,e),d(Z.$$.fragment,e),d(w.$$.fragment,e),d(h.$$.fragment,e),d(I.$$.fragment,e),d(L.$$.fragment,e),d(Y.$$.fragment,e),d(K.$$.fragment,e),d(Q.$$.fragment,e),d(D.$$.fragment,e),d(te.$$.fragment,e),d(ne.$$.fragment,e),d(oe.$$.fragment,e),d(C.$$.fragment,e),d(fe.$$.fragment,e),d(me.$$.fragment,e),rt=!1},d(e){e&&(l(m),l(i),l(M),l(ce),l(Me),l(Ue),l(H),l(Pe),l(X),l(be),l(_e),l(k),l(ve),l(j),l(we),l(z),l(he),l(Le),l(Ce),l(J),l(xe),l(G),l(ye),l(He),l(F),l(Xe),l(R),l(Ae),l(ke),l(N),l(je),l(W),l(ze),l(Je),l(Ge),l(Ve),l(S),l(Fe),l(Re),l(Ze),l(B),l(Ne),l(We),l(E),l(Ie),l(Se),l(q),l(Ye),l(Be),l(O),l(Ke),l(ee),l(Ee),l(Qe),l(le),l(qe),l(De),l(se),l(Oe),l(et),l(ae),l(tt),l(ie),l(lt),l(pe),l(nt),l(st),l(ot),l(re),l(at),l(it),l($e),l(pt),l(ue),l(ft),l(de)),l(s),c(x,e),c(b,e),c(y,e),c(A,e),c(_,e),c(v,e),c(V,e),c(Z,e),c(w,e),c(h,e),c(I,e),c(L,e),c(Y,e),c(K,e),c(Q,e),c(D,e),c(te,e),c(ne,e),c(oe,e),c(C,e),c(fe,e),c(me,e)}}}const al='{"title":"TensorFlow로 TPU에서 훈련하기","local":"training-on-tpu-with-tensorflow","sections":[{"title":"TPU가 무엇인가요?","local":"what-is-a-tpu","sections":[],"depth":3},{"title":"어떤 종류의 TPU가 있나요?","local":"what-kinds-of-tpu-are-available","sections":[],"depth":3},{"title":"어떤 크기의 TPU를 사용할 수 있나요?","local":"what-sizes-of-tpu-are-available","sections":[],"depth":3},{"title":"XLA에 대해 들어본 적이 있습니다. XLA란 무엇이고 TPU와 어떤 관련이 있나요?","local":"i-keep-hearing-about-this-xla-thing-whats-xla-and-how-does-it-relate-to-tpus","sections":[],"depth":3},{"title":"제 XLA 모델과 호환하려면 어떻게 해야 하나요?","local":"how-do-i-make-my-model-xla-compatible","sections":[{"title":"XLA 규칙 #1: 코드에서 “데이터 종속 조건문”을 사용할 수 없습니다","local":"xla-rule-1-your-code-cannot-have-datadependent-conditionals","sections":[],"depth":4},{"title":"XLA 규칙 #2: 코드에서 “데이터 종속 크기”를 가질 수 없습니다","local":"xla-rule-2-your-code-cannot-have-datadependent-shapes","sections":[],"depth":4},{"title":"XLA 규칙 #3: XLA는 각기 다른 입력 크기가 나타날 때마다 모델을 다시 컴파일해야 합니다","local":"xla-rule-3-xla-will-need-to-recompile-your-model-for-every-different-input-shape-it-sees","sections":[],"depth":4}],"depth":3},{"title":"실제 TPU로 모델을 훈련하려면 어떻게 해야 하나요?","local":"how-do-i-actually-train-my-model-on-tpu","sections":[],"depth":3},{"title":"요약","local":"summary","sections":[],"depth":3}],"depth":1}';function il(U){return Bt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ul extends Kt{constructor(s){super(),Et(this,s,il,ol,Yt,{})}}export{ul as component};
