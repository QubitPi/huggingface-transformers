import{s as el,o as tl,n as ll}from"../chunks/scheduler.56730f09.js";import{S as sl,i as al,g as p,s as a,r as m,m as nl,A as pl,h as i,f as l,c as n,j as Ot,u as r,x as M,n as il,k as gt,y as Ml,a as s,v as o,d as c,t as y,w as d}from"../chunks/index.1f144517.js";import{T as ml}from"../chunks/Tip.41e845e5.js";import{C as J}from"../chunks/CodeBlock.738eeccb.js";import{D as rl}from"../chunks/DocNotebookDropdown.243c3df7.js";import{H as be}from"../chunks/Heading.57d46534.js";function ol(he){let w,U=`많은 이미지 캡션 데이터세트에는 이미지당 여러 개의 캡션이 포함되어 있습니다.
이러한 경우, 일반적으로 학습 중에 사용 가능한 캡션 중에서 무작위로 샘플을 추출합니다.`;return{c(){w=p("p"),w.textContent=U},l(f){w=i(f,"P",{"data-svelte-h":!0}),M(w)!=="svelte-82zair"&&(w.textContent=U)},m(f,Te){s(f,w,Te)},p:ll,d(f){f&&l(w)}}}function cl(he){let w,U,f,Te,h,je,j,ge,g,$t=`이미지 캡셔닝(Image captioning)은 주어진 이미지에 대한 캡션을 예측하는 작업입니다.
이미지 캡셔닝은 시각 장애인이 다양한 상황을 탐색하는 데 도움을 줄 수 있도록 시각 장애인을 보조하는 등 실생활에서 흔히 활용됩니다.
따라서 이미지 캡셔닝은 이미지를 설명함으로써 사람들의 콘텐츠 접근성을 개선하는 데 도움이 됩니다.`,$e,$,Ct="이 가이드에서는 소개할 내용은 아래와 같습니다:",Ce,C,_t="<li>이미지 캡셔닝 모델을 파인튜닝합니다.</li> <li>파인튜닝된 모델을 추론에 사용합니다.</li>",_e,_,kt="시작하기 전에 필요한 모든 라이브러리가 설치되어 있는지 확인하세요:",ke,k,Ie,I,It=`Hugging Face 계정에 로그인하면 모델을 업로드하고 커뮤니티에 공유할 수 있습니다.
토큰을 입력하여 로그인하세요.`,Be,B,We,W,Ze,Z,Bt=`{이미지-캡션} 쌍으로 구성된 데이터세트를 가져오려면 🤗 Dataset 라이브러리를 사용합니다.
PyTorch에서 자신만의 이미지 캡션 데이터세트를 만들려면 <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb" rel="nofollow">이 노트북</a>을 참조하세요.`,ve,v,Ge,G,xe,x,Wt="이 데이터세트는 <code>image</code>와 <code>text</code>라는 두 특성을 가지고 있습니다.",Re,u,Xe,R,Zt="<code>train_test_split</code> 메소드를 사용하여 데이터세트의 학습 분할을 학습 및 테스트 세트로 나눕니다:",He,X,Ve,H,vt=`학습 세트의 샘플 몇 개를 시각화해 봅시다.
Let’s visualize a couple of samples from the training set.`,Ye,V,Ee,b,Gt='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_training_images_image_cap.png" alt="Sample training images"/>',ze,Y,Qe,E,xt="데이터세트에는 이미지와 텍스트라는 두 가지 양식이 있기 때문에, 전처리 파이프라인에서 이미지와 캡션을 모두 전처리합니다.",Ne,z,Rt="전처리 작업을 위해, 파인튜닝하려는 모델에 연결된 프로세서 클래스를 가져옵니다.",Fe,Q,Ae,N,Xt="프로세서는 내부적으로 크기 조정 및 픽셀 크기 조정을 포함한 이미지 전처리를 수행하고 캡션을 토큰화합니다.",Se,F,qe,A,Ht="데이터세트가 준비되었으니 이제 파인튜닝을 위해 모델을 설정할 수 있습니다.",Le,S,Pe,q,Vt='<a href="https://huggingface.co/microsoft/git-base" rel="nofollow">“microsoft/git-base”</a>를 <a href="https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM" rel="nofollow"><code>AutoModelForCausalLM</code></a> 객체로 가져옵니다.',Ke,L,De,P,Oe,K,Yt=`이미지 캡션 모델은 일반적으로 <a href="https://huggingface.co/spaces/evaluate-metric/rouge" rel="nofollow">Rouge 점수</a> 또는 <a href="https://huggingface.co/spaces/evaluate-metric/wer" rel="nofollow">단어 오류율(Word Error Rate)</a>로 평가합니다.
이 가이드에서는 단어 오류율(WER)을 사용합니다.`,et,D,Et=`이를 위해 🤗 Evaluate 라이브러리를 사용합니다.
WER의 잠재적 제한 사항 및 기타 문제점은 <a href="https://huggingface.co/spaces/evaluate-metric/wer" rel="nofollow">이 가이드</a>를 참조하세요.`,tt,O,lt,ee,st,te,zt="이제 모델 파인튜닝을 시작할 준비가 되었습니다. 이를 위해 🤗 <code>Trainer</code>를 사용합니다.",at,le,Qt="먼저, <code>TrainingArguments</code>를 사용하여 학습 인수를 정의합니다.",nt,se,pt,ae,Nt="학습 인수를 데이터세트, 모델과 함께 🤗 Trainer에 전달합니다.",it,ne,Mt,pe,Ft="학습을 시작하려면 <code>Trainer</code> 객체에서 <code>train()</code>을 호출하기만 하면 됩니다.",mt,ie,rt,Me,At="학습이 진행되면서 학습 손실이 원활하게 감소하는 것을 볼 수 있습니다.",ot,me,St="학습이 완료되면 모든 사람이 모델을 사용할 수 있도록 <code>push_to_hub()</code> 메소드를 사용하여 모델을 허브에 공유하세요:",ct,re,yt,oe,dt,ce,qt="<code>test_ds</code>에서 샘플 이미지를 가져와 모델을 테스트합니다.",Jt,ye,wt,T,Lt='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/test_image_image_cap.png" alt="Test image"/>',ft,de,ut,Je,Pt="<code>generate</code>를 호출하고 예측을 디코딩합니다.",bt,we,Tt,fe,Ut,ue,Kt="파인튜닝된 모델이 꽤 괜찮은 캡션을 생성한 것 같습니다!",ht,Ue,jt;return h=new be({props:{title:"이미지 캡셔닝",local:"image-captioning",headingTag:"h1"}}),j=new rl({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/image_captioning.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/image_captioning.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/image_captioning.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/image_captioning.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/image_captioning.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/image_captioning.ipynb"}]}}),k=new J({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGUlMjAtcSUwQXBpcCUyMGluc3RhbGwlMjBqaXdlciUyMC1x",highlighted:`pip install transformers datasets evaluate -q
pip install jiwer -q`,wrap:!1}}),B=new J({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

notebook_login()`,wrap:!1}}),W=new be({props:{title:"포켓몬 BLIP 캡션 데이터세트 가져오기",local:"load-the-pokmon-blip-captions-dataset",headingTag:"h2"}}),v=new J({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZHMlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIybGFtYmRhbGFicyUyRnBva2Vtb24tYmxpcC1jYXB0aW9ucyUyMiklMEFkcw==",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

ds = load_dataset(<span class="hljs-string">&quot;lambdalabs/pokemon-blip-captions&quot;</span>)
ds`,wrap:!1}}),G=new J({props:{code:"RGF0YXNldERpY3QoJTdCJTBBJTIwJTIwJTIwJTIwdHJhaW4lM0ElMjBEYXRhc2V0KCU3QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGZlYXR1cmVzJTNBJTIwJTVCJ2ltYWdlJyUyQyUyMCd0ZXh0JyU1RCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMG51bV9yb3dzJTNBJTIwODMzJTBBJTIwJTIwJTIwJTIwJTdEKSUwQSU3RCk=",highlighted:`DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;image&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>],
        num_rows: 833
    })
})`,wrap:!1}}),u=new ml({props:{$$slots:{default:[ol]},$$scope:{ctx:he}}}),X=new J({props:{code:"ZHMlMjAlM0QlMjBkcyU1QiUyMnRyYWluJTIyJTVELnRyYWluX3Rlc3Rfc3BsaXQodGVzdF9zaXplJTNEMC4xKSUwQXRyYWluX2RzJTIwJTNEJTIwZHMlNUIlMjJ0cmFpbiUyMiU1RCUwQXRlc3RfZHMlMjAlM0QlMjBkcyU1QiUyMnRlc3QlMjIlNUQ=",highlighted:`ds = ds[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(test_size=<span class="hljs-number">0.1</span>)
train_ds = ds[<span class="hljs-string">&quot;train&quot;</span>]
test_ds = ds[<span class="hljs-string">&quot;test&quot;</span>]`,wrap:!1}}),V=new J({props:{code:"ZnJvbSUyMHRleHR3cmFwJTIwaW1wb3J0JTIwd3JhcCUwQWltcG9ydCUyMG1hdHBsb3RsaWIucHlwbG90JTIwYXMlMjBwbHQlMEFpbXBvcnQlMjBudW1weSUyMGFzJTIwbnAlMEElMEElMEFkZWYlMjBwbG90X2ltYWdlcyhpbWFnZXMlMkMlMjBjYXB0aW9ucyklM0ElMEElMjAlMjAlMjAlMjBwbHQuZmlndXJlKGZpZ3NpemUlM0QoMjAlMkMlMjAyMCkpJTBBJTIwJTIwJTIwJTIwZm9yJTIwaSUyMGluJTIwcmFuZ2UobGVuKGltYWdlcykpJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYXglMjAlM0QlMjBwbHQuc3VicGxvdCgxJTJDJTIwbGVuKGltYWdlcyklMkMlMjBpJTIwJTJCJTIwMSklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBjYXB0aW9uJTIwJTNEJTIwY2FwdGlvbnMlNUJpJTVEJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwY2FwdGlvbiUyMCUzRCUyMCUyMiU1Q24lMjIuam9pbih3cmFwKGNhcHRpb24lMkMlMjAxMikpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcGx0LnRpdGxlKGNhcHRpb24pJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcGx0Lmltc2hvdyhpbWFnZXMlNUJpJTVEKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHBsdC5heGlzKCUyMm9mZiUyMiklMEElMEElMEFzYW1wbGVfaW1hZ2VzX3RvX3Zpc3VhbGl6ZSUyMCUzRCUyMCU1Qm5wLmFycmF5KHRyYWluX2RzJTVCaSU1RCU1QiUyMmltYWdlJTIyJTVEKSUyMGZvciUyMGklMjBpbiUyMHJhbmdlKDUpJTVEJTBBc2FtcGxlX2NhcHRpb25zJTIwJTNEJTIwJTVCdHJhaW5fZHMlNUJpJTVEJTVCJTIydGV4dCUyMiU1RCUyMGZvciUyMGklMjBpbiUyMHJhbmdlKDUpJTVEJTBBcGxvdF9pbWFnZXMoc2FtcGxlX2ltYWdlc190b192aXN1YWxpemUlMkMlMjBzYW1wbGVfY2FwdGlvbnMp",highlighted:`<span class="hljs-keyword">from</span> textwrap <span class="hljs-keyword">import</span> wrap
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-keyword">def</span> <span class="hljs-title function_">plot_images</span>(<span class="hljs-params">images, captions</span>):
    plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">20</span>))
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(images)):
        ax = plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-built_in">len</span>(images), i + <span class="hljs-number">1</span>)
        caption = captions[i]
        caption = <span class="hljs-string">&quot;\\n&quot;</span>.join(wrap(caption, <span class="hljs-number">12</span>))
        plt.title(caption)
        plt.imshow(images[i])
        plt.axis(<span class="hljs-string">&quot;off&quot;</span>)


sample_images_to_visualize = [np.array(train_ds[i][<span class="hljs-string">&quot;image&quot;</span>]) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)]
sample_captions = [train_ds[i][<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>)]
plot_images(sample_images_to_visualize, sample_captions)`,wrap:!1}}),Y=new be({props:{title:"데이터세트 전처리",local:"preprocess-the-dataset",headingTag:"h2"}}),Q=new J({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMEElMEFjaGVja3BvaW50JTIwJTNEJTIwJTIybWljcm9zb2Z0JTJGZ2l0LWJhc2UlMjIlMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZChjaGVja3BvaW50KQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

checkpoint = <span class="hljs-string">&quot;microsoft/git-base&quot;</span>
processor = AutoProcessor.from_pretrained(checkpoint)`,wrap:!1}}),F=new J({props:{code:"ZGVmJTIwdHJhbnNmb3JtcyhleGFtcGxlX2JhdGNoKSUzQSUwQSUyMCUyMCUyMCUyMGltYWdlcyUyMCUzRCUyMCU1QnglMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlX2JhdGNoJTVCJTIyaW1hZ2UlMjIlNUQlNUQlMEElMjAlMjAlMjAlMjBjYXB0aW9ucyUyMCUzRCUyMCU1QnglMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlX2JhdGNoJTVCJTIydGV4dCUyMiU1RCU1RCUwQSUyMCUyMCUyMCUyMGlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZXMlMkMlMjB0ZXh0JTNEY2FwdGlvbnMlMkMlMjBwYWRkaW5nJTNEJTIybWF4X2xlbmd0aCUyMiklMEElMjAlMjAlMjAlMjBpbnB1dHMudXBkYXRlKCU3QiUyMmxhYmVscyUyMiUzQSUyMGlucHV0cyU1QiUyMmlucHV0X2lkcyUyMiU1RCU3RCklMEElMjAlMjAlMjAlMjByZXR1cm4lMjBpbnB1dHMlMEElMEElMEF0cmFpbl9kcy5zZXRfdHJhbnNmb3JtKHRyYW5zZm9ybXMpJTBBdGVzdF9kcy5zZXRfdHJhbnNmb3JtKHRyYW5zZm9ybXMp",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">example_batch</span>):
    images = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;image&quot;</span>]]
    captions = [x <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;text&quot;</span>]]
    inputs = processor(images=images, text=captions, padding=<span class="hljs-string">&quot;max_length&quot;</span>)
    inputs.update({<span class="hljs-string">&quot;labels&quot;</span>: inputs[<span class="hljs-string">&quot;input_ids&quot;</span>]})
    <span class="hljs-keyword">return</span> inputs


train_ds.set_transform(transforms)
test_ds.set_transform(transforms)`,wrap:!1}}),S=new be({props:{title:"기본 모델 가져오기",local:"load-a-base-model",headingTag:"h2"}}),L=new J({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(checkpoint)`,wrap:!1}}),P=new be({props:{title:"평가",local:"evaluate",headingTag:"h2"}}),O=new J({props:{code:"ZnJvbSUyMGV2YWx1YXRlJTIwaW1wb3J0JTIwbG9hZCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBd2VyJTIwJTNEJTIwbG9hZCglMjJ3ZXIlMjIpJTBBJTBBJTBBZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMkMlMjBsYWJlbHMlMjAlM0QlMjBldmFsX3ByZWQlMEElMjAlMjAlMjAlMjBwcmVkaWN0ZWQlMjAlM0QlMjBsb2dpdHMuYXJnbWF4KC0xKSUwQSUyMCUyMCUyMCUyMGRlY29kZWRfbGFiZWxzJTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShsYWJlbHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEElMjAlMjAlMjAlMjBkZWNvZGVkX3ByZWRpY3Rpb25zJTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShwcmVkaWN0ZWQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEElMjAlMjAlMjAlMjB3ZXJfc2NvcmUlMjAlM0QlMjB3ZXIuY29tcHV0ZShwcmVkaWN0aW9ucyUzRGRlY29kZWRfcHJlZGljdGlvbnMlMkMlMjByZWZlcmVuY2VzJTNEZGVjb2RlZF9sYWJlbHMpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwJTdCJTIyd2VyX3Njb3JlJTIyJTNBJTIwd2VyX3Njb3JlJTdE",highlighted:`<span class="hljs-keyword">from</span> evaluate <span class="hljs-keyword">import</span> load
<span class="hljs-keyword">import</span> torch

wer = load(<span class="hljs-string">&quot;wer&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    logits, labels = eval_pred
    predicted = logits.argmax(-<span class="hljs-number">1</span>)
    decoded_labels = processor.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)
    decoded_predictions = processor.batch_decode(predicted, skip_special_tokens=<span class="hljs-literal">True</span>)
    wer_score = wer.compute(predictions=decoded_predictions, references=decoded_labels)
    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;wer_score&quot;</span>: wer_score}`,wrap:!1}}),ee=new be({props:{title:"학습!",local:"train!",headingTag:"h2"}}),se=new J({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluaW5nQXJndW1lbnRzJTJDJTIwVHJhaW5lciUwQSUwQW1vZGVsX25hbWUlMjAlM0QlMjBjaGVja3BvaW50LnNwbGl0KCUyMiUyRiUyMiklNUIxJTVEJTBBJTBBdHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0RmJTIyJTdCbW9kZWxfbmFtZSU3RC1wb2tlbW9uJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDVlLTUlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNENTAlMkMlMEElMjAlMjAlMjAlMjBmcDE2JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfdHJhaW5fYmF0Y2hfc2l6ZSUzRDMyJTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV9ldmFsX2JhdGNoX3NpemUlM0QzMiUyQyUwQSUyMCUyMCUyMCUyMGdyYWRpZW50X2FjY3VtdWxhdGlvbl9zdGVwcyUzRDIlMkMlMEElMjAlMjAlMjAlMjBzYXZlX3RvdGFsX2xpbWl0JTNEMyUyQyUwQSUyMCUyMCUyMCUyMGV2YWx1YXRpb25fc3RyYXRlZ3klM0QlMjJzdGVwcyUyMiUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfc3RlcHMlM0Q1MCUyQyUwQSUyMCUyMCUyMCUyMHNhdmVfc3RyYXRlZ3klM0QlMjJzdGVwcyUyMiUyQyUwQSUyMCUyMCUyMCUyMHNhdmVfc3RlcHMlM0Q1MCUyQyUwQSUyMCUyMCUyMCUyMGxvZ2dpbmdfc3RlcHMlM0Q1MCUyQyUwQSUyMCUyMCUyMCUyMHJlbW92ZV91bnVzZWRfY29sdW1ucyUzREZhbHNlJTJDJTBBJTIwJTIwJTIwJTIwcHVzaF90b19odWIlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwbGFiZWxfbmFtZXMlM0QlNUIlMjJsYWJlbHMlMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBsb2FkX2Jlc3RfbW9kZWxfYXRfZW5kJTNEVHJ1ZSUyQyUwQSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer

model_name = checkpoint.split(<span class="hljs-string">&quot;/&quot;</span>)[<span class="hljs-number">1</span>]

training_args = TrainingArguments(
    output_dir=<span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-pokemon&quot;</span>,
    learning_rate=<span class="hljs-number">5e-5</span>,
    num_train_epochs=<span class="hljs-number">50</span>,
    fp16=<span class="hljs-literal">True</span>,
    per_device_train_batch_size=<span class="hljs-number">32</span>,
    per_device_eval_batch_size=<span class="hljs-number">32</span>,
    gradient_accumulation_steps=<span class="hljs-number">2</span>,
    save_total_limit=<span class="hljs-number">3</span>,
    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
    eval_steps=<span class="hljs-number">50</span>,
    save_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
    save_steps=<span class="hljs-number">50</span>,
    logging_steps=<span class="hljs-number">50</span>,
    remove_unused_columns=<span class="hljs-literal">False</span>,
    push_to_hub=<span class="hljs-literal">True</span>,
    label_names=[<span class="hljs-string">&quot;labels&quot;</span>],
    load_best_model_at_end=<span class="hljs-literal">True</span>,
)`,wrap:!1}}),ne=new J({props:{code:"dHJhaW5lciUyMCUzRCUyMFRyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0Rtb2RlbCUyQyUwQSUyMCUyMCUyMCUyMGFyZ3MlM0R0cmFpbmluZ19hcmdzJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRHRyYWluX2RzJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9kYXRhc2V0JTNEdGVzdF9kcyUyQyUwQSUyMCUyMCUyMCUyMGNvbXB1dGVfbWV0cmljcyUzRGNvbXB1dGVfbWV0cmljcyUyQyUwQSk=",highlighted:`trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics,
)`,wrap:!1}}),ie=new J({props:{code:"dHJhaW5lci50cmFpbigp",highlighted:"trainer.train()",wrap:!1}}),re=new J({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:"trainer.push_to_hub()",wrap:!1}}),oe=new be({props:{title:"추론",local:"inference",headingTag:"h2"}}),ye=new J({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRmh1Z2dpbmdmYWNlLmNvJTJGZGF0YXNldHMlMkZzYXlha3BhdWwlMkZzYW1wbGUtZGF0YXNldHMlMkZyZXNvbHZlJTJGbWFpbiUyRnBva2Vtb24ucG5nJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW1hZ2U=",highlighted:`<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests

url = <span class="hljs-string">&quot;https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/pokemon.png&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
image`,wrap:!1}}),de=new J({props:{code:"ZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMGlmJTIwdG9yY2guY3VkYS5pc19hdmFpbGFibGUoKSUyMGVsc2UlMjAlMjJjcHUlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhkZXZpY2UpJTBBcGl4ZWxfdmFsdWVzJTIwJTNEJTIwaW5wdXRzLnBpeGVsX3ZhbHVlcw==",highlighted:`device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)
pixel_values = inputs.pixel_values`,wrap:!1}}),we=new J({props:{code:"Z2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKHBpeGVsX3ZhbHVlcyUzRHBpeGVsX3ZhbHVlcyUyQyUyMG1heF9sZW5ndGglM0Q1MCklMEFnZW5lcmF0ZWRfY2FwdGlvbiUyMCUzRCUyMHByb2Nlc3Nvci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQlMEFwcmludChnZW5lcmF0ZWRfY2FwdGlvbik=",highlighted:`generated_ids = model.generate(pixel_values=pixel_values, max_length=<span class="hljs-number">50</span>)
generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-built_in">print</span>(generated_caption)`,wrap:!1}}),fe=new J({props:{code:"YSUyMGRyYXdpbmclMjBvZiUyMGElMjBwaW5rJTIwYW5kJTIwYmx1ZSUyMHBva2Vtb24=",highlighted:"a drawing of a pink and blue pokemon",wrap:!1}}),{c(){w=p("meta"),U=a(),f=p("p"),Te=a(),m(h.$$.fragment),je=a(),m(j.$$.fragment),ge=a(),g=p("p"),g.textContent=$t,$e=a(),$=p("p"),$.textContent=Ct,Ce=a(),C=p("ul"),C.innerHTML=_t,_e=a(),_=p("p"),_.textContent=kt,ke=a(),m(k.$$.fragment),Ie=a(),I=p("p"),I.textContent=It,Be=a(),m(B.$$.fragment),We=a(),m(W.$$.fragment),Ze=a(),Z=p("p"),Z.innerHTML=Bt,ve=a(),m(v.$$.fragment),Ge=a(),m(G.$$.fragment),xe=a(),x=p("p"),x.innerHTML=Wt,Re=a(),m(u.$$.fragment),Xe=a(),R=p("p"),R.innerHTML=Zt,He=a(),m(X.$$.fragment),Ve=a(),H=p("p"),H.textContent=vt,Ye=a(),m(V.$$.fragment),Ee=a(),b=p("div"),b.innerHTML=Gt,ze=a(),m(Y.$$.fragment),Qe=a(),E=p("p"),E.textContent=xt,Ne=a(),z=p("p"),z.textContent=Rt,Fe=a(),m(Q.$$.fragment),Ae=a(),N=p("p"),N.textContent=Xt,Se=a(),m(F.$$.fragment),qe=a(),A=p("p"),A.textContent=Ht,Le=a(),m(S.$$.fragment),Pe=a(),q=p("p"),q.innerHTML=Vt,Ke=a(),m(L.$$.fragment),De=a(),m(P.$$.fragment),Oe=a(),K=p("p"),K.innerHTML=Yt,et=a(),D=p("p"),D.innerHTML=Et,tt=a(),m(O.$$.fragment),lt=a(),m(ee.$$.fragment),st=a(),te=p("p"),te.innerHTML=zt,at=a(),le=p("p"),le.innerHTML=Qt,nt=a(),m(se.$$.fragment),pt=a(),ae=p("p"),ae.textContent=Nt,it=a(),m(ne.$$.fragment),Mt=a(),pe=p("p"),pe.innerHTML=Ft,mt=a(),m(ie.$$.fragment),rt=a(),Me=p("p"),Me.textContent=At,ot=a(),me=p("p"),me.innerHTML=St,ct=a(),m(re.$$.fragment),yt=a(),m(oe.$$.fragment),dt=a(),ce=p("p"),ce.innerHTML=qt,Jt=a(),m(ye.$$.fragment),wt=a(),T=p("div"),T.innerHTML=Lt,ft=nl(`
    
모델에 사용할 이미지를 준비합니다.

	`),m(de.$$.fragment),ut=a(),Je=p("p"),Je.innerHTML=Pt,bt=a(),m(we.$$.fragment),Tt=a(),m(fe.$$.fragment),Ut=a(),ue=p("p"),ue.textContent=Kt,ht=a(),Ue=p("p"),this.h()},l(e){const t=pl("svelte-u9bgzb",document.head);w=i(t,"META",{name:!0,content:!0}),t.forEach(l),U=n(e),f=i(e,"P",{}),Ot(f).forEach(l),Te=n(e),r(h.$$.fragment,e),je=n(e),r(j.$$.fragment,e),ge=n(e),g=i(e,"P",{"data-svelte-h":!0}),M(g)!=="svelte-1sz2sm4"&&(g.textContent=$t),$e=n(e),$=i(e,"P",{"data-svelte-h":!0}),M($)!=="svelte-tq3033"&&($.textContent=Ct),Ce=n(e),C=i(e,"UL",{"data-svelte-h":!0}),M(C)!=="svelte-htt0z1"&&(C.innerHTML=_t),_e=n(e),_=i(e,"P",{"data-svelte-h":!0}),M(_)!=="svelte-18iigii"&&(_.textContent=kt),ke=n(e),r(k.$$.fragment,e),Ie=n(e),I=i(e,"P",{"data-svelte-h":!0}),M(I)!=="svelte-9xxxb4"&&(I.textContent=It),Be=n(e),r(B.$$.fragment,e),We=n(e),r(W.$$.fragment,e),Ze=n(e),Z=i(e,"P",{"data-svelte-h":!0}),M(Z)!=="svelte-fdtyxx"&&(Z.innerHTML=Bt),ve=n(e),r(v.$$.fragment,e),Ge=n(e),r(G.$$.fragment,e),xe=n(e),x=i(e,"P",{"data-svelte-h":!0}),M(x)!=="svelte-1ff7g97"&&(x.innerHTML=Wt),Re=n(e),r(u.$$.fragment,e),Xe=n(e),R=i(e,"P",{"data-svelte-h":!0}),M(R)!=="svelte-1pxdo64"&&(R.innerHTML=Zt),He=n(e),r(X.$$.fragment,e),Ve=n(e),H=i(e,"P",{"data-svelte-h":!0}),M(H)!=="svelte-1xgsiym"&&(H.textContent=vt),Ye=n(e),r(V.$$.fragment,e),Ee=n(e),b=i(e,"DIV",{class:!0,"data-svelte-h":!0}),M(b)!=="svelte-1qemygy"&&(b.innerHTML=Gt),ze=n(e),r(Y.$$.fragment,e),Qe=n(e),E=i(e,"P",{"data-svelte-h":!0}),M(E)!=="svelte-15c40fh"&&(E.textContent=xt),Ne=n(e),z=i(e,"P",{"data-svelte-h":!0}),M(z)!=="svelte-aj1022"&&(z.textContent=Rt),Fe=n(e),r(Q.$$.fragment,e),Ae=n(e),N=i(e,"P",{"data-svelte-h":!0}),M(N)!=="svelte-lek9ik"&&(N.textContent=Xt),Se=n(e),r(F.$$.fragment,e),qe=n(e),A=i(e,"P",{"data-svelte-h":!0}),M(A)!=="svelte-x9apz1"&&(A.textContent=Ht),Le=n(e),r(S.$$.fragment,e),Pe=n(e),q=i(e,"P",{"data-svelte-h":!0}),M(q)!=="svelte-1ifqfke"&&(q.innerHTML=Vt),Ke=n(e),r(L.$$.fragment,e),De=n(e),r(P.$$.fragment,e),Oe=n(e),K=i(e,"P",{"data-svelte-h":!0}),M(K)!=="svelte-1i0maa4"&&(K.innerHTML=Yt),et=n(e),D=i(e,"P",{"data-svelte-h":!0}),M(D)!=="svelte-gcfhpv"&&(D.innerHTML=Et),tt=n(e),r(O.$$.fragment,e),lt=n(e),r(ee.$$.fragment,e),st=n(e),te=i(e,"P",{"data-svelte-h":!0}),M(te)!=="svelte-1p0jzke"&&(te.innerHTML=zt),at=n(e),le=i(e,"P",{"data-svelte-h":!0}),M(le)!=="svelte-1rmrex2"&&(le.innerHTML=Qt),nt=n(e),r(se.$$.fragment,e),pt=n(e),ae=i(e,"P",{"data-svelte-h":!0}),M(ae)!=="svelte-14i42sb"&&(ae.textContent=Nt),it=n(e),r(ne.$$.fragment,e),Mt=n(e),pe=i(e,"P",{"data-svelte-h":!0}),M(pe)!=="svelte-14yorud"&&(pe.innerHTML=Ft),mt=n(e),r(ie.$$.fragment,e),rt=n(e),Me=i(e,"P",{"data-svelte-h":!0}),M(Me)!=="svelte-cnq009"&&(Me.textContent=At),ot=n(e),me=i(e,"P",{"data-svelte-h":!0}),M(me)!=="svelte-106657s"&&(me.innerHTML=St),ct=n(e),r(re.$$.fragment,e),yt=n(e),r(oe.$$.fragment,e),dt=n(e),ce=i(e,"P",{"data-svelte-h":!0}),M(ce)!=="svelte-noj8gs"&&(ce.innerHTML=qt),Jt=n(e),r(ye.$$.fragment,e),wt=n(e),T=i(e,"DIV",{class:!0,"data-svelte-h":!0}),M(T)!=="svelte-yvzmn4"&&(T.innerHTML=Lt),ft=il(e,`
    
모델에 사용할 이미지를 준비합니다.

	`),r(de.$$.fragment,e),ut=n(e),Je=i(e,"P",{"data-svelte-h":!0}),M(Je)!=="svelte-1o736s1"&&(Je.innerHTML=Pt),bt=n(e),r(we.$$.fragment,e),Tt=n(e),r(fe.$$.fragment,e),Ut=n(e),ue=i(e,"P",{"data-svelte-h":!0}),M(ue)!=="svelte-1ye6o6q"&&(ue.textContent=Kt),ht=n(e),Ue=i(e,"P",{}),Ot(Ue).forEach(l),this.h()},h(){gt(w,"name","hf:doc:metadata"),gt(w,"content",yl),gt(b,"class","flex justify-center"),gt(T,"class","flex justify-center")},m(e,t){Ml(document.head,w),s(e,U,t),s(e,f,t),s(e,Te,t),o(h,e,t),s(e,je,t),o(j,e,t),s(e,ge,t),s(e,g,t),s(e,$e,t),s(e,$,t),s(e,Ce,t),s(e,C,t),s(e,_e,t),s(e,_,t),s(e,ke,t),o(k,e,t),s(e,Ie,t),s(e,I,t),s(e,Be,t),o(B,e,t),s(e,We,t),o(W,e,t),s(e,Ze,t),s(e,Z,t),s(e,ve,t),o(v,e,t),s(e,Ge,t),o(G,e,t),s(e,xe,t),s(e,x,t),s(e,Re,t),o(u,e,t),s(e,Xe,t),s(e,R,t),s(e,He,t),o(X,e,t),s(e,Ve,t),s(e,H,t),s(e,Ye,t),o(V,e,t),s(e,Ee,t),s(e,b,t),s(e,ze,t),o(Y,e,t),s(e,Qe,t),s(e,E,t),s(e,Ne,t),s(e,z,t),s(e,Fe,t),o(Q,e,t),s(e,Ae,t),s(e,N,t),s(e,Se,t),o(F,e,t),s(e,qe,t),s(e,A,t),s(e,Le,t),o(S,e,t),s(e,Pe,t),s(e,q,t),s(e,Ke,t),o(L,e,t),s(e,De,t),o(P,e,t),s(e,Oe,t),s(e,K,t),s(e,et,t),s(e,D,t),s(e,tt,t),o(O,e,t),s(e,lt,t),o(ee,e,t),s(e,st,t),s(e,te,t),s(e,at,t),s(e,le,t),s(e,nt,t),o(se,e,t),s(e,pt,t),s(e,ae,t),s(e,it,t),o(ne,e,t),s(e,Mt,t),s(e,pe,t),s(e,mt,t),o(ie,e,t),s(e,rt,t),s(e,Me,t),s(e,ot,t),s(e,me,t),s(e,ct,t),o(re,e,t),s(e,yt,t),o(oe,e,t),s(e,dt,t),s(e,ce,t),s(e,Jt,t),o(ye,e,t),s(e,wt,t),s(e,T,t),s(e,ft,t),o(de,e,t),s(e,ut,t),s(e,Je,t),s(e,bt,t),o(we,e,t),s(e,Tt,t),o(fe,e,t),s(e,Ut,t),s(e,ue,t),s(e,ht,t),s(e,Ue,t),jt=!0},p(e,[t]){const Dt={};t&2&&(Dt.$$scope={dirty:t,ctx:e}),u.$set(Dt)},i(e){jt||(c(h.$$.fragment,e),c(j.$$.fragment,e),c(k.$$.fragment,e),c(B.$$.fragment,e),c(W.$$.fragment,e),c(v.$$.fragment,e),c(G.$$.fragment,e),c(u.$$.fragment,e),c(X.$$.fragment,e),c(V.$$.fragment,e),c(Y.$$.fragment,e),c(Q.$$.fragment,e),c(F.$$.fragment,e),c(S.$$.fragment,e),c(L.$$.fragment,e),c(P.$$.fragment,e),c(O.$$.fragment,e),c(ee.$$.fragment,e),c(se.$$.fragment,e),c(ne.$$.fragment,e),c(ie.$$.fragment,e),c(re.$$.fragment,e),c(oe.$$.fragment,e),c(ye.$$.fragment,e),c(de.$$.fragment,e),c(we.$$.fragment,e),c(fe.$$.fragment,e),jt=!0)},o(e){y(h.$$.fragment,e),y(j.$$.fragment,e),y(k.$$.fragment,e),y(B.$$.fragment,e),y(W.$$.fragment,e),y(v.$$.fragment,e),y(G.$$.fragment,e),y(u.$$.fragment,e),y(X.$$.fragment,e),y(V.$$.fragment,e),y(Y.$$.fragment,e),y(Q.$$.fragment,e),y(F.$$.fragment,e),y(S.$$.fragment,e),y(L.$$.fragment,e),y(P.$$.fragment,e),y(O.$$.fragment,e),y(ee.$$.fragment,e),y(se.$$.fragment,e),y(ne.$$.fragment,e),y(ie.$$.fragment,e),y(re.$$.fragment,e),y(oe.$$.fragment,e),y(ye.$$.fragment,e),y(de.$$.fragment,e),y(we.$$.fragment,e),y(fe.$$.fragment,e),jt=!1},d(e){e&&(l(U),l(f),l(Te),l(je),l(ge),l(g),l($e),l($),l(Ce),l(C),l(_e),l(_),l(ke),l(Ie),l(I),l(Be),l(We),l(Ze),l(Z),l(ve),l(Ge),l(xe),l(x),l(Re),l(Xe),l(R),l(He),l(Ve),l(H),l(Ye),l(Ee),l(b),l(ze),l(Qe),l(E),l(Ne),l(z),l(Fe),l(Ae),l(N),l(Se),l(qe),l(A),l(Le),l(Pe),l(q),l(Ke),l(De),l(Oe),l(K),l(et),l(D),l(tt),l(lt),l(st),l(te),l(at),l(le),l(nt),l(pt),l(ae),l(it),l(Mt),l(pe),l(mt),l(rt),l(Me),l(ot),l(me),l(ct),l(yt),l(dt),l(ce),l(Jt),l(wt),l(T),l(ft),l(ut),l(Je),l(bt),l(Tt),l(Ut),l(ue),l(ht),l(Ue)),l(w),d(h,e),d(j,e),d(k,e),d(B,e),d(W,e),d(v,e),d(G,e),d(u,e),d(X,e),d(V,e),d(Y,e),d(Q,e),d(F,e),d(S,e),d(L,e),d(P,e),d(O,e),d(ee,e),d(se,e),d(ne,e),d(ie,e),d(re,e),d(oe,e),d(ye,e),d(de,e),d(we,e),d(fe,e)}}}const yl='{"title":"이미지 캡셔닝","local":"image-captioning","sections":[{"title":"포켓몬 BLIP 캡션 데이터세트 가져오기","local":"load-the-pokmon-blip-captions-dataset","sections":[],"depth":2},{"title":"데이터세트 전처리","local":"preprocess-the-dataset","sections":[],"depth":2},{"title":"기본 모델 가져오기","local":"load-a-base-model","sections":[],"depth":2},{"title":"평가","local":"evaluate","sections":[],"depth":2},{"title":"학습!","local":"train!","sections":[],"depth":2},{"title":"추론","local":"inference","sections":[],"depth":2}],"depth":1}';function dl(he){return tl(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ul extends sl{constructor(w){super(),al(this,w,dl,cl,el,{})}}export{Ul as component};
