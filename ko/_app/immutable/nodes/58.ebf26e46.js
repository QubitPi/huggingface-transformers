import{s as qt,o as At,n as Kt}from"../chunks/scheduler.56730f09.js";import{S as Ot,i as te,g as p,s as a,r as o,A as ee,h as i,f as s,c as n,j as Dt,u as g,x as m,k as A,y as se,a as l,v as r,d,t as f,w as u,m as le,n as ae}from"../chunks/index.1f144517.js";import{T as ne}from"../chunks/Tip.41e845e5.js";import{C as M}from"../chunks/CodeBlock.738eeccb.js";import{H as kt}from"../chunks/Heading.57d46534.js";function pe(K){let c,h,w='<a href="../model_doc/dpt">DPT</a>, <a href="../model_doc/glpn">GLPN</a>';return{c(){c=le(`이 튜토리얼에서 다루는 작업은 다음 모델 아키텍처에서 지원됩니다:

`),h=p("p"),h.innerHTML=w},l(y){c=ae(y,`이 튜토리얼에서 다루는 작업은 다음 모델 아키텍처에서 지원됩니다:

`),h=i(y,"P",{"data-svelte-h":!0}),m(h)!=="svelte-940txo"&&(h.innerHTML=w)},m(y,J){l(y,c,J),l(y,h,J)},p:Kt,d(y){y&&(s(c),s(h))}}}function ie(K){let c,h,w,y,J,O,b,Wt=`단일 영상 기반 깊이 추정은 한 장면의 단일 이미지에서 장면의 깊이 정보를 예측하는 컴퓨터 비전 작업입니다.
즉, 단일 카메라 시점의 장면에 있는 물체의 거리를 예측하는 과정입니다.`,tt,v,Gt=`단일 영상 기반 깊이 추정은 3D 재구성, 증강 현실, 자율 주행, 로봇 공학 등 다양한 분야에서 응용됩니다.
조명 조건, 가려짐, 텍스처와 같은 요소의 영향을 받을 수 있는 장면 내 물체와 해당 깊이 정보 간의 복잡한 관계를 모델이 이해해야 하므로 까다로운 작업입니다.`,et,T,st,C,_t="이번 가이드에서 배울 내용은 다음과 같습니다:",lt,Z,It="<li>깊이 추정 파이프라인 만들기</li> <li>직접 깊이 추정 추론하기</li>",at,x,Bt="시작하기 전에, 필요한 모든 라이브러리가 설치되어 있는지 확인하세요:",nt,k,pt,W,it,G,Rt=`깊이 추정을 추론하는 가장 간단한 방법은 해당 기능을 제공하는 <code>pipeline()</code>을 사용하는 것입니다.
<a href="https://huggingface.co/models?pipeline_tag=depth-estimation&amp;sort=downloads" rel="nofollow">Hugging Face Hub 체크포인트</a>에서 파이프라인을 초기화합니다:`,mt,_,ct,I,Vt="다음으로, 분석할 이미지를 한 장 선택하세요:",ot,B,gt,$,Ht='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-estimation-example.jpg" alt="Photo of a busy street"/>',rt,R,Nt="이미지를 파이프라인으로 전달합니다.",dt,V,ft,H,Et=`파이프라인은 두 개의 항목을 가지는 딕셔너리를 반환합니다.
첫 번째는 <code>predicted_depth</code>로 각 픽셀의 깊이를 미터로 표현한 값을 가지는 텐서입니다.
두 번째는 <code>depth</code>로 깊이 추정 결과를 시각화하는 PIL 이미지입니다.`,ut,N,zt="이제 시각화한 결과를 살펴보겠습니다:",ht,E,yt,U,Ft='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-visualization.png" alt="Depth estimation visualization"/>',Jt,z,Mt,F,Xt=`이제 깊이 추정 파이프라인 사용법을 살펴보았으니 동일한 결과를 복제하는 방법을 살펴보겠습니다.
<a href="https://huggingface.co/models?pipeline_tag=depth-estimation&amp;sort=downloads" rel="nofollow">Hugging Face Hub 체크포인트</a>에서 모델과 관련 프로세서를 가져오는 것부터 시작합니다.
여기서 이전에 사용한 체크포인트와 동일한 것을 사용합니다:`,wt,X,Tt,Y,Yt=`필요한 이미지 변환을 처리하는 <code>image_processor</code>를 사용하여 모델에 대한 이미지 입력을 준비합니다.
<code>image_processor</code>는 크기 조정 및 정규화 등 필요한 이미지 변환을 처리합니다:`,$t,Q,Ut,S,Qt="준비한 입력을 모델로 전달합니다:",jt,L,bt,P,St="결과를 시각화합니다:",vt,D,Ct,j,Lt='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-visualization.png" alt="Depth estimation visualization"/>',Zt,q,xt;return J=new kt({props:{title:"단일 영상 기반 깊이 추정",local:"depth-estimation-pipeline",headingTag:"h1"}}),T=new ne({props:{$$slots:{default:[pe]},$$scope:{ctx:K}}}),k=new M({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1xJTIwdHJhbnNmb3JtZXJz",highlighted:"pip install -q transformers",wrap:!1}}),W=new kt({props:{title:"깊이 추정 파이프라인",local:"depth-estimation-inference-by-hand",headingTag:"h2"}}),_=new M({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2hlY2twb2ludCUyMCUzRCUyMCUyMnZpbnZpbm8wMiUyRmdscG4tbnl1JTIyJTBBZGVwdGhfZXN0aW1hdG9yJTIwJTNEJTIwcGlwZWxpbmUoJTIyZGVwdGgtZXN0aW1hdGlvbiUyMiUyQyUyMG1vZGVsJTNEY2hlY2twb2ludCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;vinvino02/glpn-nyu&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>depth_estimator = pipeline(<span class="hljs-string">&quot;depth-estimation&quot;</span>, model=checkpoint)`,wrap:!1}}),B=new M({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRnVuc3BsYXNoLmNvbSUyRnBob3RvcyUyRkh3QkFzU2JQQkRVJTJGZG93bmxvYWQlM0ZpeGlkJTNETW53eE1qQTNmREI4TVh4elpXRnlZMmg4TXpSOGZHTmhjaVV5TUdsdUpUSXdkR2hsSlRJd2MzUnlaV1YwZkdWdWZEQjhNSHg4ZkRFMk56ZzVNREV3T0RnJTI2Zm9yY2UlM0R0cnVlJTI2dyUzRDY0MCUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQWltYWdl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://unsplash.com/photos/HwBAsSbPBDU/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MzR8fGNhciUyMGluJTIwdGhlJTIwc3RyZWV0fGVufDB8MHx8fDE2Nzg5MDEwODg&amp;force=true&amp;w=640&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>image`,wrap:!1}}),V=new M({props:{code:"cHJlZGljdGlvbnMlMjAlM0QlMjBkZXB0aF9lc3RpbWF0b3IoaW1hZ2Up",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>predictions = depth_estimator(image)',wrap:!1}}),E=new M({props:{code:"cHJlZGljdGlvbnMlNUIlMjJkZXB0aCUyMiU1RA==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>predictions[<span class="hljs-string">&quot;depth&quot;</span>]',wrap:!1}}),z=new kt({props:{title:"직접 깊이 추정 추론하기",local:"depth-estimation-inference-by-hand",headingTag:"h2"}}),X=new M({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbiUwQSUwQWNoZWNrcG9pbnQlMjAlM0QlMjAlMjJ2aW52aW5vMDIlMkZnbHBuLW55dSUyMiUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;vinvino02/glpn-nyu&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(checkpoint)`,wrap:!1}}),Q=new M({props:{code:"cGl4ZWxfdmFsdWVzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikucGl4ZWxfdmFsdWVz",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values',wrap:!1}}),L=new M({props:{code:"aW1wb3J0JTIwdG9yY2glMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKHBpeGVsX3ZhbHVlcyklMEElMjAlMjAlMjAlMjBwcmVkaWN0ZWRfZGVwdGglMjAlM0QlMjBvdXRwdXRzLnByZWRpY3RlZF9kZXB0aA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(pixel_values)
<span class="hljs-meta">... </span>    predicted_depth = outputs.predicted_depth`,wrap:!1}}),D=new M({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTIzJTIwJUVDJTlCJTkwJUVCJUIzJUI4JTIwJUVDJTgyJUFDJUVDJTlEJUI0JUVDJUE2JTg4JUVCJUExJTlDJTIwJUVCJUIzJUI1JUVDJTlCJTkwJTBBcHJlZGljdGlvbiUyMCUzRCUyMHRvcmNoLm5uLmZ1bmN0aW9uYWwuaW50ZXJwb2xhdGUoJTBBJTIwJTIwJTIwJTIwcHJlZGljdGVkX2RlcHRoLnVuc3F1ZWV6ZSgxKSUyQyUwQSUyMCUyMCUyMCUyMHNpemUlM0RpbWFnZS5zaXplJTVCJTNBJTNBLTElNUQlMkMlMEElMjAlMjAlMjAlMjBtb2RlJTNEJTIyYmljdWJpYyUyMiUyQyUwQSUyMCUyMCUyMCUyMGFsaWduX2Nvcm5lcnMlM0RGYWxzZSUyQyUwQSkuc3F1ZWV6ZSgpJTBBb3V0cHV0JTIwJTNEJTIwcHJlZGljdGlvbi5udW1weSgpJTBBJTBBZm9ybWF0dGVkJTIwJTNEJTIwKG91dHB1dCUyMColMjAyNTUlMjAlMkYlMjBucC5tYXgob3V0cHV0KSkuYXN0eXBlKCUyMnVpbnQ4JTIyKSUwQWRlcHRoJTIwJTNEJTIwSW1hZ2UuZnJvbWFycmF5KGZvcm1hdHRlZCklMEFkZXB0aA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 원본 사이즈로 복원</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prediction = torch.nn.functional.interpolate(
<span class="hljs-meta">... </span>    predicted_depth.unsqueeze(<span class="hljs-number">1</span>),
<span class="hljs-meta">... </span>    size=image.size[::-<span class="hljs-number">1</span>],
<span class="hljs-meta">... </span>    mode=<span class="hljs-string">&quot;bicubic&quot;</span>,
<span class="hljs-meta">... </span>    align_corners=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>).squeeze()
<span class="hljs-meta">&gt;&gt;&gt; </span>output = prediction.numpy()

<span class="hljs-meta">&gt;&gt;&gt; </span>formatted = (output * <span class="hljs-number">255</span> / np.<span class="hljs-built_in">max</span>(output)).astype(<span class="hljs-string">&quot;uint8&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = Image.fromarray(formatted)
<span class="hljs-meta">&gt;&gt;&gt; </span>depth`,wrap:!1}}),{c(){c=p("meta"),h=a(),w=p("p"),y=a(),o(J.$$.fragment),O=a(),b=p("p"),b.textContent=Wt,tt=a(),v=p("p"),v.textContent=Gt,et=a(),o(T.$$.fragment),st=a(),C=p("p"),C.textContent=_t,lt=a(),Z=p("ul"),Z.innerHTML=It,at=a(),x=p("p"),x.textContent=Bt,nt=a(),o(k.$$.fragment),pt=a(),o(W.$$.fragment),it=a(),G=p("p"),G.innerHTML=Rt,mt=a(),o(_.$$.fragment),ct=a(),I=p("p"),I.textContent=Vt,ot=a(),o(B.$$.fragment),gt=a(),$=p("div"),$.innerHTML=Ht,rt=a(),R=p("p"),R.textContent=Nt,dt=a(),o(V.$$.fragment),ft=a(),H=p("p"),H.innerHTML=Et,ut=a(),N=p("p"),N.textContent=zt,ht=a(),o(E.$$.fragment),yt=a(),U=p("div"),U.innerHTML=Ft,Jt=a(),o(z.$$.fragment),Mt=a(),F=p("p"),F.innerHTML=Xt,wt=a(),o(X.$$.fragment),Tt=a(),Y=p("p"),Y.innerHTML=Yt,$t=a(),o(Q.$$.fragment),Ut=a(),S=p("p"),S.textContent=Qt,jt=a(),o(L.$$.fragment),bt=a(),P=p("p"),P.textContent=St,vt=a(),o(D.$$.fragment),Ct=a(),j=p("div"),j.innerHTML=Lt,Zt=a(),q=p("p"),this.h()},l(t){const e=ee("svelte-u9bgzb",document.head);c=i(e,"META",{name:!0,content:!0}),e.forEach(s),h=n(t),w=i(t,"P",{}),Dt(w).forEach(s),y=n(t),g(J.$$.fragment,t),O=n(t),b=i(t,"P",{"data-svelte-h":!0}),m(b)!=="svelte-18h62fn"&&(b.textContent=Wt),tt=n(t),v=i(t,"P",{"data-svelte-h":!0}),m(v)!=="svelte-jyvqzi"&&(v.textContent=Gt),et=n(t),g(T.$$.fragment,t),st=n(t),C=i(t,"P",{"data-svelte-h":!0}),m(C)!=="svelte-pdyjx9"&&(C.textContent=_t),lt=n(t),Z=i(t,"UL",{"data-svelte-h":!0}),m(Z)!=="svelte-1c78wn8"&&(Z.innerHTML=It),at=n(t),x=i(t,"P",{"data-svelte-h":!0}),m(x)!=="svelte-1bc8bfk"&&(x.textContent=Bt),nt=n(t),g(k.$$.fragment,t),pt=n(t),g(W.$$.fragment,t),it=n(t),G=i(t,"P",{"data-svelte-h":!0}),m(G)!=="svelte-euk5zr"&&(G.innerHTML=Rt),mt=n(t),g(_.$$.fragment,t),ct=n(t),I=i(t,"P",{"data-svelte-h":!0}),m(I)!=="svelte-16p0hj9"&&(I.textContent=Vt),ot=n(t),g(B.$$.fragment,t),gt=n(t),$=i(t,"DIV",{class:!0,"data-svelte-h":!0}),m($)!=="svelte-10bakl"&&($.innerHTML=Ht),rt=n(t),R=i(t,"P",{"data-svelte-h":!0}),m(R)!=="svelte-85qecv"&&(R.textContent=Nt),dt=n(t),g(V.$$.fragment,t),ft=n(t),H=i(t,"P",{"data-svelte-h":!0}),m(H)!=="svelte-wagku9"&&(H.innerHTML=Et),ut=n(t),N=i(t,"P",{"data-svelte-h":!0}),m(N)!=="svelte-orw6zi"&&(N.textContent=zt),ht=n(t),g(E.$$.fragment,t),yt=n(t),U=i(t,"DIV",{class:!0,"data-svelte-h":!0}),m(U)!=="svelte-43wxxb"&&(U.innerHTML=Ft),Jt=n(t),g(z.$$.fragment,t),Mt=n(t),F=i(t,"P",{"data-svelte-h":!0}),m(F)!=="svelte-1ktk46w"&&(F.innerHTML=Xt),wt=n(t),g(X.$$.fragment,t),Tt=n(t),Y=i(t,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-1hncez5"&&(Y.innerHTML=Yt),$t=n(t),g(Q.$$.fragment,t),Ut=n(t),S=i(t,"P",{"data-svelte-h":!0}),m(S)!=="svelte-1wdrd3"&&(S.textContent=Qt),jt=n(t),g(L.$$.fragment,t),bt=n(t),P=i(t,"P",{"data-svelte-h":!0}),m(P)!=="svelte-k6e5ui"&&(P.textContent=St),vt=n(t),g(D.$$.fragment,t),Ct=n(t),j=i(t,"DIV",{class:!0,"data-svelte-h":!0}),m(j)!=="svelte-43wxxb"&&(j.innerHTML=Lt),Zt=n(t),q=i(t,"P",{}),Dt(q).forEach(s),this.h()},h(){A(c,"name","hf:doc:metadata"),A(c,"content",me),A($,"class","flex justify-center"),A(U,"class","flex justify-center"),A(j,"class","flex justify-center")},m(t,e){se(document.head,c),l(t,h,e),l(t,w,e),l(t,y,e),r(J,t,e),l(t,O,e),l(t,b,e),l(t,tt,e),l(t,v,e),l(t,et,e),r(T,t,e),l(t,st,e),l(t,C,e),l(t,lt,e),l(t,Z,e),l(t,at,e),l(t,x,e),l(t,nt,e),r(k,t,e),l(t,pt,e),r(W,t,e),l(t,it,e),l(t,G,e),l(t,mt,e),r(_,t,e),l(t,ct,e),l(t,I,e),l(t,ot,e),r(B,t,e),l(t,gt,e),l(t,$,e),l(t,rt,e),l(t,R,e),l(t,dt,e),r(V,t,e),l(t,ft,e),l(t,H,e),l(t,ut,e),l(t,N,e),l(t,ht,e),r(E,t,e),l(t,yt,e),l(t,U,e),l(t,Jt,e),r(z,t,e),l(t,Mt,e),l(t,F,e),l(t,wt,e),r(X,t,e),l(t,Tt,e),l(t,Y,e),l(t,$t,e),r(Q,t,e),l(t,Ut,e),l(t,S,e),l(t,jt,e),r(L,t,e),l(t,bt,e),l(t,P,e),l(t,vt,e),r(D,t,e),l(t,Ct,e),l(t,j,e),l(t,Zt,e),l(t,q,e),xt=!0},p(t,[e]){const Pt={};e&2&&(Pt.$$scope={dirty:e,ctx:t}),T.$set(Pt)},i(t){xt||(d(J.$$.fragment,t),d(T.$$.fragment,t),d(k.$$.fragment,t),d(W.$$.fragment,t),d(_.$$.fragment,t),d(B.$$.fragment,t),d(V.$$.fragment,t),d(E.$$.fragment,t),d(z.$$.fragment,t),d(X.$$.fragment,t),d(Q.$$.fragment,t),d(L.$$.fragment,t),d(D.$$.fragment,t),xt=!0)},o(t){f(J.$$.fragment,t),f(T.$$.fragment,t),f(k.$$.fragment,t),f(W.$$.fragment,t),f(_.$$.fragment,t),f(B.$$.fragment,t),f(V.$$.fragment,t),f(E.$$.fragment,t),f(z.$$.fragment,t),f(X.$$.fragment,t),f(Q.$$.fragment,t),f(L.$$.fragment,t),f(D.$$.fragment,t),xt=!1},d(t){t&&(s(h),s(w),s(y),s(O),s(b),s(tt),s(v),s(et),s(st),s(C),s(lt),s(Z),s(at),s(x),s(nt),s(pt),s(it),s(G),s(mt),s(ct),s(I),s(ot),s(gt),s($),s(rt),s(R),s(dt),s(ft),s(H),s(ut),s(N),s(ht),s(yt),s(U),s(Jt),s(Mt),s(F),s(wt),s(Tt),s(Y),s($t),s(Ut),s(S),s(jt),s(bt),s(P),s(vt),s(Ct),s(j),s(Zt),s(q)),s(c),u(J,t),u(T,t),u(k,t),u(W,t),u(_,t),u(B,t),u(V,t),u(E,t),u(z,t),u(X,t),u(Q,t),u(L,t),u(D,t)}}}const me='{"title":"단일 영상 기반 깊이 추정","local":"depth-estimation-pipeline","sections":[{"title":"깊이 추정 파이프라인","local":"depth-estimation-inference-by-hand","sections":[],"depth":2},{"title":"직접 깊이 추정 추론하기","local":"depth-estimation-inference-by-hand","sections":[],"depth":2}],"depth":1}';function ce(K){return At(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ue extends Ot{constructor(c){super(),te(this,c,ce,ie,qt,{})}}export{ue as component};
