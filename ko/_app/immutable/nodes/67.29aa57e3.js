import{s as be,o as Ce,n as ue}from"../chunks/scheduler.56730f09.js";import{S as fe,i as ge,g as p,s as t,r as m,A as Ze,h as M,f as a,c as n,j as Ie,u as y,x as i,k as ga,y as ve,a as e,v as c,d as J,t as j,w as o}from"../chunks/index.1f144517.js";import{T as Ge}from"../chunks/Tip.41e845e5.js";import{C as w}from"../chunks/CodeBlock.738eeccb.js";import{D as We}from"../chunks/DocNotebookDropdown.243c3df7.js";import{H as qs}from"../chunks/Heading.57d46534.js";function Be(Ks){let U,f="이 튜토리얼에서 설명하는 작업은 다음 모델 아키텍처에서 지원됩니다:",d,h,I='<a href="../model_doc/timesformer">TimeSformer</a>, <a href="../model_doc/videomae">VideoMAE</a>';return{c(){U=p("p"),U.textContent=f,d=t(),h=p("p"),h.innerHTML=I},l(T){U=M(T,"P",{"data-svelte-h":!0}),i(U)!=="svelte-ce6bst"&&(U.textContent=f),d=n(T),h=M(T,"P",{"data-svelte-h":!0}),i(h)!=="svelte-wycmca"&&(h.innerHTML=I)},m(T,r){e(T,U,r),e(T,d,r),e(T,h,r)},p:ue,d(T){T&&(a(U),a(d),a(h))}}}function _e(Ks){let U,f,d,h,I,T,r,Ps,g,Za="영상 분류는 영상 전체에 레이블 또는 클래스를 지정하는 작업입니다. 각 영상에는 하나의 클래스가 있을 것으로 예상됩니다. 영상 분류 모델은 영상을 입력으로 받아 어느 클래스에 속하는지에 대한 예측을 반환합니다. 이러한 모델은 영상이 어떤 내용인지 분류하는 데 사용될 수 있습니다. 영상 분류의 실제 응용 예는 피트니스 앱에서 유용한 동작 / 운동 인식 서비스가 있습니다. 이는 또한 시각 장애인이 이동할 때 보조하는데 사용될 수 있습니다",Os,Z,va="이 가이드에서는 다음을 수행하는 방법을 보여줍니다:",sl,v,Ga='<li><a href="https://www.crcv.ucf.edu/data/UCF101.php" rel="nofollow">UCF101</a> 데이터 세트의 하위 집합을 통해 <a href="https://huggingface.co/docs/transformers/main/en/model_doc/videomae" rel="nofollow">VideoMAE</a> 모델을 미세 조정하기.</li> <li>미세 조정한 모델을 추론에 사용하기.</li>',ll,b,al,G,Wa="시작하기 전에 필요한 모든 라이브러리가 설치되었는지 확인하세요:",el,W,tl,B,Ba='영상을 처리하고 준비하기 위해 <a href="https://pytorchvideo.org/" rel="nofollow">PyTorchVideo</a>(이하 <code>pytorchvideo</code>)를 사용합니다.',nl,_,_a="커뮤니티에 모델을 업로드하고 공유할 수 있도록 Hugging Face 계정에 로그인하는 것을 권장합니다. 프롬프트가 나타나면 토큰을 입력하여 로그인하세요:",pl,A,Ml,k,il,V,Aa='<a href="https://www.crcv.ucf.edu/data/UCF101.php" rel="nofollow">UCF-101</a> 데이터 세트의 하위 집합(subset)을 불러오는 것으로 시작할 수 있습니다. 전체 데이터 세트를 학습하는데 더 많은 시간을 할애하기 전에 데이터의 하위 집합을 불러와 모든 것이 잘 작동하는지 실험하고 확인할 수 있습니다.',ml,R,yl,x,ka="데이터 세트의 하위 집합이 다운로드 되면, 압축된 파일의 압축을 해제해야 합니다:",cl,X,Jl,$,Va="전체 데이터 세트는 다음과 같이 구성되어 있습니다.",jl,F,ol,Y,Ra="정렬된 영상의 경로는 다음과 같습니다:",wl,E,Ul,z,xa="동일한 그룹/장면에 속하는 영상 클립은 파일 경로에서 <code>g</code>로 표시되어 있습니다. 예를 들면, <code>v_ApplyEyeMakeup_g07_c04.avi</code>와 <code>v_ApplyEyeMakeup_g07_c06.avi</code> 이 있습니다. 이 둘은 같은 그룹입니다.",Tl,S,Xa='검증 및 평가 데이터 분할을 할 때, <a href="https://www.kaggle.com/code/alexisbcook/data-leakage" rel="nofollow">데이터 누출(data leakage)</a>을 방지하기 위해 동일한 그룹 / 장면의 영상 클립을 사용하지 않아야 합니다. 이 튜토리얼에서 사용하는 하위 집합은 이러한 정보를 고려하고 있습니다.',hl,Q,$a="그 다음으로, 데이터 세트에 존재하는 라벨을 추출합니다. 또한, 모델을 초기화할 때 도움이 될 딕셔너리(dictionary data type)를 생성합니다.",rl,N,Fa="<li><code>label2id</code>: 클래스 이름을 정수에 매핑합니다.</li> <li><code>id2label</code>: 정수를 클래스 이름에 매핑합니다.</li>",dl,H,Il,L,Ya="이 데이터 세트에는 총 10개의 고유한 클래스가 있습니다. 각 클래스마다 30개의 영상이 훈련 세트에 있습니다",bl,q,Cl,D,Ea="사전 훈련된 체크포인트와 체크포인트에 연관된 이미지 프로세서를 사용하여 영상 분류 모델을 인스턴스화합니다. 모델의 인코더에는 미리 학습된 매개변수가 제공되며, 분류 헤드(데이터를 분류하는 마지막 레이어)는 무작위로 초기화됩니다. 데이터 세트의 전처리 파이프라인을 작성할 때는 이미지 프로세서가 유용합니다.",ul,K,fl,P,za="모델을 가져오는 동안, 다음과 같은 경고를 마주칠 수 있습니다:",gl,O,Zl,ss,Sa="위 경고는 우리가 일부 가중치(예: <code>classifier</code> 층의 가중치와 편향)를 버리고 새로운 <code>classifier</code> 층의 가중치와 편향을 무작위로 초기화하고 있다는 것을 알려줍니다. 이 경우에는 미리 학습된 가중치가 없는 새로운 헤드를 추가하고 있으므로, 라이브러리가 모델을 추론에 사용하기 전에 미세 조정하라고 경고를 보내는 것은 당연합니다. 그리고 이제 우리는 이 모델을 미세 조정할 예정입니다.",vl,ls,Qa='<strong>참고</strong> 이 <a href="https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics" rel="nofollow">체크포인트</a>는 도메인이 많이 중첩된 유사한 다운스트림 작업에 대해 미세 조정하여 얻은 체크포인트이므로 이 작업에서 더 나은 성능을 보일 수 있습니다. <code>MCG-NJU/videomae-base-finetuned-kinetics</code> 데이터 세트를 미세 조정하여 얻은 <a href="https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset" rel="nofollow">체크포인트</a>도 있습니다.',Gl,as,Wl,es,Na='영상 전처리를 위해 <a href="https://pytorchvideo.org/" rel="nofollow">PyTorchVideo 라이브러리</a>를 활용할 것입니다. 필요한 종속성을 가져오는 것으로 시작하세요.',Bl,ts,_l,ns,Ha='학습 데이터 세트 변환에는 ‘균일한 시간 샘플링(uniform temporal subsampling)’, ‘픽셀 정규화(pixel normalization)’, ‘랜덤 잘라내기(random cropping)’ 및 ‘랜덤 수평 뒤집기(random horizontal flipping)‘의 조합을 사용합니다. 검증 및 평가 데이터 세트 변환에는 ‘랜덤 잘라내기’와 ‘랜덤 뒤집기’를 제외한 동일한 변환 체인을 유지합니다. 이러한 변환에 대해 자세히 알아보려면 <a href="https://pytorchvideo.org" rel="nofollow">PyTorchVideo 공식 문서</a>를 확인하세요.',Al,ps,La="사전 훈련된 모델과 관련된 이미지 프로세서를 사용하여 다음 정보를 얻을 수 있습니다:",kl,Ms,qa="<li>영상 프레임 픽셀을 정규화하는 데 사용되는 이미지 평균과 표준 편차</li> <li>영상 프레임이 조정될 공간 해상도</li>",Vl,is,Da="먼저, 몇 가지 상수를 정의합니다.",Rl,ms,xl,ys,Ka="이제 데이터 세트에 특화된 전처리(transform)과 데이터 세트 자체를 정의합니다. 먼저 훈련 데이터 세트로 시작합니다:",Xl,cs,$l,Js,Pa="같은 방식의 작업 흐름을 검증과 평가 세트에도 적용할 수 있습니다.",Fl,js,Yl,os,Oa='<strong>참고</strong>: 위의 데이터 세트의 파이프라인은 <a href="https://pytorchvideo.org/docs/tutorial_classification#dataset" rel="nofollow">공식 파이토치 예제</a>에서 가져온 것입니다. 우리는 UCF-101 데이터셋에 맞게 <a href="https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101" rel="nofollow"><code>pytorchvideo.data.Ucf101()</code></a> 함수를 사용하고 있습니다. 내부적으로 이 함수는 <a href="https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset" rel="nofollow"><code>pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset</code></a> 객체를 반환합니다. <code>LabeledVideoDataset</code> 클래스는 PyTorchVideo 데이터셋에서 모든 영상 관련 작업의 기본 클래스입니다. 따라서 PyTorchVideo에서 미리 제공하지 않는 사용자 지정 데이터 세트를 사용하려면, 이 클래스를 적절하게 확장하면 됩니다. 더 자세한 사항이 알고 싶다면 <code>data</code> API <a href="https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html" rel="nofollow">문서</a> 를 참고하세요. 또한 위의 예시와 유사한 구조를 갖는 데이터 세트를 사용하고 있다면, <code>pytorchvideo.data.Ucf101()</code> 함수를 사용하는 데 문제가 없을 것입니다.',El,ws,se="데이터 세트에 영상의 개수를 알기 위해 <code>num_videos</code> 인수에 접근할 수 있습니다.",zl,Us,Sl,Ts,Ql,hs,Nl,C,le='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif" alt="Person playing basketball"/>',Hl,rs,Ll,ds,ae='🤗 Transformers의 <a href="https://huggingface.co/docs/transformers/main_classes/trainer" rel="nofollow"><code>Trainer</code></a>를 사용하여 모델을 훈련시켜보세요. <code>Trainer</code>를 인스턴스화하려면 훈련 설정과 평가 지표를 정의해야 합니다.  가장 중요한 것은 <a href="https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments" rel="nofollow"><code>TrainingArguments</code></a>입니다. 이 클래스는 훈련을 구성하는 모든 속성을 포함하며, 훈련 중 체크포인트를 저장할 출력 폴더 이름을 필요로 합니다. 또한 🤗 Hub의 모델 저장소의 모든 정보를 동기화하는 데 도움이 됩니다.',ql,Is,ee="대부분의 훈련 인수는 따로 설명할 필요는 없습니다. 하지만 여기에서 중요한 인수는 <code>remove_unused_columns=False</code> 입니다. 이 인자는 모델의 호출 함수에서 사용되지 않는 모든 속성 열(columns)을 삭제합니다. 기본값은 일반적으로 True입니다. 이는 사용되지 않는 기능 열을 삭제하는 것이 이상적이며, 입력을 모델의 호출 함수로 풀기(unpack)가 쉬워지기 때문입니다. 하지만 이 경우에는 <code>pixel_values</code>(모델의 입력으로 필수적인 키)를 생성하기 위해 사용되지 않는 기능(‘video’가 특히 그렇습니다)이 필요합니다. 따라서 remove_unused_columns을 False로 설정해야 합니다.",Dl,bs,Kl,Cs,te="<code>pytorchvideo.data.Ucf101()</code> 함수로 반환되는 데이터 세트는 <code>__len__</code> 메소드가 이식되어 있지 않습니다. 따라서,  <code>TrainingArguments</code>를 인스턴스화할 때 <code>max_steps</code>를 정의해야 합니다.",Pl,us,ne="다음으로, 평가지표를 불러오고, 예측값에서 평가지표를 계산할 함수를 정의합니다. 필요한 전처리 작업은 예측된 로짓(logits)에 argmax 값을 취하는 것뿐입니다:",Ol,fs,sa,gs,pe="<strong>평가에 대한 참고사항</strong>:",la,Zs,Me='<a href="https://arxiv.org/abs/2203.12602" rel="nofollow">VideoMAE 논문</a>에서 저자는 다음과 같은 평가 전략을 사용합니다. 테스트 영상에서 여러 클립을 선택하고 그 클립에 다양한 크롭을 적용하여 집계 점수를 보고합니다. 그러나 이번 튜토리얼에서는 간단함과 간결함을 위해 해당 전략을 고려하지 않습니다.',aa,vs,ie="또한, 예제를 묶어서 배치를 형성하는 <code>collate_fn</code>을 정의해야합니다. 각 배치는 <code>pixel_values</code>와 <code>labels</code>라는 2개의 키로 구성됩니다.",ea,Gs,ta,Ws,me="그런 다음 이 모든 것을 데이터 세트와 함께 <code>Trainer</code>에 전달하기만 하면 됩니다:",na,Bs,pa,_s,ye="데이터를 이미 처리했는데도 불구하고 <code>image_processor</code>를 토크나이저 인수로 넣은 이유는 JSON으로 저장되는 이미지 프로세서 구성 파일이 Hub의 저장소에 업로드되도록 하기 위함입니다.",Ma,As,ce="<code>train</code> 메소드를 호출하여 모델을 미세 조정하세요:",ia,ks,ma,Vs,Je="학습이 완료되면, 모델을 <code>push_to_hub()</code> 메소드를 사용하여 허브에 공유하여 누구나 모델을 사용할 수 있도록 합니다:",ya,Rs,ca,xs,Ja,Xs,je="좋습니다. 이제 미세 조정된 모델을 추론하는 데 사용할 수 있습니다.",ja,$s,oe="추론에 사용할 영상을 불러오세요:",oa,Fs,wa,u,we='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif" alt="Teams playing basketball"/>',Ua,Ys,Ue='미세 조정된 모델을 추론에 사용하는 가장 간단한 방법은 <a href="https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline" rel="nofollow"><code>pipeline</code></a>에서 모델을 사용하는 것입니다. 모델로 영상 분류를 하기 위해 <code>pipeline</code>을 인스턴스화하고 영상을 전달하세요:',Ta,Es,ha,zs,Te="만약 원한다면 수동으로 <code>pipeline</code>의 결과를 재현할 수 있습니다:",ra,Ss,da,Qs,he="모델에 입력값을 넣고 <code>logits</code>을 반환받으세요:",Ia,Ns,ba,Hs,re="<code>logits</code>을 디코딩하면, 우리는 다음 결과를 얻을 수 있습니다:",Ca,Ls,ua,Ds,fa;return I=new qs({props:{title:"영상 분류",local:"video-classification",headingTag:"h1"}}),r=new We({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/video_classification.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/video_classification.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/video_classification.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/video_classification.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/video_classification.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/video_classification.ipynb"}]}}),b=new Ge({props:{$$slots:{default:[Be]},$$scope:{ctx:Ks}}}),W=new w({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1xJTIwcHl0b3JjaHZpZGVvJTIwdHJhbnNmb3JtZXJzJTIwZXZhbHVhdGU=",highlighted:"pip install -q pytorchvideo transformers evaluate",wrap:!1}}),A=new w({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),k=new qs({props:{title:"UCF101 데이터셋 불러오기",local:"load-ufc101-dataset",headingTag:"h2"}}),R=new w({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMGhmX2h1Yl9kb3dubG9hZCUwQSUwQWhmX2RhdGFzZXRfaWRlbnRpZmllciUyMCUzRCUyMCUyMnNheWFrcGF1bCUyRnVjZjEwMS1zdWJzZXQlMjIlMEFmaWxlbmFtZSUyMCUzRCUyMCUyMlVDRjEwMV9zdWJzZXQudGFyLmd6JTIyJTBBZmlsZV9wYXRoJTIwJTNEJTIwaGZfaHViX2Rvd25sb2FkKHJlcG9faWQlM0RoZl9kYXRhc2V0X2lkZW50aWZpZXIlMkMlMjBmaWxlbmFtZSUzRGZpbGVuYW1lJTJDJTIwcmVwb190eXBlJTNEJTIyZGF0YXNldCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download

<span class="hljs-meta">&gt;&gt;&gt; </span>hf_dataset_identifier = <span class="hljs-string">&quot;sayakpaul/ucf101-subset&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>filename = <span class="hljs-string">&quot;UCF101_subset.tar.gz&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>)`,wrap:!1}}),X=new w({props:{code:"aW1wb3J0JTIwdGFyZmlsZSUwQSUwQXdpdGglMjB0YXJmaWxlLm9wZW4oZmlsZV9wYXRoKSUyMGFzJTIwdCUzQSUwQSUyMCUyMCUyMCUyMCUyMHQuZXh0cmFjdGFsbCglMjIuJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tarfile

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tarfile.<span class="hljs-built_in">open</span>(file_path) <span class="hljs-keyword">as</span> t:
<span class="hljs-meta">... </span>     t.extractall(<span class="hljs-string">&quot;.&quot;</span>)`,wrap:!1}}),F=new w({props:{code:"VUNGMTAxX3N1YnNldCUyRiUwQSUyMCUyMCUyMCUyMHRyYWluJTJGJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwQmFuZE1hcmNoaW5nJTJGJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwdmlkZW9fMS5tcDQlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjB2aWRlb18yLm1wNCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC4uLiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMEFyY2hlcnklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjB2aWRlb18xLm1wNCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHZpZGVvXzIubXA0JTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLi4uJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLi4uJTBBJTIwJTIwJTIwJTIwdmFsJTJGJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwQmFuZE1hcmNoaW5nJTJGJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwdmlkZW9fMS5tcDQlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjB2aWRlb18yLm1wNCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC4uLiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMEFyY2hlcnklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjB2aWRlb18xLm1wNCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHZpZGVvXzIubXA0JTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLi4uJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLi4uJTBBJTIwJTIwJTIwJTIwdGVzdCUyRiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMEJhbmRNYXJjaGluZyUyRiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHZpZGVvXzEubXA0JTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwdmlkZW9fMi5tcDQlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAuLi4lMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBBcmNoZXJ5JTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwdmlkZW9fMS5tcDQlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjB2aWRlb18yLm1wNCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC4uLiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC4uLg==",highlighted:`UCF101_subset/
    train/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    val/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...
    <span class="hljs-built_in">test</span>/
        BandMarching/
            video_1.mp4
            video_2.mp4
            ...
        Archery
            video_1.mp4
            video_2.mp4
            ...
        ...`,wrap:!1}}),E=new w({props:{code:"",highlighted:`...
<span class="hljs-string">&#x27;UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi&#x27;</span>,
<span class="hljs-string">&#x27;UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi&#x27;</span>,
<span class="hljs-string">&#x27;UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi&#x27;</span>,
<span class="hljs-string">&#x27;UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi&#x27;</span>,
<span class="hljs-string">&#x27;UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi&#x27;</span>
...`,wrap:!1}}),H=new w({props:{code:"Y2xhc3NfbGFiZWxzJTIwJTNEJTIwc29ydGVkKCU3QnN0cihwYXRoKS5zcGxpdCglMjIlMkYlMjIpJTVCMiU1RCUyMGZvciUyMHBhdGglMjBpbiUyMGFsbF92aWRlb19maWxlX3BhdGhzJTdEKSUwQWxhYmVsMmlkJTIwJTNEJTIwJTdCbGFiZWwlM0ElMjBpJTIwZm9yJTIwaSUyQyUyMGxhYmVsJTIwaW4lMjBlbnVtZXJhdGUoY2xhc3NfbGFiZWxzKSU3RCUwQWlkMmxhYmVsJTIwJTNEJTIwJTdCaSUzQSUyMGxhYmVsJTIwZm9yJTIwbGFiZWwlMkMlMjBpJTIwaW4lMjBsYWJlbDJpZC5pdGVtcygpJTdEJTBBJTBBcHJpbnQoZiUyMlVuaXF1ZSUyMGNsYXNzZXMlM0ElMjAlN0JsaXN0KGxhYmVsMmlkLmtleXMoKSklN0QuJTIyKSUwQQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>class_labels = <span class="hljs-built_in">sorted</span>({<span class="hljs-built_in">str</span>(path).split(<span class="hljs-string">&quot;/&quot;</span>)[<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> path <span class="hljs-keyword">in</span> all_video_file_paths})
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id = {label: i <span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(class_labels)}
<span class="hljs-meta">&gt;&gt;&gt; </span>id2label = {i: label <span class="hljs-keyword">for</span> label, i <span class="hljs-keyword">in</span> label2id.items()}

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Unique classes: <span class="hljs-subst">{<span class="hljs-built_in">list</span>(label2id.keys())}</span>.&quot;</span>)

<span class="hljs-comment"># Unique classes: [&#x27;ApplyEyeMakeup&#x27;, &#x27;ApplyLipstick&#x27;, &#x27;Archery&#x27;, &#x27;BabyCrawling&#x27;, &#x27;BalanceBeam&#x27;, &#x27;BandMarching&#x27;, &#x27;BaseballPitch&#x27;, &#x27;Basketball&#x27;, &#x27;BasketballDunk&#x27;, &#x27;BenchPress&#x27;].</span>`,wrap:!1}}),q=new qs({props:{title:"미세 조정하기 위해 모델 가져오기",local:"load-a-model-to-fine-tune",headingTag:"h2"}}),K=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpZGVvTUFFSW1hZ2VQcm9jZXNzb3IlMkMlMjBWaWRlb01BRUZvclZpZGVvQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbF9ja3B0JTIwJTNEJTIwJTIyTUNHLU5KVSUyRnZpZGVvbWFlLWJhc2UlMjIlMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBWaWRlb01BRUltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZChtb2RlbF9ja3B0KSUwQW1vZGVsJTIwJTNEJTIwVmlkZW9NQUVGb3JWaWRlb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjBtb2RlbF9ja3B0JTJDJTBBJTIwJTIwJTIwJTIwbGFiZWwyaWQlM0RsYWJlbDJpZCUyQyUwQSUyMCUyMCUyMCUyMGlkMmxhYmVsJTNEaWQybGFiZWwlMkMlMEElMjAlMjAlMjAlMjBpZ25vcmVfbWlzbWF0Y2hlZF9zaXplcyUzRFRydWUlMkMlMjAlMjAlMjMlMjBwcm92aWRlJTIwdGhpcyUyMGluJTIwY2FzZSUyMHlvdSdyZSUyMHBsYW5uaW5nJTIwdG8lMjBmaW5lLXR1bmUlMjBhbiUyMGFscmVhZHklMjBmaW5lLXR1bmVkJTIwY2hlY2twb2ludCUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VideoMAEImageProcessor, VideoMAEForVideoClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_ckpt = <span class="hljs-string">&quot;MCG-NJU/videomae-base&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VideoMAEForVideoClassification.from_pretrained(
<span class="hljs-meta">... </span>    model_ckpt,
<span class="hljs-meta">... </span>    label2id=label2id,
<span class="hljs-meta">... </span>    id2label=id2label,
<span class="hljs-meta">... </span>    ignore_mismatched_sizes=<span class="hljs-literal">True</span>,  <span class="hljs-comment"># provide this in case you&#x27;re planning to fine-tune an already fine-tuned checkpoint</span>
<span class="hljs-meta">... </span>)`,wrap:!1}}),O=new w({props:{code:"U29tZSUyMHdlaWdodHMlMjBvZiUyMHRoZSUyMG1vZGVsJTIwY2hlY2twb2ludCUyMGF0JTIwTUNHLU5KVSUyRnZpZGVvbWFlLWJhc2UlMjB3ZXJlJTIwbm90JTIwdXNlZCUyMHdoZW4lMjBpbml0aWFsaXppbmclMjBWaWRlb01BRUZvclZpZGVvQ2xhc3NpZmljYXRpb24lM0ElMjAlNUIuLi4lMkMlMjAnZGVjb2Rlci5kZWNvZGVyX2xheWVycy4xLmF0dGVudGlvbi5vdXRwdXQuZGVuc2UuYmlhcyclMkMlMjAnZGVjb2Rlci5kZWNvZGVyX2xheWVycy4yLmF0dGVudGlvbi5hdHRlbnRpb24ua2V5LndlaWdodCclNUQlMEEtJTIwVGhpcyUyMElTJTIwZXhwZWN0ZWQlMjBpZiUyMHlvdSUyMGFyZSUyMGluaXRpYWxpemluZyUyMFZpZGVvTUFFRm9yVmlkZW9DbGFzc2lmaWNhdGlvbiUyMGZyb20lMjB0aGUlMjBjaGVja3BvaW50JTIwb2YlMjBhJTIwbW9kZWwlMjB0cmFpbmVkJTIwb24lMjBhbm90aGVyJTIwdGFzayUyMG9yJTIwd2l0aCUyMGFub3RoZXIlMjBhcmNoaXRlY3R1cmUlMjAoZS5nLiUyMGluaXRpYWxpemluZyUyMGElMjBCZXJ0Rm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUyMG1vZGVsJTIwZnJvbSUyMGElMjBCZXJ0Rm9yUHJlVHJhaW5pbmclMjBtb2RlbCkuJTBBLSUyMFRoaXMlMjBJUyUyME5PVCUyMGV4cGVjdGVkJTIwaWYlMjB5b3UlMjBhcmUlMjBpbml0aWFsaXppbmclMjBWaWRlb01BRUZvclZpZGVvQ2xhc3NpZmljYXRpb24lMjBmcm9tJTIwdGhlJTIwY2hlY2twb2ludCUyMG9mJTIwYSUyMG1vZGVsJTIwdGhhdCUyMHlvdSUyMGV4cGVjdCUyMHRvJTIwYmUlMjBleGFjdGx5JTIwaWRlbnRpY2FsJTIwKGluaXRpYWxpemluZyUyMGElMjBCZXJ0Rm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUyMG1vZGVsJTIwZnJvbSUyMGElMjBCZXJ0Rm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUyMG1vZGVsKS4lMEFTb21lJTIwd2VpZ2h0cyUyMG9mJTIwVmlkZW9NQUVGb3JWaWRlb0NsYXNzaWZpY2F0aW9uJTIwd2VyZSUyMG5vdCUyMGluaXRpYWxpemVkJTIwZnJvbSUyMHRoZSUyMG1vZGVsJTIwY2hlY2twb2ludCUyMGF0JTIwTUNHLU5KVSUyRnZpZGVvbWFlLWJhc2UlMjBhbmQlMjBhcmUlMjBuZXdseSUyMGluaXRpYWxpemVkJTNBJTIwJTVCJ2NsYXNzaWZpZXIuYmlhcyclMkMlMjAnY2xhc3NpZmllci53ZWlnaHQnJTVEJTBBWW91JTIwc2hvdWxkJTIwcHJvYmFibHklMjBUUkFJTiUyMHRoaXMlMjBtb2RlbCUyMG9uJTIwYSUyMGRvd24tc3RyZWFtJTIwdGFzayUyMHRvJTIwYmUlMjBhYmxlJTIwdG8lMjB1c2UlMjBpdCUyMGZvciUyMHByZWRpY3Rpb25zJTIwYW5kJTIwaW5mZXJlbmNlLg==",highlighted:`Some weights of the model checkpoint at MCG-NJU/videomae-base were not used when initializing VideoMAEForVideoClassification: [..., <span class="hljs-string">&#x27;decoder.decoder_layers.1.attention.output.dense.bias&#x27;</span>, <span class="hljs-string">&#x27;decoder.decoder_layers.2.attention.attention.key.weight&#x27;</span>]
- This IS expected <span class="hljs-keyword">if</span> you are initializing VideoMAEForVideoClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected <span class="hljs-keyword">if</span> you are initializing VideoMAEForVideoClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: [<span class="hljs-string">&#x27;classifier.bias&#x27;</span>, <span class="hljs-string">&#x27;classifier.weight&#x27;</span>]
You should probably TRAIN this model on a down-stream task to be able to use it <span class="hljs-keyword">for</span> predictions and inference.`,wrap:!1}}),as=new qs({props:{title:"훈련을 위한 데이터 세트 준비하기",local:"prepare-the-datasets-for-training",headingTag:"h2"}}),ts=new w({props:{code:"aW1wb3J0JTIwcHl0b3JjaHZpZGVvLmRhdGElMEElMEFmcm9tJTIwcHl0b3JjaHZpZGVvLnRyYW5zZm9ybXMlMjBpbXBvcnQlMjAoJTBBJTIwJTIwJTIwJTIwQXBwbHlUcmFuc2Zvcm1Ub0tleSUyQyUwQSUyMCUyMCUyMCUyME5vcm1hbGl6ZSUyQyUwQSUyMCUyMCUyMCUyMFJhbmRvbVNob3J0U2lkZVNjYWxlJTJDJTBBJTIwJTIwJTIwJTIwUmVtb3ZlS2V5JTJDJTBBJTIwJTIwJTIwJTIwU2hvcnRTaWRlU2NhbGUlMkMlMEElMjAlMjAlMjAlMjBVbmlmb3JtVGVtcG9yYWxTdWJzYW1wbGUlMkMlMEEpJTBBJTBBZnJvbSUyMHRvcmNodmlzaW9uLnRyYW5zZm9ybXMlMjBpbXBvcnQlMjAoJTBBJTIwJTIwJTIwJTIwQ29tcG9zZSUyQyUwQSUyMCUyMCUyMCUyMExhbWJkYSUyQyUwQSUyMCUyMCUyMCUyMFJhbmRvbUNyb3AlMkMlMEElMjAlMjAlMjAlMjBSYW5kb21Ib3Jpem9udGFsRmxpcCUyQyUwQSUyMCUyMCUyMCUyMFJlc2l6ZSUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> pytorchvideo.data

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> pytorchvideo.transforms <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    ApplyTransformToKey,
<span class="hljs-meta">... </span>    Normalize,
<span class="hljs-meta">... </span>    RandomShortSideScale,
<span class="hljs-meta">... </span>    RemoveKey,
<span class="hljs-meta">... </span>    ShortSideScale,
<span class="hljs-meta">... </span>    UniformTemporalSubsample,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> (
<span class="hljs-meta">... </span>    Compose,
<span class="hljs-meta">... </span>    Lambda,
<span class="hljs-meta">... </span>    RandomCrop,
<span class="hljs-meta">... </span>    RandomHorizontalFlip,
<span class="hljs-meta">... </span>    Resize,
<span class="hljs-meta">... </span>)`,wrap:!1}}),ms=new w({props:{code:"bWVhbiUyMCUzRCUyMGltYWdlX3Byb2Nlc3Nvci5pbWFnZV9tZWFuJTBBc3RkJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yLmltYWdlX3N0ZCUwQWlmJTIwJTIyc2hvcnRlc3RfZWRnZSUyMiUyMGluJTIwaW1hZ2VfcHJvY2Vzc29yLnNpemUlM0ElMEElMjAlMjAlMjAlMjBoZWlnaHQlMjAlM0QlMjB3aWR0aCUyMCUzRCUyMGltYWdlX3Byb2Nlc3Nvci5zaXplJTVCJTIyc2hvcnRlc3RfZWRnZSUyMiU1RCUwQWVsc2UlM0ElMEElMjAlMjAlMjAlMjBoZWlnaHQlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3Iuc2l6ZSU1QiUyMmhlaWdodCUyMiU1RCUwQSUyMCUyMCUyMCUyMHdpZHRoJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yLnNpemUlNUIlMjJ3aWR0aCUyMiU1RCUwQXJlc2l6ZV90byUyMCUzRCUyMChoZWlnaHQlMkMlMjB3aWR0aCklMEElMEFudW1fZnJhbWVzX3RvX3NhbXBsZSUyMCUzRCUyMG1vZGVsLmNvbmZpZy5udW1fZnJhbWVzJTBBc2FtcGxlX3JhdGUlMjAlM0QlMjA0JTBBZnBzJTIwJTNEJTIwMzAlMEFjbGlwX2R1cmF0aW9uJTIwJTNEJTIwbnVtX2ZyYW1lc190b19zYW1wbGUlMjAqJTIwc2FtcGxlX3JhdGUlMjAlMkYlMjBmcHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>mean = image_processor.image_mean
<span class="hljs-meta">&gt;&gt;&gt; </span>std = image_processor.image_std
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">if</span> <span class="hljs-string">&quot;shortest_edge&quot;</span> <span class="hljs-keyword">in</span> image_processor.size:
<span class="hljs-meta">... </span>    height = width = image_processor.size[<span class="hljs-string">&quot;shortest_edge&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">else</span>:
<span class="hljs-meta">... </span>    height = image_processor.size[<span class="hljs-string">&quot;height&quot;</span>]
<span class="hljs-meta">... </span>    width = image_processor.size[<span class="hljs-string">&quot;width&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>resize_to = (height, width)

<span class="hljs-meta">&gt;&gt;&gt; </span>num_frames_to_sample = model.config.num_frames
<span class="hljs-meta">&gt;&gt;&gt; </span>sample_rate = <span class="hljs-number">4</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>fps = <span class="hljs-number">30</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>clip_duration = num_frames_to_sample * sample_rate / fps`,wrap:!1}}),cs=new w({props:{code:"dHJhaW5fdHJhbnNmb3JtJTIwJTNEJTIwQ29tcG9zZSglMEElMjAlMjAlMjAlMjAlNUIlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBBcHBseVRyYW5zZm9ybVRvS2V5KCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGtleSUzRCUyMnZpZGVvJTIyJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwdHJhbnNmb3JtJTNEQ29tcG9zZSglMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlNUIlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBVbmlmb3JtVGVtcG9yYWxTdWJzYW1wbGUobnVtX2ZyYW1lc190b19zYW1wbGUpJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwTGFtYmRhKGxhbWJkYSUyMHglM0ElMjB4JTIwJTJGJTIwMjU1LjApJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwTm9ybWFsaXplKG1lYW4lMkMlMjBzdGQpJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwUmFuZG9tU2hvcnRTaWRlU2NhbGUobWluX3NpemUlM0QyNTYlMkMlMjBtYXhfc2l6ZSUzRDMyMCklMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBSYW5kb21Dcm9wKHJlc2l6ZV90byklMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBSYW5kb21Ib3Jpem9udGFsRmxpcChwJTNEMC41KSUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCU1RCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCklMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjApJTJDJTBBJTIwJTIwJTIwJTIwJTVEJTBBKSUwQSUwQXRyYWluX2RhdGFzZXQlMjAlM0QlMjBweXRvcmNodmlkZW8uZGF0YS5VY2YxMDEoJTBBJTIwJTIwJTIwJTIwZGF0YV9wYXRoJTNEb3MucGF0aC5qb2luKGRhdGFzZXRfcm9vdF9wYXRoJTJDJTIwJTIydHJhaW4lMjIpJTJDJTBBJTIwJTIwJTIwJTIwY2xpcF9zYW1wbGVyJTNEcHl0b3JjaHZpZGVvLmRhdGEubWFrZV9jbGlwX3NhbXBsZXIoJTIycmFuZG9tJTIyJTJDJTIwY2xpcF9kdXJhdGlvbiklMkMlMEElMjAlMjAlMjAlMjBkZWNvZGVfYXVkaW8lM0RGYWxzZSUyQyUwQSUyMCUyMCUyMCUyMHRyYW5zZm9ybSUzRHRyYWluX3RyYW5zZm9ybSUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>train_transform = Compose(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        ApplyTransformToKey(
<span class="hljs-meta">... </span>            key=<span class="hljs-string">&quot;video&quot;</span>,
<span class="hljs-meta">... </span>            transform=Compose(
<span class="hljs-meta">... </span>                [
<span class="hljs-meta">... </span>                    UniformTemporalSubsample(num_frames_to_sample),
<span class="hljs-meta">... </span>                    Lambda(<span class="hljs-keyword">lambda</span> x: x / <span class="hljs-number">255.0</span>),
<span class="hljs-meta">... </span>                    Normalize(mean, std),
<span class="hljs-meta">... </span>                    RandomShortSideScale(min_size=<span class="hljs-number">256</span>, max_size=<span class="hljs-number">320</span>),
<span class="hljs-meta">... </span>                    RandomCrop(resize_to),
<span class="hljs-meta">... </span>                    RandomHorizontalFlip(p=<span class="hljs-number">0.5</span>),
<span class="hljs-meta">... </span>                ]
<span class="hljs-meta">... </span>            ),
<span class="hljs-meta">... </span>        ),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataset = pytorchvideo.data.Ucf101(
<span class="hljs-meta">... </span>    data_path=os.path.join(dataset_root_path, <span class="hljs-string">&quot;train&quot;</span>),
<span class="hljs-meta">... </span>    clip_sampler=pytorchvideo.data.make_clip_sampler(<span class="hljs-string">&quot;random&quot;</span>, clip_duration),
<span class="hljs-meta">... </span>    decode_audio=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    transform=train_transform,
<span class="hljs-meta">... </span>)`,wrap:!1}}),js=new w({props:{code:"dmFsX3RyYW5zZm9ybSUyMCUzRCUyMENvbXBvc2UoJTBBJTIwJTIwJTIwJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwQXBwbHlUcmFuc2Zvcm1Ub0tleSglMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBrZXklM0QlMjJ2aWRlbyUyMiUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHRyYW5zZm9ybSUzRENvbXBvc2UoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwVW5pZm9ybVRlbXBvcmFsU3Vic2FtcGxlKG51bV9mcmFtZXNfdG9fc2FtcGxlKSUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMExhbWJkYShsYW1iZGElMjB4JTNBJTIweCUyMCUyRiUyMDI1NS4wKSUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyME5vcm1hbGl6ZShtZWFuJTJDJTIwc3RkKSUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMFJlc2l6ZShyZXNpemVfdG8pJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTVEJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwKSUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCklMkMlMEElMjAlMjAlMjAlMjAlNUQlMEEpJTBBJTBBdmFsX2RhdGFzZXQlMjAlM0QlMjBweXRvcmNodmlkZW8uZGF0YS5VY2YxMDEoJTBBJTIwJTIwJTIwJTIwZGF0YV9wYXRoJTNEb3MucGF0aC5qb2luKGRhdGFzZXRfcm9vdF9wYXRoJTJDJTIwJTIydmFsJTIyKSUyQyUwQSUyMCUyMCUyMCUyMGNsaXBfc2FtcGxlciUzRHB5dG9yY2h2aWRlby5kYXRhLm1ha2VfY2xpcF9zYW1wbGVyKCUyMnVuaWZvcm0lMjIlMkMlMjBjbGlwX2R1cmF0aW9uKSUyQyUwQSUyMCUyMCUyMCUyMGRlY29kZV9hdWRpbyUzREZhbHNlJTJDJTBBJTIwJTIwJTIwJTIwdHJhbnNmb3JtJTNEdmFsX3RyYW5zZm9ybSUyQyUwQSklMEElMEF0ZXN0X2RhdGFzZXQlMjAlM0QlMjBweXRvcmNodmlkZW8uZGF0YS5VY2YxMDEoJTBBJTIwJTIwJTIwJTIwZGF0YV9wYXRoJTNEb3MucGF0aC5qb2luKGRhdGFzZXRfcm9vdF9wYXRoJTJDJTIwJTIydGVzdCUyMiklMkMlMEElMjAlMjAlMjAlMjBjbGlwX3NhbXBsZXIlM0RweXRvcmNodmlkZW8uZGF0YS5tYWtlX2NsaXBfc2FtcGxlciglMjJ1bmlmb3JtJTIyJTJDJTIwY2xpcF9kdXJhdGlvbiklMkMlMEElMjAlMjAlMjAlMjBkZWNvZGVfYXVkaW8lM0RGYWxzZSUyQyUwQSUyMCUyMCUyMCUyMHRyYW5zZm9ybSUzRHZhbF90cmFuc2Zvcm0lMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>val_transform = Compose(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        ApplyTransformToKey(
<span class="hljs-meta">... </span>            key=<span class="hljs-string">&quot;video&quot;</span>,
<span class="hljs-meta">... </span>            transform=Compose(
<span class="hljs-meta">... </span>                [
<span class="hljs-meta">... </span>                    UniformTemporalSubsample(num_frames_to_sample),
<span class="hljs-meta">... </span>                    Lambda(<span class="hljs-keyword">lambda</span> x: x / <span class="hljs-number">255.0</span>),
<span class="hljs-meta">... </span>                    Normalize(mean, std),
<span class="hljs-meta">... </span>                    Resize(resize_to),
<span class="hljs-meta">... </span>                ]
<span class="hljs-meta">... </span>            ),
<span class="hljs-meta">... </span>        ),
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>val_dataset = pytorchvideo.data.Ucf101(
<span class="hljs-meta">... </span>    data_path=os.path.join(dataset_root_path, <span class="hljs-string">&quot;val&quot;</span>),
<span class="hljs-meta">... </span>    clip_sampler=pytorchvideo.data.make_clip_sampler(<span class="hljs-string">&quot;uniform&quot;</span>, clip_duration),
<span class="hljs-meta">... </span>    decode_audio=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    transform=val_transform,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>test_dataset = pytorchvideo.data.Ucf101(
<span class="hljs-meta">... </span>    data_path=os.path.join(dataset_root_path, <span class="hljs-string">&quot;test&quot;</span>),
<span class="hljs-meta">... </span>    clip_sampler=pytorchvideo.data.make_clip_sampler(<span class="hljs-string">&quot;uniform&quot;</span>, clip_duration),
<span class="hljs-meta">... </span>    decode_audio=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    transform=val_transform,
<span class="hljs-meta">... </span>)`,wrap:!1}}),Us=new w({props:{code:"cHJpbnQodHJhaW5fZGF0YXNldC5udW1fdmlkZW9zJTJDJTIwdmFsX2RhdGFzZXQubnVtX3ZpZGVvcyUyQyUyMHRlc3RfZGF0YXNldC5udW1fdmlkZW9zKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)
<span class="hljs-comment"># (300, 30, 75)</span>`,wrap:!1}}),Ts=new qs({props:{title:"더 나은 디버깅을 위해 전처리 영상 시각화하기",local:"visualize-the-preprocessed-video-for-better-debugging",headingTag:"h2"}}),hs=new w({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> imageio
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> IPython.display <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">unnormalize_img</span>(<span class="hljs-params">img</span>):
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;&quot;&quot;Un-normalizes the image pixels.&quot;&quot;&quot;</span>
<span class="hljs-meta">... </span>    img = (img * std) + mean
<span class="hljs-meta">... </span>    img = (img * <span class="hljs-number">255</span>).astype(<span class="hljs-string">&quot;uint8&quot;</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> img.clip(<span class="hljs-number">0</span>, <span class="hljs-number">255</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">create_gif</span>(<span class="hljs-params">video_tensor, filename=<span class="hljs-string">&quot;sample.gif&quot;</span></span>):
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;&quot;&quot;Prepares a GIF from a video tensor.
<span class="hljs-meta">... </span>    
<span class="hljs-meta">... </span>    The video tensor is expected to have the following shape:
<span class="hljs-meta">... </span>    (num_frames, num_channels, height, width).
<span class="hljs-meta">... </span>    &quot;&quot;&quot;</span>
<span class="hljs-meta">... </span>    frames = []
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> video_frame <span class="hljs-keyword">in</span> video_tensor:
<span class="hljs-meta">... </span>        frame_unnormalized = unnormalize_img(video_frame.permute(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>).numpy())
<span class="hljs-meta">... </span>        frames.append(frame_unnormalized)
<span class="hljs-meta">... </span>    kargs = {<span class="hljs-string">&quot;duration&quot;</span>: <span class="hljs-number">0.25</span>}
<span class="hljs-meta">... </span>    imageio.mimsave(filename, frames, <span class="hljs-string">&quot;GIF&quot;</span>, **kargs)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> filename

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">display_gif</span>(<span class="hljs-params">video_tensor, gif_name=<span class="hljs-string">&quot;sample.gif&quot;</span></span>):
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;&quot;&quot;Prepares and displays a GIF from a video tensor.&quot;&quot;&quot;</span>
<span class="hljs-meta">... </span>    video_tensor = video_tensor.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">... </span>    gif_filename = create_gif(video_tensor, gif_name)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> Image(filename=gif_filename)

<span class="hljs-meta">&gt;&gt;&gt; </span>sample_video = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataset))
<span class="hljs-meta">&gt;&gt;&gt; </span>video_tensor = sample_video[<span class="hljs-string">&quot;video&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>display_gif(video_tensor)`,wrap:!1}}),rs=new qs({props:{title:"모델 훈련하기",local:"train-the-model",headingTag:"h2"}}),bs=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluaW5nQXJndW1lbnRzJTJDJTIwVHJhaW5lciUwQSUwQW1vZGVsX25hbWUlMjAlM0QlMjBtb2RlbF9ja3B0LnNwbGl0KCUyMiUyRiUyMiklNUItMSU1RCUwQW5ld19tb2RlbF9uYW1lJTIwJTNEJTIwZiUyMiU3Qm1vZGVsX25hbWUlN0QtZmluZXR1bmVkLXVjZjEwMS1zdWJzZXQlMjIlMEFudW1fZXBvY2hzJTIwJTNEJTIwNCUwQSUwQWFyZ3MlMjAlM0QlMjBUcmFpbmluZ0FyZ3VtZW50cyglMEElMjAlMjAlMjAlMjBuZXdfbW9kZWxfbmFtZSUyQyUwQSUyMCUyMCUyMCUyMHJlbW92ZV91bnVzZWRfY29sdW1ucyUzREZhbHNlJTJDJTBBJTIwJTIwJTIwJTIwZXZhbHVhdGlvbl9zdHJhdGVneSUzRCUyMmVwb2NoJTIyJTJDJTBBJTIwJTIwJTIwJTIwc2F2ZV9zdHJhdGVneSUzRCUyMmVwb2NoJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDVlLTUlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlM0RiYXRjaF9zaXplJTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV9ldmFsX2JhdGNoX3NpemUlM0RiYXRjaF9zaXplJTJDJTBBJTIwJTIwJTIwJTIwd2FybXVwX3JhdGlvJTNEMC4xJTJDJTBBJTIwJTIwJTIwJTIwbG9nZ2luZ19zdGVwcyUzRDEwJTJDJTBBJTIwJTIwJTIwJTIwbG9hZF9iZXN0X21vZGVsX2F0X2VuZCUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBtZXRyaWNfZm9yX2Jlc3RfbW9kZWwlM0QlMjJhY2N1cmFjeSUyMiUyQyUwQSUyMCUyMCUyMCUyMHB1c2hfdG9faHViJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMG1heF9zdGVwcyUzRCh0cmFpbl9kYXRhc2V0Lm51bV92aWRlb3MlMjAlMkYlMkYlMjBiYXRjaF9zaXplKSUyMColMjBudW1fZXBvY2hzJTJDJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = model_ckpt.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>new_model_name = <span class="hljs-string">f&quot;<span class="hljs-subst">{model_name}</span>-finetuned-ucf101-subset&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">4</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>args = TrainingArguments(
<span class="hljs-meta">... </span>    new_model_name,
<span class="hljs-meta">... </span>    remove_unused_columns=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">5e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=batch_size,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=batch_size,
<span class="hljs-meta">... </span>    warmup_ratio=<span class="hljs-number">0.1</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">10</span>,
<span class="hljs-meta">... </span>    load_best_model_at_end=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    metric_for_best_model=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,
<span class="hljs-meta">... </span>)`,wrap:!1}}),fs=new w({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFtZXRyaWMlMjAlM0QlMjBldmFsdWF0ZS5sb2FkKCUyMmFjY3VyYWN5JTIyKSUwQSUwQSUwQWRlZiUyMGNvbXB1dGVfbWV0cmljcyhldmFsX3ByZWQpJTNBJTBBJTIwJTIwJTIwJTIwcHJlZGljdGlvbnMlMjAlM0QlMjBucC5hcmdtYXgoZXZhbF9wcmVkLnByZWRpY3Rpb25zJTJDJTIwYXhpcyUzRDEpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwbWV0cmljLmNvbXB1dGUocHJlZGljdGlvbnMlM0RwcmVkaWN0aW9ucyUyQyUyMHJlZmVyZW5jZXMlM0RldmFsX3ByZWQubGFiZWxfaWRzKQ==",highlighted:`<span class="hljs-keyword">import</span> evaluate

metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)


<span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
    predictions = np.argmax(eval_pred.predictions, axis=<span class="hljs-number">1</span>)
    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=eval_pred.label_ids)`,wrap:!1}}),Gs=new w({props:{code:"ZGVmJTIwY29sbGF0ZV9mbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjAlMjMlMjBwZXJtdXRlJTIwdG8lMjAobnVtX2ZyYW1lcyUyQyUyMG51bV9jaGFubmVscyUyQyUyMGhlaWdodCUyQyUyMHdpZHRoKSUwQSUyMCUyMCUyMCUyMHBpeGVsX3ZhbHVlcyUyMCUzRCUyMHRvcmNoLnN0YWNrKCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCU1QmV4YW1wbGUlNUIlMjJ2aWRlbyUyMiU1RC5wZXJtdXRlKDElMkMlMjAwJTJDJTIwMiUyQyUyMDMpJTIwZm9yJTIwZXhhbXBsZSUyMGluJTIwZXhhbXBsZXMlNUQlMEElMjAlMjAlMjAlMjApJTBBJTIwJTIwJTIwJTIwbGFiZWxzJTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QmV4YW1wbGUlNUIlMjJsYWJlbCUyMiU1RCUyMGZvciUyMGV4YW1wbGUlMjBpbiUyMGV4YW1wbGVzJTVEKSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMCU3QiUyMnBpeGVsX3ZhbHVlcyUyMiUzQSUyMHBpeGVsX3ZhbHVlcyUyQyUyMCUyMmxhYmVscyUyMiUzQSUyMGxhYmVscyU3RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">collate_fn</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-comment"># permute to (num_frames, num_channels, height, width)</span>
<span class="hljs-meta">... </span>    pixel_values = torch.stack(
<span class="hljs-meta">... </span>        [example[<span class="hljs-string">&quot;video&quot;</span>].permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>) <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples]
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>    labels = torch.tensor([example[<span class="hljs-string">&quot;label&quot;</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples])
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;pixel_values&quot;</span>: pixel_values, <span class="hljs-string">&quot;labels&quot;</span>: labels}`,wrap:!1}}),Bs=new w({props:{code:"dHJhaW5lciUyMCUzRCUyMFRyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlMkMlMEElMjAlMjAlMjAlMjBhcmdzJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRHRyYWluX2RhdGFzZXQlMkMlMEElMjAlMjAlMjAlMjBldmFsX2RhdGFzZXQlM0R2YWxfZGF0YXNldCUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRGltYWdlX3Byb2Nlc3NvciUyQyUwQSUyMCUyMCUyMCUyMGNvbXB1dGVfbWV0cmljcyUzRGNvbXB1dGVfbWV0cmljcyUyQyUwQSUyMCUyMCUyMCUyMGRhdGFfY29sbGF0b3IlM0Rjb2xsYXRlX2ZuJTJDJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model,
<span class="hljs-meta">... </span>    args,
<span class="hljs-meta">... </span>    train_dataset=train_dataset,
<span class="hljs-meta">... </span>    eval_dataset=val_dataset,
<span class="hljs-meta">... </span>    tokenizer=image_processor,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>    data_collator=collate_fn,
<span class="hljs-meta">... </span>)`,wrap:!1}}),ks=new w({props:{code:"dHJhaW5fcmVzdWx0cyUyMCUzRCUyMHRyYWluZXIudHJhaW4oKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>train_results = trainer.train()',wrap:!1}}),Rs=new w({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),xs=new qs({props:{title:"추론하기",local:"inference",headingTag:"h2"}}),Fs=new w({props:{code:"c2FtcGxlX3Rlc3RfdmlkZW8lMjAlM0QlMjBuZXh0KGl0ZXIodGVzdF9kYXRhc2V0KSk=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>sample_test_video = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(test_dataset))',wrap:!1}}),Es=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBdmlkZW9fY2xzJTIwJTNEJTIwcGlwZWxpbmUobW9kZWwlM0QlMjJteV9hd2Vzb21lX3ZpZGVvX2Nsc19tb2RlbCUyMiklMEF2aWRlb19jbHMoJTIyaHR0cHMlM0ElMkYlMkZodWdnaW5nZmFjZS5jbyUyRmRhdGFzZXRzJTJGc2F5YWtwYXVsJTJGdWNmMTAxLXN1YnNldCUyRnJlc29sdmUlMkZtYWluJTJGdl9CYXNrZXRiYWxsRHVua19nMTRfYzA2LmF2aSUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>video_cls = pipeline(model=<span class="hljs-string">&quot;my_awesome_video_cls_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>video_cls(<span class="hljs-string">&quot;https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi&quot;</span>)
[{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9272987842559814</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;BasketballDunk&#x27;</span>},
 {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.017777055501937866</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;BabyCrawling&#x27;</span>},
 {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.01663011871278286</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;BalanceBeam&#x27;</span>},
 {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.009560945443809032</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;BandMarching&#x27;</span>},
 {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.0068979403004050255</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;BaseballPitch&#x27;</span>}]`,wrap:!1}}),Ss=new w({props:{code:"ZGVmJTIwcnVuX2luZmVyZW5jZShtb2RlbCUyQyUyMHZpZGVvKSUzQSUwQSUyMCUyMCUyMCUyMCUyMyUyMChudW1fZnJhbWVzJTJDJTIwbnVtX2NoYW5uZWxzJTJDJTIwaGVpZ2h0JTJDJTIwd2lkdGgpJTBBJTIwJTIwJTIwJTIwcGVydW11dGVkX3NhbXBsZV90ZXN0X3ZpZGVvJTIwJTNEJTIwdmlkZW8ucGVybXV0ZSgxJTJDJTIwMCUyQyUyMDIlMkMlMjAzKSUwQSUyMCUyMCUyMCUyMGlucHV0cyUyMCUzRCUyMCU3QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMnBpeGVsX3ZhbHVlcyUyMiUzQSUyMHBlcnVtdXRlZF9zYW1wbGVfdGVzdF92aWRlby51bnNxdWVlemUoMCklMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJsYWJlbHMlMjIlM0ElMjB0b3JjaC50ZW5zb3IoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTVCc2FtcGxlX3Rlc3RfdmlkZW8lNUIlMjJsYWJlbCUyMiU1RCU1RCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCklMkMlMjAlMjAlMjMlMjB0aGlzJTIwY2FuJTIwYmUlMjBza2lwcGVkJTIwaWYlMjB5b3UlMjBkb24ndCUyMGhhdmUlMjBsYWJlbHMlMjBhdmFpbGFibGUuJTBBJTIwJTIwJTIwJTIwJTdEJTBBJTBBJTIwJTIwJTIwJTIwZGV2aWNlJTIwJTNEJTIwdG9yY2guZGV2aWNlKCUyMmN1ZGElMjIlMjBpZiUyMHRvcmNoLmN1ZGEuaXNfYXZhaWxhYmxlKCklMjBlbHNlJTIwJTIyY3B1JTIyKSUwQSUyMCUyMCUyMCUyMGlucHV0cyUyMCUzRCUyMCU3QmslM0ElMjB2LnRvKGRldmljZSklMjBmb3IlMjBrJTJDJTIwdiUyMGluJTIwaW5wdXRzLml0ZW1zKCklN0QlMEElMjAlMjAlMjAlMjBtb2RlbCUyMCUzRCUyMG1vZGVsLnRvKGRldmljZSklMEElMEElMjAlMjAlMjAlMjAlMjMlMjBmb3J3YXJkJTIwcGFzcyUwQSUyMCUyMCUyMCUyMHdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlMEElMEElMjAlMjAlMjAlMjByZXR1cm4lMjBsb2dpdHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">run_inference</span>(<span class="hljs-params">model, video</span>):
<span class="hljs-meta">... </span>    <span class="hljs-comment"># (num_frames, num_channels, height, width)</span>
<span class="hljs-meta">... </span>    perumuted_sample_test_video = video.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>)
<span class="hljs-meta">... </span>    inputs = {
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;pixel_values&quot;</span>: perumuted_sample_test_video.unsqueeze(<span class="hljs-number">0</span>),
<span class="hljs-meta">... </span>        <span class="hljs-string">&quot;labels&quot;</span>: torch.tensor(
<span class="hljs-meta">... </span>            [sample_test_video[<span class="hljs-string">&quot;label&quot;</span>]]
<span class="hljs-meta">... </span>        ),  <span class="hljs-comment"># this can be skipped if you don&#x27;t have labels available.</span>
<span class="hljs-meta">... </span>    }

<span class="hljs-meta">... </span>    device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>)
<span class="hljs-meta">... </span>    inputs = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> inputs.items()}
<span class="hljs-meta">... </span>    model = model.to(device)

<span class="hljs-meta">... </span>    <span class="hljs-comment"># forward pass</span>
<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>        outputs = model(**inputs)
<span class="hljs-meta">... </span>        logits = outputs.logits

<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> logits`,wrap:!1}}),Ns=new w({props:{code:"bG9naXRzJTIwJTNEJTIwcnVuX2luZmVyZW5jZSh0cmFpbmVkX21vZGVsJTJDJTIwc2FtcGxlX3Rlc3RfdmlkZW8lNUIlMjJ2aWRlbyUyMiU1RCk=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>logits = run_inference(trained_model, sample_test_video[<span class="hljs-string">&quot;video&quot;</span>])',wrap:!1}}),Ls=new w({props:{code:"cHJlZGljdGVkX2NsYXNzX2lkeCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KCUyMlByZWRpY3RlZCUyMGNsYXNzJTNBJTIyJTJDJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2NsYXNzX2lkeCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_idx = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])
<span class="hljs-comment"># Predicted class: BasketballDunk</span>`,wrap:!1}}),{c(){U=p("meta"),f=t(),d=p("p"),h=t(),m(I.$$.fragment),T=t(),m(r.$$.fragment),Ps=t(),g=p("p"),g.textContent=Za,Os=t(),Z=p("p"),Z.textContent=va,sl=t(),v=p("ol"),v.innerHTML=Ga,ll=t(),m(b.$$.fragment),al=t(),G=p("p"),G.textContent=Wa,el=t(),m(W.$$.fragment),tl=t(),B=p("p"),B.innerHTML=Ba,nl=t(),_=p("p"),_.textContent=_a,pl=t(),m(A.$$.fragment),Ml=t(),m(k.$$.fragment),il=t(),V=p("p"),V.innerHTML=Aa,ml=t(),m(R.$$.fragment),yl=t(),x=p("p"),x.textContent=ka,cl=t(),m(X.$$.fragment),Jl=t(),$=p("p"),$.textContent=Va,jl=t(),m(F.$$.fragment),ol=t(),Y=p("p"),Y.textContent=Ra,wl=t(),m(E.$$.fragment),Ul=t(),z=p("p"),z.innerHTML=xa,Tl=t(),S=p("p"),S.innerHTML=Xa,hl=t(),Q=p("p"),Q.textContent=$a,rl=t(),N=p("ul"),N.innerHTML=Fa,dl=t(),m(H.$$.fragment),Il=t(),L=p("p"),L.textContent=Ya,bl=t(),m(q.$$.fragment),Cl=t(),D=p("p"),D.textContent=Ea,ul=t(),m(K.$$.fragment),fl=t(),P=p("p"),P.textContent=za,gl=t(),m(O.$$.fragment),Zl=t(),ss=p("p"),ss.innerHTML=Sa,vl=t(),ls=p("p"),ls.innerHTML=Qa,Gl=t(),m(as.$$.fragment),Wl=t(),es=p("p"),es.innerHTML=Na,Bl=t(),m(ts.$$.fragment),_l=t(),ns=p("p"),ns.innerHTML=Ha,Al=t(),ps=p("p"),ps.textContent=La,kl=t(),Ms=p("ul"),Ms.innerHTML=qa,Vl=t(),is=p("p"),is.textContent=Da,Rl=t(),m(ms.$$.fragment),xl=t(),ys=p("p"),ys.textContent=Ka,Xl=t(),m(cs.$$.fragment),$l=t(),Js=p("p"),Js.textContent=Pa,Fl=t(),m(js.$$.fragment),Yl=t(),os=p("p"),os.innerHTML=Oa,El=t(),ws=p("p"),ws.innerHTML=se,zl=t(),m(Us.$$.fragment),Sl=t(),m(Ts.$$.fragment),Ql=t(),m(hs.$$.fragment),Nl=t(),C=p("div"),C.innerHTML=le,Hl=t(),m(rs.$$.fragment),Ll=t(),ds=p("p"),ds.innerHTML=ae,ql=t(),Is=p("p"),Is.innerHTML=ee,Dl=t(),m(bs.$$.fragment),Kl=t(),Cs=p("p"),Cs.innerHTML=te,Pl=t(),us=p("p"),us.textContent=ne,Ol=t(),m(fs.$$.fragment),sa=t(),gs=p("p"),gs.innerHTML=pe,la=t(),Zs=p("p"),Zs.innerHTML=Me,aa=t(),vs=p("p"),vs.innerHTML=ie,ea=t(),m(Gs.$$.fragment),ta=t(),Ws=p("p"),Ws.innerHTML=me,na=t(),m(Bs.$$.fragment),pa=t(),_s=p("p"),_s.innerHTML=ye,Ma=t(),As=p("p"),As.innerHTML=ce,ia=t(),m(ks.$$.fragment),ma=t(),Vs=p("p"),Vs.innerHTML=Je,ya=t(),m(Rs.$$.fragment),ca=t(),m(xs.$$.fragment),Ja=t(),Xs=p("p"),Xs.textContent=je,ja=t(),$s=p("p"),$s.textContent=oe,oa=t(),m(Fs.$$.fragment),wa=t(),u=p("div"),u.innerHTML=we,Ua=t(),Ys=p("p"),Ys.innerHTML=Ue,Ta=t(),m(Es.$$.fragment),ha=t(),zs=p("p"),zs.innerHTML=Te,ra=t(),m(Ss.$$.fragment),da=t(),Qs=p("p"),Qs.innerHTML=he,Ia=t(),m(Ns.$$.fragment),ba=t(),Hs=p("p"),Hs.innerHTML=re,Ca=t(),m(Ls.$$.fragment),ua=t(),Ds=p("p"),this.h()},l(s){const l=Ze("svelte-u9bgzb",document.head);U=M(l,"META",{name:!0,content:!0}),l.forEach(a),f=n(s),d=M(s,"P",{}),Ie(d).forEach(a),h=n(s),y(I.$$.fragment,s),T=n(s),y(r.$$.fragment,s),Ps=n(s),g=M(s,"P",{"data-svelte-h":!0}),i(g)!=="svelte-1e4tw6k"&&(g.textContent=Za),Os=n(s),Z=M(s,"P",{"data-svelte-h":!0}),i(Z)!=="svelte-1p8ytws"&&(Z.textContent=va),sl=n(s),v=M(s,"OL",{"data-svelte-h":!0}),i(v)!=="svelte-az9yjs"&&(v.innerHTML=Ga),ll=n(s),y(b.$$.fragment,s),al=n(s),G=M(s,"P",{"data-svelte-h":!0}),i(G)!=="svelte-1aeak3y"&&(G.textContent=Wa),el=n(s),y(W.$$.fragment,s),tl=n(s),B=M(s,"P",{"data-svelte-h":!0}),i(B)!=="svelte-tahsr7"&&(B.innerHTML=Ba),nl=n(s),_=M(s,"P",{"data-svelte-h":!0}),i(_)!=="svelte-1ylwhj7"&&(_.textContent=_a),pl=n(s),y(A.$$.fragment,s),Ml=n(s),y(k.$$.fragment,s),il=n(s),V=M(s,"P",{"data-svelte-h":!0}),i(V)!=="svelte-iwy935"&&(V.innerHTML=Aa),ml=n(s),y(R.$$.fragment,s),yl=n(s),x=M(s,"P",{"data-svelte-h":!0}),i(x)!=="svelte-grk49t"&&(x.textContent=ka),cl=n(s),y(X.$$.fragment,s),Jl=n(s),$=M(s,"P",{"data-svelte-h":!0}),i($)!=="svelte-1kwmp31"&&($.textContent=Va),jl=n(s),y(F.$$.fragment,s),ol=n(s),Y=M(s,"P",{"data-svelte-h":!0}),i(Y)!=="svelte-19fjoci"&&(Y.textContent=Ra),wl=n(s),y(E.$$.fragment,s),Ul=n(s),z=M(s,"P",{"data-svelte-h":!0}),i(z)!=="svelte-1jzwz4y"&&(z.innerHTML=xa),Tl=n(s),S=M(s,"P",{"data-svelte-h":!0}),i(S)!=="svelte-wije3b"&&(S.innerHTML=Xa),hl=n(s),Q=M(s,"P",{"data-svelte-h":!0}),i(Q)!=="svelte-frlow"&&(Q.textContent=$a),rl=n(s),N=M(s,"UL",{"data-svelte-h":!0}),i(N)!=="svelte-mva0se"&&(N.innerHTML=Fa),dl=n(s),y(H.$$.fragment,s),Il=n(s),L=M(s,"P",{"data-svelte-h":!0}),i(L)!=="svelte-wq610i"&&(L.textContent=Ya),bl=n(s),y(q.$$.fragment,s),Cl=n(s),D=M(s,"P",{"data-svelte-h":!0}),i(D)!=="svelte-ttdm1b"&&(D.textContent=Ea),ul=n(s),y(K.$$.fragment,s),fl=n(s),P=M(s,"P",{"data-svelte-h":!0}),i(P)!=="svelte-dk8sqc"&&(P.textContent=za),gl=n(s),y(O.$$.fragment,s),Zl=n(s),ss=M(s,"P",{"data-svelte-h":!0}),i(ss)!=="svelte-1hvsgvl"&&(ss.innerHTML=Sa),vl=n(s),ls=M(s,"P",{"data-svelte-h":!0}),i(ls)!=="svelte-1y3ap4p"&&(ls.innerHTML=Qa),Gl=n(s),y(as.$$.fragment,s),Wl=n(s),es=M(s,"P",{"data-svelte-h":!0}),i(es)!=="svelte-56d3gl"&&(es.innerHTML=Na),Bl=n(s),y(ts.$$.fragment,s),_l=n(s),ns=M(s,"P",{"data-svelte-h":!0}),i(ns)!=="svelte-1nsvvgc"&&(ns.innerHTML=Ha),Al=n(s),ps=M(s,"P",{"data-svelte-h":!0}),i(ps)!=="svelte-17axae2"&&(ps.textContent=La),kl=n(s),Ms=M(s,"UL",{"data-svelte-h":!0}),i(Ms)!=="svelte-1it8h27"&&(Ms.innerHTML=qa),Vl=n(s),is=M(s,"P",{"data-svelte-h":!0}),i(is)!=="svelte-1kqynq"&&(is.textContent=Da),Rl=n(s),y(ms.$$.fragment,s),xl=n(s),ys=M(s,"P",{"data-svelte-h":!0}),i(ys)!=="svelte-16wyng8"&&(ys.textContent=Ka),Xl=n(s),y(cs.$$.fragment,s),$l=n(s),Js=M(s,"P",{"data-svelte-h":!0}),i(Js)!=="svelte-1uhq2so"&&(Js.textContent=Pa),Fl=n(s),y(js.$$.fragment,s),Yl=n(s),os=M(s,"P",{"data-svelte-h":!0}),i(os)!=="svelte-rggem1"&&(os.innerHTML=Oa),El=n(s),ws=M(s,"P",{"data-svelte-h":!0}),i(ws)!=="svelte-ueus3g"&&(ws.innerHTML=se),zl=n(s),y(Us.$$.fragment,s),Sl=n(s),y(Ts.$$.fragment,s),Ql=n(s),y(hs.$$.fragment,s),Nl=n(s),C=M(s,"DIV",{class:!0,"data-svelte-h":!0}),i(C)!=="svelte-1mxsghh"&&(C.innerHTML=le),Hl=n(s),y(rs.$$.fragment,s),Ll=n(s),ds=M(s,"P",{"data-svelte-h":!0}),i(ds)!=="svelte-u0mnbo"&&(ds.innerHTML=ae),ql=n(s),Is=M(s,"P",{"data-svelte-h":!0}),i(Is)!=="svelte-1syfva1"&&(Is.innerHTML=ee),Dl=n(s),y(bs.$$.fragment,s),Kl=n(s),Cs=M(s,"P",{"data-svelte-h":!0}),i(Cs)!=="svelte-1e5r3eo"&&(Cs.innerHTML=te),Pl=n(s),us=M(s,"P",{"data-svelte-h":!0}),i(us)!=="svelte-qq3j8q"&&(us.textContent=ne),Ol=n(s),y(fs.$$.fragment,s),sa=n(s),gs=M(s,"P",{"data-svelte-h":!0}),i(gs)!=="svelte-14h9crf"&&(gs.innerHTML=pe),la=n(s),Zs=M(s,"P",{"data-svelte-h":!0}),i(Zs)!=="svelte-k33hpd"&&(Zs.innerHTML=Me),aa=n(s),vs=M(s,"P",{"data-svelte-h":!0}),i(vs)!=="svelte-14aaz9d"&&(vs.innerHTML=ie),ea=n(s),y(Gs.$$.fragment,s),ta=n(s),Ws=M(s,"P",{"data-svelte-h":!0}),i(Ws)!=="svelte-1f0dnva"&&(Ws.innerHTML=me),na=n(s),y(Bs.$$.fragment,s),pa=n(s),_s=M(s,"P",{"data-svelte-h":!0}),i(_s)!=="svelte-qi4ah9"&&(_s.innerHTML=ye),Ma=n(s),As=M(s,"P",{"data-svelte-h":!0}),i(As)!=="svelte-2pyvog"&&(As.innerHTML=ce),ia=n(s),y(ks.$$.fragment,s),ma=n(s),Vs=M(s,"P",{"data-svelte-h":!0}),i(Vs)!=="svelte-tfexxr"&&(Vs.innerHTML=Je),ya=n(s),y(Rs.$$.fragment,s),ca=n(s),y(xs.$$.fragment,s),Ja=n(s),Xs=M(s,"P",{"data-svelte-h":!0}),i(Xs)!=="svelte-1ikqmfh"&&(Xs.textContent=je),ja=n(s),$s=M(s,"P",{"data-svelte-h":!0}),i($s)!=="svelte-qh6j99"&&($s.textContent=oe),oa=n(s),y(Fs.$$.fragment,s),wa=n(s),u=M(s,"DIV",{class:!0,"data-svelte-h":!0}),i(u)!=="svelte-556htt"&&(u.innerHTML=we),Ua=n(s),Ys=M(s,"P",{"data-svelte-h":!0}),i(Ys)!=="svelte-1wdp722"&&(Ys.innerHTML=Ue),Ta=n(s),y(Es.$$.fragment,s),ha=n(s),zs=M(s,"P",{"data-svelte-h":!0}),i(zs)!=="svelte-pg1zl2"&&(zs.innerHTML=Te),ra=n(s),y(Ss.$$.fragment,s),da=n(s),Qs=M(s,"P",{"data-svelte-h":!0}),i(Qs)!=="svelte-1yhqw6b"&&(Qs.innerHTML=he),Ia=n(s),y(Ns.$$.fragment,s),ba=n(s),Hs=M(s,"P",{"data-svelte-h":!0}),i(Hs)!=="svelte-1r50dhk"&&(Hs.innerHTML=re),Ca=n(s),y(Ls.$$.fragment,s),ua=n(s),Ds=M(s,"P",{}),Ie(Ds).forEach(a),this.h()},h(){ga(U,"name","hf:doc:metadata"),ga(U,"content",Ae),ga(C,"class","flex justify-center"),ga(u,"class","flex justify-center")},m(s,l){ve(document.head,U),e(s,f,l),e(s,d,l),e(s,h,l),c(I,s,l),e(s,T,l),c(r,s,l),e(s,Ps,l),e(s,g,l),e(s,Os,l),e(s,Z,l),e(s,sl,l),e(s,v,l),e(s,ll,l),c(b,s,l),e(s,al,l),e(s,G,l),e(s,el,l),c(W,s,l),e(s,tl,l),e(s,B,l),e(s,nl,l),e(s,_,l),e(s,pl,l),c(A,s,l),e(s,Ml,l),c(k,s,l),e(s,il,l),e(s,V,l),e(s,ml,l),c(R,s,l),e(s,yl,l),e(s,x,l),e(s,cl,l),c(X,s,l),e(s,Jl,l),e(s,$,l),e(s,jl,l),c(F,s,l),e(s,ol,l),e(s,Y,l),e(s,wl,l),c(E,s,l),e(s,Ul,l),e(s,z,l),e(s,Tl,l),e(s,S,l),e(s,hl,l),e(s,Q,l),e(s,rl,l),e(s,N,l),e(s,dl,l),c(H,s,l),e(s,Il,l),e(s,L,l),e(s,bl,l),c(q,s,l),e(s,Cl,l),e(s,D,l),e(s,ul,l),c(K,s,l),e(s,fl,l),e(s,P,l),e(s,gl,l),c(O,s,l),e(s,Zl,l),e(s,ss,l),e(s,vl,l),e(s,ls,l),e(s,Gl,l),c(as,s,l),e(s,Wl,l),e(s,es,l),e(s,Bl,l),c(ts,s,l),e(s,_l,l),e(s,ns,l),e(s,Al,l),e(s,ps,l),e(s,kl,l),e(s,Ms,l),e(s,Vl,l),e(s,is,l),e(s,Rl,l),c(ms,s,l),e(s,xl,l),e(s,ys,l),e(s,Xl,l),c(cs,s,l),e(s,$l,l),e(s,Js,l),e(s,Fl,l),c(js,s,l),e(s,Yl,l),e(s,os,l),e(s,El,l),e(s,ws,l),e(s,zl,l),c(Us,s,l),e(s,Sl,l),c(Ts,s,l),e(s,Ql,l),c(hs,s,l),e(s,Nl,l),e(s,C,l),e(s,Hl,l),c(rs,s,l),e(s,Ll,l),e(s,ds,l),e(s,ql,l),e(s,Is,l),e(s,Dl,l),c(bs,s,l),e(s,Kl,l),e(s,Cs,l),e(s,Pl,l),e(s,us,l),e(s,Ol,l),c(fs,s,l),e(s,sa,l),e(s,gs,l),e(s,la,l),e(s,Zs,l),e(s,aa,l),e(s,vs,l),e(s,ea,l),c(Gs,s,l),e(s,ta,l),e(s,Ws,l),e(s,na,l),c(Bs,s,l),e(s,pa,l),e(s,_s,l),e(s,Ma,l),e(s,As,l),e(s,ia,l),c(ks,s,l),e(s,ma,l),e(s,Vs,l),e(s,ya,l),c(Rs,s,l),e(s,ca,l),c(xs,s,l),e(s,Ja,l),e(s,Xs,l),e(s,ja,l),e(s,$s,l),e(s,oa,l),c(Fs,s,l),e(s,wa,l),e(s,u,l),e(s,Ua,l),e(s,Ys,l),e(s,Ta,l),c(Es,s,l),e(s,ha,l),e(s,zs,l),e(s,ra,l),c(Ss,s,l),e(s,da,l),e(s,Qs,l),e(s,Ia,l),c(Ns,s,l),e(s,ba,l),e(s,Hs,l),e(s,Ca,l),c(Ls,s,l),e(s,ua,l),e(s,Ds,l),fa=!0},p(s,[l]){const de={};l&2&&(de.$$scope={dirty:l,ctx:s}),b.$set(de)},i(s){fa||(J(I.$$.fragment,s),J(r.$$.fragment,s),J(b.$$.fragment,s),J(W.$$.fragment,s),J(A.$$.fragment,s),J(k.$$.fragment,s),J(R.$$.fragment,s),J(X.$$.fragment,s),J(F.$$.fragment,s),J(E.$$.fragment,s),J(H.$$.fragment,s),J(q.$$.fragment,s),J(K.$$.fragment,s),J(O.$$.fragment,s),J(as.$$.fragment,s),J(ts.$$.fragment,s),J(ms.$$.fragment,s),J(cs.$$.fragment,s),J(js.$$.fragment,s),J(Us.$$.fragment,s),J(Ts.$$.fragment,s),J(hs.$$.fragment,s),J(rs.$$.fragment,s),J(bs.$$.fragment,s),J(fs.$$.fragment,s),J(Gs.$$.fragment,s),J(Bs.$$.fragment,s),J(ks.$$.fragment,s),J(Rs.$$.fragment,s),J(xs.$$.fragment,s),J(Fs.$$.fragment,s),J(Es.$$.fragment,s),J(Ss.$$.fragment,s),J(Ns.$$.fragment,s),J(Ls.$$.fragment,s),fa=!0)},o(s){j(I.$$.fragment,s),j(r.$$.fragment,s),j(b.$$.fragment,s),j(W.$$.fragment,s),j(A.$$.fragment,s),j(k.$$.fragment,s),j(R.$$.fragment,s),j(X.$$.fragment,s),j(F.$$.fragment,s),j(E.$$.fragment,s),j(H.$$.fragment,s),j(q.$$.fragment,s),j(K.$$.fragment,s),j(O.$$.fragment,s),j(as.$$.fragment,s),j(ts.$$.fragment,s),j(ms.$$.fragment,s),j(cs.$$.fragment,s),j(js.$$.fragment,s),j(Us.$$.fragment,s),j(Ts.$$.fragment,s),j(hs.$$.fragment,s),j(rs.$$.fragment,s),j(bs.$$.fragment,s),j(fs.$$.fragment,s),j(Gs.$$.fragment,s),j(Bs.$$.fragment,s),j(ks.$$.fragment,s),j(Rs.$$.fragment,s),j(xs.$$.fragment,s),j(Fs.$$.fragment,s),j(Es.$$.fragment,s),j(Ss.$$.fragment,s),j(Ns.$$.fragment,s),j(Ls.$$.fragment,s),fa=!1},d(s){s&&(a(f),a(d),a(h),a(T),a(Ps),a(g),a(Os),a(Z),a(sl),a(v),a(ll),a(al),a(G),a(el),a(tl),a(B),a(nl),a(_),a(pl),a(Ml),a(il),a(V),a(ml),a(yl),a(x),a(cl),a(Jl),a($),a(jl),a(ol),a(Y),a(wl),a(Ul),a(z),a(Tl),a(S),a(hl),a(Q),a(rl),a(N),a(dl),a(Il),a(L),a(bl),a(Cl),a(D),a(ul),a(fl),a(P),a(gl),a(Zl),a(ss),a(vl),a(ls),a(Gl),a(Wl),a(es),a(Bl),a(_l),a(ns),a(Al),a(ps),a(kl),a(Ms),a(Vl),a(is),a(Rl),a(xl),a(ys),a(Xl),a($l),a(Js),a(Fl),a(Yl),a(os),a(El),a(ws),a(zl),a(Sl),a(Ql),a(Nl),a(C),a(Hl),a(Ll),a(ds),a(ql),a(Is),a(Dl),a(Kl),a(Cs),a(Pl),a(us),a(Ol),a(sa),a(gs),a(la),a(Zs),a(aa),a(vs),a(ea),a(ta),a(Ws),a(na),a(pa),a(_s),a(Ma),a(As),a(ia),a(ma),a(Vs),a(ya),a(ca),a(Ja),a(Xs),a(ja),a($s),a(oa),a(wa),a(u),a(Ua),a(Ys),a(Ta),a(ha),a(zs),a(ra),a(da),a(Qs),a(Ia),a(ba),a(Hs),a(Ca),a(ua),a(Ds)),a(U),o(I,s),o(r,s),o(b,s),o(W,s),o(A,s),o(k,s),o(R,s),o(X,s),o(F,s),o(E,s),o(H,s),o(q,s),o(K,s),o(O,s),o(as,s),o(ts,s),o(ms,s),o(cs,s),o(js,s),o(Us,s),o(Ts,s),o(hs,s),o(rs,s),o(bs,s),o(fs,s),o(Gs,s),o(Bs,s),o(ks,s),o(Rs,s),o(xs,s),o(Fs,s),o(Es,s),o(Ss,s),o(Ns,s),o(Ls,s)}}}const Ae='{"title":"영상 분류","local":"video-classification","sections":[{"title":"UCF101 데이터셋 불러오기","local":"load-ufc101-dataset","sections":[],"depth":2},{"title":"미세 조정하기 위해 모델 가져오기","local":"load-a-model-to-fine-tune","sections":[],"depth":2},{"title":"훈련을 위한 데이터 세트 준비하기","local":"prepare-the-datasets-for-training","sections":[],"depth":2},{"title":"더 나은 디버깅을 위해 전처리 영상 시각화하기","local":"visualize-the-preprocessed-video-for-better-debugging","sections":[],"depth":2},{"title":"모델 훈련하기","local":"train-the-model","sections":[],"depth":2},{"title":"추론하기","local":"inference","sections":[],"depth":2}],"depth":1}';function ke(Ks){return Ce(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ye extends fe{constructor(U){super(),ge(this,U,ke,_e,be,{})}}export{Ye as component};
