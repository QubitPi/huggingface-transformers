import{s as pe,o as re,n as Ot}from"../chunks/scheduler.56730f09.js";import{S as ie,i as me,g as h,s as p,r as u,A as oe,h as M,f as e,c as r,j as _t,u as $,x as j,k as Et,y as Ct,a as l,v as g,d as y,t as w,w as b}from"../chunks/index.1f144517.js";import{T as te}from"../chunks/Tip.41e845e5.js";import{Y as Dt}from"../chunks/Youtube.62e0f062.js";import{C as G}from"../chunks/CodeBlock.738eeccb.js";import{D as de}from"../chunks/DocNotebookDropdown.243c3df7.js";import{F as ne,M as ee}from"../chunks/Markdown.c541024b.js";import{H as kt}from"../chunks/Heading.57d46534.js";function ce(ct){let i,T=`사전 훈련된 가중치 중 일부가 사용되지 않고 일부 가중치가 무작위로 표시된다는 경고가 표시됩니다.
걱정마세요. 이것은 올바른 동작입니다! 사전 학습된 BERT 모델의 헤드는 폐기되고 무작위로 초기화된 분류 헤드로 대체됩니다. 이제 사전 학습된 모델의 지식으로 시퀀스 분류 작업을 위한 새로운 모델 헤드를 미세 튜닝 합니다.`;return{c(){i=h("p"),i.textContent=T},l(m){i=M(m,"P",{"data-svelte-h":!0}),j(i)!=="svelte-133r2kc"&&(i.textContent=T)},m(m,J){l(m,i,J)},p:Ot,d(m){m&&e(i)}}}function fe(ct){let i,T,m,J,Z,nt="🤗 Transformers는 🤗 Transformers 모델 훈련에 최적화된 <code>Trainer</code> 클래스를 제공하여 훈련 루프를 직접 작성하지 않고도 쉽게 훈련을 시작할 수 있습니다. <code>Trainer</code> API는 로깅(logging), 경사 누적(gradient accumulation), 혼합 정밀도(mixed precision) 등 다양한 훈련 옵션과 기능을 지원합니다.",A,pt,L='먼저 모델을 가져오고 예상되는 레이블 수를 지정합니다. Yelp 리뷰 <a href="https://huggingface.co/datasets/yelp_review_full#data-fields" rel="nofollow">데이터셋 카드</a>에서 5개의 레이블이 있음을 알 수 있습니다:',rt,R,Q,ft,W,U,ht,q,Mt="다음으로 정할 수 있는 모든 하이퍼파라미터와 다양한 훈련 옵션을 활성화하기 위한 플래그를 포함하는 <code>TrainingArguments</code> 클래스를 생성합니다.",X,H,lt='이 튜토리얼에서는 기본 훈련 <a href="https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments" rel="nofollow">하이퍼파라미터</a>로 시작하지만, 자유롭게 실험하여 여러분들에게 맞는 최적의 설정을 찾을 수 있습니다.',ut,S,$t="훈련에서 체크포인트(checkpoints)를 저장할 위치를 지정합니다:",P,gt,V,I,it,K,mt=`<code>Trainer</code>는 훈련 중에 모델 성능을 자동으로 평가하지 않습니다. 평가 지표를 계산하고 보고할 함수를 <code>Trainer</code>에 전달해야 합니다.
<a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">🤗 Evaluate</a> 라이브러리는 <a href="https://huggingface.co/spaces/evaluate-metric/accuracy" rel="nofollow"><code>evaluate.load</code></a> 함수로 로드할 수 있는 간단한 <code>accuracy</code>함수를 제공합니다 (자세한 내용은 <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">둘러보기</a>를 참조하세요):`,x,D,ot,O,dt="<code>metric</code>에서 <code>compute</code>를 호출하여 예측의 정확도를 계산합니다. 예측을 <code>compute</code>에 전달하기 전에 예측을 로짓으로 변환해야 합니다(모든 🤗 Transformers 모델은 로짓으로 반환한다는 점을 기억하세요):",z,tt,at,C,Zt="미세 튜닝 중에 평가 지표를 모니터링하려면 훈련 인수에 <code>evaluation_strategy</code> 파라미터를 지정하여 각 에폭이 끝날 때 평가 지표를 확인할 수 있습니다:",yt,k,Ut,Y,N,et,F="모델, 훈련 인수, 훈련 및 테스트 데이터셋, 평가 함수가 포함된 <code>Trainer</code> 객체를 만듭니다:",Tt,_,st,v,bt="그리고 <code>train()</code>을 호출하여 모델을 미세 튜닝합니다:",wt,B,E;return i=new Dt({props:{id:"nvBXf7s7vTI"}}),m=new kt({props:{title:"파이토치 Trainer로 훈련하기",local:"train-with-pytorch-trainer",headingTag:"h2"}}),R=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyJTJDJTIwbnVtX2xhYmVscyUzRDUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">5</span>)`,wrap:!1}}),ft=new te({props:{$$slots:{default:[ce]},$$scope:{ctx:ct}}}),U=new kt({props:{title:"하이퍼파라미터 훈련",local:"training-hyperparameters",headingTag:"h3"}}),gt=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluaW5nQXJndW1lbnRzJTBBJTBBdHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKG91dHB1dF9kaXIlM0QlMjJ0ZXN0X3RyYWluZXIlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;test_trainer&quot;</span>)`,wrap:!1}}),I=new kt({props:{title:"평가 하기",local:"evaluate",headingTag:"h3"}}),D=new G({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBaW1wb3J0JTIwZXZhbHVhdGUlMEElMEFtZXRyaWMlMjAlM0QlMjBldmFsdWF0ZS5sb2FkKCUyMmFjY3VyYWN5JTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`,wrap:!1}}),tt=new G({props:{code:"ZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMkMlMjBsYWJlbHMlMjAlM0QlMjBldmFsX3ByZWQlMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUyMCUzRCUyMG5wLmFyZ21heChsb2dpdHMlMkMlMjBheGlzJTNELTEpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwbWV0cmljLmNvbXB1dGUocHJlZGljdGlvbnMlM0RwcmVkaWN0aW9ucyUyQyUyMHJlZmVyZW5jZXMlM0RsYWJlbHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    logits, labels = eval_pred
<span class="hljs-meta">... </span>    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)`,wrap:!1}}),k=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluaW5nQXJndW1lbnRzJTJDJTIwVHJhaW5lciUwQSUwQXRyYWluaW5nX2FyZ3MlMjAlM0QlMjBUcmFpbmluZ0FyZ3VtZW50cyhvdXRwdXRfZGlyJTNEJTIydGVzdF90cmFpbmVyJTIyJTJDJTIwZXZhbHVhdGlvbl9zdHJhdGVneSUzRCUyMmVwb2NoJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;test_trainer&quot;</span>, evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>)`,wrap:!1}}),Y=new kt({props:{title:"훈련 하기",local:"trainer",headingTag:"h3"}}),_=new G({props:{code:"dHJhaW5lciUyMCUzRCUyMFRyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0Rtb2RlbCUyQyUwQSUyMCUyMCUyMCUyMGFyZ3MlM0R0cmFpbmluZ19hcmdzJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRHNtYWxsX3RyYWluX2RhdGFzZXQlMkMlMEElMjAlMjAlMjAlMjBldmFsX2RhdGFzZXQlM0RzbWFsbF9ldmFsX2RhdGFzZXQlMkMlMEElMjAlMjAlMjAlMjBjb21wdXRlX21ldHJpY3MlM0Rjb21wdXRlX21ldHJpY3MlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=small_train_dataset,
<span class="hljs-meta">... </span>    eval_dataset=small_eval_dataset,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)`,wrap:!1}}),B=new G({props:{code:"dHJhaW5lci50cmFpbigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()',wrap:!1}}),{c(){u(i.$$.fragment),T=p(),u(m.$$.fragment),J=p(),Z=h("p"),Z.innerHTML=nt,A=p(),pt=h("p"),pt.innerHTML=L,rt=p(),u(R.$$.fragment),Q=p(),u(ft.$$.fragment),W=p(),u(U.$$.fragment),ht=p(),q=h("p"),q.innerHTML=Mt,X=p(),H=h("p"),H.innerHTML=lt,ut=p(),S=h("p"),S.textContent=$t,P=p(),u(gt.$$.fragment),V=p(),u(I.$$.fragment),it=p(),K=h("p"),K.innerHTML=mt,x=p(),u(D.$$.fragment),ot=p(),O=h("p"),O.innerHTML=dt,z=p(),u(tt.$$.fragment),at=p(),C=h("p"),C.innerHTML=Zt,yt=p(),u(k.$$.fragment),Ut=p(),u(Y.$$.fragment),N=p(),et=h("p"),et.innerHTML=F,Tt=p(),u(_.$$.fragment),st=p(),v=h("p"),v.innerHTML=bt,wt=p(),u(B.$$.fragment)},l(a){$(i.$$.fragment,a),T=r(a),$(m.$$.fragment,a),J=r(a),Z=M(a,"P",{"data-svelte-h":!0}),j(Z)!=="svelte-1a30hsl"&&(Z.innerHTML=nt),A=r(a),pt=M(a,"P",{"data-svelte-h":!0}),j(pt)!=="svelte-vv5j33"&&(pt.innerHTML=L),rt=r(a),$(R.$$.fragment,a),Q=r(a),$(ft.$$.fragment,a),W=r(a),$(U.$$.fragment,a),ht=r(a),q=M(a,"P",{"data-svelte-h":!0}),j(q)!=="svelte-1sl91ao"&&(q.innerHTML=Mt),X=r(a),H=M(a,"P",{"data-svelte-h":!0}),j(H)!=="svelte-17gf6c"&&(H.innerHTML=lt),ut=r(a),S=M(a,"P",{"data-svelte-h":!0}),j(S)!=="svelte-3gh4w5"&&(S.textContent=$t),P=r(a),$(gt.$$.fragment,a),V=r(a),$(I.$$.fragment,a),it=r(a),K=M(a,"P",{"data-svelte-h":!0}),j(K)!=="svelte-1ofd291"&&(K.innerHTML=mt),x=r(a),$(D.$$.fragment,a),ot=r(a),O=M(a,"P",{"data-svelte-h":!0}),j(O)!=="svelte-fpot8d"&&(O.innerHTML=dt),z=r(a),$(tt.$$.fragment,a),at=r(a),C=M(a,"P",{"data-svelte-h":!0}),j(C)!=="svelte-b8mk68"&&(C.innerHTML=Zt),yt=r(a),$(k.$$.fragment,a),Ut=r(a),$(Y.$$.fragment,a),N=r(a),et=M(a,"P",{"data-svelte-h":!0}),j(et)!=="svelte-w01mtl"&&(et.innerHTML=F),Tt=r(a),$(_.$$.fragment,a),st=r(a),v=M(a,"P",{"data-svelte-h":!0}),j(v)!=="svelte-xfml58"&&(v.innerHTML=bt),wt=r(a),$(B.$$.fragment,a)},m(a,c){g(i,a,c),l(a,T,c),g(m,a,c),l(a,J,c),l(a,Z,c),l(a,A,c),l(a,pt,c),l(a,rt,c),g(R,a,c),l(a,Q,c),g(ft,a,c),l(a,W,c),g(U,a,c),l(a,ht,c),l(a,q,c),l(a,X,c),l(a,H,c),l(a,ut,c),l(a,S,c),l(a,P,c),g(gt,a,c),l(a,V,c),g(I,a,c),l(a,it,c),l(a,K,c),l(a,x,c),g(D,a,c),l(a,ot,c),l(a,O,c),l(a,z,c),g(tt,a,c),l(a,at,c),l(a,C,c),l(a,yt,c),g(k,a,c),l(a,Ut,c),g(Y,a,c),l(a,N,c),l(a,et,c),l(a,Tt,c),g(_,a,c),l(a,st,c),l(a,v,c),l(a,wt,c),g(B,a,c),E=!0},p(a,c){const jt={};c&2&&(jt.$$scope={dirty:c,ctx:a}),ft.$set(jt)},i(a){E||(y(i.$$.fragment,a),y(m.$$.fragment,a),y(R.$$.fragment,a),y(ft.$$.fragment,a),y(U.$$.fragment,a),y(gt.$$.fragment,a),y(I.$$.fragment,a),y(D.$$.fragment,a),y(tt.$$.fragment,a),y(k.$$.fragment,a),y(Y.$$.fragment,a),y(_.$$.fragment,a),y(B.$$.fragment,a),E=!0)},o(a){w(i.$$.fragment,a),w(m.$$.fragment,a),w(R.$$.fragment,a),w(ft.$$.fragment,a),w(U.$$.fragment,a),w(gt.$$.fragment,a),w(I.$$.fragment,a),w(D.$$.fragment,a),w(tt.$$.fragment,a),w(k.$$.fragment,a),w(Y.$$.fragment,a),w(_.$$.fragment,a),w(B.$$.fragment,a),E=!1},d(a){a&&(e(T),e(J),e(Z),e(A),e(pt),e(rt),e(Q),e(W),e(ht),e(q),e(X),e(H),e(ut),e(S),e(P),e(V),e(it),e(K),e(x),e(ot),e(O),e(z),e(at),e(C),e(yt),e(Ut),e(N),e(et),e(Tt),e(st),e(v),e(wt)),b(i,a),b(m,a),b(R,a),b(ft,a),b(U,a),b(gt,a),b(I,a),b(D,a),b(tt,a),b(k,a),b(Y,a),b(_,a),b(B,a)}}}function he(ct){let i,T;return i=new ee({props:{$$slots:{default:[fe]},$$scope:{ctx:ct}}}),{c(){u(i.$$.fragment)},l(m){$(i.$$.fragment,m)},m(m,J){g(i,m,J),T=!0},p(m,J){const Z={};J&2&&(Z.$$scope={dirty:J,ctx:m}),i.$set(Z)},i(m){T||(y(i.$$.fragment,m),T=!0)},o(m){w(i.$$.fragment,m),T=!1},d(m){b(i,m)}}}function Me(ct){let i,T=`모델을 <code>compile()</code>할 때 손실 인수를 모델에 전달할 필요가 없습니다!
이 인수를 비워두면 허깅 페이스 모델은 작업과 모델 아키텍처에 적합한 손실을 자동으로 선택합니다.
원한다면 언제든지 직접 손실을 지정하여 이를 재정의할 수 있습니다!`;return{c(){i=h("p"),i.innerHTML=T},l(m){i=M(m,"P",{"data-svelte-h":!0}),j(i)!=="svelte-z0fgk0"&&(i.innerHTML=T)},m(m,J){l(m,i,J)},p:Ot,d(m){m&&e(i)}}}function ue(ct){let i,T,m,J,Z,nt,A,pt="Keras API를 사용하여 텐서플로우에서 🤗 Transformers 모델을 훈련할 수도 있습니다!",L,rt,R,Q,ft=`Keras API로 🤗 Transformers 모델을 학습시키려면 데이터셋을 Keras가 이해할 수 있는 형식으로 변환해야 합니다.
데이터 세트가 작은 경우, 전체를 NumPy 배열로 변환하여 Keras로 전달하면 됩니다.
더 복잡한 작업을 수행하기 전에 먼저 이 작업을 시도해 보겠습니다.`,W,U,ht=`먼저 데이터 세트를 로드합니다. <a href="https://huggingface.co/datasets/glue" rel="nofollow">GLUE 벤치마크</a>의 CoLA 데이터 세트를 사용하겠습니다.
간단한 바이너리 텍스트 분류 작업이므로 지금은 훈련 데이터 분할만 사용합니다.`,q,Mt,X,H,lt="다음으로 토크나이저를 로드하고 데이터를 NumPy 배열로 토큰화합니다. 레이블은 이미 0과 1로 된 리스트이기 때문에 토큰화하지 않고 바로 NumPy 배열로 변환할 수 있습니다!",ut,S,$t,P,gt='마지막으로 모델을 로드, <a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a>, <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a>합니다:',V,I,it,K,mt,x,D=`이 접근 방식은 소규모 데이터 집합에서는 잘 작동하지만, 대규모 데이터 집합에서는 문제가 될 수 있습니다. 왜 그럴까요?
토큰화된 배열과 레이블을 메모리에 완전히 로드하고 NumPy는 “들쭉날쭉한” 배열을 처리하지 않기 때문에,
모든 토큰화된 샘플을 전체 데이터셋에서 가장 긴 샘플의 길이만큼 패딩해야 합니다. 이렇게 하면 배열이 훨씬 더 커지고 이 패딩 토큰으로 인해 학습 속도도 느려집니다!`,ot,O,dt,z,tt=`학습 속도가 느려지는 것을 피하려면 데이터를 <code>tf.data.Dataset</code>으로 로드할 수 있습니다. 원한다면 직접
<code>tf.data</code> 파이프라인을 직접 작성할 수도 있지만, 이 작업을 간편하게 수행하는 수 있는 두 가지 방법이 있습니다:`,at,C,Zt=`<li><code>prepare_tf_dataset()</code>: 대부분의 경우 이 방법을 권장합니다. 모델의 메서드이기 때문에 모델을 검사하여 모델 입력으로 사용할 수 있는 열을 자동으로 파악하고
나머지는 버려서 더 단순하고 성능이 좋은 데이터 집합을 만들 수 있습니다.</li> <li><code>to_tf_dataset</code>: 이 방법은 좀 더 낮은 수준이며, 포함할 ‘열’과 ‘레이블’을 정확히 지정하여
데이터셋을 생성하는 방법을 정확히 제어하고 싶을 때 유용하며, 포함할 ‘columns’과 ‘label_cols’을 정확히 지정할 수 있습니다.</li>`,yt,k,Ut="<code>prepare_tf_dataset()</code>을 사용하려면 먼저 다음 코드 샘플과 같이 토크나이저 출력을 데이터 세트에 열로 추가해야 합니다:",Y,N,et,F,Tt=`허깅 페이스 데이터셋은 기본적으로 디스크에 저장되므로 메모리 사용량을 늘리지 않는다는 점을 기억하세요!
열이 추가되면 데이터셋에서 배치를 스트리밍하고 각 배치에 패딩을 추가할 수 있으므로 전체 데이터셋에 패딩을 추가하는 것보다 패딩 토큰의 수를 크게 줄일 수 있습니다.`,_,st,v,bt,wt=`위의 코드 샘플에서는 배치가 로드될 때 올바르게 패딩할 수 있도록 <code>prepare_tf_dataset</code>에 토크나이저를 전달해야 합니다.
데이터셋의 모든 샘플 길이가 같고 패딩이 필요하지 않은 경우 이 인수를 건너뛸 수 있습니다.
샘플을 채우는 것보다 더 복잡한 작업(예: 마스킹된 언어의 토큰 손상 모델링)을 수행하기 위해 토큰을 손상시켜야 하는 경우,
<code>collate_fn</code> 인수를 사용하여 샘플 목록을 배치로 변환하고 원하는 전처리를 적용할 함수를 전달할 수 있습니다.
<a href="https://github.com/huggingface/transformers/tree/main/examples" rel="nofollow">예시</a> 또는
<a href="https://huggingface.co/docs/transformers/notebooks" rel="nofollow">노트북</a>을 참조하여 이 접근 방식이 실제로 작동하는 모습을 확인하세요.`,B,E,a="<code>tf.data.Dataset</code>을 생성한 후에는 이전과 마찬가지로 모델을 컴파일하고 훈련(fit)할 수 있습니다:",c,jt,Jt;return m=new Dt({props:{id:"rnTGBy2ax1c"}}),Z=new kt({props:{title:"Keras로 텐서플로우 모델 훈련하기",local:"train-a-tensorflow-model-with-keras",headingTag:"h2"}}),rt=new kt({props:{title:"Keras용 데이터 로드",local:"loading-data-for-keras",headingTag:"h3"}}),Mt=new G({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJnbHVlJTIyJTJDJTIwJTIyY29sYSUyMiklMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldCU1QiUyMnRyYWluJTIyJTVEJTIwJTIwJTIzJTIwSnVzdCUyMHRha2UlMjB0aGUlMjB0cmFpbmluZyUyMHNwbGl0JTIwZm9yJTIwbm93",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;cola&quot;</span>)
dataset = dataset[<span class="hljs-string">&quot;train&quot;</span>]  <span class="hljs-comment"># Just take the training split for now</span>`,wrap:!1}}),S=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEF0b2tlbml6ZWRfZGF0YSUyMCUzRCUyMHRva2VuaXplcihkYXRhc2V0JTVCJTIyc2VudGVuY2UlMjIlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMm5wJTIyJTJDJTIwcGFkZGluZyUzRFRydWUpJTBBJTIzJTIwVG9rZW5pemVyJTIwcmV0dXJucyUyMGElMjBCYXRjaEVuY29kaW5nJTJDJTIwYnV0JTIwd2UlMjBjb252ZXJ0JTIwdGhhdCUyMHRvJTIwYSUyMGRpY3QlMjBmb3IlMjBLZXJhcyUwQXRva2VuaXplZF9kYXRhJTIwJTNEJTIwZGljdCh0b2tlbml6ZWRfZGF0YSklMEElMEFsYWJlbHMlMjAlM0QlMjBucC5hcnJheShkYXRhc2V0JTVCJTIybGFiZWwlMjIlNUQpJTIwJTIwJTIzJTIwTGFiZWwlMjBpcyUyMGFscmVhZHklMjBhbiUyMGFycmF5JTIwb2YlMjAwJTIwYW5kJTIwMQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
tokenized_data = tokenizer(dataset[<span class="hljs-string">&quot;sentence&quot;</span>], return_tensors=<span class="hljs-string">&quot;np&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras</span>
tokenized_data = <span class="hljs-built_in">dict</span>(tokenized_data)

labels = np.array(dataset[<span class="hljs-string">&quot;label&quot;</span>])  <span class="hljs-comment"># Label is already an array of 0 and 1</span>`,wrap:!1}}),I=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQWZyb20lMjB0ZW5zb3JmbG93LmtlcmFzLm9wdGltaXplcnMlMjBpbXBvcnQlMjBBZGFtJTBBJTBBJTIzJTIwTG9hZCUyMGFuZCUyMGNvbXBpbGUlMjBvdXIlMjBtb2RlbCUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMjMlMjBMb3dlciUyMGxlYXJuaW5nJTIwcmF0ZXMlMjBhcmUlMjBvZnRlbiUyMGJldHRlciUyMGZvciUyMGZpbmUtdHVuaW5nJTIwdHJhbnNmb3JtZXJzJTBBbW9kZWwuY29tcGlsZShvcHRpbWl6ZXIlM0RBZGFtKDNlLTUpKSUwQSUwQW1vZGVsLmZpdCh0b2tlbml6ZWRfZGF0YSUyQyUyMGxhYmVscyk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

<span class="hljs-comment"># Load and compile our model</span>
model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-comment"># Lower learning rates are often better for fine-tuning transformers</span>
model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">3e-5</span>))

model.fit(tokenized_data, labels)`,wrap:!1}}),K=new te({props:{$$slots:{default:[Me]},$$scope:{ctx:ct}}}),O=new kt({props:{title:"데이터를 tf.data.Dataset으로 로드하기",local:"loading-data-as-a-tfdatadataset",headingTag:"h3"}}),N=new G({props:{code:"ZGVmJTIwdG9rZW5pemVfZGF0YXNldChkYXRhKSUzQSUwQSUyMCUyMCUyMCUyMCUyMyUyMEtleXMlMjBvZiUyMHRoZSUyMHJldHVybmVkJTIwZGljdGlvbmFyeSUyMHdpbGwlMjBiZSUyMGFkZGVkJTIwdG8lMjB0aGUlMjBkYXRhc2V0JTIwYXMlMjBjb2x1bW5zJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwdG9rZW5pemVyKGRhdGElNUIlMjJ0ZXh0JTIyJTVEKSUwQSUwQSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0Lm1hcCh0b2tlbml6ZV9kYXRhc2V0KQ==",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_dataset</span>(<span class="hljs-params">data</span>):
    <span class="hljs-comment"># Keys of the returned dictionary will be added to the dataset as columns</span>
    <span class="hljs-keyword">return</span> tokenizer(data[<span class="hljs-string">&quot;text&quot;</span>])


dataset = dataset.<span class="hljs-built_in">map</span>(tokenize_dataset)`,wrap:!1}}),st=new G({props:{code:"dGZfZGF0YXNldCUyMCUzRCUyMG1vZGVsLnByZXBhcmVfdGZfZGF0YXNldChkYXRhc2V0JTJDJTIwYmF0Y2hfc2l6ZSUzRDE2JTJDJTIwc2h1ZmZsZSUzRFRydWUlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_dataset = model.prepare_tf_dataset(dataset, batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>, tokenizer=tokenizer)',wrap:!1}}),jt=new G({props:{code:"bW9kZWwuY29tcGlsZShvcHRpbWl6ZXIlM0RBZGFtKDNlLTUpKSUwQSUwQW1vZGVsLmZpdCh0Zl9kYXRhc2V0KQ==",highlighted:`model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">3e-5</span>))

model.fit(tf_dataset)`,wrap:!1}}),{c(){i=h("a"),T=p(),u(m.$$.fragment),J=p(),u(Z.$$.fragment),nt=p(),A=h("p"),A.textContent=pt,L=p(),u(rt.$$.fragment),R=p(),Q=h("p"),Q.textContent=ft,W=p(),U=h("p"),U.innerHTML=ht,q=p(),u(Mt.$$.fragment),X=p(),H=h("p"),H.textContent=lt,ut=p(),u(S.$$.fragment),$t=p(),P=h("p"),P.innerHTML=gt,V=p(),u(I.$$.fragment),it=p(),u(K.$$.fragment),mt=p(),x=h("p"),x.textContent=D,ot=p(),u(O.$$.fragment),dt=p(),z=h("p"),z.innerHTML=tt,at=p(),C=h("ul"),C.innerHTML=Zt,yt=p(),k=h("p"),k.innerHTML=Ut,Y=p(),u(N.$$.fragment),et=p(),F=h("p"),F.textContent=Tt,_=p(),u(st.$$.fragment),v=p(),bt=h("p"),bt.innerHTML=wt,B=p(),E=h("p"),E.innerHTML=a,c=p(),u(jt.$$.fragment),this.h()},l(n){i=M(n,"A",{id:!0}),_t(i).forEach(e),T=r(n),$(m.$$.fragment,n),J=r(n),$(Z.$$.fragment,n),nt=r(n),A=M(n,"P",{"data-svelte-h":!0}),j(A)!=="svelte-ze9716"&&(A.textContent=pt),L=r(n),$(rt.$$.fragment,n),R=r(n),Q=M(n,"P",{"data-svelte-h":!0}),j(Q)!=="svelte-15pnkld"&&(Q.textContent=ft),W=r(n),U=M(n,"P",{"data-svelte-h":!0}),j(U)!=="svelte-1u2iume"&&(U.innerHTML=ht),q=r(n),$(Mt.$$.fragment,n),X=r(n),H=M(n,"P",{"data-svelte-h":!0}),j(H)!=="svelte-kduu5a"&&(H.textContent=lt),ut=r(n),$(S.$$.fragment,n),$t=r(n),P=M(n,"P",{"data-svelte-h":!0}),j(P)!=="svelte-1mtm6sq"&&(P.innerHTML=gt),V=r(n),$(I.$$.fragment,n),it=r(n),$(K.$$.fragment,n),mt=r(n),x=M(n,"P",{"data-svelte-h":!0}),j(x)!=="svelte-1ggdj2"&&(x.textContent=D),ot=r(n),$(O.$$.fragment,n),dt=r(n),z=M(n,"P",{"data-svelte-h":!0}),j(z)!=="svelte-5ru8h1"&&(z.innerHTML=tt),at=r(n),C=M(n,"UL",{"data-svelte-h":!0}),j(C)!=="svelte-10w0g1z"&&(C.innerHTML=Zt),yt=r(n),k=M(n,"P",{"data-svelte-h":!0}),j(k)!=="svelte-10gw6tz"&&(k.innerHTML=Ut),Y=r(n),$(N.$$.fragment,n),et=r(n),F=M(n,"P",{"data-svelte-h":!0}),j(F)!=="svelte-10zjmdj"&&(F.textContent=Tt),_=r(n),$(st.$$.fragment,n),v=r(n),bt=M(n,"P",{"data-svelte-h":!0}),j(bt)!=="svelte-1k6j3lz"&&(bt.innerHTML=wt),B=r(n),E=M(n,"P",{"data-svelte-h":!0}),j(E)!=="svelte-zwzwju"&&(E.innerHTML=a),c=r(n),$(jt.$$.fragment,n),this.h()},h(){Et(i,"id","keras")},m(n,f){l(n,i,f),l(n,T,f),g(m,n,f),l(n,J,f),g(Z,n,f),l(n,nt,f),l(n,A,f),l(n,L,f),g(rt,n,f),l(n,R,f),l(n,Q,f),l(n,W,f),l(n,U,f),l(n,q,f),g(Mt,n,f),l(n,X,f),l(n,H,f),l(n,ut,f),g(S,n,f),l(n,$t,f),l(n,P,f),l(n,V,f),g(I,n,f),l(n,it,f),g(K,n,f),l(n,mt,f),l(n,x,f),l(n,ot,f),g(O,n,f),l(n,dt,f),l(n,z,f),l(n,at,f),l(n,C,f),l(n,yt,f),l(n,k,f),l(n,Y,f),g(N,n,f),l(n,et,f),l(n,F,f),l(n,_,f),g(st,n,f),l(n,v,f),l(n,bt,f),l(n,B,f),l(n,E,f),l(n,c,f),g(jt,n,f),Jt=!0},p(n,f){const Gt={};f&2&&(Gt.$$scope={dirty:f,ctx:n}),K.$set(Gt)},i(n){Jt||(y(m.$$.fragment,n),y(Z.$$.fragment,n),y(rt.$$.fragment,n),y(Mt.$$.fragment,n),y(S.$$.fragment,n),y(I.$$.fragment,n),y(K.$$.fragment,n),y(O.$$.fragment,n),y(N.$$.fragment,n),y(st.$$.fragment,n),y(jt.$$.fragment,n),Jt=!0)},o(n){w(m.$$.fragment,n),w(Z.$$.fragment,n),w(rt.$$.fragment,n),w(Mt.$$.fragment,n),w(S.$$.fragment,n),w(I.$$.fragment,n),w(K.$$.fragment,n),w(O.$$.fragment,n),w(N.$$.fragment,n),w(st.$$.fragment,n),w(jt.$$.fragment,n),Jt=!1},d(n){n&&(e(i),e(T),e(J),e(nt),e(A),e(L),e(R),e(Q),e(W),e(U),e(q),e(X),e(H),e(ut),e($t),e(P),e(V),e(it),e(mt),e(x),e(ot),e(dt),e(z),e(at),e(C),e(yt),e(k),e(Y),e(et),e(F),e(_),e(v),e(bt),e(B),e(E),e(c)),b(m,n),b(Z,n),b(rt,n),b(Mt,n),b(S,n),b(I,n),b(K,n),b(O,n),b(N,n),b(st,n),b(jt,n)}}}function $e(ct){let i,T;return i=new ee({props:{$$slots:{default:[ue]},$$scope:{ctx:ct}}}),{c(){u(i.$$.fragment)},l(m){$(i.$$.fragment,m)},m(m,J){g(i,m,J),T=!0},p(m,J){const Z={};J&2&&(Z.$$scope={dirty:J,ctx:m}),i.$set(Z)},i(m){T||(y(i.$$.fragment,m),T=!0)},o(m){w(i.$$.fragment,m),T=!1},d(m){b(i,m)}}}function ge(ct){let i,T='<a href="https://colab.research.google.com/" rel="nofollow">Colaboratory</a> 또는 <a href="https://studiolab.sagemaker.aws/" rel="nofollow">SageMaker StudioLab</a>과 같은 호스팅 노트북이 없는 경우 클라우드 GPU에 무료로 액세스할 수 있습니다.';return{c(){i=h("p"),i.innerHTML=T},l(m){i=M(m,"P",{"data-svelte-h":!0}),j(i)!=="svelte-1ctkiwu"&&(i.innerHTML=T)},m(m,J){l(m,i,J)},p:Ot,d(m){m&&e(i)}}}function ye(ct){let i,T,m,J="<code>Trainer</code>는 훈련 루프를 처리하며 한 줄의 코드로 모델을 미세 조정할 수 있습니다. 직접 훈련 루프를 작성하는 것을 선호하는 사용자의 경우, 기본 PyTorch에서 🤗 Transformers 모델을 미세 조정할 수도 있습니다.",Z,nt,A="이 시점에서 노트북을 다시 시작하거나 다음 코드를 실행해 메모리를 확보해야 할 수 있습니다:",pt,L,rt,R,Q="다음으로, ‘토큰화된 데이터셋’을 수동으로 후처리하여 훈련련에 사용할 수 있도록 준비합니다.",ft,W,U,ht,q="모델이 원시 텍스트를 입력으로 허용하지 않으므로 <code>text</code> 열을 제거합니다:",Mt,X,H,lt,ut,S="모델에서 인수의 이름이 <code>labels</code>로 지정될 것으로 예상하므로 <code>label</code> 열의 이름을 <code>labels</code>로 변경합니다:",$t,P,gt,V,I,it="데이터셋의 형식을 List 대신 PyTorch 텐서를 반환하도록 설정합니다:",K,mt,x,D,ot="그리고 앞서 표시된 대로 데이터셋의 더 작은 하위 집합을 생성하여 미세 조정 속도를 높입니다:",O,dt,z,tt,at,C,Zt="훈련 및 테스트 데이터셋에 대한 ‘DataLoader’를 생성하여 데이터 배치를 반복할 수 있습니다:",yt,k,Ut,Y,N="예측을 위한 레이블 개수를 사용하여 모델을 로드합니다:",et,F,Tt,_,st,v,bt='옵티마이저와 학습 속도 스케줄러를 생성하여 모델을 미세 조정합니다. 파이토치에서 제공하는 <a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" rel="nofollow"><code>AdamW</code></a> 옵티마이저를 사용해 보겠습니다:',wt,B,E,a,c="<code>Trainer</code>에서 기본 학습 속도 스케줄러를 생성합니다:",jt,Jt,n,f,Gt="마지막으로, GPU에 액세스할 수 있는 경우 ‘device’를 지정하여 GPU를 사용하도록 합니다. 그렇지 않으면 CPU에서 훈련하며 몇 분이 아닌 몇 시간이 걸릴 수 있습니다.",It,vt,s,o,Wt,Rt,se="이제 훈련할 준비가 되었습니다! 🥳",Ht,Xt,Nt,Vt,le='훈련 진행 상황을 추적하려면 <a href="https://tqdm.github.io/" rel="nofollow">tqdm</a> 라이브러리를 사용하여 트레이닝 단계 수에 진행률 표시줄을 추가하세요:',At,xt,Lt,zt,Qt,Yt,ae="<code>Trainer</code>에 평가 함수를 추가한 방법과 마찬가지로, 훈련 루프를 직접 작성할 때도 동일한 작업을 수행해야 합니다. 하지만 이번에는 각 에포크가 끝날 때마다 평가지표를 계산하여 보고하는 대신, <code>add_batch</code>를 사용하여 모든 배치를 누적하고 맨 마지막에 평가지표를 계산합니다.",qt,Ft,St;return i=new Dt({props:{id:"Dh9CL8fyG80"}}),L=new G({props:{code:"ZGVsJTIwbW9kZWwlMEFkZWwlMjB0cmFpbmVyJTBBdG9yY2guY3VkYS5lbXB0eV9jYWNoZSgp",highlighted:`<span class="hljs-keyword">del</span> model
<span class="hljs-keyword">del</span> trainer
torch.cuda.empty_cache()`,wrap:!1}}),X=new G({props:{code:"dG9rZW5pemVkX2RhdGFzZXRzJTIwJTNEJTIwdG9rZW5pemVkX2RhdGFzZXRzLnJlbW92ZV9jb2x1bW5zKCU1QiUyMnRleHQlMjIlNUQp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = tokenized_datasets.remove_columns([<span class="hljs-string">&quot;text&quot;</span>])',wrap:!1}}),P=new G({props:{code:"dG9rZW5pemVkX2RhdGFzZXRzJTIwJTNEJTIwdG9rZW5pemVkX2RhdGFzZXRzLnJlbmFtZV9jb2x1bW4oJTIybGFiZWwlMjIlMkMlMjAlMjJsYWJlbHMlMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = tokenized_datasets.rename_column(<span class="hljs-string">&quot;label&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>)',wrap:!1}}),mt=new G({props:{code:"dG9rZW5pemVkX2RhdGFzZXRzLnNldF9mb3JtYXQoJTIydG9yY2glMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)',wrap:!1}}),dt=new G({props:{code:"c21hbGxfdHJhaW5fZGF0YXNldCUyMCUzRCUyMHRva2VuaXplZF9kYXRhc2V0cyU1QiUyMnRyYWluJTIyJTVELnNodWZmbGUoc2VlZCUzRDQyKS5zZWxlY3QocmFuZ2UoMTAwMCkpJTBBc21hbGxfZXZhbF9kYXRhc2V0JTIwJTNEJTIwdG9rZW5pemVkX2RhdGFzZXRzJTVCJTIydGVzdCUyMiU1RC5zaHVmZmxlKHNlZWQlM0Q0Mikuc2VsZWN0KHJhbmdlKDEwMDApKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>small_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>small_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))`,wrap:!1}}),tt=new kt({props:{title:"DataLoader",local:"dataloader",headingTag:"h3"}}),k=new G({props:{code:"ZnJvbSUyMHRvcmNoLnV0aWxzLmRhdGElMjBpbXBvcnQlMjBEYXRhTG9hZGVyJTBBJTBBdHJhaW5fZGF0YWxvYWRlciUyMCUzRCUyMERhdGFMb2FkZXIoc21hbGxfdHJhaW5fZGF0YXNldCUyQyUyMHNodWZmbGUlM0RUcnVlJTJDJTIwYmF0Y2hfc2l6ZSUzRDgpJTBBZXZhbF9kYXRhbG9hZGVyJTIwJTNEJTIwRGF0YUxvYWRlcihzbWFsbF9ldmFsX2RhdGFzZXQlMkMlMjBiYXRjaF9zaXplJTNEOCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataloader = DataLoader(small_train_dataset, shuffle=<span class="hljs-literal">True</span>, batch_size=<span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>eval_dataloader = DataLoader(small_eval_dataset, batch_size=<span class="hljs-number">8</span>)`,wrap:!1}}),F=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyJTJDJTIwbnVtX2xhYmVscyUzRDUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">5</span>)`,wrap:!1}}),_=new kt({props:{title:"옵티마이저 및 학습 속도 스케줄러",local:"optimizer-and-learning-rate-scheduler",headingTag:"h3"}}),B=new G({props:{code:"ZnJvbSUyMHRvcmNoLm9wdGltJTIwaW1wb3J0JTIwQWRhbVclMEElMEFvcHRpbWl6ZXIlMjAlM0QlMjBBZGFtVyhtb2RlbC5wYXJhbWV0ZXJzKCklMkMlMjBsciUzRDVlLTUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>)`,wrap:!1}}),Jt=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGdldF9zY2hlZHVsZXIlMEElMEFudW1fZXBvY2hzJTIwJTNEJTIwMyUwQW51bV90cmFpbmluZ19zdGVwcyUyMCUzRCUyMG51bV9lcG9jaHMlMjAqJTIwbGVuKHRyYWluX2RhdGFsb2FkZXIpJTBBbHJfc2NoZWR1bGVyJTIwJTNEJTIwZ2V0X3NjaGVkdWxlciglMEElMjAlMjAlMjAlMjBuYW1lJTNEJTIybGluZWFyJTIyJTJDJTIwb3B0aW1pemVyJTNEb3B0aW1pemVyJTJDJTIwbnVtX3dhcm11cF9zdGVwcyUzRDAlMkMlMjBudW1fdHJhaW5pbmdfc3RlcHMlM0RudW1fdHJhaW5pbmdfc3RlcHMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dataloader)
<span class="hljs-meta">&gt;&gt;&gt; </span>lr_scheduler = get_scheduler(
<span class="hljs-meta">... </span>    name=<span class="hljs-string">&quot;linear&quot;</span>, optimizer=optimizer, num_warmup_steps=<span class="hljs-number">0</span>, num_training_steps=num_training_steps
<span class="hljs-meta">... </span>)`,wrap:!1}}),vt=new G({props:{code:"aW1wb3J0JTIwdG9yY2glMEElMEFkZXZpY2UlMjAlM0QlMjB0b3JjaC5kZXZpY2UoJTIyY3VkYSUyMiklMjBpZiUyMHRvcmNoLmN1ZGEuaXNfYXZhaWxhYmxlKCklMjBlbHNlJTIwdG9yY2guZGV2aWNlKCUyMmNwdSUyMiklMEFtb2RlbC50byhkZXZpY2Up",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)`,wrap:!1}}),o=new te({props:{$$slots:{default:[ge]},$$scope:{ctx:ct}}}),Xt=new kt({props:{title:"훈련 루프",local:"training-loop",headingTag:"h3"}}),xt=new G({props:{code:"ZnJvbSUyMHRxZG0uYXV0byUyMGltcG9ydCUyMHRxZG0lMEElMEFwcm9ncmVzc19iYXIlMjAlM0QlMjB0cWRtKHJhbmdlKG51bV90cmFpbmluZ19zdGVwcykpJTBBJTBBbW9kZWwudHJhaW4oKSUwQWZvciUyMGVwb2NoJTIwaW4lMjByYW5nZShudW1fZXBvY2hzKSUzQSUwQSUyMCUyMCUyMCUyMGZvciUyMGJhdGNoJTIwaW4lMjB0cmFpbl9kYXRhbG9hZGVyJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYmF0Y2glMjAlM0QlMjAlN0JrJTNBJTIwdi50byhkZXZpY2UpJTIwZm9yJTIwayUyQyUyMHYlMjBpbiUyMGJhdGNoLml0ZW1zKCklN0QlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKipiYXRjaCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBsb3NzJTIwJTNEJTIwb3V0cHV0cy5sb3NzJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbG9zcy5iYWNrd2FyZCgpJTBBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwb3B0aW1pemVyLnN0ZXAoKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGxyX3NjaGVkdWxlci5zdGVwKCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvcHRpbWl6ZXIuemVyb19ncmFkKCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBwcm9ncmVzc19iYXIudXBkYXRlKDEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm

<span class="hljs-meta">&gt;&gt;&gt; </span>progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-meta">&gt;&gt;&gt; </span>model.train()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs.loss
<span class="hljs-meta">... </span>        loss.backward()

<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        lr_scheduler.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        progress_bar.update(<span class="hljs-number">1</span>)`,wrap:!1}}),zt=new kt({props:{title:"평가 하기",local:"evaluate",headingTag:"h3"}}),Ft=new G({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFtZXRyaWMlMjAlM0QlMjBldmFsdWF0ZS5sb2FkKCUyMmFjY3VyYWN5JTIyKSUwQW1vZGVsLmV2YWwoKSUwQWZvciUyMGJhdGNoJTIwaW4lMjBldmFsX2RhdGFsb2FkZXIlM0ElMEElMjAlMjAlMjAlMjBiYXRjaCUyMCUzRCUyMCU3QmslM0ElMjB2LnRvKGRldmljZSklMjBmb3IlMjBrJTJDJTIwdiUyMGluJTIwYmF0Y2guaXRlbXMoKSU3RCUwQSUyMCUyMCUyMCUyMHdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKipiYXRjaCklMEElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cyUwQSUyMCUyMCUyMCUyMHByZWRpY3Rpb25zJTIwJTNEJTIwdG9yY2guYXJnbWF4KGxvZ2l0cyUyQyUyMGRpbSUzRC0xKSUwQSUyMCUyMCUyMCUyMG1ldHJpYy5hZGRfYmF0Y2gocHJlZGljdGlvbnMlM0RwcmVkaWN0aW9ucyUyQyUyMHJlZmVyZW5jZXMlM0RiYXRjaCU1QiUyMmxhYmVscyUyMiU1RCklMEElMEFtZXRyaWMuY29tcHV0ZSgp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:
<span class="hljs-meta">... </span>    batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>        outputs = model(**batch)

<span class="hljs-meta">... </span>    logits = outputs.logits
<span class="hljs-meta">... </span>    predictions = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=batch[<span class="hljs-string">&quot;labels&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>metric.compute()`,wrap:!1}}),{c(){u(i.$$.fragment),T=p(),m=h("p"),m.innerHTML=J,Z=p(),nt=h("p"),nt.textContent=A,pt=p(),u(L.$$.fragment),rt=p(),R=h("p"),R.textContent=Q,ft=p(),W=h("ol"),U=h("li"),ht=h("p"),ht.innerHTML=q,Mt=p(),u(X.$$.fragment),H=p(),lt=h("li"),ut=h("p"),ut.innerHTML=S,$t=p(),u(P.$$.fragment),gt=p(),V=h("li"),I=h("p"),I.textContent=it,K=p(),u(mt.$$.fragment),x=p(),D=h("p"),D.textContent=ot,O=p(),u(dt.$$.fragment),z=p(),u(tt.$$.fragment),at=p(),C=h("p"),C.textContent=Zt,yt=p(),u(k.$$.fragment),Ut=p(),Y=h("p"),Y.textContent=N,et=p(),u(F.$$.fragment),Tt=p(),u(_.$$.fragment),st=p(),v=h("p"),v.innerHTML=bt,wt=p(),u(B.$$.fragment),E=p(),a=h("p"),a.innerHTML=c,jt=p(),u(Jt.$$.fragment),n=p(),f=h("p"),f.textContent=Gt,It=p(),u(vt.$$.fragment),s=p(),u(o.$$.fragment),Wt=p(),Rt=h("p"),Rt.textContent=se,Ht=p(),u(Xt.$$.fragment),Nt=p(),Vt=h("p"),Vt.innerHTML=le,At=p(),u(xt.$$.fragment),Lt=p(),u(zt.$$.fragment),Qt=p(),Yt=h("p"),Yt.innerHTML=ae,qt=p(),u(Ft.$$.fragment)},l(t){$(i.$$.fragment,t),T=r(t),m=M(t,"P",{"data-svelte-h":!0}),j(m)!=="svelte-26l7tj"&&(m.innerHTML=J),Z=r(t),nt=M(t,"P",{"data-svelte-h":!0}),j(nt)!=="svelte-192c5yp"&&(nt.textContent=A),pt=r(t),$(L.$$.fragment,t),rt=r(t),R=M(t,"P",{"data-svelte-h":!0}),j(R)!=="svelte-108uveq"&&(R.textContent=Q),ft=r(t),W=M(t,"OL",{});var d=_t(W);U=M(d,"LI",{});var Bt=_t(U);ht=M(Bt,"P",{"data-svelte-h":!0}),j(ht)!=="svelte-1y5q08j"&&(ht.innerHTML=q),Mt=r(Bt),$(X.$$.fragment,Bt),Bt.forEach(e),H=r(d),lt=M(d,"LI",{});var Pt=_t(lt);ut=M(Pt,"P",{"data-svelte-h":!0}),j(ut)!=="svelte-1j9nsiy"&&(ut.innerHTML=S),$t=r(Pt),$(P.$$.fragment,Pt),Pt.forEach(e),gt=r(d),V=M(d,"LI",{});var Kt=_t(V);I=M(Kt,"P",{"data-svelte-h":!0}),j(I)!=="svelte-1h9qay5"&&(I.textContent=it),K=r(Kt),$(mt.$$.fragment,Kt),Kt.forEach(e),d.forEach(e),x=r(t),D=M(t,"P",{"data-svelte-h":!0}),j(D)!=="svelte-hdideh"&&(D.textContent=ot),O=r(t),$(dt.$$.fragment,t),z=r(t),$(tt.$$.fragment,t),at=r(t),C=M(t,"P",{"data-svelte-h":!0}),j(C)!=="svelte-1ektf94"&&(C.textContent=Zt),yt=r(t),$(k.$$.fragment,t),Ut=r(t),Y=M(t,"P",{"data-svelte-h":!0}),j(Y)!=="svelte-sqozl7"&&(Y.textContent=N),et=r(t),$(F.$$.fragment,t),Tt=r(t),$(_.$$.fragment,t),st=r(t),v=M(t,"P",{"data-svelte-h":!0}),j(v)!=="svelte-15zsq6h"&&(v.innerHTML=bt),wt=r(t),$(B.$$.fragment,t),E=r(t),a=M(t,"P",{"data-svelte-h":!0}),j(a)!=="svelte-15mbbl4"&&(a.innerHTML=c),jt=r(t),$(Jt.$$.fragment,t),n=r(t),f=M(t,"P",{"data-svelte-h":!0}),j(f)!=="svelte-e6t5r4"&&(f.textContent=Gt),It=r(t),$(vt.$$.fragment,t),s=r(t),$(o.$$.fragment,t),Wt=r(t),Rt=M(t,"P",{"data-svelte-h":!0}),j(Rt)!=="svelte-1yeeebv"&&(Rt.textContent=se),Ht=r(t),$(Xt.$$.fragment,t),Nt=r(t),Vt=M(t,"P",{"data-svelte-h":!0}),j(Vt)!=="svelte-1atq943"&&(Vt.innerHTML=le),At=r(t),$(xt.$$.fragment,t),Lt=r(t),$(zt.$$.fragment,t),Qt=r(t),Yt=M(t,"P",{"data-svelte-h":!0}),j(Yt)!=="svelte-18p9q2b"&&(Yt.innerHTML=ae),qt=r(t),$(Ft.$$.fragment,t)},m(t,d){g(i,t,d),l(t,T,d),l(t,m,d),l(t,Z,d),l(t,nt,d),l(t,pt,d),g(L,t,d),l(t,rt,d),l(t,R,d),l(t,ft,d),l(t,W,d),Ct(W,U),Ct(U,ht),Ct(U,Mt),g(X,U,null),Ct(W,H),Ct(W,lt),Ct(lt,ut),Ct(lt,$t),g(P,lt,null),Ct(W,gt),Ct(W,V),Ct(V,I),Ct(V,K),g(mt,V,null),l(t,x,d),l(t,D,d),l(t,O,d),g(dt,t,d),l(t,z,d),g(tt,t,d),l(t,at,d),l(t,C,d),l(t,yt,d),g(k,t,d),l(t,Ut,d),l(t,Y,d),l(t,et,d),g(F,t,d),l(t,Tt,d),g(_,t,d),l(t,st,d),l(t,v,d),l(t,wt,d),g(B,t,d),l(t,E,d),l(t,a,d),l(t,jt,d),g(Jt,t,d),l(t,n,d),l(t,f,d),l(t,It,d),g(vt,t,d),l(t,s,d),g(o,t,d),l(t,Wt,d),l(t,Rt,d),l(t,Ht,d),g(Xt,t,d),l(t,Nt,d),l(t,Vt,d),l(t,At,d),g(xt,t,d),l(t,Lt,d),g(zt,t,d),l(t,Qt,d),l(t,Yt,d),l(t,qt,d),g(Ft,t,d),St=!0},p(t,d){const Bt={};d&2&&(Bt.$$scope={dirty:d,ctx:t}),o.$set(Bt)},i(t){St||(y(i.$$.fragment,t),y(L.$$.fragment,t),y(X.$$.fragment,t),y(P.$$.fragment,t),y(mt.$$.fragment,t),y(dt.$$.fragment,t),y(tt.$$.fragment,t),y(k.$$.fragment,t),y(F.$$.fragment,t),y(_.$$.fragment,t),y(B.$$.fragment,t),y(Jt.$$.fragment,t),y(vt.$$.fragment,t),y(o.$$.fragment,t),y(Xt.$$.fragment,t),y(xt.$$.fragment,t),y(zt.$$.fragment,t),y(Ft.$$.fragment,t),St=!0)},o(t){w(i.$$.fragment,t),w(L.$$.fragment,t),w(X.$$.fragment,t),w(P.$$.fragment,t),w(mt.$$.fragment,t),w(dt.$$.fragment,t),w(tt.$$.fragment,t),w(k.$$.fragment,t),w(F.$$.fragment,t),w(_.$$.fragment,t),w(B.$$.fragment,t),w(Jt.$$.fragment,t),w(vt.$$.fragment,t),w(o.$$.fragment,t),w(Xt.$$.fragment,t),w(xt.$$.fragment,t),w(zt.$$.fragment,t),w(Ft.$$.fragment,t),St=!1},d(t){t&&(e(T),e(m),e(Z),e(nt),e(pt),e(rt),e(R),e(ft),e(W),e(x),e(D),e(O),e(z),e(at),e(C),e(yt),e(Ut),e(Y),e(et),e(Tt),e(st),e(v),e(wt),e(E),e(a),e(jt),e(n),e(f),e(It),e(s),e(Wt),e(Rt),e(Ht),e(Nt),e(Vt),e(At),e(Lt),e(Qt),e(Yt),e(qt)),b(i,t),b(L,t),b(X),b(P),b(mt),b(dt,t),b(tt,t),b(k,t),b(F,t),b(_,t),b(B,t),b(Jt,t),b(vt,t),b(o,t),b(Xt,t),b(xt,t),b(zt,t),b(Ft,t)}}}function we(ct){let i,T;return i=new ee({props:{$$slots:{default:[ye]},$$scope:{ctx:ct}}}),{c(){u(i.$$.fragment)},l(m){$(i.$$.fragment,m)},m(m,J){g(i,m,J),T=!0},p(m,J){const Z={};J&2&&(Z.$$scope={dirty:J,ctx:m}),i.$set(Z)},i(m){T||(y(i.$$.fragment,m),T=!0)},o(m){w(i.$$.fragment,m),T=!1},d(m){b(i,m)}}}function be(ct){let i,T,m,J,Z,nt,A,pt,L,rt="사전 학습된 모델을 사용하면 상당한 이점이 있습니다. 계산 비용과 탄소발자국을 줄이고, 처음부터 모델을 학습시킬 필요 없이 최신 모델을 사용할 수 있습니다. 🤗 Transformers는 다양한 작업을 위해 사전 학습된 수천 개의 모델에 액세스할 수 있습니다. 사전 학습된 모델을 사용하는 경우, 자신의 작업과 관련된 데이터셋을 사용해 학습합니다. 이것은 미세 튜닝이라고 하는 매우 강력한 훈련 기법입니다. 이 튜토리얼에서는 당신이 선택한 딥러닝 프레임워크로 사전 학습된 모델을 미세 튜닝합니다:",R,Q,ft="<li>🤗 Transformers로 사전 학습된 모델 미세 튜닝하기 <code>Trainer</code>.</li> <li>Keras를 사용하여 TensorFlow에서 사전 학습된 모델을 미세 튜닝하기.</li> <li>기본 PyTorch에서 사전 학습된 모델을 미세 튜닝하기.</li>",W,U,ht,q,Mt,X,H,lt,ut="사전 학습된 모델을 미세 튜닝하기 위해서 데이터셋을 다운로드하고 훈련할 수 있도록 준비하세요. 이전 튜토리얼에서 훈련을 위해 데이터를 처리하는 방법을 보여드렸는데, 지금이 배울 걸 되짚을 기회입니다!",S,$t,P='먼저 <a href="https://huggingface.co/datasets/yelp_review_full" rel="nofollow">Yelp 리뷰</a> 데이터 세트를 로드합니다:',gt,V,I,it,K='텍스트를 처리하고 서로 다른 길이의 시퀀스 패딩 및 잘라내기 전략을 포함하려면 토크나이저가 필요합니다. 데이터셋을 한 번에 처리하려면 🤗 Dataset <a href="https://huggingface.co/docs/datasets/process#map" rel="nofollow"><code>map</code></a> 메서드를 사용하여 전체 데이터셋에 전처리 함수를 적용하세요:',mt,x,D,ot,O="필요한 경우 미세 튜닝을 위해 데이터셋의 작은 부분 집합을 만들어 미세 튜닝 작업 시간을 줄일 수 있습니다:",dt,z,tt,at,C,Zt,yt,k,Ut="여기서부터는 사용하려는 프레임워크에 해당하는 섹션을 따라야 합니다. 오른쪽 사이드바의 링크를 사용하여 원하는 프레임워크로 이동할 수 있으며, 특정 프레임워크의 모든 콘텐츠를 숨기려면 해당 프레임워크 블록의 오른쪽 상단에 있는 버튼을 사용하면 됩니다!",Y,N,et,F,Tt,_,st,v,bt,wt,B,E,a,c,jt="더 많은 미세 튜닝 예제는 다음을 참조하세요:",Jt,n,f='<li><p><a href="https://github.com/huggingface/transformers/tree/main/examples" rel="nofollow">🤗 Trnasformers 예제</a>에는 PyTorch 및 텐서플로우에서 일반적인 NLP 작업을 훈련할 수 있는 스크립트가 포함되어 있습니다.</p></li> <li><p><a href="notebooks">🤗 Transformers 노트북</a>에는 PyTorch 및 텐서플로우에서 특정 작업을 위해 모델을 미세 튜닝하는 방법에 대한 다양한 노트북이 포함되어 있습니다.</p></li>',Gt,It,vt;return Z=new kt({props:{title:"사전 학습된 모델 미세 튜닝하기",local:"finetune-a-pretrained-model",headingTag:"h1"}}),A=new de({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/training.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/training.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/training.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/training.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/pytorch/training.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ko/tensorflow/training.ipynb"}]}}),q=new kt({props:{title:"데이터셋 준비",local:"prepare-a-dataset",headingTag:"h2"}}),X=new Dt({props:{id:"_BZearw7f0w"}}),V=new G({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJ5ZWxwX3Jldmlld19mdWxsJTIyKSUwQWRhdGFzZXQlNUIlMjJ0cmFpbiUyMiU1RCU1QjEwMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;yelp_review_full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">100</span>]
{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\&#x27;s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\&#x27;s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\&quot;serving off their orders\\\\&quot; when they didn\\&#x27;t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\&#x27;t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\&#x27;ve eaten at various McDonalds restaurants for over 30 years. I\\&#x27;ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!&#x27;</span>}`,wrap:!1}}),x=new G({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMEFkZWYlMjB0b2tlbml6ZV9mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjByZXR1cm4lMjB0b2tlbml6ZXIoZXhhbXBsZXMlNUIlMjJ0ZXh0JTIyJTVEJTJDJTIwcGFkZGluZyUzRCUyMm1heF9sZW5ndGglMjIlMkMlMjB0cnVuY2F0aW9uJTNEVHJ1ZSklMEElMEElMEF0b2tlbml6ZWRfZGF0YXNldHMlMjAlM0QlMjBkYXRhc2V0Lm1hcCh0b2tlbml6ZV9mdW5jdGlvbiUyQyUyMGJhdGNoZWQlM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = dataset.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)`,wrap:!1}}),z=new G({props:{code:"c21hbGxfdHJhaW5fZGF0YXNldCUyMCUzRCUyMHRva2VuaXplZF9kYXRhc2V0cyU1QiUyMnRyYWluJTIyJTVELnNodWZmbGUoc2VlZCUzRDQyKS5zZWxlY3QocmFuZ2UoMTAwMCkpJTBBc21hbGxfZXZhbF9kYXRhc2V0JTIwJTNEJTIwdG9rZW5pemVkX2RhdGFzZXRzJTVCJTIydGVzdCUyMiU1RC5zaHVmZmxlKHNlZWQlM0Q0Mikuc2VsZWN0KHJhbmdlKDEwMDApKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>small_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>small_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))`,wrap:!1}}),Zt=new kt({props:{title:"Train",local:"train",headingTag:"h2"}}),N=new ne({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[$e],pytorch:[he]},$$scope:{ctx:ct}}}),_=new kt({props:{title:"기본 파이토치로 훈련하기",local:"train-in-native-pytorch",headingTag:"h2"}}),v=new ne({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[we]},$$scope:{ctx:ct}}}),E=new kt({props:{title:"추가 자료",local:"additional-resources",headingTag:"h2"}}),{c(){i=h("meta"),T=p(),m=h("p"),J=p(),u(Z.$$.fragment),nt=p(),u(A.$$.fragment),pt=p(),L=h("p"),L.textContent=rt,R=p(),Q=h("ul"),Q.innerHTML=ft,W=p(),U=h("a"),ht=p(),u(q.$$.fragment),Mt=p(),u(X.$$.fragment),H=p(),lt=h("p"),lt.textContent=ut,S=p(),$t=h("p"),$t.innerHTML=P,gt=p(),u(V.$$.fragment),I=p(),it=h("p"),it.innerHTML=K,mt=p(),u(x.$$.fragment),D=p(),ot=h("p"),ot.textContent=O,dt=p(),u(z.$$.fragment),tt=p(),at=h("a"),C=p(),u(Zt.$$.fragment),yt=p(),k=h("p"),k.textContent=Ut,Y=p(),u(N.$$.fragment),et=p(),F=h("a"),Tt=p(),u(_.$$.fragment),st=p(),u(v.$$.fragment),bt=p(),wt=h("a"),B=p(),u(E.$$.fragment),a=p(),c=h("p"),c.textContent=jt,Jt=p(),n=h("ul"),n.innerHTML=f,Gt=p(),It=h("p"),this.h()},l(s){const o=oe("svelte-u9bgzb",document.head);i=M(o,"META",{name:!0,content:!0}),o.forEach(e),T=r(s),m=M(s,"P",{}),_t(m).forEach(e),J=r(s),$(Z.$$.fragment,s),nt=r(s),$(A.$$.fragment,s),pt=r(s),L=M(s,"P",{"data-svelte-h":!0}),j(L)!=="svelte-lewprv"&&(L.textContent=rt),R=r(s),Q=M(s,"UL",{"data-svelte-h":!0}),j(Q)!=="svelte-6blpq0"&&(Q.innerHTML=ft),W=r(s),U=M(s,"A",{id:!0}),_t(U).forEach(e),ht=r(s),$(q.$$.fragment,s),Mt=r(s),$(X.$$.fragment,s),H=r(s),lt=M(s,"P",{"data-svelte-h":!0}),j(lt)!=="svelte-agqeit"&&(lt.textContent=ut),S=r(s),$t=M(s,"P",{"data-svelte-h":!0}),j($t)!=="svelte-1w9wcpp"&&($t.innerHTML=P),gt=r(s),$(V.$$.fragment,s),I=r(s),it=M(s,"P",{"data-svelte-h":!0}),j(it)!=="svelte-grswur"&&(it.innerHTML=K),mt=r(s),$(x.$$.fragment,s),D=r(s),ot=M(s,"P",{"data-svelte-h":!0}),j(ot)!=="svelte-jaac36"&&(ot.textContent=O),dt=r(s),$(z.$$.fragment,s),tt=r(s),at=M(s,"A",{id:!0}),_t(at).forEach(e),C=r(s),$(Zt.$$.fragment,s),yt=r(s),k=M(s,"P",{"data-svelte-h":!0}),j(k)!=="svelte-12mtsa7"&&(k.textContent=Ut),Y=r(s),$(N.$$.fragment,s),et=r(s),F=M(s,"A",{id:!0}),_t(F).forEach(e),Tt=r(s),$(_.$$.fragment,s),st=r(s),$(v.$$.fragment,s),bt=r(s),wt=M(s,"A",{id:!0}),_t(wt).forEach(e),B=r(s),$(E.$$.fragment,s),a=r(s),c=M(s,"P",{"data-svelte-h":!0}),j(c)!=="svelte-1nv5qh9"&&(c.textContent=jt),Jt=r(s),n=M(s,"UL",{"data-svelte-h":!0}),j(n)!=="svelte-7q2z1g"&&(n.innerHTML=f),Gt=r(s),It=M(s,"P",{}),_t(It).forEach(e),this.h()},h(){Et(i,"name","hf:doc:metadata"),Et(i,"content",je),Et(U,"id","data-processing"),Et(at,"id","trainer"),Et(F,"id","pytorch_native"),Et(wt,"id","additional-resources")},m(s,o){Ct(document.head,i),l(s,T,o),l(s,m,o),l(s,J,o),g(Z,s,o),l(s,nt,o),g(A,s,o),l(s,pt,o),l(s,L,o),l(s,R,o),l(s,Q,o),l(s,W,o),l(s,U,o),l(s,ht,o),g(q,s,o),l(s,Mt,o),g(X,s,o),l(s,H,o),l(s,lt,o),l(s,S,o),l(s,$t,o),l(s,gt,o),g(V,s,o),l(s,I,o),l(s,it,o),l(s,mt,o),g(x,s,o),l(s,D,o),l(s,ot,o),l(s,dt,o),g(z,s,o),l(s,tt,o),l(s,at,o),l(s,C,o),g(Zt,s,o),l(s,yt,o),l(s,k,o),l(s,Y,o),g(N,s,o),l(s,et,o),l(s,F,o),l(s,Tt,o),g(_,s,o),l(s,st,o),g(v,s,o),l(s,bt,o),l(s,wt,o),l(s,B,o),g(E,s,o),l(s,a,o),l(s,c,o),l(s,Jt,o),l(s,n,o),l(s,Gt,o),l(s,It,o),vt=!0},p(s,[o]){const Wt={};o&2&&(Wt.$$scope={dirty:o,ctx:s}),N.$set(Wt);const Rt={};o&2&&(Rt.$$scope={dirty:o,ctx:s}),v.$set(Rt)},i(s){vt||(y(Z.$$.fragment,s),y(A.$$.fragment,s),y(q.$$.fragment,s),y(X.$$.fragment,s),y(V.$$.fragment,s),y(x.$$.fragment,s),y(z.$$.fragment,s),y(Zt.$$.fragment,s),y(N.$$.fragment,s),y(_.$$.fragment,s),y(v.$$.fragment,s),y(E.$$.fragment,s),vt=!0)},o(s){w(Z.$$.fragment,s),w(A.$$.fragment,s),w(q.$$.fragment,s),w(X.$$.fragment,s),w(V.$$.fragment,s),w(x.$$.fragment,s),w(z.$$.fragment,s),w(Zt.$$.fragment,s),w(N.$$.fragment,s),w(_.$$.fragment,s),w(v.$$.fragment,s),w(E.$$.fragment,s),vt=!1},d(s){s&&(e(T),e(m),e(J),e(nt),e(pt),e(L),e(R),e(Q),e(W),e(U),e(ht),e(Mt),e(H),e(lt),e(S),e($t),e(gt),e(I),e(it),e(mt),e(D),e(ot),e(dt),e(tt),e(at),e(C),e(yt),e(k),e(Y),e(et),e(F),e(Tt),e(st),e(bt),e(wt),e(B),e(a),e(c),e(Jt),e(n),e(Gt),e(It)),e(i),b(Z,s),b(A,s),b(q,s),b(X,s),b(V,s),b(x,s),b(z,s),b(Zt,s),b(N,s),b(_,s),b(v,s),b(E,s)}}}const je='{"title":"사전 학습된 모델 미세 튜닝하기","local":"finetune-a-pretrained-model","sections":[{"title":"데이터셋 준비","local":"prepare-a-dataset","sections":[],"depth":2},{"title":"Train","local":"train","sections":[],"depth":2},{"title":"파이토치 Trainer로 훈련하기","local":"train-with-pytorch-trainer","sections":[{"title":"하이퍼파라미터 훈련","local":"training-hyperparameters","sections":[],"depth":3},{"title":"평가 하기","local":"evaluate","sections":[],"depth":3},{"title":"훈련 하기","local":"trainer","sections":[],"depth":3}],"depth":2},{"title":"Keras로 텐서플로우 모델 훈련하기","local":"train-a-tensorflow-model-with-keras","sections":[{"title":"Keras용 데이터 로드","local":"loading-data-for-keras","sections":[],"depth":3},{"title":"데이터를 tf.data.Dataset으로 로드하기","local":"loading-data-as-a-tfdatadataset","sections":[],"depth":3}],"depth":2},{"title":"기본 파이토치로 훈련하기","local":"train-in-native-pytorch","sections":[{"title":"DataLoader","local":"dataloader","sections":[],"depth":3},{"title":"옵티마이저 및 학습 속도 스케줄러","local":"optimizer-and-learning-rate-scheduler","sections":[],"depth":3},{"title":"훈련 루프","local":"training-loop","sections":[],"depth":3},{"title":"평가 하기","local":"evaluate","sections":[],"depth":3}],"depth":2},{"title":"추가 자료","local":"additional-resources","sections":[],"depth":2}],"depth":1}';function Te(ct){return re(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ie extends ie{constructor(i){super(),me(this,i,Te,be,pe,{})}}export{Ie as component};
