import{s as mX,o as cX,n as k}from"../chunks/scheduler.9bc65507.js";import{S as fX,i as gX,g as i,s as t,r as p,A as pX,h as l,f as d,c as a,j as v,u as h,x as f,k as w,y as r,a as T,v as u,d as _,t as b,w as M}from"../chunks/index.707bf1b6.js";import{T as Wx}from"../chunks/Tip.c2ecdbf4.js";import{D as x}from"../chunks/Docstring.17db21ae.js";import{C as L}from"../chunks/CodeBlock.54a9f38d.js";import{E as $}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as B}from"../chunks/Heading.342b1fa6.js";function hX(y){let n,C=`If your <code>NewModelConfig</code> is a subclass of <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, make sure its
<code>model_type</code> attribute is set to the same key you use when registering the config (here <code>&quot;new-model&quot;</code>).`,m,s,c=`Likewise, if your <code>NewModel</code> is a subclass of <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>, make sure its
<code>config_class</code> attribute is set to the same class you use when registering the model (here
<code>NewModelConfig</code>).`;return{c(){n=i("p"),n.innerHTML=C,m=t(),s=i("p"),s.innerHTML=c},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-1upn364"&&(n.innerHTML=C),m=a(e),s=l(e,"P",{"data-svelte-h":!0}),f(s)!=="svelte-19m1s9a"&&(s.innerHTML=c)},m(e,F){T(e,n,F),T(e,m,F),T(e,s,F)},p:k,d(e){e&&(d(n),d(m),d(s))}}}function uX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMCh1c2VyLXVwbG9hZGVkKSUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmRibWR6JTJGYmVydC1iYXNlLWdlcm1hbi1jYXNlZCUyMiklMEElMEElMjMlMjBJZiUyMGNvbmZpZ3VyYXRpb24lMjBmaWxlJTIwaXMlMjBpbiUyMGElMjBkaXJlY3RvcnklMjAoZS5nLiUyQyUyMHdhcyUyMHNhdmVkJTIwdXNpbmclMjAqc2F2ZV9wcmV0cmFpbmVkKCcuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJykqKS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGVzdCUyRmJlcnRfc2F2ZWRfbW9kZWwlMkYlMjIpJTBBJTBBJTIzJTIwTG9hZCUyMGElMjBzcGVjaWZpYyUyMGNvbmZpZ3VyYXRpb24lMjBmaWxlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0ZXN0JTJGYmVydF9zYXZlZF9tb2RlbCUyRm15X2NvbmZpZ3VyYXRpb24uanNvbiUyMiklMEElMEElMjMlMjBDaGFuZ2UlMjBzb21lJTIwY29uZmlnJTIwYXR0cmlidXRlcyUyMHdoZW4lMjBsb2FkaW5nJTIwYSUyMHByZXRyYWluZWQlMjBjb25maWcuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtdW5jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSUyQyUyMGZvbyUzREZhbHNlKSUwQWNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQWNvbmZpZyUyQyUyMHVudXNlZF9rd2FyZ3MlMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlJTJDJTIwZm9vJTNERmFsc2UlMkMlMjByZXR1cm5fdW51c2VkX2t3YXJncyUzRFRydWUlMEEpJTBBY29uZmlnLm91dHB1dF9hdHRlbnRpb25zJTBBJTBBdW51c2VkX2t3YXJncw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function _X(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEElMjMlMjBEb3dubG9hZCUyMHZvY2FidWxhcnklMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQSUwQSUyMyUyMERvd25sb2FkJTIwdm9jYWJ1bGFyeSUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMCh1c2VyLXVwbG9hZGVkKSUyMGFuZCUyMGNhY2hlLiUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmRibWR6JTJGYmVydC1iYXNlLWdlcm1hbi1jYXNlZCUyMiklMEElMEElMjMlMjBJZiUyMHZvY2FidWxhcnklMjBmaWxlcyUyMGFyZSUyMGluJTIwYSUyMGRpcmVjdG9yeSUyMChlLmcuJTIwdG9rZW5pemVyJTIwd2FzJTIwc2F2ZWQlMjB1c2luZyUyMCpzYXZlX3ByZXRyYWluZWQoJy4lMkZ0ZXN0JTJGc2F2ZWRfbW9kZWwlMkYnKSopJTBBJTIzJTIwdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRlc3QlMkZiZXJ0X3NhdmVkX21vZGVsJTJGJTIyKSUwQSUwQSUyMyUyMERvd25sb2FkJTIwdm9jYWJ1bGFyeSUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGRlZmluZSUyMG1vZGVsLXNwZWNpZmljJTIwYXJndW1lbnRzJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyRmFjZWJvb2tBSSUyRnJvYmVydGEtYmFzZSUyMiUyQyUyMGFkZF9wcmVmaXhfc3BhY2UlM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># tokenizer = AutoTokenizer.from_pretrained(&quot;./test/bert_saved_model/&quot;)</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and define model-specific arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;FacebookAI/roberta-base&quot;</span>, add_prefix_space=<span class="hljs-literal">True</span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function bX(y){let n,C="Passing <code>token=True</code> is required when you want to use a private model.";return{c(){n=i("p"),n.innerHTML=C},l(m){n=l(m,"P",{"data-svelte-h":!0}),f(n)!=="svelte-15auxyb"&&(n.innerHTML=C)},m(m,s){T(m,n,s)},p:k,d(m){m&&d(n)}}}function MX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBmZWF0dXJlJTIwZXh0cmFjdG9yJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlLTk2MGglMjIpJTBBJTBBJTIzJTIwSWYlMjBmZWF0dXJlJTIwZXh0cmFjdG9yJTIwZmlsZXMlMjBhcmUlMjBpbiUyMGElMjBkaXJlY3RvcnklMjAoZS5nLiUyMGZlYXR1cmUlMjBleHRyYWN0b3IlMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiklMEElMjMlMjBmZWF0dXJlX2V4dHJhY3RvciUyMCUzRCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># feature_extractor = AutoFeatureExtractor.from_pretrained(&quot;./test/saved_model/&quot;)</span>`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function FX(y){let n,C="Passing <code>token=True</code> is required when you want to use a private model.";return{c(){n=i("p"),n.innerHTML=C},l(m){n=l(m,"P",{"data-svelte-h":!0}),f(n)!=="svelte-15auxyb"&&(n.innerHTML=C)},m(m,s){T(m,n,s)},p:k,d(m){m&&d(n)}}}function TX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQSUyMyUyMERvd25sb2FkJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnZpdC1iYXNlLXBhdGNoMTYtMjI0LWluMjFrJTIyKSUwQSUwQSUyMyUyMElmJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjBmaWxlcyUyMGFyZSUyMGluJTIwYSUyMGRpcmVjdG9yeSUyMChlLmcuJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiklMEElMjMlMjBpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0ZXN0JTJGc2F2ZWRfbW9kZWwlMkYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download image processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If image processor files are in a directory (e.g. image processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># image_processor = AutoImageProcessor.from_pretrained(&quot;./test/saved_model/&quot;)</span>`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function CX(y){let n,C="Passing <code>token=True</code> is required when you want to use a private model.";return{c(){n=i("p"),n.innerHTML=C},l(m){n=l(m,"P",{"data-svelte-h":!0}),f(n)!=="svelte-15auxyb"&&(n.innerHTML=C)},m(m,s){T(m,n,s)},p:k,d(m){m&&d(n)}}}function yX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMEElMEElMjMlMjBEb3dubG9hZCUyMHByb2Nlc3NvciUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzItYmFzZS05NjBoJTIyKSUwQSUwQSUyMyUyMElmJTIwcHJvY2Vzc29yJTIwZmlsZXMlMjBhcmUlMjBpbiUyMGElMjBkaXJlY3RvcnklMjAoZS5nLiUyMHByb2Nlc3NvciUyMHdhcyUyMHNhdmVkJTIwdXNpbmclMjAqc2F2ZV9wcmV0cmFpbmVkKCcuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJykqKSUwQSUyMyUyMHByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0ZXN0JTJGc2F2ZWRfbW9kZWwlMkYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># processor = AutoProcessor.from_pretrained(&quot;./test/saved_model/&quot;)</span>`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function vX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWwlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function wX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWwlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function xX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbCUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWwuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function kX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbCUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function LX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function $X(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function BX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JQcmVUcmFpbmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yUHJlVHJhaW5pbmcuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function jX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JQcmVUcmFpbmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JQcmVUcmFpbmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function ZX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclByZVRyYWluaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function AX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclByZVRyYWluaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yUHJlVHJhaW5pbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JQcmVUcmFpbmluZy5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function RX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yUHJlVHJhaW5pbmclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JQcmVUcmFpbmluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function PX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yUHJlVHJhaW5pbmclMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function WX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function SX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function GX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function VX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function JX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function EX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function IX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yTWFza2VkTE0uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function UX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function NX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck1hc2tlZExNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function XX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck1hc2tlZExNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yTWFza2VkTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function qX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTWFza2VkTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function QX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTWFza2VkTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function DX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtdDUlMkZ0NS1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function HX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtdDUlMkZ0NS1iYXNlJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIlMkMlMjBvdXRwdXRfYXR0ZW50aW9ucyUzRFRydWUpJTBBbW9kZWwuY29uZmlnLm91dHB1dF9hdHRlbnRpb25zJTBBJTBBJTIzJTIwTG9hZGluZyUyMGZyb20lMjBhJTIwVEYlMjBjaGVja3BvaW50JTIwZmlsZSUyMGluc3RlYWQlMjBvZiUyMGElMjBQeVRvcmNoJTIwbW9kZWwlMjAoc2xvd2VyKSUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0Zl9tb2RlbCUyRnQ1X3RmX21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZ0NV90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function YX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function zX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLXQ1JTJGdDUtYmFzZSUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIlMkMlMjBvdXRwdXRfYXR0ZW50aW9ucyUzRFRydWUpJTBBbW9kZWwuY29uZmlnLm91dHB1dF9hdHRlbnRpb25zJTBBJTBBJTIzJTIwTG9hZGluZyUyMGZyb20lMjBhJTIwUHlUb3JjaCUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFRlbnNvckZsb3clMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnB0X21vZGVsJTJGdDVfcHRfbW9kZWxfY29uZmlnLmpzb24lMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGdDVfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function OX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU2VxMlNlcUxNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLXQ1JTJGdDUtYmFzZSUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function KX(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU2VxMlNlcUxNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIlMkMlMjBvdXRwdXRfYXR0ZW50aW9ucyUzRFRydWUpJTBBbW9kZWwuY29uZmlnLm91dHB1dF9hdHRlbnRpb25zJTBBJTBBJTIzJTIwTG9hZGluZyUyMGZyb20lMjBhJTIwUHlUb3JjaCUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFRlbnNvckZsb3clMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnB0X21vZGVsJTJGdDVfcHRfbW9kZWxfY29uZmlnLmpzb24lMjIpJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZ0NV9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function eq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function oq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function rq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function nq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function tq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function aq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function sq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yTXVsdGlwbGVDaG9pY2UuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function iq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function lq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function dq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yTXVsdGlwbGVDaG9pY2UuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function mq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTXVsdGlwbGVDaG9pY2UlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function cq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTXVsdGlwbGVDaG9pY2UlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function fq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function gq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function pq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yTmV4dFNlbnRlbmNlUHJlZGljdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForNextSentencePrediction.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function hq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function uq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTmV4dFNlbnRlbmNlUHJlZGljdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function _q(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTmV4dFNlbnRlbmNlUHJlZGljdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yTmV4dFNlbnRlbmNlUHJlZGljdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function bq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Mq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Fq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yVG9rZW5DbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Tq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Cq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yVG9rZW5DbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function yq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yVG9rZW5DbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yVG9rZW5DbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function vq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function wq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function xq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function kq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Lq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function $q(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Bq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEZXB0aEVzdGltYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function jq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEZXB0aEVzdGltYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yRGVwdGhFc3RpbWF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yRGVwdGhFc3RpbWF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Zq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Aq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Rq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Pq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Wq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Sq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Gq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVideoClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Vq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpZGVvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpZGVvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVideoClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Jq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Eq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZEltYWdlTW9kZWxpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZEltYWdlTW9kZWxpbmcuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Iq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck1hc2tlZEltYWdlTW9kZWxpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yTWFza2VkSW1hZ2VNb2RlbGluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedImageModeling.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Uq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck1hc2tlZEltYWdlTW9kZWxpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Nq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JPYmplY3REZXRlY3Rpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck9iamVjdERldGVjdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Xq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JPYmplY3REZXRlY3Rpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yT2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yT2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck9iamVjdERldGVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function qq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9ySW1hZ2VTZWdtZW50YXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Qq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZVNlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Dq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VtYW50aWNTZWdtZW50YXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Hq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Yq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSemanticSegmentation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function zq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU2VtYW50aWNTZWdtZW50YXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Oq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbnN0YW5jZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9ySW5zdGFuY2VTZWdtZW50YXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForInstanceSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Kq(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbnN0YW5jZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbnN0YW5jZVNlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckluc3RhbmNlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckluc3RhbmNlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForInstanceSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function e3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JVbml2ZXJzYWxTZWdtZW50YXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclVuaXZlcnNhbFNlZ21lbnRhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForUniversalSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function o3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JVbml2ZXJzYWxTZWdtZW50YXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVW5pdmVyc2FsU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVW5pdmVyc2FsU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclVuaXZlcnNhbFNlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForUniversalSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function r3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdEltYWdlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function n3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdEltYWdlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yWmVyb1Nob3RJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yWmVyb1Nob3RJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function t3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JaZXJvU2hvdEltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForZeroShotImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForZeroShotImageClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function a3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yWmVyb1Nob3RJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JaZXJvU2hvdEltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForZeroShotImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForZeroShotImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForZeroShotImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForZeroShotImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function s3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdE9iamVjdERldGVjdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yWmVyb1Nob3RPYmplY3REZXRlY3Rpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function i3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdE9iamVjdERldGVjdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdE9iamVjdERldGVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90T2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90T2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function l3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function d3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function m3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yQXVkaW9DbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForAudioClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function c3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function f3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0ZyYW1lQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvRnJhbWVDbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function g3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0ZyYW1lQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9GcmFtZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9GcmFtZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvRnJhbWVDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function p3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDVEMlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNUQy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function h3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDVEMlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ1RDLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ1RDLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNUQy5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function u3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function _3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXEuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXEuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function b3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXElMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU3BlZWNoU2VxMlNlcS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function M3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXElMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function F3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU3BlZWNoU2VxMlNlcSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXEuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSpeechSeq2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function T3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU3BlZWNoU2VxMlNlcSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yU3BlZWNoU2VxMlNlcS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXEuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function C3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb1hWZWN0b3IlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvWFZlY3Rvci5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function y3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb1hWZWN0b3IlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9YVmVjdG9yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9YVmVjdG9yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvWFZlY3Rvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function v3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUYWJsZVF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGdGFwYXMtYmFzZS1maW5ldHVuZWQtd3RxJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVGFibGVRdWVzdGlvbkFuc3dlcmluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function w3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUYWJsZVF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnRhcGFzLWJhc2UtZmluZXR1bmVkLXd0cSUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVGFibGVRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGdGFwYXMtYmFzZS1maW5ldHVuZWQtd3RxJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlKSUwQW1vZGVsLmNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQSUyMyUyMExvYWRpbmclMjBmcm9tJTIwYSUyMFRGJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwUHlUb3JjaCUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGZfbW9kZWwlMkZ0YXBhc190Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRnRhcGFzX3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function x3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZ0YXBhcy1iYXNlLWZpbmV0dW5lZC13dHElMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function k3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JUYWJsZVF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZ0YXBhcy1iYXNlLWZpbmV0dW5lZC13dHElMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yVGFibGVRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGdGFwYXMtYmFzZS1maW5ldHVuZWQtd3RxJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlKSUwQW1vZGVsLmNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQSUyMyUyMExvYWRpbmclMjBmcm9tJTIwYSUyMFB5VG9yY2glMjBjaGVja3BvaW50JTIwZmlsZSUyMGluc3RlYWQlMjBvZiUyMGElMjBUZW5zb3JGbG93JTIwbW9kZWwlMjAoc2xvd2VyKSUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZwdF9tb2RlbCUyRnRhcGFzX3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JUYWJsZVF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZ0YXBhc19weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function L3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyaW1waXJhJTJGbGF5b3V0bG0tZG9jdW1lbnQtcWElMjIlMkMlMjByZXZpc2lvbiUzRCUyMjUyZTAxYjMlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDocumentQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function $3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRvY3VtZW50UXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmltcGlyYSUyRmxheW91dGxtLWRvY3VtZW50LXFhJTIyJTJDJTIwcmV2aXNpb24lM0QlMjI1MmUwMWIzJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJpbXBpcmElMkZsYXlvdXRsbS1kb2N1bWVudC1xYSUyMiUyQyUyMHJldmlzaW9uJTNEJTIyNTJlMDFiMyUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGbGF5b3V0bG1fdGZfbW9kZWxfY29uZmlnLmpzb24lMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZsYXlvdXRsbV90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDocumentQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/layoutlm_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/layoutlm_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function B3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckRvY3VtZW50UXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJpbXBpcmElMkZsYXlvdXRsbS1kb2N1bWVudC1xYSUyMiUyQyUyMHJldmlzaW9uJTNEJTIyNTJlMDFiMyUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yRG9jdW1lbnRRdWVzdGlvbkFuc3dlcmluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForDocumentQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForDocumentQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function j3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckRvY3VtZW50UXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJpbXBpcmElMkZsYXlvdXRsbS1kb2N1bWVudC1xYSUyMiUyQyUyMHJldmlzaW9uJTNEJTIyNTJlMDFiMyUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJpbXBpcmElMkZsYXlvdXRsbS1kb2N1bWVudC1xYSUyMiUyQyUyMHJldmlzaW9uJTNEJTIyNTJlMDFiMyUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZsYXlvdXRsbV9wdF9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yRG9jdW1lbnRRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGbGF5b3V0bG1fcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForDocumentQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForDocumentQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForDocumentQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/layoutlm_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForDocumentQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/layoutlm_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function Z3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmRhbmRlbGluJTJGdmlsdC1iMzItZmluZXR1bmVkLXZxYSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpc3VhbFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVisualQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dandelin/vilt-b32-finetuned-vqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function A3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGFuZGVsaW4lMkZ2aWx0LWIzMi1maW5ldHVuZWQtdnFhJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGFuZGVsaW4lMkZ2aWx0LWIzMi1maW5ldHVuZWQtdnFhJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlKSUwQW1vZGVsLmNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQSUyMyUyMExvYWRpbmclMjBmcm9tJTIwYSUyMFRGJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwUHlUb3JjaCUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGZfbW9kZWwlMkZ2aWx0X3RmX21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVmlzdWFsUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRnZpbHRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVisualQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;dandelin/vilt-b32-finetuned-vqa&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;dandelin/vilt-b32-finetuned-vqa&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/vilt_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/vilt_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function R3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaXNpb24yU2VxJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function P3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaXNpb24yU2VxJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpc2lvbjJTZXEuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpc2lvbjJTZXEuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function W3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclZpc2lvbjJTZXElMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yVmlzaW9uMlNlcS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function S3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclZpc2lvbjJTZXElMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function G3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yVmlzaW9uMlNlcSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclZpc2lvbjJTZXEuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function V3(y){let n,C="Examples:",m,s,c;return s=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yVmlzaW9uMlNlcSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yVmlzaW9uMlNlcS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclZpc2lvbjJTZXEuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=C,m=t(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=C),m=a(e),h(s.$$.fragment,e)},m(e,F){T(e,n,F),T(e,m,F),u(s,e,F),c=!0},p:k,i(e){c||(_(s.$$.fragment,e),c=!0)},o(e){b(s.$$.fragment,e),c=!1},d(e){e&&(d(n),d(m)),M(s,e)}}}function J3(y){let n,C,m,s,c,e,F,EV=`In many cases, the architecture you want to use can be guessed from the name or the path of the pretrained model you
are supplying to the <code>from_pretrained()</code> method. AutoClasses are here to do this job for you so that you
automatically retrieve the relevant model given the name/path to the pretrained weights/config/vocabulary.`,kv,yd,IV=`Instantiating one of <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig">AutoConfig</a>, <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModel">AutoModel</a>, and
<a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> will directly create a class of the relevant architecture. For instance`,Lv,vd,$v,wd,UV='will create a model that is an instance of <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertModel">BertModel</a>.',Bv,xd,NV="There is one class of <code>AutoModel</code> for each task, and for each backend (PyTorch, TensorFlow, or Flax).",jv,kd,Zv,Ld,XV=`Each of the auto classes has a method to be extended with your custom classes. For instance, if you have defined a
custom class of model <code>NewModel</code>, make sure you have a <code>NewModelConfig</code> then you can add those to the auto
classes like this:`,Av,$d,Rv,Bd,qV="You will then be able to use the auto classes like you would usually do!",Pv,ys,Wv,jd,Sv,me,Zd,Sx,ph,QV=`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig.from_pretrained">from_pretrained()</a> class method.`,Gx,hh,DV="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Vx,Io,Ad,Jx,uh,HV="Instantiate one of the configuration classes of the library from a pretrained model configuration.",Ex,_h,YV=`The configuration class to instantiate is selected based on the <code>model_type</code> property of the config object that
is loaded, or when its missing, by falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Ix,bh,zV='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> (ALBERT model)</li> <li><strong>align</strong>  <a href="/docs/transformers/main/en/model_doc/align#transformers.AlignConfig">AlignConfig</a> (ALIGN model)</li> <li><strong>altclip</strong>  <a href="/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPConfig">AltCLIPConfig</a> (AltCLIP model)</li> <li><strong>audio-spectrogram-transformer</strong>  <a href="/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a> (Audio Spectrogram Transformer model)</li> <li><strong>autoformer</strong>  <a href="/docs/transformers/main/en/model_doc/autoformer#transformers.AutoformerConfig">AutoformerConfig</a> (Autoformer model)</li> <li><strong>bark</strong>  <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkConfig">BarkConfig</a> (Bark model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> (BART model)</li> <li><strong>beit</strong>  <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> (BEiT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> (BERT model)</li> <li><strong>bert-generation</strong>  <a href="/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> (Bert Generation model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> (BioGpt model)</li> <li><strong>bit</strong>  <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitConfig">BitConfig</a> (BiT model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> (BlenderbotSmall model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> (BLIP-2 model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> (BLOOM model)</li> <li><strong>bridgetower</strong>  <a href="/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a> (BridgeTower model)</li> <li><strong>bros</strong>  <a href="/docs/transformers/main/en/model_doc/bros#transformers.BrosConfig">BrosConfig</a> (BROS model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> (CANINE model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig">ChineseCLIPConfig</a> (Chinese-CLIP model)</li> <li><strong>clap</strong>  <a href="/docs/transformers/main/en/model_doc/clap#transformers.ClapConfig">ClapConfig</a> (CLAP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> (CLIP model)</li> <li><strong>clip_vision_model</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPVisionConfig">CLIPVisionConfig</a> (CLIPVisionModel model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> (CLIPSeg model)</li> <li><strong>clvp</strong>  <a href="/docs/transformers/main/en/model_doc/clvp#transformers.ClvpConfig">ClvpConfig</a> (CLVP model)</li> <li><strong>code_llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a> (CodeLlama model)</li> <li><strong>codegen</strong>  <a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenConfig">CodeGenConfig</a> (CodeGen model)</li> <li><strong>conditional_detr</strong>  <a href="/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig">ConditionalDetrConfig</a> (Conditional DETR model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> (ConvBERT model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong>  <a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> (ConvNeXTV2 model)</li> <li><strong>cpmant</strong>  <a href="/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a> (CPM-Ant model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> (CTRL model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/main/en/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> (CvT model)</li> <li><strong>data2vec-audio</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> (Data2VecAudio model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> (Data2VecText model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> (Data2VecVision model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> (DeBERTa-v2 model)</li> <li><strong>decision_transformer</strong>  <a href="/docs/transformers/main/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig">DecisionTransformerConfig</a> (Decision Transformer model)</li> <li><strong>deformable_detr</strong>  <a href="/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrConfig">DeformableDetrConfig</a> (Deformable DETR model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> (DeiT model)</li> <li><strong>depth_anything</strong>  <a href="/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingConfig">DepthAnythingConfig</a> (Depth Anything model)</li> <li><strong>deta</strong>  <a href="/docs/transformers/main/en/model_doc/deta#transformers.DetaConfig">DetaConfig</a> (DETA model)</li> <li><strong>detr</strong>  <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> (DETR model)</li> <li><strong>dinat</strong>  <a href="/docs/transformers/main/en/model_doc/dinat#transformers.DinatConfig">DinatConfig</a> (DiNAT model)</li> <li><strong>dinov2</strong>  <a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config">Dinov2Config</a> (DINOv2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> (DistilBERT model)</li> <li><strong>donut-swin</strong>  <a href="/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinConfig">DonutSwinConfig</a> (DonutSwin model)</li> <li><strong>dpr</strong>  <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> (DPR model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a> (DPT model)</li> <li><strong>efficientformer</strong>  <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerConfig">EfficientFormerConfig</a> (EfficientFormer model)</li> <li><strong>efficientnet</strong>  <a href="/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetConfig">EfficientNetConfig</a> (EfficientNet model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> (ELECTRA model)</li> <li><strong>encodec</strong>  <a href="/docs/transformers/main/en/model_doc/encodec#transformers.EncodecConfig">EncodecConfig</a> (EnCodec model)</li> <li><strong>encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> (Encoder decoder model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> (ErnieM model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> (ESM model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> (Falcon model)</li> <li><strong>fastspeech2_conformer</strong>  <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerConfig">FastSpeech2ConformerConfig</a> (FastSpeech2Conformer model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> (FlauBERT model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a> (FLAVA model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> (FNet model)</li> <li><strong>focalnet</strong>  <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a> (FocalNet model)</li> <li><strong>fsmt</strong>  <a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> (Funnel Transformer model)</li> <li><strong>fuyu</strong>  <a href="/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig">FuyuConfig</a> (Fuyu model)</li> <li><strong>git</strong>  <a href="/docs/transformers/main/en/model_doc/git#transformers.GitConfig">GitConfig</a> (GIT model)</li> <li><strong>glpn</strong>  <a href="/docs/transformers/main/en/model_doc/glpn#transformers.GLPNConfig">GLPNConfig</a> (GLPN model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig">GPTNeoXJapaneseConfig</a> (GPT NeoX Japanese model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> (GPT-J model)</li> <li><strong>gptsan-japanese</strong>  <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig">GPTSanJapaneseConfig</a> (GPTSAN-japanese model)</li> <li><strong>graphormer</strong>  <a href="/docs/transformers/main/en/model_doc/graphormer#transformers.GraphormerConfig">GraphormerConfig</a> (Graphormer model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTConfig">GroupViTConfig</a> (GroupViT model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> (Hubert model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> (I-BERT model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig">IdeficsConfig</a> (IDEFICS model)</li> <li><strong>imagegpt</strong>  <a href="/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> (ImageGPT model)</li> <li><strong>informer</strong>  <a href="/docs/transformers/main/en/model_doc/informer#transformers.InformerConfig">InformerConfig</a> (Informer model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipConfig">InstructBlipConfig</a> (InstructBLIP model)</li> <li><strong>jukebox</strong>  <a href="/docs/transformers/main/en/model_doc/jukebox#transformers.JukeboxConfig">JukeboxConfig</a> (Jukebox model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Config">Kosmos2Config</a> (KOSMOS-2 model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> (LayoutLMv3 model)</li> <li><strong>led</strong>  <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> (LED model)</li> <li><strong>levit</strong>  <a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitConfig">LevitConfig</a> (LeViT model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig">LiltConfig</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a> (LLaMA model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig">LlavaConfig</a> (LLaVa model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> (Longformer model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config">LongT5Config</a> (LongT5 model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> (LUKE model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> (LXMERT model)</li> <li><strong>m2m_100</strong>  <a href="/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> (M2M100 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> (Marian model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig">MarkupLMConfig</a> (MarkupLM model)</li> <li><strong>mask2former</strong>  <a href="/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerConfig">Mask2FormerConfig</a> (Mask2Former model)</li> <li><strong>maskformer</strong>  <a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> (MaskFormer model)</li> <li><strong>maskformer-swin</strong>  <code>MaskFormerSwinConfig</code> (MaskFormerSwin model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> (mBART model)</li> <li><strong>mctct</strong>  <a href="/docs/transformers/main/en/model_doc/mctct#transformers.MCTCTConfig">MCTCTConfig</a> (M-CTC-T model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> (Megatron-BERT model)</li> <li><strong>mgp-str</strong>  <a href="/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrConfig">MgpstrConfig</a> (MGP-STR model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> (Mistral model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a> (Mixtral model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> (MobileBERT model)</li> <li><strong>mobilenet_v1</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config">MobileNetV1Config</a> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config">MobileNetV2Config</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> (MobileViT model)</li> <li><strong>mobilevitv2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a> (MobileViTV2 model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraConfig">MraConfig</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> (MT5 model)</li> <li><strong>musicgen</strong>  <a href="/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenConfig">MusicgenConfig</a> (MusicGen model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> (MVP model)</li> <li><strong>nat</strong>  <a href="/docs/transformers/main/en/model_doc/nat#transformers.NatConfig">NatConfig</a> (NAT model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> (Nezha model)</li> <li><strong>nllb-moe</strong>  <a href="/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig">NllbMoeConfig</a> (NLLB-MOE model)</li> <li><strong>nougat</strong>  <a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> (Nougat model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> (Nystrmformer model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerConfig">OneFormerConfig</a> (OneFormer model)</li> <li><strong>open-llama</strong>  <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a> (OpenLlama model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> (OPT model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Config">Owlv2Config</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a> (OWL-ViT model)</li> <li><strong>patchtsmixer</strong>  <a href="/docs/transformers/main/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig">PatchTSMixerConfig</a> (PatchTSMixer model)</li> <li><strong>patchtst</strong>  <a href="/docs/transformers/main/en/model_doc/patchtst#transformers.PatchTSTConfig">PatchTSTConfig</a> (PatchTST model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> (Pegasus model)</li> <li><strong>pegasus_x</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXConfig">PegasusXConfig</a> (PEGASUS-X model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> (Perceiver model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig">PersimmonConfig</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig">PhiConfig</a> (Phi model)</li> <li><strong>pix2struct</strong>  <a href="/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructConfig">Pix2StructConfig</a> (Pix2Struct model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> (PLBart model)</li> <li><strong>poolformer</strong>  <a href="/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> (PoolFormer model)</li> <li><strong>pop2piano</strong>  <a href="/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoConfig">Pop2PianoConfig</a> (Pop2Piano model)</li> <li><strong>prophetnet</strong>  <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> (ProphetNet model)</li> <li><strong>pvt</strong>  <a href="/docs/transformers/main/en/model_doc/pvt#transformers.PvtConfig">PvtConfig</a> (PVT model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config">Qwen2Config</a> (Qwen2 model)</li> <li><strong>rag</strong>  <a href="/docs/transformers/main/en/model_doc/rag#transformers.RagConfig">RagConfig</a> (RAG model)</li> <li><strong>realm</strong>  <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmConfig">RealmConfig</a> (REALM model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> (Reformer model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig">RegNetConfig</a> (RegNet model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> (RemBERT model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> (ResNet model)</li> <li><strong>retribert</strong>  <a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> (RetriBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> (RoFormer model)</li> <li><strong>rwkv</strong>  <a href="/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig">RwkvConfig</a> (RWKV model)</li> <li><strong>sam</strong>  <a href="/docs/transformers/main/en/model_doc/sam#transformers.SamConfig">SamConfig</a> (SAM model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig">SeamlessM4TConfig</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config">SeamlessM4Tv2Config</a> (SeamlessM4Tv2 model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> (SegFormer model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> (SEW-D model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig">SiglipConfig</a> (SigLIP model)</li> <li><strong>siglip_vision_model</strong>  <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipVisionConfig">SiglipVisionConfig</a> (SiglipVisionModel model)</li> <li><strong>speech-encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> (Speech Encoder decoder model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> (Speech2Text model)</li> <li><strong>speech_to_text_2</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> (Speech2Text2 model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Config">SpeechT5Config</a> (SpeechT5 model)</li> <li><strong>splinter</strong>  <a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> (Splinter model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> (SqueezeBERT model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a> (StableLm model)</li> <li><strong>swiftformer</strong>  <a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig">SwiftFormerConfig</a> (SwiftFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> (Swin Transformer model)</li> <li><strong>swin2sr</strong>  <a href="/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRConfig">Swin2SRConfig</a> (Swin2SR model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a> (Swin Transformer V2 model)</li> <li><strong>switch_transformers</strong>  <a href="/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig">SwitchTransformersConfig</a> (SwitchTransformers model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> (T5 model)</li> <li><strong>table-transformer</strong>  <a href="/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerConfig">TableTransformerConfig</a> (Table Transformer model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> (TAPAS model)</li> <li><strong>time_series_transformer</strong>  <a href="/docs/transformers/main/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig">TimeSeriesTransformerConfig</a> (Time Series Transformer model)</li> <li><strong>timesformer</strong>  <a href="/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerConfig">TimesformerConfig</a> (TimeSformer model)</li> <li><strong>timm_backbone</strong>  <a href="/docs/transformers/main/en/main_classes/backbones#transformers.TimmBackboneConfig">TimmBackboneConfig</a> (TimmBackbone model)</li> <li><strong>trajectory_transformer</strong>  <a href="/docs/transformers/main/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerConfig">TrajectoryTransformerConfig</a> (Trajectory Transformer model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> (Transformer-XL model)</li> <li><strong>trocr</strong>  <a href="/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> (TrOCR model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/main/en/model_doc/tvlt#transformers.TvltConfig">TvltConfig</a> (TVLT model)</li> <li><strong>tvp</strong>  <a href="/docs/transformers/main/en/model_doc/tvp#transformers.TvpConfig">TvpConfig</a> (TVP model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> (UMT5 model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> (UniSpeechSat model)</li> <li><strong>univnet</strong>  <a href="/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetConfig">UnivNetConfig</a> (UnivNet model)</li> <li><strong>upernet</strong>  <a href="/docs/transformers/main/en/model_doc/upernet#transformers.UperNetConfig">UperNetConfig</a> (UPerNet model)</li> <li><strong>van</strong>  <a href="/docs/transformers/main/en/model_doc/van#transformers.VanConfig">VanConfig</a> (VAN model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig">VideoMAEConfig</a> (VideoMAE model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/main/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> (ViLT model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig">VipLlavaConfig</a> (VipLlava model)</li> <li><strong>vision-encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> (Vision Encoder decoder model)</li> <li><strong>vision-text-dual-encoder</strong>  <a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> (VisionTextDualEncoder model)</li> <li><strong>visual_bert</strong>  <a href="/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> (VisualBERT model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> (ViT model)</li> <li><strong>vit_hybrid</strong>  <a href="/docs/transformers/main/en/model_doc/vit_hybrid#transformers.ViTHybridConfig">ViTHybridConfig</a> (ViT Hybrid model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> (ViTMAE model)</li> <li><strong>vit_msn</strong>  <a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig">ViTMSNConfig</a> (ViTMSN model)</li> <li><strong>vitdet</strong>  <a href="/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetConfig">VitDetConfig</a> (VitDet model)</li> <li><strong>vitmatte</strong>  <a href="/docs/transformers/main/en/model_doc/vitmatte#transformers.VitMatteConfig">VitMatteConfig</a> (ViTMatte model)</li> <li><strong>vits</strong>  <a href="/docs/transformers/main/en/model_doc/vits#transformers.VitsConfig">VitsConfig</a> (VITS model)</li> <li><strong>vivit</strong>  <a href="/docs/transformers/main/en/model_doc/vivit#transformers.VivitConfig">VivitConfig</a> (ViViT model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> (WavLM model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> (Whisper model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPConfig">XCLIPConfig</a> (X-CLIP model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> (XGLM model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> (XLM model)</li> <li><strong>xlm-prophetnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> (X-MOD model)</li> <li><strong>yolos</strong>  <a href="/docs/transformers/main/en/model_doc/yolos#transformers.YolosConfig">YolosConfig</a> (YOLOS model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> (YOSO model)</li>',Ux,vs,Nx,ws,Rd,Xx,Mh,OV="Register a new configuration for this class.",Gv,Pd,Vv,ce,Wd,qx,Fh,KV=`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer.from_pretrained">AutoTokenizer.from_pretrained()</a> class method.`,Qx,Th,eJ="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Dx,Uo,Sd,Hx,Ch,oJ="Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.",Yx,yh,rJ=`The tokenizer class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,zx,vh,nJ='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertTokenizer">AlbertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertTokenizerFast">AlbertTokenizerFast</a> (ALBERT model)</li> <li><strong>align</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (ALIGN model)</li> <li><strong>bark</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Bark model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartTokenizer">BartTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartTokenizerFast">BartTokenizerFast</a> (BART model)</li> <li><strong>barthez</strong>  <a href="/docs/transformers/main/en/model_doc/barthez#transformers.BarthezTokenizer">BarthezTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/barthez#transformers.BarthezTokenizerFast">BarthezTokenizerFast</a> (BARThez model)</li> <li><strong>bartpho</strong>  <a href="/docs/transformers/main/en/model_doc/bartpho#transformers.BartphoTokenizer">BartphoTokenizer</a> (BARTpho model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (BERT model)</li> <li><strong>bert-generation</strong>  <a href="/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationTokenizer">BertGenerationTokenizer</a> (Bert Generation model)</li> <li><strong>bert-japanese</strong>  <a href="/docs/transformers/main/en/model_doc/bert-japanese#transformers.BertJapaneseTokenizer">BertJapaneseTokenizer</a> (BertJapanese model)</li> <li><strong>bertweet</strong>  <a href="/docs/transformers/main/en/model_doc/bertweet#transformers.BertweetTokenizer">BertweetTokenizer</a> (BERTweet model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdTokenizer">BigBirdTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdTokenizerFast">BigBirdTokenizerFast</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusTokenizer">PegasusTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusTokenizerFast">PegasusTokenizerFast</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptTokenizer">BioGptTokenizer</a> (BioGpt model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotTokenizer">BlenderbotTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotTokenizerFast">BlenderbotTokenizerFast</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer">BlenderbotSmallTokenizer</a> (BlenderbotSmall model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (BLIP-2 model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomTokenizerFast">BloomTokenizerFast</a> (BLOOM model)</li> <li><strong>bridgetower</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (BridgeTower model)</li> <li><strong>bros</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (BROS model)</li> <li><strong>byt5</strong>  <a href="/docs/transformers/main/en/model_doc/byt5#transformers.ByT5Tokenizer">ByT5Tokenizer</a> (ByT5 model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertTokenizer">CamembertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertTokenizerFast">CamembertTokenizerFast</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineTokenizer">CanineTokenizer</a> (CANINE model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Chinese-CLIP model)</li> <li><strong>clap</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (CLAP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (CLIP model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (CLIPSeg model)</li> <li><strong>clvp</strong>  <a href="/docs/transformers/main/en/model_doc/clvp#transformers.ClvpTokenizer">ClvpTokenizer</a> (CLVP model)</li> <li><strong>code_llama</strong>  <a href="/docs/transformers/main/en/model_doc/code_llama#transformers.CodeLlamaTokenizer">CodeLlamaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/code_llama#transformers.CodeLlamaTokenizerFast">CodeLlamaTokenizerFast</a> (CodeLlama model)</li> <li><strong>codegen</strong>  <a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenTokenizer">CodeGenTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenTokenizerFast">CodeGenTokenizerFast</a> (CodeGen model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertTokenizer">ConvBertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertTokenizerFast">ConvBertTokenizerFast</a> (ConvBERT model)</li> <li><strong>cpm</strong>  <a href="/docs/transformers/main/en/model_doc/cpm#transformers.CpmTokenizer">CpmTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/cpm#transformers.CpmTokenizerFast">CpmTokenizerFast</a> (CPM model)</li> <li><strong>cpmant</strong>  <a href="/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntTokenizer">CpmAntTokenizer</a> (CPM-Ant model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLTokenizer">CTRLTokenizer</a> (CTRL model)</li> <li><strong>data2vec-audio</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer">Wav2Vec2CTCTokenizer</a> (Data2VecAudio model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaTokenizer">DebertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaTokenizerFast">DebertaTokenizerFast</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Tokenizer">DebertaV2Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2TokenizerFast">DebertaV2TokenizerFast</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizer">DistilBertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertTokenizerFast">DistilBertTokenizerFast</a> (DistilBERT model)</li> <li><strong>dpr</strong>  <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizer">DPRQuestionEncoderTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoderTokenizerFast">DPRQuestionEncoderTokenizerFast</a> (DPR model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraTokenizer">ElectraTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraTokenizerFast">ElectraTokenizerFast</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMTokenizer">ErnieMTokenizer</a> (ErnieM model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmTokenizer">EsmTokenizer</a> (ESM model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (Falcon model)</li> <li><strong>fastspeech2_conformer</strong>   (FastSpeech2Conformer model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertTokenizer">FlaubertTokenizer</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetTokenizer">FNetTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetTokenizerFast">FNetTokenizerFast</a> (FNet model)</li> <li><strong>fsmt</strong>  <a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTTokenizer">FSMTTokenizer</a> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizer">FunnelTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelTokenizerFast">FunnelTokenizerFast</a> (Funnel Transformer model)</li> <li><strong>git</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (GIT model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt-sw3#transformers.GPTSw3Tokenizer">GPTSw3Tokenizer</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseTokenizer">GPTNeoXJapaneseTokenizer</a> (GPT NeoX Japanese model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (GPT-J model)</li> <li><strong>gptsan-japanese</strong>  <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseTokenizer">GPTSanJapaneseTokenizer</a> (GPTSAN-japanese model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (GroupViT model)</li> <li><strong>herbert</strong>  <a href="/docs/transformers/main/en/model_doc/herbert#transformers.HerbertTokenizer">HerbertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/herbert#transformers.HerbertTokenizerFast">HerbertTokenizerFast</a> (HerBERT model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer">Wav2Vec2CTCTokenizer</a> (Hubert model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (I-BERT model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (IDEFICS model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (InstructBLIP model)</li> <li><strong>jukebox</strong>  <a href="/docs/transformers/main/en/model_doc/jukebox#transformers.JukeboxTokenizer">JukeboxTokenizer</a> (Jukebox model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a> (KOSMOS-2 model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMTokenizer">LayoutLMTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMTokenizerFast">LayoutLMTokenizerFast</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Tokenizer">LayoutLMv2Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2TokenizerFast">LayoutLMv2TokenizerFast</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer">LayoutLMv3Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast">LayoutLMv3TokenizerFast</a> (LayoutLMv3 model)</li> <li><strong>layoutxlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizer">LayoutXLMTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/layoutxlm#transformers.LayoutXLMTokenizerFast">LayoutXLMTokenizerFast</a> (LayoutXLM model)</li> <li><strong>led</strong>  <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDTokenizer">LEDTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDTokenizerFast">LEDTokenizerFast</a> (LED model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Tokenizer">LayoutLMv3Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3TokenizerFast">LayoutLMv3TokenizerFast</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (LLaMA model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (LLaVa model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerTokenizer">LongformerTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerTokenizerFast">LongformerTokenizerFast</a> (Longformer model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5TokenizerFast">T5TokenizerFast</a> (LongT5 model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeTokenizer">LukeTokenizer</a> (LUKE model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertTokenizer">LxmertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertTokenizerFast">LxmertTokenizerFast</a> (LXMERT model)</li> <li><strong>m2m_100</strong>  <a href="/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Tokenizer">M2M100Tokenizer</a> (M2M100 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianTokenizer">MarianTokenizer</a> (Marian model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartTokenizer">MBartTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartTokenizerFast">MBartTokenizerFast</a> (mBART model)</li> <li><strong>mbart50</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBart50Tokenizer">MBart50Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBart50TokenizerFast">MBart50TokenizerFast</a> (mBART-50 model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Megatron-BERT model)</li> <li><strong>mgp-str</strong>  <a href="/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrTokenizer">MgpstrTokenizer</a> (MGP-STR model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Mistral model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Mixtral model)</li> <li><strong>mluke</strong>  <a href="/docs/transformers/main/en/model_doc/mluke#transformers.MLukeTokenizer">MLukeTokenizer</a> (mLUKE model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertTokenizer">MobileBertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertTokenizerFast">MobileBertTokenizerFast</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetTokenizer">MPNetTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetTokenizerFast">MPNetTokenizerFast</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5Tokenizer">MT5Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5TokenizerFast">MT5TokenizerFast</a> (MT5 model)</li> <li><strong>musicgen</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5TokenizerFast">T5TokenizerFast</a> (MusicGen model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpTokenizer">MvpTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpTokenizerFast">MvpTokenizerFast</a> (MVP model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Nezha model)</li> <li><strong>nllb</strong>  <a href="/docs/transformers/main/en/model_doc/nllb#transformers.NllbTokenizer">NllbTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/nllb#transformers.NllbTokenizerFast">NllbTokenizerFast</a> (NLLB model)</li> <li><strong>nllb-moe</strong>  <a href="/docs/transformers/main/en/model_doc/nllb#transformers.NllbTokenizer">NllbTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/nllb#transformers.NllbTokenizerFast">NllbTokenizerFast</a> (NLLB-MOE model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertTokenizer">AlbertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertTokenizerFast">AlbertTokenizerFast</a> (Nystrmformer model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (OneFormer model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizer">OpenAIGPTTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTTokenizerFast">OpenAIGPTTokenizerFast</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Tokenizer">GPT2Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2TokenizerFast">GPT2TokenizerFast</a> (OPT model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (OWL-ViT model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusTokenizer">PegasusTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusTokenizerFast">PegasusTokenizerFast</a> (Pegasus model)</li> <li><strong>pegasus_x</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusTokenizer">PegasusTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusTokenizerFast">PegasusTokenizerFast</a> (PEGASUS-X model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverTokenizer">PerceiverTokenizer</a> (Perceiver model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenTokenizer">CodeGenTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenTokenizerFast">CodeGenTokenizerFast</a> (Phi model)</li> <li><strong>phobert</strong>  <a href="/docs/transformers/main/en/model_doc/phobert#transformers.PhobertTokenizer">PhobertTokenizer</a> (PhoBERT model)</li> <li><strong>pix2struct</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5TokenizerFast">T5TokenizerFast</a> (Pix2Struct model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartTokenizer">PLBartTokenizer</a> (PLBart model)</li> <li><strong>prophetnet</strong>  <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetTokenizer">ProphetNetTokenizer</a> (ProphetNet model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Tokenizer">Qwen2Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2TokenizerFast">Qwen2TokenizerFast</a> (Qwen2 model)</li> <li><strong>rag</strong>  <a href="/docs/transformers/main/en/model_doc/rag#transformers.RagTokenizer">RagTokenizer</a> (RAG model)</li> <li><strong>realm</strong>  <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizer">RealmTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/realm#transformers.RealmTokenizerFast">RealmTokenizerFast</a> (REALM model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerTokenizer">ReformerTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerTokenizerFast">ReformerTokenizerFast</a> (Reformer model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertTokenizer">RemBertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertTokenizerFast">RemBertTokenizerFast</a> (RemBERT model)</li> <li><strong>retribert</strong>  <a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertTokenizer">RetriBertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertTokenizerFast">RetriBertTokenizerFast</a> (RetriBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizerFast">RobertaTokenizerFast</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertTokenizer">RoCBertTokenizer</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerTokenizer">RoFormerTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerTokenizerFast">RoFormerTokenizerFast</a> (RoFormer model)</li> <li><strong>rwkv</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (RWKV model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer">SeamlessM4TTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast">SeamlessM4TTokenizerFast</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizer">SeamlessM4TTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TTokenizerFast">SeamlessM4TTokenizerFast</a> (SeamlessM4Tv2 model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipTokenizer">SiglipTokenizer</a> (SigLIP model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextTokenizer">Speech2TextTokenizer</a> (Speech2Text model)</li> <li><strong>speech_to_text_2</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text_2#transformers.Speech2Text2Tokenizer">Speech2Text2Tokenizer</a> (Speech2Text2 model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Tokenizer">SpeechT5Tokenizer</a> (SpeechT5 model)</li> <li><strong>splinter</strong>  <a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterTokenizer">SplinterTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterTokenizerFast">SplinterTokenizerFast</a> (Splinter model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertTokenizer">SqueezeBertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertTokenizerFast">SqueezeBertTokenizerFast</a> (SqueezeBERT model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a> (StableLm model)</li> <li><strong>switch_transformers</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5TokenizerFast">T5TokenizerFast</a> (SwitchTransformers model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5TokenizerFast">T5TokenizerFast</a> (T5 model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasTokenizer">TapasTokenizer</a> (TAPAS model)</li> <li><strong>tapex</strong>  <a href="/docs/transformers/main/en/model_doc/tapex#transformers.TapexTokenizer">TapexTokenizer</a> (TAPEX model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLTokenizer">TransfoXLTokenizer</a> (Transformer-XL model)</li> <li><strong>tvp</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (TVP model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5Tokenizer">T5Tokenizer</a> or <a href="/docs/transformers/main/en/model_doc/mt5#transformers.T5TokenizerFast">T5TokenizerFast</a> (UMT5 model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (ViLT model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizer">LlamaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a> (VipLlava model)</li> <li><strong>visual_bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (VisualBERT model)</li> <li><strong>vits</strong>  <a href="/docs/transformers/main/en/model_doc/vits#transformers.VitsTokenizer">VitsTokenizer</a> (VITS model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer">Wav2Vec2CTCTokenizer</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer">Wav2Vec2CTCTokenizer</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2CTCTokenizer">Wav2Vec2CTCTokenizer</a> (Wav2Vec2-Conformer model)</li> <li><strong>wav2vec2_phoneme</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2_phoneme#transformers.Wav2Vec2PhonemeCTCTokenizer">Wav2Vec2PhonemeCTCTokenizer</a> (Wav2Vec2Phoneme model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperTokenizer">WhisperTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperTokenizerFast">WhisperTokenizerFast</a> (Whisper model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (X-CLIP model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMTokenizer">XGLMTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMTokenizerFast">XGLMTokenizerFast</a> (XGLM model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMTokenizer">XLMTokenizer</a> (XLM model)</li> <li><strong>xlm-prophetnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetTokenizer">XLMProphetNetTokenizer</a> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetTokenizer">XLNetTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetTokenizerFast">XLNetTokenizerFast</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizer">XLMRobertaTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaTokenizerFast">XLMRobertaTokenizerFast</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertTokenizer">AlbertTokenizer</a> or <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertTokenizerFast">AlbertTokenizerFast</a> (YOSO model)</li>',Ox,xs,Kx,ks,Gd,ek,wh,tJ="Register a new tokenizer in this mapping.",Jv,Vd,Ev,fe,Jd,ok,xh,aJ=`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained">AutoFeatureExtractor.from_pretrained()</a> class method.`,rk,kh,sJ="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",nk,j,Ed,tk,Lh,iJ="Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary.",ak,$h,lJ=`The feature extractor class to instantiate is selected based on the <code>model_type</code> property of the config object
(either passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its
missing, by falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,sk,Bh,dJ='<li><strong>audio-spectrogram-transformer</strong>  <a href="/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor">ASTFeatureExtractor</a> (Audio Spectrogram Transformer model)</li> <li><strong>beit</strong>  <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitFeatureExtractor">BeitFeatureExtractor</a> (BEiT model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPFeatureExtractor">ChineseCLIPFeatureExtractor</a> (Chinese-CLIP model)</li> <li><strong>clap</strong>  <a href="/docs/transformers/main/en/model_doc/clap#transformers.ClapFeatureExtractor">ClapFeatureExtractor</a> (CLAP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPFeatureExtractor">CLIPFeatureExtractor</a> (CLIP model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (CLIPSeg model)</li> <li><strong>clvp</strong>  <a href="/docs/transformers/main/en/model_doc/clvp#transformers.ClvpFeatureExtractor">ClvpFeatureExtractor</a> (CLVP model)</li> <li><strong>conditional_detr</strong>  <a href="/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrFeatureExtractor">ConditionalDetrFeatureExtractor</a> (Conditional DETR model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (ConvNeXT model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (CvT model)</li> <li><strong>data2vec-audio</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Data2VecAudio model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitFeatureExtractor">BeitFeatureExtractor</a> (Data2VecVision model)</li> <li><strong>deformable_detr</strong>  <a href="/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrFeatureExtractor">DeformableDetrFeatureExtractor</a> (Deformable DETR model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTFeatureExtractor">DeiTFeatureExtractor</a> (DeiT model)</li> <li><strong>detr</strong>  <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrFeatureExtractor">DetrFeatureExtractor</a> (DETR model)</li> <li><strong>dinat</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (DiNAT model)</li> <li><strong>donut-swin</strong>  <a href="/docs/transformers/main/en/model_doc/donut#transformers.DonutFeatureExtractor">DonutFeatureExtractor</a> (DonutSwin model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTFeatureExtractor">DPTFeatureExtractor</a> (DPT model)</li> <li><strong>encodec</strong>  <a href="/docs/transformers/main/en/model_doc/encodec#transformers.EncodecFeatureExtractor">EncodecFeatureExtractor</a> (EnCodec model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaFeatureExtractor">FlavaFeatureExtractor</a> (FLAVA model)</li> <li><strong>glpn</strong>  <a href="/docs/transformers/main/en/model_doc/glpn#transformers.GLPNFeatureExtractor">GLPNFeatureExtractor</a> (GLPN model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPFeatureExtractor">CLIPFeatureExtractor</a> (GroupViT model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Hubert model)</li> <li><strong>imagegpt</strong>  <a href="/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTFeatureExtractor">ImageGPTFeatureExtractor</a> (ImageGPT model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor">LayoutLMv2FeatureExtractor</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3FeatureExtractor">LayoutLMv3FeatureExtractor</a> (LayoutLMv3 model)</li> <li><strong>levit</strong>  <a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitFeatureExtractor">LevitFeatureExtractor</a> (LeViT model)</li> <li><strong>maskformer</strong>  <a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerFeatureExtractor">MaskFormerFeatureExtractor</a> (MaskFormer model)</li> <li><strong>mctct</strong>  <a href="/docs/transformers/main/en/model_doc/mctct#transformers.MCTCTFeatureExtractor">MCTCTFeatureExtractor</a> (M-CTC-T model)</li> <li><strong>mobilenet_v1</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1FeatureExtractor">MobileNetV1FeatureExtractor</a> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2FeatureExtractor">MobileNetV2FeatureExtractor</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTFeatureExtractor">MobileViTFeatureExtractor</a> (MobileViT model)</li> <li><strong>nat</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (NAT model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTFeatureExtractor">OwlViTFeatureExtractor</a> (OWL-ViT model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverFeatureExtractor">PerceiverFeatureExtractor</a> (Perceiver model)</li> <li><strong>poolformer</strong>  <a href="/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerFeatureExtractor">PoolFormerFeatureExtractor</a> (PoolFormer model)</li> <li><strong>pop2piano</strong>  <a href="/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoFeatureExtractor">Pop2PianoFeatureExtractor</a> (Pop2Piano model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (RegNet model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (ResNet model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor">SeamlessM4TFeatureExtractor</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TFeatureExtractor">SeamlessM4TFeatureExtractor</a> (SeamlessM4Tv2 model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerFeatureExtractor">SegformerFeatureExtractor</a> (SegFormer model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (SEW-D model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextFeatureExtractor">Speech2TextFeatureExtractor</a> (Speech2Text model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5FeatureExtractor">SpeechT5FeatureExtractor</a> (SpeechT5 model)</li> <li><strong>swiftformer</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (SwiftFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (Swin Transformer model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (Swin Transformer V2 model)</li> <li><strong>table-transformer</strong>  <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrFeatureExtractor">DetrFeatureExtractor</a> (Table Transformer model)</li> <li><strong>timesformer</strong>  <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEFeatureExtractor">VideoMAEFeatureExtractor</a> (TimeSformer model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/main/en/model_doc/tvlt#transformers.TvltFeatureExtractor">TvltFeatureExtractor</a> (TVLT model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (UniSpeechSat model)</li> <li><strong>univnet</strong>  <a href="/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetFeatureExtractor">UnivNetFeatureExtractor</a> (UnivNet model)</li> <li><strong>van</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (VAN model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEFeatureExtractor">VideoMAEFeatureExtractor</a> (VideoMAE model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/main/en/model_doc/vilt#transformers.ViltFeatureExtractor">ViltFeatureExtractor</a> (ViLT model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (ViT model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (ViTMAE model)</li> <li><strong>vit_msn</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTFeatureExtractor">ViTFeatureExtractor</a> (ViTMSN model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2FeatureExtractor">Wav2Vec2FeatureExtractor</a> (WavLM model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperFeatureExtractor">WhisperFeatureExtractor</a> (Whisper model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPFeatureExtractor">CLIPFeatureExtractor</a> (X-CLIP model)</li> <li><strong>yolos</strong>  <a href="/docs/transformers/main/en/model_doc/yolos#transformers.YolosFeatureExtractor">YolosFeatureExtractor</a> (YOLOS model)</li>',ik,Ls,lk,$s,dk,Bs,Id,mk,jh,mJ="Register a new feature extractor for this class.",Iv,Ud,Uv,ge,Nd,ck,Zh,cJ=`This is a generic image processor class that will be instantiated as one of the image processor classes of the
library when created with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor.from_pretrained">AutoImageProcessor.from_pretrained()</a> class method.`,fk,Ah,fJ="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",gk,Z,Xd,pk,Rh,gJ="Instantiate one of the image processor classes of the library from a pretrained model vocabulary.",hk,Ph,pJ=`The image processor class to instantiate is selected based on the <code>model_type</code> property of the config object
(either passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its
missing, by falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,uk,Wh,hJ='<li><strong>align</strong>  <a href="/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetImageProcessor">EfficientNetImageProcessor</a> (ALIGN model)</li> <li><strong>beit</strong>  <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitImageProcessor">BeitImageProcessor</a> (BEiT model)</li> <li><strong>bit</strong>  <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> (BiT model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> (BLIP-2 model)</li> <li><strong>bridgetower</strong>  <a href="/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerImageProcessor">BridgeTowerImageProcessor</a> (BridgeTower model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPImageProcessor">ChineseCLIPImageProcessor</a> (Chinese-CLIP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (CLIP model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> (CLIPSeg model)</li> <li><strong>conditional_detr</strong>  <a href="/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor">ConditionalDetrImageProcessor</a> (Conditional DETR model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (ConvNeXTV2 model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (CvT model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitImageProcessor">BeitImageProcessor</a> (Data2VecVision model)</li> <li><strong>deformable_detr</strong>  <a href="/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrImageProcessor">DeformableDetrImageProcessor</a> (Deformable DETR model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTImageProcessor">DeiTImageProcessor</a> (DeiT model)</li> <li><strong>depth_anything</strong>  <a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTImageProcessor">DPTImageProcessor</a> (Depth Anything model)</li> <li><strong>deta</strong>  <a href="/docs/transformers/main/en/model_doc/deta#transformers.DetaImageProcessor">DetaImageProcessor</a> (DETA model)</li> <li><strong>detr</strong>  <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrImageProcessor">DetrImageProcessor</a> (DETR model)</li> <li><strong>dinat</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> (DiNAT model)</li> <li><strong>dinov2</strong>  <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> (DINOv2 model)</li> <li><strong>donut-swin</strong>  <a href="/docs/transformers/main/en/model_doc/donut#transformers.DonutImageProcessor">DonutImageProcessor</a> (DonutSwin model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTImageProcessor">DPTImageProcessor</a> (DPT model)</li> <li><strong>efficientformer</strong>  <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerImageProcessor">EfficientFormerImageProcessor</a> (EfficientFormer model)</li> <li><strong>efficientnet</strong>  <a href="/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetImageProcessor">EfficientNetImageProcessor</a> (EfficientNet model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a> (FLAVA model)</li> <li><strong>focalnet</strong>  <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> (FocalNet model)</li> <li><strong>fuyu</strong>  <a href="/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuImageProcessor">FuyuImageProcessor</a> (Fuyu model)</li> <li><strong>git</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (GIT model)</li> <li><strong>glpn</strong>  <a href="/docs/transformers/main/en/model_doc/glpn#transformers.GLPNImageProcessor">GLPNImageProcessor</a> (GLPN model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (GroupViT model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsImageProcessor">IdeficsImageProcessor</a> (IDEFICS model)</li> <li><strong>imagegpt</strong>  <a href="/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTImageProcessor">ImageGPTImageProcessor</a> (ImageGPT model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> (InstructBLIP model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (KOSMOS-2 model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ImageProcessor">LayoutLMv2ImageProcessor</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ImageProcessor">LayoutLMv3ImageProcessor</a> (LayoutLMv3 model)</li> <li><strong>levit</strong>  <a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitImageProcessor">LevitImageProcessor</a> (LeViT model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (LLaVa model)</li> <li><strong>mask2former</strong>  <a href="/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerImageProcessor">Mask2FormerImageProcessor</a> (Mask2Former model)</li> <li><strong>maskformer</strong>  <a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerImageProcessor">MaskFormerImageProcessor</a> (MaskFormer model)</li> <li><strong>mgp-str</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> (MGP-STR model)</li> <li><strong>mobilenet_v1</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1ImageProcessor">MobileNetV1ImageProcessor</a> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ImageProcessor">MobileNetV2ImageProcessor</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a> (MobileViT model)</li> <li><strong>mobilevitv2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTImageProcessor">MobileViTImageProcessor</a> (MobileViTV2 model)</li> <li><strong>nat</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> (NAT model)</li> <li><strong>nougat</strong>  <a href="/docs/transformers/main/en/model_doc/nougat#transformers.NougatImageProcessor">NougatImageProcessor</a> (Nougat model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerImageProcessor">OneFormerImageProcessor</a> (OneFormer model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2ImageProcessor">Owlv2ImageProcessor</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTImageProcessor">OwlViTImageProcessor</a> (OWL-ViT model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverImageProcessor">PerceiverImageProcessor</a> (Perceiver model)</li> <li><strong>pix2struct</strong>  <a href="/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructImageProcessor">Pix2StructImageProcessor</a> (Pix2Struct model)</li> <li><strong>poolformer</strong>  <a href="/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerImageProcessor">PoolFormerImageProcessor</a> (PoolFormer model)</li> <li><strong>pvt</strong>  <a href="/docs/transformers/main/en/model_doc/pvt#transformers.PvtImageProcessor">PvtImageProcessor</a> (PVT model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (RegNet model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (ResNet model)</li> <li><strong>sam</strong>  <a href="/docs/transformers/main/en/model_doc/sam#transformers.SamImageProcessor">SamImageProcessor</a> (SAM model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerImageProcessor">SegformerImageProcessor</a> (SegFormer model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipImageProcessor">SiglipImageProcessor</a> (SigLIP model)</li> <li><strong>swiftformer</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> (SwiftFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> (Swin Transformer model)</li> <li><strong>swin2sr</strong>  <a href="/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRImageProcessor">Swin2SRImageProcessor</a> (Swin2SR model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> (Swin Transformer V2 model)</li> <li><strong>table-transformer</strong>  <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrImageProcessor">DetrImageProcessor</a> (Table Transformer model)</li> <li><strong>timesformer</strong>  <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEImageProcessor">VideoMAEImageProcessor</a> (TimeSformer model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/main/en/model_doc/tvlt#transformers.TvltImageProcessor">TvltImageProcessor</a> (TVLT model)</li> <li><strong>tvp</strong>  <a href="/docs/transformers/main/en/model_doc/tvp#transformers.TvpImageProcessor">TvpImageProcessor</a> (TVP model)</li> <li><strong>upernet</strong>  <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerImageProcessor">SegformerImageProcessor</a> (UPerNet model)</li> <li><strong>van</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (VAN model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEImageProcessor">VideoMAEImageProcessor</a> (VideoMAE model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/main/en/model_doc/vilt#transformers.ViltImageProcessor">ViltImageProcessor</a> (ViLT model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (VipLlava model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> (ViT model)</li> <li><strong>vit_hybrid</strong>  <a href="/docs/transformers/main/en/model_doc/vit_hybrid#transformers.ViTHybridImageProcessor">ViTHybridImageProcessor</a> (ViT Hybrid model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> (ViTMAE model)</li> <li><strong>vit_msn</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTImageProcessor">ViTImageProcessor</a> (ViTMSN model)</li> <li><strong>vitmatte</strong>  <a href="/docs/transformers/main/en/model_doc/vitmatte#transformers.VitMatteImageProcessor">VitMatteImageProcessor</a> (ViTMatte model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (X-CLIP model)</li> <li><strong>yolos</strong>  <a href="/docs/transformers/main/en/model_doc/yolos#transformers.YolosImageProcessor">YolosImageProcessor</a> (YOLOS model)</li>',_k,js,bk,Zs,Mk,As,qd,Fk,Sh,uJ="Register a new image processor for this class.",Nv,Qd,Xv,pe,Dd,Tk,Gh,_J=`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoProcessor.from_pretrained">AutoProcessor.from_pretrained()</a> class method.`,Ck,Vh,bJ="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",yk,A,Hd,vk,Jh,MJ="Instantiate one of the processor classes of the library from a pretrained model vocabulary.",wk,Eh,FJ=`The processor class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible):`,xk,Ih,TJ='<li><strong>align</strong>  <a href="/docs/transformers/main/en/model_doc/align#transformers.AlignProcessor">AlignProcessor</a> (ALIGN model)</li> <li><strong>altclip</strong>  <a href="/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a> (AltCLIP model)</li> <li><strong>bark</strong>  <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkProcessor">BarkProcessor</a> (Bark model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipProcessor">BlipProcessor</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> (BLIP-2 model)</li> <li><strong>bridgetower</strong>  <a href="/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerProcessor">BridgeTowerProcessor</a> (BridgeTower model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPProcessor">ChineseCLIPProcessor</a> (Chinese-CLIP model)</li> <li><strong>clap</strong>  <a href="/docs/transformers/main/en/model_doc/clap#transformers.ClapProcessor">ClapProcessor</a> (CLAP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> (CLIP model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> (CLIPSeg model)</li> <li><strong>clvp</strong>  <a href="/docs/transformers/main/en/model_doc/clvp#transformers.ClvpProcessor">ClvpProcessor</a> (CLVP model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaProcessor">FlavaProcessor</a> (FLAVA model)</li> <li><strong>fuyu</strong>  <a href="/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuProcessor">FuyuProcessor</a> (Fuyu model)</li> <li><strong>git</strong>  <a href="/docs/transformers/main/en/model_doc/git#transformers.GitProcessor">GitProcessor</a> (GIT model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> (GroupViT model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (Hubert model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsProcessor">IdeficsProcessor</a> (IDEFICS model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipProcessor">InstructBlipProcessor</a> (InstructBLIP model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Processor">Kosmos2Processor</a> (KOSMOS-2 model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Processor">LayoutLMv2Processor</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Processor">LayoutLMv3Processor</a> (LayoutLMv3 model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaProcessor">LlavaProcessor</a> (LLaVa model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMProcessor">MarkupLMProcessor</a> (MarkupLM model)</li> <li><strong>mctct</strong>  <a href="/docs/transformers/main/en/model_doc/mctct#transformers.MCTCTProcessor">MCTCTProcessor</a> (M-CTC-T model)</li> <li><strong>mgp-str</strong>  <a href="/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrProcessor">MgpstrProcessor</a> (MGP-STR model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerProcessor">OneFormerProcessor</a> (OneFormer model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Processor">Owlv2Processor</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTProcessor">OwlViTProcessor</a> (OWL-ViT model)</li> <li><strong>pix2struct</strong>  <a href="/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructProcessor">Pix2StructProcessor</a> (Pix2Struct model)</li> <li><strong>pop2piano</strong>  <a href="/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoProcessor">Pop2PianoProcessor</a> (Pop2Piano model)</li> <li><strong>sam</strong>  <a href="/docs/transformers/main/en/model_doc/sam#transformers.SamProcessor">SamProcessor</a> (SAM model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TProcessor">SeamlessM4TProcessor</a> (SeamlessM4T model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (SEW-D model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipProcessor">SiglipProcessor</a> (SigLIP model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextProcessor">Speech2TextProcessor</a> (Speech2Text model)</li> <li><strong>speech_to_text_2</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text_2#transformers.Speech2Text2Processor">Speech2Text2Processor</a> (Speech2Text2 model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Processor">SpeechT5Processor</a> (SpeechT5 model)</li> <li><strong>trocr</strong>  <a href="/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRProcessor">TrOCRProcessor</a> (TrOCR model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/main/en/model_doc/tvlt#transformers.TvltProcessor">TvltProcessor</a> (TVLT model)</li> <li><strong>tvp</strong>  <a href="/docs/transformers/main/en/model_doc/tvp#transformers.TvpProcessor">TvpProcessor</a> (TVP model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (UniSpeechSat model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/main/en/model_doc/vilt#transformers.ViltProcessor">ViltProcessor</a> (ViLT model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaProcessor">LlavaProcessor</a> (VipLlava model)</li> <li><strong>vision-text-dual-encoder</strong>  <a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderProcessor">VisionTextDualEncoderProcessor</a> (VisionTextDualEncoder model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Processor">Wav2Vec2Processor</a> (WavLM model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperProcessor">WhisperProcessor</a> (Whisper model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPProcessor">XCLIPProcessor</a> (X-CLIP model)</li>',kk,Rs,Lk,Ps,$k,Ws,Yd,Bk,Uh,CJ="Register a new processor for this class.",qv,zd,Qv,Od,yJ="The following auto classes are available for instantiating a base model class without a specific head.",Dv,Kd,Hv,he,em,jk,Nh,vJ=`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,Zk,Xh,wJ="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Ak,Sn,om,Rk,qh,xJ="Instantiates one of the base model classes of the library from a configuration.",Pk,Qh,kJ=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Wk,Ss,Sk,R,rm,Gk,Dh,LJ="Instantiate one of the base model classes of the library from a pretrained model.",Vk,Hh,$J=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Jk,Yh,BJ='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li> <li><strong>align</strong>  <a href="/docs/transformers/main/en/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li> <li><strong>altclip</strong>  <a href="/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li> <li><strong>audio-spectrogram-transformer</strong>  <a href="/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel">ASTModel</a> (Audio Spectrogram Transformer model)</li> <li><strong>autoformer</strong>  <a href="/docs/transformers/main/en/model_doc/autoformer#transformers.AutoformerModel">AutoformerModel</a> (Autoformer model)</li> <li><strong>bark</strong>  <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkModel">BarkModel</a> (Bark model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li> <li><strong>beit</strong>  <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li> <li><strong>bert-generation</strong>  <a href="/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptModel">BioGptModel</a> (BioGpt model)</li> <li><strong>bit</strong>  <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitModel">BitModel</a> (BiT model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> (BLIP-2 model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomModel">BloomModel</a> (BLOOM model)</li> <li><strong>bridgetower</strong>  <a href="/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a> (BridgeTower model)</li> <li><strong>bros</strong>  <a href="/docs/transformers/main/en/model_doc/bros#transformers.BrosModel">BrosModel</a> (BROS model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (CANINE model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li> <li><strong>clap</strong>  <a href="/docs/transformers/main/en/model_doc/clap#transformers.ClapModel">ClapModel</a> (CLAP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li> <li><strong>clip_vision_model</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPVisionModel">CLIPVisionModel</a> (CLIPVisionModel model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li> <li><strong>clvp</strong>  <a href="/docs/transformers/main/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration">ClvpModelForConditionalGeneration</a> (CLVP model)</li> <li><strong>code_llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel">LlamaModel</a> (CodeLlama model)</li> <li><strong>codegen</strong>  <a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenModel">CodeGenModel</a> (CodeGen model)</li> <li><strong>conditional_detr</strong>  <a href="/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel">ConditionalDetrModel</a> (Conditional DETR model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong>  <a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Model">ConvNextV2Model</a> (ConvNeXTV2 model)</li> <li><strong>cpmant</strong>  <a href="/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntModel">CpmAntModel</a> (CPM-Ant model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/main/en/model_doc/cvt#transformers.CvtModel">CvtModel</a> (CvT model)</li> <li><strong>data2vec-audio</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionModel">Data2VecVisionModel</a> (Data2VecVision model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li> <li><strong>decision_transformer</strong>  <a href="/docs/transformers/main/en/model_doc/decision_transformer#transformers.DecisionTransformerModel">DecisionTransformerModel</a> (Decision Transformer model)</li> <li><strong>deformable_detr</strong>  <a href="/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrModel">DeformableDetrModel</a> (Deformable DETR model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li> <li><strong>deta</strong>  <a href="/docs/transformers/main/en/model_doc/deta#transformers.DetaModel">DetaModel</a> (DETA model)</li> <li><strong>detr</strong>  <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li> <li><strong>dinat</strong>  <a href="/docs/transformers/main/en/model_doc/dinat#transformers.DinatModel">DinatModel</a> (DiNAT model)</li> <li><strong>dinov2</strong>  <a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Model">Dinov2Model</a> (DINOv2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li> <li><strong>donut-swin</strong>  <a href="/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinModel">DonutSwinModel</a> (DonutSwin model)</li> <li><strong>dpr</strong>  <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTModel">DPTModel</a> (DPT model)</li> <li><strong>efficientformer</strong>  <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerModel">EfficientFormerModel</a> (EfficientFormer model)</li> <li><strong>efficientnet</strong>  <a href="/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetModel">EfficientNetModel</a> (EfficientNet model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li> <li><strong>encodec</strong>  <a href="/docs/transformers/main/en/model_doc/encodec#transformers.EncodecModel">EncodecModel</a> (EnCodec model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieModel">ErnieModel</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMModel">ErnieMModel</a> (ErnieM model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmModel">EsmModel</a> (ESM model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconModel">FalconModel</a> (Falcon model)</li> <li><strong>fastspeech2_conformer</strong>  <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerModel">FastSpeech2ConformerModel</a> (FastSpeech2Conformer model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel">FlavaModel</a> (FLAVA model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li> <li><strong>focalnet</strong>  <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetModel">FocalNetModel</a> (FocalNet model)</li> <li><strong>fsmt</strong>  <a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li> <li><strong>git</strong>  <a href="/docs/transformers/main/en/model_doc/git#transformers.GitModel">GitModel</a> (GIT model)</li> <li><strong>glpn</strong>  <a href="/docs/transformers/main/en/model_doc/glpn#transformers.GLPNModel">GLPNModel</a> (GLPN model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeModel">GPTBigCodeModel</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXModel">GPTNeoXModel</a> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel">GPTNeoXJapaneseModel</a> (GPT NeoX Japanese model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li> <li><strong>gptsan-japanese</strong>  <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li> <li><strong>graphormer</strong>  <a href="/docs/transformers/main/en/model_doc/graphormer#transformers.GraphormerModel">GraphormerModel</a> (Graphormer model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTModel">GroupViTModel</a> (GroupViT model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsModel">IdeficsModel</a> (IDEFICS model)</li> <li><strong>imagegpt</strong>  <a href="/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li> <li><strong>informer</strong>  <a href="/docs/transformers/main/en/model_doc/informer#transformers.InformerModel">InformerModel</a> (Informer model)</li> <li><strong>jukebox</strong>  <a href="/docs/transformers/main/en/model_doc/jukebox#transformers.JukeboxModel">JukeboxModel</a> (Jukebox model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Model">Kosmos2Model</a> (KOSMOS-2 model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model">LayoutLMv3Model</a> (LayoutLMv3 model)</li> <li><strong>led</strong>  <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li> <li><strong>levit</strong>  <a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitModel">LevitModel</a> (LeViT model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltModel">LiltModel</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel">LlamaModel</a> (LLaMA model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Model">LongT5Model</a> (LongT5 model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li> <li><strong>m2m_100</strong>  <a href="/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMModel">MarkupLMModel</a> (MarkupLM model)</li> <li><strong>mask2former</strong>  <a href="/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerModel">Mask2FormerModel</a> (Mask2Former model)</li> <li><strong>maskformer</strong>  <a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li> <li><strong>maskformer-swin</strong>  <code>MaskFormerSwinModel</code> (MaskFormerSwin model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li> <li><strong>mctct</strong>  <a href="/docs/transformers/main/en/model_doc/mctct#transformers.MCTCTModel">MCTCTModel</a> (M-CTC-T model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaModel">MegaModel</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (Megatron-BERT model)</li> <li><strong>mgp-str</strong>  <a href="/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrForSceneTextRecognition">MgpstrForSceneTextRecognition</a> (MGP-STR model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralModel">MistralModel</a> (Mistral model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralModel">MixtralModel</a> (Mixtral model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li> <li><strong>mobilenet_v1</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Model">MobileNetV1Model</a> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model">MobileNetV2Model</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTModel">MobileViTModel</a> (MobileViT model)</li> <li><strong>mobilevitv2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Model">MobileViTV2Model</a> (MobileViTV2 model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptModel">MptModel</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraModel">MraModel</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (MT5 model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpModel">MvpModel</a> (MVP model)</li> <li><strong>nat</strong>  <a href="/docs/transformers/main/en/model_doc/nat#transformers.NatModel">NatModel</a> (NAT model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaModel">NezhaModel</a> (Nezha model)</li> <li><strong>nllb-moe</strong>  <a href="/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeModel">NllbMoeModel</a> (NLLB-MOE model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystrmformer model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerModel">OneFormerModel</a> (OneFormer model)</li> <li><strong>open-llama</strong>  <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaModel">OpenLlamaModel</a> (OpenLlama model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTModel">OPTModel</a> (OPT model)</li> <li><strong>owlv2</strong>  <a href="/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Model">Owlv2Model</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel">OwlViTModel</a> (OWL-ViT model)</li> <li><strong>patchtsmixer</strong>  <a href="/docs/transformers/main/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel">PatchTSMixerModel</a> (PatchTSMixer model)</li> <li><strong>patchtst</strong>  <a href="/docs/transformers/main/en/model_doc/patchtst#transformers.PatchTSTModel">PatchTSTModel</a> (PatchTST model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li> <li><strong>pegasus_x</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXModel">PegasusXModel</a> (PEGASUS-X model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonModel">PersimmonModel</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiModel">PhiModel</a> (Phi model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li> <li><strong>poolformer</strong>  <a href="/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li> <li><strong>prophetnet</strong>  <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li> <li><strong>pvt</strong>  <a href="/docs/transformers/main/en/model_doc/pvt#transformers.PvtModel">PvtModel</a> (PVT model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Model">Qwen2Model</a> (Qwen2 model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetModel">RegNetModel</a> (RegNet model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetModel">ResNetModel</a> (ResNet model)</li> <li><strong>retribert</strong>  <a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormModel">RobertaPreLayerNormModel</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertModel">RoCBertModel</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li> <li><strong>rwkv</strong>  <a href="/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvModel">RwkvModel</a> (RWKV model)</li> <li><strong>sam</strong>  <a href="/docs/transformers/main/en/model_doc/sam#transformers.SamModel">SamModel</a> (SAM model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel">SeamlessM4TModel</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Model">SeamlessM4Tv2Model</a> (SeamlessM4Tv2 model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/main/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipModel">SiglipModel</a> (SigLIP model)</li> <li><strong>siglip_vision_model</strong>  <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipVisionModel">SiglipVisionModel</a> (SiglipVisionModel model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Model">SpeechT5Model</a> (SpeechT5 model)</li> <li><strong>splinter</strong>  <a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmModel">StableLmModel</a> (StableLm model)</li> <li><strong>swiftformer</strong>  <a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerModel">SwiftFormerModel</a> (SwiftFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin Transformer model)</li> <li><strong>swin2sr</strong>  <a href="/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRModel">Swin2SRModel</a> (Swin2SR model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Model">Swinv2Model</a> (Swin Transformer V2 model)</li> <li><strong>switch_transformers</strong>  <a href="/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersModel">SwitchTransformersModel</a> (SwitchTransformers model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li> <li><strong>table-transformer</strong>  <a href="/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerModel">TableTransformerModel</a> (Table Transformer model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li> <li><strong>time_series_transformer</strong>  <a href="/docs/transformers/main/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel">TimeSeriesTransformerModel</a> (Time Series Transformer model)</li> <li><strong>timesformer</strong>  <a href="/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerModel">TimesformerModel</a> (TimeSformer model)</li> <li><strong>timm_backbone</strong>  <a href="/docs/transformers/main/en/main_classes/backbones#transformers.TimmBackbone">TimmBackbone</a> (TimmBackbone model)</li> <li><strong>trajectory_transformer</strong>  <a href="/docs/transformers/main/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel">TrajectoryTransformerModel</a> (Trajectory Transformer model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/main/en/model_doc/tvlt#transformers.TvltModel">TvltModel</a> (TVLT model)</li> <li><strong>tvp</strong>  <a href="/docs/transformers/main/en/model_doc/tvp#transformers.TvpModel">TvpModel</a> (TVP model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Model">UMT5Model</a> (UMT5 model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li> <li><strong>univnet</strong>  <a href="/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetModel">UnivNetModel</a> (UnivNet model)</li> <li><strong>van</strong>  <a href="/docs/transformers/main/en/model_doc/van#transformers.VanModel">VanModel</a> (VAN model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEModel">VideoMAEModel</a> (VideoMAE model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/main/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li> <li><strong>vision-text-dual-encoder</strong>  <a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li> <li><strong>visual_bert</strong>  <a href="/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBERT model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li> <li><strong>vit_hybrid</strong>  <a href="/docs/transformers/main/en/model_doc/vit_hybrid#transformers.ViTHybridModel">ViTHybridModel</a> (ViT Hybrid model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li> <li><strong>vit_msn</strong>  <a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNModel">ViTMSNModel</a> (ViTMSN model)</li> <li><strong>vitdet</strong>  <a href="/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetModel">VitDetModel</a> (VitDet model)</li> <li><strong>vits</strong>  <a href="/docs/transformers/main/en/model_doc/vits#transformers.VitsModel">VitsModel</a> (VITS model)</li> <li><strong>vivit</strong>  <a href="/docs/transformers/main/en/model_doc/vivit#transformers.VivitModel">VivitModel</a> (ViViT model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel">Wav2Vec2BertModel</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel">Wav2Vec2ConformerModel</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperModel">WhisperModel</a> (Whisper model)</li> <li><strong>xclip</strong>  <a href="/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPModel">XCLIPModel</a> (X-CLIP model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li> <li><strong>xlm-prophetnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodModel">XmodModel</a> (X-MOD model)</li> <li><strong>yolos</strong>  <a href="/docs/transformers/main/en/model_doc/yolos#transformers.YolosModel">YolosModel</a> (YOLOS model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>',Ek,zh,jJ=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,Ik,Gs,Yv,nm,zv,ue,tm,Uk,Oh,ZJ=`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,Nk,Kh,AJ="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Xk,Gn,am,qk,eu,RJ="Instantiates one of the base model classes of the library from a configuration.",Qk,ou,PJ=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Dk,Vs,Hk,No,sm,Yk,ru,WJ="Instantiate one of the base model classes of the library from a pretrained model.",zk,nu,SJ=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Ok,tu,GJ='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.TFBlipModel">TFBlipModel</a> (BLIP model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong>  <a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.TFConvNextV2Model">TFConvNextV2Model</a> (ConvNeXTV2 model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/main/en/model_doc/cvt#transformers.TFCvtModel">TFCvtModel</a> (CvT model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.TFData2VecVisionModel">TFData2VecVisionModel</a> (Data2VecVision model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/main/en/model_doc/deit#transformers.TFDeiTModel">TFDeiTModel</a> (DeiT model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li> <li><strong>dpr</strong>  <a href="/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li> <li><strong>efficientformer</strong>  <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.TFEfficientFormerModel">TFEfficientFormerModel</a> (EfficientFormer model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/main/en/model_doc/esm#transformers.TFEsmModel">TFEsmModel</a> (ESM model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.TFGPTJModel">TFGPTJModel</a> (GPT-J model)</li> <li><strong>groupvit</strong>  <a href="/docs/transformers/main/en/model_doc/groupvit#transformers.TFGroupViTModel">TFGroupViTModel</a> (GroupViT model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/main/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3Model">TFLayoutLMv3Model</a> (LayoutLMv3 model)</li> <li><strong>led</strong>  <a href="/docs/transformers/main/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/main/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.TFMobileViTModel">TFMobileViTModel</a> (MobileViT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (MT5 model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/main/en/model_doc/opt#transformers.TFOPTModel">TFOPTModel</a> (OPT model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/main/en/model_doc/regnet#transformers.TFRegNetModel">TFRegNetModel</a> (RegNet model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/main/en/model_doc/resnet#transformers.TFResNetModel">TFResNetModel</a> (ResNet model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormModel">TFRobertaPreLayerNormModel</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li> <li><strong>sam</strong>  <a href="/docs/transformers/main/en/model_doc/sam#transformers.TFSamModel">TFSamModel</a> (SAM model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/main/en/model_doc/segformer#transformers.TFSegformerModel">TFSegformerModel</a> (SegFormer model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/main/en/model_doc/swin#transformers.TFSwinModel">TFSwinModel</a> (Swin Transformer model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li> <li><strong>vision-text-dual-encoder</strong>  <a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.TFVisionTextDualEncoderModel">TFVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.TFViTMAEModel">TFViTMAEModel</a> (ViTMAE model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.TFWhisperModel">TFWhisperModel</a> (Whisper model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/main/en/model_doc/xglm#transformers.TFXGLMModel">TFXGLMModel</a> (XGLM model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>',Kk,Js,Ov,im,Kv,_e,lm,eL,au,VJ=`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,oL,su,JJ="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",rL,Vn,dm,nL,iu,EJ="Instantiates one of the base model classes of the library from a configuration.",tL,lu,IJ=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,aL,Es,sL,Xo,mm,iL,du,UJ="Instantiate one of the base model classes of the library from a pretrained model.",lL,mu,NJ=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,dL,cu,XJ='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li> <li><strong>beit</strong>  <a href="/docs/transformers/main/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/main/en/model_doc/bloom#transformers.FlaxBloomModel">FlaxBloomModel</a> (BLOOM model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama#transformers.FlaxLlamaModel">FlaxLlamaModel</a> (LLaMA model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/main/en/model_doc/longt5#transformers.FlaxLongT5Model">FlaxLongT5Model</a> (LongT5 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/main/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/main/en/model_doc/mistral#transformers.FlaxMistralModel">FlaxMistralModel</a> (Mistral model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (MT5 model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/main/en/model_doc/opt#transformers.FlaxOPTModel">FlaxOPTModel</a> (OPT model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/main/en/model_doc/regnet#transformers.FlaxRegNetModel">FlaxRegNetModel</a> (RegNet model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/main/en/model_doc/resnet#transformers.FlaxResNetModel">FlaxResNetModel</a> (ResNet model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormModel">FlaxRobertaPreLayerNormModel</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li> <li><strong>vision-text-dual-encoder</strong>  <a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.FlaxWhisperModel">FlaxWhisperModel</a> (Whisper model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/main/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaModel">FlaxXLMRobertaModel</a> (XLM-RoBERTa model)</li>',mL,Is,e2,cm,o2,fm,qJ="The following auto classes are available for instantiating a model with a pretraining head.",r2,gm,n2,be,pm,cL,fu,QJ=`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,fL,gu,DJ="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",gL,Jn,hm,pL,pu,HJ="Instantiates one of the model classes of the library (with a pretraining head) from a configuration.",hL,hu,YJ=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,uL,Us,_L,P,um,bL,uu,zJ="Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model.",ML,_u,OJ=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,FL,bu,KJ='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForPreTraining">ErnieForPreTraining</a> (ERNIE model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li> <li><strong>flava</strong>  <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaForPreTraining">FlavaForPreTraining</a> (FLAVA model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li> <li><strong>fsmt</strong>  <a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM">GPTBigCodeForCausalLM</a> (GPTBigCode model)</li> <li><strong>gptsan-japanese</strong>  <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li> <li><strong>idefics</strong>  <a href="/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text">IdeficsForVisionText2Text</a> (IDEFICS model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> (LLaVa model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForMaskedLM">LukeForMaskedLM</a> (LUKE model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForMaskedLM">MegaForMaskedLM</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (Megatron-BERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptForCausalLM">MptForCausalLM</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForMaskedLM">MraForMaskedLM</a> (MRA model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForPreTraining">NezhaForPreTraining</a> (Nezha model)</li> <li><strong>nllb-moe</strong>  <a href="/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration">NllbMoeForConditionalGeneration</a> (NLLB-MOE model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li> <li><strong>retribert</strong>  <a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM">RobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForPreTraining">RoCBertForPreTraining</a> (RoCBert model)</li> <li><strong>rwkv</strong>  <a href="/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvForCausalLM">RwkvForCausalLM</a> (RWKV model)</li> <li><strong>splinter</strong>  <a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterForPreTraining">SplinterForPreTraining</a> (Splinter model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li> <li><strong>switch_transformers</strong>  <a href="/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration">SwitchTransformersForConditionalGeneration</a> (SwitchTransformers model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li> <li><strong>tvlt</strong>  <a href="/docs/transformers/main/en/model_doc/tvlt#transformers.TvltForPreTraining">TvltForPreTraining</a> (TVLT model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEForPreTraining">VideoMAEForPreTraining</a> (VideoMAE model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration">VipLlavaForConditionalGeneration</a> (VipLlava model)</li> <li><strong>visual_bert</strong>  <a href="/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBERT model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForPreTraining">Wav2Vec2ConformerForPreTraining</a> (Wav2Vec2-Conformer model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMaskedLM">XmodForMaskedLM</a> (X-MOD model)</li>',TL,Mu,e0=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,CL,Ns,t2,_m,a2,Me,bm,yL,Fu,o0=`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,vL,Tu,r0="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",wL,En,Mm,xL,Cu,n0="Instantiates one of the model classes of the library (with a pretraining head) from a configuration.",kL,yu,t0=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,LL,Xs,$L,qo,Fm,BL,vu,a0="Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model.",jL,wu,s0=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,ZL,xu,i0='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForMaskedLM">TFRobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li> <li><strong>vit_mae</strong>  <a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.TFViTMAEForPreTraining">TFViTMAEForPreTraining</a> (ViTMAE model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>',AL,qs,s2,Tm,i2,Fe,Cm,RL,ku,l0=`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,PL,Lu,d0="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",WL,In,ym,SL,$u,m0="Instantiates one of the model classes of the library (with a pretraining head) from a configuration.",GL,Bu,c0=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,VL,Qs,JL,Qo,vm,EL,ju,f0="Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model.",IL,Zu,g0=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,UL,Au,p0='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/main/en/model_doc/longt5#transformers.FlaxLongT5ForConditionalGeneration">FlaxLongT5ForConditionalGeneration</a> (LongT5 model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (MT5 model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForMaskedLM">FlaxRobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.FlaxWhisperForConditionalGeneration">FlaxWhisperForConditionalGeneration</a> (Whisper model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM">FlaxXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>',NL,Ds,l2,wm,d2,xm,h0="The following auto classes are available for the following natural language processing tasks.",m2,km,c2,Te,Lm,XL,Ru,u0=`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,qL,Pu,_0="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",QL,Un,$m,DL,Wu,b0="Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration.",HL,Su,M0=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,YL,Hs,zL,W,Bm,OL,Gu,F0="Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.",KL,Vu,T0=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,e1,Ju,C0='<li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li> <li><strong>bert-generation</strong>  <a href="/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForCausalLM">BioGptForCausalLM</a> (BioGpt model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li> <li><strong>code_llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForCausalLM">LlamaForCausalLM</a> (CodeLlama model)</li> <li><strong>codegen</strong>  <a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenForCausalLM">CodeGenForCausalLM</a> (CodeGen model)</li> <li><strong>cpmant</strong>  <a href="/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntForCausalLM">CpmAntForCausalLM</a> (CPM-Ant model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForCausalLM">ErnieForCausalLM</a> (ERNIE model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconForCausalLM">FalconForCausalLM</a> (Falcon model)</li> <li><strong>fuyu</strong>  <a href="/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM">FuyuForCausalLM</a> (Fuyu model)</li> <li><strong>git</strong>  <a href="/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a> (GIT model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM">GPTBigCodeForCausalLM</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForCausalLM">GPTNeoXForCausalLM</a> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM">GPTNeoXJapaneseForCausalLM</a> (GPT NeoX Japanese model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForCausalLM">LlamaForCausalLM</a> (LLaMA model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForCausalLM">MegaForCausalLM</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (Megatron-BERT model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralForCausalLM">MistralForCausalLM</a> (Mistral model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForCausalLM">MixtralForCausalLM</a> (Mixtral model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptForCausalLM">MptForCausalLM</a> (MPT model)</li> <li><strong>musicgen</strong>  <a href="/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenForCausalLM">MusicgenForCausalLM</a> (MusicGen model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForCausalLM">MvpForCausalLM</a> (MVP model)</li> <li><strong>open-llama</strong>  <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaForCausalLM">OpenLlamaForCausalLM</a> (OpenLlama model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTForCausalLM">OPTForCausalLM</a> (OPT model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForCausalLM">PersimmonForCausalLM</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiForCausalLM">PhiForCausalLM</a> (Phi model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li> <li><strong>prophetnet</strong>  <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForCausalLM">Qwen2ForCausalLM</a> (Qwen2 model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForCausalLM">RobertaPreLayerNormForCausalLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForCausalLM">RoCBertForCausalLM</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li> <li><strong>rwkv</strong>  <a href="/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvForCausalLM">RwkvForCausalLM</a> (RWKV model)</li> <li><strong>speech_to_text_2</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForCausalLM">StableLmForCausalLM</a> (StableLm model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li> <li><strong>trocr</strong>  <a href="/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForCausalLM">WhisperForCausalLM</a> (Whisper model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li> <li><strong>xlm-prophetnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForCausalLM">XmodForCausalLM</a> (X-MOD model)</li>',o1,Eu,y0=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,r1,Ys,f2,jm,g2,Ce,Zm,n1,Iu,v0=`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,t1,Uu,w0="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",a1,Nn,Am,s1,Nu,x0="Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration.",i1,Xu,k0=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,l1,zs,d1,Do,Rm,m1,qu,L0="Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.",c1,Qu,$0=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,f1,Du,B0='<li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForCausalLM">TFCamembertForCausalLM</a> (CamemBERT model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.TFGPTJForCausalLM">TFGPTJForCausalLM</a> (GPT-J model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/main/en/model_doc/opt#transformers.TFOPTForCausalLM">TFOPTForCausalLM</a> (OPT model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForCausalLM">TFRobertaPreLayerNormForCausalLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/main/en/model_doc/xglm#transformers.TFXGLMForCausalLM">TFXGLMForCausalLM</a> (XGLM model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForCausalLM">TFXLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>',g1,Os,p2,Pm,h2,ye,Wm,p1,Hu,j0=`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,h1,Yu,Z0="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",u1,Xn,Sm,_1,zu,A0="Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration.",b1,Ou,R0=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,M1,Ks,F1,Ho,Gm,T1,Ku,P0="Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.",C1,e_,W0=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,y1,o_,S0='<li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForCausalLM">FlaxBartForCausalLM</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForCausalLM">FlaxBertForCausalLM</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForCausalLM">FlaxBigBirdForCausalLM</a> (BigBird model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/main/en/model_doc/bloom#transformers.FlaxBloomForCausalLM">FlaxBloomForCausalLM</a> (BLOOM model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForCausalLM">FlaxElectraForCausalLM</a> (ELECTRA model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama#transformers.FlaxLlamaForCausalLM">FlaxLlamaForCausalLM</a> (LLaMA model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/main/en/model_doc/mistral#transformers.FlaxMistralForCausalLM">FlaxMistralForCausalLM</a> (Mistral model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/main/en/model_doc/opt#transformers.FlaxOPTForCausalLM">FlaxOPTForCausalLM</a> (OPT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForCausalLM">FlaxRobertaForCausalLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForCausalLM">FlaxRobertaPreLayerNormForCausalLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>xglm</strong>  <a href="/docs/transformers/main/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForCausalLM">FlaxXLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>',v1,ei,u2,Vm,_2,ve,Jm,w1,r_,G0=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,x1,n_,V0="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",k1,qn,Em,L1,t_,J0="Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration.",$1,a_,E0=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,B1,oi,j1,S,Im,Z1,s_,I0="Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model.",A1,i_,U0=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,R1,l_,N0='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForMaskedLM">ErnieForMaskedLM</a> (ERNIE model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmForMaskedLM">EsmForMaskedLM</a> (ESM model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForMaskedLM">LukeForMaskedLM</a> (LUKE model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForMaskedLM">MegaForMaskedLM</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (Megatron-BERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForMaskedLM">MraForMaskedLM</a> (MRA model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForMaskedLM">NezhaForMaskedLM</a> (Nezha model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystrmformer model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM">RobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForMaskedLM">RoCBertForMaskedLM</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li> <li><strong>wav2vec2</strong>  <code>Wav2Vec2ForMaskedLM</code> (Wav2Vec2 model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMaskedLM">XmodForMaskedLM</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>',P1,d_,X0=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,W1,ri,b2,Um,M2,we,Nm,S1,m_,q0=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,G1,c_,Q0="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",V1,Qn,Xm,J1,f_,D0="Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration.",E1,g_,H0=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,I1,ni,U1,Yo,qm,N1,p_,Y0="Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model.",X1,h_,z0=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,q1,u_,O0='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/main/en/model_doc/esm#transformers.TFEsmForMaskedLM">TFEsmForMaskedLM</a> (ESM model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForMaskedLM">TFRobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>',Q1,ti,F2,Qm,T2,xe,Dm,D1,__,K0=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,H1,b_,eE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Y1,Dn,Hm,z1,M_,oE="Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration.",O1,F_,rE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,K1,ai,e$,zo,Ym,o$,T_,nE="Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model.",r$,C_,tE=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,n$,y_,aE='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForMaskedLM">FlaxRobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM">FlaxXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>',t$,si,C2,zm,y2,Om,Km,v2,ec,w2,oc,rc,x2,nc,k2,ke,tc,a$,v_,sE=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,s$,w_,iE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",i$,Hn,ac,l$,x_,lE="Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration.",d$,k_,dE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,m$,ii,c$,G,sc,f$,L_,mE="Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model.",g$,$_,cE=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,p$,B_,fE='<li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBird-Pegasus model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li> <li><strong>encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li> <li><strong>fsmt</strong>  <a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li> <li><strong>gptsan-japanese</strong>  <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li> <li><strong>led</strong>  <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/main/en/model_doc/longt5#transformers.LongT5ForConditionalGeneration">LongT5ForConditionalGeneration</a> (LongT5 model)</li> <li><strong>m2m_100</strong>  <a href="/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (MT5 model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li> <li><strong>nllb-moe</strong>  <a href="/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration">NllbMoeForConditionalGeneration</a> (NLLB-MOE model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li> <li><strong>pegasus_x</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXForConditionalGeneration">PegasusXForConditionalGeneration</a> (PEGASUS-X model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li> <li><strong>prophetnet</strong>  <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText">SeamlessM4TForTextToText</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForTextToText">SeamlessM4Tv2ForTextToText</a> (SeamlessM4Tv2 model)</li> <li><strong>switch_transformers</strong>  <a href="/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration">SwitchTransformersForConditionalGeneration</a> (SwitchTransformers model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForConditionalGeneration">UMT5ForConditionalGeneration</a> (UMT5 model)</li> <li><strong>xlm-prophetnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLM-ProphetNet model)</li>',h$,j_,gE=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,u$,li,L2,ic,$2,Le,lc,_$,Z_,pE=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,b$,A_,hE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",M$,Yn,dc,F$,R_,uE="Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration.",T$,P_,_E=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,C$,di,y$,Oo,mc,v$,W_,bE="Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model.",w$,S_,ME=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,x$,G_,FE='<li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li> <li><strong>encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li> <li><strong>led</strong>  <a href="/docs/transformers/main/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/main/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (MT5 model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>',k$,mi,B2,cc,j2,$e,fc,L$,V_,TE=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,$$,J_,CE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",B$,zn,gc,j$,E_,yE="Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration.",Z$,I_,vE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,A$,ci,R$,Ko,pc,P$,U_,wE="Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model.",W$,N_,xE=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,S$,X_,kE='<li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li> <li><strong>blenderbot</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong>  <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li> <li><strong>encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li> <li><strong>longt5</strong>  <a href="/docs/transformers/main/en/model_doc/longt5#transformers.FlaxLongT5ForConditionalGeneration">FlaxLongT5ForConditionalGeneration</a> (LongT5 model)</li> <li><strong>marian</strong>  <a href="/docs/transformers/main/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (MT5 model)</li> <li><strong>pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>',G$,fi,Z2,hc,A2,Be,uc,V$,q_,LE=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,J$,Q_,$E="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",E$,On,_c,I$,D_,BE="Instantiates one of the model classes of the library (with a sequence classification head) from a configuration.",U$,H_,jE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,N$,gi,X$,V,bc,q$,Y_,ZE="Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.",Q$,z_,AE=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,D$,O_,RE='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForSequenceClassification">BioGptForSequenceClassification</a> (BioGpt model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomForSequenceClassification">BloomForSequenceClassification</a> (BLOOM model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (CANINE model)</li> <li><strong>code_llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForSequenceClassification">LlamaForSequenceClassification</a> (CodeLlama model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForSequenceClassification">ErnieForSequenceClassification</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMForSequenceClassification">ErnieMForSequenceClassification</a> (ErnieM model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmForSequenceClassification">EsmForSequenceClassification</a> (ESM model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconForSequenceClassification">FalconForSequenceClassification</a> (Falcon model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForSequenceClassification">GPTBigCodeForSequenceClassification</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForSequenceClassification">GPTNeoXForSequenceClassification</a> (GPT NeoX model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForSequenceClassification">LayoutLMv3ForSequenceClassification</a> (LayoutLMv3 model)</li> <li><strong>led</strong>  <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltForSequenceClassification">LiltForSequenceClassification</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForSequenceClassification">LlamaForSequenceClassification</a> (LLaMA model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForSequenceClassification">LukeForSequenceClassification</a> (LUKE model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForSequenceClassification">MarkupLMForSequenceClassification</a> (MarkupLM model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForSequenceClassification">MegaForSequenceClassification</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (Megatron-BERT model)</li> <li><strong>mistral</strong>  <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralForSequenceClassification">MistralForSequenceClassification</a> (Mistral model)</li> <li><strong>mixtral</strong>  <a href="/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForSequenceClassification">MixtralForSequenceClassification</a> (Mixtral model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptForSequenceClassification">MptForSequenceClassification</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForSequenceClassification">MraForSequenceClassification</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForSequenceClassification">MT5ForSequenceClassification</a> (MT5 model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForSequenceClassification">MvpForSequenceClassification</a> (MVP model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForSequenceClassification">NezhaForSequenceClassification</a> (Nezha model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystrmformer model)</li> <li><strong>open-llama</strong>  <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaForSequenceClassification">OpenLlamaForSequenceClassification</a> (OpenLlama model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTForSequenceClassification">OPTForSequenceClassification</a> (OPT model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li> <li><strong>persimmon</strong>  <a href="/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification">PersimmonForSequenceClassification</a> (Persimmon model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiForSequenceClassification">PhiForSequenceClassification</a> (Phi model)</li> <li><strong>plbart</strong>  <a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li> <li><strong>qwen2</strong>  <a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification">Qwen2ForSequenceClassification</a> (Qwen2 model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForSequenceClassification">RobertaPreLayerNormForSequenceClassification</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForSequenceClassification">RoCBertForSequenceClassification</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li> <li><strong>stablelm</strong>  <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForSequenceClassification">StableLmForSequenceClassification</a> (StableLm model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5ForSequenceClassification">T5ForSequenceClassification</a> (T5 model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForSequenceClassification">UMT5ForSequenceClassification</a> (UMT5 model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForSequenceClassification">XmodForSequenceClassification</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>',H$,K_,PE=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,Y$,pi,R2,Mc,P2,je,Fc,z$,eb,WE=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,O$,ob,SE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",K$,Kn,Tc,eB,rb,GE="Instantiates one of the model classes of the library (with a sequence classification head) from a configuration.",oB,nb,VE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,rB,hi,nB,er,Cc,tB,tb,JE="Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.",aB,ab,EE=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,sB,sb,IE='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.TFBartForSequenceClassification">TFBartForSequenceClassification</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li> <li><strong>ctrl</strong>  <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/main/en/model_doc/esm#transformers.TFEsmForSequenceClassification">TFEsmForSequenceClassification</a> (ESM model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.TFGPTJForSequenceClassification">TFGPTJForSequenceClassification</a> (GPT-J model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForSequenceClassification">TFLayoutLMv3ForSequenceClassification</a> (LayoutLMv3 model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li> <li><strong>openai-gpt</strong>  <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForSequenceClassification">TFRobertaPreLayerNormForSequenceClassification</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li> <li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li> <li><strong>transfo-xl</strong>  <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>',iB,ui,W2,yc,S2,Ze,vc,lB,ib,UE=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,dB,lb,NE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",mB,et,wc,cB,db,XE="Instantiates one of the model classes of the library (with a sequence classification head) from a configuration.",fB,mb,qE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,gB,_i,pB,or,xc,hB,cb,QE="Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.",uB,fb,DE=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,_B,gb,HE='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForSequenceClassification">FlaxRobertaPreLayerNormForSequenceClassification</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForSequenceClassification">FlaxXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>',bB,bi,G2,kc,V2,Ae,Lc,MB,pb,YE=`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,FB,hb,zE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",TB,ot,$c,CB,ub,OE="Instantiates one of the model classes of the library (with a multiple choice head) from a configuration.",yB,_b,KE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,vB,Mi,wB,J,Bc,xB,bb,eI="Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model.",kB,Mb,oI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,LB,Fb,rI='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (CANINE model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMultipleChoice">DebertaV2ForMultipleChoice</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForMultipleChoice">ErnieForMultipleChoice</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMForMultipleChoice">ErnieMForMultipleChoice</a> (ErnieM model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForMultipleChoice">LukeForMultipleChoice</a> (LUKE model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForMultipleChoice">MegaForMultipleChoice</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (Megatron-BERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForMultipleChoice">MraForMultipleChoice</a> (MRA model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForMultipleChoice">NezhaForMultipleChoice</a> (Nezha model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystrmformer model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMultipleChoice">RobertaPreLayerNormForMultipleChoice</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForMultipleChoice">RoCBertForMultipleChoice</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMultipleChoice">XmodForMultipleChoice</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>',$B,Tb,nI=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,BB,Fi,J2,jc,E2,Re,Zc,jB,Cb,tI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,ZB,yb,aI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",AB,rt,Ac,RB,vb,sI="Instantiates one of the model classes of the library (with a multiple choice head) from a configuration.",PB,wb,iI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,WB,Ti,SB,rr,Rc,GB,xb,lI="Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model.",VB,kb,dI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,JB,Lb,mI='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMultipleChoice">TFDebertaV2ForMultipleChoice</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForMultipleChoice">TFRobertaPreLayerNormForMultipleChoice</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>',EB,Ci,I2,Pc,U2,Pe,Wc,IB,$b,cI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,UB,Bb,fI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",NB,nt,Sc,XB,jb,gI="Instantiates one of the model classes of the library (with a multiple choice head) from a configuration.",qB,Zb,pI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,QB,yi,DB,nr,Gc,HB,Ab,hI="Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model.",YB,Rb,uI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,zB,Pb,_I='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForMultipleChoice">FlaxRobertaPreLayerNormForMultipleChoice</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMultipleChoice">FlaxXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>',OB,vi,N2,Vc,X2,We,Jc,KB,Wb,bI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,ej,Sb,MI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",oj,tt,Ec,rj,Gb,FI="Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration.",nj,Vb,TI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,tj,wi,aj,E,Ic,sj,Jb,CI="Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model.",ij,Eb,yI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,lj,Ib,vI='<li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForNextSentencePrediction">ErnieForNextSentencePrediction</a> (ERNIE model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (Megatron-BERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForNextSentencePrediction">NezhaForNextSentencePrediction</a> (Nezha model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>',dj,Ub,wI=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,mj,xi,q2,Uc,Q2,Se,Nc,cj,Nb,xI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,fj,Xb,kI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",gj,at,Xc,pj,qb,LI="Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration.",hj,Qb,$I=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,uj,ki,_j,tr,qc,bj,Db,BI="Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model.",Mj,Hb,jI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Fj,Yb,ZI='<li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForNextSentencePrediction">TFBertForNextSentencePrediction</a> (BERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForNextSentencePrediction">TFMobileBertForNextSentencePrediction</a> (MobileBERT model)</li>',Tj,Li,D2,Qc,H2,Ge,Dc,Cj,zb,AI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,yj,Ob,RI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",vj,st,Hc,wj,Kb,PI="Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration.",xj,eM,WI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,kj,$i,Lj,ar,Yc,$j,oM,SI="Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model.",Bj,rM,GI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,jj,nM,VI='<li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>',Zj,Bi,Y2,zc,z2,Ve,Oc,Aj,tM,JI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,Rj,aM,EI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Pj,it,Kc,Wj,sM,II="Instantiates one of the model classes of the library (with a token classification head) from a configuration.",Sj,iM,UI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Gj,ji,Vj,I,ef,Jj,lM,NI="Instantiate one of the model classes of the library (with a token classification head) from a pretrained model.",Ej,dM,XI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Ij,mM,qI='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li> <li><strong>biogpt</strong>  <a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForTokenClassification">BioGptForTokenClassification</a> (BioGpt model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomForTokenClassification">BloomForTokenClassification</a> (BLOOM model)</li> <li><strong>bros</strong>  <a href="/docs/transformers/main/en/model_doc/bros#transformers.BrosForTokenClassification">BrosForTokenClassification</a> (BROS model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (CANINE model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForTokenClassification">ErnieForTokenClassification</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMForTokenClassification">ErnieMForTokenClassification</a> (ErnieM model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmForTokenClassification">EsmForTokenClassification</a> (ESM model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconForTokenClassification">FalconForTokenClassification</a> (Falcon model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (GPT-Sw3 model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForTokenClassification">GPTBigCodeForTokenClassification</a> (GPTBigCode model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForTokenClassification">GPTNeoForTokenClassification</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForTokenClassification">GPTNeoXForTokenClassification</a> (GPT NeoX model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification">LayoutLMv3ForTokenClassification</a> (LayoutLMv3 model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltForTokenClassification">LiltForTokenClassification</a> (LiLT model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForTokenClassification">LukeForTokenClassification</a> (LUKE model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForTokenClassification">MarkupLMForTokenClassification</a> (MarkupLM model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForTokenClassification">MegaForTokenClassification</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (Megatron-BERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptForTokenClassification">MptForTokenClassification</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForTokenClassification">MraForTokenClassification</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForTokenClassification">MT5ForTokenClassification</a> (MT5 model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForTokenClassification">NezhaForTokenClassification</a> (Nezha model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystrmformer model)</li> <li><strong>phi</strong>  <a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiForTokenClassification">PhiForTokenClassification</a> (Phi model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForTokenClassification">RobertaPreLayerNormForTokenClassification</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForTokenClassification">RoCBertForTokenClassification</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5ForTokenClassification">T5ForTokenClassification</a> (T5 model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForTokenClassification">UMT5ForTokenClassification</a> (UMT5 model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForTokenClassification">XmodForTokenClassification</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>',Uj,cM,QI=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,Nj,Zi,O2,of,K2,Je,rf,Xj,fM,DI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,qj,gM,HI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Qj,lt,nf,Dj,pM,YI="Instantiates one of the model classes of the library (with a token classification head) from a configuration.",Hj,hM,zI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Yj,Ai,zj,sr,tf,Oj,uM,OI="Instantiate one of the model classes of the library (with a token classification head) from a pretrained model.",Kj,_M,KI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,eZ,bM,eU='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li> <li><strong>esm</strong>  <a href="/docs/transformers/main/en/model_doc/esm#transformers.TFEsmForTokenClassification">TFEsmForTokenClassification</a> (ESM model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li> <li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForTokenClassification">TFLayoutLMv3ForTokenClassification</a> (LayoutLMv3 model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForTokenClassification">TFRobertaPreLayerNormForTokenClassification</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>',oZ,Ri,ew,af,ow,Ee,sf,rZ,MM,oU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,nZ,FM,rU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",tZ,dt,lf,aZ,TM,nU="Instantiates one of the model classes of the library (with a token classification head) from a configuration.",sZ,CM,tU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,iZ,Pi,lZ,ir,df,dZ,yM,aU="Instantiate one of the model classes of the library (with a token classification head) from a pretrained model.",mZ,vM,sU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,cZ,wM,iU='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForTokenClassification">FlaxRobertaPreLayerNormForTokenClassification</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForTokenClassification">FlaxXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>',fZ,Wi,rw,mf,nw,Ie,cf,gZ,xM,lU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,pZ,kM,dU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",hZ,mt,ff,uZ,LM,mU="Instantiates one of the model classes of the library (with a question answering head) from a configuration.",_Z,$M,cU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,bZ,Si,MZ,U,gf,FZ,BM,fU="Instantiate one of the model classes of the library (with a question answering head) from a pretrained model.",TZ,jM,gU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,CZ,ZM,pU='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong>  <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBird-Pegasus model)</li> <li><strong>bloom</strong>  <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomForQuestionAnswering">BloomForQuestionAnswering</a> (BLOOM model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li> <li><strong>canine</strong>  <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (CANINE model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li> <li><strong>ernie</strong>  <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForQuestionAnswering">ErnieForQuestionAnswering</a> (ERNIE model)</li> <li><strong>ernie_m</strong>  <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMForQuestionAnswering">ErnieMForQuestionAnswering</a> (ErnieM model)</li> <li><strong>falcon</strong>  <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconForQuestionAnswering">FalconForQuestionAnswering</a> (Falcon model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li> <li><strong>fnet</strong>  <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li> <li><strong>gpt2</strong>  <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForQuestionAnswering">GPT2ForQuestionAnswering</a> (OpenAI GPT-2 model)</li> <li><strong>gpt_neo</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForQuestionAnswering">GPTNeoForQuestionAnswering</a> (GPT Neo model)</li> <li><strong>gpt_neox</strong>  <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForQuestionAnswering">GPTNeoXForQuestionAnswering</a> (GPT NeoX model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li> <li><strong>ibert</strong>  <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering">LayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li> <li><strong>led</strong>  <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li> <li><strong>lilt</strong>  <a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltForQuestionAnswering">LiltForQuestionAnswering</a> (LiLT model)</li> <li><strong>llama</strong>  <a href="/docs/transformers/main/en/model_doc/llama#transformers.LlamaForQuestionAnswering">LlamaForQuestionAnswering</a> (LLaMA model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li> <li><strong>luke</strong>  <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForQuestionAnswering">LukeForQuestionAnswering</a> (LUKE model)</li> <li><strong>lxmert</strong>  <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li> <li><strong>markuplm</strong>  <a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForQuestionAnswering">MarkupLMForQuestionAnswering</a> (MarkupLM model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li> <li><strong>mega</strong>  <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForQuestionAnswering">MegaForQuestionAnswering</a> (MEGA model)</li> <li><strong>megatron-bert</strong>  <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (Megatron-BERT model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li> <li><strong>mpt</strong>  <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptForQuestionAnswering">MptForQuestionAnswering</a> (MPT model)</li> <li><strong>mra</strong>  <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForQuestionAnswering">MraForQuestionAnswering</a> (MRA model)</li> <li><strong>mt5</strong>  <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForQuestionAnswering">MT5ForQuestionAnswering</a> (MT5 model)</li> <li><strong>mvp</strong>  <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForQuestionAnswering">MvpForQuestionAnswering</a> (MVP model)</li> <li><strong>nezha</strong>  <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForQuestionAnswering">NezhaForQuestionAnswering</a> (Nezha model)</li> <li><strong>nystromformer</strong>  <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystrmformer model)</li> <li><strong>opt</strong>  <a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTForQuestionAnswering">OPTForQuestionAnswering</a> (OPT model)</li> <li><strong>qdqbert</strong>  <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li> <li><strong>reformer</strong>  <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForQuestionAnswering">RobertaPreLayerNormForQuestionAnswering</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong>  <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForQuestionAnswering">RoCBertForQuestionAnswering</a> (RoCBert model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li> <li><strong>splinter</strong>  <a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li> <li><strong>squeezebert</strong>  <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li> <li><strong>t5</strong>  <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5ForQuestionAnswering">T5ForQuestionAnswering</a> (T5 model)</li> <li><strong>umt5</strong>  <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForQuestionAnswering">UMT5ForQuestionAnswering</a> (UMT5 model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li> <li><strong>xmod</strong>  <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForQuestionAnswering">XmodForQuestionAnswering</a> (X-MOD model)</li> <li><strong>yoso</strong>  <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>',yZ,AM,hU=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,vZ,Gi,tw,pf,aw,Ue,hf,wZ,RM,uU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,xZ,PM,_U="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",kZ,ct,uf,LZ,WM,bU="Instantiates one of the model classes of the library (with a question answering head) from a configuration.",$Z,SM,MU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,BZ,Vi,jZ,lr,_f,ZZ,GM,FU="Instantiate one of the model classes of the library (with a question answering head) from a pretrained model.",AZ,VM,TU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,RZ,JM,CU='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li> <li><strong>camembert</strong>  <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li> <li><strong>convbert</strong>  <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li> <li><strong>deberta</strong>  <a href="/docs/transformers/main/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong>  <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li> <li><strong>flaubert</strong>  <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li> <li><strong>funnel</strong>  <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li> <li><strong>gptj</strong>  <a href="/docs/transformers/main/en/model_doc/gptj#transformers.TFGPTJForQuestionAnswering">TFGPTJForQuestionAnswering</a> (GPT-J model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForQuestionAnswering">TFLayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li> <li><strong>longformer</strong>  <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li> <li><strong>mobilebert</strong>  <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li> <li><strong>mpnet</strong>  <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li> <li><strong>rembert</strong>  <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForQuestionAnswering">TFRobertaPreLayerNormForQuestionAnswering</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li> <li><strong>xlm</strong>  <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong>  <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>',PZ,Ji,sw,bf,iw,Ne,Mf,WZ,EM,yU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,SZ,IM,vU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",GZ,ft,Ff,VZ,UM,wU="Instantiates one of the model classes of the library (with a question answering head) from a configuration.",JZ,NM,xU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,EZ,Ei,IZ,dr,Tf,UZ,XM,kU="Instantiate one of the model classes of the library (with a question answering head) from a pretrained model.",NZ,qM,LU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,XZ,QM,$U='<li><strong>albert</strong>  <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li> <li><strong>bart</strong>  <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li> <li><strong>bert</strong>  <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li> <li><strong>big_bird</strong>  <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li> <li><strong>distilbert</strong>  <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li> <li><strong>electra</strong>  <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li> <li><strong>mbart</strong>  <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li> <li><strong>roberta</strong>  <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong>  <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForQuestionAnswering">FlaxRobertaPreLayerNormForQuestionAnswering</a> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong>  <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li> <li><strong>xlm-roberta</strong>  <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForQuestionAnswering">FlaxXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>',qZ,Ii,lw,Cf,dw,yf,vf,mw,wf,cw,xf,kf,fw,Lf,gw,$f,BU="The following auto classes are available for the following computer vision tasks.",pw,Bf,hw,Xe,jf,QZ,DM,jU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a depth estimation head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,DZ,HM,ZU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",HZ,gt,Zf,YZ,YM,AU="Instantiates one of the model classes of the library (with a depth estimation head) from a configuration.",zZ,zM,RU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,OZ,Ui,KZ,N,Af,eA,OM,PU="Instantiate one of the model classes of the library (with a depth estimation head) from a pretrained model.",oA,KM,WU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,rA,eF,SU='<li><strong>depth_anything</strong>  <a href="/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingForDepthEstimation">DepthAnythingForDepthEstimation</a> (Depth Anything model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTForDepthEstimation">DPTForDepthEstimation</a> (DPT model)</li> <li><strong>glpn</strong>  <a href="/docs/transformers/main/en/model_doc/glpn#transformers.GLPNForDepthEstimation">GLPNForDepthEstimation</a> (GLPN model)</li>',nA,oF,GU=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,tA,Ni,uw,Rf,_w,qe,Pf,aA,rF,VU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,sA,nF,JU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",iA,pt,Wf,lA,tF,EU="Instantiates one of the model classes of the library (with a image classification head) from a configuration.",dA,aF,IU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,mA,Xi,cA,X,Sf,fA,sF,UU="Instantiate one of the model classes of the library (with a image classification head) from a pretrained model.",gA,iF,NU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,pA,lF,XU='<li><strong>beit</strong>  <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li> <li><strong>bit</strong>  <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitForImageClassification">BitForImageClassification</a> (BiT model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPForImageClassification">CLIPForImageClassification</a> (CLIP model)</li> <li><strong>convnext</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong>  <a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification">ConvNextV2ForImageClassification</a> (ConvNeXTV2 model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/main/en/model_doc/cvt#transformers.CvtForImageClassification">CvtForImageClassification</a> (CvT model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification">Data2VecVisionForImageClassification</a> (Data2VecVision model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li> <li><strong>dinat</strong>  <a href="/docs/transformers/main/en/model_doc/dinat#transformers.DinatForImageClassification">DinatForImageClassification</a> (DiNAT model)</li> <li><strong>dinov2</strong>  <a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2ForImageClassification">Dinov2ForImageClassification</a> (DINOv2 model)</li> <li><strong>efficientformer</strong>  <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassification">EfficientFormerForImageClassification</a> or <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassificationWithTeacher">EfficientFormerForImageClassificationWithTeacher</a> (EfficientFormer model)</li> <li><strong>efficientnet</strong>  <a href="/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetForImageClassification">EfficientNetForImageClassification</a> (EfficientNet model)</li> <li><strong>focalnet</strong>  <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForImageClassification">FocalNetForImageClassification</a> (FocalNet model)</li> <li><strong>imagegpt</strong>  <a href="/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li> <li><strong>levit</strong>  <a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitForImageClassification">LevitForImageClassification</a> or <a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher">LevitForImageClassificationWithTeacher</a> (LeViT model)</li> <li><strong>mobilenet_v1</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1ForImageClassification">MobileNetV1ForImageClassification</a> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification">MobileNetV2ForImageClassification</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTForImageClassification">MobileViTForImageClassification</a> (MobileViT model)</li> <li><strong>mobilevitv2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForImageClassification">MobileViTV2ForImageClassification</a> (MobileViTV2 model)</li> <li><strong>nat</strong>  <a href="/docs/transformers/main/en/model_doc/nat#transformers.NatForImageClassification">NatForImageClassification</a> (NAT model)</li> <li><strong>perceiver</strong>  <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li> <li><strong>poolformer</strong>  <a href="/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li> <li><strong>pvt</strong>  <a href="/docs/transformers/main/en/model_doc/pvt#transformers.PvtForImageClassification">PvtForImageClassification</a> (PVT model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetForImageClassification">RegNetForImageClassification</a> (RegNet model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetForImageClassification">ResNetForImageClassification</a> (ResNet model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipForImageClassification">SiglipForImageClassification</a> (SigLIP model)</li> <li><strong>swiftformer</strong>  <a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerForImageClassification">SwiftFormerForImageClassification</a> (SwiftFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin Transformer model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForImageClassification">Swinv2ForImageClassification</a> (Swin Transformer V2 model)</li> <li><strong>van</strong>  <a href="/docs/transformers/main/en/model_doc/van#transformers.VanForImageClassification">VanForImageClassification</a> (VAN model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li> <li><strong>vit_hybrid</strong>  <a href="/docs/transformers/main/en/model_doc/vit_hybrid#transformers.ViTHybridForImageClassification">ViTHybridForImageClassification</a> (ViT Hybrid model)</li> <li><strong>vit_msn</strong>  <a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification">ViTMSNForImageClassification</a> (ViTMSN model)</li>',hA,dF,qU=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,uA,qi,bw,Gf,Mw,Qe,Vf,_A,mF,QU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,bA,cF,DU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",MA,ht,Jf,FA,fF,HU="Instantiates one of the model classes of the library (with a image classification head) from a configuration.",TA,gF,YU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,CA,Qi,yA,mr,Ef,vA,pF,zU="Instantiate one of the model classes of the library (with a image classification head) from a pretrained model.",wA,hF,OU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,xA,uF,KU='<li><strong>convnext</strong>  <a href="/docs/transformers/main/en/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong>  <a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.TFConvNextV2ForImageClassification">TFConvNextV2ForImageClassification</a> (ConvNeXTV2 model)</li> <li><strong>cvt</strong>  <a href="/docs/transformers/main/en/model_doc/cvt#transformers.TFCvtForImageClassification">TFCvtForImageClassification</a> (CvT model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.TFData2VecVisionForImageClassification">TFData2VecVisionForImageClassification</a> (Data2VecVision model)</li> <li><strong>deit</strong>  <a href="/docs/transformers/main/en/model_doc/deit#transformers.TFDeiTForImageClassification">TFDeiTForImageClassification</a> or <a href="/docs/transformers/main/en/model_doc/deit#transformers.TFDeiTForImageClassificationWithTeacher">TFDeiTForImageClassificationWithTeacher</a> (DeiT model)</li> <li><strong>efficientformer</strong>  <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.TFEfficientFormerForImageClassification">TFEfficientFormerForImageClassification</a> or <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.TFEfficientFormerForImageClassificationWithTeacher">TFEfficientFormerForImageClassificationWithTeacher</a> (EfficientFormer model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.TFMobileViTForImageClassification">TFMobileViTForImageClassification</a> (MobileViT model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/main/en/model_doc/regnet#transformers.TFRegNetForImageClassification">TFRegNetForImageClassification</a> (RegNet model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/main/en/model_doc/resnet#transformers.TFResNetForImageClassification">TFResNetForImageClassification</a> (ResNet model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/main/en/model_doc/segformer#transformers.TFSegformerForImageClassification">TFSegformerForImageClassification</a> (SegFormer model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/main/en/model_doc/swin#transformers.TFSwinForImageClassification">TFSwinForImageClassification</a> (Swin Transformer model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>',kA,Di,Fw,If,Tw,De,Uf,LA,_F,e9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,$A,bF,o9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",BA,ut,Nf,jA,MF,r9="Instantiates one of the model classes of the library (with a image classification head) from a configuration.",ZA,FF,n9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,AA,Hi,RA,cr,Xf,PA,TF,t9="Instantiate one of the model classes of the library (with a image classification head) from a pretrained model.",WA,CF,a9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,SA,yF,s9='<li><strong>beit</strong>  <a href="/docs/transformers/main/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li> <li><strong>regnet</strong>  <a href="/docs/transformers/main/en/model_doc/regnet#transformers.FlaxRegNetForImageClassification">FlaxRegNetForImageClassification</a> (RegNet model)</li> <li><strong>resnet</strong>  <a href="/docs/transformers/main/en/model_doc/resnet#transformers.FlaxResNetForImageClassification">FlaxResNetForImageClassification</a> (ResNet model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>',GA,Yi,Cw,qf,yw,He,Qf,VA,vF,i9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a video classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,JA,wF,l9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",EA,_t,Df,IA,xF,d9="Instantiates one of the model classes of the library (with a video classification head) from a configuration.",UA,kF,m9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,NA,zi,XA,q,Hf,qA,LF,c9="Instantiate one of the model classes of the library (with a video classification head) from a pretrained model.",QA,$F,f9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,DA,BF,g9='<li><strong>timesformer</strong>  <a href="/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerForVideoClassification">TimesformerForVideoClassification</a> (TimeSformer model)</li> <li><strong>videomae</strong>  <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEForVideoClassification">VideoMAEForVideoClassification</a> (VideoMAE model)</li> <li><strong>vivit</strong>  <a href="/docs/transformers/main/en/model_doc/vivit#transformers.VivitForVideoClassification">VivitForVideoClassification</a> (ViViT model)</li>',HA,jF,p9=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,YA,Oi,vw,Yf,ww,Ye,zf,zA,ZF,h9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,OA,AF,u9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",KA,bt,Of,eR,RF,_9="Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration.",oR,PF,b9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,rR,Ki,nR,Q,Kf,tR,WF,M9="Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model.",aR,SF,F9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,sR,GF,T9='<li><strong>deit</strong>  <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li> <li><strong>focalnet</strong>  <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForMaskedImageModeling">FocalNetForMaskedImageModeling</a> (FocalNet model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin Transformer model)</li> <li><strong>swinv2</strong>  <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling">Swinv2ForMaskedImageModeling</a> (Swin Transformer V2 model)</li> <li><strong>vit</strong>  <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>',iR,VF,C9=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,lR,el,xw,eg,kw,ze,og,dR,JF,y9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,mR,EF,v9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",cR,Mt,rg,fR,IF,w9="Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration.",gR,UF,x9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,pR,ol,hR,fr,ng,uR,NF,k9="Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model.",_R,XF,L9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,bR,qF,$9='<li><strong>deit</strong>  <a href="/docs/transformers/main/en/model_doc/deit#transformers.TFDeiTForMaskedImageModeling">TFDeiTForMaskedImageModeling</a> (DeiT model)</li> <li><strong>swin</strong>  <a href="/docs/transformers/main/en/model_doc/swin#transformers.TFSwinForMaskedImageModeling">TFSwinForMaskedImageModeling</a> (Swin Transformer model)</li>',MR,rl,Lw,tg,$w,Oe,ag,FR,QF,B9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,TR,DF,j9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",CR,Ft,sg,yR,HF,Z9="Instantiates one of the model classes of the library (with a object detection head) from a configuration.",vR,YF,A9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,wR,nl,xR,D,ig,kR,zF,R9="Instantiate one of the model classes of the library (with a object detection head) from a pretrained model.",LR,OF,P9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,$R,KF,W9='<li><strong>conditional_detr</strong>  <a href="/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection">ConditionalDetrForObjectDetection</a> (Conditional DETR model)</li> <li><strong>deformable_detr</strong>  <a href="/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrForObjectDetection">DeformableDetrForObjectDetection</a> (Deformable DETR model)</li> <li><strong>deta</strong>  <a href="/docs/transformers/main/en/model_doc/deta#transformers.DetaForObjectDetection">DetaForObjectDetection</a> (DETA model)</li> <li><strong>detr</strong>  <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li> <li><strong>table-transformer</strong>  <a href="/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerForObjectDetection">TableTransformerForObjectDetection</a> (Table Transformer model)</li> <li><strong>yolos</strong>  <a href="/docs/transformers/main/en/model_doc/yolos#transformers.YolosForObjectDetection">YolosForObjectDetection</a> (YOLOS model)</li>',BR,eT,S9=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,jR,tl,Bw,lg,jw,Ke,dg,ZR,oT,G9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,AR,rT,V9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",RR,Tt,mg,PR,nT,J9="Instantiates one of the model classes of the library (with a image segmentation head) from a configuration.",WR,tT,E9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,SR,al,GR,H,cg,VR,aT,I9="Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model.",JR,sT,U9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,ER,iT,N9='<li><strong>detr</strong>  <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>',IR,lT,X9=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,UR,sl,Zw,fg,Aw,gg,pg,Rw,hg,Pw,eo,ug,NR,dT,q9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,XR,mT,Q9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",qR,Ct,_g,QR,cT,D9="Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration.",DR,fT,H9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,HR,il,YR,Y,bg,zR,gT,Y9="Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model.",OR,pT,z9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,KR,hT,O9='<li><strong>beit</strong>  <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li> <li><strong>data2vec-vision</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionForSemanticSegmentation">Data2VecVisionForSemanticSegmentation</a> (Data2VecVision model)</li> <li><strong>dpt</strong>  <a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTForSemanticSegmentation">DPTForSemanticSegmentation</a> (DPT model)</li> <li><strong>mobilenet_v2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation">MobileNetV2ForSemanticSegmentation</a> (MobileNetV2 model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation">MobileViTForSemanticSegmentation</a> (MobileViT model)</li> <li><strong>mobilevitv2</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForSemanticSegmentation">MobileViTV2ForSemanticSegmentation</a> (MobileViTV2 model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li> <li><strong>upernet</strong>  <a href="/docs/transformers/main/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a> (UPerNet model)</li>',eP,uT,K9=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,oP,ll,Ww,Mg,Sw,oo,Fg,rP,_T,eN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,nP,bT,oN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",tP,yt,Tg,aP,MT,rN="Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration.",sP,FT,nN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,iP,dl,lP,gr,Cg,dP,TT,tN="Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model.",mP,CT,aN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,cP,yT,sN='<li><strong>data2vec-vision</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.TFData2VecVisionForSemanticSegmentation">TFData2VecVisionForSemanticSegmentation</a> (Data2VecVision model)</li> <li><strong>mobilevit</strong>  <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.TFMobileViTForSemanticSegmentation">TFMobileViTForSemanticSegmentation</a> (MobileViT model)</li> <li><strong>segformer</strong>  <a href="/docs/transformers/main/en/model_doc/segformer#transformers.TFSegformerForSemanticSegmentation">TFSegformerForSemanticSegmentation</a> (SegFormer model)</li>',fP,ml,Gw,yg,Vw,ro,vg,gP,vT,iN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a instance segmentation head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,pP,wT,lN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",hP,vt,wg,uP,xT,dN="Instantiates one of the model classes of the library (with a instance segmentation head) from a configuration.",_P,kT,mN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,bP,cl,MP,z,xg,FP,LT,cN="Instantiate one of the model classes of the library (with a instance segmentation head) from a pretrained model.",TP,$T,fN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,CP,BT,gN='<li><strong>maskformer</strong>  <a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation">MaskFormerForInstanceSegmentation</a> (MaskFormer model)</li>',yP,jT,pN=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,vP,fl,Jw,kg,Ew,no,Lg,wP,ZT,hN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a universal image segmentation head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,xP,AT,uN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",kP,wt,$g,LP,RT,_N="Instantiates one of the model classes of the library (with a universal image segmentation head) from a configuration.",$P,PT,bN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,BP,gl,jP,O,Bg,ZP,WT,MN="Instantiate one of the model classes of the library (with a universal image segmentation head) from a pretrained model.",AP,ST,FN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,RP,GT,TN='<li><strong>detr</strong>  <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li> <li><strong>mask2former</strong>  <a href="/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerForUniversalSegmentation">Mask2FormerForUniversalSegmentation</a> (Mask2Former model)</li> <li><strong>maskformer</strong>  <a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation">MaskFormerForInstanceSegmentation</a> (MaskFormer model)</li> <li><strong>oneformer</strong>  <a href="/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation">OneFormerForUniversalSegmentation</a> (OneFormer model)</li>',PP,VT,CN=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,WP,pl,Iw,jg,Uw,to,Zg,SP,JT,yN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a zero-shot image classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,GP,ET,vN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",VP,xt,Ag,JP,IT,wN="Instantiates one of the model classes of the library (with a zero-shot image classification head) from a configuration.",EP,UT,xN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,IP,hl,UP,K,Rg,NP,NT,kN="Instantiate one of the model classes of the library (with a zero-shot image classification head) from a pretrained model.",XP,XT,LN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,qP,qT,$N='<li><strong>align</strong>  <a href="/docs/transformers/main/en/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li> <li><strong>altclip</strong>  <a href="/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li> <li><strong>blip</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li> <li><strong>chinese_clip</strong>  <a href="/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li> <li><strong>clipseg</strong>  <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li> <li><strong>siglip</strong>  <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipModel">SiglipModel</a> (SigLIP model)</li>',QP,QT,BN=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,DP,ul,Nw,Pg,Xw,ao,Wg,HP,DT,jN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a zero-shot image classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,YP,HT,ZN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",zP,kt,Sg,OP,YT,AN="Instantiates one of the model classes of the library (with a zero-shot image classification head) from a configuration.",KP,zT,RN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,eW,_l,oW,pr,Gg,rW,OT,PN="Instantiate one of the model classes of the library (with a zero-shot image classification head) from a pretrained model.",nW,KT,WN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,tW,eC,SN='<li><strong>blip</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.TFBlipModel">TFBlipModel</a> (BLIP model)</li> <li><strong>clip</strong>  <a href="/docs/transformers/main/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>',aW,bl,qw,Vg,Qw,so,Jg,sW,oC,GN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a zero-shot object detection head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,iW,rC,VN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",lW,Lt,Eg,dW,nC,JN="Instantiates one of the model classes of the library (with a zero-shot object detection head) from a configuration.",mW,tC,EN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,cW,Ml,fW,ee,Ig,gW,aC,IN="Instantiate one of the model classes of the library (with a zero-shot object detection head) from a pretrained model.",pW,sC,UN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,hW,iC,NN='<li><strong>owlv2</strong>  <a href="/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2ForObjectDetection">Owlv2ForObjectDetection</a> (OWLv2 model)</li> <li><strong>owlvit</strong>  <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection">OwlViTForObjectDetection</a> (OWL-ViT model)</li>',uW,lC,XN=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,_W,Fl,Dw,Ug,Hw,Ng,qN="The following auto classes are available for the following audio tasks.",Yw,Xg,zw,io,qg,bW,dC,QN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,MW,mC,DN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",FW,$t,Qg,TW,cC,HN="Instantiates one of the model classes of the library (with a audio classification head) from a configuration.",CW,fC,YN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,yW,Tl,vW,oe,Dg,wW,gC,zN="Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model.",xW,pC,ON=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,kW,hC,KN='<li><strong>audio-spectrogram-transformer</strong>  <a href="/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification">ASTForAudioClassification</a> (Audio Spectrogram Transformer model)</li> <li><strong>data2vec-audio</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/main/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification">Wav2Vec2BertForSequenceClassification</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForSequenceClassification">Wav2Vec2ConformerForSequenceClassification</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForAudioClassification">WhisperForAudioClassification</a> (Whisper model)</li>',LW,uC,e5=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,$W,Cl,Ow,Hg,Kw,lo,Yg,BW,_C,o5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,jW,bC,r5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",ZW,Bt,zg,AW,MC,n5="Instantiates one of the model classes of the library (with a audio classification head) from a configuration.",RW,FC,t5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,PW,yl,WW,hr,Og,SW,TC,a5="Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model.",GW,CC,s5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,VW,yC,i5='<li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.TFWav2Vec2ForSequenceClassification">TFWav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>',JW,vl,ex,Kg,ox,mo,ep,EW,vC,l5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,IW,wC,d5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",UW,jt,op,NW,xC,m5="Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration.",XW,kC,c5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,qW,wl,QW,re,rp,DW,LC,f5="Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model.",HW,$C,g5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,YW,BC,p5='<li><strong>data2vec-audio</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForAudioFrameClassification">Wav2Vec2BertForAudioFrameClassification</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForAudioFrameClassification">Wav2Vec2ConformerForAudioFrameClassification</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>',zW,jC,h5=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,OW,xl,rx,np,nx,co,tp,KW,ZC,u5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,eS,AC,_5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",oS,Zt,ap,rS,RC,b5="Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration.",nS,PC,M5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,tS,kl,aS,ne,sp,sS,WC,F5="Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model.",iS,SC,T5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,lS,GC,C5='<li><strong>data2vec-audio</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li> <li><strong>hubert</strong>  <a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li> <li><strong>mctct</strong>  <a href="/docs/transformers/main/en/model_doc/mctct#transformers.MCTCTForCTC">MCTCTForCTC</a> (M-CTC-T model)</li> <li><strong>sew</strong>  <a href="/docs/transformers/main/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li> <li><strong>sew-d</strong>  <a href="/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li> <li><strong>unispeech</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC">Wav2Vec2BertForCTC</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC">Wav2Vec2ConformerForCTC</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>',dS,VC,y5=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,mS,Ll,tx,ip,ax,fo,lp,cS,JC,v5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,fS,EC,w5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",gS,At,dp,pS,IC,x5="Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration.",hS,UC,k5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,uS,$l,_S,te,mp,bS,NC,L5="Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model.",MS,XC,$5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,FS,qC,B5='<li><strong>pop2piano</strong>  <a href="/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration">Pop2PianoForConditionalGeneration</a> (Pop2Piano model)</li> <li><strong>seamless_m4t</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText">SeamlessM4TForSpeechToText</a> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong>  <a href="/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForSpeechToText">SeamlessM4Tv2ForSpeechToText</a> (SeamlessM4Tv2 model)</li> <li><strong>speech-encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li> <li><strong>speech_to_text</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li> <li><strong>speecht5</strong>  <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5ForSpeechToText">SpeechT5ForSpeechToText</a> (SpeechT5 model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForConditionalGeneration">WhisperForConditionalGeneration</a> (Whisper model)</li>',TS,QC,j5=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,CS,Bl,sx,cp,ix,go,fp,yS,DC,Z5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,vS,HC,A5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",wS,Rt,gp,xS,YC,R5="Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration.",kS,zC,P5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,LS,jl,$S,ur,pp,BS,OC,W5="Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model.",jS,KC,S5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,ZS,ey,G5='<li><strong>speech_to_text</strong>  <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.TFWhisperForConditionalGeneration">TFWhisperForConditionalGeneration</a> (Whisper model)</li>',AS,Zl,lx,hp,dx,po,up,RS,oy,V5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,PS,ry,J5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",WS,Pt,_p,SS,ny,E5="Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration.",GS,ty,I5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,VS,Al,JS,_r,bp,ES,ay,U5="Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model.",IS,sy,N5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,US,iy,X5='<li><strong>speech-encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.FlaxSpeechEncoderDecoderModel">FlaxSpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li> <li><strong>whisper</strong>  <a href="/docs/transformers/main/en/model_doc/whisper#transformers.FlaxWhisperForConditionalGeneration">FlaxWhisperForConditionalGeneration</a> (Whisper model)</li>',NS,Rl,mx,Mp,cx,ho,Fp,XS,ly,q5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,qS,dy,Q5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",QS,Wt,Tp,DS,my,D5="Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration.",HS,cy,H5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,YS,Pl,zS,ae,Cp,OS,fy,Y5="Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model.",KS,gy,z5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,eG,py,O5='<li><strong>data2vec-audio</strong>  <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li> <li><strong>unispeech-sat</strong>  <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForXVector">Wav2Vec2BertForXVector</a> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong>  <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForXVector">Wav2Vec2ConformerForXVector</a> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong>  <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>',oG,hy,K5=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,rG,Wl,fx,yp,gx,vp,wp,px,xp,hx,kp,Lp,ux,$p,_x,Bp,e4="The following auto classes are available for the following multimodal tasks.",bx,jp,Mx,uo,Zp,nG,uy,o4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,tG,_y,r4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",aG,St,Ap,sG,by,n4="Instantiates one of the model classes of the library (with a table question answering head) from a configuration.",iG,My,t4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,lG,Sl,dG,se,Rp,mG,Fy,a4="Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model.",cG,Ty,s4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,fG,Cy,i4='<li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>',gG,yy,l4=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,pG,Gl,Fx,Pp,Tx,_o,Wp,hG,vy,d4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,uG,wy,m4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",_G,Gt,Sp,bG,xy,c4="Instantiates one of the model classes of the library (with a table question answering head) from a configuration.",MG,ky,f4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,FG,Vl,TG,br,Gp,CG,Ly,g4="Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model.",yG,$y,p4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,vG,By,h4='<li><strong>tapas</strong>  <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>',wG,Jl,Cx,Vp,yx,bo,Jp,xG,jy,u4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a document question answering head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,kG,Zy,_4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",LG,Vt,Ep,$G,Ay,b4="Instantiates one of the model classes of the library (with a document question answering head) from a configuration.",BG,Ry,M4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,jG,El,ZG,ie,Ip,AG,Py,F4="Instantiate one of the model classes of the library (with a document question answering head) from a pretrained model.",RG,Wy,T4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,PG,Sy,C4='<li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForQuestionAnswering">LayoutLMForQuestionAnswering</a> (LayoutLM model)</li> <li><strong>layoutlmv2</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering">LayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li>',WG,Gy,y4=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,SG,Il,vx,Up,wx,Mo,Np,GG,Vy,v4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a document question answering head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,VG,Jy,w4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",JG,Jt,Xp,EG,Ey,x4="Instantiates one of the model classes of the library (with a document question answering head) from a configuration.",IG,Iy,k4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,UG,Ul,NG,Mr,qp,XG,Uy,L4="Instantiate one of the model classes of the library (with a document question answering head) from a pretrained model.",qG,Ny,$4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,QG,Xy,B4='<li><strong>layoutlm</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMForQuestionAnswering">TFLayoutLMForQuestionAnswering</a> (LayoutLM model)</li> <li><strong>layoutlmv3</strong>  <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForQuestionAnswering">TFLayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li>',DG,Nl,xx,Qp,kx,Fo,Dp,HG,qy,j4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a visual question answering head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,YG,Qy,Z4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",zG,Et,Hp,OG,Dy,A4="Instantiates one of the model classes of the library (with a visual question answering head) from a configuration.",KG,Hy,R4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,eV,Xl,oV,le,Yp,rV,Yy,P4="Instantiate one of the model classes of the library (with a visual question answering head) from a pretrained model.",nV,zy,W4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,tV,Oy,S4='<li><strong>blip-2</strong>  <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li> <li><strong>vilt</strong>  <a href="/docs/transformers/main/en/model_doc/vilt#transformers.ViltForQuestionAnswering">ViltForQuestionAnswering</a> (ViLT model)</li>',aV,Ky,G4=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,sV,ql,Lx,zp,$x,To,Op,iV,ev,V4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,lV,ov,J4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",dV,It,Kp,mV,rv,E4="Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration.",cV,nv,I4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,fV,Ql,gV,de,eh,pV,tv,U4="Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model.",hV,av,N4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,uV,sv,X4='<li><strong>blip</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipForConditionalGeneration">BlipForConditionalGeneration</a> (BLIP model)</li> <li><strong>blip-2</strong>  <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li> <li><strong>git</strong>  <a href="/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a> (GIT model)</li> <li><strong>instructblip</strong>  <a href="/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration">InstructBlipForConditionalGeneration</a> (InstructBLIP model)</li> <li><strong>kosmos-2</strong>  <a href="/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration">Kosmos2ForConditionalGeneration</a> (KOSMOS-2 model)</li> <li><strong>llava</strong>  <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> (LLaVa model)</li> <li><strong>pix2struct</strong>  <a href="/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration">Pix2StructForConditionalGeneration</a> (Pix2Struct model)</li> <li><strong>vipllava</strong>  <a href="/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration">VipLlavaForConditionalGeneration</a> (VipLlava model)</li> <li><strong>vision-encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>',_V,iv,q4=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,bV,Dl,Bx,oh,jx,Co,rh,MV,lv,Q4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,FV,dv,D4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",TV,Ut,nh,CV,mv,H4="Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration.",yV,cv,Y4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,vV,Hl,wV,Fr,th,xV,fv,z4="Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model.",kV,gv,O4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,LV,pv,K4='<li><strong>blip</strong>  <a href="/docs/transformers/main/en/model_doc/blip#transformers.TFBlipForConditionalGeneration">TFBlipForConditionalGeneration</a> (BLIP model)</li> <li><strong>vision-encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>',$V,Yl,Zx,ah,Ax,yo,sh,BV,hv,eX=`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,jV,uv,oX="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",ZV,Nt,ih,AV,_v,rX="Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration.",RV,bv,nX=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
models configuration. Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,PV,zl,WV,Tr,lh,SV,Mv,tX="Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model.",GV,Fv,aX=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when its missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,VV,Tv,sX='<li><strong>vision-encoder-decoder</strong>  <a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>',JV,Ol,Rx,Cv,Px;return c=new B({props:{title:"Auto Classes",local:"auto-classes",headingTag:"h1"}}),vd=new L({props:{code:"bW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKQ==",highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)',wrap:!1}}),kd=new B({props:{title:"Extending the Auto Classes",local:"extending-the-auto-classes",headingTag:"h2"}}),$d=new L({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWwlMEElMEFBdXRvQ29uZmlnLnJlZ2lzdGVyKCUyMm5ldy1tb2RlbCUyMiUyQyUyME5ld01vZGVsQ29uZmlnKSUwQUF1dG9Nb2RlbC5yZWdpc3RlcihOZXdNb2RlbENvbmZpZyUyQyUyME5ld01vZGVsKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`,wrap:!1}}),ys=new Wx({props:{warning:!0,$$slots:{default:[hX]},$$scope:{ctx:y}}}),jd=new B({props:{title:"AutoConfig",local:"transformers.AutoConfig",headingTag:"h2"}}),Zd=new x({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/configuration_auto.py#L981"}}),Ad=new x({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/configuration_auto.py#L1004"}}),vs=new $({props:{anchor:"transformers.AutoConfig.from_pretrained.example",$$slots:{default:[uX]},$$scope:{ctx:y}}}),Rd=new x({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/configuration_auto.py#L1143"}}),Pd=new B({props:{title:"AutoTokenizer",local:"transformers.AutoTokenizer",headingTag:"h2"}}),Wd=new x({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/tokenization_auto.py#L620"}}),Sd=new x({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to determine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Use a <a href="https://huggingface.co/docs/tokenizers/index" rel="nofollow">fast Rust-based tokenizer</a> if it is supported for
a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer
is returned instead.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/tokenization_auto.py#L634"}}),xs=new $({props:{anchor:"transformers.AutoTokenizer.from_pretrained.example",$$slots:{default:[_X]},$$scope:{ctx:y}}}),Gd=new x({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.fast_tokenizer_class",description:`<strong>fast_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"fast_tokenizer_class"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/tokenization_auto.py#L851"}}),Vd=new B({props:{title:"AutoFeatureExtractor",local:"transformers.AutoFeatureExtractor",headingTag:"h2"}}),Jd=new x({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/feature_extraction_auto.py#L238"}}),Ed=new x({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/feature_extraction_auto.py#L252"}}),Ls=new Wx({props:{$$slots:{default:[bX]},$$scope:{ctx:y}}}),$s=new $({props:{anchor:"transformers.AutoFeatureExtractor.from_pretrained.example",$$slots:{default:[MX]},$$scope:{ctx:y}}}),Id=new x({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/feature_extraction_auto.py#L386"}}),Ud=new B({props:{title:"AutoImageProcessor",local:"transformers.AutoImageProcessor",headingTag:"h2"}}),Nd=new x({props:{name:"class transformers.AutoImageProcessor",anchor:"transformers.AutoImageProcessor",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/image_processing_auto.py#L251"}}),Xd=new x({props:{name:"from_pretrained",anchor:"transformers.AutoImageProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoImageProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained image_processor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a image processor file saved using the
<a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved image processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoImageProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model image processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoImageProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the image processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.AutoImageProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoImageProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoImageProcessor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.AutoImageProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoImageProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final image processor object. If <code>True</code>, then this
functions returns a <code>Tuple(image_processor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not image processor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>image_processor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoImageProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoImageProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are image processor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> image processor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/image_processing_auto.py#L265"}}),js=new Wx({props:{$$slots:{default:[FX]},$$scope:{ctx:y}}}),Zs=new $({props:{anchor:"transformers.AutoImageProcessor.from_pretrained.example",$$slots:{default:[TX]},$$scope:{ctx:y}}}),qd=new x({props:{name:"register",anchor:"transformers.AutoImageProcessor.register",parameters:[{name:"config_class",val:""},{name:"image_processor_class",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoImageProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoImageProcessor.register.image_processor_class",description:'<strong>image_processor_class</strong> (<a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a>) &#x2014; The image processor to register.',name:"image_processor_class"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/image_processing_auto.py#L421"}}),Qd=new B({props:{title:"AutoProcessor",local:"transformers.AutoProcessor",headingTag:"h2"}}),Dd=new x({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/processing_auto.py#L129"}}),Hd=new x({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/processing_auto.py#L143"}}),Rs=new Wx({props:{$$slots:{default:[CX]},$$scope:{ctx:y}}}),Ps=new $({props:{anchor:"transformers.AutoProcessor.from_pretrained.example",$$slots:{default:[yX]},$$scope:{ctx:y}}}),Yd=new x({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/processing_auto.py#L346"}}),zd=new B({props:{title:"Generic model classes",local:"generic-model-classes",headingTag:"h2"}}),Kd=new B({props:{title:"AutoModel",local:"transformers.AutoModel",headingTag:"h3"}}),em=new x({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1314"}}),om=new x({props:{name:"from_config",anchor:"transformers.AutoModel.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModel.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTModel">ASTModel</a> (Audio Spectrogram Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/align#transformers.AlignConfig">AlignConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li>
<li><a href="/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPConfig">AltCLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/autoformer#transformers.AutoformerConfig">AutoformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/autoformer#transformers.AutoformerModel">AutoformerModel</a> (Autoformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkConfig">BarkConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkModel">BarkModel</a> (Bark model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptModel">BioGptModel</a> (BioGpt model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bit#transformers.BitConfig">BitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitModel">BitModel</a> (BiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> (BLIP-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomModel">BloomModel</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a> (BridgeTower model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bros#transformers.BrosConfig">BrosConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bros#transformers.BrosModel">BrosModel</a> (BROS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPVisionConfig">CLIPVisionConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPVisionModel">CLIPVisionModel</a> (CLIPVisionModel model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineModel">CanineModel</a> (CANINE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig">ChineseCLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clap#transformers.ClapConfig">ClapConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clap#transformers.ClapModel">ClapModel</a> (CLAP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clvp#transformers.ClvpConfig">ClvpConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clvp#transformers.ClvpModelForConditionalGeneration">ClvpModelForConditionalGeneration</a> (CLVP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenConfig">CodeGenConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenModel">CodeGenModel</a> (CodeGen model)</li>
<li><a href="/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig">ConditionalDetrConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrModel">ConditionalDetrModel</a> (Conditional DETR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNeXT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Model">ConvNextV2Model</a> (ConvNeXTV2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntModel">CpmAntModel</a> (CPM-Ant model)</li>
<li><a href="/docs/transformers/main/en/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/cvt#transformers.CvtModel">CvtModel</a> (CvT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRQuestionEncoder">DPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTModel">DPTModel</a> (DPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionModel">Data2VecVisionModel</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/decision_transformer#transformers.DecisionTransformerConfig">DecisionTransformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/decision_transformer#transformers.DecisionTransformerModel">DecisionTransformerModel</a> (Decision Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrConfig">DeformableDetrConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrModel">DeformableDetrModel</a> (Deformable DETR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deta#transformers.DetaConfig">DetaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deta#transformers.DetaModel">DetaModel</a> (DETA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/dinat#transformers.DinatConfig">DinatConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/dinat#transformers.DinatModel">DinatModel</a> (DiNAT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config">Dinov2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Model">Dinov2Model</a> (DINOv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertModel">DistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinConfig">DonutSwinConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/donut#transformers.DonutSwinModel">DonutSwinModel</a> (DonutSwin model)</li>
<li><a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerConfig">EfficientFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerModel">EfficientFormerModel</a> (EfficientFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetConfig">EfficientNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetModel">EfficientNetModel</a> (EfficientNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraModel">ElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/encodec#transformers.EncodecConfig">EncodecConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/encodec#transformers.EncodecModel">EncodecModel</a> (EnCodec model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieModel">ErnieModel</a> (ERNIE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMModel">ErnieMModel</a> (ErnieM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmModel">EsmModel</a> (ESM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetModel">FNetModel</a> (FNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTModel">FSMTModel</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconModel">FalconModel</a> (Falcon model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerConfig">FastSpeech2ConformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerModel">FastSpeech2ConformerModel</a> (FastSpeech2Conformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertModel">FlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel">FlavaModel</a> (FLAVA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetModel">FocalNetModel</a> (FocalNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelModel">FunnelModel</a> or <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelBaseModel">FunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/glpn#transformers.GLPNConfig">GLPNConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/glpn#transformers.GLPNModel">GLPNModel</a> (GLPN model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Model">GPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeModel">GPTBigCodeModel</a> (GPTBigCode model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJModel">GPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoModel">GPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXModel">GPTNeoXModel</a> (GPT NeoX model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig">GPTNeoXJapaneseConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel">GPTNeoXJapaneseModel</a> (GPT NeoX Japanese model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig">GPTSanJapaneseConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li>
<li><a href="/docs/transformers/main/en/model_doc/git#transformers.GitConfig">GitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/git#transformers.GitModel">GitModel</a> (GIT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/graphormer#transformers.GraphormerConfig">GraphormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/graphormer#transformers.GraphormerModel">GraphormerModel</a> (Graphormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTConfig">GroupViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTModel">GroupViTModel</a> (GroupViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertModel">HubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertModel">IBertModel</a> (I-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig">IdeficsConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsModel">IdeficsModel</a> (IDEFICS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTModel">ImageGPTModel</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/informer#transformers.InformerConfig">InformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/informer#transformers.InformerModel">InformerModel</a> (Informer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/jukebox#transformers.JukeboxConfig">JukeboxConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/jukebox#transformers.JukeboxModel">JukeboxModel</a> (Jukebox model)</li>
<li><a href="/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Config">Kosmos2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Model">Kosmos2Model</a> (KOSMOS-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDModel">LEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMModel">LayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Model">LayoutLMv2Model</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Model">LayoutLMv3Model</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitConfig">LevitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitModel">LevitModel</a> (LeViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig">LiltConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltModel">LiltModel</a> (LiLT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaModel">LlamaModel</a> (LLaMA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config">LongT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Model">LongT5Model</a> (LongT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerModel">LongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeModel">LukeModel</a> (LUKE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertModel">LxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Model">M2M100Model</a> (M2M100 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartModel">MBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mctct#transformers.MCTCTConfig">MCTCTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mctct#transformers.MCTCTModel">MCTCTModel</a> (M-CTC-T model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetModel">MPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Model">MT5Model</a> (MT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianModel">MarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig">MarkupLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMModel">MarkupLMModel</a> (MarkupLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerConfig">Mask2FormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerModel">Mask2FormerModel</a> (Mask2Former model)</li>
<li><a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerModel">MaskFormerModel</a> (MaskFormer model)</li>
<li><code>MaskFormerSwinConfig</code> configuration class: <code>MaskFormerSwinModel</code> (MaskFormerSwin model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaModel">MegaModel</a> (MEGA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertModel">MegatronBertModel</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrConfig">MgpstrConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mgp-str#transformers.MgpstrForSceneTextRecognition">MgpstrForSceneTextRecognition</a> (MGP-STR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralModel">MistralModel</a> (Mistral model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralModel">MixtralModel</a> (Mixtral model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertModel">MobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config">MobileNetV1Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Model">MobileNetV1Model</a> (MobileNetV1 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config">MobileNetV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Model">MobileNetV2Model</a> (MobileNetV2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTModel">MobileViTModel</a> (MobileViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Model">MobileViTV2Model</a> (MobileViTV2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptModel">MptModel</a> (MPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraModel">MraModel</a> (MRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpModel">MvpModel</a> (MVP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nat#transformers.NatConfig">NatConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nat#transformers.NatModel">NatModel</a> (NAT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaModel">NezhaModel</a> (Nezha model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig">NllbMoeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeModel">NllbMoeModel</a> (NLLB-MOE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerModel">NystromformerModel</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTModel">OPTModel</a> (OPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerConfig">OneFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerModel">OneFormerModel</a> (OneFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTModel">OpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaModel">OpenLlamaModel</a> (OpenLlama model)</li>
<li><a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTModel">OwlViTModel</a> (OWL-ViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Config">Owlv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Model">Owlv2Model</a> (OWLv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartModel">PLBartModel</a> (PLBart model)</li>
<li><a href="/docs/transformers/main/en/model_doc/patchtsmixer#transformers.PatchTSMixerConfig">PatchTSMixerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/patchtsmixer#transformers.PatchTSMixerModel">PatchTSMixerModel</a> (PatchTSMixer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/patchtst#transformers.PatchTSTConfig">PatchTSTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/patchtst#transformers.PatchTSTModel">PatchTSTModel</a> (PatchTST model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusModel">PegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXConfig">PegasusXConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXModel">PegasusXModel</a> (PEGASUS-X model)</li>
<li><a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverModel">PerceiverModel</a> (Perceiver model)</li>
<li><a href="/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig">PersimmonConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonModel">PersimmonModel</a> (Persimmon model)</li>
<li><a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig">PhiConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiModel">PhiModel</a> (Phi model)</li>
<li><a href="/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerModel">PoolFormerModel</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetModel">ProphetNetModel</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pvt#transformers.PvtConfig">PvtConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pvt#transformers.PvtModel">PvtModel</a> (PVT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertModel">QDQBertModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config">Qwen2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Model">Qwen2Model</a> (Qwen2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerModel">ReformerModel</a> (Reformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig">RegNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetModel">RegNetModel</a> (RegNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertModel">RemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetModel">ResNetModel</a> (ResNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertModel">RoCBertModel</a> (RoCBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerModel">RoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaModel">RobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormModel">RobertaPreLayerNormModel</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig">RwkvConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvModel">RwkvModel</a> (RWKV model)</li>
<li><a href="/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/sew#transformers.SEWModel">SEWModel</a> (SEW model)</li>
<li><a href="/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDModel">SEWDModel</a> (SEW-D model)</li>
<li><a href="/docs/transformers/main/en/model_doc/sam#transformers.SamConfig">SamConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/sam#transformers.SamModel">SamModel</a> (SAM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig">SeamlessM4TConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TModel">SeamlessM4TModel</a> (SeamlessM4T model)</li>
<li><a href="/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config">SeamlessM4Tv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Model">SeamlessM4Tv2Model</a> (SeamlessM4Tv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerModel">SegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig">SiglipConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipModel">SiglipModel</a> (SigLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipVisionConfig">SiglipVisionConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipVisionModel">SiglipVisionModel</a> (SiglipVisionModel model)</li>
<li><a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextModel">Speech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Config">SpeechT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Model">SpeechT5Model</a> (SpeechT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterModel">SplinterModel</a> (Splinter model)</li>
<li><a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertModel">SqueezeBertModel</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmModel">StableLmModel</a> (StableLm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig">SwiftFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerModel">SwiftFormerModel</a> (SwiftFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRConfig">Swin2SRConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swin2sr#transformers.Swin2SRModel">Swin2SRModel</a> (Swin2SR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinModel">SwinModel</a> (Swin Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Model">Swinv2Model</a> (Swin Transformer V2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig">SwitchTransformersConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersModel">SwitchTransformersModel</a> (SwitchTransformers model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Model">T5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerConfig">TableTransformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerModel">TableTransformerModel</a> (Table Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasModel">TapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerConfig">TimeSeriesTransformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/time_series_transformer#transformers.TimeSeriesTransformerModel">TimeSeriesTransformerModel</a> (Time Series Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerConfig">TimesformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerModel">TimesformerModel</a> (TimeSformer model)</li>
<li><a href="/docs/transformers/main/en/main_classes/backbones#transformers.TimmBackboneConfig">TimmBackboneConfig</a> configuration class: <a href="/docs/transformers/main/en/main_classes/backbones#transformers.TimmBackbone">TimmBackbone</a> (TimmBackbone model)</li>
<li><a href="/docs/transformers/main/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerConfig">TrajectoryTransformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/trajectory_transformer#transformers.TrajectoryTransformerModel">TrajectoryTransformerModel</a> (Trajectory Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLModel">TransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tvlt#transformers.TvltConfig">TvltConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tvlt#transformers.TvltModel">TvltModel</a> (TVLT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tvp#transformers.TvpConfig">TvpConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tvp#transformers.TvpModel">TvpModel</a> (TVP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Model">UMT5Model</a> (UMT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechModel">UniSpeechModel</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel">UniSpeechSatModel</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetConfig">UnivNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/univnet#transformers.UnivNetModel">UnivNetModel</a> (UnivNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/van#transformers.VanConfig">VanConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/van#transformers.VanModel">VanModel</a> (VAN model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTModel">ViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit_hybrid#transformers.ViTHybridConfig">ViTHybridConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit_hybrid#transformers.ViTHybridModel">ViTHybridModel</a> (ViT Hybrid model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEModel">ViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig">ViTMSNConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNModel">ViTMSNModel</a> (ViTMSN model)</li>
<li><a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig">VideoMAEConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEModel">VideoMAEModel</a> (VideoMAE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vilt#transformers.ViltModel">ViltModel</a> (ViLT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderModel">VisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertModel">VisualBertModel</a> (VisualBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetConfig">VitDetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetModel">VitDetModel</a> (VitDet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vits#transformers.VitsConfig">VitsConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vits#transformers.VitsModel">VitsModel</a> (VITS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vivit#transformers.VivitConfig">VivitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vivit#transformers.VivitModel">VivitModel</a> (ViViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertModel">Wav2Vec2BertModel</a> (Wav2Vec2-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Model">Wav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerModel">Wav2Vec2ConformerModel</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMModel">WavLMModel</a> (WavLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperModel">WhisperModel</a> (Whisper model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPConfig">XCLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xclip#transformers.XCLIPModel">XCLIPModel</a> (X-CLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMModel">XGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMModel">XLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetModel">XLMProphetNetModel</a> (XLM-ProphetNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaModel">XLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLModel">XLMRobertaXLModel</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetModel">XLNetModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodModel">XmodModel</a> (X-MOD model)</li>
<li><a href="/docs/transformers/main/en/model_doc/yolos#transformers.YolosConfig">YolosConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/yolos#transformers.YolosModel">YolosModel</a> (YOLOS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoModel">YosoModel</a> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ss=new $({props:{anchor:"transformers.AutoModel.from_config.example",$$slots:{default:[vX]},$$scope:{ctx:y}}}),rm=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModel.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModel.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModel.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModel.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModel.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModel.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModel.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModel.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModel.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModel.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModel.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModel.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModel.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModel.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModel.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModel.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Gs=new $({props:{anchor:"transformers.AutoModel.from_pretrained.example",$$slots:{default:[wX]},$$scope:{ctx:y}}}),nm=new B({props:{title:"TFAutoModel",local:"transformers.TFAutoModel",headingTag:"h3"}}),tm=new x({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L531"}}),am=new x({props:{name:"from_config",anchor:"transformers.TFAutoModel.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModel.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blip#transformers.TFBlipModel">TFBlipModel</a> (BLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNeXT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.TFConvNextV2Model">TFConvNextV2Model</a> (ConvNeXTV2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/cvt#transformers.TFCvtModel">TFCvtModel</a> (CvT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/dpr#transformers.DPRConfig">DPRConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/dpr#transformers.TFDPRQuestionEncoder">TFDPRQuestionEncoder</a> (DPR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.TFData2VecVisionModel">TFData2VecVisionModel</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deit#transformers.TFDeiTModel">TFDeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertModel">TFDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerConfig">EfficientFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.TFEfficientFormerModel">TFEfficientFormerModel</a> (EfficientFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraModel">TFElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/esm#transformers.TFEsmModel">TFEsmModel</a> (ESM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertModel">TFFlaubertModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelModel">TFFunnelModel</a> or <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelBaseModel">TFFunnelBaseModel</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2Model">TFGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptj#transformers.TFGPTJModel">TFGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/main/en/model_doc/groupvit#transformers.GroupViTConfig">GroupViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/groupvit#transformers.TFGroupViTModel">TFGroupViTModel</a> (GroupViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/hubert#transformers.TFHubertModel">TFHubertModel</a> (Hubert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/led#transformers.TFLEDModel">TFLEDModel</a> (LED model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMModel">TFLayoutLMModel</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3Model">TFLayoutLMv3Model</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerModel">TFLongformerModel</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.TFLxmertModel">TFLxmertModel</a> (LXMERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.TFMBartModel">TFMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetModel">TFMPNetModel</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mt5#transformers.TFMT5Model">TFMT5Model</a> (MT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/marian#transformers.TFMarianModel">TFMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertModel">TFMobileBertModel</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.TFMobileViTModel">TFMobileViTModel</a> (MobileViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/opt#transformers.TFOPTModel">TFOPTModel</a> (OPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.TFOpenAIGPTModel">TFOpenAIGPTModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.TFPegasusModel">TFPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig">RegNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/regnet#transformers.TFRegNetModel">TFRegNetModel</a> (RegNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertModel">TFRemBertModel</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/resnet#transformers.TFResNetModel">TFResNetModel</a> (ResNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerModel">TFRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaModel">TFRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormModel">TFRobertaPreLayerNormModel</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/sam#transformers.SamConfig">SamConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/sam#transformers.TFSamModel">TFSamModel</a> (SAM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/segformer#transformers.TFSegformerModel">TFSegformerModel</a> (SegFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.TFSpeech2TextModel">TFSpeech2TextModel</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swin#transformers.TFSwinModel">TFSwinModel</a> (Swin Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.TFT5Model">TFT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TFTapasModel">TFTapasModel</a> (TAPAS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TFTransfoXLModel">TFTransfoXLModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit#transformers.TFViTModel">TFViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.TFViTMAEModel">TFViTMAEModel</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.TFVisionTextDualEncoderModel">TFVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.TFWav2Vec2Model">TFWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/whisper#transformers.TFWhisperModel">TFWhisperModel</a> (Whisper model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xglm#transformers.TFXGLMModel">TFXGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMModel">TFXLMModel</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaModel">TFXLMRobertaModel</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetModel">TFXLNetModel</a> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Vs=new $({props:{anchor:"transformers.TFAutoModel.from_config.example",$$slots:{default:[xX]},$$scope:{ctx:y}}}),sm=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModel.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModel.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModel.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModel.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModel.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModel.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModel.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModel.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModel.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModel.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModel.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModel.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModel.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModel.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModel.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Js=new $({props:{anchor:"transformers.TFAutoModel.from_pretrained.example",$$slots:{default:[kX]},$$scope:{ctx:y}}}),im=new B({props:{title:"FlaxAutoModel",local:"transformers.FlaxAutoModel",headingTag:"h3"}}),lm=new x({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L278"}}),dm=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModel.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModel.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bloom#transformers.FlaxBloomModel">FlaxBloomModel</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertModel">FlaxDistilBertModel</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraModel">FlaxElectraModel</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.FlaxGPT2Model">FlaxGPT2Model</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptj#transformers.FlaxGPTJModel">FlaxGPTJModel</a> (GPT-J model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.FlaxGPTNeoModel">FlaxGPTNeoModel</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/llama#transformers.FlaxLlamaModel">FlaxLlamaModel</a> (LLaMA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config">LongT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longt5#transformers.FlaxLongT5Model">FlaxLongT5Model</a> (LongT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartModel">FlaxMBartModel</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mt5#transformers.FlaxMT5Model">FlaxMT5Model</a> (MT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/marian#transformers.FlaxMarianModel">FlaxMarianModel</a> (Marian model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mistral#transformers.FlaxMistralModel">FlaxMistralModel</a> (Mistral model)</li>
<li><a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/opt#transformers.FlaxOPTModel">FlaxOPTModel</a> (OPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.FlaxPegasusModel">FlaxPegasusModel</a> (Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig">RegNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/regnet#transformers.FlaxRegNetModel">FlaxRegNetModel</a> (RegNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/resnet#transformers.FlaxResNetModel">FlaxResNetModel</a> (ResNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerModel">FlaxRoFormerModel</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaModel">FlaxRobertaModel</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormModel">FlaxRobertaPreLayerNormModel</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5Model">FlaxT5Model</a> (T5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit#transformers.FlaxViTModel">FlaxViTModel</a> (ViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.VisionTextDualEncoderConfig">VisionTextDualEncoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vision-text-dual-encoder#transformers.FlaxVisionTextDualEncoderModel">FlaxVisionTextDualEncoderModel</a> (VisionTextDualEncoder model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2Model">FlaxWav2Vec2Model</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/whisper#transformers.FlaxWhisperModel">FlaxWhisperModel</a> (Whisper model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xglm#transformers.FlaxXGLMModel">FlaxXGLMModel</a> (XGLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaModel">FlaxXLMRobertaModel</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Es=new $({props:{anchor:"transformers.FlaxAutoModel.from_config.example",$$slots:{default:[LX]},$$scope:{ctx:y}}}),mm=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModel.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModel.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModel.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModel.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModel.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModel.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModel.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModel.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModel.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModel.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModel.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModel.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModel.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModel.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModel.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Is=new $({props:{anchor:"transformers.FlaxAutoModel.from_pretrained.example",$$slots:{default:[$X]},$$scope:{ctx:y}}}),cm=new B({props:{title:"Generic pretraining classes",local:"generic-pretraining-classes",headingTag:"h2"}}),gm=new B({props:{title:"AutoModelForPreTraining",local:"transformers.AutoModelForPreTraining",headingTag:"h3"}}),pm=new x({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1321"}}),hm=new x({props:{name:"from_config",anchor:"transformers.AutoModelForPreTraining.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForPreTraining.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForPreTraining">ElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForPreTraining">ErnieForPreTraining</a> (ERNIE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForPreTraining">FNetForPreTraining</a> (FNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaForPreTraining">FlavaForPreTraining</a> (FLAVA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForPreTraining">FunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM">GPTBigCodeForCausalLM</a> (GPTBigCode model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig">GPTSanJapaneseConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsConfig">IdeficsConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/idefics#transformers.IdeficsForVisionText2Text">IdeficsForVisionText2Text</a> (IDEFICS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig">LlavaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> (LLaVa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForMaskedLM">LukeForMaskedLM</a> (LUKE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertForPreTraining">LxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForMaskedLM">MegaForMaskedLM</a> (MEGA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForPreTraining">MegatronBertForPreTraining</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForPreTraining">MobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptForCausalLM">MptForCausalLM</a> (MPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForMaskedLM">MraForMaskedLM</a> (MRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForPreTraining">NezhaForPreTraining</a> (Nezha model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig">NllbMoeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration">NllbMoeForConditionalGeneration</a> (NLLB-MOE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertConfig">RetriBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/retribert#transformers.RetriBertModel">RetriBertModel</a> (RetriBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForPreTraining">RoCBertForPreTraining</a> (RoCBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM">RobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig">RwkvConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvForCausalLM">RwkvForCausalLM</a> (RWKV model)</li>
<li><a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterForPreTraining">SplinterForPreTraining</a> (Splinter model)</li>
<li><a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig">SwitchTransformersConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration">SwitchTransformersForConditionalGeneration</a> (SwitchTransformers model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tvlt#transformers.TvltConfig">TvltConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tvlt#transformers.TvltForPreTraining">TvltForPreTraining</a> (TVLT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForPreTraining">UniSpeechForPreTraining</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForPreTraining">UniSpeechSatForPreTraining</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEForPreTraining">ViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig">VideoMAEConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEForPreTraining">VideoMAEForPreTraining</a> (VideoMAE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig">VipLlavaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration">VipLlavaForConditionalGeneration</a> (VipLlava model)</li>
<li><a href="/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertConfig">VisualBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/visual_bert#transformers.VisualBertForPreTraining">VisualBertForPreTraining</a> (VisualBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForPreTraining">Wav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForPreTraining">Wav2Vec2ConformerForPreTraining</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMaskedLM">XmodForMaskedLM</a> (X-MOD model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Us=new $({props:{anchor:"transformers.AutoModelForPreTraining.from_config.example",$$slots:{default:[BX]},$$scope:{ctx:y}}}),um=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForPreTraining.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForPreTraining.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ns=new $({props:{anchor:"transformers.AutoModelForPreTraining.from_pretrained.example",$$slots:{default:[jX]},$$scope:{ctx:y}}}),_m=new B({props:{title:"TFAutoModelForPreTraining",local:"transformers.TFAutoModelForPreTraining",headingTag:"h3"}}),bm=new x({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L547"}}),Mm=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForPreTraining.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForPreTraining.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForPreTraining">TFElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForPreTraining">TFFunnelForPreTraining</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.TFLxmertForPreTraining">TFLxmertForPreTraining</a> (LXMERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForPreTraining">TFMobileBertForPreTraining</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForMaskedLM">TFRobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.ViTMAEConfig">ViTMAEConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit_mae#transformers.TFViTMAEForPreTraining">TFViTMAEForPreTraining</a> (ViTMAE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Xs=new $({props:{anchor:"transformers.TFAutoModelForPreTraining.from_config.example",$$slots:{default:[ZX]},$$scope:{ctx:y}}}),Fm=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForPreTraining.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),qs=new $({props:{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.example",$$slots:{default:[AX]},$$scope:{ctx:y}}}),Tm=new B({props:{title:"FlaxAutoModelForPreTraining",local:"transformers.FlaxAutoModelForPreTraining",headingTag:"h3"}}),Cm=new x({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L285"}}),ym=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForPreTraining.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForPreTraining.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForPreTraining">FlaxElectraForPreTraining</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config">LongT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longt5#transformers.FlaxLongT5ForConditionalGeneration">FlaxLongT5ForConditionalGeneration</a> (LongT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (MT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForMaskedLM">FlaxRobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.FlaxWav2Vec2ForPreTraining">FlaxWav2Vec2ForPreTraining</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/whisper#transformers.FlaxWhisperForConditionalGeneration">FlaxWhisperForConditionalGeneration</a> (Whisper model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM">FlaxXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Qs=new $({props:{anchor:"transformers.FlaxAutoModelForPreTraining.from_config.example",$$slots:{default:[RX]},$$scope:{ctx:y}}}),vm=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ds=new $({props:{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.example",$$slots:{default:[PX]},$$scope:{ctx:y}}}),wm=new B({props:{title:"Natural Language Processing",local:"natural-language-processing",headingTag:"h2"}}),km=new B({props:{title:"AutoModelForCausalLM",local:"transformers.AutoModelForCausalLM",headingTag:"h3"}}),Lm=new x({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1336"}}),$m=new x({props:{name:"from_config",anchor:"transformers.AutoModelForCausalLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCausalLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForCausalLM">BioGptForCausalLM</a> (BioGpt model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenConfig">CodeGenConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/codegen#transformers.CodeGenForCausalLM">CodeGenForCausalLM</a> (CodeGen model)</li>
<li><a href="/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/cpmant#transformers.CpmAntForCausalLM">CpmAntForCausalLM</a> (CPM-Ant model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForCausalLM">ElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForCausalLM">ErnieForCausalLM</a> (ERNIE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconForCausalLM">FalconForCausalLM</a> (Falcon model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuConfig">FuyuConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fuyu#transformers.FuyuForCausalLM">FuyuForCausalLM</a> (Fuyu model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2LMHeadModel">GPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForCausalLM">GPTBigCodeForCausalLM</a> (GPTBigCode model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForCausalLM">GPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForCausalLM">GPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForCausalLM">GPTNeoXForCausalLM</a> (GPT NeoX model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig">GPTNeoXJapaneseConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM">GPTNeoXJapaneseForCausalLM</a> (GPT NeoX Japanese model)</li>
<li><a href="/docs/transformers/main/en/model_doc/git#transformers.GitConfig">GitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a> (GIT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForCausalLM">LlamaForCausalLM</a> (LLaMA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartForCausalLM">MBartForCausalLM</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianForCausalLM">MarianForCausalLM</a> (Marian model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForCausalLM">MegaForCausalLM</a> (MEGA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForCausalLM">MegatronBertForCausalLM</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralForCausalLM">MistralForCausalLM</a> (Mistral model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForCausalLM">MixtralForCausalLM</a> (Mixtral model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptForCausalLM">MptForCausalLM</a> (MPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenConfig">MusicgenConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/musicgen#transformers.MusicgenForCausalLM">MusicgenForCausalLM</a> (MusicGen model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForCausalLM">MvpForCausalLM</a> (MVP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTForCausalLM">OPTForCausalLM</a> (OPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTLMHeadModel">OpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaForCausalLM">OpenLlamaForCausalLM</a> (OpenLlama model)</li>
<li><a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForCausalLM">PLBartForCausalLM</a> (PLBart model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusForCausalLM">PegasusForCausalLM</a> (Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig">PersimmonConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForCausalLM">PersimmonForCausalLM</a> (Persimmon model)</li>
<li><a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig">PhiConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiForCausalLM">PhiForCausalLM</a> (Phi model)</li>
<li><a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForCausalLM">ProphetNetForCausalLM</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertLMHeadModel">QDQBertLMHeadModel</a> (QDQBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config">Qwen2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForCausalLM">Qwen2ForCausalLM</a> (Qwen2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerModelWithLMHead">ReformerModelWithLMHead</a> (Reformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForCausalLM">RemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForCausalLM">RoCBertForCausalLM</a> (RoCBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForCausalLM">RoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForCausalLM">RobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForCausalLM">RobertaPreLayerNormForCausalLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvConfig">RwkvConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rwkv#transformers.RwkvForCausalLM">RwkvForCausalLM</a> (RWKV model)</li>
<li><a href="/docs/transformers/main/en/model_doc/speech_to_text_2#transformers.Speech2Text2Config">Speech2Text2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/speech_to_text_2#transformers.Speech2Text2ForCausalLM">Speech2Text2ForCausalLM</a> (Speech2Text2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForCausalLM">StableLmForCausalLM</a> (StableLm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRConfig">TrOCRConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/trocr#transformers.TrOCRForCausalLM">TrOCRForCausalLM</a> (TrOCR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLLMHeadModel">TransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForCausalLM">WhisperForCausalLM</a> (Whisper model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMForCausalLM">XGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForCausalLM">XLMProphetNetForCausalLM</a> (XLM-ProphetNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForCausalLM">XLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForCausalLM">XLMRobertaXLForCausalLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetLMHeadModel">XLNetLMHeadModel</a> (XLNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForCausalLM">XmodForCausalLM</a> (X-MOD model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Hs=new $({props:{anchor:"transformers.AutoModelForCausalLM.from_config.example",$$slots:{default:[WX]},$$scope:{ctx:y}}}),Bm=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForCausalLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCausalLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ys=new $({props:{anchor:"transformers.AutoModelForCausalLM.from_pretrained.example",$$slots:{default:[SX]},$$scope:{ctx:y}}}),jm=new B({props:{title:"TFAutoModelForCausalLM",local:"transformers.TFAutoModelForCausalLM",headingTag:"h3"}}),Zm=new x({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L562"}}),Am=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForCausalLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForCausalLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForCausalLM">TFCamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2LMHeadModel">TFGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptj#transformers.TFGPTJForCausalLM">TFGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/opt#transformers.TFOPTForCausalLM">TFOPTForCausalLM</a> (OPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.TFOpenAIGPTLMHeadModel">TFOpenAIGPTLMHeadModel</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForCausalLM">TFRemBertForCausalLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForCausalLM">TFRoFormerForCausalLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForCausalLM">TFRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForCausalLM">TFRobertaPreLayerNormForCausalLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TFTransfoXLLMHeadModel">TFTransfoXLLMHeadModel</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xglm#transformers.TFXGLMForCausalLM">TFXGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForCausalLM">TFXLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetLMHeadModel">TFXLNetLMHeadModel</a> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),zs=new $({props:{anchor:"transformers.TFAutoModelForCausalLM.from_config.example",$$slots:{default:[GX]},$$scope:{ctx:y}}}),Rm=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForCausalLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Os=new $({props:{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.example",$$slots:{default:[VX]},$$scope:{ctx:y}}}),Pm=new B({props:{title:"FlaxAutoModelForCausalLM",local:"transformers.FlaxAutoModelForCausalLM",headingTag:"h3"}}),Wm=new x({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L292"}}),Sm=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForCausalLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForCausalLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForCausalLM">FlaxBartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForCausalLM">FlaxBertForCausalLM</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForCausalLM">FlaxBigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bloom#transformers.FlaxBloomForCausalLM">FlaxBloomForCausalLM</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForCausalLM">FlaxElectraForCausalLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.FlaxGPT2LMHeadModel">FlaxGPT2LMHeadModel</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptj#transformers.FlaxGPTJForCausalLM">FlaxGPTJForCausalLM</a> (GPT-J model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.FlaxGPTNeoForCausalLM">FlaxGPTNeoForCausalLM</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/llama#transformers.FlaxLlamaForCausalLM">FlaxLlamaForCausalLM</a> (LLaMA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mistral#transformers.FlaxMistralForCausalLM">FlaxMistralForCausalLM</a> (Mistral model)</li>
<li><a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/opt#transformers.FlaxOPTForCausalLM">FlaxOPTForCausalLM</a> (OPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForCausalLM">FlaxRobertaForCausalLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForCausalLM">FlaxRobertaPreLayerNormForCausalLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xglm#transformers.XGLMConfig">XGLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xglm#transformers.FlaxXGLMForCausalLM">FlaxXGLMForCausalLM</a> (XGLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForCausalLM">FlaxXLMRobertaForCausalLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ks=new $({props:{anchor:"transformers.FlaxAutoModelForCausalLM.from_config.example",$$slots:{default:[JX]},$$scope:{ctx:y}}}),Gm=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ei=new $({props:{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.example",$$slots:{default:[EX]},$$scope:{ctx:y}}}),Vm=new B({props:{title:"AutoModelForMaskedLM",local:"transformers.AutoModelForMaskedLM",headingTag:"h3"}}),Jm=new x({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1343"}}),Em=new x({props:{name:"from_config",anchor:"transformers.AutoModelForMaskedLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMaskedLM">DistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForMaskedLM">ElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForMaskedLM">ErnieForMaskedLM</a> (ERNIE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmForMaskedLM">EsmForMaskedLM</a> (ESM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForMaskedLM">FNetForMaskedLM</a> (FNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertWithLMHeadModel">FlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMaskedLM">FunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMaskedLM">IBertForMaskedLM</a> (I-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForMaskedLM">LayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMaskedLM">LongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForMaskedLM">LukeForMaskedLM</a> (LUKE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMaskedLM">MPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForMaskedLM">MegaForMaskedLM</a> (MEGA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForMaskedLM">MegatronBertForMaskedLM</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForMaskedLM">MobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForMaskedLM">MraForMaskedLM</a> (MRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForMaskedLM">NezhaForMaskedLM</a> (Nezha model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForMaskedLM">NystromformerForMaskedLM</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForMaskedLM">PerceiverForMaskedLM</a> (Perceiver model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForMaskedLM">QDQBertForMaskedLM</a> (QDQBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForMaskedLM">ReformerForMaskedLM</a> (Reformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForMaskedLM">RemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForMaskedLM">RoCBertForMaskedLM</a> (RoCBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForMaskedLM">RoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMaskedLM">RobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMaskedLM">RobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMaskedLM">SqueezeBertForMaskedLM</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasForMaskedLM">TapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <code>Wav2Vec2ForMaskedLM</code> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMWithLMHeadModel">XLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMaskedLM">XLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMaskedLM">XLMRobertaXLForMaskedLM</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMaskedLM">XmodForMaskedLM</a> (X-MOD model)</li>
<li><a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoForMaskedLM">YosoForMaskedLM</a> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),oi=new $({props:{anchor:"transformers.AutoModelForMaskedLM.from_config.example",$$slots:{default:[IX]},$$scope:{ctx:y}}}),Im=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForMaskedLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ri=new $({props:{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.example",$$slots:{default:[UX]},$$scope:{ctx:y}}}),Um=new B({props:{title:"TFAutoModelForMaskedLM",local:"transformers.TFAutoModelForMaskedLM",headingTag:"h3"}}),Nm=new x({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L612"}}),Xm=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForMaskedLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMaskedLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMaskedLM">TFDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForMaskedLM">TFElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/esm#transformers.TFEsmForMaskedLM">TFEsmForMaskedLM</a> (ESM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertWithLMHeadModel">TFFlaubertWithLMHeadModel</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForMaskedLM">TFFunnelForMaskedLM</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMForMaskedLM">TFLayoutLMForMaskedLM</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerForMaskedLM">TFLongformerForMaskedLM</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForMaskedLM">TFMPNetForMaskedLM</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForMaskedLM">TFMobileBertForMaskedLM</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForMaskedLM">TFRemBertForMaskedLM</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForMaskedLM">TFRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForMaskedLM">TFRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForMaskedLM">TFRobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TFTapasForMaskedLM">TFTapasForMaskedLM</a> (TAPAS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMWithLMHeadModel">TFXLMWithLMHeadModel</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMaskedLM">TFXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ni=new $({props:{anchor:"transformers.TFAutoModelForMaskedLM.from_config.example",$$slots:{default:[NX]},$$scope:{ctx:y}}}),qm=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ti=new $({props:{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.example",$$slots:{default:[XX]},$$scope:{ctx:y}}}),Qm=new B({props:{title:"FlaxAutoModelForMaskedLM",local:"transformers.FlaxAutoModelForMaskedLM",headingTag:"h3"}}),Dm=new x({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L299"}}),Hm=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForMaskedLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForMaskedLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForMaskedLM">FlaxDistilBertForMaskedLM</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForMaskedLM">FlaxElectraForMaskedLM</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForMaskedLM">FlaxRoFormerForMaskedLM</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForMaskedLM">FlaxRobertaForMaskedLM</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForMaskedLM">FlaxRobertaPreLayerNormForMaskedLM</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMaskedLM">FlaxXLMRobertaForMaskedLM</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ai=new $({props:{anchor:"transformers.FlaxAutoModelForMaskedLM.from_config.example",$$slots:{default:[qX]},$$scope:{ctx:y}}}),Ym=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),si=new $({props:{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.example",$$slots:{default:[QX]},$$scope:{ctx:y}}}),zm=new B({props:{title:"AutoModelForMaskGeneration",local:"transformers.AutoModelForMaskGeneration",headingTag:"h3"}}),Km=new x({props:{name:"class transformers.AutoModelForMaskGeneration",anchor:"transformers.AutoModelForMaskGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1302"}}),ec=new B({props:{title:"TFAutoModelForMaskGeneration",local:"transformers.TFAutoModelForMaskGeneration",headingTag:"h3"}}),rc=new x({props:{name:"class transformers.TFAutoModelForMaskGeneration",anchor:"transformers.TFAutoModelForMaskGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L523"}}),nc=new B({props:{title:"AutoModelForSeq2SeqLM",local:"transformers.AutoModelForSeq2SeqLM",headingTag:"h3"}}),tc=new x({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1350"}}),ac=new x({props:{name:"from_config",anchor:"transformers.AutoModelForSeq2SeqLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSeq2SeqLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderModel">EncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTConfig">FSMTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fsmt#transformers.FSMTForConditionalGeneration">FSMTForConditionalGeneration</a> (FairSeq Machine-Translation model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseConfig">GPTSanJapaneseConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptsan-japanese#transformers.GPTSanJapaneseForConditionalGeneration">GPTSanJapaneseForConditionalGeneration</a> (GPTSAN-japanese model)</li>
<li><a href="/docs/transformers/main/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDForConditionalGeneration">LEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config">LongT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longt5#transformers.LongT5ForConditionalGeneration">LongT5ForConditionalGeneration</a> (LongT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100Config">M2M100Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/m2m_100#transformers.M2M100ForConditionalGeneration">M2M100ForConditionalGeneration</a> (M2M100 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartForConditionalGeneration">MBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForConditionalGeneration">MT5ForConditionalGeneration</a> (MT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianMTModel">MarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForConditionalGeneration">MvpForConditionalGeneration</a> (MVP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeConfig">NllbMoeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nllb-moe#transformers.NllbMoeForConditionalGeneration">NllbMoeForConditionalGeneration</a> (NLLB-MOE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForConditionalGeneration">PLBartForConditionalGeneration</a> (PLBart model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusForConditionalGeneration">PegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXConfig">PegasusXConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pegasus_x#transformers.PegasusXForConditionalGeneration">PegasusXForConditionalGeneration</a> (PEGASUS-X model)</li>
<li><a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetConfig">ProphetNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/prophetnet#transformers.ProphetNetForConditionalGeneration">ProphetNetForConditionalGeneration</a> (ProphetNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig">SeamlessM4TConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TForTextToText">SeamlessM4TForTextToText</a> (SeamlessM4T model)</li>
<li><a href="/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config">SeamlessM4Tv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForTextToText">SeamlessM4Tv2ForTextToText</a> (SeamlessM4Tv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersConfig">SwitchTransformersConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/switch_transformers#transformers.SwitchTransformersForConditionalGeneration">SwitchTransformersForConditionalGeneration</a> (SwitchTransformers model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5ForConditionalGeneration">T5ForConditionalGeneration</a> (T5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForConditionalGeneration">UMT5ForConditionalGeneration</a> (UMT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetConfig">XLMProphetNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-prophetnet#transformers.XLMProphetNetForConditionalGeneration">XLMProphetNetForConditionalGeneration</a> (XLM-ProphetNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ii=new $({props:{anchor:"transformers.AutoModelForSeq2SeqLM.from_config.example",$$slots:{default:[DX]},$$scope:{ctx:y}}}),sc=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),li=new $({props:{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.example",$$slots:{default:[HX]},$$scope:{ctx:y}}}),ic=new B({props:{title:"TFAutoModelForSeq2SeqLM",local:"transformers.TFAutoModelForSeq2SeqLM",headingTag:"h3"}}),lc=new x({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L619"}}),dc=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForSeq2SeqLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/encoder-decoder#transformers.TFEncoderDecoderModel">TFEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/main/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/led#transformers.TFLEDForConditionalGeneration">TFLEDForConditionalGeneration</a> (LED model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.TFMBartForConditionalGeneration">TFMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mt5#transformers.TFMT5ForConditionalGeneration">TFMT5ForConditionalGeneration</a> (MT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/marian#transformers.TFMarianMTModel">TFMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.TFPegasusForConditionalGeneration">TFPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.TFT5ForConditionalGeneration">TFT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),di=new $({props:{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_config.example",$$slots:{default:[YX]},$$scope:{ctx:y}}}),mc=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),mi=new $({props:{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.example",$$slots:{default:[zX]},$$scope:{ctx:y}}}),cc=new B({props:{title:"FlaxAutoModelForSeq2SeqLM",local:"transformers.FlaxAutoModelForSeq2SeqLM",headingTag:"h3"}}),fc=new x({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L306"}}),gc=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/en/model_doc/encoder-decoder#transformers.EncoderDecoderConfig">EncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/encoder-decoder#transformers.FlaxEncoderDecoderModel">FlaxEncoderDecoderModel</a> (Encoder decoder model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longt5#transformers.LongT5Config">LongT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longt5#transformers.FlaxLongT5ForConditionalGeneration">FlaxLongT5ForConditionalGeneration</a> (LongT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartForConditionalGeneration">FlaxMBartForConditionalGeneration</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mt5#transformers.FlaxMT5ForConditionalGeneration">FlaxMT5ForConditionalGeneration</a> (MT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/marian#transformers.MarianConfig">MarianConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/marian#transformers.FlaxMarianMTModel">FlaxMarianMTModel</a> (Marian model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pegasus#transformers.PegasusConfig">PegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pegasus#transformers.FlaxPegasusForConditionalGeneration">FlaxPegasusForConditionalGeneration</a> (Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.FlaxT5ForConditionalGeneration">FlaxT5ForConditionalGeneration</a> (T5 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ci=new $({props:{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_config.example",$$slots:{default:[OX]},$$scope:{ctx:y}}}),pc=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),fi=new $({props:{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.example",$$slots:{default:[KX]},$$scope:{ctx:y}}}),hc=new B({props:{title:"AutoModelForSequenceClassification",local:"transformers.AutoModelForSequenceClassification",headingTag:"h3"}}),uc=new x({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1361"}}),_c=new x({props:{name:"from_config",anchor:"transformers.AutoModelForSequenceClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSequenceClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForSequenceClassification">BioGptForSequenceClassification</a> (BioGpt model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomForSequenceClassification">BloomForSequenceClassification</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (CANINE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForSequenceClassification">DistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForSequenceClassification">ElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForSequenceClassification">ErnieForSequenceClassification</a> (ERNIE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMForSequenceClassification">ErnieMForSequenceClassification</a> (ErnieM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmForSequenceClassification">EsmForSequenceClassification</a> (ESM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForSequenceClassification">FNetForSequenceClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconForSequenceClassification">FalconForSequenceClassification</a> (Falcon model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForSequenceClassification">FlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForSequenceClassification">FunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForSequenceClassification">GPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForSequenceClassification">GPTBigCodeForSequenceClassification</a> (GPTBigCode model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForSequenceClassification">GPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForSequenceClassification">GPTNeoForSequenceClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForSequenceClassification">GPTNeoXForSequenceClassification</a> (GPT NeoX model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForSequenceClassification">IBertForSequenceClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDForSequenceClassification">LEDForSequenceClassification</a> (LED model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForSequenceClassification">LayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForSequenceClassification">LayoutLMv2ForSequenceClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForSequenceClassification">LayoutLMv3ForSequenceClassification</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig">LiltConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltForSequenceClassification">LiltForSequenceClassification</a> (LiLT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaForSequenceClassification">LlamaForSequenceClassification</a> (LLaMA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForSequenceClassification">LongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForSequenceClassification">LukeForSequenceClassification</a> (LUKE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartForSequenceClassification">MBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForSequenceClassification">MPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForSequenceClassification">MT5ForSequenceClassification</a> (MT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig">MarkupLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForSequenceClassification">MarkupLMForSequenceClassification</a> (MarkupLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForSequenceClassification">MegaForSequenceClassification</a> (MEGA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForSequenceClassification">MegatronBertForSequenceClassification</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralForSequenceClassification">MistralForSequenceClassification</a> (Mistral model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralConfig">MixtralConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mixtral#transformers.MixtralForSequenceClassification">MixtralForSequenceClassification</a> (Mixtral model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForSequenceClassification">MobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptForSequenceClassification">MptForSequenceClassification</a> (MPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForSequenceClassification">MraForSequenceClassification</a> (MRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForSequenceClassification">MvpForSequenceClassification</a> (MVP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForSequenceClassification">NezhaForSequenceClassification</a> (Nezha model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForSequenceClassification">NystromformerForSequenceClassification</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTForSequenceClassification">OPTForSequenceClassification</a> (OPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTForSequenceClassification">OpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaConfig">OpenLlamaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/open-llama#transformers.OpenLlamaForSequenceClassification">OpenLlamaForSequenceClassification</a> (OpenLlama model)</li>
<li><a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartConfig">PLBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/plbart#transformers.PLBartForSequenceClassification">PLBartForSequenceClassification</a> (PLBart model)</li>
<li><a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForSequenceClassification">PerceiverForSequenceClassification</a> (Perceiver model)</li>
<li><a href="/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonConfig">PersimmonConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/persimmon#transformers.PersimmonForSequenceClassification">PersimmonForSequenceClassification</a> (Persimmon model)</li>
<li><a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig">PhiConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiForSequenceClassification">PhiForSequenceClassification</a> (Phi model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForSequenceClassification">QDQBertForSequenceClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2Config">Qwen2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qwen2#transformers.Qwen2ForSequenceClassification">Qwen2ForSequenceClassification</a> (Qwen2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForSequenceClassification">ReformerForSequenceClassification</a> (Reformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForSequenceClassification">RemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForSequenceClassification">RoCBertForSequenceClassification</a> (RoCBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForSequenceClassification">RoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForSequenceClassification">RobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForSequenceClassification">RobertaPreLayerNormForSequenceClassification</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForSequenceClassification">SqueezeBertForSequenceClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForSequenceClassification">StableLmForSequenceClassification</a> (StableLm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5ForSequenceClassification">T5ForSequenceClassification</a> (T5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasForSequenceClassification">TapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLForSequenceClassification">TransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForSequenceClassification">UMT5ForSequenceClassification</a> (UMT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMForSequenceClassification">XLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForSequenceClassification">XLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForSequenceClassification">XLMRobertaXLForSequenceClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForSequenceClassification">XLNetForSequenceClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForSequenceClassification">XmodForSequenceClassification</a> (X-MOD model)</li>
<li><a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoForSequenceClassification">YosoForSequenceClassification</a> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),gi=new $({props:{anchor:"transformers.AutoModelForSequenceClassification.from_config.example",$$slots:{default:[eq]},$$scope:{ctx:y}}}),bc=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSequenceClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),pi=new $({props:{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.example",$$slots:{default:[oq]},$$scope:{ctx:y}}}),Mc=new B({props:{title:"TFAutoModelForSequenceClassification",local:"transformers.TFAutoModelForSequenceClassification",headingTag:"h3"}}),Fc=new x({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L630"}}),Tc=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForSequenceClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSequenceClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.TFBartForSequenceClassification">TFBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForSequenceClassification">TFDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForSequenceClassification">TFElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/esm#transformers.TFEsmForSequenceClassification">TFEsmForSequenceClassification</a> (ESM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertForSequenceClassification">TFFlaubertForSequenceClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForSequenceClassification">TFFunnelForSequenceClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.TFGPT2ForSequenceClassification">TFGPT2ForSequenceClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptj#transformers.TFGPTJForSequenceClassification">TFGPTJForSequenceClassification</a> (GPT-J model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMForSequenceClassification">TFLayoutLMForSequenceClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForSequenceClassification">TFLayoutLMv3ForSequenceClassification</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerForSequenceClassification">TFLongformerForSequenceClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForSequenceClassification">TFMPNetForSequenceClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForSequenceClassification">TFMobileBertForSequenceClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.OpenAIGPTConfig">OpenAIGPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/openai-gpt#transformers.TFOpenAIGPTForSequenceClassification">TFOpenAIGPTForSequenceClassification</a> (OpenAI GPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForSequenceClassification">TFRemBertForSequenceClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForSequenceClassification">TFRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForSequenceClassification">TFRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForSequenceClassification">TFRobertaPreLayerNormForSequenceClassification</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TFTapasForSequenceClassification">TFTapasForSequenceClassification</a> (TAPAS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TransfoXLConfig">TransfoXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/transfo-xl#transformers.TFTransfoXLForSequenceClassification">TFTransfoXLForSequenceClassification</a> (Transformer-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMForSequenceClassification">TFXLMForSequenceClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForSequenceClassification">TFXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetForSequenceClassification">TFXLNetForSequenceClassification</a> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),hi=new $({props:{anchor:"transformers.TFAutoModelForSequenceClassification.from_config.example",$$slots:{default:[rq]},$$scope:{ctx:y}}}),Cc=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ui=new $({props:{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.example",$$slots:{default:[nq]},$$scope:{ctx:y}}}),yc=new B({props:{title:"FlaxAutoModelForSequenceClassification",local:"transformers.FlaxAutoModelForSequenceClassification",headingTag:"h3"}}),vc=new x({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L317"}}),wc=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForSequenceClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForSequenceClassification">FlaxDistilBertForSequenceClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForSequenceClassification">FlaxElectraForSequenceClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartForSequenceClassification">FlaxMBartForSequenceClassification</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForSequenceClassification">FlaxRoFormerForSequenceClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForSequenceClassification">FlaxRobertaForSequenceClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForSequenceClassification">FlaxRobertaPreLayerNormForSequenceClassification</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForSequenceClassification">FlaxXLMRobertaForSequenceClassification</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),_i=new $({props:{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_config.example",$$slots:{default:[tq]},$$scope:{ctx:y}}}),xc=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),bi=new $({props:{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.example",$$slots:{default:[aq]},$$scope:{ctx:y}}}),kc=new B({props:{title:"AutoModelForMultipleChoice",local:"transformers.AutoModelForMultipleChoice",headingTag:"h3"}}),Lc=new x({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1417"}}),$c=new x({props:{name:"from_config",anchor:"transformers.AutoModelForMultipleChoice.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMultipleChoice.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (CANINE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForMultipleChoice">DebertaV2ForMultipleChoice</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForMultipleChoice">DistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForMultipleChoice">ElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForMultipleChoice">ErnieForMultipleChoice</a> (ERNIE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMForMultipleChoice">ErnieMForMultipleChoice</a> (ErnieM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForMultipleChoice">FNetForMultipleChoice</a> (FNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForMultipleChoice">FlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForMultipleChoice">FunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForMultipleChoice">IBertForMultipleChoice</a> (I-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForMultipleChoice">LongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForMultipleChoice">LukeForMultipleChoice</a> (LUKE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForMultipleChoice">MPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForMultipleChoice">MegaForMultipleChoice</a> (MEGA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForMultipleChoice">MegatronBertForMultipleChoice</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForMultipleChoice">MobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForMultipleChoice">MraForMultipleChoice</a> (MRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForMultipleChoice">NezhaForMultipleChoice</a> (Nezha model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForMultipleChoice">NystromformerForMultipleChoice</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForMultipleChoice">QDQBertForMultipleChoice</a> (QDQBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForMultipleChoice">RemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForMultipleChoice">RoCBertForMultipleChoice</a> (RoCBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForMultipleChoice">RoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForMultipleChoice">RobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForMultipleChoice">RobertaPreLayerNormForMultipleChoice</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForMultipleChoice">SqueezeBertForMultipleChoice</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMForMultipleChoice">XLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForMultipleChoice">XLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForMultipleChoice">XLMRobertaXLForMultipleChoice</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForMultipleChoice">XLNetForMultipleChoice</a> (XLNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForMultipleChoice">XmodForMultipleChoice</a> (X-MOD model)</li>
<li><a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoForMultipleChoice">YosoForMultipleChoice</a> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Mi=new $({props:{anchor:"transformers.AutoModelForMultipleChoice.from_config.example",$$slots:{default:[sq]},$$scope:{ctx:y}}}),Bc=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForMultipleChoice.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Fi=new $({props:{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.example",$$slots:{default:[iq]},$$scope:{ctx:y}}}),jc=new B({props:{title:"TFAutoModelForMultipleChoice",local:"transformers.TFAutoModelForMultipleChoice",headingTag:"h3"}}),Zc=new x({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L677"}}),Ac=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForMultipleChoice.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMultipleChoice.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2ForMultipleChoice">TFDebertaV2ForMultipleChoice</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForMultipleChoice">TFDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForMultipleChoice">TFElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertForMultipleChoice">TFFlaubertForMultipleChoice</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForMultipleChoice">TFFunnelForMultipleChoice</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerForMultipleChoice">TFLongformerForMultipleChoice</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForMultipleChoice">TFMPNetForMultipleChoice</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForMultipleChoice">TFMobileBertForMultipleChoice</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForMultipleChoice">TFRemBertForMultipleChoice</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForMultipleChoice">TFRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForMultipleChoice">TFRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForMultipleChoice">TFRobertaPreLayerNormForMultipleChoice</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMForMultipleChoice">TFXLMForMultipleChoice</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForMultipleChoice">TFXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetForMultipleChoice">TFXLNetForMultipleChoice</a> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ti=new $({props:{anchor:"transformers.TFAutoModelForMultipleChoice.from_config.example",$$slots:{default:[lq]},$$scope:{ctx:y}}}),Rc=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ci=new $({props:{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.example",$$slots:{default:[dq]},$$scope:{ctx:y}}}),Pc=new B({props:{title:"FlaxAutoModelForMultipleChoice",local:"transformers.FlaxAutoModelForMultipleChoice",headingTag:"h3"}}),Wc=new x({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L342"}}),Sc=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForMultipleChoice.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForMultipleChoice">FlaxDistilBertForMultipleChoice</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForMultipleChoice">FlaxElectraForMultipleChoice</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForMultipleChoice">FlaxRoFormerForMultipleChoice</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForMultipleChoice">FlaxRobertaForMultipleChoice</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForMultipleChoice">FlaxRobertaPreLayerNormForMultipleChoice</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForMultipleChoice">FlaxXLMRobertaForMultipleChoice</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),yi=new $({props:{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_config.example",$$slots:{default:[mq]},$$scope:{ctx:y}}}),Gc=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),vi=new $({props:{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.example",$$slots:{default:[cq]},$$scope:{ctx:y}}}),Vc=new B({props:{title:"AutoModelForNextSentencePrediction",local:"transformers.AutoModelForNextSentencePrediction",headingTag:"h3"}}),Jc=new x({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1424"}}),Ec=new x({props:{name:"from_config",anchor:"transformers.AutoModelForNextSentencePrediction.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForNextSentencePrediction.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForNextSentencePrediction">ErnieForNextSentencePrediction</a> (ERNIE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForNextSentencePrediction">FNetForNextSentencePrediction</a> (FNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForNextSentencePrediction">MegatronBertForNextSentencePrediction</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForNextSentencePrediction">MobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForNextSentencePrediction">NezhaForNextSentencePrediction</a> (Nezha model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForNextSentencePrediction">QDQBertForNextSentencePrediction</a> (QDQBert model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),wi=new $({props:{anchor:"transformers.AutoModelForNextSentencePrediction.from_config.example",$$slots:{default:[fq]},$$scope:{ctx:y}}}),Ic=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),xi=new $({props:{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.example",$$slots:{default:[gq]},$$scope:{ctx:y}}}),Uc=new B({props:{title:"TFAutoModelForNextSentencePrediction",local:"transformers.TFAutoModelForNextSentencePrediction",headingTag:"h3"}}),Nc=new x({props:{name:"class transformers.TFAutoModelForNextSentencePrediction",anchor:"transformers.TFAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L684"}}),Xc=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForNextSentencePrediction.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForNextSentencePrediction">TFBertForNextSentencePrediction</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForNextSentencePrediction">TFMobileBertForNextSentencePrediction</a> (MobileBERT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ki=new $({props:{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_config.example",$$slots:{default:[pq]},$$scope:{ctx:y}}}),qc=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Li=new $({props:{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.example",$$slots:{default:[hq]},$$scope:{ctx:y}}}),Qc=new B({props:{title:"FlaxAutoModelForNextSentencePrediction",local:"transformers.FlaxAutoModelForNextSentencePrediction",headingTag:"h3"}}),Dc=new x({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L349"}}),Hc=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),$i=new $({props:{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_config.example",$$slots:{default:[uq]},$$scope:{ctx:y}}}),Yc=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Bi=new $({props:{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.example",$$slots:{default:[_q]},$$scope:{ctx:y}}}),zc=new B({props:{title:"AutoModelForTokenClassification",local:"transformers.AutoModelForTokenClassification",headingTag:"h3"}}),Oc=new x({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1410"}}),Kc=new x({props:{name:"from_config",anchor:"transformers.AutoModelForTokenClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTokenClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptForTokenClassification">BioGptForTokenClassification</a> (BioGpt model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomForTokenClassification">BloomForTokenClassification</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bros#transformers.BrosConfig">BrosConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bros#transformers.BrosForTokenClassification">BrosForTokenClassification</a> (BROS model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (CANINE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForTokenClassification">DistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForTokenClassification">ElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForTokenClassification">ErnieForTokenClassification</a> (ERNIE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMForTokenClassification">ErnieMForTokenClassification</a> (ErnieM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmForTokenClassification">EsmForTokenClassification</a> (ESM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForTokenClassification">FNetForTokenClassification</a> (FNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconForTokenClassification">FalconForTokenClassification</a> (Falcon model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForTokenClassification">FlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForTokenClassification">FunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForTokenClassification">GPT2ForTokenClassification</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeConfig">GPTBigCodeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_bigcode#transformers.GPTBigCodeForTokenClassification">GPTBigCodeForTokenClassification</a> (GPTBigCode model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForTokenClassification">GPTNeoForTokenClassification</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForTokenClassification">GPTNeoXForTokenClassification</a> (GPT NeoX model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForTokenClassification">IBertForTokenClassification</a> (I-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForTokenClassification">LayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForTokenClassification">LayoutLMv2ForTokenClassification</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForTokenClassification">LayoutLMv3ForTokenClassification</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig">LiltConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltForTokenClassification">LiltForTokenClassification</a> (LiLT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForTokenClassification">LongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForTokenClassification">LukeForTokenClassification</a> (LUKE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForTokenClassification">MPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForTokenClassification">MT5ForTokenClassification</a> (MT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig">MarkupLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForTokenClassification">MarkupLMForTokenClassification</a> (MarkupLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForTokenClassification">MegaForTokenClassification</a> (MEGA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForTokenClassification">MegatronBertForTokenClassification</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForTokenClassification">MobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptForTokenClassification">MptForTokenClassification</a> (MPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForTokenClassification">MraForTokenClassification</a> (MRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForTokenClassification">NezhaForTokenClassification</a> (Nezha model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForTokenClassification">NystromformerForTokenClassification</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiConfig">PhiConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/phi#transformers.PhiForTokenClassification">PhiForTokenClassification</a> (Phi model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForTokenClassification">QDQBertForTokenClassification</a> (QDQBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForTokenClassification">RemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForTokenClassification">RoCBertForTokenClassification</a> (RoCBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForTokenClassification">RoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForTokenClassification">RobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForTokenClassification">RobertaPreLayerNormForTokenClassification</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForTokenClassification">SqueezeBertForTokenClassification</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5ForTokenClassification">T5ForTokenClassification</a> (T5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForTokenClassification">UMT5ForTokenClassification</a> (UMT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMForTokenClassification">XLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForTokenClassification">XLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForTokenClassification">XLMRobertaXLForTokenClassification</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForTokenClassification">XLNetForTokenClassification</a> (XLNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForTokenClassification">XmodForTokenClassification</a> (X-MOD model)</li>
<li><a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoForTokenClassification">YosoForTokenClassification</a> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ji=new $({props:{anchor:"transformers.AutoModelForTokenClassification.from_config.example",$$slots:{default:[bq]},$$scope:{ctx:y}}}),ef=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForTokenClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Zi=new $({props:{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.example",$$slots:{default:[Mq]},$$scope:{ctx:y}}}),of=new B({props:{title:"TFAutoModelForTokenClassification",local:"transformers.TFAutoModelForTokenClassification",headingTag:"h3"}}),rf=new x({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L668"}}),nf=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForTokenClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForTokenClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForTokenClassification">TFDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForTokenClassification">TFElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/esm#transformers.EsmConfig">EsmConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/esm#transformers.TFEsmForTokenClassification">TFEsmForTokenClassification</a> (ESM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertForTokenClassification">TFFlaubertForTokenClassification</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForTokenClassification">TFFunnelForTokenClassification</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMForTokenClassification">TFLayoutLMForTokenClassification</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForTokenClassification">TFLayoutLMv3ForTokenClassification</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerForTokenClassification">TFLongformerForTokenClassification</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForTokenClassification">TFMPNetForTokenClassification</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForTokenClassification">TFMobileBertForTokenClassification</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForTokenClassification">TFRemBertForTokenClassification</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForTokenClassification">TFRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForTokenClassification">TFRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForTokenClassification">TFRobertaPreLayerNormForTokenClassification</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMForTokenClassification">TFXLMForTokenClassification</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForTokenClassification">TFXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetForTokenClassification">TFXLNetForTokenClassification</a> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ai=new $({props:{anchor:"transformers.TFAutoModelForTokenClassification.from_config.example",$$slots:{default:[Fq]},$$scope:{ctx:y}}}),tf=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ri=new $({props:{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.example",$$slots:{default:[Tq]},$$scope:{ctx:y}}}),af=new B({props:{title:"FlaxAutoModelForTokenClassification",local:"transformers.FlaxAutoModelForTokenClassification",headingTag:"h3"}}),sf=new x({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L333"}}),lf=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForTokenClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForTokenClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForTokenClassification">FlaxDistilBertForTokenClassification</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForTokenClassification">FlaxElectraForTokenClassification</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForTokenClassification">FlaxRoFormerForTokenClassification</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForTokenClassification">FlaxRobertaForTokenClassification</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForTokenClassification">FlaxRobertaPreLayerNormForTokenClassification</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForTokenClassification">FlaxXLMRobertaForTokenClassification</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Pi=new $({props:{anchor:"transformers.FlaxAutoModelForTokenClassification.from_config.example",$$slots:{default:[Cq]},$$scope:{ctx:y}}}),df=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Wi=new $({props:{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.example",$$slots:{default:[yq]},$$scope:{ctx:y}}}),mf=new B({props:{title:"AutoModelForQuestionAnswering",local:"transformers.AutoModelForQuestionAnswering",headingTag:"h3"}}),cf=new x({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1370"}}),ff=new x({props:{name:"from_config",anchor:"transformers.AutoModelForQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bloom#transformers.BloomForQuestionAnswering">BloomForQuestionAnswering</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (CANINE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertForQuestionAnswering">DistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraForQuestionAnswering">ElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieConfig">ErnieConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie#transformers.ErnieForQuestionAnswering">ErnieForQuestionAnswering</a> (ERNIE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMConfig">ErnieMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ernie_m#transformers.ErnieMForQuestionAnswering">ErnieMForQuestionAnswering</a> (ErnieM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetConfig">FNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/fnet#transformers.FNetForQuestionAnswering">FNetForQuestionAnswering</a> (FNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconConfig">FalconConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/falcon#transformers.FalconForQuestionAnswering">FalconForQuestionAnswering</a> (Falcon model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertForQuestionAnsweringSimple">FlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelForQuestionAnswering">FunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2Config">GPT2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt2#transformers.GPT2ForQuestionAnswering">GPT2ForQuestionAnswering</a> (OpenAI GPT-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJForQuestionAnswering">GPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoConfig">GPTNeoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neo#transformers.GPTNeoForQuestionAnswering">GPTNeoForQuestionAnswering</a> (GPT Neo model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXConfig">GPTNeoXConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXForQuestionAnswering">GPTNeoXForQuestionAnswering</a> (GPT NeoX model)</li>
<li><a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertConfig">IBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/ibert#transformers.IBertForQuestionAnswering">IBertForQuestionAnswering</a> (I-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/led#transformers.LEDConfig">LEDConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/led#transformers.LEDForQuestionAnswering">LEDForQuestionAnswering</a> (LED model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering">LayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltConfig">LiltConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/lilt#transformers.LiltForQuestionAnswering">LiltForQuestionAnswering</a> (LiLT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaConfig">LlamaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/llama#transformers.LlamaForQuestionAnswering">LlamaForQuestionAnswering</a> (LLaMA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerForQuestionAnswering">LongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeConfig">LukeConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/luke#transformers.LukeForQuestionAnswering">LukeForQuestionAnswering</a> (LUKE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertConfig">LxmertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/lxmert#transformers.LxmertForQuestionAnswering">LxmertForQuestionAnswering</a> (LXMERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartForQuestionAnswering">MBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetForQuestionAnswering">MPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5Config">MT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mt5#transformers.MT5ForQuestionAnswering">MT5ForQuestionAnswering</a> (MT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMConfig">MarkupLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/markuplm#transformers.MarkupLMForQuestionAnswering">MarkupLMForQuestionAnswering</a> (MarkupLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaConfig">MegaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mega#transformers.MegaForQuestionAnswering">MegaForQuestionAnswering</a> (MEGA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertConfig">MegatronBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/megatron-bert#transformers.MegatronBertForQuestionAnswering">MegatronBertForQuestionAnswering</a> (Megatron-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertForQuestionAnswering">MobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptConfig">MptConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpt#transformers.MptForQuestionAnswering">MptForQuestionAnswering</a> (MPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mra#transformers.MraConfig">MraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mra#transformers.MraForQuestionAnswering">MraForQuestionAnswering</a> (MRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpConfig">MvpConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mvp#transformers.MvpForQuestionAnswering">MvpForQuestionAnswering</a> (MVP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaConfig">NezhaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nezha#transformers.NezhaForQuestionAnswering">NezhaForQuestionAnswering</a> (Nezha model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerConfig">NystromformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nystromformer#transformers.NystromformerForQuestionAnswering">NystromformerForQuestionAnswering</a> (Nystr&#xF6;mformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTConfig">OPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/opt#transformers.OPTForQuestionAnswering">OPTForQuestionAnswering</a> (OPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertConfig">QDQBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/qdqbert#transformers.QDQBertForQuestionAnswering">QDQBertForQuestionAnswering</a> (QDQBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerConfig">ReformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/reformer#transformers.ReformerForQuestionAnswering">ReformerForQuestionAnswering</a> (Reformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertForQuestionAnswering">RemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertConfig">RoCBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roc_bert#transformers.RoCBertForQuestionAnswering">RoCBertForQuestionAnswering</a> (RoCBert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerForQuestionAnswering">RoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaForQuestionAnswering">RobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormForQuestionAnswering">RobertaPreLayerNormForQuestionAnswering</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterConfig">SplinterConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/splinter#transformers.SplinterForQuestionAnswering">SplinterForQuestionAnswering</a> (Splinter model)</li>
<li><a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertConfig">SqueezeBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/squeezebert#transformers.SqueezeBertForQuestionAnswering">SqueezeBertForQuestionAnswering</a> (SqueezeBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/t5#transformers.T5Config">T5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/t5#transformers.T5ForQuestionAnswering">T5ForQuestionAnswering</a> (T5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5Config">UMT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/umt5#transformers.UMT5ForQuestionAnswering">UMT5ForQuestionAnswering</a> (UMT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMForQuestionAnsweringSimple">XLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaForQuestionAnswering">XLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLConfig">XLMRobertaXLConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta-xl#transformers.XLMRobertaXLForQuestionAnswering">XLMRobertaXLForQuestionAnswering</a> (XLM-RoBERTa-XL model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetForQuestionAnsweringSimple">XLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodConfig">XmodConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xmod#transformers.XmodForQuestionAnswering">XmodForQuestionAnswering</a> (X-MOD model)</li>
<li><a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoConfig">YosoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/yoso#transformers.YosoForQuestionAnswering">YosoForQuestionAnswering</a> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Si=new $({props:{anchor:"transformers.AutoModelForQuestionAnswering.from_config.example",$$slots:{default:[vq]},$$scope:{ctx:y}}}),gf=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Gi=new $({props:{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.example",$$slots:{default:[wq]},$$scope:{ctx:y}}}),pf=new B({props:{title:"TFAutoModelForQuestionAnswering",local:"transformers.TFAutoModelForQuestionAnswering",headingTag:"h3"}}),hf=new x({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L639"}}),uf=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.TFDistilBertForQuestionAnswering">TFDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.TFElectraForQuestionAnswering">TFElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/flaubert#transformers.FlaubertConfig">FlaubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/flaubert#transformers.TFFlaubertForQuestionAnsweringSimple">TFFlaubertForQuestionAnsweringSimple</a> (FlauBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/funnel#transformers.FunnelConfig">FunnelConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/funnel#transformers.TFFunnelForQuestionAnswering">TFFunnelForQuestionAnswering</a> (Funnel Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/gptj#transformers.GPTJConfig">GPTJConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/gptj#transformers.TFGPTJForQuestionAnswering">TFGPTJForQuestionAnswering</a> (GPT-J model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForQuestionAnswering">TFLayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/longformer#transformers.LongformerConfig">LongformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/longformer#transformers.TFLongformerForQuestionAnswering">TFLongformerForQuestionAnswering</a> (Longformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mpnet#transformers.MPNetConfig">MPNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mpnet#transformers.TFMPNetForQuestionAnswering">TFMPNetForQuestionAnswering</a> (MPNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.MobileBertConfig">MobileBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilebert#transformers.TFMobileBertForQuestionAnswering">TFMobileBertForQuestionAnswering</a> (MobileBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/rembert#transformers.RemBertConfig">RemBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/rembert#transformers.TFRemBertForQuestionAnswering">TFRemBertForQuestionAnswering</a> (RemBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.TFRoFormerForQuestionAnswering">TFRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.TFRobertaForQuestionAnswering">TFRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.TFRobertaPreLayerNormForQuestionAnswering">TFRobertaPreLayerNormForQuestionAnswering</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm#transformers.XLMConfig">XLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm#transformers.TFXLMForQuestionAnsweringSimple">TFXLMForQuestionAnsweringSimple</a> (XLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.TFXLMRobertaForQuestionAnswering">TFXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetConfig">XLNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.TFXLNetForQuestionAnsweringSimple">TFXLNetForQuestionAnsweringSimple</a> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Vi=new $({props:{anchor:"transformers.TFAutoModelForQuestionAnswering.from_config.example",$$slots:{default:[xq]},$$scope:{ctx:y}}}),_f=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ji=new $({props:{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.example",$$slots:{default:[kq]},$$scope:{ctx:y}}}),bf=new B({props:{title:"FlaxAutoModelForQuestionAnswering",local:"transformers.FlaxAutoModelForQuestionAnswering",headingTag:"h3"}}),Mf=new x({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L326"}}),Ff=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/en/model_doc/distilbert#transformers.DistilBertConfig">DistilBertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/distilbert#transformers.FlaxDistilBertForQuestionAnswering">FlaxDistilBertForQuestionAnswering</a> (DistilBERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/electra#transformers.ElectraConfig">ElectraConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/electra#transformers.FlaxElectraForQuestionAnswering">FlaxElectraForQuestionAnswering</a> (ELECTRA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mbart#transformers.MBartConfig">MBartConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mbart#transformers.FlaxMBartForQuestionAnswering">FlaxMBartForQuestionAnswering</a> (mBART model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roformer#transformers.RoFormerConfig">RoFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roformer#transformers.FlaxRoFormerForQuestionAnswering">FlaxRoFormerForQuestionAnswering</a> (RoFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaConfig">RobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta#transformers.FlaxRobertaForQuestionAnswering">FlaxRobertaForQuestionAnswering</a> (RoBERTa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.RobertaPreLayerNormConfig">RobertaPreLayerNormConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/roberta-prelayernorm#transformers.FlaxRobertaPreLayerNormForQuestionAnswering">FlaxRobertaPreLayerNormForQuestionAnswering</a> (RoBERTa-PreLayerNorm model)</li>
<li><a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.XLMRobertaConfig">XLMRobertaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/xlm-roberta#transformers.FlaxXLMRobertaForQuestionAnswering">FlaxXLMRobertaForQuestionAnswering</a> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ei=new $({props:{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_config.example",$$slots:{default:[Lq]},$$scope:{ctx:y}}}),Tf=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ii=new $({props:{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.example",$$slots:{default:[$q]},$$scope:{ctx:y}}}),Cf=new B({props:{title:"AutoModelForTextEncoding",local:"transformers.AutoModelForTextEncoding",headingTag:"h3"}}),vf=new x({props:{name:"class transformers.AutoModelForTextEncoding",anchor:"transformers.AutoModelForTextEncoding",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1306"}}),wf=new B({props:{title:"TFAutoModelForTextEncoding",local:"transformers.TFAutoModelForTextEncoding",headingTag:"h3"}}),kf=new x({props:{name:"class transformers.TFAutoModelForTextEncoding",anchor:"transformers.TFAutoModelForTextEncoding",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L527"}}),Lf=new B({props:{title:"Computer vision",local:"computer-vision",headingTag:"h2"}}),Bf=new B({props:{title:"AutoModelForDepthEstimation",local:"transformers.AutoModelForDepthEstimation",headingTag:"h3"}}),jf=new x({props:{name:"class transformers.AutoModelForDepthEstimation",anchor:"transformers.AutoModelForDepthEstimation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1499"}}),Zf=new x({props:{name:"from_config",anchor:"transformers.AutoModelForDepthEstimation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDepthEstimation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTForDepthEstimation">DPTForDepthEstimation</a> (DPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingConfig">DepthAnythingConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/depth_anything#transformers.DepthAnythingForDepthEstimation">DepthAnythingForDepthEstimation</a> (Depth Anything model)</li>
<li><a href="/docs/transformers/main/en/model_doc/glpn#transformers.GLPNConfig">GLPNConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/glpn#transformers.GLPNForDepthEstimation">GLPNForDepthEstimation</a> (GLPN model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ui=new $({props:{anchor:"transformers.AutoModelForDepthEstimation.from_config.example",$$slots:{default:[Bq]},$$scope:{ctx:y}}}),Af=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForDepthEstimation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ni=new $({props:{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.example",$$slots:{default:[jq]},$$scope:{ctx:y}}}),Rf=new B({props:{title:"AutoModelForImageClassification",local:"transformers.AutoModelForImageClassification",headingTag:"h3"}}),Pf=new x({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1433"}}),Wf=new x({props:{name:"from_config",anchor:"transformers.AutoModelForImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/bit#transformers.BitConfig">BitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/bit#transformers.BitForImageClassification">BitForImageClassification</a> (BiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPForImageClassification">CLIPForImageClassification</a> (CLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNeXT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification">ConvNextV2ForImageClassification</a> (ConvNeXTV2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/cvt#transformers.CvtForImageClassification">CvtForImageClassification</a> (CvT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionForImageClassification">Data2VecVisionForImageClassification</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/dinat#transformers.DinatConfig">DinatConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/dinat#transformers.DinatForImageClassification">DinatForImageClassification</a> (DiNAT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2Config">Dinov2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/dinov2#transformers.Dinov2ForImageClassification">Dinov2ForImageClassification</a> (DINOv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerConfig">EfficientFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassification">EfficientFormerForImageClassification</a> or <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerForImageClassificationWithTeacher">EfficientFormerForImageClassificationWithTeacher</a> (EfficientFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetConfig">EfficientNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/efficientnet#transformers.EfficientNetForImageClassification">EfficientNetForImageClassification</a> (EfficientNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForImageClassification">FocalNetForImageClassification</a> (FocalNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTConfig">ImageGPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/imagegpt#transformers.ImageGPTForImageClassification">ImageGPTForImageClassification</a> (ImageGPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitConfig">LevitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitForImageClassification">LevitForImageClassification</a> or <a href="/docs/transformers/main/en/model_doc/levit#transformers.LevitForImageClassificationWithTeacher">LevitForImageClassificationWithTeacher</a> (LeViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1Config">MobileNetV1Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilenet_v1#transformers.MobileNetV1ForImageClassification">MobileNetV1ForImageClassification</a> (MobileNetV1 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config">MobileNetV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForImageClassification">MobileNetV2ForImageClassification</a> (MobileNetV2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTForImageClassification">MobileViTForImageClassification</a> (MobileViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForImageClassification">MobileViTV2ForImageClassification</a> (MobileViTV2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/nat#transformers.NatConfig">NatConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/nat#transformers.NatForImageClassification">NatForImageClassification</a> (NAT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverConfig">PerceiverConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationLearned">PerceiverForImageClassificationLearned</a> or <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationFourier">PerceiverForImageClassificationFourier</a> or <a href="/docs/transformers/main/en/model_doc/perceiver#transformers.PerceiverForImageClassificationConvProcessing">PerceiverForImageClassificationConvProcessing</a> (Perceiver model)</li>
<li><a href="/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerConfig">PoolFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/poolformer#transformers.PoolFormerForImageClassification">PoolFormerForImageClassification</a> (PoolFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pvt#transformers.PvtConfig">PvtConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pvt#transformers.PvtForImageClassification">PvtForImageClassification</a> (PVT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig">RegNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetForImageClassification">RegNetForImageClassification</a> (RegNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetForImageClassification">ResNetForImageClassification</a> (ResNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForImageClassification">SegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig">SiglipConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipForImageClassification">SiglipForImageClassification</a> (SigLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig">SwiftFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerForImageClassification">SwiftFormerForImageClassification</a> (SwiftFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinForImageClassification">SwinForImageClassification</a> (Swin Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForImageClassification">Swinv2ForImageClassification</a> (Swin Transformer V2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/van#transformers.VanConfig">VanConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/van#transformers.VanForImageClassification">VanForImageClassification</a> (VAN model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTForImageClassification">ViTForImageClassification</a> (ViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit_hybrid#transformers.ViTHybridConfig">ViTHybridConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit_hybrid#transformers.ViTHybridForImageClassification">ViTHybridForImageClassification</a> (ViT Hybrid model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig">ViTMSNConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification">ViTMSNForImageClassification</a> (ViTMSN model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Xi=new $({props:{anchor:"transformers.AutoModelForImageClassification.from_config.example",$$slots:{default:[Zq]},$$scope:{ctx:y}}}),Sf=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),qi=new $({props:{anchor:"transformers.AutoModelForImageClassification.from_pretrained.example",$$slots:{default:[Aq]},$$scope:{ctx:y}}}),Gf=new B({props:{title:"TFAutoModelForImageClassification",local:"transformers.TFAutoModelForImageClassification",headingTag:"h3"}}),Vf=new x({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L578"}}),Jf=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNeXT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/convnextv2#transformers.TFConvNextV2ForImageClassification">TFConvNextV2ForImageClassification</a> (ConvNeXTV2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/cvt#transformers.TFCvtForImageClassification">TFCvtForImageClassification</a> (CvT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.TFData2VecVisionForImageClassification">TFData2VecVisionForImageClassification</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deit#transformers.TFDeiTForImageClassification">TFDeiTForImageClassification</a> or <a href="/docs/transformers/main/en/model_doc/deit#transformers.TFDeiTForImageClassificationWithTeacher">TFDeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.EfficientFormerConfig">EfficientFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.TFEfficientFormerForImageClassification">TFEfficientFormerForImageClassification</a> or <a href="/docs/transformers/main/en/model_doc/efficientformer#transformers.TFEfficientFormerForImageClassificationWithTeacher">TFEfficientFormerForImageClassificationWithTeacher</a> (EfficientFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.TFMobileViTForImageClassification">TFMobileViTForImageClassification</a> (MobileViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig">RegNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/regnet#transformers.TFRegNetForImageClassification">TFRegNetForImageClassification</a> (RegNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/resnet#transformers.TFResNetForImageClassification">TFResNetForImageClassification</a> (ResNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/segformer#transformers.TFSegformerForImageClassification">TFSegformerForImageClassification</a> (SegFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swin#transformers.TFSwinForImageClassification">TFSwinForImageClassification</a> (Swin Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit#transformers.TFViTForImageClassification">TFViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Qi=new $({props:{anchor:"transformers.TFAutoModelForImageClassification.from_config.example",$$slots:{default:[Rq]},$$scope:{ctx:y}}}),Ef=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Di=new $({props:{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.example",$$slots:{default:[Pq]},$$scope:{ctx:y}}}),If=new B({props:{title:"FlaxAutoModelForImageClassification",local:"transformers.FlaxAutoModelForImageClassification",headingTag:"h3"}}),Uf=new x({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L358"}}),Nf=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/regnet#transformers.RegNetConfig">RegNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/regnet#transformers.FlaxRegNetForImageClassification">FlaxRegNetForImageClassification</a> (RegNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/resnet#transformers.ResNetConfig">ResNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/resnet#transformers.FlaxResNetForImageClassification">FlaxResNetForImageClassification</a> (ResNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit#transformers.FlaxViTForImageClassification">FlaxViTForImageClassification</a> (ViT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Hi=new $({props:{anchor:"transformers.FlaxAutoModelForImageClassification.from_config.example",$$slots:{default:[Wq]},$$scope:{ctx:y}}}),Xf=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Yi=new $({props:{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.example",$$slots:{default:[Sq]},$$scope:{ctx:y}}}),qf=new B({props:{title:"AutoModelForVideoClassification",local:"transformers.AutoModelForVideoClassification",headingTag:"h3"}}),Qf=new x({props:{name:"class transformers.AutoModelForVideoClassification",anchor:"transformers.AutoModelForVideoClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1506"}}),Df=new x({props:{name:"from_config",anchor:"transformers.AutoModelForVideoClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVideoClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerConfig">TimesformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/timesformer#transformers.TimesformerForVideoClassification">TimesformerForVideoClassification</a> (TimeSformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEConfig">VideoMAEConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/videomae#transformers.VideoMAEForVideoClassification">VideoMAEForVideoClassification</a> (VideoMAE model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vivit#transformers.VivitConfig">VivitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vivit#transformers.VivitForVideoClassification">VivitForVideoClassification</a> (ViViT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),zi=new $({props:{anchor:"transformers.AutoModelForVideoClassification.from_config.example",$$slots:{default:[Gq]},$$scope:{ctx:y}}}),Hf=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForVideoClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Oi=new $({props:{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.example",$$slots:{default:[Vq]},$$scope:{ctx:y}}}),Yf=new B({props:{title:"AutoModelForMaskedImageModeling",local:"transformers.AutoModelForMaskedImageModeling",headingTag:"h3"}}),zf=new x({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1571"}}),Of=new x({props:{name:"from_config",anchor:"transformers.AutoModelForMaskedImageModeling.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedImageModeling.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetConfig">FocalNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/focalnet#transformers.FocalNetForMaskedImageModeling">FocalNetForMaskedImageModeling</a> (FocalNet model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinForMaskedImageModeling">SwinForMaskedImageModeling</a> (Swin Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling">Swinv2ForMaskedImageModeling</a> (Swin Transformer V2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTConfig">ViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vit#transformers.ViTForMaskedImageModeling">ViTForMaskedImageModeling</a> (ViT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ki=new $({props:{anchor:"transformers.AutoModelForMaskedImageModeling.from_config.example",$$slots:{default:[Jq]},$$scope:{ctx:y}}}),Kf=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),el=new $({props:{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.example",$$slots:{default:[Eq]},$$scope:{ctx:y}}}),eg=new B({props:{title:"TFAutoModelForMaskedImageModeling",local:"transformers.TFAutoModelForMaskedImageModeling",headingTag:"h3"}}),og=new x({props:{name:"class transformers.TFAutoModelForMaskedImageModeling",anchor:"transformers.TFAutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L569"}}),rg=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForMaskedImageModeling.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deit#transformers.TFDeiTForMaskedImageModeling">TFDeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/swin#transformers.SwinConfig">SwinConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/swin#transformers.TFSwinForMaskedImageModeling">TFSwinForMaskedImageModeling</a> (Swin Transformer model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ol=new $({props:{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_config.example",$$slots:{default:[Iq]},$$scope:{ctx:y}}}),ng=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),rl=new $({props:{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.example",$$slots:{default:[Uq]},$$scope:{ctx:y}}}),tg=new B({props:{title:"AutoModelForObjectDetection",local:"transformers.AutoModelForObjectDetection",headingTag:"h3"}}),ag=new x({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1483"}}),sg=new x({props:{name:"from_config",anchor:"transformers.AutoModelForObjectDetection.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForObjectDetection.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrConfig">ConditionalDetrConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection">ConditionalDetrForObjectDetection</a> (Conditional DETR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrConfig">DeformableDetrConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deformable_detr#transformers.DeformableDetrForObjectDetection">DeformableDetrForObjectDetection</a> (Deformable DETR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/deta#transformers.DetaConfig">DetaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/deta#transformers.DetaForObjectDetection">DetaForObjectDetection</a> (DETA model)</li>
<li><a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerConfig">TableTransformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/table-transformer#transformers.TableTransformerForObjectDetection">TableTransformerForObjectDetection</a> (Table Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/yolos#transformers.YolosConfig">YolosConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/yolos#transformers.YolosForObjectDetection">YolosForObjectDetection</a> (YOLOS model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),nl=new $({props:{anchor:"transformers.AutoModelForObjectDetection.from_config.example",$$slots:{default:[Nq]},$$scope:{ctx:y}}}),ig=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForObjectDetection.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),tl=new $({props:{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.example",$$slots:{default:[Xq]},$$scope:{ctx:y}}}),lg=new B({props:{title:"AutoModelForImageSegmentation",local:"transformers.AutoModelForImageSegmentation",headingTag:"h3"}}),dg=new x({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1449"}}),mg=new x({props:{name:"from_config",anchor:"transformers.AutoModelForImageSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),al=new $({props:{anchor:"transformers.AutoModelForImageSegmentation.from_config.example",$$slots:{default:[qq]},$$scope:{ctx:y}}}),cg=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForImageSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),sl=new $({props:{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.example",$$slots:{default:[Qq]},$$scope:{ctx:y}}}),fg=new B({props:{title:"AutoModelForImageToImage",local:"transformers.AutoModelForImageToImage",headingTag:"h3"}}),pg=new x({props:{name:"class transformers.AutoModelForImageToImage",anchor:"transformers.AutoModelForImageToImage",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1310"}}),hg=new B({props:{title:"AutoModelForSemanticSegmentation",local:"transformers.AutoModelForSemanticSegmentation",headingTag:"h3"}}),ug=new x({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1456"}}),_g=new x({props:{name:"from_config",anchor:"transformers.AutoModelForSemanticSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSemanticSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTConfig">DPTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/dpt#transformers.DPTForSemanticSegmentation">DPTForSemanticSegmentation</a> (DPT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionForSemanticSegmentation">Data2VecVisionForSemanticSegmentation</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2Config">MobileNetV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilenet_v2#transformers.MobileNetV2ForSemanticSegmentation">MobileNetV2ForSemanticSegmentation</a> (MobileNetV2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTForSemanticSegmentation">MobileViTForSemanticSegmentation</a> (MobileViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2Config">MobileViTV2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilevitv2#transformers.MobileViTV2ForSemanticSegmentation">MobileViTV2ForSemanticSegmentation</a> (MobileViTV2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerForSemanticSegmentation">SegformerForSemanticSegmentation</a> (SegFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/upernet#transformers.UperNetConfig">UperNetConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a> (UPerNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),il=new $({props:{anchor:"transformers.AutoModelForSemanticSegmentation.from_config.example",$$slots:{default:[Dq]},$$scope:{ctx:y}}}),bg=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ll=new $({props:{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.example",$$slots:{default:[Hq]},$$scope:{ctx:y}}}),Mg=new B({props:{title:"TFAutoModelForSemanticSegmentation",local:"transformers.TFAutoModelForSemanticSegmentation",headingTag:"h3"}}),Fg=new x({props:{name:"class transformers.TFAutoModelForSemanticSegmentation",anchor:"transformers.TFAutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L596"}}),Tg=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForSemanticSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.TFData2VecVisionForSemanticSegmentation">TFData2VecVisionForSemanticSegmentation</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.MobileViTConfig">MobileViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mobilevit#transformers.TFMobileViTForSemanticSegmentation">TFMobileViTForSemanticSegmentation</a> (MobileViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerConfig">SegformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/segformer#transformers.TFSegformerForSemanticSegmentation">TFSegformerForSemanticSegmentation</a> (SegFormer model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),dl=new $({props:{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_config.example",$$slots:{default:[Yq]},$$scope:{ctx:y}}}),Cg=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ml=new $({props:{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.example",$$slots:{default:[zq]},$$scope:{ctx:y}}}),yg=new B({props:{title:"AutoModelForInstanceSegmentation",local:"transformers.AutoModelForInstanceSegmentation",headingTag:"h3"}}),vg=new x({props:{name:"class transformers.AutoModelForInstanceSegmentation",anchor:"transformers.AutoModelForInstanceSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1474"}}),wg=new x({props:{name:"from_config",anchor:"transformers.AutoModelForInstanceSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForInstanceSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation">MaskFormerForInstanceSegmentation</a> (MaskFormer model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),cl=new $({props:{anchor:"transformers.AutoModelForInstanceSegmentation.from_config.example",$$slots:{default:[Oq]},$$scope:{ctx:y}}}),xg=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),fl=new $({props:{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.example",$$slots:{default:[Kq]},$$scope:{ctx:y}}}),kg=new B({props:{title:"AutoModelForUniversalSegmentation",local:"transformers.AutoModelForUniversalSegmentation",headingTag:"h3"}}),Lg=new x({props:{name:"class transformers.AutoModelForUniversalSegmentation",anchor:"transformers.AutoModelForUniversalSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1465"}}),$g=new x({props:{name:"from_config",anchor:"transformers.AutoModelForUniversalSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForUniversalSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerConfig">Mask2FormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mask2former#transformers.Mask2FormerForUniversalSegmentation">Mask2FormerForUniversalSegmentation</a> (Mask2Former model)</li>
<li><a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerConfig">MaskFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/maskformer#transformers.MaskFormerForInstanceSegmentation">MaskFormerForInstanceSegmentation</a> (MaskFormer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerConfig">OneFormerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/oneformer#transformers.OneFormerForUniversalSegmentation">OneFormerForUniversalSegmentation</a> (OneFormer model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),gl=new $({props:{anchor:"transformers.AutoModelForUniversalSegmentation.from_config.example",$$slots:{default:[e3]},$$scope:{ctx:y}}}),Bg=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),pl=new $({props:{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.example",$$slots:{default:[o3]},$$scope:{ctx:y}}}),jg=new B({props:{title:"AutoModelForZeroShotImageClassification",local:"transformers.AutoModelForZeroShotImageClassification",headingTag:"h3"}}),Zg=new x({props:{name:"class transformers.AutoModelForZeroShotImageClassification",anchor:"transformers.AutoModelForZeroShotImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1440"}}),Ag=new x({props:{name:"from_config",anchor:"transformers.AutoModelForZeroShotImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/align#transformers.AlignConfig">AlignConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li>
<li><a href="/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPConfig">AltCLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li>
<li><a href="/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPConfig">ChineseCLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipConfig">SiglipConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/siglip#transformers.SiglipModel">SiglipModel</a> (SigLIP model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),hl=new $({props:{anchor:"transformers.AutoModelForZeroShotImageClassification.from_config.example",$$slots:{default:[r3]},$$scope:{ctx:y}}}),Rg=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ul=new $({props:{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.example",$$slots:{default:[n3]},$$scope:{ctx:y}}}),Pg=new B({props:{title:"TFAutoModelForZeroShotImageClassification",local:"transformers.TFAutoModelForZeroShotImageClassification",headingTag:"h3"}}),Wg=new x({props:{name:"class transformers.TFAutoModelForZeroShotImageClassification",anchor:"transformers.TFAutoModelForZeroShotImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L587"}}),Sg=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blip#transformers.TFBlipModel">TFBlipModel</a> (BLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),_l=new $({props:{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_config.example",$$slots:{default:[t3]},$$scope:{ctx:y}}}),Gg=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),bl=new $({props:{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.example",$$slots:{default:[a3]},$$scope:{ctx:y}}}),Vg=new B({props:{title:"AutoModelForZeroShotObjectDetection",local:"transformers.AutoModelForZeroShotObjectDetection",headingTag:"h3"}}),Jg=new x({props:{name:"class transformers.AutoModelForZeroShotObjectDetection",anchor:"transformers.AutoModelForZeroShotObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1490"}}),Eg=new x({props:{name:"from_config",anchor:"transformers.AutoModelForZeroShotObjectDetection.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTConfig">OwlViTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/owlvit#transformers.OwlViTForObjectDetection">OwlViTForObjectDetection</a> (OWL-ViT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2Config">Owlv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/owlv2#transformers.Owlv2ForObjectDetection">Owlv2ForObjectDetection</a> (OWLv2 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ml=new $({props:{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_config.example",$$slots:{default:[s3]},$$scope:{ctx:y}}}),Ig=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Fl=new $({props:{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.example",$$slots:{default:[i3]},$$scope:{ctx:y}}}),Ug=new B({props:{title:"Audio",local:"audio",headingTag:"h2"}}),Xg=new B({props:{title:"AutoModelForAudioClassification",local:"transformers.AutoModelForAudioClassification",headingTag:"h3"}}),qg=new x({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1520"}}),Qg=new x({props:{name:"from_config",anchor:"transformers.AutoModelForAudioClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification">ASTForAudioClassification</a> (Audio Spectrogram Transformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertForSequenceClassification">HubertForSequenceClassification</a> (Hubert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/sew#transformers.SEWForSequenceClassification">SEWForSequenceClassification</a> (SEW model)</li>
<li><a href="/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDForSequenceClassification">SEWDForSequenceClassification</a> (SEW-D model)</li>
<li><a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForSequenceClassification">UniSpeechForSequenceClassification</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForSequenceClassification">UniSpeechSatForSequenceClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForSequenceClassification">Wav2Vec2BertForSequenceClassification</a> (Wav2Vec2-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForSequenceClassification">Wav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForSequenceClassification">Wav2Vec2ConformerForSequenceClassification</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForSequenceClassification">WavLMForSequenceClassification</a> (WavLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForAudioClassification">WhisperForAudioClassification</a> (Whisper model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Tl=new $({props:{anchor:"transformers.AutoModelForAudioClassification.from_config.example",$$slots:{default:[l3]},$$scope:{ctx:y}}}),Dg=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForAudioClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Cl=new $({props:{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.example",$$slots:{default:[d3]},$$scope:{ctx:y}}}),Hg=new B({props:{title:"AutoModelForAudioFrameClassification",local:"transformers.TFAutoModelForAudioClassification",headingTag:"h3"}}),Yg=new x({props:{name:"class transformers.TFAutoModelForAudioClassification",anchor:"transformers.TFAutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L538"}}),zg=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForAudioClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForAudioClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.TFWav2Vec2ForSequenceClassification">TFWav2Vec2ForSequenceClassification</a> (Wav2Vec2 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),yl=new $({props:{anchor:"transformers.TFAutoModelForAudioClassification.from_config.example",$$slots:{default:[m3]},$$scope:{ctx:y}}}),Og=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),vl=new $({props:{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.example",$$slots:{default:[c3]},$$scope:{ctx:y}}}),Kg=new B({props:{title:"TFAutoModelForAudioFrameClassification",local:"transformers.AutoModelForAudioFrameClassification",headingTag:"h3"}}),ep=new x({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1543"}}),op=new x({props:{name:"from_config",anchor:"transformers.AutoModelForAudioFrameClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioFrameClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForAudioFrameClassification">UniSpeechSatForAudioFrameClassification</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForAudioFrameClassification">Wav2Vec2BertForAudioFrameClassification</a> (Wav2Vec2-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForAudioFrameClassification">Wav2Vec2ForAudioFrameClassification</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForAudioFrameClassification">Wav2Vec2ConformerForAudioFrameClassification</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForAudioFrameClassification">WavLMForAudioFrameClassification</a> (WavLM model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),wl=new $({props:{anchor:"transformers.AutoModelForAudioFrameClassification.from_config.example",$$slots:{default:[f3]},$$scope:{ctx:y}}}),rp=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),xl=new $({props:{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.example",$$slots:{default:[g3]},$$scope:{ctx:y}}}),np=new B({props:{title:"AutoModelForCTC",local:"transformers.AutoModelForCTC",headingTag:"h3"}}),tp=new x({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1527"}}),ap=new x({props:{name:"from_config",anchor:"transformers.AutoModelForCTC.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCTC.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertConfig">HubertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/hubert#transformers.HubertForCTC">HubertForCTC</a> (Hubert model)</li>
<li><a href="/docs/transformers/main/en/model_doc/mctct#transformers.MCTCTConfig">MCTCTConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/mctct#transformers.MCTCTForCTC">MCTCTForCTC</a> (M-CTC-T model)</li>
<li><a href="/docs/transformers/main/en/model_doc/sew#transformers.SEWConfig">SEWConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/sew#transformers.SEWForCTC">SEWForCTC</a> (SEW model)</li>
<li><a href="/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDConfig">SEWDConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/sew-d#transformers.SEWDForCTC">SEWDForCTC</a> (SEW-D model)</li>
<li><a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechConfig">UniSpeechConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/unispeech#transformers.UniSpeechForCTC">UniSpeechForCTC</a> (UniSpeech model)</li>
<li><a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForCTC">UniSpeechSatForCTC</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForCTC">Wav2Vec2BertForCTC</a> (Wav2Vec2-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForCTC">Wav2Vec2ForCTC</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForCTC">Wav2Vec2ConformerForCTC</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForCTC">WavLMForCTC</a> (WavLM model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),kl=new $({props:{anchor:"transformers.AutoModelForCTC.from_config.example",$$slots:{default:[p3]},$$scope:{ctx:y}}}),sp=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForCTC.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCTC.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForCTC.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForCTC.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForCTC.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForCTC.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForCTC.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForCTC.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForCTC.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForCTC.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForCTC.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForCTC.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForCTC.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForCTC.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForCTC.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForCTC.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ll=new $({props:{anchor:"transformers.AutoModelForCTC.from_pretrained.example",$$slots:{default:[h3]},$$scope:{ctx:y}}}),ip=new B({props:{title:"AutoModelForSpeechSeq2Seq",local:"transformers.AutoModelForSpeechSeq2Seq",headingTag:"h3"}}),lp=new x({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1534"}}),dp=new x({props:{name:"from_config",anchor:"transformers.AutoModelForSpeechSeq2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoConfig">Pop2PianoConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pop2piano#transformers.Pop2PianoForConditionalGeneration">Pop2PianoForConditionalGeneration</a> (Pop2Piano model)</li>
<li><a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TConfig">SeamlessM4TConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/seamless_m4t#transformers.SeamlessM4TForSpeechToText">SeamlessM4TForSpeechToText</a> (SeamlessM4T model)</li>
<li><a href="/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2Config">SeamlessM4Tv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/seamless_m4t_v2#transformers.SeamlessM4Tv2ForSpeechToText">SeamlessM4Tv2ForSpeechToText</a> (SeamlessM4Tv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextForConditionalGeneration">Speech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderModel">SpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
<li><a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Config">SpeechT5Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5ForSpeechToText">SpeechT5ForSpeechToText</a> (SpeechT5 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperForConditionalGeneration">WhisperForConditionalGeneration</a> (Whisper model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),$l=new $({props:{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_config.example",$$slots:{default:[u3]},$$scope:{ctx:y}}}),mp=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Bl=new $({props:{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.example",$$slots:{default:[_3]},$$scope:{ctx:y}}}),cp=new B({props:{title:"TFAutoModelForSpeechSeq2Seq",local:"transformers.TFAutoModelForSpeechSeq2Seq",headingTag:"h3"}}),fp=new x({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L693"}}),gp=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.Speech2TextConfig">Speech2TextConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/speech_to_text#transformers.TFSpeech2TextForConditionalGeneration">TFSpeech2TextForConditionalGeneration</a> (Speech2Text model)</li>
<li><a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/whisper#transformers.TFWhisperForConditionalGeneration">TFWhisperForConditionalGeneration</a> (Whisper model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),jl=new $({props:{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_config.example",$$slots:{default:[b3]},$$scope:{ctx:y}}}),pp=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Zl=new $({props:{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.example",$$slots:{default:[M3]},$$scope:{ctx:y}}}),hp=new B({props:{title:"FlaxAutoModelForSpeechSeq2Seq",local:"transformers.FlaxAutoModelForSpeechSeq2Seq",headingTag:"h3"}}),up=new x({props:{name:"class transformers.FlaxAutoModelForSpeechSeq2Seq",anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L374"}}),_p=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.SpeechEncoderDecoderConfig">SpeechEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/speech-encoder-decoder#transformers.FlaxSpeechEncoderDecoderModel">FlaxSpeechEncoderDecoderModel</a> (Speech Encoder decoder model)</li>
<li><a href="/docs/transformers/main/en/model_doc/whisper#transformers.WhisperConfig">WhisperConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/whisper#transformers.FlaxWhisperForConditionalGeneration">FlaxWhisperForConditionalGeneration</a> (Whisper model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Al=new $({props:{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_config.example",$$slots:{default:[F3]},$$scope:{ctx:y}}}),bp=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Rl=new $({props:{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.example",$$slots:{default:[T3]},$$scope:{ctx:y}}}),Mp=new B({props:{title:"AutoModelForAudioXVector",local:"transformers.AutoModelForAudioXVector",headingTag:"h3"}}),Fp=new x({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1552"}}),Tp=new x({props:{name:"from_config",anchor:"transformers.AutoModelForAudioXVector.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioXVector.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatConfig">UniSpeechSatConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/unispeech-sat#transformers.UniSpeechSatForXVector">UniSpeechSatForXVector</a> (UniSpeechSat model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertConfig">Wav2Vec2BertConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-bert#transformers.Wav2Vec2BertForXVector">Wav2Vec2BertForXVector</a> (Wav2Vec2-BERT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2Config">Wav2Vec2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2#transformers.Wav2Vec2ForXVector">Wav2Vec2ForXVector</a> (Wav2Vec2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerConfig">Wav2Vec2ConformerConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wav2vec2-conformer#transformers.Wav2Vec2ConformerForXVector">Wav2Vec2ConformerForXVector</a> (Wav2Vec2-Conformer model)</li>
<li><a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMConfig">WavLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/wavlm#transformers.WavLMForXVector">WavLMForXVector</a> (WavLM model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Pl=new $({props:{anchor:"transformers.AutoModelForAudioXVector.from_config.example",$$slots:{default:[C3]},$$scope:{ctx:y}}}),Cp=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForAudioXVector.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Wl=new $({props:{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.example",$$slots:{default:[y3]},$$scope:{ctx:y}}}),yp=new B({props:{title:"AutoModelForTextToSpectrogram",local:"transformers.AutoModelForTextToSpectrogram",headingTag:"h3"}}),wp=new x({props:{name:"class transformers.AutoModelForTextToSpectrogram",anchor:"transformers.AutoModelForTextToSpectrogram",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1556"}}),xp=new B({props:{title:"AutoModelForTextToWaveform",local:"transformers.AutoModelForTextToWaveform",headingTag:"h3"}}),Lp=new x({props:{name:"class transformers.AutoModelForTextToWaveform",anchor:"transformers.AutoModelForTextToWaveform",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1560"}}),$p=new B({props:{title:"Multimodal",local:"multimodal",headingTag:"h2"}}),jp=new B({props:{title:"AutoModelForTableQuestionAnswering",local:"transformers.AutoModelForTableQuestionAnswering",headingTag:"h3"}}),Zp=new x({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1377"}}),Ap=new x({props:{name:"from_config",anchor:"transformers.AutoModelForTableQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTableQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasForQuestionAnswering">TapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Sl=new $({props:{anchor:"transformers.AutoModelForTableQuestionAnswering.from_config.example",$$slots:{default:[v3]},$$scope:{ctx:y}}}),Rp=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Gl=new $({props:{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.example",$$slots:{default:[w3]},$$scope:{ctx:y}}}),Pp=new B({props:{title:"TFAutoModelForTableQuestionAnswering",local:"transformers.TFAutoModelForTableQuestionAnswering",headingTag:"h3"}}),Wp=new x({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L657"}}),Sp=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/tapas#transformers.TapasConfig">TapasConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/tapas#transformers.TFTapasForQuestionAnswering">TFTapasForQuestionAnswering</a> (TAPAS model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Vl=new $({props:{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_config.example",$$slots:{default:[x3]},$$scope:{ctx:y}}}),Gp=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Jl=new $({props:{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.example",$$slots:{default:[k3]},$$scope:{ctx:y}}}),Vp=new B({props:{title:"AutoModelForDocumentQuestionAnswering",local:"transformers.AutoModelForDocumentQuestionAnswering",headingTag:"h3"}}),Jp=new x({props:{name:"class transformers.AutoModelForDocumentQuestionAnswering",anchor:"transformers.AutoModelForDocumentQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1399"}}),Ep=new x({props:{name:"from_config",anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMForQuestionAnswering">LayoutLMForQuestionAnswering</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2Config">LayoutLMv2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2ForQuestionAnswering">LayoutLMv2ForQuestionAnswering</a> (LayoutLMv2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3ForQuestionAnswering">LayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),El=new $({props:{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_config.example",$$slots:{default:[L3]},$$scope:{ctx:y}}}),Ip=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Il=new $({props:{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.example",$$slots:{default:[$3]},$$scope:{ctx:y}}}),Up=new B({props:{title:"TFAutoModelForDocumentQuestionAnswering",local:"transformers.TFAutoModelForDocumentQuestionAnswering",headingTag:"h3"}}),Np=new x({props:{name:"class transformers.TFAutoModelForDocumentQuestionAnswering",anchor:"transformers.TFAutoModelForDocumentQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L646"}}),Xp=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.LayoutLMConfig">LayoutLMConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlm#transformers.TFLayoutLMForQuestionAnswering">TFLayoutLMForQuestionAnswering</a> (LayoutLM model)</li>
<li><a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.LayoutLMv3Config">LayoutLMv3Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/layoutlmv3#transformers.TFLayoutLMv3ForQuestionAnswering">TFLayoutLMv3ForQuestionAnswering</a> (LayoutLMv3 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ul=new $({props:{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_config.example",$$slots:{default:[B3]},$$scope:{ctx:y}}}),qp=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Nl=new $({props:{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.example",$$slots:{default:[j3]},$$scope:{ctx:y}}}),Qp=new B({props:{title:"AutoModelForVisualQuestionAnswering",local:"transformers.AutoModelForVisualQuestionAnswering",headingTag:"h3"}}),Dp=new x({props:{name:"class transformers.AutoModelForVisualQuestionAnswering",anchor:"transformers.AutoModelForVisualQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1388"}}),Hp=new x({props:{name:"from_config",anchor:"transformers.AutoModelForVisualQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vilt#transformers.ViltConfig">ViltConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vilt#transformers.ViltForQuestionAnswering">ViltForQuestionAnswering</a> (ViLT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Xl=new $({props:{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_config.example",$$slots:{default:[Z3]},$$scope:{ctx:y}}}),Yp=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ql=new $({props:{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.example",$$slots:{default:[A3]},$$scope:{ctx:y}}}),zp=new B({props:{title:"AutoModelForVision2Seq",local:"transformers.AutoModelForVision2Seq",headingTag:"h3"}}),Op=new x({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1513"}}),Kp=new x({props:{name:"from_config",anchor:"transformers.AutoModelForVision2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVision2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipForConditionalGeneration">BlipForConditionalGeneration</a> (BLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/git#transformers.GitConfig">GitConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/git#transformers.GitForCausalLM">GitForCausalLM</a> (GIT model)</li>
<li><a href="/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipConfig">InstructBlipConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/instructblip#transformers.InstructBlipForConditionalGeneration">InstructBlipForConditionalGeneration</a> (InstructBLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2Config">Kosmos2Config</a> configuration class: <a href="/docs/transformers/main/en/model_doc/kosmos-2#transformers.Kosmos2ForConditionalGeneration">Kosmos2ForConditionalGeneration</a> (KOSMOS-2 model)</li>
<li><a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig">LlavaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> (LLaVa model)</li>
<li><a href="/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructConfig">Pix2StructConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/pix2struct#transformers.Pix2StructForConditionalGeneration">Pix2StructForConditionalGeneration</a> (Pix2Struct model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaConfig">VipLlavaConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vipllava#transformers.VipLlavaForConditionalGeneration">VipLlavaForConditionalGeneration</a> (VipLlava model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderModel">VisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ql=new $({props:{anchor:"transformers.AutoModelForVision2Seq.from_config.example",$$slots:{default:[R3]},$$scope:{ctx:y}}}),eh=new x({props:{name:"from_pretrained",anchor:"transformers.AutoModelForVision2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Dl=new $({props:{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.example",$$slots:{default:[P3]},$$scope:{ctx:y}}}),oh=new B({props:{title:"TFAutoModelForVision2Seq",local:"transformers.TFAutoModelForVision2Seq",headingTag:"h3"}}),rh=new x({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L605"}}),nh=new x({props:{name:"from_config",anchor:"transformers.TFAutoModelForVision2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForVision2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/blip#transformers.TFBlipForConditionalGeneration">TFBlipForConditionalGeneration</a> (BLIP model)</li>
<li><a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.TFVisionEncoderDecoderModel">TFVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Hl=new $({props:{anchor:"transformers.TFAutoModelForVision2Seq.from_config.example",$$slots:{default:[W3]},$$scope:{ctx:y}}}),th=new x({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Yl=new $({props:{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.example",$$slots:{default:[S3]},$$scope:{ctx:y}}}),ah=new B({props:{title:"FlaxAutoModelForVision2Seq",local:"transformers.FlaxAutoModelForVision2Seq",headingTag:"h3"}}),sh=new x({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L367"}}),ih=new x({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForVision2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForVision2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.VisionEncoderDecoderConfig">VisionEncoderDecoderConfig</a> configuration class: <a href="/docs/transformers/main/en/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel">FlaxVisionEncoderDecoderModel</a> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),zl=new $({props:{anchor:"transformers.FlaxAutoModelForVision2Seq.from_config.example",$$slots:{default:[G3]},$$scope:{ctx:y}}}),lh=new x({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ol=new $({props:{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.example",$$slots:{default:[V3]},$$scope:{ctx:y}}}),{c(){n=i("meta"),C=t(),m=i("p"),s=t(),p(c.$$.fragment),e=t(),F=i("p"),F.innerHTML=EV,kv=t(),yd=i("p"),yd.innerHTML=IV,Lv=t(),p(vd.$$.fragment),$v=t(),wd=i("p"),wd.innerHTML=UV,Bv=t(),xd=i("p"),xd.innerHTML=NV,jv=t(),p(kd.$$.fragment),Zv=t(),Ld=i("p"),Ld.innerHTML=XV,Av=t(),p($d.$$.fragment),Rv=t(),Bd=i("p"),Bd.textContent=qV,Pv=t(),p(ys.$$.fragment),Wv=t(),p(jd.$$.fragment),Sv=t(),me=i("div"),p(Zd.$$.fragment),Sx=t(),ph=i("p"),ph.innerHTML=QV,Gx=t(),hh=i("p"),hh.innerHTML=DV,Vx=t(),Io=i("div"),p(Ad.$$.fragment),Jx=t(),uh=i("p"),uh.textContent=HV,Ex=t(),_h=i("p"),_h.innerHTML=YV,Ix=t(),bh=i("ul"),bh.innerHTML=zV,Ux=t(),p(vs.$$.fragment),Nx=t(),ws=i("div"),p(Rd.$$.fragment),Xx=t(),Mh=i("p"),Mh.textContent=OV,Gv=t(),p(Pd.$$.fragment),Vv=t(),ce=i("div"),p(Wd.$$.fragment),qx=t(),Fh=i("p"),Fh.innerHTML=KV,Qx=t(),Th=i("p"),Th.innerHTML=eJ,Dx=t(),Uo=i("div"),p(Sd.$$.fragment),Hx=t(),Ch=i("p"),Ch.textContent=oJ,Yx=t(),yh=i("p"),yh.innerHTML=rJ,zx=t(),vh=i("ul"),vh.innerHTML=nJ,Ox=t(),p(xs.$$.fragment),Kx=t(),ks=i("div"),p(Gd.$$.fragment),ek=t(),wh=i("p"),wh.textContent=tJ,Jv=t(),p(Vd.$$.fragment),Ev=t(),fe=i("div"),p(Jd.$$.fragment),ok=t(),xh=i("p"),xh.innerHTML=aJ,rk=t(),kh=i("p"),kh.innerHTML=sJ,nk=t(),j=i("div"),p(Ed.$$.fragment),tk=t(),Lh=i("p"),Lh.textContent=iJ,ak=t(),$h=i("p"),$h.innerHTML=lJ,sk=t(),Bh=i("ul"),Bh.innerHTML=dJ,ik=t(),p(Ls.$$.fragment),lk=t(),p($s.$$.fragment),dk=t(),Bs=i("div"),p(Id.$$.fragment),mk=t(),jh=i("p"),jh.textContent=mJ,Iv=t(),p(Ud.$$.fragment),Uv=t(),ge=i("div"),p(Nd.$$.fragment),ck=t(),Zh=i("p"),Zh.innerHTML=cJ,fk=t(),Ah=i("p"),Ah.innerHTML=fJ,gk=t(),Z=i("div"),p(Xd.$$.fragment),pk=t(),Rh=i("p"),Rh.textContent=gJ,hk=t(),Ph=i("p"),Ph.innerHTML=pJ,uk=t(),Wh=i("ul"),Wh.innerHTML=hJ,_k=t(),p(js.$$.fragment),bk=t(),p(Zs.$$.fragment),Mk=t(),As=i("div"),p(qd.$$.fragment),Fk=t(),Sh=i("p"),Sh.textContent=uJ,Nv=t(),p(Qd.$$.fragment),Xv=t(),pe=i("div"),p(Dd.$$.fragment),Tk=t(),Gh=i("p"),Gh.innerHTML=_J,Ck=t(),Vh=i("p"),Vh.innerHTML=bJ,yk=t(),A=i("div"),p(Hd.$$.fragment),vk=t(),Jh=i("p"),Jh.textContent=MJ,wk=t(),Eh=i("p"),Eh.innerHTML=FJ,xk=t(),Ih=i("ul"),Ih.innerHTML=TJ,kk=t(),p(Rs.$$.fragment),Lk=t(),p(Ps.$$.fragment),$k=t(),Ws=i("div"),p(Yd.$$.fragment),Bk=t(),Uh=i("p"),Uh.textContent=CJ,qv=t(),p(zd.$$.fragment),Qv=t(),Od=i("p"),Od.textContent=yJ,Dv=t(),p(Kd.$$.fragment),Hv=t(),he=i("div"),p(em.$$.fragment),jk=t(),Nh=i("p"),Nh.innerHTML=vJ,Zk=t(),Xh=i("p"),Xh.innerHTML=wJ,Ak=t(),Sn=i("div"),p(om.$$.fragment),Rk=t(),qh=i("p"),qh.textContent=xJ,Pk=t(),Qh=i("p"),Qh.innerHTML=kJ,Wk=t(),p(Ss.$$.fragment),Sk=t(),R=i("div"),p(rm.$$.fragment),Gk=t(),Dh=i("p"),Dh.textContent=LJ,Vk=t(),Hh=i("p"),Hh.innerHTML=$J,Jk=t(),Yh=i("ul"),Yh.innerHTML=BJ,Ek=t(),zh=i("p"),zh.innerHTML=jJ,Ik=t(),p(Gs.$$.fragment),Yv=t(),p(nm.$$.fragment),zv=t(),ue=i("div"),p(tm.$$.fragment),Uk=t(),Oh=i("p"),Oh.innerHTML=ZJ,Nk=t(),Kh=i("p"),Kh.innerHTML=AJ,Xk=t(),Gn=i("div"),p(am.$$.fragment),qk=t(),eu=i("p"),eu.textContent=RJ,Qk=t(),ou=i("p"),ou.innerHTML=PJ,Dk=t(),p(Vs.$$.fragment),Hk=t(),No=i("div"),p(sm.$$.fragment),Yk=t(),ru=i("p"),ru.textContent=WJ,zk=t(),nu=i("p"),nu.innerHTML=SJ,Ok=t(),tu=i("ul"),tu.innerHTML=GJ,Kk=t(),p(Js.$$.fragment),Ov=t(),p(im.$$.fragment),Kv=t(),_e=i("div"),p(lm.$$.fragment),eL=t(),au=i("p"),au.innerHTML=VJ,oL=t(),su=i("p"),su.innerHTML=JJ,rL=t(),Vn=i("div"),p(dm.$$.fragment),nL=t(),iu=i("p"),iu.textContent=EJ,tL=t(),lu=i("p"),lu.innerHTML=IJ,aL=t(),p(Es.$$.fragment),sL=t(),Xo=i("div"),p(mm.$$.fragment),iL=t(),du=i("p"),du.textContent=UJ,lL=t(),mu=i("p"),mu.innerHTML=NJ,dL=t(),cu=i("ul"),cu.innerHTML=XJ,mL=t(),p(Is.$$.fragment),e2=t(),p(cm.$$.fragment),o2=t(),fm=i("p"),fm.textContent=qJ,r2=t(),p(gm.$$.fragment),n2=t(),be=i("div"),p(pm.$$.fragment),cL=t(),fu=i("p"),fu.innerHTML=QJ,fL=t(),gu=i("p"),gu.innerHTML=DJ,gL=t(),Jn=i("div"),p(hm.$$.fragment),pL=t(),pu=i("p"),pu.textContent=HJ,hL=t(),hu=i("p"),hu.innerHTML=YJ,uL=t(),p(Us.$$.fragment),_L=t(),P=i("div"),p(um.$$.fragment),bL=t(),uu=i("p"),uu.textContent=zJ,ML=t(),_u=i("p"),_u.innerHTML=OJ,FL=t(),bu=i("ul"),bu.innerHTML=KJ,TL=t(),Mu=i("p"),Mu.innerHTML=e0,CL=t(),p(Ns.$$.fragment),t2=t(),p(_m.$$.fragment),a2=t(),Me=i("div"),p(bm.$$.fragment),yL=t(),Fu=i("p"),Fu.innerHTML=o0,vL=t(),Tu=i("p"),Tu.innerHTML=r0,wL=t(),En=i("div"),p(Mm.$$.fragment),xL=t(),Cu=i("p"),Cu.textContent=n0,kL=t(),yu=i("p"),yu.innerHTML=t0,LL=t(),p(Xs.$$.fragment),$L=t(),qo=i("div"),p(Fm.$$.fragment),BL=t(),vu=i("p"),vu.textContent=a0,jL=t(),wu=i("p"),wu.innerHTML=s0,ZL=t(),xu=i("ul"),xu.innerHTML=i0,AL=t(),p(qs.$$.fragment),s2=t(),p(Tm.$$.fragment),i2=t(),Fe=i("div"),p(Cm.$$.fragment),RL=t(),ku=i("p"),ku.innerHTML=l0,PL=t(),Lu=i("p"),Lu.innerHTML=d0,WL=t(),In=i("div"),p(ym.$$.fragment),SL=t(),$u=i("p"),$u.textContent=m0,GL=t(),Bu=i("p"),Bu.innerHTML=c0,VL=t(),p(Qs.$$.fragment),JL=t(),Qo=i("div"),p(vm.$$.fragment),EL=t(),ju=i("p"),ju.textContent=f0,IL=t(),Zu=i("p"),Zu.innerHTML=g0,UL=t(),Au=i("ul"),Au.innerHTML=p0,NL=t(),p(Ds.$$.fragment),l2=t(),p(wm.$$.fragment),d2=t(),xm=i("p"),xm.textContent=h0,m2=t(),p(km.$$.fragment),c2=t(),Te=i("div"),p(Lm.$$.fragment),XL=t(),Ru=i("p"),Ru.innerHTML=u0,qL=t(),Pu=i("p"),Pu.innerHTML=_0,QL=t(),Un=i("div"),p($m.$$.fragment),DL=t(),Wu=i("p"),Wu.textContent=b0,HL=t(),Su=i("p"),Su.innerHTML=M0,YL=t(),p(Hs.$$.fragment),zL=t(),W=i("div"),p(Bm.$$.fragment),OL=t(),Gu=i("p"),Gu.textContent=F0,KL=t(),Vu=i("p"),Vu.innerHTML=T0,e1=t(),Ju=i("ul"),Ju.innerHTML=C0,o1=t(),Eu=i("p"),Eu.innerHTML=y0,r1=t(),p(Ys.$$.fragment),f2=t(),p(jm.$$.fragment),g2=t(),Ce=i("div"),p(Zm.$$.fragment),n1=t(),Iu=i("p"),Iu.innerHTML=v0,t1=t(),Uu=i("p"),Uu.innerHTML=w0,a1=t(),Nn=i("div"),p(Am.$$.fragment),s1=t(),Nu=i("p"),Nu.textContent=x0,i1=t(),Xu=i("p"),Xu.innerHTML=k0,l1=t(),p(zs.$$.fragment),d1=t(),Do=i("div"),p(Rm.$$.fragment),m1=t(),qu=i("p"),qu.textContent=L0,c1=t(),Qu=i("p"),Qu.innerHTML=$0,f1=t(),Du=i("ul"),Du.innerHTML=B0,g1=t(),p(Os.$$.fragment),p2=t(),p(Pm.$$.fragment),h2=t(),ye=i("div"),p(Wm.$$.fragment),p1=t(),Hu=i("p"),Hu.innerHTML=j0,h1=t(),Yu=i("p"),Yu.innerHTML=Z0,u1=t(),Xn=i("div"),p(Sm.$$.fragment),_1=t(),zu=i("p"),zu.textContent=A0,b1=t(),Ou=i("p"),Ou.innerHTML=R0,M1=t(),p(Ks.$$.fragment),F1=t(),Ho=i("div"),p(Gm.$$.fragment),T1=t(),Ku=i("p"),Ku.textContent=P0,C1=t(),e_=i("p"),e_.innerHTML=W0,y1=t(),o_=i("ul"),o_.innerHTML=S0,v1=t(),p(ei.$$.fragment),u2=t(),p(Vm.$$.fragment),_2=t(),ve=i("div"),p(Jm.$$.fragment),w1=t(),r_=i("p"),r_.innerHTML=G0,x1=t(),n_=i("p"),n_.innerHTML=V0,k1=t(),qn=i("div"),p(Em.$$.fragment),L1=t(),t_=i("p"),t_.textContent=J0,$1=t(),a_=i("p"),a_.innerHTML=E0,B1=t(),p(oi.$$.fragment),j1=t(),S=i("div"),p(Im.$$.fragment),Z1=t(),s_=i("p"),s_.textContent=I0,A1=t(),i_=i("p"),i_.innerHTML=U0,R1=t(),l_=i("ul"),l_.innerHTML=N0,P1=t(),d_=i("p"),d_.innerHTML=X0,W1=t(),p(ri.$$.fragment),b2=t(),p(Um.$$.fragment),M2=t(),we=i("div"),p(Nm.$$.fragment),S1=t(),m_=i("p"),m_.innerHTML=q0,G1=t(),c_=i("p"),c_.innerHTML=Q0,V1=t(),Qn=i("div"),p(Xm.$$.fragment),J1=t(),f_=i("p"),f_.textContent=D0,E1=t(),g_=i("p"),g_.innerHTML=H0,I1=t(),p(ni.$$.fragment),U1=t(),Yo=i("div"),p(qm.$$.fragment),N1=t(),p_=i("p"),p_.textContent=Y0,X1=t(),h_=i("p"),h_.innerHTML=z0,q1=t(),u_=i("ul"),u_.innerHTML=O0,Q1=t(),p(ti.$$.fragment),F2=t(),p(Qm.$$.fragment),T2=t(),xe=i("div"),p(Dm.$$.fragment),D1=t(),__=i("p"),__.innerHTML=K0,H1=t(),b_=i("p"),b_.innerHTML=eE,Y1=t(),Dn=i("div"),p(Hm.$$.fragment),z1=t(),M_=i("p"),M_.textContent=oE,O1=t(),F_=i("p"),F_.innerHTML=rE,K1=t(),p(ai.$$.fragment),e$=t(),zo=i("div"),p(Ym.$$.fragment),o$=t(),T_=i("p"),T_.textContent=nE,r$=t(),C_=i("p"),C_.innerHTML=tE,n$=t(),y_=i("ul"),y_.innerHTML=aE,t$=t(),p(si.$$.fragment),C2=t(),p(zm.$$.fragment),y2=t(),Om=i("div"),p(Km.$$.fragment),v2=t(),p(ec.$$.fragment),w2=t(),oc=i("div"),p(rc.$$.fragment),x2=t(),p(nc.$$.fragment),k2=t(),ke=i("div"),p(tc.$$.fragment),a$=t(),v_=i("p"),v_.innerHTML=sE,s$=t(),w_=i("p"),w_.innerHTML=iE,i$=t(),Hn=i("div"),p(ac.$$.fragment),l$=t(),x_=i("p"),x_.textContent=lE,d$=t(),k_=i("p"),k_.innerHTML=dE,m$=t(),p(ii.$$.fragment),c$=t(),G=i("div"),p(sc.$$.fragment),f$=t(),L_=i("p"),L_.textContent=mE,g$=t(),$_=i("p"),$_.innerHTML=cE,p$=t(),B_=i("ul"),B_.innerHTML=fE,h$=t(),j_=i("p"),j_.innerHTML=gE,u$=t(),p(li.$$.fragment),L2=t(),p(ic.$$.fragment),$2=t(),Le=i("div"),p(lc.$$.fragment),_$=t(),Z_=i("p"),Z_.innerHTML=pE,b$=t(),A_=i("p"),A_.innerHTML=hE,M$=t(),Yn=i("div"),p(dc.$$.fragment),F$=t(),R_=i("p"),R_.textContent=uE,T$=t(),P_=i("p"),P_.innerHTML=_E,C$=t(),p(di.$$.fragment),y$=t(),Oo=i("div"),p(mc.$$.fragment),v$=t(),W_=i("p"),W_.textContent=bE,w$=t(),S_=i("p"),S_.innerHTML=ME,x$=t(),G_=i("ul"),G_.innerHTML=FE,k$=t(),p(mi.$$.fragment),B2=t(),p(cc.$$.fragment),j2=t(),$e=i("div"),p(fc.$$.fragment),L$=t(),V_=i("p"),V_.innerHTML=TE,$$=t(),J_=i("p"),J_.innerHTML=CE,B$=t(),zn=i("div"),p(gc.$$.fragment),j$=t(),E_=i("p"),E_.textContent=yE,Z$=t(),I_=i("p"),I_.innerHTML=vE,A$=t(),p(ci.$$.fragment),R$=t(),Ko=i("div"),p(pc.$$.fragment),P$=t(),U_=i("p"),U_.textContent=wE,W$=t(),N_=i("p"),N_.innerHTML=xE,S$=t(),X_=i("ul"),X_.innerHTML=kE,G$=t(),p(fi.$$.fragment),Z2=t(),p(hc.$$.fragment),A2=t(),Be=i("div"),p(uc.$$.fragment),V$=t(),q_=i("p"),q_.innerHTML=LE,J$=t(),Q_=i("p"),Q_.innerHTML=$E,E$=t(),On=i("div"),p(_c.$$.fragment),I$=t(),D_=i("p"),D_.textContent=BE,U$=t(),H_=i("p"),H_.innerHTML=jE,N$=t(),p(gi.$$.fragment),X$=t(),V=i("div"),p(bc.$$.fragment),q$=t(),Y_=i("p"),Y_.textContent=ZE,Q$=t(),z_=i("p"),z_.innerHTML=AE,D$=t(),O_=i("ul"),O_.innerHTML=RE,H$=t(),K_=i("p"),K_.innerHTML=PE,Y$=t(),p(pi.$$.fragment),R2=t(),p(Mc.$$.fragment),P2=t(),je=i("div"),p(Fc.$$.fragment),z$=t(),eb=i("p"),eb.innerHTML=WE,O$=t(),ob=i("p"),ob.innerHTML=SE,K$=t(),Kn=i("div"),p(Tc.$$.fragment),eB=t(),rb=i("p"),rb.textContent=GE,oB=t(),nb=i("p"),nb.innerHTML=VE,rB=t(),p(hi.$$.fragment),nB=t(),er=i("div"),p(Cc.$$.fragment),tB=t(),tb=i("p"),tb.textContent=JE,aB=t(),ab=i("p"),ab.innerHTML=EE,sB=t(),sb=i("ul"),sb.innerHTML=IE,iB=t(),p(ui.$$.fragment),W2=t(),p(yc.$$.fragment),S2=t(),Ze=i("div"),p(vc.$$.fragment),lB=t(),ib=i("p"),ib.innerHTML=UE,dB=t(),lb=i("p"),lb.innerHTML=NE,mB=t(),et=i("div"),p(wc.$$.fragment),cB=t(),db=i("p"),db.textContent=XE,fB=t(),mb=i("p"),mb.innerHTML=qE,gB=t(),p(_i.$$.fragment),pB=t(),or=i("div"),p(xc.$$.fragment),hB=t(),cb=i("p"),cb.textContent=QE,uB=t(),fb=i("p"),fb.innerHTML=DE,_B=t(),gb=i("ul"),gb.innerHTML=HE,bB=t(),p(bi.$$.fragment),G2=t(),p(kc.$$.fragment),V2=t(),Ae=i("div"),p(Lc.$$.fragment),MB=t(),pb=i("p"),pb.innerHTML=YE,FB=t(),hb=i("p"),hb.innerHTML=zE,TB=t(),ot=i("div"),p($c.$$.fragment),CB=t(),ub=i("p"),ub.textContent=OE,yB=t(),_b=i("p"),_b.innerHTML=KE,vB=t(),p(Mi.$$.fragment),wB=t(),J=i("div"),p(Bc.$$.fragment),xB=t(),bb=i("p"),bb.textContent=eI,kB=t(),Mb=i("p"),Mb.innerHTML=oI,LB=t(),Fb=i("ul"),Fb.innerHTML=rI,$B=t(),Tb=i("p"),Tb.innerHTML=nI,BB=t(),p(Fi.$$.fragment),J2=t(),p(jc.$$.fragment),E2=t(),Re=i("div"),p(Zc.$$.fragment),jB=t(),Cb=i("p"),Cb.innerHTML=tI,ZB=t(),yb=i("p"),yb.innerHTML=aI,AB=t(),rt=i("div"),p(Ac.$$.fragment),RB=t(),vb=i("p"),vb.textContent=sI,PB=t(),wb=i("p"),wb.innerHTML=iI,WB=t(),p(Ti.$$.fragment),SB=t(),rr=i("div"),p(Rc.$$.fragment),GB=t(),xb=i("p"),xb.textContent=lI,VB=t(),kb=i("p"),kb.innerHTML=dI,JB=t(),Lb=i("ul"),Lb.innerHTML=mI,EB=t(),p(Ci.$$.fragment),I2=t(),p(Pc.$$.fragment),U2=t(),Pe=i("div"),p(Wc.$$.fragment),IB=t(),$b=i("p"),$b.innerHTML=cI,UB=t(),Bb=i("p"),Bb.innerHTML=fI,NB=t(),nt=i("div"),p(Sc.$$.fragment),XB=t(),jb=i("p"),jb.textContent=gI,qB=t(),Zb=i("p"),Zb.innerHTML=pI,QB=t(),p(yi.$$.fragment),DB=t(),nr=i("div"),p(Gc.$$.fragment),HB=t(),Ab=i("p"),Ab.textContent=hI,YB=t(),Rb=i("p"),Rb.innerHTML=uI,zB=t(),Pb=i("ul"),Pb.innerHTML=_I,OB=t(),p(vi.$$.fragment),N2=t(),p(Vc.$$.fragment),X2=t(),We=i("div"),p(Jc.$$.fragment),KB=t(),Wb=i("p"),Wb.innerHTML=bI,ej=t(),Sb=i("p"),Sb.innerHTML=MI,oj=t(),tt=i("div"),p(Ec.$$.fragment),rj=t(),Gb=i("p"),Gb.textContent=FI,nj=t(),Vb=i("p"),Vb.innerHTML=TI,tj=t(),p(wi.$$.fragment),aj=t(),E=i("div"),p(Ic.$$.fragment),sj=t(),Jb=i("p"),Jb.textContent=CI,ij=t(),Eb=i("p"),Eb.innerHTML=yI,lj=t(),Ib=i("ul"),Ib.innerHTML=vI,dj=t(),Ub=i("p"),Ub.innerHTML=wI,mj=t(),p(xi.$$.fragment),q2=t(),p(Uc.$$.fragment),Q2=t(),Se=i("div"),p(Nc.$$.fragment),cj=t(),Nb=i("p"),Nb.innerHTML=xI,fj=t(),Xb=i("p"),Xb.innerHTML=kI,gj=t(),at=i("div"),p(Xc.$$.fragment),pj=t(),qb=i("p"),qb.textContent=LI,hj=t(),Qb=i("p"),Qb.innerHTML=$I,uj=t(),p(ki.$$.fragment),_j=t(),tr=i("div"),p(qc.$$.fragment),bj=t(),Db=i("p"),Db.textContent=BI,Mj=t(),Hb=i("p"),Hb.innerHTML=jI,Fj=t(),Yb=i("ul"),Yb.innerHTML=ZI,Tj=t(),p(Li.$$.fragment),D2=t(),p(Qc.$$.fragment),H2=t(),Ge=i("div"),p(Dc.$$.fragment),Cj=t(),zb=i("p"),zb.innerHTML=AI,yj=t(),Ob=i("p"),Ob.innerHTML=RI,vj=t(),st=i("div"),p(Hc.$$.fragment),wj=t(),Kb=i("p"),Kb.textContent=PI,xj=t(),eM=i("p"),eM.innerHTML=WI,kj=t(),p($i.$$.fragment),Lj=t(),ar=i("div"),p(Yc.$$.fragment),$j=t(),oM=i("p"),oM.textContent=SI,Bj=t(),rM=i("p"),rM.innerHTML=GI,jj=t(),nM=i("ul"),nM.innerHTML=VI,Zj=t(),p(Bi.$$.fragment),Y2=t(),p(zc.$$.fragment),z2=t(),Ve=i("div"),p(Oc.$$.fragment),Aj=t(),tM=i("p"),tM.innerHTML=JI,Rj=t(),aM=i("p"),aM.innerHTML=EI,Pj=t(),it=i("div"),p(Kc.$$.fragment),Wj=t(),sM=i("p"),sM.textContent=II,Sj=t(),iM=i("p"),iM.innerHTML=UI,Gj=t(),p(ji.$$.fragment),Vj=t(),I=i("div"),p(ef.$$.fragment),Jj=t(),lM=i("p"),lM.textContent=NI,Ej=t(),dM=i("p"),dM.innerHTML=XI,Ij=t(),mM=i("ul"),mM.innerHTML=qI,Uj=t(),cM=i("p"),cM.innerHTML=QI,Nj=t(),p(Zi.$$.fragment),O2=t(),p(of.$$.fragment),K2=t(),Je=i("div"),p(rf.$$.fragment),Xj=t(),fM=i("p"),fM.innerHTML=DI,qj=t(),gM=i("p"),gM.innerHTML=HI,Qj=t(),lt=i("div"),p(nf.$$.fragment),Dj=t(),pM=i("p"),pM.textContent=YI,Hj=t(),hM=i("p"),hM.innerHTML=zI,Yj=t(),p(Ai.$$.fragment),zj=t(),sr=i("div"),p(tf.$$.fragment),Oj=t(),uM=i("p"),uM.textContent=OI,Kj=t(),_M=i("p"),_M.innerHTML=KI,eZ=t(),bM=i("ul"),bM.innerHTML=eU,oZ=t(),p(Ri.$$.fragment),ew=t(),p(af.$$.fragment),ow=t(),Ee=i("div"),p(sf.$$.fragment),rZ=t(),MM=i("p"),MM.innerHTML=oU,nZ=t(),FM=i("p"),FM.innerHTML=rU,tZ=t(),dt=i("div"),p(lf.$$.fragment),aZ=t(),TM=i("p"),TM.textContent=nU,sZ=t(),CM=i("p"),CM.innerHTML=tU,iZ=t(),p(Pi.$$.fragment),lZ=t(),ir=i("div"),p(df.$$.fragment),dZ=t(),yM=i("p"),yM.textContent=aU,mZ=t(),vM=i("p"),vM.innerHTML=sU,cZ=t(),wM=i("ul"),wM.innerHTML=iU,fZ=t(),p(Wi.$$.fragment),rw=t(),p(mf.$$.fragment),nw=t(),Ie=i("div"),p(cf.$$.fragment),gZ=t(),xM=i("p"),xM.innerHTML=lU,pZ=t(),kM=i("p"),kM.innerHTML=dU,hZ=t(),mt=i("div"),p(ff.$$.fragment),uZ=t(),LM=i("p"),LM.textContent=mU,_Z=t(),$M=i("p"),$M.innerHTML=cU,bZ=t(),p(Si.$$.fragment),MZ=t(),U=i("div"),p(gf.$$.fragment),FZ=t(),BM=i("p"),BM.textContent=fU,TZ=t(),jM=i("p"),jM.innerHTML=gU,CZ=t(),ZM=i("ul"),ZM.innerHTML=pU,yZ=t(),AM=i("p"),AM.innerHTML=hU,vZ=t(),p(Gi.$$.fragment),tw=t(),p(pf.$$.fragment),aw=t(),Ue=i("div"),p(hf.$$.fragment),wZ=t(),RM=i("p"),RM.innerHTML=uU,xZ=t(),PM=i("p"),PM.innerHTML=_U,kZ=t(),ct=i("div"),p(uf.$$.fragment),LZ=t(),WM=i("p"),WM.textContent=bU,$Z=t(),SM=i("p"),SM.innerHTML=MU,BZ=t(),p(Vi.$$.fragment),jZ=t(),lr=i("div"),p(_f.$$.fragment),ZZ=t(),GM=i("p"),GM.textContent=FU,AZ=t(),VM=i("p"),VM.innerHTML=TU,RZ=t(),JM=i("ul"),JM.innerHTML=CU,PZ=t(),p(Ji.$$.fragment),sw=t(),p(bf.$$.fragment),iw=t(),Ne=i("div"),p(Mf.$$.fragment),WZ=t(),EM=i("p"),EM.innerHTML=yU,SZ=t(),IM=i("p"),IM.innerHTML=vU,GZ=t(),ft=i("div"),p(Ff.$$.fragment),VZ=t(),UM=i("p"),UM.textContent=wU,JZ=t(),NM=i("p"),NM.innerHTML=xU,EZ=t(),p(Ei.$$.fragment),IZ=t(),dr=i("div"),p(Tf.$$.fragment),UZ=t(),XM=i("p"),XM.textContent=kU,NZ=t(),qM=i("p"),qM.innerHTML=LU,XZ=t(),QM=i("ul"),QM.innerHTML=$U,qZ=t(),p(Ii.$$.fragment),lw=t(),p(Cf.$$.fragment),dw=t(),yf=i("div"),p(vf.$$.fragment),mw=t(),p(wf.$$.fragment),cw=t(),xf=i("div"),p(kf.$$.fragment),fw=t(),p(Lf.$$.fragment),gw=t(),$f=i("p"),$f.textContent=BU,pw=t(),p(Bf.$$.fragment),hw=t(),Xe=i("div"),p(jf.$$.fragment),QZ=t(),DM=i("p"),DM.innerHTML=jU,DZ=t(),HM=i("p"),HM.innerHTML=ZU,HZ=t(),gt=i("div"),p(Zf.$$.fragment),YZ=t(),YM=i("p"),YM.textContent=AU,zZ=t(),zM=i("p"),zM.innerHTML=RU,OZ=t(),p(Ui.$$.fragment),KZ=t(),N=i("div"),p(Af.$$.fragment),eA=t(),OM=i("p"),OM.textContent=PU,oA=t(),KM=i("p"),KM.innerHTML=WU,rA=t(),eF=i("ul"),eF.innerHTML=SU,nA=t(),oF=i("p"),oF.innerHTML=GU,tA=t(),p(Ni.$$.fragment),uw=t(),p(Rf.$$.fragment),_w=t(),qe=i("div"),p(Pf.$$.fragment),aA=t(),rF=i("p"),rF.innerHTML=VU,sA=t(),nF=i("p"),nF.innerHTML=JU,iA=t(),pt=i("div"),p(Wf.$$.fragment),lA=t(),tF=i("p"),tF.textContent=EU,dA=t(),aF=i("p"),aF.innerHTML=IU,mA=t(),p(Xi.$$.fragment),cA=t(),X=i("div"),p(Sf.$$.fragment),fA=t(),sF=i("p"),sF.textContent=UU,gA=t(),iF=i("p"),iF.innerHTML=NU,pA=t(),lF=i("ul"),lF.innerHTML=XU,hA=t(),dF=i("p"),dF.innerHTML=qU,uA=t(),p(qi.$$.fragment),bw=t(),p(Gf.$$.fragment),Mw=t(),Qe=i("div"),p(Vf.$$.fragment),_A=t(),mF=i("p"),mF.innerHTML=QU,bA=t(),cF=i("p"),cF.innerHTML=DU,MA=t(),ht=i("div"),p(Jf.$$.fragment),FA=t(),fF=i("p"),fF.textContent=HU,TA=t(),gF=i("p"),gF.innerHTML=YU,CA=t(),p(Qi.$$.fragment),yA=t(),mr=i("div"),p(Ef.$$.fragment),vA=t(),pF=i("p"),pF.textContent=zU,wA=t(),hF=i("p"),hF.innerHTML=OU,xA=t(),uF=i("ul"),uF.innerHTML=KU,kA=t(),p(Di.$$.fragment),Fw=t(),p(If.$$.fragment),Tw=t(),De=i("div"),p(Uf.$$.fragment),LA=t(),_F=i("p"),_F.innerHTML=e9,$A=t(),bF=i("p"),bF.innerHTML=o9,BA=t(),ut=i("div"),p(Nf.$$.fragment),jA=t(),MF=i("p"),MF.textContent=r9,ZA=t(),FF=i("p"),FF.innerHTML=n9,AA=t(),p(Hi.$$.fragment),RA=t(),cr=i("div"),p(Xf.$$.fragment),PA=t(),TF=i("p"),TF.textContent=t9,WA=t(),CF=i("p"),CF.innerHTML=a9,SA=t(),yF=i("ul"),yF.innerHTML=s9,GA=t(),p(Yi.$$.fragment),Cw=t(),p(qf.$$.fragment),yw=t(),He=i("div"),p(Qf.$$.fragment),VA=t(),vF=i("p"),vF.innerHTML=i9,JA=t(),wF=i("p"),wF.innerHTML=l9,EA=t(),_t=i("div"),p(Df.$$.fragment),IA=t(),xF=i("p"),xF.textContent=d9,UA=t(),kF=i("p"),kF.innerHTML=m9,NA=t(),p(zi.$$.fragment),XA=t(),q=i("div"),p(Hf.$$.fragment),qA=t(),LF=i("p"),LF.textContent=c9,QA=t(),$F=i("p"),$F.innerHTML=f9,DA=t(),BF=i("ul"),BF.innerHTML=g9,HA=t(),jF=i("p"),jF.innerHTML=p9,YA=t(),p(Oi.$$.fragment),vw=t(),p(Yf.$$.fragment),ww=t(),Ye=i("div"),p(zf.$$.fragment),zA=t(),ZF=i("p"),ZF.innerHTML=h9,OA=t(),AF=i("p"),AF.innerHTML=u9,KA=t(),bt=i("div"),p(Of.$$.fragment),eR=t(),RF=i("p"),RF.textContent=_9,oR=t(),PF=i("p"),PF.innerHTML=b9,rR=t(),p(Ki.$$.fragment),nR=t(),Q=i("div"),p(Kf.$$.fragment),tR=t(),WF=i("p"),WF.textContent=M9,aR=t(),SF=i("p"),SF.innerHTML=F9,sR=t(),GF=i("ul"),GF.innerHTML=T9,iR=t(),VF=i("p"),VF.innerHTML=C9,lR=t(),p(el.$$.fragment),xw=t(),p(eg.$$.fragment),kw=t(),ze=i("div"),p(og.$$.fragment),dR=t(),JF=i("p"),JF.innerHTML=y9,mR=t(),EF=i("p"),EF.innerHTML=v9,cR=t(),Mt=i("div"),p(rg.$$.fragment),fR=t(),IF=i("p"),IF.textContent=w9,gR=t(),UF=i("p"),UF.innerHTML=x9,pR=t(),p(ol.$$.fragment),hR=t(),fr=i("div"),p(ng.$$.fragment),uR=t(),NF=i("p"),NF.textContent=k9,_R=t(),XF=i("p"),XF.innerHTML=L9,bR=t(),qF=i("ul"),qF.innerHTML=$9,MR=t(),p(rl.$$.fragment),Lw=t(),p(tg.$$.fragment),$w=t(),Oe=i("div"),p(ag.$$.fragment),FR=t(),QF=i("p"),QF.innerHTML=B9,TR=t(),DF=i("p"),DF.innerHTML=j9,CR=t(),Ft=i("div"),p(sg.$$.fragment),yR=t(),HF=i("p"),HF.textContent=Z9,vR=t(),YF=i("p"),YF.innerHTML=A9,wR=t(),p(nl.$$.fragment),xR=t(),D=i("div"),p(ig.$$.fragment),kR=t(),zF=i("p"),zF.textContent=R9,LR=t(),OF=i("p"),OF.innerHTML=P9,$R=t(),KF=i("ul"),KF.innerHTML=W9,BR=t(),eT=i("p"),eT.innerHTML=S9,jR=t(),p(tl.$$.fragment),Bw=t(),p(lg.$$.fragment),jw=t(),Ke=i("div"),p(dg.$$.fragment),ZR=t(),oT=i("p"),oT.innerHTML=G9,AR=t(),rT=i("p"),rT.innerHTML=V9,RR=t(),Tt=i("div"),p(mg.$$.fragment),PR=t(),nT=i("p"),nT.textContent=J9,WR=t(),tT=i("p"),tT.innerHTML=E9,SR=t(),p(al.$$.fragment),GR=t(),H=i("div"),p(cg.$$.fragment),VR=t(),aT=i("p"),aT.textContent=I9,JR=t(),sT=i("p"),sT.innerHTML=U9,ER=t(),iT=i("ul"),iT.innerHTML=N9,IR=t(),lT=i("p"),lT.innerHTML=X9,UR=t(),p(sl.$$.fragment),Zw=t(),p(fg.$$.fragment),Aw=t(),gg=i("div"),p(pg.$$.fragment),Rw=t(),p(hg.$$.fragment),Pw=t(),eo=i("div"),p(ug.$$.fragment),NR=t(),dT=i("p"),dT.innerHTML=q9,XR=t(),mT=i("p"),mT.innerHTML=Q9,qR=t(),Ct=i("div"),p(_g.$$.fragment),QR=t(),cT=i("p"),cT.textContent=D9,DR=t(),fT=i("p"),fT.innerHTML=H9,HR=t(),p(il.$$.fragment),YR=t(),Y=i("div"),p(bg.$$.fragment),zR=t(),gT=i("p"),gT.textContent=Y9,OR=t(),pT=i("p"),pT.innerHTML=z9,KR=t(),hT=i("ul"),hT.innerHTML=O9,eP=t(),uT=i("p"),uT.innerHTML=K9,oP=t(),p(ll.$$.fragment),Ww=t(),p(Mg.$$.fragment),Sw=t(),oo=i("div"),p(Fg.$$.fragment),rP=t(),_T=i("p"),_T.innerHTML=eN,nP=t(),bT=i("p"),bT.innerHTML=oN,tP=t(),yt=i("div"),p(Tg.$$.fragment),aP=t(),MT=i("p"),MT.textContent=rN,sP=t(),FT=i("p"),FT.innerHTML=nN,iP=t(),p(dl.$$.fragment),lP=t(),gr=i("div"),p(Cg.$$.fragment),dP=t(),TT=i("p"),TT.textContent=tN,mP=t(),CT=i("p"),CT.innerHTML=aN,cP=t(),yT=i("ul"),yT.innerHTML=sN,fP=t(),p(ml.$$.fragment),Gw=t(),p(yg.$$.fragment),Vw=t(),ro=i("div"),p(vg.$$.fragment),gP=t(),vT=i("p"),vT.innerHTML=iN,pP=t(),wT=i("p"),wT.innerHTML=lN,hP=t(),vt=i("div"),p(wg.$$.fragment),uP=t(),xT=i("p"),xT.textContent=dN,_P=t(),kT=i("p"),kT.innerHTML=mN,bP=t(),p(cl.$$.fragment),MP=t(),z=i("div"),p(xg.$$.fragment),FP=t(),LT=i("p"),LT.textContent=cN,TP=t(),$T=i("p"),$T.innerHTML=fN,CP=t(),BT=i("ul"),BT.innerHTML=gN,yP=t(),jT=i("p"),jT.innerHTML=pN,vP=t(),p(fl.$$.fragment),Jw=t(),p(kg.$$.fragment),Ew=t(),no=i("div"),p(Lg.$$.fragment),wP=t(),ZT=i("p"),ZT.innerHTML=hN,xP=t(),AT=i("p"),AT.innerHTML=uN,kP=t(),wt=i("div"),p($g.$$.fragment),LP=t(),RT=i("p"),RT.textContent=_N,$P=t(),PT=i("p"),PT.innerHTML=bN,BP=t(),p(gl.$$.fragment),jP=t(),O=i("div"),p(Bg.$$.fragment),ZP=t(),WT=i("p"),WT.textContent=MN,AP=t(),ST=i("p"),ST.innerHTML=FN,RP=t(),GT=i("ul"),GT.innerHTML=TN,PP=t(),VT=i("p"),VT.innerHTML=CN,WP=t(),p(pl.$$.fragment),Iw=t(),p(jg.$$.fragment),Uw=t(),to=i("div"),p(Zg.$$.fragment),SP=t(),JT=i("p"),JT.innerHTML=yN,GP=t(),ET=i("p"),ET.innerHTML=vN,VP=t(),xt=i("div"),p(Ag.$$.fragment),JP=t(),IT=i("p"),IT.textContent=wN,EP=t(),UT=i("p"),UT.innerHTML=xN,IP=t(),p(hl.$$.fragment),UP=t(),K=i("div"),p(Rg.$$.fragment),NP=t(),NT=i("p"),NT.textContent=kN,XP=t(),XT=i("p"),XT.innerHTML=LN,qP=t(),qT=i("ul"),qT.innerHTML=$N,QP=t(),QT=i("p"),QT.innerHTML=BN,DP=t(),p(ul.$$.fragment),Nw=t(),p(Pg.$$.fragment),Xw=t(),ao=i("div"),p(Wg.$$.fragment),HP=t(),DT=i("p"),DT.innerHTML=jN,YP=t(),HT=i("p"),HT.innerHTML=ZN,zP=t(),kt=i("div"),p(Sg.$$.fragment),OP=t(),YT=i("p"),YT.textContent=AN,KP=t(),zT=i("p"),zT.innerHTML=RN,eW=t(),p(_l.$$.fragment),oW=t(),pr=i("div"),p(Gg.$$.fragment),rW=t(),OT=i("p"),OT.textContent=PN,nW=t(),KT=i("p"),KT.innerHTML=WN,tW=t(),eC=i("ul"),eC.innerHTML=SN,aW=t(),p(bl.$$.fragment),qw=t(),p(Vg.$$.fragment),Qw=t(),so=i("div"),p(Jg.$$.fragment),sW=t(),oC=i("p"),oC.innerHTML=GN,iW=t(),rC=i("p"),rC.innerHTML=VN,lW=t(),Lt=i("div"),p(Eg.$$.fragment),dW=t(),nC=i("p"),nC.textContent=JN,mW=t(),tC=i("p"),tC.innerHTML=EN,cW=t(),p(Ml.$$.fragment),fW=t(),ee=i("div"),p(Ig.$$.fragment),gW=t(),aC=i("p"),aC.textContent=IN,pW=t(),sC=i("p"),sC.innerHTML=UN,hW=t(),iC=i("ul"),iC.innerHTML=NN,uW=t(),lC=i("p"),lC.innerHTML=XN,_W=t(),p(Fl.$$.fragment),Dw=t(),p(Ug.$$.fragment),Hw=t(),Ng=i("p"),Ng.textContent=qN,Yw=t(),p(Xg.$$.fragment),zw=t(),io=i("div"),p(qg.$$.fragment),bW=t(),dC=i("p"),dC.innerHTML=QN,MW=t(),mC=i("p"),mC.innerHTML=DN,FW=t(),$t=i("div"),p(Qg.$$.fragment),TW=t(),cC=i("p"),cC.textContent=HN,CW=t(),fC=i("p"),fC.innerHTML=YN,yW=t(),p(Tl.$$.fragment),vW=t(),oe=i("div"),p(Dg.$$.fragment),wW=t(),gC=i("p"),gC.textContent=zN,xW=t(),pC=i("p"),pC.innerHTML=ON,kW=t(),hC=i("ul"),hC.innerHTML=KN,LW=t(),uC=i("p"),uC.innerHTML=e5,$W=t(),p(Cl.$$.fragment),Ow=t(),p(Hg.$$.fragment),Kw=t(),lo=i("div"),p(Yg.$$.fragment),BW=t(),_C=i("p"),_C.innerHTML=o5,jW=t(),bC=i("p"),bC.innerHTML=r5,ZW=t(),Bt=i("div"),p(zg.$$.fragment),AW=t(),MC=i("p"),MC.textContent=n5,RW=t(),FC=i("p"),FC.innerHTML=t5,PW=t(),p(yl.$$.fragment),WW=t(),hr=i("div"),p(Og.$$.fragment),SW=t(),TC=i("p"),TC.textContent=a5,GW=t(),CC=i("p"),CC.innerHTML=s5,VW=t(),yC=i("ul"),yC.innerHTML=i5,JW=t(),p(vl.$$.fragment),ex=t(),p(Kg.$$.fragment),ox=t(),mo=i("div"),p(ep.$$.fragment),EW=t(),vC=i("p"),vC.innerHTML=l5,IW=t(),wC=i("p"),wC.innerHTML=d5,UW=t(),jt=i("div"),p(op.$$.fragment),NW=t(),xC=i("p"),xC.textContent=m5,XW=t(),kC=i("p"),kC.innerHTML=c5,qW=t(),p(wl.$$.fragment),QW=t(),re=i("div"),p(rp.$$.fragment),DW=t(),LC=i("p"),LC.textContent=f5,HW=t(),$C=i("p"),$C.innerHTML=g5,YW=t(),BC=i("ul"),BC.innerHTML=p5,zW=t(),jC=i("p"),jC.innerHTML=h5,OW=t(),p(xl.$$.fragment),rx=t(),p(np.$$.fragment),nx=t(),co=i("div"),p(tp.$$.fragment),KW=t(),ZC=i("p"),ZC.innerHTML=u5,eS=t(),AC=i("p"),AC.innerHTML=_5,oS=t(),Zt=i("div"),p(ap.$$.fragment),rS=t(),RC=i("p"),RC.textContent=b5,nS=t(),PC=i("p"),PC.innerHTML=M5,tS=t(),p(kl.$$.fragment),aS=t(),ne=i("div"),p(sp.$$.fragment),sS=t(),WC=i("p"),WC.textContent=F5,iS=t(),SC=i("p"),SC.innerHTML=T5,lS=t(),GC=i("ul"),GC.innerHTML=C5,dS=t(),VC=i("p"),VC.innerHTML=y5,mS=t(),p(Ll.$$.fragment),tx=t(),p(ip.$$.fragment),ax=t(),fo=i("div"),p(lp.$$.fragment),cS=t(),JC=i("p"),JC.innerHTML=v5,fS=t(),EC=i("p"),EC.innerHTML=w5,gS=t(),At=i("div"),p(dp.$$.fragment),pS=t(),IC=i("p"),IC.textContent=x5,hS=t(),UC=i("p"),UC.innerHTML=k5,uS=t(),p($l.$$.fragment),_S=t(),te=i("div"),p(mp.$$.fragment),bS=t(),NC=i("p"),NC.textContent=L5,MS=t(),XC=i("p"),XC.innerHTML=$5,FS=t(),qC=i("ul"),qC.innerHTML=B5,TS=t(),QC=i("p"),QC.innerHTML=j5,CS=t(),p(Bl.$$.fragment),sx=t(),p(cp.$$.fragment),ix=t(),go=i("div"),p(fp.$$.fragment),yS=t(),DC=i("p"),DC.innerHTML=Z5,vS=t(),HC=i("p"),HC.innerHTML=A5,wS=t(),Rt=i("div"),p(gp.$$.fragment),xS=t(),YC=i("p"),YC.textContent=R5,kS=t(),zC=i("p"),zC.innerHTML=P5,LS=t(),p(jl.$$.fragment),$S=t(),ur=i("div"),p(pp.$$.fragment),BS=t(),OC=i("p"),OC.textContent=W5,jS=t(),KC=i("p"),KC.innerHTML=S5,ZS=t(),ey=i("ul"),ey.innerHTML=G5,AS=t(),p(Zl.$$.fragment),lx=t(),p(hp.$$.fragment),dx=t(),po=i("div"),p(up.$$.fragment),RS=t(),oy=i("p"),oy.innerHTML=V5,PS=t(),ry=i("p"),ry.innerHTML=J5,WS=t(),Pt=i("div"),p(_p.$$.fragment),SS=t(),ny=i("p"),ny.textContent=E5,GS=t(),ty=i("p"),ty.innerHTML=I5,VS=t(),p(Al.$$.fragment),JS=t(),_r=i("div"),p(bp.$$.fragment),ES=t(),ay=i("p"),ay.textContent=U5,IS=t(),sy=i("p"),sy.innerHTML=N5,US=t(),iy=i("ul"),iy.innerHTML=X5,NS=t(),p(Rl.$$.fragment),mx=t(),p(Mp.$$.fragment),cx=t(),ho=i("div"),p(Fp.$$.fragment),XS=t(),ly=i("p"),ly.innerHTML=q5,qS=t(),dy=i("p"),dy.innerHTML=Q5,QS=t(),Wt=i("div"),p(Tp.$$.fragment),DS=t(),my=i("p"),my.textContent=D5,HS=t(),cy=i("p"),cy.innerHTML=H5,YS=t(),p(Pl.$$.fragment),zS=t(),ae=i("div"),p(Cp.$$.fragment),OS=t(),fy=i("p"),fy.textContent=Y5,KS=t(),gy=i("p"),gy.innerHTML=z5,eG=t(),py=i("ul"),py.innerHTML=O5,oG=t(),hy=i("p"),hy.innerHTML=K5,rG=t(),p(Wl.$$.fragment),fx=t(),p(yp.$$.fragment),gx=t(),vp=i("div"),p(wp.$$.fragment),px=t(),p(xp.$$.fragment),hx=t(),kp=i("div"),p(Lp.$$.fragment),ux=t(),p($p.$$.fragment),_x=t(),Bp=i("p"),Bp.textContent=e4,bx=t(),p(jp.$$.fragment),Mx=t(),uo=i("div"),p(Zp.$$.fragment),nG=t(),uy=i("p"),uy.innerHTML=o4,tG=t(),_y=i("p"),_y.innerHTML=r4,aG=t(),St=i("div"),p(Ap.$$.fragment),sG=t(),by=i("p"),by.textContent=n4,iG=t(),My=i("p"),My.innerHTML=t4,lG=t(),p(Sl.$$.fragment),dG=t(),se=i("div"),p(Rp.$$.fragment),mG=t(),Fy=i("p"),Fy.textContent=a4,cG=t(),Ty=i("p"),Ty.innerHTML=s4,fG=t(),Cy=i("ul"),Cy.innerHTML=i4,gG=t(),yy=i("p"),yy.innerHTML=l4,pG=t(),p(Gl.$$.fragment),Fx=t(),p(Pp.$$.fragment),Tx=t(),_o=i("div"),p(Wp.$$.fragment),hG=t(),vy=i("p"),vy.innerHTML=d4,uG=t(),wy=i("p"),wy.innerHTML=m4,_G=t(),Gt=i("div"),p(Sp.$$.fragment),bG=t(),xy=i("p"),xy.textContent=c4,MG=t(),ky=i("p"),ky.innerHTML=f4,FG=t(),p(Vl.$$.fragment),TG=t(),br=i("div"),p(Gp.$$.fragment),CG=t(),Ly=i("p"),Ly.textContent=g4,yG=t(),$y=i("p"),$y.innerHTML=p4,vG=t(),By=i("ul"),By.innerHTML=h4,wG=t(),p(Jl.$$.fragment),Cx=t(),p(Vp.$$.fragment),yx=t(),bo=i("div"),p(Jp.$$.fragment),xG=t(),jy=i("p"),jy.innerHTML=u4,kG=t(),Zy=i("p"),Zy.innerHTML=_4,LG=t(),Vt=i("div"),p(Ep.$$.fragment),$G=t(),Ay=i("p"),Ay.textContent=b4,BG=t(),Ry=i("p"),Ry.innerHTML=M4,jG=t(),p(El.$$.fragment),ZG=t(),ie=i("div"),p(Ip.$$.fragment),AG=t(),Py=i("p"),Py.textContent=F4,RG=t(),Wy=i("p"),Wy.innerHTML=T4,PG=t(),Sy=i("ul"),Sy.innerHTML=C4,WG=t(),Gy=i("p"),Gy.innerHTML=y4,SG=t(),p(Il.$$.fragment),vx=t(),p(Up.$$.fragment),wx=t(),Mo=i("div"),p(Np.$$.fragment),GG=t(),Vy=i("p"),Vy.innerHTML=v4,VG=t(),Jy=i("p"),Jy.innerHTML=w4,JG=t(),Jt=i("div"),p(Xp.$$.fragment),EG=t(),Ey=i("p"),Ey.textContent=x4,IG=t(),Iy=i("p"),Iy.innerHTML=k4,UG=t(),p(Ul.$$.fragment),NG=t(),Mr=i("div"),p(qp.$$.fragment),XG=t(),Uy=i("p"),Uy.textContent=L4,qG=t(),Ny=i("p"),Ny.innerHTML=$4,QG=t(),Xy=i("ul"),Xy.innerHTML=B4,DG=t(),p(Nl.$$.fragment),xx=t(),p(Qp.$$.fragment),kx=t(),Fo=i("div"),p(Dp.$$.fragment),HG=t(),qy=i("p"),qy.innerHTML=j4,YG=t(),Qy=i("p"),Qy.innerHTML=Z4,zG=t(),Et=i("div"),p(Hp.$$.fragment),OG=t(),Dy=i("p"),Dy.textContent=A4,KG=t(),Hy=i("p"),Hy.innerHTML=R4,eV=t(),p(Xl.$$.fragment),oV=t(),le=i("div"),p(Yp.$$.fragment),rV=t(),Yy=i("p"),Yy.textContent=P4,nV=t(),zy=i("p"),zy.innerHTML=W4,tV=t(),Oy=i("ul"),Oy.innerHTML=S4,aV=t(),Ky=i("p"),Ky.innerHTML=G4,sV=t(),p(ql.$$.fragment),Lx=t(),p(zp.$$.fragment),$x=t(),To=i("div"),p(Op.$$.fragment),iV=t(),ev=i("p"),ev.innerHTML=V4,lV=t(),ov=i("p"),ov.innerHTML=J4,dV=t(),It=i("div"),p(Kp.$$.fragment),mV=t(),rv=i("p"),rv.textContent=E4,cV=t(),nv=i("p"),nv.innerHTML=I4,fV=t(),p(Ql.$$.fragment),gV=t(),de=i("div"),p(eh.$$.fragment),pV=t(),tv=i("p"),tv.textContent=U4,hV=t(),av=i("p"),av.innerHTML=N4,uV=t(),sv=i("ul"),sv.innerHTML=X4,_V=t(),iv=i("p"),iv.innerHTML=q4,bV=t(),p(Dl.$$.fragment),Bx=t(),p(oh.$$.fragment),jx=t(),Co=i("div"),p(rh.$$.fragment),MV=t(),lv=i("p"),lv.innerHTML=Q4,FV=t(),dv=i("p"),dv.innerHTML=D4,TV=t(),Ut=i("div"),p(nh.$$.fragment),CV=t(),mv=i("p"),mv.textContent=H4,yV=t(),cv=i("p"),cv.innerHTML=Y4,vV=t(),p(Hl.$$.fragment),wV=t(),Fr=i("div"),p(th.$$.fragment),xV=t(),fv=i("p"),fv.textContent=z4,kV=t(),gv=i("p"),gv.innerHTML=O4,LV=t(),pv=i("ul"),pv.innerHTML=K4,$V=t(),p(Yl.$$.fragment),Zx=t(),p(ah.$$.fragment),Ax=t(),yo=i("div"),p(sh.$$.fragment),BV=t(),hv=i("p"),hv.innerHTML=eX,jV=t(),uv=i("p"),uv.innerHTML=oX,ZV=t(),Nt=i("div"),p(ih.$$.fragment),AV=t(),_v=i("p"),_v.textContent=rX,RV=t(),bv=i("p"),bv.innerHTML=nX,PV=t(),p(zl.$$.fragment),WV=t(),Tr=i("div"),p(lh.$$.fragment),SV=t(),Mv=i("p"),Mv.textContent=tX,GV=t(),Fv=i("p"),Fv.innerHTML=aX,VV=t(),Tv=i("ul"),Tv.innerHTML=sX,JV=t(),p(Ol.$$.fragment),Rx=t(),Cv=i("p"),this.h()},l(o){const g=pX("svelte-u9bgzb",document.head);n=l(g,"META",{name:!0,content:!0}),g.forEach(d),C=a(o),m=l(o,"P",{}),v(m).forEach(d),s=a(o),h(c.$$.fragment,o),e=a(o),F=l(o,"P",{"data-svelte-h":!0}),f(F)!=="svelte-1sl6sct"&&(F.innerHTML=EV),kv=a(o),yd=l(o,"P",{"data-svelte-h":!0}),f(yd)!=="svelte-a9dor6"&&(yd.innerHTML=IV),Lv=a(o),h(vd.$$.fragment,o),$v=a(o),wd=l(o,"P",{"data-svelte-h":!0}),f(wd)!=="svelte-1a1e244"&&(wd.innerHTML=UV),Bv=a(o),xd=l(o,"P",{"data-svelte-h":!0}),f(xd)!=="svelte-km6kzj"&&(xd.innerHTML=NV),jv=a(o),h(kd.$$.fragment,o),Zv=a(o),Ld=l(o,"P",{"data-svelte-h":!0}),f(Ld)!=="svelte-11r6347"&&(Ld.innerHTML=XV),Av=a(o),h($d.$$.fragment,o),Rv=a(o),Bd=l(o,"P",{"data-svelte-h":!0}),f(Bd)!=="svelte-18w8uiz"&&(Bd.textContent=qV),Pv=a(o),h(ys.$$.fragment,o),Wv=a(o),h(jd.$$.fragment,o),Sv=a(o),me=l(o,"DIV",{class:!0});var Cr=v(me);h(Zd.$$.fragment,Cr),Sx=a(Cr),ph=l(Cr,"P",{"data-svelte-h":!0}),f(ph)!=="svelte-1lf76xf"&&(ph.innerHTML=QV),Gx=a(Cr),hh=l(Cr,"P",{"data-svelte-h":!0}),f(hh)!=="svelte-1n2cphd"&&(hh.innerHTML=DV),Vx=a(Cr),Io=l(Cr,"DIV",{class:!0});var yr=v(Io);h(Ad.$$.fragment,yr),Jx=a(yr),uh=l(yr,"P",{"data-svelte-h":!0}),f(uh)!=="svelte-102bmt5"&&(uh.textContent=HV),Ex=a(yr),_h=l(yr,"P",{"data-svelte-h":!0}),f(_h)!=="svelte-dfwk4w"&&(_h.innerHTML=YV),Ix=a(yr),bh=l(yr,"UL",{"data-svelte-h":!0}),f(bh)!=="svelte-1h9sy5q"&&(bh.innerHTML=zV),Ux=a(yr),h(vs.$$.fragment,yr),yr.forEach(d),Nx=a(Cr),ws=l(Cr,"DIV",{class:!0});var dh=v(ws);h(Rd.$$.fragment,dh),Xx=a(dh),Mh=l(dh,"P",{"data-svelte-h":!0}),f(Mh)!=="svelte-3x5rn5"&&(Mh.textContent=OV),dh.forEach(d),Cr.forEach(d),Gv=a(o),h(Pd.$$.fragment,o),Vv=a(o),ce=l(o,"DIV",{class:!0});var vr=v(ce);h(Wd.$$.fragment,vr),qx=a(vr),Fh=l(vr,"P",{"data-svelte-h":!0}),f(Fh)!=="svelte-1t2s2dk"&&(Fh.innerHTML=KV),Qx=a(vr),Th=l(vr,"P",{"data-svelte-h":!0}),f(Th)!=="svelte-1n2cphd"&&(Th.innerHTML=eJ),Dx=a(vr),Uo=l(vr,"DIV",{class:!0});var wr=v(Uo);h(Sd.$$.fragment,wr),Hx=a(wr),Ch=l(wr,"P",{"data-svelte-h":!0}),f(Ch)!=="svelte-1cgdf6a"&&(Ch.textContent=oJ),Yx=a(wr),yh=l(wr,"P",{"data-svelte-h":!0}),f(yh)!=="svelte-yjv30u"&&(yh.innerHTML=rJ),zx=a(wr),vh=l(wr,"UL",{"data-svelte-h":!0}),f(vh)!=="svelte-g2w2pr"&&(vh.innerHTML=nJ),Ox=a(wr),h(xs.$$.fragment,wr),wr.forEach(d),Kx=a(vr),ks=l(vr,"DIV",{class:!0});var mh=v(ks);h(Gd.$$.fragment,mh),ek=a(mh),wh=l(mh,"P",{"data-svelte-h":!0}),f(wh)!=="svelte-1dj9ip8"&&(wh.textContent=tJ),mh.forEach(d),vr.forEach(d),Jv=a(o),h(Vd.$$.fragment,o),Ev=a(o),fe=l(o,"DIV",{class:!0});var xr=v(fe);h(Jd.$$.fragment,xr),ok=a(xr),xh=l(xr,"P",{"data-svelte-h":!0}),f(xh)!=="svelte-fas6d4"&&(xh.innerHTML=aJ),rk=a(xr),kh=l(xr,"P",{"data-svelte-h":!0}),f(kh)!=="svelte-1n2cphd"&&(kh.innerHTML=sJ),nk=a(xr),j=l(xr,"DIV",{class:!0});var vo=v(j);h(Ed.$$.fragment,vo),tk=a(vo),Lh=l(vo,"P",{"data-svelte-h":!0}),f(Lh)!=="svelte-brndlr"&&(Lh.textContent=iJ),ak=a(vo),$h=l(vo,"P",{"data-svelte-h":!0}),f($h)!=="svelte-8hskw5"&&($h.innerHTML=lJ),sk=a(vo),Bh=l(vo,"UL",{"data-svelte-h":!0}),f(Bh)!=="svelte-186n9qe"&&(Bh.innerHTML=dJ),ik=a(vo),h(Ls.$$.fragment,vo),lk=a(vo),h($s.$$.fragment,vo),vo.forEach(d),dk=a(xr),Bs=l(xr,"DIV",{class:!0});var ch=v(Bs);h(Id.$$.fragment,ch),mk=a(ch),jh=l(ch,"P",{"data-svelte-h":!0}),f(jh)!=="svelte-1a46i61"&&(jh.textContent=mJ),ch.forEach(d),xr.forEach(d),Iv=a(o),h(Ud.$$.fragment,o),Uv=a(o),ge=l(o,"DIV",{class:!0});var kr=v(ge);h(Nd.$$.fragment,kr),ck=a(kr),Zh=l(kr,"P",{"data-svelte-h":!0}),f(Zh)!=="svelte-1hg3z8e"&&(Zh.innerHTML=cJ),fk=a(kr),Ah=l(kr,"P",{"data-svelte-h":!0}),f(Ah)!=="svelte-1n2cphd"&&(Ah.innerHTML=fJ),gk=a(kr),Z=l(kr,"DIV",{class:!0});var wo=v(Z);h(Xd.$$.fragment,wo),pk=a(wo),Rh=l(wo,"P",{"data-svelte-h":!0}),f(Rh)!=="svelte-cyiy9y"&&(Rh.textContent=gJ),hk=a(wo),Ph=l(wo,"P",{"data-svelte-h":!0}),f(Ph)!=="svelte-1vjaq5m"&&(Ph.innerHTML=pJ),uk=a(wo),Wh=l(wo,"UL",{"data-svelte-h":!0}),f(Wh)!=="svelte-19u9pwb"&&(Wh.innerHTML=hJ),_k=a(wo),h(js.$$.fragment,wo),bk=a(wo),h(Zs.$$.fragment,wo),wo.forEach(d),Mk=a(kr),As=l(kr,"DIV",{class:!0});var fh=v(As);h(qd.$$.fragment,fh),Fk=a(fh),Sh=l(fh,"P",{"data-svelte-h":!0}),f(Sh)!=="svelte-8qw1k0"&&(Sh.textContent=uJ),fh.forEach(d),kr.forEach(d),Nv=a(o),h(Qd.$$.fragment,o),Xv=a(o),pe=l(o,"DIV",{class:!0});var Lr=v(pe);h(Dd.$$.fragment,Lr),Tk=a(Lr),Gh=l(Lr,"P",{"data-svelte-h":!0}),f(Gh)!=="svelte-6onqlw"&&(Gh.innerHTML=_J),Ck=a(Lr),Vh=l(Lr,"P",{"data-svelte-h":!0}),f(Vh)!=="svelte-1n2cphd"&&(Vh.innerHTML=bJ),yk=a(Lr),A=l(Lr,"DIV",{class:!0});var xo=v(A);h(Hd.$$.fragment,xo),vk=a(xo),Jh=l(xo,"P",{"data-svelte-h":!0}),f(Jh)!=="svelte-ampft1"&&(Jh.textContent=MJ),wk=a(xo),Eh=l(xo,"P",{"data-svelte-h":!0}),f(Eh)!=="svelte-1lz4kgk"&&(Eh.innerHTML=FJ),xk=a(xo),Ih=l(xo,"UL",{"data-svelte-h":!0}),f(Ih)!=="svelte-oid12n"&&(Ih.innerHTML=TJ),kk=a(xo),h(Rs.$$.fragment,xo),Lk=a(xo),h(Ps.$$.fragment,xo),xo.forEach(d),$k=a(Lr),Ws=l(Lr,"DIV",{class:!0});var gh=v(Ws);h(Yd.$$.fragment,gh),Bk=a(gh),Uh=l(gh,"P",{"data-svelte-h":!0}),f(Uh)!=="svelte-1rfmb83"&&(Uh.textContent=CJ),gh.forEach(d),Lr.forEach(d),qv=a(o),h(zd.$$.fragment,o),Qv=a(o),Od=l(o,"P",{"data-svelte-h":!0}),f(Od)!=="svelte-waxqds"&&(Od.textContent=yJ),Dv=a(o),h(Kd.$$.fragment,o),Hv=a(o),he=l(o,"DIV",{class:!0});var $r=v(he);h(em.$$.fragment,$r),jk=a($r),Nh=l($r,"P",{"data-svelte-h":!0}),f(Nh)!=="svelte-mq8qch"&&(Nh.innerHTML=vJ),Zk=a($r),Xh=l($r,"P",{"data-svelte-h":!0}),f(Xh)!=="svelte-1n2cphd"&&(Xh.innerHTML=wJ),Ak=a($r),Sn=l($r,"DIV",{class:!0});var sa=v(Sn);h(om.$$.fragment,sa),Rk=a(sa),qh=l(sa,"P",{"data-svelte-h":!0}),f(qh)!=="svelte-ws3pq1"&&(qh.textContent=xJ),Pk=a(sa),Qh=l(sa,"P",{"data-svelte-h":!0}),f(Qh)!=="svelte-ora4gm"&&(Qh.innerHTML=kJ),Wk=a(sa),h(Ss.$$.fragment,sa),sa.forEach(d),Sk=a($r),R=l($r,"DIV",{class:!0});var ko=v(R);h(rm.$$.fragment,ko),Gk=a(ko),Dh=l(ko,"P",{"data-svelte-h":!0}),f(Dh)!=="svelte-mt76tr"&&(Dh.textContent=LJ),Vk=a(ko),Hh=l(ko,"P",{"data-svelte-h":!0}),f(Hh)!=="svelte-1enn9p2"&&(Hh.innerHTML=$J),Jk=a(ko),Yh=l(ko,"UL",{"data-svelte-h":!0}),f(Yh)!=="svelte-pi3yat"&&(Yh.innerHTML=BJ),Ek=a(ko),zh=l(ko,"P",{"data-svelte-h":!0}),f(zh)!=="svelte-tebhz6"&&(zh.innerHTML=jJ),Ik=a(ko),h(Gs.$$.fragment,ko),ko.forEach(d),$r.forEach(d),Yv=a(o),h(nm.$$.fragment,o),zv=a(o),ue=l(o,"DIV",{class:!0});var Br=v(ue);h(tm.$$.fragment,Br),Uk=a(Br),Oh=l(Br,"P",{"data-svelte-h":!0}),f(Oh)!=="svelte-mq8qch"&&(Oh.innerHTML=ZJ),Nk=a(Br),Kh=l(Br,"P",{"data-svelte-h":!0}),f(Kh)!=="svelte-1n2cphd"&&(Kh.innerHTML=AJ),Xk=a(Br),Gn=l(Br,"DIV",{class:!0});var ia=v(Gn);h(am.$$.fragment,ia),qk=a(ia),eu=l(ia,"P",{"data-svelte-h":!0}),f(eu)!=="svelte-ws3pq1"&&(eu.textContent=RJ),Qk=a(ia),ou=l(ia,"P",{"data-svelte-h":!0}),f(ou)!=="svelte-ora4gm"&&(ou.innerHTML=PJ),Dk=a(ia),h(Vs.$$.fragment,ia),ia.forEach(d),Hk=a(Br),No=l(Br,"DIV",{class:!0});var jr=v(No);h(sm.$$.fragment,jr),Yk=a(jr),ru=l(jr,"P",{"data-svelte-h":!0}),f(ru)!=="svelte-mt76tr"&&(ru.textContent=WJ),zk=a(jr),nu=l(jr,"P",{"data-svelte-h":!0}),f(nu)!=="svelte-1enn9p2"&&(nu.innerHTML=SJ),Ok=a(jr),tu=l(jr,"UL",{"data-svelte-h":!0}),f(tu)!=="svelte-irretu"&&(tu.innerHTML=GJ),Kk=a(jr),h(Js.$$.fragment,jr),jr.forEach(d),Br.forEach(d),Ov=a(o),h(im.$$.fragment,o),Kv=a(o),_e=l(o,"DIV",{class:!0});var Zr=v(_e);h(lm.$$.fragment,Zr),eL=a(Zr),au=l(Zr,"P",{"data-svelte-h":!0}),f(au)!=="svelte-mq8qch"&&(au.innerHTML=VJ),oL=a(Zr),su=l(Zr,"P",{"data-svelte-h":!0}),f(su)!=="svelte-1n2cphd"&&(su.innerHTML=JJ),rL=a(Zr),Vn=l(Zr,"DIV",{class:!0});var la=v(Vn);h(dm.$$.fragment,la),nL=a(la),iu=l(la,"P",{"data-svelte-h":!0}),f(iu)!=="svelte-ws3pq1"&&(iu.textContent=EJ),tL=a(la),lu=l(la,"P",{"data-svelte-h":!0}),f(lu)!=="svelte-ora4gm"&&(lu.innerHTML=IJ),aL=a(la),h(Es.$$.fragment,la),la.forEach(d),sL=a(Zr),Xo=l(Zr,"DIV",{class:!0});var Ar=v(Xo);h(mm.$$.fragment,Ar),iL=a(Ar),du=l(Ar,"P",{"data-svelte-h":!0}),f(du)!=="svelte-mt76tr"&&(du.textContent=UJ),lL=a(Ar),mu=l(Ar,"P",{"data-svelte-h":!0}),f(mu)!=="svelte-1enn9p2"&&(mu.innerHTML=NJ),dL=a(Ar),cu=l(Ar,"UL",{"data-svelte-h":!0}),f(cu)!=="svelte-erc8yu"&&(cu.innerHTML=XJ),mL=a(Ar),h(Is.$$.fragment,Ar),Ar.forEach(d),Zr.forEach(d),e2=a(o),h(cm.$$.fragment,o),o2=a(o),fm=l(o,"P",{"data-svelte-h":!0}),f(fm)!=="svelte-1lm34wk"&&(fm.textContent=qJ),r2=a(o),h(gm.$$.fragment,o),n2=a(o),be=l(o,"DIV",{class:!0});var Rr=v(be);h(pm.$$.fragment,Rr),cL=a(Rr),fu=l(Rr,"P",{"data-svelte-h":!0}),f(fu)!=="svelte-l4sbyr"&&(fu.innerHTML=QJ),fL=a(Rr),gu=l(Rr,"P",{"data-svelte-h":!0}),f(gu)!=="svelte-1n2cphd"&&(gu.innerHTML=DJ),gL=a(Rr),Jn=l(Rr,"DIV",{class:!0});var da=v(Jn);h(hm.$$.fragment,da),pL=a(da),pu=l(da,"P",{"data-svelte-h":!0}),f(pu)!=="svelte-qeb48j"&&(pu.textContent=HJ),hL=a(da),hu=l(da,"P",{"data-svelte-h":!0}),f(hu)!=="svelte-ora4gm"&&(hu.innerHTML=YJ),uL=a(da),h(Us.$$.fragment,da),da.forEach(d),_L=a(Rr),P=l(Rr,"DIV",{class:!0});var Lo=v(P);h(um.$$.fragment,Lo),bL=a(Lo),uu=l(Lo,"P",{"data-svelte-h":!0}),f(uu)!=="svelte-141b7sb"&&(uu.textContent=zJ),ML=a(Lo),_u=l(Lo,"P",{"data-svelte-h":!0}),f(_u)!=="svelte-1enn9p2"&&(_u.innerHTML=OJ),FL=a(Lo),bu=l(Lo,"UL",{"data-svelte-h":!0}),f(bu)!=="svelte-14cqy0e"&&(bu.innerHTML=KJ),TL=a(Lo),Mu=l(Lo,"P",{"data-svelte-h":!0}),f(Mu)!=="svelte-tebhz6"&&(Mu.innerHTML=e0),CL=a(Lo),h(Ns.$$.fragment,Lo),Lo.forEach(d),Rr.forEach(d),t2=a(o),h(_m.$$.fragment,o),a2=a(o),Me=l(o,"DIV",{class:!0});var Pr=v(Me);h(bm.$$.fragment,Pr),yL=a(Pr),Fu=l(Pr,"P",{"data-svelte-h":!0}),f(Fu)!=="svelte-l4sbyr"&&(Fu.innerHTML=o0),vL=a(Pr),Tu=l(Pr,"P",{"data-svelte-h":!0}),f(Tu)!=="svelte-1n2cphd"&&(Tu.innerHTML=r0),wL=a(Pr),En=l(Pr,"DIV",{class:!0});var ma=v(En);h(Mm.$$.fragment,ma),xL=a(ma),Cu=l(ma,"P",{"data-svelte-h":!0}),f(Cu)!=="svelte-qeb48j"&&(Cu.textContent=n0),kL=a(ma),yu=l(ma,"P",{"data-svelte-h":!0}),f(yu)!=="svelte-ora4gm"&&(yu.innerHTML=t0),LL=a(ma),h(Xs.$$.fragment,ma),ma.forEach(d),$L=a(Pr),qo=l(Pr,"DIV",{class:!0});var Wr=v(qo);h(Fm.$$.fragment,Wr),BL=a(Wr),vu=l(Wr,"P",{"data-svelte-h":!0}),f(vu)!=="svelte-141b7sb"&&(vu.textContent=a0),jL=a(Wr),wu=l(Wr,"P",{"data-svelte-h":!0}),f(wu)!=="svelte-1enn9p2"&&(wu.innerHTML=s0),ZL=a(Wr),xu=l(Wr,"UL",{"data-svelte-h":!0}),f(xu)!=="svelte-1naszaq"&&(xu.innerHTML=i0),AL=a(Wr),h(qs.$$.fragment,Wr),Wr.forEach(d),Pr.forEach(d),s2=a(o),h(Tm.$$.fragment,o),i2=a(o),Fe=l(o,"DIV",{class:!0});var Sr=v(Fe);h(Cm.$$.fragment,Sr),RL=a(Sr),ku=l(Sr,"P",{"data-svelte-h":!0}),f(ku)!=="svelte-l4sbyr"&&(ku.innerHTML=l0),PL=a(Sr),Lu=l(Sr,"P",{"data-svelte-h":!0}),f(Lu)!=="svelte-1n2cphd"&&(Lu.innerHTML=d0),WL=a(Sr),In=l(Sr,"DIV",{class:!0});var ca=v(In);h(ym.$$.fragment,ca),SL=a(ca),$u=l(ca,"P",{"data-svelte-h":!0}),f($u)!=="svelte-qeb48j"&&($u.textContent=m0),GL=a(ca),Bu=l(ca,"P",{"data-svelte-h":!0}),f(Bu)!=="svelte-ora4gm"&&(Bu.innerHTML=c0),VL=a(ca),h(Qs.$$.fragment,ca),ca.forEach(d),JL=a(Sr),Qo=l(Sr,"DIV",{class:!0});var Gr=v(Qo);h(vm.$$.fragment,Gr),EL=a(Gr),ju=l(Gr,"P",{"data-svelte-h":!0}),f(ju)!=="svelte-141b7sb"&&(ju.textContent=f0),IL=a(Gr),Zu=l(Gr,"P",{"data-svelte-h":!0}),f(Zu)!=="svelte-1enn9p2"&&(Zu.innerHTML=g0),UL=a(Gr),Au=l(Gr,"UL",{"data-svelte-h":!0}),f(Au)!=="svelte-1mkg22s"&&(Au.innerHTML=p0),NL=a(Gr),h(Ds.$$.fragment,Gr),Gr.forEach(d),Sr.forEach(d),l2=a(o),h(wm.$$.fragment,o),d2=a(o),xm=l(o,"P",{"data-svelte-h":!0}),f(xm)!=="svelte-pqqo9r"&&(xm.textContent=h0),m2=a(o),h(km.$$.fragment,o),c2=a(o),Te=l(o,"DIV",{class:!0});var Vr=v(Te);h(Lm.$$.fragment,Vr),XL=a(Vr),Ru=l(Vr,"P",{"data-svelte-h":!0}),f(Ru)!=="svelte-1khuyf2"&&(Ru.innerHTML=u0),qL=a(Vr),Pu=l(Vr,"P",{"data-svelte-h":!0}),f(Pu)!=="svelte-1n2cphd"&&(Pu.innerHTML=_0),QL=a(Vr),Un=l(Vr,"DIV",{class:!0});var fa=v(Un);h($m.$$.fragment,fa),DL=a(fa),Wu=l(fa,"P",{"data-svelte-h":!0}),f(Wu)!=="svelte-6spxaa"&&(Wu.textContent=b0),HL=a(fa),Su=l(fa,"P",{"data-svelte-h":!0}),f(Su)!=="svelte-ora4gm"&&(Su.innerHTML=M0),YL=a(fa),h(Hs.$$.fragment,fa),fa.forEach(d),zL=a(Vr),W=l(Vr,"DIV",{class:!0});var $o=v(W);h(Bm.$$.fragment,$o),OL=a($o),Gu=l($o,"P",{"data-svelte-h":!0}),f(Gu)!=="svelte-1b19um"&&(Gu.textContent=F0),KL=a($o),Vu=l($o,"P",{"data-svelte-h":!0}),f(Vu)!=="svelte-1enn9p2"&&(Vu.innerHTML=T0),e1=a($o),Ju=l($o,"UL",{"data-svelte-h":!0}),f(Ju)!=="svelte-1ouhuhz"&&(Ju.innerHTML=C0),o1=a($o),Eu=l($o,"P",{"data-svelte-h":!0}),f(Eu)!=="svelte-tebhz6"&&(Eu.innerHTML=y0),r1=a($o),h(Ys.$$.fragment,$o),$o.forEach(d),Vr.forEach(d),f2=a(o),h(jm.$$.fragment,o),g2=a(o),Ce=l(o,"DIV",{class:!0});var Jr=v(Ce);h(Zm.$$.fragment,Jr),n1=a(Jr),Iu=l(Jr,"P",{"data-svelte-h":!0}),f(Iu)!=="svelte-1khuyf2"&&(Iu.innerHTML=v0),t1=a(Jr),Uu=l(Jr,"P",{"data-svelte-h":!0}),f(Uu)!=="svelte-1n2cphd"&&(Uu.innerHTML=w0),a1=a(Jr),Nn=l(Jr,"DIV",{class:!0});var ga=v(Nn);h(Am.$$.fragment,ga),s1=a(ga),Nu=l(ga,"P",{"data-svelte-h":!0}),f(Nu)!=="svelte-6spxaa"&&(Nu.textContent=x0),i1=a(ga),Xu=l(ga,"P",{"data-svelte-h":!0}),f(Xu)!=="svelte-ora4gm"&&(Xu.innerHTML=k0),l1=a(ga),h(zs.$$.fragment,ga),ga.forEach(d),d1=a(Jr),Do=l(Jr,"DIV",{class:!0});var Er=v(Do);h(Rm.$$.fragment,Er),m1=a(Er),qu=l(Er,"P",{"data-svelte-h":!0}),f(qu)!=="svelte-1b19um"&&(qu.textContent=L0),c1=a(Er),Qu=l(Er,"P",{"data-svelte-h":!0}),f(Qu)!=="svelte-1enn9p2"&&(Qu.innerHTML=$0),f1=a(Er),Du=l(Er,"UL",{"data-svelte-h":!0}),f(Du)!=="svelte-11pesv1"&&(Du.innerHTML=B0),g1=a(Er),h(Os.$$.fragment,Er),Er.forEach(d),Jr.forEach(d),p2=a(o),h(Pm.$$.fragment,o),h2=a(o),ye=l(o,"DIV",{class:!0});var Ir=v(ye);h(Wm.$$.fragment,Ir),p1=a(Ir),Hu=l(Ir,"P",{"data-svelte-h":!0}),f(Hu)!=="svelte-1khuyf2"&&(Hu.innerHTML=j0),h1=a(Ir),Yu=l(Ir,"P",{"data-svelte-h":!0}),f(Yu)!=="svelte-1n2cphd"&&(Yu.innerHTML=Z0),u1=a(Ir),Xn=l(Ir,"DIV",{class:!0});var pa=v(Xn);h(Sm.$$.fragment,pa),_1=a(pa),zu=l(pa,"P",{"data-svelte-h":!0}),f(zu)!=="svelte-6spxaa"&&(zu.textContent=A0),b1=a(pa),Ou=l(pa,"P",{"data-svelte-h":!0}),f(Ou)!=="svelte-ora4gm"&&(Ou.innerHTML=R0),M1=a(pa),h(Ks.$$.fragment,pa),pa.forEach(d),F1=a(Ir),Ho=l(Ir,"DIV",{class:!0});var Ur=v(Ho);h(Gm.$$.fragment,Ur),T1=a(Ur),Ku=l(Ur,"P",{"data-svelte-h":!0}),f(Ku)!=="svelte-1b19um"&&(Ku.textContent=P0),C1=a(Ur),e_=l(Ur,"P",{"data-svelte-h":!0}),f(e_)!=="svelte-1enn9p2"&&(e_.innerHTML=W0),y1=a(Ur),o_=l(Ur,"UL",{"data-svelte-h":!0}),f(o_)!=="svelte-1vfqrr6"&&(o_.innerHTML=S0),v1=a(Ur),h(ei.$$.fragment,Ur),Ur.forEach(d),Ir.forEach(d),u2=a(o),h(Vm.$$.fragment,o),_2=a(o),ve=l(o,"DIV",{class:!0});var Nr=v(ve);h(Jm.$$.fragment,Nr),w1=a(Nr),r_=l(Nr,"P",{"data-svelte-h":!0}),f(r_)!=="svelte-1f40sf2"&&(r_.innerHTML=G0),x1=a(Nr),n_=l(Nr,"P",{"data-svelte-h":!0}),f(n_)!=="svelte-1n2cphd"&&(n_.innerHTML=V0),k1=a(Nr),qn=l(Nr,"DIV",{class:!0});var ha=v(qn);h(Em.$$.fragment,ha),L1=a(ha),t_=l(ha,"P",{"data-svelte-h":!0}),f(t_)!=="svelte-1l4n5m2"&&(t_.textContent=J0),$1=a(ha),a_=l(ha,"P",{"data-svelte-h":!0}),f(a_)!=="svelte-ora4gm"&&(a_.innerHTML=E0),B1=a(ha),h(oi.$$.fragment,ha),ha.forEach(d),j1=a(Nr),S=l(Nr,"DIV",{class:!0});var Bo=v(S);h(Im.$$.fragment,Bo),Z1=a(Bo),s_=l(Bo,"P",{"data-svelte-h":!0}),f(s_)!=="svelte-1hs4l6u"&&(s_.textContent=I0),A1=a(Bo),i_=l(Bo,"P",{"data-svelte-h":!0}),f(i_)!=="svelte-1enn9p2"&&(i_.innerHTML=U0),R1=a(Bo),l_=l(Bo,"UL",{"data-svelte-h":!0}),f(l_)!=="svelte-9x63u2"&&(l_.innerHTML=N0),P1=a(Bo),d_=l(Bo,"P",{"data-svelte-h":!0}),f(d_)!=="svelte-tebhz6"&&(d_.innerHTML=X0),W1=a(Bo),h(ri.$$.fragment,Bo),Bo.forEach(d),Nr.forEach(d),b2=a(o),h(Um.$$.fragment,o),M2=a(o),we=l(o,"DIV",{class:!0});var Xr=v(we);h(Nm.$$.fragment,Xr),S1=a(Xr),m_=l(Xr,"P",{"data-svelte-h":!0}),f(m_)!=="svelte-1f40sf2"&&(m_.innerHTML=q0),G1=a(Xr),c_=l(Xr,"P",{"data-svelte-h":!0}),f(c_)!=="svelte-1n2cphd"&&(c_.innerHTML=Q0),V1=a(Xr),Qn=l(Xr,"DIV",{class:!0});var ua=v(Qn);h(Xm.$$.fragment,ua),J1=a(ua),f_=l(ua,"P",{"data-svelte-h":!0}),f(f_)!=="svelte-1l4n5m2"&&(f_.textContent=D0),E1=a(ua),g_=l(ua,"P",{"data-svelte-h":!0}),f(g_)!=="svelte-ora4gm"&&(g_.innerHTML=H0),I1=a(ua),h(ni.$$.fragment,ua),ua.forEach(d),U1=a(Xr),Yo=l(Xr,"DIV",{class:!0});var qr=v(Yo);h(qm.$$.fragment,qr),N1=a(qr),p_=l(qr,"P",{"data-svelte-h":!0}),f(p_)!=="svelte-1hs4l6u"&&(p_.textContent=Y0),X1=a(qr),h_=l(qr,"P",{"data-svelte-h":!0}),f(h_)!=="svelte-1enn9p2"&&(h_.innerHTML=z0),q1=a(qr),u_=l(qr,"UL",{"data-svelte-h":!0}),f(u_)!=="svelte-1gouak"&&(u_.innerHTML=O0),Q1=a(qr),h(ti.$$.fragment,qr),qr.forEach(d),Xr.forEach(d),F2=a(o),h(Qm.$$.fragment,o),T2=a(o),xe=l(o,"DIV",{class:!0});var Qr=v(xe);h(Dm.$$.fragment,Qr),D1=a(Qr),__=l(Qr,"P",{"data-svelte-h":!0}),f(__)!=="svelte-1f40sf2"&&(__.innerHTML=K0),H1=a(Qr),b_=l(Qr,"P",{"data-svelte-h":!0}),f(b_)!=="svelte-1n2cphd"&&(b_.innerHTML=eE),Y1=a(Qr),Dn=l(Qr,"DIV",{class:!0});var _a=v(Dn);h(Hm.$$.fragment,_a),z1=a(_a),M_=l(_a,"P",{"data-svelte-h":!0}),f(M_)!=="svelte-1l4n5m2"&&(M_.textContent=oE),O1=a(_a),F_=l(_a,"P",{"data-svelte-h":!0}),f(F_)!=="svelte-ora4gm"&&(F_.innerHTML=rE),K1=a(_a),h(ai.$$.fragment,_a),_a.forEach(d),e$=a(Qr),zo=l(Qr,"DIV",{class:!0});var Dr=v(zo);h(Ym.$$.fragment,Dr),o$=a(Dr),T_=l(Dr,"P",{"data-svelte-h":!0}),f(T_)!=="svelte-1hs4l6u"&&(T_.textContent=nE),r$=a(Dr),C_=l(Dr,"P",{"data-svelte-h":!0}),f(C_)!=="svelte-1enn9p2"&&(C_.innerHTML=tE),n$=a(Dr),y_=l(Dr,"UL",{"data-svelte-h":!0}),f(y_)!=="svelte-gaeouc"&&(y_.innerHTML=aE),t$=a(Dr),h(si.$$.fragment,Dr),Dr.forEach(d),Qr.forEach(d),C2=a(o),h(zm.$$.fragment,o),y2=a(o),Om=l(o,"DIV",{class:!0});var yv=v(Om);h(Km.$$.fragment,yv),yv.forEach(d),v2=a(o),h(ec.$$.fragment,o),w2=a(o),oc=l(o,"DIV",{class:!0});var vv=v(oc);h(rc.$$.fragment,vv),vv.forEach(d),x2=a(o),h(nc.$$.fragment,o),k2=a(o),ke=l(o,"DIV",{class:!0});var Hr=v(ke);h(tc.$$.fragment,Hr),a$=a(Hr),v_=l(Hr,"P",{"data-svelte-h":!0}),f(v_)!=="svelte-269w72"&&(v_.innerHTML=sE),s$=a(Hr),w_=l(Hr,"P",{"data-svelte-h":!0}),f(w_)!=="svelte-1n2cphd"&&(w_.innerHTML=iE),i$=a(Hr),Hn=l(Hr,"DIV",{class:!0});var ba=v(Hn);h(ac.$$.fragment,ba),l$=a(ba),x_=l(ba,"P",{"data-svelte-h":!0}),f(x_)!=="svelte-1d0i3aa"&&(x_.textContent=lE),d$=a(ba),k_=l(ba,"P",{"data-svelte-h":!0}),f(k_)!=="svelte-ora4gm"&&(k_.innerHTML=dE),m$=a(ba),h(ii.$$.fragment,ba),ba.forEach(d),c$=a(Hr),G=l(Hr,"DIV",{class:!0});var jo=v(G);h(sc.$$.fragment,jo),f$=a(jo),L_=l(jo,"P",{"data-svelte-h":!0}),f(L_)!=="svelte-196fe96"&&(L_.textContent=mE),g$=a(jo),$_=l(jo,"P",{"data-svelte-h":!0}),f($_)!=="svelte-1enn9p2"&&($_.innerHTML=cE),p$=a(jo),B_=l(jo,"UL",{"data-svelte-h":!0}),f(B_)!=="svelte-8ixvm"&&(B_.innerHTML=fE),h$=a(jo),j_=l(jo,"P",{"data-svelte-h":!0}),f(j_)!=="svelte-tebhz6"&&(j_.innerHTML=gE),u$=a(jo),h(li.$$.fragment,jo),jo.forEach(d),Hr.forEach(d),L2=a(o),h(ic.$$.fragment,o),$2=a(o),Le=l(o,"DIV",{class:!0});var Yr=v(Le);h(lc.$$.fragment,Yr),_$=a(Yr),Z_=l(Yr,"P",{"data-svelte-h":!0}),f(Z_)!=="svelte-269w72"&&(Z_.innerHTML=pE),b$=a(Yr),A_=l(Yr,"P",{"data-svelte-h":!0}),f(A_)!=="svelte-1n2cphd"&&(A_.innerHTML=hE),M$=a(Yr),Yn=l(Yr,"DIV",{class:!0});var Ma=v(Yn);h(dc.$$.fragment,Ma),F$=a(Ma),R_=l(Ma,"P",{"data-svelte-h":!0}),f(R_)!=="svelte-1d0i3aa"&&(R_.textContent=uE),T$=a(Ma),P_=l(Ma,"P",{"data-svelte-h":!0}),f(P_)!=="svelte-ora4gm"&&(P_.innerHTML=_E),C$=a(Ma),h(di.$$.fragment,Ma),Ma.forEach(d),y$=a(Yr),Oo=l(Yr,"DIV",{class:!0});var zr=v(Oo);h(mc.$$.fragment,zr),v$=a(zr),W_=l(zr,"P",{"data-svelte-h":!0}),f(W_)!=="svelte-196fe96"&&(W_.textContent=bE),w$=a(zr),S_=l(zr,"P",{"data-svelte-h":!0}),f(S_)!=="svelte-1enn9p2"&&(S_.innerHTML=ME),x$=a(zr),G_=l(zr,"UL",{"data-svelte-h":!0}),f(G_)!=="svelte-2f0zyu"&&(G_.innerHTML=FE),k$=a(zr),h(mi.$$.fragment,zr),zr.forEach(d),Yr.forEach(d),B2=a(o),h(cc.$$.fragment,o),j2=a(o),$e=l(o,"DIV",{class:!0});var Or=v($e);h(fc.$$.fragment,Or),L$=a(Or),V_=l(Or,"P",{"data-svelte-h":!0}),f(V_)!=="svelte-269w72"&&(V_.innerHTML=TE),$$=a(Or),J_=l(Or,"P",{"data-svelte-h":!0}),f(J_)!=="svelte-1n2cphd"&&(J_.innerHTML=CE),B$=a(Or),zn=l(Or,"DIV",{class:!0});var Fa=v(zn);h(gc.$$.fragment,Fa),j$=a(Fa),E_=l(Fa,"P",{"data-svelte-h":!0}),f(E_)!=="svelte-1d0i3aa"&&(E_.textContent=yE),Z$=a(Fa),I_=l(Fa,"P",{"data-svelte-h":!0}),f(I_)!=="svelte-ora4gm"&&(I_.innerHTML=vE),A$=a(Fa),h(ci.$$.fragment,Fa),Fa.forEach(d),R$=a(Or),Ko=l(Or,"DIV",{class:!0});var Kr=v(Ko);h(pc.$$.fragment,Kr),P$=a(Kr),U_=l(Kr,"P",{"data-svelte-h":!0}),f(U_)!=="svelte-196fe96"&&(U_.textContent=wE),W$=a(Kr),N_=l(Kr,"P",{"data-svelte-h":!0}),f(N_)!=="svelte-1enn9p2"&&(N_.innerHTML=xE),S$=a(Kr),X_=l(Kr,"UL",{"data-svelte-h":!0}),f(X_)!=="svelte-13xfjpk"&&(X_.innerHTML=kE),G$=a(Kr),h(fi.$$.fragment,Kr),Kr.forEach(d),Or.forEach(d),Z2=a(o),h(hc.$$.fragment,o),A2=a(o),Be=l(o,"DIV",{class:!0});var en=v(Be);h(uc.$$.fragment,en),V$=a(en),q_=l(en,"P",{"data-svelte-h":!0}),f(q_)!=="svelte-pnrvlz"&&(q_.innerHTML=LE),J$=a(en),Q_=l(en,"P",{"data-svelte-h":!0}),f(Q_)!=="svelte-1n2cphd"&&(Q_.innerHTML=$E),E$=a(en),On=l(en,"DIV",{class:!0});var Ta=v(On);h(_c.$$.fragment,Ta),I$=a(Ta),D_=l(Ta,"P",{"data-svelte-h":!0}),f(D_)!=="svelte-6xp6fj"&&(D_.textContent=BE),U$=a(Ta),H_=l(Ta,"P",{"data-svelte-h":!0}),f(H_)!=="svelte-ora4gm"&&(H_.innerHTML=jE),N$=a(Ta),h(gi.$$.fragment,Ta),Ta.forEach(d),X$=a(en),V=l(en,"DIV",{class:!0});var Zo=v(V);h(bc.$$.fragment,Zo),q$=a(Zo),Y_=l(Zo,"P",{"data-svelte-h":!0}),f(Y_)!=="svelte-10e8w47"&&(Y_.textContent=ZE),Q$=a(Zo),z_=l(Zo,"P",{"data-svelte-h":!0}),f(z_)!=="svelte-1enn9p2"&&(z_.innerHTML=AE),D$=a(Zo),O_=l(Zo,"UL",{"data-svelte-h":!0}),f(O_)!=="svelte-ygn5br"&&(O_.innerHTML=RE),H$=a(Zo),K_=l(Zo,"P",{"data-svelte-h":!0}),f(K_)!=="svelte-tebhz6"&&(K_.innerHTML=PE),Y$=a(Zo),h(pi.$$.fragment,Zo),Zo.forEach(d),en.forEach(d),R2=a(o),h(Mc.$$.fragment,o),P2=a(o),je=l(o,"DIV",{class:!0});var on=v(je);h(Fc.$$.fragment,on),z$=a(on),eb=l(on,"P",{"data-svelte-h":!0}),f(eb)!=="svelte-pnrvlz"&&(eb.innerHTML=WE),O$=a(on),ob=l(on,"P",{"data-svelte-h":!0}),f(ob)!=="svelte-1n2cphd"&&(ob.innerHTML=SE),K$=a(on),Kn=l(on,"DIV",{class:!0});var Ca=v(Kn);h(Tc.$$.fragment,Ca),eB=a(Ca),rb=l(Ca,"P",{"data-svelte-h":!0}),f(rb)!=="svelte-6xp6fj"&&(rb.textContent=GE),oB=a(Ca),nb=l(Ca,"P",{"data-svelte-h":!0}),f(nb)!=="svelte-ora4gm"&&(nb.innerHTML=VE),rB=a(Ca),h(hi.$$.fragment,Ca),Ca.forEach(d),nB=a(on),er=l(on,"DIV",{class:!0});var rn=v(er);h(Cc.$$.fragment,rn),tB=a(rn),tb=l(rn,"P",{"data-svelte-h":!0}),f(tb)!=="svelte-10e8w47"&&(tb.textContent=JE),aB=a(rn),ab=l(rn,"P",{"data-svelte-h":!0}),f(ab)!=="svelte-1enn9p2"&&(ab.innerHTML=EE),sB=a(rn),sb=l(rn,"UL",{"data-svelte-h":!0}),f(sb)!=="svelte-1ibqzy2"&&(sb.innerHTML=IE),iB=a(rn),h(ui.$$.fragment,rn),rn.forEach(d),on.forEach(d),W2=a(o),h(yc.$$.fragment,o),S2=a(o),Ze=l(o,"DIV",{class:!0});var nn=v(Ze);h(vc.$$.fragment,nn),lB=a(nn),ib=l(nn,"P",{"data-svelte-h":!0}),f(ib)!=="svelte-pnrvlz"&&(ib.innerHTML=UE),dB=a(nn),lb=l(nn,"P",{"data-svelte-h":!0}),f(lb)!=="svelte-1n2cphd"&&(lb.innerHTML=NE),mB=a(nn),et=l(nn,"DIV",{class:!0});var ya=v(et);h(wc.$$.fragment,ya),cB=a(ya),db=l(ya,"P",{"data-svelte-h":!0}),f(db)!=="svelte-6xp6fj"&&(db.textContent=XE),fB=a(ya),mb=l(ya,"P",{"data-svelte-h":!0}),f(mb)!=="svelte-ora4gm"&&(mb.innerHTML=qE),gB=a(ya),h(_i.$$.fragment,ya),ya.forEach(d),pB=a(nn),or=l(nn,"DIV",{class:!0});var tn=v(or);h(xc.$$.fragment,tn),hB=a(tn),cb=l(tn,"P",{"data-svelte-h":!0}),f(cb)!=="svelte-10e8w47"&&(cb.textContent=QE),uB=a(tn),fb=l(tn,"P",{"data-svelte-h":!0}),f(fb)!=="svelte-1enn9p2"&&(fb.innerHTML=DE),_B=a(tn),gb=l(tn,"UL",{"data-svelte-h":!0}),f(gb)!=="svelte-9592ms"&&(gb.innerHTML=HE),bB=a(tn),h(bi.$$.fragment,tn),tn.forEach(d),nn.forEach(d),G2=a(o),h(kc.$$.fragment,o),V2=a(o),Ae=l(o,"DIV",{class:!0});var an=v(Ae);h(Lc.$$.fragment,an),MB=a(an),pb=l(an,"P",{"data-svelte-h":!0}),f(pb)!=="svelte-yy14nj"&&(pb.innerHTML=YE),FB=a(an),hb=l(an,"P",{"data-svelte-h":!0}),f(hb)!=="svelte-1n2cphd"&&(hb.innerHTML=zE),TB=a(an),ot=l(an,"DIV",{class:!0});var va=v(ot);h($c.$$.fragment,va),CB=a(va),ub=l(va,"P",{"data-svelte-h":!0}),f(ub)!=="svelte-snlhsn"&&(ub.textContent=OE),yB=a(va),_b=l(va,"P",{"data-svelte-h":!0}),f(_b)!=="svelte-ora4gm"&&(_b.innerHTML=KE),vB=a(va),h(Mi.$$.fragment,va),va.forEach(d),wB=a(an),J=l(an,"DIV",{class:!0});var Ao=v(J);h(Bc.$$.fragment,Ao),xB=a(Ao),bb=l(Ao,"P",{"data-svelte-h":!0}),f(bb)!=="svelte-jtqyi7"&&(bb.textContent=eI),kB=a(Ao),Mb=l(Ao,"P",{"data-svelte-h":!0}),f(Mb)!=="svelte-1enn9p2"&&(Mb.innerHTML=oI),LB=a(Ao),Fb=l(Ao,"UL",{"data-svelte-h":!0}),f(Fb)!=="svelte-1sp6ypg"&&(Fb.innerHTML=rI),$B=a(Ao),Tb=l(Ao,"P",{"data-svelte-h":!0}),f(Tb)!=="svelte-tebhz6"&&(Tb.innerHTML=nI),BB=a(Ao),h(Fi.$$.fragment,Ao),Ao.forEach(d),an.forEach(d),J2=a(o),h(jc.$$.fragment,o),E2=a(o),Re=l(o,"DIV",{class:!0});var sn=v(Re);h(Zc.$$.fragment,sn),jB=a(sn),Cb=l(sn,"P",{"data-svelte-h":!0}),f(Cb)!=="svelte-yy14nj"&&(Cb.innerHTML=tI),ZB=a(sn),yb=l(sn,"P",{"data-svelte-h":!0}),f(yb)!=="svelte-1n2cphd"&&(yb.innerHTML=aI),AB=a(sn),rt=l(sn,"DIV",{class:!0});var wa=v(rt);h(Ac.$$.fragment,wa),RB=a(wa),vb=l(wa,"P",{"data-svelte-h":!0}),f(vb)!=="svelte-snlhsn"&&(vb.textContent=sI),PB=a(wa),wb=l(wa,"P",{"data-svelte-h":!0}),f(wb)!=="svelte-ora4gm"&&(wb.innerHTML=iI),WB=a(wa),h(Ti.$$.fragment,wa),wa.forEach(d),SB=a(sn),rr=l(sn,"DIV",{class:!0});var ln=v(rr);h(Rc.$$.fragment,ln),GB=a(ln),xb=l(ln,"P",{"data-svelte-h":!0}),f(xb)!=="svelte-jtqyi7"&&(xb.textContent=lI),VB=a(ln),kb=l(ln,"P",{"data-svelte-h":!0}),f(kb)!=="svelte-1enn9p2"&&(kb.innerHTML=dI),JB=a(ln),Lb=l(ln,"UL",{"data-svelte-h":!0}),f(Lb)!=="svelte-1eys2yo"&&(Lb.innerHTML=mI),EB=a(ln),h(Ci.$$.fragment,ln),ln.forEach(d),sn.forEach(d),I2=a(o),h(Pc.$$.fragment,o),U2=a(o),Pe=l(o,"DIV",{class:!0});var dn=v(Pe);h(Wc.$$.fragment,dn),IB=a(dn),$b=l(dn,"P",{"data-svelte-h":!0}),f($b)!=="svelte-yy14nj"&&($b.innerHTML=cI),UB=a(dn),Bb=l(dn,"P",{"data-svelte-h":!0}),f(Bb)!=="svelte-1n2cphd"&&(Bb.innerHTML=fI),NB=a(dn),nt=l(dn,"DIV",{class:!0});var xa=v(nt);h(Sc.$$.fragment,xa),XB=a(xa),jb=l(xa,"P",{"data-svelte-h":!0}),f(jb)!=="svelte-snlhsn"&&(jb.textContent=gI),qB=a(xa),Zb=l(xa,"P",{"data-svelte-h":!0}),f(Zb)!=="svelte-ora4gm"&&(Zb.innerHTML=pI),QB=a(xa),h(yi.$$.fragment,xa),xa.forEach(d),DB=a(dn),nr=l(dn,"DIV",{class:!0});var mn=v(nr);h(Gc.$$.fragment,mn),HB=a(mn),Ab=l(mn,"P",{"data-svelte-h":!0}),f(Ab)!=="svelte-jtqyi7"&&(Ab.textContent=hI),YB=a(mn),Rb=l(mn,"P",{"data-svelte-h":!0}),f(Rb)!=="svelte-1enn9p2"&&(Rb.innerHTML=uI),zB=a(mn),Pb=l(mn,"UL",{"data-svelte-h":!0}),f(Pb)!=="svelte-1n3o8or"&&(Pb.innerHTML=_I),OB=a(mn),h(vi.$$.fragment,mn),mn.forEach(d),dn.forEach(d),N2=a(o),h(Vc.$$.fragment,o),X2=a(o),We=l(o,"DIV",{class:!0});var cn=v(We);h(Jc.$$.fragment,cn),KB=a(cn),Wb=l(cn,"P",{"data-svelte-h":!0}),f(Wb)!=="svelte-iq2e91"&&(Wb.innerHTML=bI),ej=a(cn),Sb=l(cn,"P",{"data-svelte-h":!0}),f(Sb)!=="svelte-1n2cphd"&&(Sb.innerHTML=MI),oj=a(cn),tt=l(cn,"DIV",{class:!0});var ka=v(tt);h(Ec.$$.fragment,ka),rj=a(ka),Gb=l(ka,"P",{"data-svelte-h":!0}),f(Gb)!=="svelte-1yu679h"&&(Gb.textContent=FI),nj=a(ka),Vb=l(ka,"P",{"data-svelte-h":!0}),f(Vb)!=="svelte-ora4gm"&&(Vb.innerHTML=TI),tj=a(ka),h(wi.$$.fragment,ka),ka.forEach(d),aj=a(cn),E=l(cn,"DIV",{class:!0});var Ro=v(E);h(Ic.$$.fragment,Ro),sj=a(Ro),Jb=l(Ro,"P",{"data-svelte-h":!0}),f(Jb)!=="svelte-hx8awn"&&(Jb.textContent=CI),ij=a(Ro),Eb=l(Ro,"P",{"data-svelte-h":!0}),f(Eb)!=="svelte-1enn9p2"&&(Eb.innerHTML=yI),lj=a(Ro),Ib=l(Ro,"UL",{"data-svelte-h":!0}),f(Ib)!=="svelte-hux9nl"&&(Ib.innerHTML=vI),dj=a(Ro),Ub=l(Ro,"P",{"data-svelte-h":!0}),f(Ub)!=="svelte-tebhz6"&&(Ub.innerHTML=wI),mj=a(Ro),h(xi.$$.fragment,Ro),Ro.forEach(d),cn.forEach(d),q2=a(o),h(Uc.$$.fragment,o),Q2=a(o),Se=l(o,"DIV",{class:!0});var fn=v(Se);h(Nc.$$.fragment,fn),cj=a(fn),Nb=l(fn,"P",{"data-svelte-h":!0}),f(Nb)!=="svelte-iq2e91"&&(Nb.innerHTML=xI),fj=a(fn),Xb=l(fn,"P",{"data-svelte-h":!0}),f(Xb)!=="svelte-1n2cphd"&&(Xb.innerHTML=kI),gj=a(fn),at=l(fn,"DIV",{class:!0});var La=v(at);h(Xc.$$.fragment,La),pj=a(La),qb=l(La,"P",{"data-svelte-h":!0}),f(qb)!=="svelte-1yu679h"&&(qb.textContent=LI),hj=a(La),Qb=l(La,"P",{"data-svelte-h":!0}),f(Qb)!=="svelte-ora4gm"&&(Qb.innerHTML=$I),uj=a(La),h(ki.$$.fragment,La),La.forEach(d),_j=a(fn),tr=l(fn,"DIV",{class:!0});var gn=v(tr);h(qc.$$.fragment,gn),bj=a(gn),Db=l(gn,"P",{"data-svelte-h":!0}),f(Db)!=="svelte-hx8awn"&&(Db.textContent=BI),Mj=a(gn),Hb=l(gn,"P",{"data-svelte-h":!0}),f(Hb)!=="svelte-1enn9p2"&&(Hb.innerHTML=jI),Fj=a(gn),Yb=l(gn,"UL",{"data-svelte-h":!0}),f(Yb)!=="svelte-l1kxk6"&&(Yb.innerHTML=ZI),Tj=a(gn),h(Li.$$.fragment,gn),gn.forEach(d),fn.forEach(d),D2=a(o),h(Qc.$$.fragment,o),H2=a(o),Ge=l(o,"DIV",{class:!0});var pn=v(Ge);h(Dc.$$.fragment,pn),Cj=a(pn),zb=l(pn,"P",{"data-svelte-h":!0}),f(zb)!=="svelte-iq2e91"&&(zb.innerHTML=AI),yj=a(pn),Ob=l(pn,"P",{"data-svelte-h":!0}),f(Ob)!=="svelte-1n2cphd"&&(Ob.innerHTML=RI),vj=a(pn),st=l(pn,"DIV",{class:!0});var $a=v(st);h(Hc.$$.fragment,$a),wj=a($a),Kb=l($a,"P",{"data-svelte-h":!0}),f(Kb)!=="svelte-1yu679h"&&(Kb.textContent=PI),xj=a($a),eM=l($a,"P",{"data-svelte-h":!0}),f(eM)!=="svelte-ora4gm"&&(eM.innerHTML=WI),kj=a($a),h($i.$$.fragment,$a),$a.forEach(d),Lj=a(pn),ar=l(pn,"DIV",{class:!0});var hn=v(ar);h(Yc.$$.fragment,hn),$j=a(hn),oM=l(hn,"P",{"data-svelte-h":!0}),f(oM)!=="svelte-hx8awn"&&(oM.textContent=SI),Bj=a(hn),rM=l(hn,"P",{"data-svelte-h":!0}),f(rM)!=="svelte-1enn9p2"&&(rM.innerHTML=GI),jj=a(hn),nM=l(hn,"UL",{"data-svelte-h":!0}),f(nM)!=="svelte-1gw21xa"&&(nM.innerHTML=VI),Zj=a(hn),h(Bi.$$.fragment,hn),hn.forEach(d),pn.forEach(d),Y2=a(o),h(zc.$$.fragment,o),z2=a(o),Ve=l(o,"DIV",{class:!0});var un=v(Ve);h(Oc.$$.fragment,un),Aj=a(un),tM=l(un,"P",{"data-svelte-h":!0}),f(tM)!=="svelte-lyxahp"&&(tM.innerHTML=JI),Rj=a(un),aM=l(un,"P",{"data-svelte-h":!0}),f(aM)!=="svelte-1n2cphd"&&(aM.innerHTML=EI),Pj=a(un),it=l(un,"DIV",{class:!0});var Ba=v(it);h(Kc.$$.fragment,Ba),Wj=a(Ba),sM=l(Ba,"P",{"data-svelte-h":!0}),f(sM)!=="svelte-81lf85"&&(sM.textContent=II),Sj=a(Ba),iM=l(Ba,"P",{"data-svelte-h":!0}),f(iM)!=="svelte-ora4gm"&&(iM.innerHTML=UI),Gj=a(Ba),h(ji.$$.fragment,Ba),Ba.forEach(d),Vj=a(un),I=l(un,"DIV",{class:!0});var Po=v(I);h(ef.$$.fragment,Po),Jj=a(Po),lM=l(Po,"P",{"data-svelte-h":!0}),f(lM)!=="svelte-16vvmzn"&&(lM.textContent=NI),Ej=a(Po),dM=l(Po,"P",{"data-svelte-h":!0}),f(dM)!=="svelte-1enn9p2"&&(dM.innerHTML=XI),Ij=a(Po),mM=l(Po,"UL",{"data-svelte-h":!0}),f(mM)!=="svelte-1t8x6wl"&&(mM.innerHTML=qI),Uj=a(Po),cM=l(Po,"P",{"data-svelte-h":!0}),f(cM)!=="svelte-tebhz6"&&(cM.innerHTML=QI),Nj=a(Po),h(Zi.$$.fragment,Po),Po.forEach(d),un.forEach(d),O2=a(o),h(of.$$.fragment,o),K2=a(o),Je=l(o,"DIV",{class:!0});var _n=v(Je);h(rf.$$.fragment,_n),Xj=a(_n),fM=l(_n,"P",{"data-svelte-h":!0}),f(fM)!=="svelte-lyxahp"&&(fM.innerHTML=DI),qj=a(_n),gM=l(_n,"P",{"data-svelte-h":!0}),f(gM)!=="svelte-1n2cphd"&&(gM.innerHTML=HI),Qj=a(_n),lt=l(_n,"DIV",{class:!0});var ja=v(lt);h(nf.$$.fragment,ja),Dj=a(ja),pM=l(ja,"P",{"data-svelte-h":!0}),f(pM)!=="svelte-81lf85"&&(pM.textContent=YI),Hj=a(ja),hM=l(ja,"P",{"data-svelte-h":!0}),f(hM)!=="svelte-ora4gm"&&(hM.innerHTML=zI),Yj=a(ja),h(Ai.$$.fragment,ja),ja.forEach(d),zj=a(_n),sr=l(_n,"DIV",{class:!0});var bn=v(sr);h(tf.$$.fragment,bn),Oj=a(bn),uM=l(bn,"P",{"data-svelte-h":!0}),f(uM)!=="svelte-16vvmzn"&&(uM.textContent=OI),Kj=a(bn),_M=l(bn,"P",{"data-svelte-h":!0}),f(_M)!=="svelte-1enn9p2"&&(_M.innerHTML=KI),eZ=a(bn),bM=l(bn,"UL",{"data-svelte-h":!0}),f(bM)!=="svelte-919fcx"&&(bM.innerHTML=eU),oZ=a(bn),h(Ri.$$.fragment,bn),bn.forEach(d),_n.forEach(d),ew=a(o),h(af.$$.fragment,o),ow=a(o),Ee=l(o,"DIV",{class:!0});var Mn=v(Ee);h(sf.$$.fragment,Mn),rZ=a(Mn),MM=l(Mn,"P",{"data-svelte-h":!0}),f(MM)!=="svelte-lyxahp"&&(MM.innerHTML=oU),nZ=a(Mn),FM=l(Mn,"P",{"data-svelte-h":!0}),f(FM)!=="svelte-1n2cphd"&&(FM.innerHTML=rU),tZ=a(Mn),dt=l(Mn,"DIV",{class:!0});var Za=v(dt);h(lf.$$.fragment,Za),aZ=a(Za),TM=l(Za,"P",{"data-svelte-h":!0}),f(TM)!=="svelte-81lf85"&&(TM.textContent=nU),sZ=a(Za),CM=l(Za,"P",{"data-svelte-h":!0}),f(CM)!=="svelte-ora4gm"&&(CM.innerHTML=tU),iZ=a(Za),h(Pi.$$.fragment,Za),Za.forEach(d),lZ=a(Mn),ir=l(Mn,"DIV",{class:!0});var Fn=v(ir);h(df.$$.fragment,Fn),dZ=a(Fn),yM=l(Fn,"P",{"data-svelte-h":!0}),f(yM)!=="svelte-16vvmzn"&&(yM.textContent=aU),mZ=a(Fn),vM=l(Fn,"P",{"data-svelte-h":!0}),f(vM)!=="svelte-1enn9p2"&&(vM.innerHTML=sU),cZ=a(Fn),wM=l(Fn,"UL",{"data-svelte-h":!0}),f(wM)!=="svelte-ee2yqh"&&(wM.innerHTML=iU),fZ=a(Fn),h(Wi.$$.fragment,Fn),Fn.forEach(d),Mn.forEach(d),rw=a(o),h(mf.$$.fragment,o),nw=a(o),Ie=l(o,"DIV",{class:!0});var Tn=v(Ie);h(cf.$$.fragment,Tn),gZ=a(Tn),xM=l(Tn,"P",{"data-svelte-h":!0}),f(xM)!=="svelte-1ppojco"&&(xM.innerHTML=lU),pZ=a(Tn),kM=l(Tn,"P",{"data-svelte-h":!0}),f(kM)!=="svelte-1n2cphd"&&(kM.innerHTML=dU),hZ=a(Tn),mt=l(Tn,"DIV",{class:!0});var Aa=v(mt);h(ff.$$.fragment,Aa),uZ=a(Aa),LM=l(Aa,"P",{"data-svelte-h":!0}),f(LM)!=="svelte-n52yuc"&&(LM.textContent=mU),_Z=a(Aa),$M=l(Aa,"P",{"data-svelte-h":!0}),f($M)!=="svelte-ora4gm"&&($M.innerHTML=cU),bZ=a(Aa),h(Si.$$.fragment,Aa),Aa.forEach(d),MZ=a(Tn),U=l(Tn,"DIV",{class:!0});var Wo=v(U);h(gf.$$.fragment,Wo),FZ=a(Wo),BM=l(Wo,"P",{"data-svelte-h":!0}),f(BM)!=="svelte-1h7oepk"&&(BM.textContent=fU),TZ=a(Wo),jM=l(Wo,"P",{"data-svelte-h":!0}),f(jM)!=="svelte-1enn9p2"&&(jM.innerHTML=gU),CZ=a(Wo),ZM=l(Wo,"UL",{"data-svelte-h":!0}),f(ZM)!=="svelte-ifr3t2"&&(ZM.innerHTML=pU),yZ=a(Wo),AM=l(Wo,"P",{"data-svelte-h":!0}),f(AM)!=="svelte-tebhz6"&&(AM.innerHTML=hU),vZ=a(Wo),h(Gi.$$.fragment,Wo),Wo.forEach(d),Tn.forEach(d),tw=a(o),h(pf.$$.fragment,o),aw=a(o),Ue=l(o,"DIV",{class:!0});var Cn=v(Ue);h(hf.$$.fragment,Cn),wZ=a(Cn),RM=l(Cn,"P",{"data-svelte-h":!0}),f(RM)!=="svelte-1ppojco"&&(RM.innerHTML=uU),xZ=a(Cn),PM=l(Cn,"P",{"data-svelte-h":!0}),f(PM)!=="svelte-1n2cphd"&&(PM.innerHTML=_U),kZ=a(Cn),ct=l(Cn,"DIV",{class:!0});var Ra=v(ct);h(uf.$$.fragment,Ra),LZ=a(Ra),WM=l(Ra,"P",{"data-svelte-h":!0}),f(WM)!=="svelte-n52yuc"&&(WM.textContent=bU),$Z=a(Ra),SM=l(Ra,"P",{"data-svelte-h":!0}),f(SM)!=="svelte-ora4gm"&&(SM.innerHTML=MU),BZ=a(Ra),h(Vi.$$.fragment,Ra),Ra.forEach(d),jZ=a(Cn),lr=l(Cn,"DIV",{class:!0});var yn=v(lr);h(_f.$$.fragment,yn),ZZ=a(yn),GM=l(yn,"P",{"data-svelte-h":!0}),f(GM)!=="svelte-1h7oepk"&&(GM.textContent=FU),AZ=a(yn),VM=l(yn,"P",{"data-svelte-h":!0}),f(VM)!=="svelte-1enn9p2"&&(VM.innerHTML=TU),RZ=a(yn),JM=l(yn,"UL",{"data-svelte-h":!0}),f(JM)!=="svelte-s24k16"&&(JM.innerHTML=CU),PZ=a(yn),h(Ji.$$.fragment,yn),yn.forEach(d),Cn.forEach(d),sw=a(o),h(bf.$$.fragment,o),iw=a(o),Ne=l(o,"DIV",{class:!0});var vn=v(Ne);h(Mf.$$.fragment,vn),WZ=a(vn),EM=l(vn,"P",{"data-svelte-h":!0}),f(EM)!=="svelte-1ppojco"&&(EM.innerHTML=yU),SZ=a(vn),IM=l(vn,"P",{"data-svelte-h":!0}),f(IM)!=="svelte-1n2cphd"&&(IM.innerHTML=vU),GZ=a(vn),ft=l(vn,"DIV",{class:!0});var Pa=v(ft);h(Ff.$$.fragment,Pa),VZ=a(Pa),UM=l(Pa,"P",{"data-svelte-h":!0}),f(UM)!=="svelte-n52yuc"&&(UM.textContent=wU),JZ=a(Pa),NM=l(Pa,"P",{"data-svelte-h":!0}),f(NM)!=="svelte-ora4gm"&&(NM.innerHTML=xU),EZ=a(Pa),h(Ei.$$.fragment,Pa),Pa.forEach(d),IZ=a(vn),dr=l(vn,"DIV",{class:!0});var wn=v(dr);h(Tf.$$.fragment,wn),UZ=a(wn),XM=l(wn,"P",{"data-svelte-h":!0}),f(XM)!=="svelte-1h7oepk"&&(XM.textContent=kU),NZ=a(wn),qM=l(wn,"P",{"data-svelte-h":!0}),f(qM)!=="svelte-1enn9p2"&&(qM.innerHTML=LU),XZ=a(wn),QM=l(wn,"UL",{"data-svelte-h":!0}),f(QM)!=="svelte-y103rk"&&(QM.innerHTML=$U),qZ=a(wn),h(Ii.$$.fragment,wn),wn.forEach(d),vn.forEach(d),lw=a(o),h(Cf.$$.fragment,o),dw=a(o),yf=l(o,"DIV",{class:!0});var wv=v(yf);h(vf.$$.fragment,wv),wv.forEach(d),mw=a(o),h(wf.$$.fragment,o),cw=a(o),xf=l(o,"DIV",{class:!0});var xv=v(xf);h(kf.$$.fragment,xv),xv.forEach(d),fw=a(o),h(Lf.$$.fragment,o),gw=a(o),$f=l(o,"P",{"data-svelte-h":!0}),f($f)!=="svelte-1a2lih2"&&($f.textContent=BU),pw=a(o),h(Bf.$$.fragment,o),hw=a(o),Xe=l(o,"DIV",{class:!0});var xn=v(Xe);h(jf.$$.fragment,xn),QZ=a(xn),DM=l(xn,"P",{"data-svelte-h":!0}),f(DM)!=="svelte-1umo34m"&&(DM.innerHTML=jU),DZ=a(xn),HM=l(xn,"P",{"data-svelte-h":!0}),f(HM)!=="svelte-1n2cphd"&&(HM.innerHTML=ZU),HZ=a(xn),gt=l(xn,"DIV",{class:!0});var Wa=v(gt);h(Zf.$$.fragment,Wa),YZ=a(Wa),YM=l(Wa,"P",{"data-svelte-h":!0}),f(YM)!=="svelte-1o4g80a"&&(YM.textContent=AU),zZ=a(Wa),zM=l(Wa,"P",{"data-svelte-h":!0}),f(zM)!=="svelte-ora4gm"&&(zM.innerHTML=RU),OZ=a(Wa),h(Ui.$$.fragment,Wa),Wa.forEach(d),KZ=a(xn),N=l(xn,"DIV",{class:!0});var So=v(N);h(Af.$$.fragment,So),eA=a(So),OM=l(So,"P",{"data-svelte-h":!0}),f(OM)!=="svelte-100to3e"&&(OM.textContent=PU),oA=a(So),KM=l(So,"P",{"data-svelte-h":!0}),f(KM)!=="svelte-1enn9p2"&&(KM.innerHTML=WU),rA=a(So),eF=l(So,"UL",{"data-svelte-h":!0}),f(eF)!=="svelte-mteq7z"&&(eF.innerHTML=SU),nA=a(So),oF=l(So,"P",{"data-svelte-h":!0}),f(oF)!=="svelte-tebhz6"&&(oF.innerHTML=GU),tA=a(So),h(Ni.$$.fragment,So),So.forEach(d),xn.forEach(d),uw=a(o),h(Rf.$$.fragment,o),_w=a(o),qe=l(o,"DIV",{class:!0});var kn=v(qe);h(Pf.$$.fragment,kn),aA=a(kn),rF=l(kn,"P",{"data-svelte-h":!0}),f(rF)!=="svelte-408hph"&&(rF.innerHTML=VU),sA=a(kn),nF=l(kn,"P",{"data-svelte-h":!0}),f(nF)!=="svelte-1n2cphd"&&(nF.innerHTML=JU),iA=a(kn),pt=l(kn,"DIV",{class:!0});var Sa=v(pt);h(Wf.$$.fragment,Sa),lA=a(Sa),tF=l(Sa,"P",{"data-svelte-h":!0}),f(tF)!=="svelte-3mnohx"&&(tF.textContent=EU),dA=a(Sa),aF=l(Sa,"P",{"data-svelte-h":!0}),f(aF)!=="svelte-ora4gm"&&(aF.innerHTML=IU),mA=a(Sa),h(Xi.$$.fragment,Sa),Sa.forEach(d),cA=a(kn),X=l(kn,"DIV",{class:!0});var Go=v(X);h(Sf.$$.fragment,Go),fA=a(Go),sF=l(Go,"P",{"data-svelte-h":!0}),f(sF)!=="svelte-1ezjudv"&&(sF.textContent=UU),gA=a(Go),iF=l(Go,"P",{"data-svelte-h":!0}),f(iF)!=="svelte-1enn9p2"&&(iF.innerHTML=NU),pA=a(Go),lF=l(Go,"UL",{"data-svelte-h":!0}),f(lF)!=="svelte-h71qz"&&(lF.innerHTML=XU),hA=a(Go),dF=l(Go,"P",{"data-svelte-h":!0}),f(dF)!=="svelte-tebhz6"&&(dF.innerHTML=qU),uA=a(Go),h(qi.$$.fragment,Go),Go.forEach(d),kn.forEach(d),bw=a(o),h(Gf.$$.fragment,o),Mw=a(o),Qe=l(o,"DIV",{class:!0});var Ln=v(Qe);h(Vf.$$.fragment,Ln),_A=a(Ln),mF=l(Ln,"P",{"data-svelte-h":!0}),f(mF)!=="svelte-408hph"&&(mF.innerHTML=QU),bA=a(Ln),cF=l(Ln,"P",{"data-svelte-h":!0}),f(cF)!=="svelte-1n2cphd"&&(cF.innerHTML=DU),MA=a(Ln),ht=l(Ln,"DIV",{class:!0});var Ga=v(ht);h(Jf.$$.fragment,Ga),FA=a(Ga),fF=l(Ga,"P",{"data-svelte-h":!0}),f(fF)!=="svelte-3mnohx"&&(fF.textContent=HU),TA=a(Ga),gF=l(Ga,"P",{"data-svelte-h":!0}),f(gF)!=="svelte-ora4gm"&&(gF.innerHTML=YU),CA=a(Ga),h(Qi.$$.fragment,Ga),Ga.forEach(d),yA=a(Ln),mr=l(Ln,"DIV",{class:!0});var $n=v(mr);h(Ef.$$.fragment,$n),vA=a($n),pF=l($n,"P",{"data-svelte-h":!0}),f(pF)!=="svelte-1ezjudv"&&(pF.textContent=zU),wA=a($n),hF=l($n,"P",{"data-svelte-h":!0}),f(hF)!=="svelte-1enn9p2"&&(hF.innerHTML=OU),xA=a($n),uF=l($n,"UL",{"data-svelte-h":!0}),f(uF)!=="svelte-p5px0o"&&(uF.innerHTML=KU),kA=a($n),h(Di.$$.fragment,$n),$n.forEach(d),Ln.forEach(d),Fw=a(o),h(If.$$.fragment,o),Tw=a(o),De=l(o,"DIV",{class:!0});var Bn=v(De);h(Uf.$$.fragment,Bn),LA=a(Bn),_F=l(Bn,"P",{"data-svelte-h":!0}),f(_F)!=="svelte-408hph"&&(_F.innerHTML=e9),$A=a(Bn),bF=l(Bn,"P",{"data-svelte-h":!0}),f(bF)!=="svelte-1n2cphd"&&(bF.innerHTML=o9),BA=a(Bn),ut=l(Bn,"DIV",{class:!0});var Va=v(ut);h(Nf.$$.fragment,Va),jA=a(Va),MF=l(Va,"P",{"data-svelte-h":!0}),f(MF)!=="svelte-3mnohx"&&(MF.textContent=r9),ZA=a(Va),FF=l(Va,"P",{"data-svelte-h":!0}),f(FF)!=="svelte-ora4gm"&&(FF.innerHTML=n9),AA=a(Va),h(Hi.$$.fragment,Va),Va.forEach(d),RA=a(Bn),cr=l(Bn,"DIV",{class:!0});var jn=v(cr);h(Xf.$$.fragment,jn),PA=a(jn),TF=l(jn,"P",{"data-svelte-h":!0}),f(TF)!=="svelte-1ezjudv"&&(TF.textContent=t9),WA=a(jn),CF=l(jn,"P",{"data-svelte-h":!0}),f(CF)!=="svelte-1enn9p2"&&(CF.innerHTML=a9),SA=a(jn),yF=l(jn,"UL",{"data-svelte-h":!0}),f(yF)!=="svelte-tjlx0r"&&(yF.innerHTML=s9),GA=a(jn),h(Yi.$$.fragment,jn),jn.forEach(d),Bn.forEach(d),Cw=a(o),h(qf.$$.fragment,o),yw=a(o),He=l(o,"DIV",{class:!0});var Zn=v(He);h(Qf.$$.fragment,Zn),VA=a(Zn),vF=l(Zn,"P",{"data-svelte-h":!0}),f(vF)!=="svelte-z3yo53"&&(vF.innerHTML=i9),JA=a(Zn),wF=l(Zn,"P",{"data-svelte-h":!0}),f(wF)!=="svelte-1n2cphd"&&(wF.innerHTML=l9),EA=a(Zn),_t=l(Zn,"DIV",{class:!0});var Ja=v(_t);h(Df.$$.fragment,Ja),IA=a(Ja),xF=l(Ja,"P",{"data-svelte-h":!0}),f(xF)!=="svelte-1269scv"&&(xF.textContent=d9),UA=a(Ja),kF=l(Ja,"P",{"data-svelte-h":!0}),f(kF)!=="svelte-ora4gm"&&(kF.innerHTML=m9),NA=a(Ja),h(zi.$$.fragment,Ja),Ja.forEach(d),XA=a(Zn),q=l(Zn,"DIV",{class:!0});var Vo=v(q);h(Hf.$$.fragment,Vo),qA=a(Vo),LF=l(Vo,"P",{"data-svelte-h":!0}),f(LF)!=="svelte-xeutyd"&&(LF.textContent=c9),QA=a(Vo),$F=l(Vo,"P",{"data-svelte-h":!0}),f($F)!=="svelte-1enn9p2"&&($F.innerHTML=f9),DA=a(Vo),BF=l(Vo,"UL",{"data-svelte-h":!0}),f(BF)!=="svelte-1plz0ja"&&(BF.innerHTML=g9),HA=a(Vo),jF=l(Vo,"P",{"data-svelte-h":!0}),f(jF)!=="svelte-tebhz6"&&(jF.innerHTML=p9),YA=a(Vo),h(Oi.$$.fragment,Vo),Vo.forEach(d),Zn.forEach(d),vw=a(o),h(Yf.$$.fragment,o),ww=a(o),Ye=l(o,"DIV",{class:!0});var An=v(Ye);h(zf.$$.fragment,An),zA=a(An),ZF=l(An,"P",{"data-svelte-h":!0}),f(ZF)!=="svelte-omjr9v"&&(ZF.innerHTML=h9),OA=a(An),AF=l(An,"P",{"data-svelte-h":!0}),f(AF)!=="svelte-1n2cphd"&&(AF.innerHTML=u9),KA=a(An),bt=l(An,"DIV",{class:!0});var Ea=v(bt);h(Of.$$.fragment,Ea),eR=a(Ea),RF=l(Ea,"P",{"data-svelte-h":!0}),f(RF)!=="svelte-vmkzub"&&(RF.textContent=_9),oR=a(Ea),PF=l(Ea,"P",{"data-svelte-h":!0}),f(PF)!=="svelte-ora4gm"&&(PF.innerHTML=b9),rR=a(Ea),h(Ki.$$.fragment,Ea),Ea.forEach(d),nR=a(An),Q=l(An,"DIV",{class:!0});var Jo=v(Q);h(Kf.$$.fragment,Jo),tR=a(Jo),WF=l(Jo,"P",{"data-svelte-h":!0}),f(WF)!=="svelte-up2an7"&&(WF.textContent=M9),aR=a(Jo),SF=l(Jo,"P",{"data-svelte-h":!0}),f(SF)!=="svelte-1enn9p2"&&(SF.innerHTML=F9),sR=a(Jo),GF=l(Jo,"UL",{"data-svelte-h":!0}),f(GF)!=="svelte-jwobsc"&&(GF.innerHTML=T9),iR=a(Jo),VF=l(Jo,"P",{"data-svelte-h":!0}),f(VF)!=="svelte-tebhz6"&&(VF.innerHTML=C9),lR=a(Jo),h(el.$$.fragment,Jo),Jo.forEach(d),An.forEach(d),xw=a(o),h(eg.$$.fragment,o),kw=a(o),ze=l(o,"DIV",{class:!0});var Rn=v(ze);h(og.$$.fragment,Rn),dR=a(Rn),JF=l(Rn,"P",{"data-svelte-h":!0}),f(JF)!=="svelte-omjr9v"&&(JF.innerHTML=y9),mR=a(Rn),EF=l(Rn,"P",{"data-svelte-h":!0}),f(EF)!=="svelte-1n2cphd"&&(EF.innerHTML=v9),cR=a(Rn),Mt=l(Rn,"DIV",{class:!0});var Ia=v(Mt);h(rg.$$.fragment,Ia),fR=a(Ia),IF=l(Ia,"P",{"data-svelte-h":!0}),f(IF)!=="svelte-vmkzub"&&(IF.textContent=w9),gR=a(Ia),UF=l(Ia,"P",{"data-svelte-h":!0}),f(UF)!=="svelte-ora4gm"&&(UF.innerHTML=x9),pR=a(Ia),h(ol.$$.fragment,Ia),Ia.forEach(d),hR=a(Rn),fr=l(Rn,"DIV",{class:!0});var Pn=v(fr);h(ng.$$.fragment,Pn),uR=a(Pn),NF=l(Pn,"P",{"data-svelte-h":!0}),f(NF)!=="svelte-up2an7"&&(NF.textContent=k9),_R=a(Pn),XF=l(Pn,"P",{"data-svelte-h":!0}),f(XF)!=="svelte-1enn9p2"&&(XF.innerHTML=L9),bR=a(Pn),qF=l(Pn,"UL",{"data-svelte-h":!0}),f(qF)!=="svelte-1aklram"&&(qF.innerHTML=$9),MR=a(Pn),h(rl.$$.fragment,Pn),Pn.forEach(d),Rn.forEach(d),Lw=a(o),h(tg.$$.fragment,o),$w=a(o),Oe=l(o,"DIV",{class:!0});var Wn=v(Oe);h(ag.$$.fragment,Wn),FR=a(Wn),QF=l(Wn,"P",{"data-svelte-h":!0}),f(QF)!=="svelte-1h1jhwu"&&(QF.innerHTML=B9),TR=a(Wn),DF=l(Wn,"P",{"data-svelte-h":!0}),f(DF)!=="svelte-1n2cphd"&&(DF.innerHTML=j9),CR=a(Wn),Ft=l(Wn,"DIV",{class:!0});var Ua=v(Ft);h(sg.$$.fragment,Ua),yR=a(Ua),HF=l(Ua,"P",{"data-svelte-h":!0}),f(HF)!=="svelte-1dstum2"&&(HF.textContent=Z9),vR=a(Ua),YF=l(Ua,"P",{"data-svelte-h":!0}),f(YF)!=="svelte-ora4gm"&&(YF.innerHTML=A9),wR=a(Ua),h(nl.$$.fragment,Ua),Ua.forEach(d),xR=a(Wn),D=l(Wn,"DIV",{class:!0});var Eo=v(D);h(ig.$$.fragment,Eo),kR=a(Eo),zF=l(Eo,"P",{"data-svelte-h":!0}),f(zF)!=="svelte-7xylam"&&(zF.textContent=R9),LR=a(Eo),OF=l(Eo,"P",{"data-svelte-h":!0}),f(OF)!=="svelte-1enn9p2"&&(OF.innerHTML=P9),$R=a(Eo),KF=l(Eo,"UL",{"data-svelte-h":!0}),f(KF)!=="svelte-nr8thr"&&(KF.innerHTML=W9),BR=a(Eo),eT=l(Eo,"P",{"data-svelte-h":!0}),f(eT)!=="svelte-tebhz6"&&(eT.innerHTML=S9),jR=a(Eo),h(tl.$$.fragment,Eo),Eo.forEach(d),Wn.forEach(d),Bw=a(o),h(lg.$$.fragment,o),jw=a(o),Ke=l(o,"DIV",{class:!0});var Na=v(Ke);h(dg.$$.fragment,Na),ZR=a(Na),oT=l(Na,"P",{"data-svelte-h":!0}),f(oT)!=="svelte-m2g7d3"&&(oT.innerHTML=G9),AR=a(Na),rT=l(Na,"P",{"data-svelte-h":!0}),f(rT)!=="svelte-1n2cphd"&&(rT.innerHTML=V9),RR=a(Na),Tt=l(Na,"DIV",{class:!0});var Kl=v(Tt);h(mg.$$.fragment,Kl),PR=a(Kl),nT=l(Kl,"P",{"data-svelte-h":!0}),f(nT)!=="svelte-a638l3"&&(nT.textContent=J9),WR=a(Kl),tT=l(Kl,"P",{"data-svelte-h":!0}),f(tT)!=="svelte-ora4gm"&&(tT.innerHTML=E9),SR=a(Kl),h(al.$$.fragment,Kl),Kl.forEach(d),GR=a(Na),H=l(Na,"DIV",{class:!0});var Xt=v(H);h(cg.$$.fragment,Xt),VR=a(Xt),aT=l(Xt,"P",{"data-svelte-h":!0}),f(aT)!=="svelte-1rj5rvl"&&(aT.textContent=I9),JR=a(Xt),sT=l(Xt,"P",{"data-svelte-h":!0}),f(sT)!=="svelte-1enn9p2"&&(sT.innerHTML=U9),ER=a(Xt),iT=l(Xt,"UL",{"data-svelte-h":!0}),f(iT)!=="svelte-1bcbzcg"&&(iT.innerHTML=N9),IR=a(Xt),lT=l(Xt,"P",{"data-svelte-h":!0}),f(lT)!=="svelte-tebhz6"&&(lT.innerHTML=X9),UR=a(Xt),h(sl.$$.fragment,Xt),Xt.forEach(d),Na.forEach(d),Zw=a(o),h(fg.$$.fragment,o),Aw=a(o),gg=l(o,"DIV",{class:!0});var iX=v(gg);h(pg.$$.fragment,iX),iX.forEach(d),Rw=a(o),h(hg.$$.fragment,o),Pw=a(o),eo=l(o,"DIV",{class:!0});var Xa=v(eo);h(ug.$$.fragment,Xa),NR=a(Xa),dT=l(Xa,"P",{"data-svelte-h":!0}),f(dT)!=="svelte-49w040"&&(dT.innerHTML=q9),XR=a(Xa),mT=l(Xa,"P",{"data-svelte-h":!0}),f(mT)!=="svelte-1n2cphd"&&(mT.innerHTML=Q9),qR=a(Xa),Ct=l(Xa,"DIV",{class:!0});var ed=v(Ct);h(_g.$$.fragment,ed),QR=a(ed),cT=l(ed,"P",{"data-svelte-h":!0}),f(cT)!=="svelte-ne8wj0"&&(cT.textContent=D9),DR=a(ed),fT=l(ed,"P",{"data-svelte-h":!0}),f(fT)!=="svelte-ora4gm"&&(fT.innerHTML=H9),HR=a(ed),h(il.$$.fragment,ed),ed.forEach(d),YR=a(Xa),Y=l(Xa,"DIV",{class:!0});var qt=v(Y);h(bg.$$.fragment,qt),zR=a(qt),gT=l(qt,"P",{"data-svelte-h":!0}),f(gT)!=="svelte-xit1mq"&&(gT.textContent=Y9),OR=a(qt),pT=l(qt,"P",{"data-svelte-h":!0}),f(pT)!=="svelte-1enn9p2"&&(pT.innerHTML=z9),KR=a(qt),hT=l(qt,"UL",{"data-svelte-h":!0}),f(hT)!=="svelte-1tg2k9p"&&(hT.innerHTML=O9),eP=a(qt),uT=l(qt,"P",{"data-svelte-h":!0}),f(uT)!=="svelte-tebhz6"&&(uT.innerHTML=K9),oP=a(qt),h(ll.$$.fragment,qt),qt.forEach(d),Xa.forEach(d),Ww=a(o),h(Mg.$$.fragment,o),Sw=a(o),oo=l(o,"DIV",{class:!0});var qa=v(oo);h(Fg.$$.fragment,qa),rP=a(qa),_T=l(qa,"P",{"data-svelte-h":!0}),f(_T)!=="svelte-49w040"&&(_T.innerHTML=eN),nP=a(qa),bT=l(qa,"P",{"data-svelte-h":!0}),f(bT)!=="svelte-1n2cphd"&&(bT.innerHTML=oN),tP=a(qa),yt=l(qa,"DIV",{class:!0});var od=v(yt);h(Tg.$$.fragment,od),aP=a(od),MT=l(od,"P",{"data-svelte-h":!0}),f(MT)!=="svelte-ne8wj0"&&(MT.textContent=rN),sP=a(od),FT=l(od,"P",{"data-svelte-h":!0}),f(FT)!=="svelte-ora4gm"&&(FT.innerHTML=nN),iP=a(od),h(dl.$$.fragment,od),od.forEach(d),lP=a(qa),gr=l(qa,"DIV",{class:!0});var Qa=v(gr);h(Cg.$$.fragment,Qa),dP=a(Qa),TT=l(Qa,"P",{"data-svelte-h":!0}),f(TT)!=="svelte-xit1mq"&&(TT.textContent=tN),mP=a(Qa),CT=l(Qa,"P",{"data-svelte-h":!0}),f(CT)!=="svelte-1enn9p2"&&(CT.innerHTML=aN),cP=a(Qa),yT=l(Qa,"UL",{"data-svelte-h":!0}),f(yT)!=="svelte-ywdqjd"&&(yT.innerHTML=sN),fP=a(Qa),h(ml.$$.fragment,Qa),Qa.forEach(d),qa.forEach(d),Gw=a(o),h(yg.$$.fragment,o),Vw=a(o),ro=l(o,"DIV",{class:!0});var Da=v(ro);h(vg.$$.fragment,Da),gP=a(Da),vT=l(Da,"P",{"data-svelte-h":!0}),f(vT)!=="svelte-nhe5p7"&&(vT.innerHTML=iN),pP=a(Da),wT=l(Da,"P",{"data-svelte-h":!0}),f(wT)!=="svelte-1n2cphd"&&(wT.innerHTML=lN),hP=a(Da),vt=l(Da,"DIV",{class:!0});var rd=v(vt);h(wg.$$.fragment,rd),uP=a(rd),xT=l(rd,"P",{"data-svelte-h":!0}),f(xT)!=="svelte-15xznv"&&(xT.textContent=dN),_P=a(rd),kT=l(rd,"P",{"data-svelte-h":!0}),f(kT)!=="svelte-ora4gm"&&(kT.innerHTML=mN),bP=a(rd),h(cl.$$.fragment,rd),rd.forEach(d),MP=a(Da),z=l(Da,"DIV",{class:!0});var Qt=v(z);h(xg.$$.fragment,Qt),FP=a(Qt),LT=l(Qt,"P",{"data-svelte-h":!0}),f(LT)!=="svelte-1r6a2oz"&&(LT.textContent=cN),TP=a(Qt),$T=l(Qt,"P",{"data-svelte-h":!0}),f($T)!=="svelte-1enn9p2"&&($T.innerHTML=fN),CP=a(Qt),BT=l(Qt,"UL",{"data-svelte-h":!0}),f(BT)!=="svelte-1uzvxcs"&&(BT.innerHTML=gN),yP=a(Qt),jT=l(Qt,"P",{"data-svelte-h":!0}),f(jT)!=="svelte-tebhz6"&&(jT.innerHTML=pN),vP=a(Qt),h(fl.$$.fragment,Qt),Qt.forEach(d),Da.forEach(d),Jw=a(o),h(kg.$$.fragment,o),Ew=a(o),no=l(o,"DIV",{class:!0});var Ha=v(no);h(Lg.$$.fragment,Ha),wP=a(Ha),ZT=l(Ha,"P",{"data-svelte-h":!0}),f(ZT)!=="svelte-15qssro"&&(ZT.innerHTML=hN),xP=a(Ha),AT=l(Ha,"P",{"data-svelte-h":!0}),f(AT)!=="svelte-1n2cphd"&&(AT.innerHTML=uN),kP=a(Ha),wt=l(Ha,"DIV",{class:!0});var nd=v(wt);h($g.$$.fragment,nd),LP=a(nd),RT=l(nd,"P",{"data-svelte-h":!0}),f(RT)!=="svelte-zddwso"&&(RT.textContent=_N),$P=a(nd),PT=l(nd,"P",{"data-svelte-h":!0}),f(PT)!=="svelte-ora4gm"&&(PT.innerHTML=bN),BP=a(nd),h(gl.$$.fragment,nd),nd.forEach(d),jP=a(Ha),O=l(Ha,"DIV",{class:!0});var Dt=v(O);h(Bg.$$.fragment,Dt),ZP=a(Dt),WT=l(Dt,"P",{"data-svelte-h":!0}),f(WT)!=="svelte-1wyb2t8"&&(WT.textContent=MN),AP=a(Dt),ST=l(Dt,"P",{"data-svelte-h":!0}),f(ST)!=="svelte-1enn9p2"&&(ST.innerHTML=FN),RP=a(Dt),GT=l(Dt,"UL",{"data-svelte-h":!0}),f(GT)!=="svelte-1pt105m"&&(GT.innerHTML=TN),PP=a(Dt),VT=l(Dt,"P",{"data-svelte-h":!0}),f(VT)!=="svelte-tebhz6"&&(VT.innerHTML=CN),WP=a(Dt),h(pl.$$.fragment,Dt),Dt.forEach(d),Ha.forEach(d),Iw=a(o),h(jg.$$.fragment,o),Uw=a(o),to=l(o,"DIV",{class:!0});var Ya=v(to);h(Zg.$$.fragment,Ya),SP=a(Ya),JT=l(Ya,"P",{"data-svelte-h":!0}),f(JT)!=="svelte-3dms9q"&&(JT.innerHTML=yN),GP=a(Ya),ET=l(Ya,"P",{"data-svelte-h":!0}),f(ET)!=="svelte-1n2cphd"&&(ET.innerHTML=vN),VP=a(Ya),xt=l(Ya,"DIV",{class:!0});var td=v(xt);h(Ag.$$.fragment,td),JP=a(td),IT=l(td,"P",{"data-svelte-h":!0}),f(IT)!=="svelte-1xqzdzm"&&(IT.textContent=wN),EP=a(td),UT=l(td,"P",{"data-svelte-h":!0}),f(UT)!=="svelte-ora4gm"&&(UT.innerHTML=xN),IP=a(td),h(hl.$$.fragment,td),td.forEach(d),UP=a(Ya),K=l(Ya,"DIV",{class:!0});var Ht=v(K);h(Rg.$$.fragment,Ht),NP=a(Ht),NT=l(Ht,"P",{"data-svelte-h":!0}),f(NT)!=="svelte-1kp6e9m"&&(NT.textContent=kN),XP=a(Ht),XT=l(Ht,"P",{"data-svelte-h":!0}),f(XT)!=="svelte-1enn9p2"&&(XT.innerHTML=LN),qP=a(Ht),qT=l(Ht,"UL",{"data-svelte-h":!0}),f(qT)!=="svelte-aw9eg9"&&(qT.innerHTML=$N),QP=a(Ht),QT=l(Ht,"P",{"data-svelte-h":!0}),f(QT)!=="svelte-tebhz6"&&(QT.innerHTML=BN),DP=a(Ht),h(ul.$$.fragment,Ht),Ht.forEach(d),Ya.forEach(d),Nw=a(o),h(Pg.$$.fragment,o),Xw=a(o),ao=l(o,"DIV",{class:!0});var za=v(ao);h(Wg.$$.fragment,za),HP=a(za),DT=l(za,"P",{"data-svelte-h":!0}),f(DT)!=="svelte-3dms9q"&&(DT.innerHTML=jN),YP=a(za),HT=l(za,"P",{"data-svelte-h":!0}),f(HT)!=="svelte-1n2cphd"&&(HT.innerHTML=ZN),zP=a(za),kt=l(za,"DIV",{class:!0});var ad=v(kt);h(Sg.$$.fragment,ad),OP=a(ad),YT=l(ad,"P",{"data-svelte-h":!0}),f(YT)!=="svelte-1xqzdzm"&&(YT.textContent=AN),KP=a(ad),zT=l(ad,"P",{"data-svelte-h":!0}),f(zT)!=="svelte-ora4gm"&&(zT.innerHTML=RN),eW=a(ad),h(_l.$$.fragment,ad),ad.forEach(d),oW=a(za),pr=l(za,"DIV",{class:!0});var Oa=v(pr);h(Gg.$$.fragment,Oa),rW=a(Oa),OT=l(Oa,"P",{"data-svelte-h":!0}),f(OT)!=="svelte-1kp6e9m"&&(OT.textContent=PN),nW=a(Oa),KT=l(Oa,"P",{"data-svelte-h":!0}),f(KT)!=="svelte-1enn9p2"&&(KT.innerHTML=WN),tW=a(Oa),eC=l(Oa,"UL",{"data-svelte-h":!0}),f(eC)!=="svelte-1bzq5yp"&&(eC.innerHTML=SN),aW=a(Oa),h(bl.$$.fragment,Oa),Oa.forEach(d),za.forEach(d),qw=a(o),h(Vg.$$.fragment,o),Qw=a(o),so=l(o,"DIV",{class:!0});var Ka=v(so);h(Jg.$$.fragment,Ka),sW=a(Ka),oC=l(Ka,"P",{"data-svelte-h":!0}),f(oC)!=="svelte-146994x"&&(oC.innerHTML=GN),iW=a(Ka),rC=l(Ka,"P",{"data-svelte-h":!0}),f(rC)!=="svelte-1n2cphd"&&(rC.innerHTML=VN),lW=a(Ka),Lt=l(Ka,"DIV",{class:!0});var sd=v(Lt);h(Eg.$$.fragment,sd),dW=a(sd),nC=l(sd,"P",{"data-svelte-h":!0}),f(nC)!=="svelte-1t88i0p"&&(nC.textContent=JN),mW=a(sd),tC=l(sd,"P",{"data-svelte-h":!0}),f(tC)!=="svelte-ora4gm"&&(tC.innerHTML=EN),cW=a(sd),h(Ml.$$.fragment,sd),sd.forEach(d),fW=a(Ka),ee=l(Ka,"DIV",{class:!0});var Yt=v(ee);h(Ig.$$.fragment,Yt),gW=a(Yt),aC=l(Yt,"P",{"data-svelte-h":!0}),f(aC)!=="svelte-xtgygr"&&(aC.textContent=IN),pW=a(Yt),sC=l(Yt,"P",{"data-svelte-h":!0}),f(sC)!=="svelte-1enn9p2"&&(sC.innerHTML=UN),hW=a(Yt),iC=l(Yt,"UL",{"data-svelte-h":!0}),f(iC)!=="svelte-1i22l30"&&(iC.innerHTML=NN),uW=a(Yt),lC=l(Yt,"P",{"data-svelte-h":!0}),f(lC)!=="svelte-tebhz6"&&(lC.innerHTML=XN),_W=a(Yt),h(Fl.$$.fragment,Yt),Yt.forEach(d),Ka.forEach(d),Dw=a(o),h(Ug.$$.fragment,o),Hw=a(o),Ng=l(o,"P",{"data-svelte-h":!0}),f(Ng)!=="svelte-1htpfc5"&&(Ng.textContent=qN),Yw=a(o),h(Xg.$$.fragment,o),zw=a(o),io=l(o,"DIV",{class:!0});var es=v(io);h(qg.$$.fragment,es),bW=a(es),dC=l(es,"P",{"data-svelte-h":!0}),f(dC)!=="svelte-5lzug4"&&(dC.innerHTML=QN),MW=a(es),mC=l(es,"P",{"data-svelte-h":!0}),f(mC)!=="svelte-1n2cphd"&&(mC.innerHTML=DN),FW=a(es),$t=l(es,"DIV",{class:!0});var id=v($t);h(Qg.$$.fragment,id),TW=a(id),cC=l(id,"P",{"data-svelte-h":!0}),f(cC)!=="svelte-1jtlz68"&&(cC.textContent=HN),CW=a(id),fC=l(id,"P",{"data-svelte-h":!0}),f(fC)!=="svelte-ora4gm"&&(fC.innerHTML=YN),yW=a(id),h(Tl.$$.fragment,id),id.forEach(d),vW=a(es),oe=l(es,"DIV",{class:!0});var zt=v(oe);h(Dg.$$.fragment,zt),wW=a(zt),gC=l(zt,"P",{"data-svelte-h":!0}),f(gC)!=="svelte-mj39cs"&&(gC.textContent=zN),xW=a(zt),pC=l(zt,"P",{"data-svelte-h":!0}),f(pC)!=="svelte-1enn9p2"&&(pC.innerHTML=ON),kW=a(zt),hC=l(zt,"UL",{"data-svelte-h":!0}),f(hC)!=="svelte-efi0kf"&&(hC.innerHTML=KN),LW=a(zt),uC=l(zt,"P",{"data-svelte-h":!0}),f(uC)!=="svelte-tebhz6"&&(uC.innerHTML=e5),$W=a(zt),h(Cl.$$.fragment,zt),zt.forEach(d),es.forEach(d),Ow=a(o),h(Hg.$$.fragment,o),Kw=a(o),lo=l(o,"DIV",{class:!0});var os=v(lo);h(Yg.$$.fragment,os),BW=a(os),_C=l(os,"P",{"data-svelte-h":!0}),f(_C)!=="svelte-5lzug4"&&(_C.innerHTML=o5),jW=a(os),bC=l(os,"P",{"data-svelte-h":!0}),f(bC)!=="svelte-1n2cphd"&&(bC.innerHTML=r5),ZW=a(os),Bt=l(os,"DIV",{class:!0});var ld=v(Bt);h(zg.$$.fragment,ld),AW=a(ld),MC=l(ld,"P",{"data-svelte-h":!0}),f(MC)!=="svelte-1jtlz68"&&(MC.textContent=n5),RW=a(ld),FC=l(ld,"P",{"data-svelte-h":!0}),f(FC)!=="svelte-ora4gm"&&(FC.innerHTML=t5),PW=a(ld),h(yl.$$.fragment,ld),ld.forEach(d),WW=a(os),hr=l(os,"DIV",{class:!0});var rs=v(hr);h(Og.$$.fragment,rs),SW=a(rs),TC=l(rs,"P",{"data-svelte-h":!0}),f(TC)!=="svelte-mj39cs"&&(TC.textContent=a5),GW=a(rs),CC=l(rs,"P",{"data-svelte-h":!0}),f(CC)!=="svelte-1enn9p2"&&(CC.innerHTML=s5),VW=a(rs),yC=l(rs,"UL",{"data-svelte-h":!0}),f(yC)!=="svelte-1o44d3x"&&(yC.innerHTML=i5),JW=a(rs),h(vl.$$.fragment,rs),rs.forEach(d),os.forEach(d),ex=a(o),h(Kg.$$.fragment,o),ox=a(o),mo=l(o,"DIV",{class:!0});var ns=v(mo);h(ep.$$.fragment,ns),EW=a(ns),vC=l(ns,"P",{"data-svelte-h":!0}),f(vC)!=="svelte-8dlszb"&&(vC.innerHTML=l5),IW=a(ns),wC=l(ns,"P",{"data-svelte-h":!0}),f(wC)!=="svelte-1n2cphd"&&(wC.innerHTML=d5),UW=a(ns),jt=l(ns,"DIV",{class:!0});var dd=v(jt);h(op.$$.fragment,dd),NW=a(dd),xC=l(dd,"P",{"data-svelte-h":!0}),f(xC)!=="svelte-qoovm7"&&(xC.textContent=m5),XW=a(dd),kC=l(dd,"P",{"data-svelte-h":!0}),f(kC)!=="svelte-ora4gm"&&(kC.innerHTML=c5),qW=a(dd),h(wl.$$.fragment,dd),dd.forEach(d),QW=a(ns),re=l(ns,"DIV",{class:!0});var Ot=v(re);h(rp.$$.fragment,Ot),DW=a(Ot),LC=l(Ot,"P",{"data-svelte-h":!0}),f(LC)!=="svelte-1ldaoop"&&(LC.textContent=f5),HW=a(Ot),$C=l(Ot,"P",{"data-svelte-h":!0}),f($C)!=="svelte-1enn9p2"&&($C.innerHTML=g5),YW=a(Ot),BC=l(Ot,"UL",{"data-svelte-h":!0}),f(BC)!=="svelte-3e2yqw"&&(BC.innerHTML=p5),zW=a(Ot),jC=l(Ot,"P",{"data-svelte-h":!0}),f(jC)!=="svelte-tebhz6"&&(jC.innerHTML=h5),OW=a(Ot),h(xl.$$.fragment,Ot),Ot.forEach(d),ns.forEach(d),rx=a(o),h(np.$$.fragment,o),nx=a(o),co=l(o,"DIV",{class:!0});var ts=v(co);h(tp.$$.fragment,ts),KW=a(ts),ZC=l(ts,"P",{"data-svelte-h":!0}),f(ZC)!=="svelte-52lfgw"&&(ZC.innerHTML=u5),eS=a(ts),AC=l(ts,"P",{"data-svelte-h":!0}),f(AC)!=="svelte-1n2cphd"&&(AC.innerHTML=_5),oS=a(ts),Zt=l(ts,"DIV",{class:!0});var md=v(Zt);h(ap.$$.fragment,md),rS=a(md),RC=l(md,"P",{"data-svelte-h":!0}),f(RC)!=="svelte-ovue0k"&&(RC.textContent=b5),nS=a(md),PC=l(md,"P",{"data-svelte-h":!0}),f(PC)!=="svelte-ora4gm"&&(PC.innerHTML=M5),tS=a(md),h(kl.$$.fragment,md),md.forEach(d),aS=a(ts),ne=l(ts,"DIV",{class:!0});var Kt=v(ne);h(sp.$$.fragment,Kt),sS=a(Kt),WC=l(Kt,"P",{"data-svelte-h":!0}),f(WC)!=="svelte-1jrxhfe"&&(WC.textContent=F5),iS=a(Kt),SC=l(Kt,"P",{"data-svelte-h":!0}),f(SC)!=="svelte-1enn9p2"&&(SC.innerHTML=T5),lS=a(Kt),GC=l(Kt,"UL",{"data-svelte-h":!0}),f(GC)!=="svelte-1bu12l1"&&(GC.innerHTML=C5),dS=a(Kt),VC=l(Kt,"P",{"data-svelte-h":!0}),f(VC)!=="svelte-tebhz6"&&(VC.innerHTML=y5),mS=a(Kt),h(Ll.$$.fragment,Kt),Kt.forEach(d),ts.forEach(d),tx=a(o),h(ip.$$.fragment,o),ax=a(o),fo=l(o,"DIV",{class:!0});var as=v(fo);h(lp.$$.fragment,as),cS=a(as),JC=l(as,"P",{"data-svelte-h":!0}),f(JC)!=="svelte-1su2vvs"&&(JC.innerHTML=v5),fS=a(as),EC=l(as,"P",{"data-svelte-h":!0}),f(EC)!=="svelte-1n2cphd"&&(EC.innerHTML=w5),gS=a(as),At=l(as,"DIV",{class:!0});var cd=v(At);h(dp.$$.fragment,cd),pS=a(cd),IC=l(cd,"P",{"data-svelte-h":!0}),f(IC)!=="svelte-1aufdt0"&&(IC.textContent=x5),hS=a(cd),UC=l(cd,"P",{"data-svelte-h":!0}),f(UC)!=="svelte-ora4gm"&&(UC.innerHTML=k5),uS=a(cd),h($l.$$.fragment,cd),cd.forEach(d),_S=a(as),te=l(as,"DIV",{class:!0});var ea=v(te);h(mp.$$.fragment,ea),bS=a(ea),NC=l(ea,"P",{"data-svelte-h":!0}),f(NC)!=="svelte-73ck9g"&&(NC.textContent=L5),MS=a(ea),XC=l(ea,"P",{"data-svelte-h":!0}),f(XC)!=="svelte-1enn9p2"&&(XC.innerHTML=$5),FS=a(ea),qC=l(ea,"UL",{"data-svelte-h":!0}),f(qC)!=="svelte-h5mlm7"&&(qC.innerHTML=B5),TS=a(ea),QC=l(ea,"P",{"data-svelte-h":!0}),f(QC)!=="svelte-tebhz6"&&(QC.innerHTML=j5),CS=a(ea),h(Bl.$$.fragment,ea),ea.forEach(d),as.forEach(d),sx=a(o),h(cp.$$.fragment,o),ix=a(o),go=l(o,"DIV",{class:!0});var ss=v(go);h(fp.$$.fragment,ss),yS=a(ss),DC=l(ss,"P",{"data-svelte-h":!0}),f(DC)!=="svelte-1su2vvs"&&(DC.innerHTML=Z5),vS=a(ss),HC=l(ss,"P",{"data-svelte-h":!0}),f(HC)!=="svelte-1n2cphd"&&(HC.innerHTML=A5),wS=a(ss),Rt=l(ss,"DIV",{class:!0});var fd=v(Rt);h(gp.$$.fragment,fd),xS=a(fd),YC=l(fd,"P",{"data-svelte-h":!0}),f(YC)!=="svelte-1aufdt0"&&(YC.textContent=R5),kS=a(fd),zC=l(fd,"P",{"data-svelte-h":!0}),f(zC)!=="svelte-ora4gm"&&(zC.innerHTML=P5),LS=a(fd),h(jl.$$.fragment,fd),fd.forEach(d),$S=a(ss),ur=l(ss,"DIV",{class:!0});var is=v(ur);h(pp.$$.fragment,is),BS=a(is),OC=l(is,"P",{"data-svelte-h":!0}),f(OC)!=="svelte-73ck9g"&&(OC.textContent=W5),jS=a(is),KC=l(is,"P",{"data-svelte-h":!0}),f(KC)!=="svelte-1enn9p2"&&(KC.innerHTML=S5),ZS=a(is),ey=l(is,"UL",{"data-svelte-h":!0}),f(ey)!=="svelte-cf23a9"&&(ey.innerHTML=G5),AS=a(is),h(Zl.$$.fragment,is),is.forEach(d),ss.forEach(d),lx=a(o),h(hp.$$.fragment,o),dx=a(o),po=l(o,"DIV",{class:!0});var ls=v(po);h(up.$$.fragment,ls),RS=a(ls),oy=l(ls,"P",{"data-svelte-h":!0}),f(oy)!=="svelte-1su2vvs"&&(oy.innerHTML=V5),PS=a(ls),ry=l(ls,"P",{"data-svelte-h":!0}),f(ry)!=="svelte-1n2cphd"&&(ry.innerHTML=J5),WS=a(ls),Pt=l(ls,"DIV",{class:!0});var gd=v(Pt);h(_p.$$.fragment,gd),SS=a(gd),ny=l(gd,"P",{"data-svelte-h":!0}),f(ny)!=="svelte-1aufdt0"&&(ny.textContent=E5),GS=a(gd),ty=l(gd,"P",{"data-svelte-h":!0}),f(ty)!=="svelte-ora4gm"&&(ty.innerHTML=I5),VS=a(gd),h(Al.$$.fragment,gd),gd.forEach(d),JS=a(ls),_r=l(ls,"DIV",{class:!0});var ds=v(_r);h(bp.$$.fragment,ds),ES=a(ds),ay=l(ds,"P",{"data-svelte-h":!0}),f(ay)!=="svelte-73ck9g"&&(ay.textContent=U5),IS=a(ds),sy=l(ds,"P",{"data-svelte-h":!0}),f(sy)!=="svelte-1enn9p2"&&(sy.innerHTML=N5),US=a(ds),iy=l(ds,"UL",{"data-svelte-h":!0}),f(iy)!=="svelte-18wl3x2"&&(iy.innerHTML=X5),NS=a(ds),h(Rl.$$.fragment,ds),ds.forEach(d),ls.forEach(d),mx=a(o),h(Mp.$$.fragment,o),cx=a(o),ho=l(o,"DIV",{class:!0});var ms=v(ho);h(Fp.$$.fragment,ms),XS=a(ms),ly=l(ms,"P",{"data-svelte-h":!0}),f(ly)!=="svelte-14qqbss"&&(ly.innerHTML=q5),qS=a(ms),dy=l(ms,"P",{"data-svelte-h":!0}),f(dy)!=="svelte-1n2cphd"&&(dy.innerHTML=Q5),QS=a(ms),Wt=l(ms,"DIV",{class:!0});var pd=v(Wt);h(Tp.$$.fragment,pd),DS=a(pd),my=l(pd,"P",{"data-svelte-h":!0}),f(my)!=="svelte-1wxna3s"&&(my.textContent=D5),HS=a(pd),cy=l(pd,"P",{"data-svelte-h":!0}),f(cy)!=="svelte-ora4gm"&&(cy.innerHTML=H5),YS=a(pd),h(Pl.$$.fragment,pd),pd.forEach(d),zS=a(ms),ae=l(ms,"DIV",{class:!0});var oa=v(ae);h(Cp.$$.fragment,oa),OS=a(oa),fy=l(oa,"P",{"data-svelte-h":!0}),f(fy)!=="svelte-1n6mjno"&&(fy.textContent=Y5),KS=a(oa),gy=l(oa,"P",{"data-svelte-h":!0}),f(gy)!=="svelte-1enn9p2"&&(gy.innerHTML=z5),eG=a(oa),py=l(oa,"UL",{"data-svelte-h":!0}),f(py)!=="svelte-ob07wa"&&(py.innerHTML=O5),oG=a(oa),hy=l(oa,"P",{"data-svelte-h":!0}),f(hy)!=="svelte-tebhz6"&&(hy.innerHTML=K5),rG=a(oa),h(Wl.$$.fragment,oa),oa.forEach(d),ms.forEach(d),fx=a(o),h(yp.$$.fragment,o),gx=a(o),vp=l(o,"DIV",{class:!0});var lX=v(vp);h(wp.$$.fragment,lX),lX.forEach(d),px=a(o),h(xp.$$.fragment,o),hx=a(o),kp=l(o,"DIV",{class:!0});var dX=v(kp);h(Lp.$$.fragment,dX),dX.forEach(d),ux=a(o),h($p.$$.fragment,o),_x=a(o),Bp=l(o,"P",{"data-svelte-h":!0}),f(Bp)!=="svelte-1rj2vi9"&&(Bp.textContent=e4),bx=a(o),h(jp.$$.fragment,o),Mx=a(o),uo=l(o,"DIV",{class:!0});var cs=v(uo);h(Zp.$$.fragment,cs),nG=a(cs),uy=l(cs,"P",{"data-svelte-h":!0}),f(uy)!=="svelte-1ygy1mg"&&(uy.innerHTML=o4),tG=a(cs),_y=l(cs,"P",{"data-svelte-h":!0}),f(_y)!=="svelte-1n2cphd"&&(_y.innerHTML=r4),aG=a(cs),St=l(cs,"DIV",{class:!0});var hd=v(St);h(Ap.$$.fragment,hd),sG=a(hd),by=l(hd,"P",{"data-svelte-h":!0}),f(by)!=="svelte-syrugc"&&(by.textContent=n4),iG=a(hd),My=l(hd,"P",{"data-svelte-h":!0}),f(My)!=="svelte-ora4gm"&&(My.innerHTML=t4),lG=a(hd),h(Sl.$$.fragment,hd),hd.forEach(d),dG=a(cs),se=l(cs,"DIV",{class:!0});var ra=v(se);h(Rp.$$.fragment,ra),mG=a(ra),Fy=l(ra,"P",{"data-svelte-h":!0}),f(Fy)!=="svelte-1ontymo"&&(Fy.textContent=a4),cG=a(ra),Ty=l(ra,"P",{"data-svelte-h":!0}),f(Ty)!=="svelte-1enn9p2"&&(Ty.innerHTML=s4),fG=a(ra),Cy=l(ra,"UL",{"data-svelte-h":!0}),f(Cy)!=="svelte-1azi6ae"&&(Cy.innerHTML=i4),gG=a(ra),yy=l(ra,"P",{"data-svelte-h":!0}),f(yy)!=="svelte-tebhz6"&&(yy.innerHTML=l4),pG=a(ra),h(Gl.$$.fragment,ra),ra.forEach(d),cs.forEach(d),Fx=a(o),h(Pp.$$.fragment,o),Tx=a(o),_o=l(o,"DIV",{class:!0});var fs=v(_o);h(Wp.$$.fragment,fs),hG=a(fs),vy=l(fs,"P",{"data-svelte-h":!0}),f(vy)!=="svelte-1ygy1mg"&&(vy.innerHTML=d4),uG=a(fs),wy=l(fs,"P",{"data-svelte-h":!0}),f(wy)!=="svelte-1n2cphd"&&(wy.innerHTML=m4),_G=a(fs),Gt=l(fs,"DIV",{class:!0});var ud=v(Gt);h(Sp.$$.fragment,ud),bG=a(ud),xy=l(ud,"P",{"data-svelte-h":!0}),f(xy)!=="svelte-syrugc"&&(xy.textContent=c4),MG=a(ud),ky=l(ud,"P",{"data-svelte-h":!0}),f(ky)!=="svelte-ora4gm"&&(ky.innerHTML=f4),FG=a(ud),h(Vl.$$.fragment,ud),ud.forEach(d),TG=a(fs),br=l(fs,"DIV",{class:!0});var gs=v(br);h(Gp.$$.fragment,gs),CG=a(gs),Ly=l(gs,"P",{"data-svelte-h":!0}),f(Ly)!=="svelte-1ontymo"&&(Ly.textContent=g4),yG=a(gs),$y=l(gs,"P",{"data-svelte-h":!0}),f($y)!=="svelte-1enn9p2"&&($y.innerHTML=p4),vG=a(gs),By=l(gs,"UL",{"data-svelte-h":!0}),f(By)!=="svelte-1wx3oge"&&(By.innerHTML=h4),wG=a(gs),h(Jl.$$.fragment,gs),gs.forEach(d),fs.forEach(d),Cx=a(o),h(Vp.$$.fragment,o),yx=a(o),bo=l(o,"DIV",{class:!0});var ps=v(bo);h(Jp.$$.fragment,ps),xG=a(ps),jy=l(ps,"P",{"data-svelte-h":!0}),f(jy)!=="svelte-1iy9ybd"&&(jy.innerHTML=u4),kG=a(ps),Zy=l(ps,"P",{"data-svelte-h":!0}),f(Zy)!=="svelte-1n2cphd"&&(Zy.innerHTML=_4),LG=a(ps),Vt=l(ps,"DIV",{class:!0});var _d=v(Vt);h(Ep.$$.fragment,_d),$G=a(_d),Ay=l(_d,"P",{"data-svelte-h":!0}),f(Ay)!=="svelte-axumax"&&(Ay.textContent=b4),BG=a(_d),Ry=l(_d,"P",{"data-svelte-h":!0}),f(Ry)!=="svelte-ora4gm"&&(Ry.innerHTML=M4),jG=a(_d),h(El.$$.fragment,_d),_d.forEach(d),ZG=a(ps),ie=l(ps,"DIV",{class:!0});var na=v(ie);h(Ip.$$.fragment,na),AG=a(na),Py=l(na,"P",{"data-svelte-h":!0}),f(Py)!=="svelte-dbgbst"&&(Py.textContent=F4),RG=a(na),Wy=l(na,"P",{"data-svelte-h":!0}),f(Wy)!=="svelte-1enn9p2"&&(Wy.innerHTML=T4),PG=a(na),Sy=l(na,"UL",{"data-svelte-h":!0}),f(Sy)!=="svelte-172lo5t"&&(Sy.innerHTML=C4),WG=a(na),Gy=l(na,"P",{"data-svelte-h":!0}),f(Gy)!=="svelte-tebhz6"&&(Gy.innerHTML=y4),SG=a(na),h(Il.$$.fragment,na),na.forEach(d),ps.forEach(d),vx=a(o),h(Up.$$.fragment,o),wx=a(o),Mo=l(o,"DIV",{class:!0});var hs=v(Mo);h(Np.$$.fragment,hs),GG=a(hs),Vy=l(hs,"P",{"data-svelte-h":!0}),f(Vy)!=="svelte-1iy9ybd"&&(Vy.innerHTML=v4),VG=a(hs),Jy=l(hs,"P",{"data-svelte-h":!0}),f(Jy)!=="svelte-1n2cphd"&&(Jy.innerHTML=w4),JG=a(hs),Jt=l(hs,"DIV",{class:!0});var bd=v(Jt);h(Xp.$$.fragment,bd),EG=a(bd),Ey=l(bd,"P",{"data-svelte-h":!0}),f(Ey)!=="svelte-axumax"&&(Ey.textContent=x4),IG=a(bd),Iy=l(bd,"P",{"data-svelte-h":!0}),f(Iy)!=="svelte-ora4gm"&&(Iy.innerHTML=k4),UG=a(bd),h(Ul.$$.fragment,bd),bd.forEach(d),NG=a(hs),Mr=l(hs,"DIV",{class:!0});var us=v(Mr);h(qp.$$.fragment,us),XG=a(us),Uy=l(us,"P",{"data-svelte-h":!0}),f(Uy)!=="svelte-dbgbst"&&(Uy.textContent=L4),qG=a(us),Ny=l(us,"P",{"data-svelte-h":!0}),f(Ny)!=="svelte-1enn9p2"&&(Ny.innerHTML=$4),QG=a(us),Xy=l(us,"UL",{"data-svelte-h":!0}),f(Xy)!=="svelte-7bq0nh"&&(Xy.innerHTML=B4),DG=a(us),h(Nl.$$.fragment,us),us.forEach(d),hs.forEach(d),xx=a(o),h(Qp.$$.fragment,o),kx=a(o),Fo=l(o,"DIV",{class:!0});var _s=v(Fo);h(Dp.$$.fragment,_s),HG=a(_s),qy=l(_s,"P",{"data-svelte-h":!0}),f(qy)!=="svelte-1dnnzz0"&&(qy.innerHTML=j4),YG=a(_s),Qy=l(_s,"P",{"data-svelte-h":!0}),f(Qy)!=="svelte-1n2cphd"&&(Qy.innerHTML=Z4),zG=a(_s),Et=l(_s,"DIV",{class:!0});var Md=v(Et);h(Hp.$$.fragment,Md),OG=a(Md),Dy=l(Md,"P",{"data-svelte-h":!0}),f(Dy)!=="svelte-1nom7tk"&&(Dy.textContent=A4),KG=a(Md),Hy=l(Md,"P",{"data-svelte-h":!0}),f(Hy)!=="svelte-ora4gm"&&(Hy.innerHTML=R4),eV=a(Md),h(Xl.$$.fragment,Md),Md.forEach(d),oV=a(_s),le=l(_s,"DIV",{class:!0});var ta=v(le);h(Yp.$$.fragment,ta),rV=a(ta),Yy=l(ta,"P",{"data-svelte-h":!0}),f(Yy)!=="svelte-7pdp5q"&&(Yy.textContent=P4),nV=a(ta),zy=l(ta,"P",{"data-svelte-h":!0}),f(zy)!=="svelte-1enn9p2"&&(zy.innerHTML=W4),tV=a(ta),Oy=l(ta,"UL",{"data-svelte-h":!0}),f(Oy)!=="svelte-1uylm59"&&(Oy.innerHTML=S4),aV=a(ta),Ky=l(ta,"P",{"data-svelte-h":!0}),f(Ky)!=="svelte-tebhz6"&&(Ky.innerHTML=G4),sV=a(ta),h(ql.$$.fragment,ta),ta.forEach(d),_s.forEach(d),Lx=a(o),h(zp.$$.fragment,o),$x=a(o),To=l(o,"DIV",{class:!0});var bs=v(To);h(Op.$$.fragment,bs),iV=a(bs),ev=l(bs,"P",{"data-svelte-h":!0}),f(ev)!=="svelte-a7jxk9"&&(ev.innerHTML=V4),lV=a(bs),ov=l(bs,"P",{"data-svelte-h":!0}),f(ov)!=="svelte-1n2cphd"&&(ov.innerHTML=J4),dV=a(bs),It=l(bs,"DIV",{class:!0});var Fd=v(It);h(Kp.$$.fragment,Fd),mV=a(Fd),rv=l(Fd,"P",{"data-svelte-h":!0}),f(rv)!=="svelte-10jw89"&&(rv.textContent=E4),cV=a(Fd),nv=l(Fd,"P",{"data-svelte-h":!0}),f(nv)!=="svelte-ora4gm"&&(nv.innerHTML=I4),fV=a(Fd),h(Ql.$$.fragment,Fd),Fd.forEach(d),gV=a(bs),de=l(bs,"DIV",{class:!0});var aa=v(de);h(eh.$$.fragment,aa),pV=a(aa),tv=l(aa,"P",{"data-svelte-h":!0}),f(tv)!=="svelte-1mjzf2l"&&(tv.textContent=U4),hV=a(aa),av=l(aa,"P",{"data-svelte-h":!0}),f(av)!=="svelte-1enn9p2"&&(av.innerHTML=N4),uV=a(aa),sv=l(aa,"UL",{"data-svelte-h":!0}),f(sv)!=="svelte-7cpzqn"&&(sv.innerHTML=X4),_V=a(aa),iv=l(aa,"P",{"data-svelte-h":!0}),f(iv)!=="svelte-tebhz6"&&(iv.innerHTML=q4),bV=a(aa),h(Dl.$$.fragment,aa),aa.forEach(d),bs.forEach(d),Bx=a(o),h(oh.$$.fragment,o),jx=a(o),Co=l(o,"DIV",{class:!0});var Ms=v(Co);h(rh.$$.fragment,Ms),MV=a(Ms),lv=l(Ms,"P",{"data-svelte-h":!0}),f(lv)!=="svelte-a7jxk9"&&(lv.innerHTML=Q4),FV=a(Ms),dv=l(Ms,"P",{"data-svelte-h":!0}),f(dv)!=="svelte-1n2cphd"&&(dv.innerHTML=D4),TV=a(Ms),Ut=l(Ms,"DIV",{class:!0});var Td=v(Ut);h(nh.$$.fragment,Td),CV=a(Td),mv=l(Td,"P",{"data-svelte-h":!0}),f(mv)!=="svelte-10jw89"&&(mv.textContent=H4),yV=a(Td),cv=l(Td,"P",{"data-svelte-h":!0}),f(cv)!=="svelte-ora4gm"&&(cv.innerHTML=Y4),vV=a(Td),h(Hl.$$.fragment,Td),Td.forEach(d),wV=a(Ms),Fr=l(Ms,"DIV",{class:!0});var Fs=v(Fr);h(th.$$.fragment,Fs),xV=a(Fs),fv=l(Fs,"P",{"data-svelte-h":!0}),f(fv)!=="svelte-1mjzf2l"&&(fv.textContent=z4),kV=a(Fs),gv=l(Fs,"P",{"data-svelte-h":!0}),f(gv)!=="svelte-1enn9p2"&&(gv.innerHTML=O4),LV=a(Fs),pv=l(Fs,"UL",{"data-svelte-h":!0}),f(pv)!=="svelte-3v6j3b"&&(pv.innerHTML=K4),$V=a(Fs),h(Yl.$$.fragment,Fs),Fs.forEach(d),Ms.forEach(d),Zx=a(o),h(ah.$$.fragment,o),Ax=a(o),yo=l(o,"DIV",{class:!0});var Ts=v(yo);h(sh.$$.fragment,Ts),BV=a(Ts),hv=l(Ts,"P",{"data-svelte-h":!0}),f(hv)!=="svelte-a7jxk9"&&(hv.innerHTML=eX),jV=a(Ts),uv=l(Ts,"P",{"data-svelte-h":!0}),f(uv)!=="svelte-1n2cphd"&&(uv.innerHTML=oX),ZV=a(Ts),Nt=l(Ts,"DIV",{class:!0});var Cd=v(Nt);h(ih.$$.fragment,Cd),AV=a(Cd),_v=l(Cd,"P",{"data-svelte-h":!0}),f(_v)!=="svelte-10jw89"&&(_v.textContent=rX),RV=a(Cd),bv=l(Cd,"P",{"data-svelte-h":!0}),f(bv)!=="svelte-ora4gm"&&(bv.innerHTML=nX),PV=a(Cd),h(zl.$$.fragment,Cd),Cd.forEach(d),WV=a(Ts),Tr=l(Ts,"DIV",{class:!0});var Cs=v(Tr);h(lh.$$.fragment,Cs),SV=a(Cs),Mv=l(Cs,"P",{"data-svelte-h":!0}),f(Mv)!=="svelte-1mjzf2l"&&(Mv.textContent=tX),GV=a(Cs),Fv=l(Cs,"P",{"data-svelte-h":!0}),f(Fv)!=="svelte-1enn9p2"&&(Fv.innerHTML=aX),VV=a(Cs),Tv=l(Cs,"UL",{"data-svelte-h":!0}),f(Tv)!=="svelte-2sl2nt"&&(Tv.innerHTML=sX),JV=a(Cs),h(Ol.$$.fragment,Cs),Cs.forEach(d),Ts.forEach(d),Rx=a(o),Cv=l(o,"P",{}),v(Cv).forEach(d),this.h()},h(){w(n,"name","hf:doc:metadata"),w(n,"content",E3),w(Io,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ws,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Uo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ks,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Bs,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(As,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ws,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Sn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Gn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(No,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Vn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Xo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Jn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(En,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(qo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(In,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Qo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Un,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Nn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Do,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Xn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ho,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(qn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Qn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Yo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Dn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(zo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Om,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(oc,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Hn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Yn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Oo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(zn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ko,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(On,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Kn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(er,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(et,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(or,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(rr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(nr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(at,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(tr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(st,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ar,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(it,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(sr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ir,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(lr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(dr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(yf,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(xf,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(mr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(cr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(_t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(fr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(gg,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(eo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(gr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(oo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ro,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(no,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(xt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(to,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(kt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(pr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ao,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(so,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w($t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(io,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(hr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(lo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(mo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Zt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(co,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(At,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(fo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ur,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(go,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(_r,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(po,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Wt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ho,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(vp,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(kp,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(St,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(uo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(br,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(_o,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Vt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(bo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Mr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Mo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Et,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Fo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(It,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(To,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Fr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Co,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Tr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(yo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,g){r(document.head,n),T(o,C,g),T(o,m,g),T(o,s,g),u(c,o,g),T(o,e,g),T(o,F,g),T(o,kv,g),T(o,yd,g),T(o,Lv,g),u(vd,o,g),T(o,$v,g),T(o,wd,g),T(o,Bv,g),T(o,xd,g),T(o,jv,g),u(kd,o,g),T(o,Zv,g),T(o,Ld,g),T(o,Av,g),u($d,o,g),T(o,Rv,g),T(o,Bd,g),T(o,Pv,g),u(ys,o,g),T(o,Wv,g),u(jd,o,g),T(o,Sv,g),T(o,me,g),u(Zd,me,null),r(me,Sx),r(me,ph),r(me,Gx),r(me,hh),r(me,Vx),r(me,Io),u(Ad,Io,null),r(Io,Jx),r(Io,uh),r(Io,Ex),r(Io,_h),r(Io,Ix),r(Io,bh),r(Io,Ux),u(vs,Io,null),r(me,Nx),r(me,ws),u(Rd,ws,null),r(ws,Xx),r(ws,Mh),T(o,Gv,g),u(Pd,o,g),T(o,Vv,g),T(o,ce,g),u(Wd,ce,null),r(ce,qx),r(ce,Fh),r(ce,Qx),r(ce,Th),r(ce,Dx),r(ce,Uo),u(Sd,Uo,null),r(Uo,Hx),r(Uo,Ch),r(Uo,Yx),r(Uo,yh),r(Uo,zx),r(Uo,vh),r(Uo,Ox),u(xs,Uo,null),r(ce,Kx),r(ce,ks),u(Gd,ks,null),r(ks,ek),r(ks,wh),T(o,Jv,g),u(Vd,o,g),T(o,Ev,g),T(o,fe,g),u(Jd,fe,null),r(fe,ok),r(fe,xh),r(fe,rk),r(fe,kh),r(fe,nk),r(fe,j),u(Ed,j,null),r(j,tk),r(j,Lh),r(j,ak),r(j,$h),r(j,sk),r(j,Bh),r(j,ik),u(Ls,j,null),r(j,lk),u($s,j,null),r(fe,dk),r(fe,Bs),u(Id,Bs,null),r(Bs,mk),r(Bs,jh),T(o,Iv,g),u(Ud,o,g),T(o,Uv,g),T(o,ge,g),u(Nd,ge,null),r(ge,ck),r(ge,Zh),r(ge,fk),r(ge,Ah),r(ge,gk),r(ge,Z),u(Xd,Z,null),r(Z,pk),r(Z,Rh),r(Z,hk),r(Z,Ph),r(Z,uk),r(Z,Wh),r(Z,_k),u(js,Z,null),r(Z,bk),u(Zs,Z,null),r(ge,Mk),r(ge,As),u(qd,As,null),r(As,Fk),r(As,Sh),T(o,Nv,g),u(Qd,o,g),T(o,Xv,g),T(o,pe,g),u(Dd,pe,null),r(pe,Tk),r(pe,Gh),r(pe,Ck),r(pe,Vh),r(pe,yk),r(pe,A),u(Hd,A,null),r(A,vk),r(A,Jh),r(A,wk),r(A,Eh),r(A,xk),r(A,Ih),r(A,kk),u(Rs,A,null),r(A,Lk),u(Ps,A,null),r(pe,$k),r(pe,Ws),u(Yd,Ws,null),r(Ws,Bk),r(Ws,Uh),T(o,qv,g),u(zd,o,g),T(o,Qv,g),T(o,Od,g),T(o,Dv,g),u(Kd,o,g),T(o,Hv,g),T(o,he,g),u(em,he,null),r(he,jk),r(he,Nh),r(he,Zk),r(he,Xh),r(he,Ak),r(he,Sn),u(om,Sn,null),r(Sn,Rk),r(Sn,qh),r(Sn,Pk),r(Sn,Qh),r(Sn,Wk),u(Ss,Sn,null),r(he,Sk),r(he,R),u(rm,R,null),r(R,Gk),r(R,Dh),r(R,Vk),r(R,Hh),r(R,Jk),r(R,Yh),r(R,Ek),r(R,zh),r(R,Ik),u(Gs,R,null),T(o,Yv,g),u(nm,o,g),T(o,zv,g),T(o,ue,g),u(tm,ue,null),r(ue,Uk),r(ue,Oh),r(ue,Nk),r(ue,Kh),r(ue,Xk),r(ue,Gn),u(am,Gn,null),r(Gn,qk),r(Gn,eu),r(Gn,Qk),r(Gn,ou),r(Gn,Dk),u(Vs,Gn,null),r(ue,Hk),r(ue,No),u(sm,No,null),r(No,Yk),r(No,ru),r(No,zk),r(No,nu),r(No,Ok),r(No,tu),r(No,Kk),u(Js,No,null),T(o,Ov,g),u(im,o,g),T(o,Kv,g),T(o,_e,g),u(lm,_e,null),r(_e,eL),r(_e,au),r(_e,oL),r(_e,su),r(_e,rL),r(_e,Vn),u(dm,Vn,null),r(Vn,nL),r(Vn,iu),r(Vn,tL),r(Vn,lu),r(Vn,aL),u(Es,Vn,null),r(_e,sL),r(_e,Xo),u(mm,Xo,null),r(Xo,iL),r(Xo,du),r(Xo,lL),r(Xo,mu),r(Xo,dL),r(Xo,cu),r(Xo,mL),u(Is,Xo,null),T(o,e2,g),u(cm,o,g),T(o,o2,g),T(o,fm,g),T(o,r2,g),u(gm,o,g),T(o,n2,g),T(o,be,g),u(pm,be,null),r(be,cL),r(be,fu),r(be,fL),r(be,gu),r(be,gL),r(be,Jn),u(hm,Jn,null),r(Jn,pL),r(Jn,pu),r(Jn,hL),r(Jn,hu),r(Jn,uL),u(Us,Jn,null),r(be,_L),r(be,P),u(um,P,null),r(P,bL),r(P,uu),r(P,ML),r(P,_u),r(P,FL),r(P,bu),r(P,TL),r(P,Mu),r(P,CL),u(Ns,P,null),T(o,t2,g),u(_m,o,g),T(o,a2,g),T(o,Me,g),u(bm,Me,null),r(Me,yL),r(Me,Fu),r(Me,vL),r(Me,Tu),r(Me,wL),r(Me,En),u(Mm,En,null),r(En,xL),r(En,Cu),r(En,kL),r(En,yu),r(En,LL),u(Xs,En,null),r(Me,$L),r(Me,qo),u(Fm,qo,null),r(qo,BL),r(qo,vu),r(qo,jL),r(qo,wu),r(qo,ZL),r(qo,xu),r(qo,AL),u(qs,qo,null),T(o,s2,g),u(Tm,o,g),T(o,i2,g),T(o,Fe,g),u(Cm,Fe,null),r(Fe,RL),r(Fe,ku),r(Fe,PL),r(Fe,Lu),r(Fe,WL),r(Fe,In),u(ym,In,null),r(In,SL),r(In,$u),r(In,GL),r(In,Bu),r(In,VL),u(Qs,In,null),r(Fe,JL),r(Fe,Qo),u(vm,Qo,null),r(Qo,EL),r(Qo,ju),r(Qo,IL),r(Qo,Zu),r(Qo,UL),r(Qo,Au),r(Qo,NL),u(Ds,Qo,null),T(o,l2,g),u(wm,o,g),T(o,d2,g),T(o,xm,g),T(o,m2,g),u(km,o,g),T(o,c2,g),T(o,Te,g),u(Lm,Te,null),r(Te,XL),r(Te,Ru),r(Te,qL),r(Te,Pu),r(Te,QL),r(Te,Un),u($m,Un,null),r(Un,DL),r(Un,Wu),r(Un,HL),r(Un,Su),r(Un,YL),u(Hs,Un,null),r(Te,zL),r(Te,W),u(Bm,W,null),r(W,OL),r(W,Gu),r(W,KL),r(W,Vu),r(W,e1),r(W,Ju),r(W,o1),r(W,Eu),r(W,r1),u(Ys,W,null),T(o,f2,g),u(jm,o,g),T(o,g2,g),T(o,Ce,g),u(Zm,Ce,null),r(Ce,n1),r(Ce,Iu),r(Ce,t1),r(Ce,Uu),r(Ce,a1),r(Ce,Nn),u(Am,Nn,null),r(Nn,s1),r(Nn,Nu),r(Nn,i1),r(Nn,Xu),r(Nn,l1),u(zs,Nn,null),r(Ce,d1),r(Ce,Do),u(Rm,Do,null),r(Do,m1),r(Do,qu),r(Do,c1),r(Do,Qu),r(Do,f1),r(Do,Du),r(Do,g1),u(Os,Do,null),T(o,p2,g),u(Pm,o,g),T(o,h2,g),T(o,ye,g),u(Wm,ye,null),r(ye,p1),r(ye,Hu),r(ye,h1),r(ye,Yu),r(ye,u1),r(ye,Xn),u(Sm,Xn,null),r(Xn,_1),r(Xn,zu),r(Xn,b1),r(Xn,Ou),r(Xn,M1),u(Ks,Xn,null),r(ye,F1),r(ye,Ho),u(Gm,Ho,null),r(Ho,T1),r(Ho,Ku),r(Ho,C1),r(Ho,e_),r(Ho,y1),r(Ho,o_),r(Ho,v1),u(ei,Ho,null),T(o,u2,g),u(Vm,o,g),T(o,_2,g),T(o,ve,g),u(Jm,ve,null),r(ve,w1),r(ve,r_),r(ve,x1),r(ve,n_),r(ve,k1),r(ve,qn),u(Em,qn,null),r(qn,L1),r(qn,t_),r(qn,$1),r(qn,a_),r(qn,B1),u(oi,qn,null),r(ve,j1),r(ve,S),u(Im,S,null),r(S,Z1),r(S,s_),r(S,A1),r(S,i_),r(S,R1),r(S,l_),r(S,P1),r(S,d_),r(S,W1),u(ri,S,null),T(o,b2,g),u(Um,o,g),T(o,M2,g),T(o,we,g),u(Nm,we,null),r(we,S1),r(we,m_),r(we,G1),r(we,c_),r(we,V1),r(we,Qn),u(Xm,Qn,null),r(Qn,J1),r(Qn,f_),r(Qn,E1),r(Qn,g_),r(Qn,I1),u(ni,Qn,null),r(we,U1),r(we,Yo),u(qm,Yo,null),r(Yo,N1),r(Yo,p_),r(Yo,X1),r(Yo,h_),r(Yo,q1),r(Yo,u_),r(Yo,Q1),u(ti,Yo,null),T(o,F2,g),u(Qm,o,g),T(o,T2,g),T(o,xe,g),u(Dm,xe,null),r(xe,D1),r(xe,__),r(xe,H1),r(xe,b_),r(xe,Y1),r(xe,Dn),u(Hm,Dn,null),r(Dn,z1),r(Dn,M_),r(Dn,O1),r(Dn,F_),r(Dn,K1),u(ai,Dn,null),r(xe,e$),r(xe,zo),u(Ym,zo,null),r(zo,o$),r(zo,T_),r(zo,r$),r(zo,C_),r(zo,n$),r(zo,y_),r(zo,t$),u(si,zo,null),T(o,C2,g),u(zm,o,g),T(o,y2,g),T(o,Om,g),u(Km,Om,null),T(o,v2,g),u(ec,o,g),T(o,w2,g),T(o,oc,g),u(rc,oc,null),T(o,x2,g),u(nc,o,g),T(o,k2,g),T(o,ke,g),u(tc,ke,null),r(ke,a$),r(ke,v_),r(ke,s$),r(ke,w_),r(ke,i$),r(ke,Hn),u(ac,Hn,null),r(Hn,l$),r(Hn,x_),r(Hn,d$),r(Hn,k_),r(Hn,m$),u(ii,Hn,null),r(ke,c$),r(ke,G),u(sc,G,null),r(G,f$),r(G,L_),r(G,g$),r(G,$_),r(G,p$),r(G,B_),r(G,h$),r(G,j_),r(G,u$),u(li,G,null),T(o,L2,g),u(ic,o,g),T(o,$2,g),T(o,Le,g),u(lc,Le,null),r(Le,_$),r(Le,Z_),r(Le,b$),r(Le,A_),r(Le,M$),r(Le,Yn),u(dc,Yn,null),r(Yn,F$),r(Yn,R_),r(Yn,T$),r(Yn,P_),r(Yn,C$),u(di,Yn,null),r(Le,y$),r(Le,Oo),u(mc,Oo,null),r(Oo,v$),r(Oo,W_),r(Oo,w$),r(Oo,S_),r(Oo,x$),r(Oo,G_),r(Oo,k$),u(mi,Oo,null),T(o,B2,g),u(cc,o,g),T(o,j2,g),T(o,$e,g),u(fc,$e,null),r($e,L$),r($e,V_),r($e,$$),r($e,J_),r($e,B$),r($e,zn),u(gc,zn,null),r(zn,j$),r(zn,E_),r(zn,Z$),r(zn,I_),r(zn,A$),u(ci,zn,null),r($e,R$),r($e,Ko),u(pc,Ko,null),r(Ko,P$),r(Ko,U_),r(Ko,W$),r(Ko,N_),r(Ko,S$),r(Ko,X_),r(Ko,G$),u(fi,Ko,null),T(o,Z2,g),u(hc,o,g),T(o,A2,g),T(o,Be,g),u(uc,Be,null),r(Be,V$),r(Be,q_),r(Be,J$),r(Be,Q_),r(Be,E$),r(Be,On),u(_c,On,null),r(On,I$),r(On,D_),r(On,U$),r(On,H_),r(On,N$),u(gi,On,null),r(Be,X$),r(Be,V),u(bc,V,null),r(V,q$),r(V,Y_),r(V,Q$),r(V,z_),r(V,D$),r(V,O_),r(V,H$),r(V,K_),r(V,Y$),u(pi,V,null),T(o,R2,g),u(Mc,o,g),T(o,P2,g),T(o,je,g),u(Fc,je,null),r(je,z$),r(je,eb),r(je,O$),r(je,ob),r(je,K$),r(je,Kn),u(Tc,Kn,null),r(Kn,eB),r(Kn,rb),r(Kn,oB),r(Kn,nb),r(Kn,rB),u(hi,Kn,null),r(je,nB),r(je,er),u(Cc,er,null),r(er,tB),r(er,tb),r(er,aB),r(er,ab),r(er,sB),r(er,sb),r(er,iB),u(ui,er,null),T(o,W2,g),u(yc,o,g),T(o,S2,g),T(o,Ze,g),u(vc,Ze,null),r(Ze,lB),r(Ze,ib),r(Ze,dB),r(Ze,lb),r(Ze,mB),r(Ze,et),u(wc,et,null),r(et,cB),r(et,db),r(et,fB),r(et,mb),r(et,gB),u(_i,et,null),r(Ze,pB),r(Ze,or),u(xc,or,null),r(or,hB),r(or,cb),r(or,uB),r(or,fb),r(or,_B),r(or,gb),r(or,bB),u(bi,or,null),T(o,G2,g),u(kc,o,g),T(o,V2,g),T(o,Ae,g),u(Lc,Ae,null),r(Ae,MB),r(Ae,pb),r(Ae,FB),r(Ae,hb),r(Ae,TB),r(Ae,ot),u($c,ot,null),r(ot,CB),r(ot,ub),r(ot,yB),r(ot,_b),r(ot,vB),u(Mi,ot,null),r(Ae,wB),r(Ae,J),u(Bc,J,null),r(J,xB),r(J,bb),r(J,kB),r(J,Mb),r(J,LB),r(J,Fb),r(J,$B),r(J,Tb),r(J,BB),u(Fi,J,null),T(o,J2,g),u(jc,o,g),T(o,E2,g),T(o,Re,g),u(Zc,Re,null),r(Re,jB),r(Re,Cb),r(Re,ZB),r(Re,yb),r(Re,AB),r(Re,rt),u(Ac,rt,null),r(rt,RB),r(rt,vb),r(rt,PB),r(rt,wb),r(rt,WB),u(Ti,rt,null),r(Re,SB),r(Re,rr),u(Rc,rr,null),r(rr,GB),r(rr,xb),r(rr,VB),r(rr,kb),r(rr,JB),r(rr,Lb),r(rr,EB),u(Ci,rr,null),T(o,I2,g),u(Pc,o,g),T(o,U2,g),T(o,Pe,g),u(Wc,Pe,null),r(Pe,IB),r(Pe,$b),r(Pe,UB),r(Pe,Bb),r(Pe,NB),r(Pe,nt),u(Sc,nt,null),r(nt,XB),r(nt,jb),r(nt,qB),r(nt,Zb),r(nt,QB),u(yi,nt,null),r(Pe,DB),r(Pe,nr),u(Gc,nr,null),r(nr,HB),r(nr,Ab),r(nr,YB),r(nr,Rb),r(nr,zB),r(nr,Pb),r(nr,OB),u(vi,nr,null),T(o,N2,g),u(Vc,o,g),T(o,X2,g),T(o,We,g),u(Jc,We,null),r(We,KB),r(We,Wb),r(We,ej),r(We,Sb),r(We,oj),r(We,tt),u(Ec,tt,null),r(tt,rj),r(tt,Gb),r(tt,nj),r(tt,Vb),r(tt,tj),u(wi,tt,null),r(We,aj),r(We,E),u(Ic,E,null),r(E,sj),r(E,Jb),r(E,ij),r(E,Eb),r(E,lj),r(E,Ib),r(E,dj),r(E,Ub),r(E,mj),u(xi,E,null),T(o,q2,g),u(Uc,o,g),T(o,Q2,g),T(o,Se,g),u(Nc,Se,null),r(Se,cj),r(Se,Nb),r(Se,fj),r(Se,Xb),r(Se,gj),r(Se,at),u(Xc,at,null),r(at,pj),r(at,qb),r(at,hj),r(at,Qb),r(at,uj),u(ki,at,null),r(Se,_j),r(Se,tr),u(qc,tr,null),r(tr,bj),r(tr,Db),r(tr,Mj),r(tr,Hb),r(tr,Fj),r(tr,Yb),r(tr,Tj),u(Li,tr,null),T(o,D2,g),u(Qc,o,g),T(o,H2,g),T(o,Ge,g),u(Dc,Ge,null),r(Ge,Cj),r(Ge,zb),r(Ge,yj),r(Ge,Ob),r(Ge,vj),r(Ge,st),u(Hc,st,null),r(st,wj),r(st,Kb),r(st,xj),r(st,eM),r(st,kj),u($i,st,null),r(Ge,Lj),r(Ge,ar),u(Yc,ar,null),r(ar,$j),r(ar,oM),r(ar,Bj),r(ar,rM),r(ar,jj),r(ar,nM),r(ar,Zj),u(Bi,ar,null),T(o,Y2,g),u(zc,o,g),T(o,z2,g),T(o,Ve,g),u(Oc,Ve,null),r(Ve,Aj),r(Ve,tM),r(Ve,Rj),r(Ve,aM),r(Ve,Pj),r(Ve,it),u(Kc,it,null),r(it,Wj),r(it,sM),r(it,Sj),r(it,iM),r(it,Gj),u(ji,it,null),r(Ve,Vj),r(Ve,I),u(ef,I,null),r(I,Jj),r(I,lM),r(I,Ej),r(I,dM),r(I,Ij),r(I,mM),r(I,Uj),r(I,cM),r(I,Nj),u(Zi,I,null),T(o,O2,g),u(of,o,g),T(o,K2,g),T(o,Je,g),u(rf,Je,null),r(Je,Xj),r(Je,fM),r(Je,qj),r(Je,gM),r(Je,Qj),r(Je,lt),u(nf,lt,null),r(lt,Dj),r(lt,pM),r(lt,Hj),r(lt,hM),r(lt,Yj),u(Ai,lt,null),r(Je,zj),r(Je,sr),u(tf,sr,null),r(sr,Oj),r(sr,uM),r(sr,Kj),r(sr,_M),r(sr,eZ),r(sr,bM),r(sr,oZ),u(Ri,sr,null),T(o,ew,g),u(af,o,g),T(o,ow,g),T(o,Ee,g),u(sf,Ee,null),r(Ee,rZ),r(Ee,MM),r(Ee,nZ),r(Ee,FM),r(Ee,tZ),r(Ee,dt),u(lf,dt,null),r(dt,aZ),r(dt,TM),r(dt,sZ),r(dt,CM),r(dt,iZ),u(Pi,dt,null),r(Ee,lZ),r(Ee,ir),u(df,ir,null),r(ir,dZ),r(ir,yM),r(ir,mZ),r(ir,vM),r(ir,cZ),r(ir,wM),r(ir,fZ),u(Wi,ir,null),T(o,rw,g),u(mf,o,g),T(o,nw,g),T(o,Ie,g),u(cf,Ie,null),r(Ie,gZ),r(Ie,xM),r(Ie,pZ),r(Ie,kM),r(Ie,hZ),r(Ie,mt),u(ff,mt,null),r(mt,uZ),r(mt,LM),r(mt,_Z),r(mt,$M),r(mt,bZ),u(Si,mt,null),r(Ie,MZ),r(Ie,U),u(gf,U,null),r(U,FZ),r(U,BM),r(U,TZ),r(U,jM),r(U,CZ),r(U,ZM),r(U,yZ),r(U,AM),r(U,vZ),u(Gi,U,null),T(o,tw,g),u(pf,o,g),T(o,aw,g),T(o,Ue,g),u(hf,Ue,null),r(Ue,wZ),r(Ue,RM),r(Ue,xZ),r(Ue,PM),r(Ue,kZ),r(Ue,ct),u(uf,ct,null),r(ct,LZ),r(ct,WM),r(ct,$Z),r(ct,SM),r(ct,BZ),u(Vi,ct,null),r(Ue,jZ),r(Ue,lr),u(_f,lr,null),r(lr,ZZ),r(lr,GM),r(lr,AZ),r(lr,VM),r(lr,RZ),r(lr,JM),r(lr,PZ),u(Ji,lr,null),T(o,sw,g),u(bf,o,g),T(o,iw,g),T(o,Ne,g),u(Mf,Ne,null),r(Ne,WZ),r(Ne,EM),r(Ne,SZ),r(Ne,IM),r(Ne,GZ),r(Ne,ft),u(Ff,ft,null),r(ft,VZ),r(ft,UM),r(ft,JZ),r(ft,NM),r(ft,EZ),u(Ei,ft,null),r(Ne,IZ),r(Ne,dr),u(Tf,dr,null),r(dr,UZ),r(dr,XM),r(dr,NZ),r(dr,qM),r(dr,XZ),r(dr,QM),r(dr,qZ),u(Ii,dr,null),T(o,lw,g),u(Cf,o,g),T(o,dw,g),T(o,yf,g),u(vf,yf,null),T(o,mw,g),u(wf,o,g),T(o,cw,g),T(o,xf,g),u(kf,xf,null),T(o,fw,g),u(Lf,o,g),T(o,gw,g),T(o,$f,g),T(o,pw,g),u(Bf,o,g),T(o,hw,g),T(o,Xe,g),u(jf,Xe,null),r(Xe,QZ),r(Xe,DM),r(Xe,DZ),r(Xe,HM),r(Xe,HZ),r(Xe,gt),u(Zf,gt,null),r(gt,YZ),r(gt,YM),r(gt,zZ),r(gt,zM),r(gt,OZ),u(Ui,gt,null),r(Xe,KZ),r(Xe,N),u(Af,N,null),r(N,eA),r(N,OM),r(N,oA),r(N,KM),r(N,rA),r(N,eF),r(N,nA),r(N,oF),r(N,tA),u(Ni,N,null),T(o,uw,g),u(Rf,o,g),T(o,_w,g),T(o,qe,g),u(Pf,qe,null),r(qe,aA),r(qe,rF),r(qe,sA),r(qe,nF),r(qe,iA),r(qe,pt),u(Wf,pt,null),r(pt,lA),r(pt,tF),r(pt,dA),r(pt,aF),r(pt,mA),u(Xi,pt,null),r(qe,cA),r(qe,X),u(Sf,X,null),r(X,fA),r(X,sF),r(X,gA),r(X,iF),r(X,pA),r(X,lF),r(X,hA),r(X,dF),r(X,uA),u(qi,X,null),T(o,bw,g),u(Gf,o,g),T(o,Mw,g),T(o,Qe,g),u(Vf,Qe,null),r(Qe,_A),r(Qe,mF),r(Qe,bA),r(Qe,cF),r(Qe,MA),r(Qe,ht),u(Jf,ht,null),r(ht,FA),r(ht,fF),r(ht,TA),r(ht,gF),r(ht,CA),u(Qi,ht,null),r(Qe,yA),r(Qe,mr),u(Ef,mr,null),r(mr,vA),r(mr,pF),r(mr,wA),r(mr,hF),r(mr,xA),r(mr,uF),r(mr,kA),u(Di,mr,null),T(o,Fw,g),u(If,o,g),T(o,Tw,g),T(o,De,g),u(Uf,De,null),r(De,LA),r(De,_F),r(De,$A),r(De,bF),r(De,BA),r(De,ut),u(Nf,ut,null),r(ut,jA),r(ut,MF),r(ut,ZA),r(ut,FF),r(ut,AA),u(Hi,ut,null),r(De,RA),r(De,cr),u(Xf,cr,null),r(cr,PA),r(cr,TF),r(cr,WA),r(cr,CF),r(cr,SA),r(cr,yF),r(cr,GA),u(Yi,cr,null),T(o,Cw,g),u(qf,o,g),T(o,yw,g),T(o,He,g),u(Qf,He,null),r(He,VA),r(He,vF),r(He,JA),r(He,wF),r(He,EA),r(He,_t),u(Df,_t,null),r(_t,IA),r(_t,xF),r(_t,UA),r(_t,kF),r(_t,NA),u(zi,_t,null),r(He,XA),r(He,q),u(Hf,q,null),r(q,qA),r(q,LF),r(q,QA),r(q,$F),r(q,DA),r(q,BF),r(q,HA),r(q,jF),r(q,YA),u(Oi,q,null),T(o,vw,g),u(Yf,o,g),T(o,ww,g),T(o,Ye,g),u(zf,Ye,null),r(Ye,zA),r(Ye,ZF),r(Ye,OA),r(Ye,AF),r(Ye,KA),r(Ye,bt),u(Of,bt,null),r(bt,eR),r(bt,RF),r(bt,oR),r(bt,PF),r(bt,rR),u(Ki,bt,null),r(Ye,nR),r(Ye,Q),u(Kf,Q,null),r(Q,tR),r(Q,WF),r(Q,aR),r(Q,SF),r(Q,sR),r(Q,GF),r(Q,iR),r(Q,VF),r(Q,lR),u(el,Q,null),T(o,xw,g),u(eg,o,g),T(o,kw,g),T(o,ze,g),u(og,ze,null),r(ze,dR),r(ze,JF),r(ze,mR),r(ze,EF),r(ze,cR),r(ze,Mt),u(rg,Mt,null),r(Mt,fR),r(Mt,IF),r(Mt,gR),r(Mt,UF),r(Mt,pR),u(ol,Mt,null),r(ze,hR),r(ze,fr),u(ng,fr,null),r(fr,uR),r(fr,NF),r(fr,_R),r(fr,XF),r(fr,bR),r(fr,qF),r(fr,MR),u(rl,fr,null),T(o,Lw,g),u(tg,o,g),T(o,$w,g),T(o,Oe,g),u(ag,Oe,null),r(Oe,FR),r(Oe,QF),r(Oe,TR),r(Oe,DF),r(Oe,CR),r(Oe,Ft),u(sg,Ft,null),r(Ft,yR),r(Ft,HF),r(Ft,vR),r(Ft,YF),r(Ft,wR),u(nl,Ft,null),r(Oe,xR),r(Oe,D),u(ig,D,null),r(D,kR),r(D,zF),r(D,LR),r(D,OF),r(D,$R),r(D,KF),r(D,BR),r(D,eT),r(D,jR),u(tl,D,null),T(o,Bw,g),u(lg,o,g),T(o,jw,g),T(o,Ke,g),u(dg,Ke,null),r(Ke,ZR),r(Ke,oT),r(Ke,AR),r(Ke,rT),r(Ke,RR),r(Ke,Tt),u(mg,Tt,null),r(Tt,PR),r(Tt,nT),r(Tt,WR),r(Tt,tT),r(Tt,SR),u(al,Tt,null),r(Ke,GR),r(Ke,H),u(cg,H,null),r(H,VR),r(H,aT),r(H,JR),r(H,sT),r(H,ER),r(H,iT),r(H,IR),r(H,lT),r(H,UR),u(sl,H,null),T(o,Zw,g),u(fg,o,g),T(o,Aw,g),T(o,gg,g),u(pg,gg,null),T(o,Rw,g),u(hg,o,g),T(o,Pw,g),T(o,eo,g),u(ug,eo,null),r(eo,NR),r(eo,dT),r(eo,XR),r(eo,mT),r(eo,qR),r(eo,Ct),u(_g,Ct,null),r(Ct,QR),r(Ct,cT),r(Ct,DR),r(Ct,fT),r(Ct,HR),u(il,Ct,null),r(eo,YR),r(eo,Y),u(bg,Y,null),r(Y,zR),r(Y,gT),r(Y,OR),r(Y,pT),r(Y,KR),r(Y,hT),r(Y,eP),r(Y,uT),r(Y,oP),u(ll,Y,null),T(o,Ww,g),u(Mg,o,g),T(o,Sw,g),T(o,oo,g),u(Fg,oo,null),r(oo,rP),r(oo,_T),r(oo,nP),r(oo,bT),r(oo,tP),r(oo,yt),u(Tg,yt,null),r(yt,aP),r(yt,MT),r(yt,sP),r(yt,FT),r(yt,iP),u(dl,yt,null),r(oo,lP),r(oo,gr),u(Cg,gr,null),r(gr,dP),r(gr,TT),r(gr,mP),r(gr,CT),r(gr,cP),r(gr,yT),r(gr,fP),u(ml,gr,null),T(o,Gw,g),u(yg,o,g),T(o,Vw,g),T(o,ro,g),u(vg,ro,null),r(ro,gP),r(ro,vT),r(ro,pP),r(ro,wT),r(ro,hP),r(ro,vt),u(wg,vt,null),r(vt,uP),r(vt,xT),r(vt,_P),r(vt,kT),r(vt,bP),u(cl,vt,null),r(ro,MP),r(ro,z),u(xg,z,null),r(z,FP),r(z,LT),r(z,TP),r(z,$T),r(z,CP),r(z,BT),r(z,yP),r(z,jT),r(z,vP),u(fl,z,null),T(o,Jw,g),u(kg,o,g),T(o,Ew,g),T(o,no,g),u(Lg,no,null),r(no,wP),r(no,ZT),r(no,xP),r(no,AT),r(no,kP),r(no,wt),u($g,wt,null),r(wt,LP),r(wt,RT),r(wt,$P),r(wt,PT),r(wt,BP),u(gl,wt,null),r(no,jP),r(no,O),u(Bg,O,null),r(O,ZP),r(O,WT),r(O,AP),r(O,ST),r(O,RP),r(O,GT),r(O,PP),r(O,VT),r(O,WP),u(pl,O,null),T(o,Iw,g),u(jg,o,g),T(o,Uw,g),T(o,to,g),u(Zg,to,null),r(to,SP),r(to,JT),r(to,GP),r(to,ET),r(to,VP),r(to,xt),u(Ag,xt,null),r(xt,JP),r(xt,IT),r(xt,EP),r(xt,UT),r(xt,IP),u(hl,xt,null),r(to,UP),r(to,K),u(Rg,K,null),r(K,NP),r(K,NT),r(K,XP),r(K,XT),r(K,qP),r(K,qT),r(K,QP),r(K,QT),r(K,DP),u(ul,K,null),T(o,Nw,g),u(Pg,o,g),T(o,Xw,g),T(o,ao,g),u(Wg,ao,null),r(ao,HP),r(ao,DT),r(ao,YP),r(ao,HT),r(ao,zP),r(ao,kt),u(Sg,kt,null),r(kt,OP),r(kt,YT),r(kt,KP),r(kt,zT),r(kt,eW),u(_l,kt,null),r(ao,oW),r(ao,pr),u(Gg,pr,null),r(pr,rW),r(pr,OT),r(pr,nW),r(pr,KT),r(pr,tW),r(pr,eC),r(pr,aW),u(bl,pr,null),T(o,qw,g),u(Vg,o,g),T(o,Qw,g),T(o,so,g),u(Jg,so,null),r(so,sW),r(so,oC),r(so,iW),r(so,rC),r(so,lW),r(so,Lt),u(Eg,Lt,null),r(Lt,dW),r(Lt,nC),r(Lt,mW),r(Lt,tC),r(Lt,cW),u(Ml,Lt,null),r(so,fW),r(so,ee),u(Ig,ee,null),r(ee,gW),r(ee,aC),r(ee,pW),r(ee,sC),r(ee,hW),r(ee,iC),r(ee,uW),r(ee,lC),r(ee,_W),u(Fl,ee,null),T(o,Dw,g),u(Ug,o,g),T(o,Hw,g),T(o,Ng,g),T(o,Yw,g),u(Xg,o,g),T(o,zw,g),T(o,io,g),u(qg,io,null),r(io,bW),r(io,dC),r(io,MW),r(io,mC),r(io,FW),r(io,$t),u(Qg,$t,null),r($t,TW),r($t,cC),r($t,CW),r($t,fC),r($t,yW),u(Tl,$t,null),r(io,vW),r(io,oe),u(Dg,oe,null),r(oe,wW),r(oe,gC),r(oe,xW),r(oe,pC),r(oe,kW),r(oe,hC),r(oe,LW),r(oe,uC),r(oe,$W),u(Cl,oe,null),T(o,Ow,g),u(Hg,o,g),T(o,Kw,g),T(o,lo,g),u(Yg,lo,null),r(lo,BW),r(lo,_C),r(lo,jW),r(lo,bC),r(lo,ZW),r(lo,Bt),u(zg,Bt,null),r(Bt,AW),r(Bt,MC),r(Bt,RW),r(Bt,FC),r(Bt,PW),u(yl,Bt,null),r(lo,WW),r(lo,hr),u(Og,hr,null),r(hr,SW),r(hr,TC),r(hr,GW),r(hr,CC),r(hr,VW),r(hr,yC),r(hr,JW),u(vl,hr,null),T(o,ex,g),u(Kg,o,g),T(o,ox,g),T(o,mo,g),u(ep,mo,null),r(mo,EW),r(mo,vC),r(mo,IW),r(mo,wC),r(mo,UW),r(mo,jt),u(op,jt,null),r(jt,NW),r(jt,xC),r(jt,XW),r(jt,kC),r(jt,qW),u(wl,jt,null),r(mo,QW),r(mo,re),u(rp,re,null),r(re,DW),r(re,LC),r(re,HW),r(re,$C),r(re,YW),r(re,BC),r(re,zW),r(re,jC),r(re,OW),u(xl,re,null),T(o,rx,g),u(np,o,g),T(o,nx,g),T(o,co,g),u(tp,co,null),r(co,KW),r(co,ZC),r(co,eS),r(co,AC),r(co,oS),r(co,Zt),u(ap,Zt,null),r(Zt,rS),r(Zt,RC),r(Zt,nS),r(Zt,PC),r(Zt,tS),u(kl,Zt,null),r(co,aS),r(co,ne),u(sp,ne,null),r(ne,sS),r(ne,WC),r(ne,iS),r(ne,SC),r(ne,lS),r(ne,GC),r(ne,dS),r(ne,VC),r(ne,mS),u(Ll,ne,null),T(o,tx,g),u(ip,o,g),T(o,ax,g),T(o,fo,g),u(lp,fo,null),r(fo,cS),r(fo,JC),r(fo,fS),r(fo,EC),r(fo,gS),r(fo,At),u(dp,At,null),r(At,pS),r(At,IC),r(At,hS),r(At,UC),r(At,uS),u($l,At,null),r(fo,_S),r(fo,te),u(mp,te,null),r(te,bS),r(te,NC),r(te,MS),r(te,XC),r(te,FS),r(te,qC),r(te,TS),r(te,QC),r(te,CS),u(Bl,te,null),T(o,sx,g),u(cp,o,g),T(o,ix,g),T(o,go,g),u(fp,go,null),r(go,yS),r(go,DC),r(go,vS),r(go,HC),r(go,wS),r(go,Rt),u(gp,Rt,null),r(Rt,xS),r(Rt,YC),r(Rt,kS),r(Rt,zC),r(Rt,LS),u(jl,Rt,null),r(go,$S),r(go,ur),u(pp,ur,null),r(ur,BS),r(ur,OC),r(ur,jS),r(ur,KC),r(ur,ZS),r(ur,ey),r(ur,AS),u(Zl,ur,null),T(o,lx,g),u(hp,o,g),T(o,dx,g),T(o,po,g),u(up,po,null),r(po,RS),r(po,oy),r(po,PS),r(po,ry),r(po,WS),r(po,Pt),u(_p,Pt,null),r(Pt,SS),r(Pt,ny),r(Pt,GS),r(Pt,ty),r(Pt,VS),u(Al,Pt,null),r(po,JS),r(po,_r),u(bp,_r,null),r(_r,ES),r(_r,ay),r(_r,IS),r(_r,sy),r(_r,US),r(_r,iy),r(_r,NS),u(Rl,_r,null),T(o,mx,g),u(Mp,o,g),T(o,cx,g),T(o,ho,g),u(Fp,ho,null),r(ho,XS),r(ho,ly),r(ho,qS),r(ho,dy),r(ho,QS),r(ho,Wt),u(Tp,Wt,null),r(Wt,DS),r(Wt,my),r(Wt,HS),r(Wt,cy),r(Wt,YS),u(Pl,Wt,null),r(ho,zS),r(ho,ae),u(Cp,ae,null),r(ae,OS),r(ae,fy),r(ae,KS),r(ae,gy),r(ae,eG),r(ae,py),r(ae,oG),r(ae,hy),r(ae,rG),u(Wl,ae,null),T(o,fx,g),u(yp,o,g),T(o,gx,g),T(o,vp,g),u(wp,vp,null),T(o,px,g),u(xp,o,g),T(o,hx,g),T(o,kp,g),u(Lp,kp,null),T(o,ux,g),u($p,o,g),T(o,_x,g),T(o,Bp,g),T(o,bx,g),u(jp,o,g),T(o,Mx,g),T(o,uo,g),u(Zp,uo,null),r(uo,nG),r(uo,uy),r(uo,tG),r(uo,_y),r(uo,aG),r(uo,St),u(Ap,St,null),r(St,sG),r(St,by),r(St,iG),r(St,My),r(St,lG),u(Sl,St,null),r(uo,dG),r(uo,se),u(Rp,se,null),r(se,mG),r(se,Fy),r(se,cG),r(se,Ty),r(se,fG),r(se,Cy),r(se,gG),r(se,yy),r(se,pG),u(Gl,se,null),T(o,Fx,g),u(Pp,o,g),T(o,Tx,g),T(o,_o,g),u(Wp,_o,null),r(_o,hG),r(_o,vy),r(_o,uG),r(_o,wy),r(_o,_G),r(_o,Gt),u(Sp,Gt,null),r(Gt,bG),r(Gt,xy),r(Gt,MG),r(Gt,ky),r(Gt,FG),u(Vl,Gt,null),r(_o,TG),r(_o,br),u(Gp,br,null),r(br,CG),r(br,Ly),r(br,yG),r(br,$y),r(br,vG),r(br,By),r(br,wG),u(Jl,br,null),T(o,Cx,g),u(Vp,o,g),T(o,yx,g),T(o,bo,g),u(Jp,bo,null),r(bo,xG),r(bo,jy),r(bo,kG),r(bo,Zy),r(bo,LG),r(bo,Vt),u(Ep,Vt,null),r(Vt,$G),r(Vt,Ay),r(Vt,BG),r(Vt,Ry),r(Vt,jG),u(El,Vt,null),r(bo,ZG),r(bo,ie),u(Ip,ie,null),r(ie,AG),r(ie,Py),r(ie,RG),r(ie,Wy),r(ie,PG),r(ie,Sy),r(ie,WG),r(ie,Gy),r(ie,SG),u(Il,ie,null),T(o,vx,g),u(Up,o,g),T(o,wx,g),T(o,Mo,g),u(Np,Mo,null),r(Mo,GG),r(Mo,Vy),r(Mo,VG),r(Mo,Jy),r(Mo,JG),r(Mo,Jt),u(Xp,Jt,null),r(Jt,EG),r(Jt,Ey),r(Jt,IG),r(Jt,Iy),r(Jt,UG),u(Ul,Jt,null),r(Mo,NG),r(Mo,Mr),u(qp,Mr,null),r(Mr,XG),r(Mr,Uy),r(Mr,qG),r(Mr,Ny),r(Mr,QG),r(Mr,Xy),r(Mr,DG),u(Nl,Mr,null),T(o,xx,g),u(Qp,o,g),T(o,kx,g),T(o,Fo,g),u(Dp,Fo,null),r(Fo,HG),r(Fo,qy),r(Fo,YG),r(Fo,Qy),r(Fo,zG),r(Fo,Et),u(Hp,Et,null),r(Et,OG),r(Et,Dy),r(Et,KG),r(Et,Hy),r(Et,eV),u(Xl,Et,null),r(Fo,oV),r(Fo,le),u(Yp,le,null),r(le,rV),r(le,Yy),r(le,nV),r(le,zy),r(le,tV),r(le,Oy),r(le,aV),r(le,Ky),r(le,sV),u(ql,le,null),T(o,Lx,g),u(zp,o,g),T(o,$x,g),T(o,To,g),u(Op,To,null),r(To,iV),r(To,ev),r(To,lV),r(To,ov),r(To,dV),r(To,It),u(Kp,It,null),r(It,mV),r(It,rv),r(It,cV),r(It,nv),r(It,fV),u(Ql,It,null),r(To,gV),r(To,de),u(eh,de,null),r(de,pV),r(de,tv),r(de,hV),r(de,av),r(de,uV),r(de,sv),r(de,_V),r(de,iv),r(de,bV),u(Dl,de,null),T(o,Bx,g),u(oh,o,g),T(o,jx,g),T(o,Co,g),u(rh,Co,null),r(Co,MV),r(Co,lv),r(Co,FV),r(Co,dv),r(Co,TV),r(Co,Ut),u(nh,Ut,null),r(Ut,CV),r(Ut,mv),r(Ut,yV),r(Ut,cv),r(Ut,vV),u(Hl,Ut,null),r(Co,wV),r(Co,Fr),u(th,Fr,null),r(Fr,xV),r(Fr,fv),r(Fr,kV),r(Fr,gv),r(Fr,LV),r(Fr,pv),r(Fr,$V),u(Yl,Fr,null),T(o,Zx,g),u(ah,o,g),T(o,Ax,g),T(o,yo,g),u(sh,yo,null),r(yo,BV),r(yo,hv),r(yo,jV),r(yo,uv),r(yo,ZV),r(yo,Nt),u(ih,Nt,null),r(Nt,AV),r(Nt,_v),r(Nt,RV),r(Nt,bv),r(Nt,PV),u(zl,Nt,null),r(yo,WV),r(yo,Tr),u(lh,Tr,null),r(Tr,SV),r(Tr,Mv),r(Tr,GV),r(Tr,Fv),r(Tr,VV),r(Tr,Tv),r(Tr,JV),u(Ol,Tr,null),T(o,Rx,g),T(o,Cv,g),Px=!0},p(o,[g]){const Cr={};g&2&&(Cr.$$scope={dirty:g,ctx:o}),ys.$set(Cr);const yr={};g&2&&(yr.$$scope={dirty:g,ctx:o}),vs.$set(yr);const dh={};g&2&&(dh.$$scope={dirty:g,ctx:o}),xs.$set(dh);const vr={};g&2&&(vr.$$scope={dirty:g,ctx:o}),Ls.$set(vr);const wr={};g&2&&(wr.$$scope={dirty:g,ctx:o}),$s.$set(wr);const mh={};g&2&&(mh.$$scope={dirty:g,ctx:o}),js.$set(mh);const xr={};g&2&&(xr.$$scope={dirty:g,ctx:o}),Zs.$set(xr);const vo={};g&2&&(vo.$$scope={dirty:g,ctx:o}),Rs.$set(vo);const ch={};g&2&&(ch.$$scope={dirty:g,ctx:o}),Ps.$set(ch);const kr={};g&2&&(kr.$$scope={dirty:g,ctx:o}),Ss.$set(kr);const wo={};g&2&&(wo.$$scope={dirty:g,ctx:o}),Gs.$set(wo);const fh={};g&2&&(fh.$$scope={dirty:g,ctx:o}),Vs.$set(fh);const Lr={};g&2&&(Lr.$$scope={dirty:g,ctx:o}),Js.$set(Lr);const xo={};g&2&&(xo.$$scope={dirty:g,ctx:o}),Es.$set(xo);const gh={};g&2&&(gh.$$scope={dirty:g,ctx:o}),Is.$set(gh);const $r={};g&2&&($r.$$scope={dirty:g,ctx:o}),Us.$set($r);const sa={};g&2&&(sa.$$scope={dirty:g,ctx:o}),Ns.$set(sa);const ko={};g&2&&(ko.$$scope={dirty:g,ctx:o}),Xs.$set(ko);const Br={};g&2&&(Br.$$scope={dirty:g,ctx:o}),qs.$set(Br);const ia={};g&2&&(ia.$$scope={dirty:g,ctx:o}),Qs.$set(ia);const jr={};g&2&&(jr.$$scope={dirty:g,ctx:o}),Ds.$set(jr);const Zr={};g&2&&(Zr.$$scope={dirty:g,ctx:o}),Hs.$set(Zr);const la={};g&2&&(la.$$scope={dirty:g,ctx:o}),Ys.$set(la);const Ar={};g&2&&(Ar.$$scope={dirty:g,ctx:o}),zs.$set(Ar);const Rr={};g&2&&(Rr.$$scope={dirty:g,ctx:o}),Os.$set(Rr);const da={};g&2&&(da.$$scope={dirty:g,ctx:o}),Ks.$set(da);const Lo={};g&2&&(Lo.$$scope={dirty:g,ctx:o}),ei.$set(Lo);const Pr={};g&2&&(Pr.$$scope={dirty:g,ctx:o}),oi.$set(Pr);const ma={};g&2&&(ma.$$scope={dirty:g,ctx:o}),ri.$set(ma);const Wr={};g&2&&(Wr.$$scope={dirty:g,ctx:o}),ni.$set(Wr);const Sr={};g&2&&(Sr.$$scope={dirty:g,ctx:o}),ti.$set(Sr);const ca={};g&2&&(ca.$$scope={dirty:g,ctx:o}),ai.$set(ca);const Gr={};g&2&&(Gr.$$scope={dirty:g,ctx:o}),si.$set(Gr);const Vr={};g&2&&(Vr.$$scope={dirty:g,ctx:o}),ii.$set(Vr);const fa={};g&2&&(fa.$$scope={dirty:g,ctx:o}),li.$set(fa);const $o={};g&2&&($o.$$scope={dirty:g,ctx:o}),di.$set($o);const Jr={};g&2&&(Jr.$$scope={dirty:g,ctx:o}),mi.$set(Jr);const ga={};g&2&&(ga.$$scope={dirty:g,ctx:o}),ci.$set(ga);const Er={};g&2&&(Er.$$scope={dirty:g,ctx:o}),fi.$set(Er);const Ir={};g&2&&(Ir.$$scope={dirty:g,ctx:o}),gi.$set(Ir);const pa={};g&2&&(pa.$$scope={dirty:g,ctx:o}),pi.$set(pa);const Ur={};g&2&&(Ur.$$scope={dirty:g,ctx:o}),hi.$set(Ur);const Nr={};g&2&&(Nr.$$scope={dirty:g,ctx:o}),ui.$set(Nr);const ha={};g&2&&(ha.$$scope={dirty:g,ctx:o}),_i.$set(ha);const Bo={};g&2&&(Bo.$$scope={dirty:g,ctx:o}),bi.$set(Bo);const Xr={};g&2&&(Xr.$$scope={dirty:g,ctx:o}),Mi.$set(Xr);const ua={};g&2&&(ua.$$scope={dirty:g,ctx:o}),Fi.$set(ua);const qr={};g&2&&(qr.$$scope={dirty:g,ctx:o}),Ti.$set(qr);const Qr={};g&2&&(Qr.$$scope={dirty:g,ctx:o}),Ci.$set(Qr);const _a={};g&2&&(_a.$$scope={dirty:g,ctx:o}),yi.$set(_a);const Dr={};g&2&&(Dr.$$scope={dirty:g,ctx:o}),vi.$set(Dr);const yv={};g&2&&(yv.$$scope={dirty:g,ctx:o}),wi.$set(yv);const vv={};g&2&&(vv.$$scope={dirty:g,ctx:o}),xi.$set(vv);const Hr={};g&2&&(Hr.$$scope={dirty:g,ctx:o}),ki.$set(Hr);const ba={};g&2&&(ba.$$scope={dirty:g,ctx:o}),Li.$set(ba);const jo={};g&2&&(jo.$$scope={dirty:g,ctx:o}),$i.$set(jo);const Yr={};g&2&&(Yr.$$scope={dirty:g,ctx:o}),Bi.$set(Yr);const Ma={};g&2&&(Ma.$$scope={dirty:g,ctx:o}),ji.$set(Ma);const zr={};g&2&&(zr.$$scope={dirty:g,ctx:o}),Zi.$set(zr);const Or={};g&2&&(Or.$$scope={dirty:g,ctx:o}),Ai.$set(Or);const Fa={};g&2&&(Fa.$$scope={dirty:g,ctx:o}),Ri.$set(Fa);const Kr={};g&2&&(Kr.$$scope={dirty:g,ctx:o}),Pi.$set(Kr);const en={};g&2&&(en.$$scope={dirty:g,ctx:o}),Wi.$set(en);const Ta={};g&2&&(Ta.$$scope={dirty:g,ctx:o}),Si.$set(Ta);const Zo={};g&2&&(Zo.$$scope={dirty:g,ctx:o}),Gi.$set(Zo);const on={};g&2&&(on.$$scope={dirty:g,ctx:o}),Vi.$set(on);const Ca={};g&2&&(Ca.$$scope={dirty:g,ctx:o}),Ji.$set(Ca);const rn={};g&2&&(rn.$$scope={dirty:g,ctx:o}),Ei.$set(rn);const nn={};g&2&&(nn.$$scope={dirty:g,ctx:o}),Ii.$set(nn);const ya={};g&2&&(ya.$$scope={dirty:g,ctx:o}),Ui.$set(ya);const tn={};g&2&&(tn.$$scope={dirty:g,ctx:o}),Ni.$set(tn);const an={};g&2&&(an.$$scope={dirty:g,ctx:o}),Xi.$set(an);const va={};g&2&&(va.$$scope={dirty:g,ctx:o}),qi.$set(va);const Ao={};g&2&&(Ao.$$scope={dirty:g,ctx:o}),Qi.$set(Ao);const sn={};g&2&&(sn.$$scope={dirty:g,ctx:o}),Di.$set(sn);const wa={};g&2&&(wa.$$scope={dirty:g,ctx:o}),Hi.$set(wa);const ln={};g&2&&(ln.$$scope={dirty:g,ctx:o}),Yi.$set(ln);const dn={};g&2&&(dn.$$scope={dirty:g,ctx:o}),zi.$set(dn);const xa={};g&2&&(xa.$$scope={dirty:g,ctx:o}),Oi.$set(xa);const mn={};g&2&&(mn.$$scope={dirty:g,ctx:o}),Ki.$set(mn);const cn={};g&2&&(cn.$$scope={dirty:g,ctx:o}),el.$set(cn);const ka={};g&2&&(ka.$$scope={dirty:g,ctx:o}),ol.$set(ka);const Ro={};g&2&&(Ro.$$scope={dirty:g,ctx:o}),rl.$set(Ro);const fn={};g&2&&(fn.$$scope={dirty:g,ctx:o}),nl.$set(fn);const La={};g&2&&(La.$$scope={dirty:g,ctx:o}),tl.$set(La);const gn={};g&2&&(gn.$$scope={dirty:g,ctx:o}),al.$set(gn);const pn={};g&2&&(pn.$$scope={dirty:g,ctx:o}),sl.$set(pn);const $a={};g&2&&($a.$$scope={dirty:g,ctx:o}),il.$set($a);const hn={};g&2&&(hn.$$scope={dirty:g,ctx:o}),ll.$set(hn);const un={};g&2&&(un.$$scope={dirty:g,ctx:o}),dl.$set(un);const Ba={};g&2&&(Ba.$$scope={dirty:g,ctx:o}),ml.$set(Ba);const Po={};g&2&&(Po.$$scope={dirty:g,ctx:o}),cl.$set(Po);const _n={};g&2&&(_n.$$scope={dirty:g,ctx:o}),fl.$set(_n);const ja={};g&2&&(ja.$$scope={dirty:g,ctx:o}),gl.$set(ja);const bn={};g&2&&(bn.$$scope={dirty:g,ctx:o}),pl.$set(bn);const Mn={};g&2&&(Mn.$$scope={dirty:g,ctx:o}),hl.$set(Mn);const Za={};g&2&&(Za.$$scope={dirty:g,ctx:o}),ul.$set(Za);const Fn={};g&2&&(Fn.$$scope={dirty:g,ctx:o}),_l.$set(Fn);const Tn={};g&2&&(Tn.$$scope={dirty:g,ctx:o}),bl.$set(Tn);const Aa={};g&2&&(Aa.$$scope={dirty:g,ctx:o}),Ml.$set(Aa);const Wo={};g&2&&(Wo.$$scope={dirty:g,ctx:o}),Fl.$set(Wo);const Cn={};g&2&&(Cn.$$scope={dirty:g,ctx:o}),Tl.$set(Cn);const Ra={};g&2&&(Ra.$$scope={dirty:g,ctx:o}),Cl.$set(Ra);const yn={};g&2&&(yn.$$scope={dirty:g,ctx:o}),yl.$set(yn);const vn={};g&2&&(vn.$$scope={dirty:g,ctx:o}),vl.$set(vn);const Pa={};g&2&&(Pa.$$scope={dirty:g,ctx:o}),wl.$set(Pa);const wn={};g&2&&(wn.$$scope={dirty:g,ctx:o}),xl.$set(wn);const wv={};g&2&&(wv.$$scope={dirty:g,ctx:o}),kl.$set(wv);const xv={};g&2&&(xv.$$scope={dirty:g,ctx:o}),Ll.$set(xv);const xn={};g&2&&(xn.$$scope={dirty:g,ctx:o}),$l.$set(xn);const Wa={};g&2&&(Wa.$$scope={dirty:g,ctx:o}),Bl.$set(Wa);const So={};g&2&&(So.$$scope={dirty:g,ctx:o}),jl.$set(So);const kn={};g&2&&(kn.$$scope={dirty:g,ctx:o}),Zl.$set(kn);const Sa={};g&2&&(Sa.$$scope={dirty:g,ctx:o}),Al.$set(Sa);const Go={};g&2&&(Go.$$scope={dirty:g,ctx:o}),Rl.$set(Go);const Ln={};g&2&&(Ln.$$scope={dirty:g,ctx:o}),Pl.$set(Ln);const Ga={};g&2&&(Ga.$$scope={dirty:g,ctx:o}),Wl.$set(Ga);const $n={};g&2&&($n.$$scope={dirty:g,ctx:o}),Sl.$set($n);const Bn={};g&2&&(Bn.$$scope={dirty:g,ctx:o}),Gl.$set(Bn);const Va={};g&2&&(Va.$$scope={dirty:g,ctx:o}),Vl.$set(Va);const jn={};g&2&&(jn.$$scope={dirty:g,ctx:o}),Jl.$set(jn);const Zn={};g&2&&(Zn.$$scope={dirty:g,ctx:o}),El.$set(Zn);const Ja={};g&2&&(Ja.$$scope={dirty:g,ctx:o}),Il.$set(Ja);const Vo={};g&2&&(Vo.$$scope={dirty:g,ctx:o}),Ul.$set(Vo);const An={};g&2&&(An.$$scope={dirty:g,ctx:o}),Nl.$set(An);const Ea={};g&2&&(Ea.$$scope={dirty:g,ctx:o}),Xl.$set(Ea);const Jo={};g&2&&(Jo.$$scope={dirty:g,ctx:o}),ql.$set(Jo);const Rn={};g&2&&(Rn.$$scope={dirty:g,ctx:o}),Ql.$set(Rn);const Ia={};g&2&&(Ia.$$scope={dirty:g,ctx:o}),Dl.$set(Ia);const Pn={};g&2&&(Pn.$$scope={dirty:g,ctx:o}),Hl.$set(Pn);const Wn={};g&2&&(Wn.$$scope={dirty:g,ctx:o}),Yl.$set(Wn);const Ua={};g&2&&(Ua.$$scope={dirty:g,ctx:o}),zl.$set(Ua);const Eo={};g&2&&(Eo.$$scope={dirty:g,ctx:o}),Ol.$set(Eo)},i(o){Px||(_(c.$$.fragment,o),_(vd.$$.fragment,o),_(kd.$$.fragment,o),_($d.$$.fragment,o),_(ys.$$.fragment,o),_(jd.$$.fragment,o),_(Zd.$$.fragment,o),_(Ad.$$.fragment,o),_(vs.$$.fragment,o),_(Rd.$$.fragment,o),_(Pd.$$.fragment,o),_(Wd.$$.fragment,o),_(Sd.$$.fragment,o),_(xs.$$.fragment,o),_(Gd.$$.fragment,o),_(Vd.$$.fragment,o),_(Jd.$$.fragment,o),_(Ed.$$.fragment,o),_(Ls.$$.fragment,o),_($s.$$.fragment,o),_(Id.$$.fragment,o),_(Ud.$$.fragment,o),_(Nd.$$.fragment,o),_(Xd.$$.fragment,o),_(js.$$.fragment,o),_(Zs.$$.fragment,o),_(qd.$$.fragment,o),_(Qd.$$.fragment,o),_(Dd.$$.fragment,o),_(Hd.$$.fragment,o),_(Rs.$$.fragment,o),_(Ps.$$.fragment,o),_(Yd.$$.fragment,o),_(zd.$$.fragment,o),_(Kd.$$.fragment,o),_(em.$$.fragment,o),_(om.$$.fragment,o),_(Ss.$$.fragment,o),_(rm.$$.fragment,o),_(Gs.$$.fragment,o),_(nm.$$.fragment,o),_(tm.$$.fragment,o),_(am.$$.fragment,o),_(Vs.$$.fragment,o),_(sm.$$.fragment,o),_(Js.$$.fragment,o),_(im.$$.fragment,o),_(lm.$$.fragment,o),_(dm.$$.fragment,o),_(Es.$$.fragment,o),_(mm.$$.fragment,o),_(Is.$$.fragment,o),_(cm.$$.fragment,o),_(gm.$$.fragment,o),_(pm.$$.fragment,o),_(hm.$$.fragment,o),_(Us.$$.fragment,o),_(um.$$.fragment,o),_(Ns.$$.fragment,o),_(_m.$$.fragment,o),_(bm.$$.fragment,o),_(Mm.$$.fragment,o),_(Xs.$$.fragment,o),_(Fm.$$.fragment,o),_(qs.$$.fragment,o),_(Tm.$$.fragment,o),_(Cm.$$.fragment,o),_(ym.$$.fragment,o),_(Qs.$$.fragment,o),_(vm.$$.fragment,o),_(Ds.$$.fragment,o),_(wm.$$.fragment,o),_(km.$$.fragment,o),_(Lm.$$.fragment,o),_($m.$$.fragment,o),_(Hs.$$.fragment,o),_(Bm.$$.fragment,o),_(Ys.$$.fragment,o),_(jm.$$.fragment,o),_(Zm.$$.fragment,o),_(Am.$$.fragment,o),_(zs.$$.fragment,o),_(Rm.$$.fragment,o),_(Os.$$.fragment,o),_(Pm.$$.fragment,o),_(Wm.$$.fragment,o),_(Sm.$$.fragment,o),_(Ks.$$.fragment,o),_(Gm.$$.fragment,o),_(ei.$$.fragment,o),_(Vm.$$.fragment,o),_(Jm.$$.fragment,o),_(Em.$$.fragment,o),_(oi.$$.fragment,o),_(Im.$$.fragment,o),_(ri.$$.fragment,o),_(Um.$$.fragment,o),_(Nm.$$.fragment,o),_(Xm.$$.fragment,o),_(ni.$$.fragment,o),_(qm.$$.fragment,o),_(ti.$$.fragment,o),_(Qm.$$.fragment,o),_(Dm.$$.fragment,o),_(Hm.$$.fragment,o),_(ai.$$.fragment,o),_(Ym.$$.fragment,o),_(si.$$.fragment,o),_(zm.$$.fragment,o),_(Km.$$.fragment,o),_(ec.$$.fragment,o),_(rc.$$.fragment,o),_(nc.$$.fragment,o),_(tc.$$.fragment,o),_(ac.$$.fragment,o),_(ii.$$.fragment,o),_(sc.$$.fragment,o),_(li.$$.fragment,o),_(ic.$$.fragment,o),_(lc.$$.fragment,o),_(dc.$$.fragment,o),_(di.$$.fragment,o),_(mc.$$.fragment,o),_(mi.$$.fragment,o),_(cc.$$.fragment,o),_(fc.$$.fragment,o),_(gc.$$.fragment,o),_(ci.$$.fragment,o),_(pc.$$.fragment,o),_(fi.$$.fragment,o),_(hc.$$.fragment,o),_(uc.$$.fragment,o),_(_c.$$.fragment,o),_(gi.$$.fragment,o),_(bc.$$.fragment,o),_(pi.$$.fragment,o),_(Mc.$$.fragment,o),_(Fc.$$.fragment,o),_(Tc.$$.fragment,o),_(hi.$$.fragment,o),_(Cc.$$.fragment,o),_(ui.$$.fragment,o),_(yc.$$.fragment,o),_(vc.$$.fragment,o),_(wc.$$.fragment,o),_(_i.$$.fragment,o),_(xc.$$.fragment,o),_(bi.$$.fragment,o),_(kc.$$.fragment,o),_(Lc.$$.fragment,o),_($c.$$.fragment,o),_(Mi.$$.fragment,o),_(Bc.$$.fragment,o),_(Fi.$$.fragment,o),_(jc.$$.fragment,o),_(Zc.$$.fragment,o),_(Ac.$$.fragment,o),_(Ti.$$.fragment,o),_(Rc.$$.fragment,o),_(Ci.$$.fragment,o),_(Pc.$$.fragment,o),_(Wc.$$.fragment,o),_(Sc.$$.fragment,o),_(yi.$$.fragment,o),_(Gc.$$.fragment,o),_(vi.$$.fragment,o),_(Vc.$$.fragment,o),_(Jc.$$.fragment,o),_(Ec.$$.fragment,o),_(wi.$$.fragment,o),_(Ic.$$.fragment,o),_(xi.$$.fragment,o),_(Uc.$$.fragment,o),_(Nc.$$.fragment,o),_(Xc.$$.fragment,o),_(ki.$$.fragment,o),_(qc.$$.fragment,o),_(Li.$$.fragment,o),_(Qc.$$.fragment,o),_(Dc.$$.fragment,o),_(Hc.$$.fragment,o),_($i.$$.fragment,o),_(Yc.$$.fragment,o),_(Bi.$$.fragment,o),_(zc.$$.fragment,o),_(Oc.$$.fragment,o),_(Kc.$$.fragment,o),_(ji.$$.fragment,o),_(ef.$$.fragment,o),_(Zi.$$.fragment,o),_(of.$$.fragment,o),_(rf.$$.fragment,o),_(nf.$$.fragment,o),_(Ai.$$.fragment,o),_(tf.$$.fragment,o),_(Ri.$$.fragment,o),_(af.$$.fragment,o),_(sf.$$.fragment,o),_(lf.$$.fragment,o),_(Pi.$$.fragment,o),_(df.$$.fragment,o),_(Wi.$$.fragment,o),_(mf.$$.fragment,o),_(cf.$$.fragment,o),_(ff.$$.fragment,o),_(Si.$$.fragment,o),_(gf.$$.fragment,o),_(Gi.$$.fragment,o),_(pf.$$.fragment,o),_(hf.$$.fragment,o),_(uf.$$.fragment,o),_(Vi.$$.fragment,o),_(_f.$$.fragment,o),_(Ji.$$.fragment,o),_(bf.$$.fragment,o),_(Mf.$$.fragment,o),_(Ff.$$.fragment,o),_(Ei.$$.fragment,o),_(Tf.$$.fragment,o),_(Ii.$$.fragment,o),_(Cf.$$.fragment,o),_(vf.$$.fragment,o),_(wf.$$.fragment,o),_(kf.$$.fragment,o),_(Lf.$$.fragment,o),_(Bf.$$.fragment,o),_(jf.$$.fragment,o),_(Zf.$$.fragment,o),_(Ui.$$.fragment,o),_(Af.$$.fragment,o),_(Ni.$$.fragment,o),_(Rf.$$.fragment,o),_(Pf.$$.fragment,o),_(Wf.$$.fragment,o),_(Xi.$$.fragment,o),_(Sf.$$.fragment,o),_(qi.$$.fragment,o),_(Gf.$$.fragment,o),_(Vf.$$.fragment,o),_(Jf.$$.fragment,o),_(Qi.$$.fragment,o),_(Ef.$$.fragment,o),_(Di.$$.fragment,o),_(If.$$.fragment,o),_(Uf.$$.fragment,o),_(Nf.$$.fragment,o),_(Hi.$$.fragment,o),_(Xf.$$.fragment,o),_(Yi.$$.fragment,o),_(qf.$$.fragment,o),_(Qf.$$.fragment,o),_(Df.$$.fragment,o),_(zi.$$.fragment,o),_(Hf.$$.fragment,o),_(Oi.$$.fragment,o),_(Yf.$$.fragment,o),_(zf.$$.fragment,o),_(Of.$$.fragment,o),_(Ki.$$.fragment,o),_(Kf.$$.fragment,o),_(el.$$.fragment,o),_(eg.$$.fragment,o),_(og.$$.fragment,o),_(rg.$$.fragment,o),_(ol.$$.fragment,o),_(ng.$$.fragment,o),_(rl.$$.fragment,o),_(tg.$$.fragment,o),_(ag.$$.fragment,o),_(sg.$$.fragment,o),_(nl.$$.fragment,o),_(ig.$$.fragment,o),_(tl.$$.fragment,o),_(lg.$$.fragment,o),_(dg.$$.fragment,o),_(mg.$$.fragment,o),_(al.$$.fragment,o),_(cg.$$.fragment,o),_(sl.$$.fragment,o),_(fg.$$.fragment,o),_(pg.$$.fragment,o),_(hg.$$.fragment,o),_(ug.$$.fragment,o),_(_g.$$.fragment,o),_(il.$$.fragment,o),_(bg.$$.fragment,o),_(ll.$$.fragment,o),_(Mg.$$.fragment,o),_(Fg.$$.fragment,o),_(Tg.$$.fragment,o),_(dl.$$.fragment,o),_(Cg.$$.fragment,o),_(ml.$$.fragment,o),_(yg.$$.fragment,o),_(vg.$$.fragment,o),_(wg.$$.fragment,o),_(cl.$$.fragment,o),_(xg.$$.fragment,o),_(fl.$$.fragment,o),_(kg.$$.fragment,o),_(Lg.$$.fragment,o),_($g.$$.fragment,o),_(gl.$$.fragment,o),_(Bg.$$.fragment,o),_(pl.$$.fragment,o),_(jg.$$.fragment,o),_(Zg.$$.fragment,o),_(Ag.$$.fragment,o),_(hl.$$.fragment,o),_(Rg.$$.fragment,o),_(ul.$$.fragment,o),_(Pg.$$.fragment,o),_(Wg.$$.fragment,o),_(Sg.$$.fragment,o),_(_l.$$.fragment,o),_(Gg.$$.fragment,o),_(bl.$$.fragment,o),_(Vg.$$.fragment,o),_(Jg.$$.fragment,o),_(Eg.$$.fragment,o),_(Ml.$$.fragment,o),_(Ig.$$.fragment,o),_(Fl.$$.fragment,o),_(Ug.$$.fragment,o),_(Xg.$$.fragment,o),_(qg.$$.fragment,o),_(Qg.$$.fragment,o),_(Tl.$$.fragment,o),_(Dg.$$.fragment,o),_(Cl.$$.fragment,o),_(Hg.$$.fragment,o),_(Yg.$$.fragment,o),_(zg.$$.fragment,o),_(yl.$$.fragment,o),_(Og.$$.fragment,o),_(vl.$$.fragment,o),_(Kg.$$.fragment,o),_(ep.$$.fragment,o),_(op.$$.fragment,o),_(wl.$$.fragment,o),_(rp.$$.fragment,o),_(xl.$$.fragment,o),_(np.$$.fragment,o),_(tp.$$.fragment,o),_(ap.$$.fragment,o),_(kl.$$.fragment,o),_(sp.$$.fragment,o),_(Ll.$$.fragment,o),_(ip.$$.fragment,o),_(lp.$$.fragment,o),_(dp.$$.fragment,o),_($l.$$.fragment,o),_(mp.$$.fragment,o),_(Bl.$$.fragment,o),_(cp.$$.fragment,o),_(fp.$$.fragment,o),_(gp.$$.fragment,o),_(jl.$$.fragment,o),_(pp.$$.fragment,o),_(Zl.$$.fragment,o),_(hp.$$.fragment,o),_(up.$$.fragment,o),_(_p.$$.fragment,o),_(Al.$$.fragment,o),_(bp.$$.fragment,o),_(Rl.$$.fragment,o),_(Mp.$$.fragment,o),_(Fp.$$.fragment,o),_(Tp.$$.fragment,o),_(Pl.$$.fragment,o),_(Cp.$$.fragment,o),_(Wl.$$.fragment,o),_(yp.$$.fragment,o),_(wp.$$.fragment,o),_(xp.$$.fragment,o),_(Lp.$$.fragment,o),_($p.$$.fragment,o),_(jp.$$.fragment,o),_(Zp.$$.fragment,o),_(Ap.$$.fragment,o),_(Sl.$$.fragment,o),_(Rp.$$.fragment,o),_(Gl.$$.fragment,o),_(Pp.$$.fragment,o),_(Wp.$$.fragment,o),_(Sp.$$.fragment,o),_(Vl.$$.fragment,o),_(Gp.$$.fragment,o),_(Jl.$$.fragment,o),_(Vp.$$.fragment,o),_(Jp.$$.fragment,o),_(Ep.$$.fragment,o),_(El.$$.fragment,o),_(Ip.$$.fragment,o),_(Il.$$.fragment,o),_(Up.$$.fragment,o),_(Np.$$.fragment,o),_(Xp.$$.fragment,o),_(Ul.$$.fragment,o),_(qp.$$.fragment,o),_(Nl.$$.fragment,o),_(Qp.$$.fragment,o),_(Dp.$$.fragment,o),_(Hp.$$.fragment,o),_(Xl.$$.fragment,o),_(Yp.$$.fragment,o),_(ql.$$.fragment,o),_(zp.$$.fragment,o),_(Op.$$.fragment,o),_(Kp.$$.fragment,o),_(Ql.$$.fragment,o),_(eh.$$.fragment,o),_(Dl.$$.fragment,o),_(oh.$$.fragment,o),_(rh.$$.fragment,o),_(nh.$$.fragment,o),_(Hl.$$.fragment,o),_(th.$$.fragment,o),_(Yl.$$.fragment,o),_(ah.$$.fragment,o),_(sh.$$.fragment,o),_(ih.$$.fragment,o),_(zl.$$.fragment,o),_(lh.$$.fragment,o),_(Ol.$$.fragment,o),Px=!0)},o(o){b(c.$$.fragment,o),b(vd.$$.fragment,o),b(kd.$$.fragment,o),b($d.$$.fragment,o),b(ys.$$.fragment,o),b(jd.$$.fragment,o),b(Zd.$$.fragment,o),b(Ad.$$.fragment,o),b(vs.$$.fragment,o),b(Rd.$$.fragment,o),b(Pd.$$.fragment,o),b(Wd.$$.fragment,o),b(Sd.$$.fragment,o),b(xs.$$.fragment,o),b(Gd.$$.fragment,o),b(Vd.$$.fragment,o),b(Jd.$$.fragment,o),b(Ed.$$.fragment,o),b(Ls.$$.fragment,o),b($s.$$.fragment,o),b(Id.$$.fragment,o),b(Ud.$$.fragment,o),b(Nd.$$.fragment,o),b(Xd.$$.fragment,o),b(js.$$.fragment,o),b(Zs.$$.fragment,o),b(qd.$$.fragment,o),b(Qd.$$.fragment,o),b(Dd.$$.fragment,o),b(Hd.$$.fragment,o),b(Rs.$$.fragment,o),b(Ps.$$.fragment,o),b(Yd.$$.fragment,o),b(zd.$$.fragment,o),b(Kd.$$.fragment,o),b(em.$$.fragment,o),b(om.$$.fragment,o),b(Ss.$$.fragment,o),b(rm.$$.fragment,o),b(Gs.$$.fragment,o),b(nm.$$.fragment,o),b(tm.$$.fragment,o),b(am.$$.fragment,o),b(Vs.$$.fragment,o),b(sm.$$.fragment,o),b(Js.$$.fragment,o),b(im.$$.fragment,o),b(lm.$$.fragment,o),b(dm.$$.fragment,o),b(Es.$$.fragment,o),b(mm.$$.fragment,o),b(Is.$$.fragment,o),b(cm.$$.fragment,o),b(gm.$$.fragment,o),b(pm.$$.fragment,o),b(hm.$$.fragment,o),b(Us.$$.fragment,o),b(um.$$.fragment,o),b(Ns.$$.fragment,o),b(_m.$$.fragment,o),b(bm.$$.fragment,o),b(Mm.$$.fragment,o),b(Xs.$$.fragment,o),b(Fm.$$.fragment,o),b(qs.$$.fragment,o),b(Tm.$$.fragment,o),b(Cm.$$.fragment,o),b(ym.$$.fragment,o),b(Qs.$$.fragment,o),b(vm.$$.fragment,o),b(Ds.$$.fragment,o),b(wm.$$.fragment,o),b(km.$$.fragment,o),b(Lm.$$.fragment,o),b($m.$$.fragment,o),b(Hs.$$.fragment,o),b(Bm.$$.fragment,o),b(Ys.$$.fragment,o),b(jm.$$.fragment,o),b(Zm.$$.fragment,o),b(Am.$$.fragment,o),b(zs.$$.fragment,o),b(Rm.$$.fragment,o),b(Os.$$.fragment,o),b(Pm.$$.fragment,o),b(Wm.$$.fragment,o),b(Sm.$$.fragment,o),b(Ks.$$.fragment,o),b(Gm.$$.fragment,o),b(ei.$$.fragment,o),b(Vm.$$.fragment,o),b(Jm.$$.fragment,o),b(Em.$$.fragment,o),b(oi.$$.fragment,o),b(Im.$$.fragment,o),b(ri.$$.fragment,o),b(Um.$$.fragment,o),b(Nm.$$.fragment,o),b(Xm.$$.fragment,o),b(ni.$$.fragment,o),b(qm.$$.fragment,o),b(ti.$$.fragment,o),b(Qm.$$.fragment,o),b(Dm.$$.fragment,o),b(Hm.$$.fragment,o),b(ai.$$.fragment,o),b(Ym.$$.fragment,o),b(si.$$.fragment,o),b(zm.$$.fragment,o),b(Km.$$.fragment,o),b(ec.$$.fragment,o),b(rc.$$.fragment,o),b(nc.$$.fragment,o),b(tc.$$.fragment,o),b(ac.$$.fragment,o),b(ii.$$.fragment,o),b(sc.$$.fragment,o),b(li.$$.fragment,o),b(ic.$$.fragment,o),b(lc.$$.fragment,o),b(dc.$$.fragment,o),b(di.$$.fragment,o),b(mc.$$.fragment,o),b(mi.$$.fragment,o),b(cc.$$.fragment,o),b(fc.$$.fragment,o),b(gc.$$.fragment,o),b(ci.$$.fragment,o),b(pc.$$.fragment,o),b(fi.$$.fragment,o),b(hc.$$.fragment,o),b(uc.$$.fragment,o),b(_c.$$.fragment,o),b(gi.$$.fragment,o),b(bc.$$.fragment,o),b(pi.$$.fragment,o),b(Mc.$$.fragment,o),b(Fc.$$.fragment,o),b(Tc.$$.fragment,o),b(hi.$$.fragment,o),b(Cc.$$.fragment,o),b(ui.$$.fragment,o),b(yc.$$.fragment,o),b(vc.$$.fragment,o),b(wc.$$.fragment,o),b(_i.$$.fragment,o),b(xc.$$.fragment,o),b(bi.$$.fragment,o),b(kc.$$.fragment,o),b(Lc.$$.fragment,o),b($c.$$.fragment,o),b(Mi.$$.fragment,o),b(Bc.$$.fragment,o),b(Fi.$$.fragment,o),b(jc.$$.fragment,o),b(Zc.$$.fragment,o),b(Ac.$$.fragment,o),b(Ti.$$.fragment,o),b(Rc.$$.fragment,o),b(Ci.$$.fragment,o),b(Pc.$$.fragment,o),b(Wc.$$.fragment,o),b(Sc.$$.fragment,o),b(yi.$$.fragment,o),b(Gc.$$.fragment,o),b(vi.$$.fragment,o),b(Vc.$$.fragment,o),b(Jc.$$.fragment,o),b(Ec.$$.fragment,o),b(wi.$$.fragment,o),b(Ic.$$.fragment,o),b(xi.$$.fragment,o),b(Uc.$$.fragment,o),b(Nc.$$.fragment,o),b(Xc.$$.fragment,o),b(ki.$$.fragment,o),b(qc.$$.fragment,o),b(Li.$$.fragment,o),b(Qc.$$.fragment,o),b(Dc.$$.fragment,o),b(Hc.$$.fragment,o),b($i.$$.fragment,o),b(Yc.$$.fragment,o),b(Bi.$$.fragment,o),b(zc.$$.fragment,o),b(Oc.$$.fragment,o),b(Kc.$$.fragment,o),b(ji.$$.fragment,o),b(ef.$$.fragment,o),b(Zi.$$.fragment,o),b(of.$$.fragment,o),b(rf.$$.fragment,o),b(nf.$$.fragment,o),b(Ai.$$.fragment,o),b(tf.$$.fragment,o),b(Ri.$$.fragment,o),b(af.$$.fragment,o),b(sf.$$.fragment,o),b(lf.$$.fragment,o),b(Pi.$$.fragment,o),b(df.$$.fragment,o),b(Wi.$$.fragment,o),b(mf.$$.fragment,o),b(cf.$$.fragment,o),b(ff.$$.fragment,o),b(Si.$$.fragment,o),b(gf.$$.fragment,o),b(Gi.$$.fragment,o),b(pf.$$.fragment,o),b(hf.$$.fragment,o),b(uf.$$.fragment,o),b(Vi.$$.fragment,o),b(_f.$$.fragment,o),b(Ji.$$.fragment,o),b(bf.$$.fragment,o),b(Mf.$$.fragment,o),b(Ff.$$.fragment,o),b(Ei.$$.fragment,o),b(Tf.$$.fragment,o),b(Ii.$$.fragment,o),b(Cf.$$.fragment,o),b(vf.$$.fragment,o),b(wf.$$.fragment,o),b(kf.$$.fragment,o),b(Lf.$$.fragment,o),b(Bf.$$.fragment,o),b(jf.$$.fragment,o),b(Zf.$$.fragment,o),b(Ui.$$.fragment,o),b(Af.$$.fragment,o),b(Ni.$$.fragment,o),b(Rf.$$.fragment,o),b(Pf.$$.fragment,o),b(Wf.$$.fragment,o),b(Xi.$$.fragment,o),b(Sf.$$.fragment,o),b(qi.$$.fragment,o),b(Gf.$$.fragment,o),b(Vf.$$.fragment,o),b(Jf.$$.fragment,o),b(Qi.$$.fragment,o),b(Ef.$$.fragment,o),b(Di.$$.fragment,o),b(If.$$.fragment,o),b(Uf.$$.fragment,o),b(Nf.$$.fragment,o),b(Hi.$$.fragment,o),b(Xf.$$.fragment,o),b(Yi.$$.fragment,o),b(qf.$$.fragment,o),b(Qf.$$.fragment,o),b(Df.$$.fragment,o),b(zi.$$.fragment,o),b(Hf.$$.fragment,o),b(Oi.$$.fragment,o),b(Yf.$$.fragment,o),b(zf.$$.fragment,o),b(Of.$$.fragment,o),b(Ki.$$.fragment,o),b(Kf.$$.fragment,o),b(el.$$.fragment,o),b(eg.$$.fragment,o),b(og.$$.fragment,o),b(rg.$$.fragment,o),b(ol.$$.fragment,o),b(ng.$$.fragment,o),b(rl.$$.fragment,o),b(tg.$$.fragment,o),b(ag.$$.fragment,o),b(sg.$$.fragment,o),b(nl.$$.fragment,o),b(ig.$$.fragment,o),b(tl.$$.fragment,o),b(lg.$$.fragment,o),b(dg.$$.fragment,o),b(mg.$$.fragment,o),b(al.$$.fragment,o),b(cg.$$.fragment,o),b(sl.$$.fragment,o),b(fg.$$.fragment,o),b(pg.$$.fragment,o),b(hg.$$.fragment,o),b(ug.$$.fragment,o),b(_g.$$.fragment,o),b(il.$$.fragment,o),b(bg.$$.fragment,o),b(ll.$$.fragment,o),b(Mg.$$.fragment,o),b(Fg.$$.fragment,o),b(Tg.$$.fragment,o),b(dl.$$.fragment,o),b(Cg.$$.fragment,o),b(ml.$$.fragment,o),b(yg.$$.fragment,o),b(vg.$$.fragment,o),b(wg.$$.fragment,o),b(cl.$$.fragment,o),b(xg.$$.fragment,o),b(fl.$$.fragment,o),b(kg.$$.fragment,o),b(Lg.$$.fragment,o),b($g.$$.fragment,o),b(gl.$$.fragment,o),b(Bg.$$.fragment,o),b(pl.$$.fragment,o),b(jg.$$.fragment,o),b(Zg.$$.fragment,o),b(Ag.$$.fragment,o),b(hl.$$.fragment,o),b(Rg.$$.fragment,o),b(ul.$$.fragment,o),b(Pg.$$.fragment,o),b(Wg.$$.fragment,o),b(Sg.$$.fragment,o),b(_l.$$.fragment,o),b(Gg.$$.fragment,o),b(bl.$$.fragment,o),b(Vg.$$.fragment,o),b(Jg.$$.fragment,o),b(Eg.$$.fragment,o),b(Ml.$$.fragment,o),b(Ig.$$.fragment,o),b(Fl.$$.fragment,o),b(Ug.$$.fragment,o),b(Xg.$$.fragment,o),b(qg.$$.fragment,o),b(Qg.$$.fragment,o),b(Tl.$$.fragment,o),b(Dg.$$.fragment,o),b(Cl.$$.fragment,o),b(Hg.$$.fragment,o),b(Yg.$$.fragment,o),b(zg.$$.fragment,o),b(yl.$$.fragment,o),b(Og.$$.fragment,o),b(vl.$$.fragment,o),b(Kg.$$.fragment,o),b(ep.$$.fragment,o),b(op.$$.fragment,o),b(wl.$$.fragment,o),b(rp.$$.fragment,o),b(xl.$$.fragment,o),b(np.$$.fragment,o),b(tp.$$.fragment,o),b(ap.$$.fragment,o),b(kl.$$.fragment,o),b(sp.$$.fragment,o),b(Ll.$$.fragment,o),b(ip.$$.fragment,o),b(lp.$$.fragment,o),b(dp.$$.fragment,o),b($l.$$.fragment,o),b(mp.$$.fragment,o),b(Bl.$$.fragment,o),b(cp.$$.fragment,o),b(fp.$$.fragment,o),b(gp.$$.fragment,o),b(jl.$$.fragment,o),b(pp.$$.fragment,o),b(Zl.$$.fragment,o),b(hp.$$.fragment,o),b(up.$$.fragment,o),b(_p.$$.fragment,o),b(Al.$$.fragment,o),b(bp.$$.fragment,o),b(Rl.$$.fragment,o),b(Mp.$$.fragment,o),b(Fp.$$.fragment,o),b(Tp.$$.fragment,o),b(Pl.$$.fragment,o),b(Cp.$$.fragment,o),b(Wl.$$.fragment,o),b(yp.$$.fragment,o),b(wp.$$.fragment,o),b(xp.$$.fragment,o),b(Lp.$$.fragment,o),b($p.$$.fragment,o),b(jp.$$.fragment,o),b(Zp.$$.fragment,o),b(Ap.$$.fragment,o),b(Sl.$$.fragment,o),b(Rp.$$.fragment,o),b(Gl.$$.fragment,o),b(Pp.$$.fragment,o),b(Wp.$$.fragment,o),b(Sp.$$.fragment,o),b(Vl.$$.fragment,o),b(Gp.$$.fragment,o),b(Jl.$$.fragment,o),b(Vp.$$.fragment,o),b(Jp.$$.fragment,o),b(Ep.$$.fragment,o),b(El.$$.fragment,o),b(Ip.$$.fragment,o),b(Il.$$.fragment,o),b(Up.$$.fragment,o),b(Np.$$.fragment,o),b(Xp.$$.fragment,o),b(Ul.$$.fragment,o),b(qp.$$.fragment,o),b(Nl.$$.fragment,o),b(Qp.$$.fragment,o),b(Dp.$$.fragment,o),b(Hp.$$.fragment,o),b(Xl.$$.fragment,o),b(Yp.$$.fragment,o),b(ql.$$.fragment,o),b(zp.$$.fragment,o),b(Op.$$.fragment,o),b(Kp.$$.fragment,o),b(Ql.$$.fragment,o),b(eh.$$.fragment,o),b(Dl.$$.fragment,o),b(oh.$$.fragment,o),b(rh.$$.fragment,o),b(nh.$$.fragment,o),b(Hl.$$.fragment,o),b(th.$$.fragment,o),b(Yl.$$.fragment,o),b(ah.$$.fragment,o),b(sh.$$.fragment,o),b(ih.$$.fragment,o),b(zl.$$.fragment,o),b(lh.$$.fragment,o),b(Ol.$$.fragment,o),Px=!1},d(o){o&&(d(C),d(m),d(s),d(e),d(F),d(kv),d(yd),d(Lv),d($v),d(wd),d(Bv),d(xd),d(jv),d(Zv),d(Ld),d(Av),d(Rv),d(Bd),d(Pv),d(Wv),d(Sv),d(me),d(Gv),d(Vv),d(ce),d(Jv),d(Ev),d(fe),d(Iv),d(Uv),d(ge),d(Nv),d(Xv),d(pe),d(qv),d(Qv),d(Od),d(Dv),d(Hv),d(he),d(Yv),d(zv),d(ue),d(Ov),d(Kv),d(_e),d(e2),d(o2),d(fm),d(r2),d(n2),d(be),d(t2),d(a2),d(Me),d(s2),d(i2),d(Fe),d(l2),d(d2),d(xm),d(m2),d(c2),d(Te),d(f2),d(g2),d(Ce),d(p2),d(h2),d(ye),d(u2),d(_2),d(ve),d(b2),d(M2),d(we),d(F2),d(T2),d(xe),d(C2),d(y2),d(Om),d(v2),d(w2),d(oc),d(x2),d(k2),d(ke),d(L2),d($2),d(Le),d(B2),d(j2),d($e),d(Z2),d(A2),d(Be),d(R2),d(P2),d(je),d(W2),d(S2),d(Ze),d(G2),d(V2),d(Ae),d(J2),d(E2),d(Re),d(I2),d(U2),d(Pe),d(N2),d(X2),d(We),d(q2),d(Q2),d(Se),d(D2),d(H2),d(Ge),d(Y2),d(z2),d(Ve),d(O2),d(K2),d(Je),d(ew),d(ow),d(Ee),d(rw),d(nw),d(Ie),d(tw),d(aw),d(Ue),d(sw),d(iw),d(Ne),d(lw),d(dw),d(yf),d(mw),d(cw),d(xf),d(fw),d(gw),d($f),d(pw),d(hw),d(Xe),d(uw),d(_w),d(qe),d(bw),d(Mw),d(Qe),d(Fw),d(Tw),d(De),d(Cw),d(yw),d(He),d(vw),d(ww),d(Ye),d(xw),d(kw),d(ze),d(Lw),d($w),d(Oe),d(Bw),d(jw),d(Ke),d(Zw),d(Aw),d(gg),d(Rw),d(Pw),d(eo),d(Ww),d(Sw),d(oo),d(Gw),d(Vw),d(ro),d(Jw),d(Ew),d(no),d(Iw),d(Uw),d(to),d(Nw),d(Xw),d(ao),d(qw),d(Qw),d(so),d(Dw),d(Hw),d(Ng),d(Yw),d(zw),d(io),d(Ow),d(Kw),d(lo),d(ex),d(ox),d(mo),d(rx),d(nx),d(co),d(tx),d(ax),d(fo),d(sx),d(ix),d(go),d(lx),d(dx),d(po),d(mx),d(cx),d(ho),d(fx),d(gx),d(vp),d(px),d(hx),d(kp),d(ux),d(_x),d(Bp),d(bx),d(Mx),d(uo),d(Fx),d(Tx),d(_o),d(Cx),d(yx),d(bo),d(vx),d(wx),d(Mo),d(xx),d(kx),d(Fo),d(Lx),d($x),d(To),d(Bx),d(jx),d(Co),d(Zx),d(Ax),d(yo),d(Rx),d(Cv)),d(n),M(c,o),M(vd,o),M(kd,o),M($d,o),M(ys,o),M(jd,o),M(Zd),M(Ad),M(vs),M(Rd),M(Pd,o),M(Wd),M(Sd),M(xs),M(Gd),M(Vd,o),M(Jd),M(Ed),M(Ls),M($s),M(Id),M(Ud,o),M(Nd),M(Xd),M(js),M(Zs),M(qd),M(Qd,o),M(Dd),M(Hd),M(Rs),M(Ps),M(Yd),M(zd,o),M(Kd,o),M(em),M(om),M(Ss),M(rm),M(Gs),M(nm,o),M(tm),M(am),M(Vs),M(sm),M(Js),M(im,o),M(lm),M(dm),M(Es),M(mm),M(Is),M(cm,o),M(gm,o),M(pm),M(hm),M(Us),M(um),M(Ns),M(_m,o),M(bm),M(Mm),M(Xs),M(Fm),M(qs),M(Tm,o),M(Cm),M(ym),M(Qs),M(vm),M(Ds),M(wm,o),M(km,o),M(Lm),M($m),M(Hs),M(Bm),M(Ys),M(jm,o),M(Zm),M(Am),M(zs),M(Rm),M(Os),M(Pm,o),M(Wm),M(Sm),M(Ks),M(Gm),M(ei),M(Vm,o),M(Jm),M(Em),M(oi),M(Im),M(ri),M(Um,o),M(Nm),M(Xm),M(ni),M(qm),M(ti),M(Qm,o),M(Dm),M(Hm),M(ai),M(Ym),M(si),M(zm,o),M(Km),M(ec,o),M(rc),M(nc,o),M(tc),M(ac),M(ii),M(sc),M(li),M(ic,o),M(lc),M(dc),M(di),M(mc),M(mi),M(cc,o),M(fc),M(gc),M(ci),M(pc),M(fi),M(hc,o),M(uc),M(_c),M(gi),M(bc),M(pi),M(Mc,o),M(Fc),M(Tc),M(hi),M(Cc),M(ui),M(yc,o),M(vc),M(wc),M(_i),M(xc),M(bi),M(kc,o),M(Lc),M($c),M(Mi),M(Bc),M(Fi),M(jc,o),M(Zc),M(Ac),M(Ti),M(Rc),M(Ci),M(Pc,o),M(Wc),M(Sc),M(yi),M(Gc),M(vi),M(Vc,o),M(Jc),M(Ec),M(wi),M(Ic),M(xi),M(Uc,o),M(Nc),M(Xc),M(ki),M(qc),M(Li),M(Qc,o),M(Dc),M(Hc),M($i),M(Yc),M(Bi),M(zc,o),M(Oc),M(Kc),M(ji),M(ef),M(Zi),M(of,o),M(rf),M(nf),M(Ai),M(tf),M(Ri),M(af,o),M(sf),M(lf),M(Pi),M(df),M(Wi),M(mf,o),M(cf),M(ff),M(Si),M(gf),M(Gi),M(pf,o),M(hf),M(uf),M(Vi),M(_f),M(Ji),M(bf,o),M(Mf),M(Ff),M(Ei),M(Tf),M(Ii),M(Cf,o),M(vf),M(wf,o),M(kf),M(Lf,o),M(Bf,o),M(jf),M(Zf),M(Ui),M(Af),M(Ni),M(Rf,o),M(Pf),M(Wf),M(Xi),M(Sf),M(qi),M(Gf,o),M(Vf),M(Jf),M(Qi),M(Ef),M(Di),M(If,o),M(Uf),M(Nf),M(Hi),M(Xf),M(Yi),M(qf,o),M(Qf),M(Df),M(zi),M(Hf),M(Oi),M(Yf,o),M(zf),M(Of),M(Ki),M(Kf),M(el),M(eg,o),M(og),M(rg),M(ol),M(ng),M(rl),M(tg,o),M(ag),M(sg),M(nl),M(ig),M(tl),M(lg,o),M(dg),M(mg),M(al),M(cg),M(sl),M(fg,o),M(pg),M(hg,o),M(ug),M(_g),M(il),M(bg),M(ll),M(Mg,o),M(Fg),M(Tg),M(dl),M(Cg),M(ml),M(yg,o),M(vg),M(wg),M(cl),M(xg),M(fl),M(kg,o),M(Lg),M($g),M(gl),M(Bg),M(pl),M(jg,o),M(Zg),M(Ag),M(hl),M(Rg),M(ul),M(Pg,o),M(Wg),M(Sg),M(_l),M(Gg),M(bl),M(Vg,o),M(Jg),M(Eg),M(Ml),M(Ig),M(Fl),M(Ug,o),M(Xg,o),M(qg),M(Qg),M(Tl),M(Dg),M(Cl),M(Hg,o),M(Yg),M(zg),M(yl),M(Og),M(vl),M(Kg,o),M(ep),M(op),M(wl),M(rp),M(xl),M(np,o),M(tp),M(ap),M(kl),M(sp),M(Ll),M(ip,o),M(lp),M(dp),M($l),M(mp),M(Bl),M(cp,o),M(fp),M(gp),M(jl),M(pp),M(Zl),M(hp,o),M(up),M(_p),M(Al),M(bp),M(Rl),M(Mp,o),M(Fp),M(Tp),M(Pl),M(Cp),M(Wl),M(yp,o),M(wp),M(xp,o),M(Lp),M($p,o),M(jp,o),M(Zp),M(Ap),M(Sl),M(Rp),M(Gl),M(Pp,o),M(Wp),M(Sp),M(Vl),M(Gp),M(Jl),M(Vp,o),M(Jp),M(Ep),M(El),M(Ip),M(Il),M(Up,o),M(Np),M(Xp),M(Ul),M(qp),M(Nl),M(Qp,o),M(Dp),M(Hp),M(Xl),M(Yp),M(ql),M(zp,o),M(Op),M(Kp),M(Ql),M(eh),M(Dl),M(oh,o),M(rh),M(nh),M(Hl),M(th),M(Yl),M(ah,o),M(sh),M(ih),M(zl),M(lh),M(Ol)}}}const E3='{"title":"Auto Classes","local":"auto-classes","sections":[{"title":"Extending the Auto Classes","local":"extending-the-auto-classes","sections":[],"depth":2},{"title":"AutoConfig","local":"transformers.AutoConfig","sections":[],"depth":2},{"title":"AutoTokenizer","local":"transformers.AutoTokenizer","sections":[],"depth":2},{"title":"AutoFeatureExtractor","local":"transformers.AutoFeatureExtractor","sections":[],"depth":2},{"title":"AutoImageProcessor","local":"transformers.AutoImageProcessor","sections":[],"depth":2},{"title":"AutoProcessor","local":"transformers.AutoProcessor","sections":[],"depth":2},{"title":"Generic model classes","local":"generic-model-classes","sections":[{"title":"AutoModel","local":"transformers.AutoModel","sections":[],"depth":3},{"title":"TFAutoModel","local":"transformers.TFAutoModel","sections":[],"depth":3},{"title":"FlaxAutoModel","local":"transformers.FlaxAutoModel","sections":[],"depth":3}],"depth":2},{"title":"Generic pretraining classes","local":"generic-pretraining-classes","sections":[{"title":"AutoModelForPreTraining","local":"transformers.AutoModelForPreTraining","sections":[],"depth":3},{"title":"TFAutoModelForPreTraining","local":"transformers.TFAutoModelForPreTraining","sections":[],"depth":3},{"title":"FlaxAutoModelForPreTraining","local":"transformers.FlaxAutoModelForPreTraining","sections":[],"depth":3}],"depth":2},{"title":"Natural Language Processing","local":"natural-language-processing","sections":[{"title":"AutoModelForCausalLM","local":"transformers.AutoModelForCausalLM","sections":[],"depth":3},{"title":"TFAutoModelForCausalLM","local":"transformers.TFAutoModelForCausalLM","sections":[],"depth":3},{"title":"FlaxAutoModelForCausalLM","local":"transformers.FlaxAutoModelForCausalLM","sections":[],"depth":3},{"title":"AutoModelForMaskedLM","local":"transformers.AutoModelForMaskedLM","sections":[],"depth":3},{"title":"TFAutoModelForMaskedLM","local":"transformers.TFAutoModelForMaskedLM","sections":[],"depth":3},{"title":"FlaxAutoModelForMaskedLM","local":"transformers.FlaxAutoModelForMaskedLM","sections":[],"depth":3},{"title":"AutoModelForMaskGeneration","local":"transformers.AutoModelForMaskGeneration","sections":[],"depth":3},{"title":"TFAutoModelForMaskGeneration","local":"transformers.TFAutoModelForMaskGeneration","sections":[],"depth":3},{"title":"AutoModelForSeq2SeqLM","local":"transformers.AutoModelForSeq2SeqLM","sections":[],"depth":3},{"title":"TFAutoModelForSeq2SeqLM","local":"transformers.TFAutoModelForSeq2SeqLM","sections":[],"depth":3},{"title":"FlaxAutoModelForSeq2SeqLM","local":"transformers.FlaxAutoModelForSeq2SeqLM","sections":[],"depth":3},{"title":"AutoModelForSequenceClassification","local":"transformers.AutoModelForSequenceClassification","sections":[],"depth":3},{"title":"TFAutoModelForSequenceClassification","local":"transformers.TFAutoModelForSequenceClassification","sections":[],"depth":3},{"title":"FlaxAutoModelForSequenceClassification","local":"transformers.FlaxAutoModelForSequenceClassification","sections":[],"depth":3},{"title":"AutoModelForMultipleChoice","local":"transformers.AutoModelForMultipleChoice","sections":[],"depth":3},{"title":"TFAutoModelForMultipleChoice","local":"transformers.TFAutoModelForMultipleChoice","sections":[],"depth":3},{"title":"FlaxAutoModelForMultipleChoice","local":"transformers.FlaxAutoModelForMultipleChoice","sections":[],"depth":3},{"title":"AutoModelForNextSentencePrediction","local":"transformers.AutoModelForNextSentencePrediction","sections":[],"depth":3},{"title":"TFAutoModelForNextSentencePrediction","local":"transformers.TFAutoModelForNextSentencePrediction","sections":[],"depth":3},{"title":"FlaxAutoModelForNextSentencePrediction","local":"transformers.FlaxAutoModelForNextSentencePrediction","sections":[],"depth":3},{"title":"AutoModelForTokenClassification","local":"transformers.AutoModelForTokenClassification","sections":[],"depth":3},{"title":"TFAutoModelForTokenClassification","local":"transformers.TFAutoModelForTokenClassification","sections":[],"depth":3},{"title":"FlaxAutoModelForTokenClassification","local":"transformers.FlaxAutoModelForTokenClassification","sections":[],"depth":3},{"title":"AutoModelForQuestionAnswering","local":"transformers.AutoModelForQuestionAnswering","sections":[],"depth":3},{"title":"TFAutoModelForQuestionAnswering","local":"transformers.TFAutoModelForQuestionAnswering","sections":[],"depth":3},{"title":"FlaxAutoModelForQuestionAnswering","local":"transformers.FlaxAutoModelForQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForTextEncoding","local":"transformers.AutoModelForTextEncoding","sections":[],"depth":3},{"title":"TFAutoModelForTextEncoding","local":"transformers.TFAutoModelForTextEncoding","sections":[],"depth":3}],"depth":2},{"title":"Computer vision","local":"computer-vision","sections":[{"title":"AutoModelForDepthEstimation","local":"transformers.AutoModelForDepthEstimation","sections":[],"depth":3},{"title":"AutoModelForImageClassification","local":"transformers.AutoModelForImageClassification","sections":[],"depth":3},{"title":"TFAutoModelForImageClassification","local":"transformers.TFAutoModelForImageClassification","sections":[],"depth":3},{"title":"FlaxAutoModelForImageClassification","local":"transformers.FlaxAutoModelForImageClassification","sections":[],"depth":3},{"title":"AutoModelForVideoClassification","local":"transformers.AutoModelForVideoClassification","sections":[],"depth":3},{"title":"AutoModelForMaskedImageModeling","local":"transformers.AutoModelForMaskedImageModeling","sections":[],"depth":3},{"title":"TFAutoModelForMaskedImageModeling","local":"transformers.TFAutoModelForMaskedImageModeling","sections":[],"depth":3},{"title":"AutoModelForObjectDetection","local":"transformers.AutoModelForObjectDetection","sections":[],"depth":3},{"title":"AutoModelForImageSegmentation","local":"transformers.AutoModelForImageSegmentation","sections":[],"depth":3},{"title":"AutoModelForImageToImage","local":"transformers.AutoModelForImageToImage","sections":[],"depth":3},{"title":"AutoModelForSemanticSegmentation","local":"transformers.AutoModelForSemanticSegmentation","sections":[],"depth":3},{"title":"TFAutoModelForSemanticSegmentation","local":"transformers.TFAutoModelForSemanticSegmentation","sections":[],"depth":3},{"title":"AutoModelForInstanceSegmentation","local":"transformers.AutoModelForInstanceSegmentation","sections":[],"depth":3},{"title":"AutoModelForUniversalSegmentation","local":"transformers.AutoModelForUniversalSegmentation","sections":[],"depth":3},{"title":"AutoModelForZeroShotImageClassification","local":"transformers.AutoModelForZeroShotImageClassification","sections":[],"depth":3},{"title":"TFAutoModelForZeroShotImageClassification","local":"transformers.TFAutoModelForZeroShotImageClassification","sections":[],"depth":3},{"title":"AutoModelForZeroShotObjectDetection","local":"transformers.AutoModelForZeroShotObjectDetection","sections":[],"depth":3}],"depth":2},{"title":"Audio","local":"audio","sections":[{"title":"AutoModelForAudioClassification","local":"transformers.AutoModelForAudioClassification","sections":[],"depth":3},{"title":"AutoModelForAudioFrameClassification","local":"transformers.TFAutoModelForAudioClassification","sections":[],"depth":3},{"title":"TFAutoModelForAudioFrameClassification","local":"transformers.AutoModelForAudioFrameClassification","sections":[],"depth":3},{"title":"AutoModelForCTC","local":"transformers.AutoModelForCTC","sections":[],"depth":3},{"title":"AutoModelForSpeechSeq2Seq","local":"transformers.AutoModelForSpeechSeq2Seq","sections":[],"depth":3},{"title":"TFAutoModelForSpeechSeq2Seq","local":"transformers.TFAutoModelForSpeechSeq2Seq","sections":[],"depth":3},{"title":"FlaxAutoModelForSpeechSeq2Seq","local":"transformers.FlaxAutoModelForSpeechSeq2Seq","sections":[],"depth":3},{"title":"AutoModelForAudioXVector","local":"transformers.AutoModelForAudioXVector","sections":[],"depth":3},{"title":"AutoModelForTextToSpectrogram","local":"transformers.AutoModelForTextToSpectrogram","sections":[],"depth":3},{"title":"AutoModelForTextToWaveform","local":"transformers.AutoModelForTextToWaveform","sections":[],"depth":3}],"depth":2},{"title":"Multimodal","local":"multimodal","sections":[{"title":"AutoModelForTableQuestionAnswering","local":"transformers.AutoModelForTableQuestionAnswering","sections":[],"depth":3},{"title":"TFAutoModelForTableQuestionAnswering","local":"transformers.TFAutoModelForTableQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForDocumentQuestionAnswering","local":"transformers.AutoModelForDocumentQuestionAnswering","sections":[],"depth":3},{"title":"TFAutoModelForDocumentQuestionAnswering","local":"transformers.TFAutoModelForDocumentQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForVisualQuestionAnswering","local":"transformers.AutoModelForVisualQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForVision2Seq","local":"transformers.AutoModelForVision2Seq","sections":[],"depth":3},{"title":"TFAutoModelForVision2Seq","local":"transformers.TFAutoModelForVision2Seq","sections":[],"depth":3},{"title":"FlaxAutoModelForVision2Seq","local":"transformers.FlaxAutoModelForVision2Seq","sections":[],"depth":3}],"depth":2}],"depth":1}';function I3(y){return cX(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Y3 extends fX{constructor(n){super(),gX(this,n,I3,J3,mX,{})}}export{Y3 as component};
