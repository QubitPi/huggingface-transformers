import{s as te,o as ne,n as ae}from"../chunks/scheduler.9bc65507.js";import{S as ie,i as oe,g as r,s as o,r as I,A as le,h as p,f as n,c as l,j as V,u as R,x as f,k as ee,y as se,a,v as U,d as W,t as z,w as O}from"../chunks/index.707bf1b6.js";import{T as re}from"../chunks/Tip.c2ecdbf4.js";import{H as B}from"../chunks/Heading.342b1fa6.js";function pe(b){let i,u='DialoGPT’s architecture is based on the GPT2 model, refer to <a href="gpt2">GPT2’s documentation page</a> for API reference and examples.';return{c(){i=r("p"),i.innerHTML=u},l(s){i=p(s,"P",{"data-svelte-h":!0}),f(i)!=="svelte-c7ppar"&&(i.innerHTML=u)},m(s,w){a(s,i,w)},p:ae,d(s){s&&n(i)}}}function me(b){let i,u,s,w,d,M,c,C,h,F=`DialoGPT was proposed in <a href="https://arxiv.org/abs/1911.00536" rel="nofollow">DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation</a> by Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao,
Jianfeng Gao, Jingjing Liu, Bill Dolan. It’s a GPT2 Model trained on 147M conversation-like exchanges extracted from
Reddit.`,L,g,J="The abstract from the paper is the following:",D,v,N=`<em>We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained
transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning
from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human
both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems
that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline
systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response
generation and the development of more intelligent open-domain dialogue systems.</em>`,H,T,Y='The original code can be found <a href="https://github.com/microsoft/DialoGPT" rel="nofollow">here</a>.',y,$,k,P,X=`<li>DialoGPT is a model with absolute position embeddings so it’s usually advised to pad the inputs on the right rather
than the left.</li> <li>DialoGPT was trained with a causal language modeling (CLM) objective on conversational data and is therefore powerful
at response generation in open-domain dialogue systems.</li> <li>DialoGPT enables the user to create a chat bot in just 10 lines of code as shown on <a href="https://huggingface.co/microsoft/DialoGPT-medium" rel="nofollow">DialoGPT’s model card</a>.</li>`,S,x,Z="Training:",j,_,K=`In order to train or fine-tune DialoGPT, one can use causal language modeling training. To cite the official paper: <em>We
follow the OpenAI GPT-2 to model a multiturn dialogue session as a long text and frame the generation task as language
modeling. We first concatenate all dialog turns within a dialogue session into a long text x_1,…, x_N (N is the
sequence length), ended by the end-of-text token.</em> For more information please confer to the original paper.`,q,m,A,G,E;return d=new B({props:{title:"DialoGPT",local:"dialogpt",headingTag:"h1"}}),c=new B({props:{title:"Overview",local:"overview",headingTag:"h2"}}),$=new B({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),m=new re({props:{$$slots:{default:[pe]},$$scope:{ctx:b}}}),{c(){i=r("meta"),u=o(),s=r("p"),w=o(),I(d.$$.fragment),M=o(),I(c.$$.fragment),C=o(),h=r("p"),h.innerHTML=F,L=o(),g=r("p"),g.textContent=J,D=o(),v=r("p"),v.innerHTML=N,H=o(),T=r("p"),T.innerHTML=Y,y=o(),I($.$$.fragment),k=o(),P=r("ul"),P.innerHTML=X,S=o(),x=r("p"),x.textContent=Z,j=o(),_=r("p"),_.innerHTML=K,q=o(),I(m.$$.fragment),A=o(),G=r("p"),this.h()},l(e){const t=le("svelte-u9bgzb",document.head);i=p(t,"META",{name:!0,content:!0}),t.forEach(n),u=l(e),s=p(e,"P",{}),V(s).forEach(n),w=l(e),R(d.$$.fragment,e),M=l(e),R(c.$$.fragment,e),C=l(e),h=p(e,"P",{"data-svelte-h":!0}),f(h)!=="svelte-1r5vgq6"&&(h.innerHTML=F),L=l(e),g=p(e,"P",{"data-svelte-h":!0}),f(g)!=="svelte-vfdo9a"&&(g.textContent=J),D=l(e),v=p(e,"P",{"data-svelte-h":!0}),f(v)!=="svelte-7bhez3"&&(v.innerHTML=N),H=l(e),T=p(e,"P",{"data-svelte-h":!0}),f(T)!=="svelte-17b2666"&&(T.innerHTML=Y),y=l(e),R($.$$.fragment,e),k=l(e),P=p(e,"UL",{"data-svelte-h":!0}),f(P)!=="svelte-1kdkvca"&&(P.innerHTML=X),S=l(e),x=p(e,"P",{"data-svelte-h":!0}),f(x)!=="svelte-1igpel8"&&(x.textContent=Z),j=l(e),_=p(e,"P",{"data-svelte-h":!0}),f(_)!=="svelte-1yc28b0"&&(_.innerHTML=K),q=l(e),R(m.$$.fragment,e),A=l(e),G=p(e,"P",{}),V(G).forEach(n),this.h()},h(){ee(i,"name","hf:doc:metadata"),ee(i,"content",fe)},m(e,t){se(document.head,i),a(e,u,t),a(e,s,t),a(e,w,t),U(d,e,t),a(e,M,t),U(c,e,t),a(e,C,t),a(e,h,t),a(e,L,t),a(e,g,t),a(e,D,t),a(e,v,t),a(e,H,t),a(e,T,t),a(e,y,t),U($,e,t),a(e,k,t),a(e,P,t),a(e,S,t),a(e,x,t),a(e,j,t),a(e,_,t),a(e,q,t),U(m,e,t),a(e,A,t),a(e,G,t),E=!0},p(e,[t]){const Q={};t&2&&(Q.$$scope={dirty:t,ctx:e}),m.$set(Q)},i(e){E||(W(d.$$.fragment,e),W(c.$$.fragment,e),W($.$$.fragment,e),W(m.$$.fragment,e),E=!0)},o(e){z(d.$$.fragment,e),z(c.$$.fragment,e),z($.$$.fragment,e),z(m.$$.fragment,e),E=!1},d(e){e&&(n(u),n(s),n(w),n(M),n(C),n(h),n(L),n(g),n(D),n(v),n(H),n(T),n(y),n(k),n(P),n(S),n(x),n(j),n(_),n(q),n(A),n(G)),n(i),O(d,e),O(c,e),O($,e),O(m,e)}}}const fe='{"title":"DialoGPT","local":"dialogpt","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2}],"depth":1}';function ue(b){return ne(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ve extends ie{constructor(i){super(),oe(this,i,ue,me,te,{})}}export{ve as component};
