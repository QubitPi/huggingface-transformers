import{s as St,o as zt,n as Xt}from"../chunks/scheduler.9bc65507.js";import{S as Yt,i as qt,g as o,s as a,r as i,A as Dt,h as r,f as s,c as l,j as Lt,u as p,x as f,k as gt,y as Kt,a as n,v as m,d as c,t as u,w as d}from"../chunks/index.707bf1b6.js";import{T as Nt}from"../chunks/Tip.c2ecdbf4.js";import{Y as Pt}from"../chunks/Youtube.e1129c6f.js";import{C as y}from"../chunks/CodeBlock.54a9f38d.js";import{H as T}from"../chunks/Heading.342b1fa6.js";function Ot(ye){let h,w='Refer to the Performance <a href="performance">guide</a> for more details about memory-saving techniques.';return{c(){h=o("p"),h.innerHTML=w},l(g){h=r(g,"P",{"data-svelte-h":!0}),f(h)!=="svelte-xgjmzv"&&(h.innerHTML=w)},m(g,M){n(g,h,M)},p:Xt,d(g){g&&s(h)}}}function es(ye){let h,w="By default, the tokenizer creates an <code>attention_mask</code> for you based on your specific tokenizerâ€™s defaults.";return{c(){h=o("p"),h.innerHTML=w},l(g){h=r(g,"P",{"data-svelte-h":!0}),f(h)!=="svelte-fvd42m"&&(h.innerHTML=w)},m(g,M){n(g,h,M)},p:Xt,d(g){g&&s(h)}}}function ts(ye){let h,w,g,M,v,Me,k,yt="Sometimes errors occur, but we are here to help! This guide covers some of the most common issues weâ€™ve seen and how you can resolve them. However, this guide isnâ€™t meant to be a comprehensive collection of every ðŸ¤— Transformers issue. For more help with troubleshooting your issue, try:",be,U,$e,C,wt='<li>Asking for help on the <a href="https://discuss.huggingface.co/" rel="nofollow">forums</a>. There are specific categories you can post your question to, like <a href="https://discuss.huggingface.co/c/beginners/5" rel="nofollow">Beginners</a> or <a href="https://discuss.huggingface.co/c/transformers/9" rel="nofollow">ðŸ¤— Transformers</a>. Make sure you write a good descriptive forum post with some reproducible code to maximize the likelihood that your problem is solved!</li>',je,J,Te,b,Mt='<li><p>Create an <a href="https://github.com/huggingface/transformers/issues/new/choose" rel="nofollow">Issue</a> on the ðŸ¤— Transformers repository if it is a bug related to the library. Try to include as much information describing the bug as possible to help us better figure out whatâ€™s wrong and how we can fix it.</p></li> <li><p>Check the <a href="migration">Migration</a> guide if you use an older version of ðŸ¤— Transformers since some important changes have been introduced between versions.</p></li>',ve,_,bt='For more details about troubleshooting and getting help, take a look at <a href="https://huggingface.co/course/chapter8/1?fw=pt" rel="nofollow">Chapter 8</a> of the Hugging Face course.',ke,V,Ue,Z,$t="Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection error. When your script attempts to download model weights or datasets, the download will hang and then timeout with the following message:",Ce,x,Je,G,jt='In this case, you should try to run ðŸ¤— Transformers on <a href="installation#offline-mode">offline mode</a> to avoid the connection error.',_e,I,Ve,W,Tt="Training large models with millions of parameters can be challenging without the appropriate hardware. A common error you may encounter when the GPU runs out of memory is:",Ze,H,xe,B,vt="Here are some potential solutions you can try to lessen memory use:",Ge,Q,kt='<li>Reduce the <a href="main_classes/trainer#transformers.TrainingArguments.per_device_train_batch_size"><code>per_device_train_batch_size</code></a> value in <a href="/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>.</li> <li>Try using <a href="main_classes/trainer#transformers.TrainingArguments.gradient_accumulation_steps"><code>gradient_accumulation_steps</code></a> in <a href="/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> to effectively increase overall batch size.</li>',Ie,$,We,F,He,A,Ut='TensorFlowâ€™s <a href="https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model" rel="nofollow">model.save</a> method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because ðŸ¤— Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:',Be,E,Ct='<li>Save the model weights as a <code>h5</code> file extension with <a href="https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model" rel="nofollow"><code>model.save_weights</code></a> and then reload the model with <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a>:</li>',Qe,R,Fe,L,Jt='<li>Save the model with <code>~TFPretrainedModel.save_pretrained</code> and load it again with <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a>:</li>',Ae,N,Ee,P,Re,X,_t="Another common error you may encounter, especially if it is a newly released model, is <code>ImportError</code>:",Le,S,Ne,z,Vt="For these error types, check to make sure you have the latest version of ðŸ¤— Transformers installed to access the most recent models:",Pe,Y,Xe,q,Se,D,Zt="Sometimes you may run into a generic CUDA error about an error in the device code.",ze,K,Ye,O,xt="You should try to run the code on a CPU first to get a more descriptive error message. Add the following environment variable to the beginning of your code to switch to a CPU:",qe,ee,De,te,Gt="Another option is to get a better traceback from the GPU. Add the following environment variable to the beginning of your code to get the traceback to point to the source of the error:",Ke,se,Oe,ne,et,ae,It="In some cases, the output <code>hidden_state</code> may be incorrect if the <code>input_ids</code> include padding tokens. To demonstrate, load a model and tokenizer. You can access a modelâ€™s <code>pad_token_id</code> to see its value. The <code>pad_token_id</code> may be <code>None</code> for some models, but you can always manually set it.",tt,le,st,oe,Wt="The following example shows the output without masking the padding tokens:",nt,re,at,ie,Ht="Here is the actual output of the second sequence:",lt,pe,ot,me,Bt="Most of the time, you should provide an <code>attention_mask</code> to your model to ignore the padding tokens to avoid this silent error. Now the output of the second sequence matches its actual output:",rt,j,it,ce,pt,ue,Qt="ðŸ¤— Transformers doesnâ€™t automatically create an <code>attention_mask</code> to mask a padding token if it is provided because:",mt,de,Ft="<li>Some models donâ€™t have a padding token.</li> <li>For some use-cases, users want a model to attend to a padding token.</li>",ct,fe,ut,he,At=`Generally, we recommend using the <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class to load pretrained instances of models. This class
can automatically infer and load the correct architecture from a given checkpoint based on the configuration. If you see
this <code>ValueError</code> when loading a model from a checkpoint, this means the Auto class couldnâ€™t find a mapping from
the configuration in the given checkpoint to the kind of model you are trying to load. Most commonly, this happens when a
checkpoint doesnâ€™t support a given task.
For instance, youâ€™ll see this error in the following example because there is no GPT2 for question answering:`,dt,ge,ft,we,ht;return v=new T({props:{title:"Troubleshoot",local:"troubleshoot",headingTag:"h1"}}),U=new Pt({props:{id:"S2EEG3JIt2A"}}),J=new Pt({props:{id:"_PAli-V4wj0"}}),V=new T({props:{title:"Firewalled environments",local:"firewalled-environments",headingTag:"h2"}}),x=new y({props:{code:"VmFsdWVFcnJvciUzQSUyMENvbm5lY3Rpb24lMjBlcnJvciUyQyUyMGFuZCUyMHdlJTIwY2Fubm90JTIwZmluZCUyMHRoZSUyMHJlcXVlc3RlZCUyMGZpbGVzJTIwaW4lMjB0aGUlMjBjYWNoZWQlMjBwYXRoLiUwQVBsZWFzZSUyMHRyeSUyMGFnYWluJTIwb3IlMjBtYWtlJTIwc3VyZSUyMHlvdXIlMjBJbnRlcm5ldCUyMGNvbm5lY3Rpb24lMjBpcyUyMG9uLg==",highlighted:`ValueError: Connection error, <span class="hljs-built_in">and</span> we cannot <span class="hljs-keyword">find</span> the requested <span class="hljs-keyword">files</span> in the cached path.
Please <span class="hljs-keyword">try</span> again <span class="hljs-built_in">or</span> <span class="hljs-keyword">make</span> sure your Internet connection <span class="hljs-keyword">is</span> <span class="hljs-keyword">on</span>.`,wrap:!1}}),I=new T({props:{title:"CUDA out of memory",local:"cuda-out-of-memory",headingTag:"h2"}}),H=new y({props:{code:"Q1VEQSUyMG91dCUyMG9mJTIwbWVtb3J5LiUyMFRyaWVkJTIwdG8lMjBhbGxvY2F0ZSUyMDI1Ni4wMCUyME1pQiUyMChHUFUlMjAwJTNCJTIwMTEuMTclMjBHaUIlMjB0b3RhbCUyMGNhcGFjaXR5JTNCJTIwOS43MCUyMEdpQiUyMGFscmVhZHklMjBhbGxvY2F0ZWQlM0IlMjAxNzkuODElMjBNaUIlMjBmcmVlJTNCJTIwOS44NSUyMEdpQiUyMHJlc2VydmVkJTIwaW4lMjB0b3RhbCUyMGJ5JTIwUHlUb3JjaCk=",highlighted:'<span class="hljs-attribute">CUDA</span> out of memory. Tried to allocate <span class="hljs-number">256</span>.<span class="hljs-number">00</span> MiB (GPU <span class="hljs-number">0</span>; <span class="hljs-number">11</span>.<span class="hljs-number">17</span> GiB total capacity; <span class="hljs-number">9</span>.<span class="hljs-number">70</span> GiB already allocated; <span class="hljs-number">179</span>.<span class="hljs-number">81</span> MiB free; <span class="hljs-number">9</span>.<span class="hljs-number">85</span> GiB reserved in total by PyTorch)',wrap:!1}}),$=new Nt({props:{$$slots:{default:[Ot]},$$scope:{ctx:ye}}}),F=new T({props:{title:"Unable to load a saved TensorFlow model",local:"unable-to-load-a-saved-tensorflow-model",headingTag:"h2"}}),R=new y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGUHJlVHJhaW5lZE1vZGVsJTBBZnJvbSUyMHRlbnNvcmZsb3clMjBpbXBvcnQlMjBrZXJhcyUwQSUwQW1vZGVsLnNhdmVfd2VpZ2h0cyglMjJzb21lX2ZvbGRlciUyRnRmX21vZGVsLmg1JTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZQcmVUcmFpbmVkTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnNvbWVfZm9sZGVyJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFPreTrainedModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras

<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_weights(<span class="hljs-string">&quot;some_folder/tf_model.h5&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFPreTrainedModel.from_pretrained(<span class="hljs-string">&quot;some_folder&quot;</span>)`,wrap:!1}}),N=new y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGUHJlVHJhaW5lZE1vZGVsJTBBJTBBbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKCUyMnBhdGhfdG8lMkZtb2RlbCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGUHJlVHJhaW5lZE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJwYXRoX3RvJTJGbW9kZWwlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFPreTrainedModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model.save_pretrained(<span class="hljs-string">&quot;path_to/model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFPreTrainedModel.from_pretrained(<span class="hljs-string">&quot;path_to/model&quot;</span>)`,wrap:!1}}),P=new T({props:{title:"ImportError",local:"importerror",headingTag:"h2"}}),S=new y({props:{code:"SW1wb3J0RXJyb3IlM0ElMjBjYW5ub3QlMjBpbXBvcnQlMjBuYW1lJTIwJ0ltYWdlR1BUSW1hZ2VQcm9jZXNzb3InJTIwZnJvbSUyMCd0cmFuc2Zvcm1lcnMnJTIwKHVua25vd24lMjBsb2NhdGlvbik=",highlighted:'ImportError: cannot <span class="hljs-keyword">import</span> <span class="hljs-type">name</span> <span class="hljs-string">&#x27;ImageGPTImageProcessor&#x27;</span> <span class="hljs-keyword">from</span> <span class="hljs-string">&#x27;transformers&#x27;</span> (<span class="hljs-type">unknown</span> <span class="hljs-keyword">location</span>)',wrap:!1}}),Y=new y({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMC0tdXBncmFkZQ==",highlighted:"pip install transformers --upgrade",wrap:!1}}),q=new T({props:{title:"CUDA error: device-side assert triggered",local:"cuda-error-device-side-assert-triggered",headingTag:"h2"}}),K=new y({props:{code:"UnVudGltZUVycm9yJTNBJTIwQ1VEQSUyMGVycm9yJTNBJTIwZGV2aWNlLXNpZGUlMjBhc3NlcnQlMjB0cmlnZ2VyZWQ=",highlighted:'RuntimeError: CUDA <span class="hljs-literal">error</span>: device-<span class="hljs-literal">side</span> <span class="hljs-keyword">assert</span> triggered',wrap:!1}}),ee=new y({props:{code:"aW1wb3J0JTIwb3MlMEElMEFvcy5lbnZpcm9uJTVCJTIyQ1VEQV9WSVNJQkxFX0RFVklDRVMlMjIlNUQlMjAlM0QlMjAlMjIlMjI=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> os

<span class="hljs-meta">&gt;&gt;&gt; </span>os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = <span class="hljs-string">&quot;&quot;</span>`,wrap:!1}}),se=new y({props:{code:"aW1wb3J0JTIwb3MlMEElMEFvcy5lbnZpcm9uJTVCJTIyQ1VEQV9MQVVOQ0hfQkxPQ0tJTkclMjIlNUQlMjAlM0QlMjAlMjIxJTIy",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> os

<span class="hljs-meta">&gt;&gt;&gt; </span>os.environ[<span class="hljs-string">&quot;CUDA_LAUNCH_BLOCKING&quot;</span>] = <span class="hljs-string">&quot;1&quot;</span>`,wrap:!1}}),ne=new T({props:{title:"Incorrect output when padding tokens arenâ€™t masked",local:"incorrect-output-when-padding-tokens-arent-masked",headingTag:"h2"}}),le=new y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtdW5jYXNlZCUyMiklMEFtb2RlbC5jb25maWcucGFkX3Rva2VuX2lk",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.pad_token_id
<span class="hljs-number">0</span>`,wrap:!1}}),re=new y({props:{code:"aW5wdXRfaWRzJTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QiU1Qjc1OTIlMkMlMjAyMDU3JTJDJTIwMjA5NyUyQyUyMDIzOTMlMkMlMjA5NjExJTJDJTIwMjExNSU1RCUyQyUyMCU1Qjc1OTIlMkMlMjAwJTJDJTIwMCUyQyUyMDAlMkMlMjAwJTJDJTIwMCU1RCU1RCklMEFvdXRwdXQlMjAlM0QlMjBtb2RlbChpbnB1dF9pZHMpJTBBcHJpbnQob3V0cHV0LmxvZ2l0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([[<span class="hljs-number">7592</span>, <span class="hljs-number">2057</span>, <span class="hljs-number">2097</span>, <span class="hljs-number">2393</span>, <span class="hljs-number">9611</span>, <span class="hljs-number">2115</span>], [<span class="hljs-number">7592</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>output = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(output.logits)
tensor([[ <span class="hljs-number">0.0082</span>, -<span class="hljs-number">0.2307</span>],
        [ <span class="hljs-number">0.1317</span>, -<span class="hljs-number">0.1683</span>]], grad_fn=&lt;AddmmBackward0&gt;)`,wrap:!1}}),pe=new y({props:{code:"aW5wdXRfaWRzJTIwJTNEJTIwdG9yY2gudGVuc29yKCU1QiU1Qjc1OTIlNUQlNUQpJTBBb3V0cHV0JTIwJTNEJTIwbW9kZWwoaW5wdXRfaWRzKSUwQXByaW50KG91dHB1dC5sb2dpdHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([[<span class="hljs-number">7592</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>output = model(input_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(output.logits)
tensor([[-<span class="hljs-number">0.1008</span>, -<span class="hljs-number">0.4061</span>]], grad_fn=&lt;AddmmBackward0&gt;)`,wrap:!1}}),j=new Nt({props:{$$slots:{default:[es]},$$scope:{ctx:ye}}}),ce=new y({props:{code:"YXR0ZW50aW9uX21hc2slMjAlM0QlMjB0b3JjaC50ZW5zb3IoJTVCJTVCMSUyQyUyMDElMkMlMjAxJTJDJTIwMSUyQyUyMDElMkMlMjAxJTVEJTJDJTIwJTVCMSUyQyUyMDAlMkMlMjAwJTJDJTIwMCUyQyUyMDAlMkMlMjAwJTVEJTVEKSUwQW91dHB1dCUyMCUzRCUyMG1vZGVsKGlucHV0X2lkcyUyQyUyMGF0dGVudGlvbl9tYXNrJTNEYXR0ZW50aW9uX21hc2spJTBBcHJpbnQob3V0cHV0LmxvZ2l0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>attention_mask = torch.tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>], [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]])
<span class="hljs-meta">&gt;&gt;&gt; </span>output = model(input_ids, attention_mask=attention_mask)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(output.logits)
tensor([[ <span class="hljs-number">0.0082</span>, -<span class="hljs-number">0.2307</span>],
        [-<span class="hljs-number">0.1008</span>, -<span class="hljs-number">0.4061</span>]], grad_fn=&lt;AddmmBackward0&gt;)`,wrap:!1}}),fe=new T({props:{title:"ValueError: Unrecognized configuration class XYZ for this kind of AutoModel",local:"valueerror-unrecognized-configuration-class-xyz-for-this-kind-of-automodel",headingTag:"h2"}}),ge=new y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haS1jb21tdW5pdHklMkZncHQyLW1lZGl1bSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWktY29tbXVuaXR5JTJGZ3B0Mi1tZWRpdW0lMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai-community/gpt2-medium&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;openai-community/gpt2-medium&quot;</span>)
ValueError: Unrecognized configuration <span class="hljs-keyword">class</span> &lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;transformers.models.gpt2.configuration_gpt2.GPT2Config&#x27;</span>&gt; <span class="hljs-keyword">for</span> this kind of AutoModel: AutoModelForQuestionAnswering.
Model <span class="hljs-built_in">type</span> should be one of AlbertConfig, BartConfig, BertConfig, BigBirdConfig, BigBirdPegasusConfig, BloomConfig, ...`,wrap:!1}}),{c(){h=o("meta"),w=a(),g=o("p"),M=a(),i(v.$$.fragment),Me=a(),k=o("p"),k.textContent=yt,be=a(),i(U.$$.fragment),$e=a(),C=o("ol"),C.innerHTML=wt,je=a(),i(J.$$.fragment),Te=a(),b=o("ol"),b.innerHTML=Mt,ve=a(),_=o("p"),_.innerHTML=bt,ke=a(),i(V.$$.fragment),Ue=a(),Z=o("p"),Z.textContent=$t,Ce=a(),i(x.$$.fragment),Je=a(),G=o("p"),G.innerHTML=jt,_e=a(),i(I.$$.fragment),Ve=a(),W=o("p"),W.textContent=Tt,Ze=a(),i(H.$$.fragment),xe=a(),B=o("p"),B.textContent=vt,Ge=a(),Q=o("ul"),Q.innerHTML=kt,Ie=a(),i($.$$.fragment),We=a(),i(F.$$.fragment),He=a(),A=o("p"),A.innerHTML=Ut,Be=a(),E=o("ul"),E.innerHTML=Ct,Qe=a(),i(R.$$.fragment),Fe=a(),L=o("ul"),L.innerHTML=Jt,Ae=a(),i(N.$$.fragment),Ee=a(),i(P.$$.fragment),Re=a(),X=o("p"),X.innerHTML=_t,Le=a(),i(S.$$.fragment),Ne=a(),z=o("p"),z.textContent=Vt,Pe=a(),i(Y.$$.fragment),Xe=a(),i(q.$$.fragment),Se=a(),D=o("p"),D.textContent=Zt,ze=a(),i(K.$$.fragment),Ye=a(),O=o("p"),O.textContent=xt,qe=a(),i(ee.$$.fragment),De=a(),te=o("p"),te.textContent=Gt,Ke=a(),i(se.$$.fragment),Oe=a(),i(ne.$$.fragment),et=a(),ae=o("p"),ae.innerHTML=It,tt=a(),i(le.$$.fragment),st=a(),oe=o("p"),oe.textContent=Wt,nt=a(),i(re.$$.fragment),at=a(),ie=o("p"),ie.textContent=Ht,lt=a(),i(pe.$$.fragment),ot=a(),me=o("p"),me.innerHTML=Bt,rt=a(),i(j.$$.fragment),it=a(),i(ce.$$.fragment),pt=a(),ue=o("p"),ue.innerHTML=Qt,mt=a(),de=o("ul"),de.innerHTML=Ft,ct=a(),i(fe.$$.fragment),ut=a(),he=o("p"),he.innerHTML=At,dt=a(),i(ge.$$.fragment),ft=a(),we=o("p"),this.h()},l(e){const t=Dt("svelte-u9bgzb",document.head);h=r(t,"META",{name:!0,content:!0}),t.forEach(s),w=l(e),g=r(e,"P",{}),Lt(g).forEach(s),M=l(e),p(v.$$.fragment,e),Me=l(e),k=r(e,"P",{"data-svelte-h":!0}),f(k)!=="svelte-1cx76wd"&&(k.textContent=yt),be=l(e),p(U.$$.fragment,e),$e=l(e),C=r(e,"OL",{"data-svelte-h":!0}),f(C)!=="svelte-wyi91t"&&(C.innerHTML=wt),je=l(e),p(J.$$.fragment,e),Te=l(e),b=r(e,"OL",{start:!0,"data-svelte-h":!0}),f(b)!=="svelte-l1kxjh"&&(b.innerHTML=Mt),ve=l(e),_=r(e,"P",{"data-svelte-h":!0}),f(_)!=="svelte-c2f5jg"&&(_.innerHTML=bt),ke=l(e),p(V.$$.fragment,e),Ue=l(e),Z=r(e,"P",{"data-svelte-h":!0}),f(Z)!=="svelte-lpgdhl"&&(Z.textContent=$t),Ce=l(e),p(x.$$.fragment,e),Je=l(e),G=r(e,"P",{"data-svelte-h":!0}),f(G)!=="svelte-170em04"&&(G.innerHTML=jt),_e=l(e),p(I.$$.fragment,e),Ve=l(e),W=r(e,"P",{"data-svelte-h":!0}),f(W)!=="svelte-28duqs"&&(W.textContent=Tt),Ze=l(e),p(H.$$.fragment,e),xe=l(e),B=r(e,"P",{"data-svelte-h":!0}),f(B)!=="svelte-1wd6425"&&(B.textContent=vt),Ge=l(e),Q=r(e,"UL",{"data-svelte-h":!0}),f(Q)!=="svelte-3xmtjw"&&(Q.innerHTML=kt),Ie=l(e),p($.$$.fragment,e),We=l(e),p(F.$$.fragment,e),He=l(e),A=r(e,"P",{"data-svelte-h":!0}),f(A)!=="svelte-dxswyr"&&(A.innerHTML=Ut),Be=l(e),E=r(e,"UL",{"data-svelte-h":!0}),f(E)!=="svelte-axsa61"&&(E.innerHTML=Ct),Qe=l(e),p(R.$$.fragment,e),Fe=l(e),L=r(e,"UL",{"data-svelte-h":!0}),f(L)!=="svelte-m1h2rm"&&(L.innerHTML=Jt),Ae=l(e),p(N.$$.fragment,e),Ee=l(e),p(P.$$.fragment,e),Re=l(e),X=r(e,"P",{"data-svelte-h":!0}),f(X)!=="svelte-1yk3oka"&&(X.innerHTML=_t),Le=l(e),p(S.$$.fragment,e),Ne=l(e),z=r(e,"P",{"data-svelte-h":!0}),f(z)!=="svelte-4f4dig"&&(z.textContent=Vt),Pe=l(e),p(Y.$$.fragment,e),Xe=l(e),p(q.$$.fragment,e),Se=l(e),D=r(e,"P",{"data-svelte-h":!0}),f(D)!=="svelte-1u0kwcf"&&(D.textContent=Zt),ze=l(e),p(K.$$.fragment,e),Ye=l(e),O=r(e,"P",{"data-svelte-h":!0}),f(O)!=="svelte-aeboz0"&&(O.textContent=xt),qe=l(e),p(ee.$$.fragment,e),De=l(e),te=r(e,"P",{"data-svelte-h":!0}),f(te)!=="svelte-ctfxbh"&&(te.textContent=Gt),Ke=l(e),p(se.$$.fragment,e),Oe=l(e),p(ne.$$.fragment,e),et=l(e),ae=r(e,"P",{"data-svelte-h":!0}),f(ae)!=="svelte-1hu02cc"&&(ae.innerHTML=It),tt=l(e),p(le.$$.fragment,e),st=l(e),oe=r(e,"P",{"data-svelte-h":!0}),f(oe)!=="svelte-i1hsji"&&(oe.textContent=Wt),nt=l(e),p(re.$$.fragment,e),at=l(e),ie=r(e,"P",{"data-svelte-h":!0}),f(ie)!=="svelte-101z1g5"&&(ie.textContent=Ht),lt=l(e),p(pe.$$.fragment,e),ot=l(e),me=r(e,"P",{"data-svelte-h":!0}),f(me)!=="svelte-1p57h5c"&&(me.innerHTML=Bt),rt=l(e),p(j.$$.fragment,e),it=l(e),p(ce.$$.fragment,e),pt=l(e),ue=r(e,"P",{"data-svelte-h":!0}),f(ue)!=="svelte-150uvc6"&&(ue.innerHTML=Qt),mt=l(e),de=r(e,"UL",{"data-svelte-h":!0}),f(de)!=="svelte-qm9a5i"&&(de.innerHTML=Ft),ct=l(e),p(fe.$$.fragment,e),ut=l(e),he=r(e,"P",{"data-svelte-h":!0}),f(he)!=="svelte-11l04n2"&&(he.innerHTML=At),dt=l(e),p(ge.$$.fragment,e),ft=l(e),we=r(e,"P",{}),Lt(we).forEach(s),this.h()},h(){gt(h,"name","hf:doc:metadata"),gt(h,"content",ss),gt(b,"start","2")},m(e,t){Kt(document.head,h),n(e,w,t),n(e,g,t),n(e,M,t),m(v,e,t),n(e,Me,t),n(e,k,t),n(e,be,t),m(U,e,t),n(e,$e,t),n(e,C,t),n(e,je,t),m(J,e,t),n(e,Te,t),n(e,b,t),n(e,ve,t),n(e,_,t),n(e,ke,t),m(V,e,t),n(e,Ue,t),n(e,Z,t),n(e,Ce,t),m(x,e,t),n(e,Je,t),n(e,G,t),n(e,_e,t),m(I,e,t),n(e,Ve,t),n(e,W,t),n(e,Ze,t),m(H,e,t),n(e,xe,t),n(e,B,t),n(e,Ge,t),n(e,Q,t),n(e,Ie,t),m($,e,t),n(e,We,t),m(F,e,t),n(e,He,t),n(e,A,t),n(e,Be,t),n(e,E,t),n(e,Qe,t),m(R,e,t),n(e,Fe,t),n(e,L,t),n(e,Ae,t),m(N,e,t),n(e,Ee,t),m(P,e,t),n(e,Re,t),n(e,X,t),n(e,Le,t),m(S,e,t),n(e,Ne,t),n(e,z,t),n(e,Pe,t),m(Y,e,t),n(e,Xe,t),m(q,e,t),n(e,Se,t),n(e,D,t),n(e,ze,t),m(K,e,t),n(e,Ye,t),n(e,O,t),n(e,qe,t),m(ee,e,t),n(e,De,t),n(e,te,t),n(e,Ke,t),m(se,e,t),n(e,Oe,t),m(ne,e,t),n(e,et,t),n(e,ae,t),n(e,tt,t),m(le,e,t),n(e,st,t),n(e,oe,t),n(e,nt,t),m(re,e,t),n(e,at,t),n(e,ie,t),n(e,lt,t),m(pe,e,t),n(e,ot,t),n(e,me,t),n(e,rt,t),m(j,e,t),n(e,it,t),m(ce,e,t),n(e,pt,t),n(e,ue,t),n(e,mt,t),n(e,de,t),n(e,ct,t),m(fe,e,t),n(e,ut,t),n(e,he,t),n(e,dt,t),m(ge,e,t),n(e,ft,t),n(e,we,t),ht=!0},p(e,[t]){const Et={};t&2&&(Et.$$scope={dirty:t,ctx:e}),$.$set(Et);const Rt={};t&2&&(Rt.$$scope={dirty:t,ctx:e}),j.$set(Rt)},i(e){ht||(c(v.$$.fragment,e),c(U.$$.fragment,e),c(J.$$.fragment,e),c(V.$$.fragment,e),c(x.$$.fragment,e),c(I.$$.fragment,e),c(H.$$.fragment,e),c($.$$.fragment,e),c(F.$$.fragment,e),c(R.$$.fragment,e),c(N.$$.fragment,e),c(P.$$.fragment,e),c(S.$$.fragment,e),c(Y.$$.fragment,e),c(q.$$.fragment,e),c(K.$$.fragment,e),c(ee.$$.fragment,e),c(se.$$.fragment,e),c(ne.$$.fragment,e),c(le.$$.fragment,e),c(re.$$.fragment,e),c(pe.$$.fragment,e),c(j.$$.fragment,e),c(ce.$$.fragment,e),c(fe.$$.fragment,e),c(ge.$$.fragment,e),ht=!0)},o(e){u(v.$$.fragment,e),u(U.$$.fragment,e),u(J.$$.fragment,e),u(V.$$.fragment,e),u(x.$$.fragment,e),u(I.$$.fragment,e),u(H.$$.fragment,e),u($.$$.fragment,e),u(F.$$.fragment,e),u(R.$$.fragment,e),u(N.$$.fragment,e),u(P.$$.fragment,e),u(S.$$.fragment,e),u(Y.$$.fragment,e),u(q.$$.fragment,e),u(K.$$.fragment,e),u(ee.$$.fragment,e),u(se.$$.fragment,e),u(ne.$$.fragment,e),u(le.$$.fragment,e),u(re.$$.fragment,e),u(pe.$$.fragment,e),u(j.$$.fragment,e),u(ce.$$.fragment,e),u(fe.$$.fragment,e),u(ge.$$.fragment,e),ht=!1},d(e){e&&(s(w),s(g),s(M),s(Me),s(k),s(be),s($e),s(C),s(je),s(Te),s(b),s(ve),s(_),s(ke),s(Ue),s(Z),s(Ce),s(Je),s(G),s(_e),s(Ve),s(W),s(Ze),s(xe),s(B),s(Ge),s(Q),s(Ie),s(We),s(He),s(A),s(Be),s(E),s(Qe),s(Fe),s(L),s(Ae),s(Ee),s(Re),s(X),s(Le),s(Ne),s(z),s(Pe),s(Xe),s(Se),s(D),s(ze),s(Ye),s(O),s(qe),s(De),s(te),s(Ke),s(Oe),s(et),s(ae),s(tt),s(st),s(oe),s(nt),s(at),s(ie),s(lt),s(ot),s(me),s(rt),s(it),s(pt),s(ue),s(mt),s(de),s(ct),s(ut),s(he),s(dt),s(ft),s(we)),s(h),d(v,e),d(U,e),d(J,e),d(V,e),d(x,e),d(I,e),d(H,e),d($,e),d(F,e),d(R,e),d(N,e),d(P,e),d(S,e),d(Y,e),d(q,e),d(K,e),d(ee,e),d(se,e),d(ne,e),d(le,e),d(re,e),d(pe,e),d(j,e),d(ce,e),d(fe,e),d(ge,e)}}}const ss='{"title":"Troubleshoot","local":"troubleshoot","sections":[{"title":"Firewalled environments","local":"firewalled-environments","sections":[],"depth":2},{"title":"CUDA out of memory","local":"cuda-out-of-memory","sections":[],"depth":2},{"title":"Unable to load a saved TensorFlow model","local":"unable-to-load-a-saved-tensorflow-model","sections":[],"depth":2},{"title":"ImportError","local":"importerror","sections":[],"depth":2},{"title":"CUDA error: device-side assert triggered","local":"cuda-error-device-side-assert-triggered","sections":[],"depth":2},{"title":"Incorrect output when padding tokens arenâ€™t masked","local":"incorrect-output-when-padding-tokens-arent-masked","sections":[],"depth":2},{"title":"ValueError: Unrecognized configuration class XYZ for this kind of AutoModel","local":"valueerror-unrecognized-configuration-class-xyz-for-this-kind-of-automodel","sections":[],"depth":2}],"depth":1}';function ns(ye){return zt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ms extends Yt{constructor(h){super(),qt(this,h,ns,ts,St,{})}}export{ms as component};
