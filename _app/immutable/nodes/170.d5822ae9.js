import{s as Vt,f as Bt,o as St,n as ut}from"../chunks/scheduler.9bc65507.js";import{S as Et,i as Ht,g as i,s as a,r as f,A as At,h as l,f as n,c as s,j as Z,u as h,x as m,k as J,y as c,a as o,v as g,d as u,t as _,w as v}from"../chunks/index.707bf1b6.js";import{T as qt}from"../chunks/Tip.c2ecdbf4.js";import{D as Le}from"../chunks/Docstring.17db21ae.js";import{C as Ke}from"../chunks/CodeBlock.54a9f38d.js";import{E as Zt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as Qt}from"../chunks/PipelineTag.44585822.js";import{H as V}from"../chunks/Heading.342b1fa6.js";function Rt(z){let d,L="Example:",b,p,T;return p=new Ke({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMExsYXZhRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTJDJTIwTGxhdmFDb25maWclMkMlMjBDTElQVmlzaW9uQ29uZmlnJTJDJTIwTGxhbWFDb25maWclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUC12aXNpb24lMjBjb25maWclMEF2aXNpb25fY29uZmlnJTIwJTNEJTIwQ0xJUFZpc2lvbkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMExsYW1hJTIwY29uZmlnJTBBdGV4dF9jb25maWclMjAlM0QlMjBMbGFtYUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMExsYXZhJTIwbGxhdmEtMS41LTdiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMExsYXZhQ29uZmlnKHZpc2lvbl9jb25maWclMkMlMjB0ZXh0X2NvbmZpZyklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjBmcm9tJTIwdGhlJTIwbGxhdmEtMS41LTdiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBMbGF2YUZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbihjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIP-vision config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>vision_config = CLIPVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Llama config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>text_config = LlamaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Llava llava-1.5-7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = LlavaConfig(vision_config, text_config)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the llava-1.5-7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = LlavaForConditionalGeneration(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=L,b=a(),f(p.$$.fragment)},l(r){d=l(r,"P",{"data-svelte-h":!0}),m(d)!=="svelte-11lpom8"&&(d.textContent=L),b=s(r),h(p.$$.fragment,r)},m(r,y){o(r,d,y),o(r,b,y),g(p,r,y),T=!0},p:ut,i(r){T||(u(p.$$.fragment,r),T=!0)},o(r){_(p.$$.fragment,r),T=!1},d(r){r&&(n(d),n(b)),v(p,r)}}}function Xt(z){let d,L=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=L},l(b){d=l(b,"P",{"data-svelte-h":!0}),m(d)!=="svelte-fincs2"&&(d.innerHTML=L)},m(b,p){o(b,d,p)},p:ut,d(b){b&&n(d)}}}function Yt(z){let d,L="Example:",b,p,T;return p=new Ke({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMExsYXZhRm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTBBJTBBbW9kZWwlMjAlM0QlMjBMbGF2YUZvckNvbmRpdGlvbmFsR2VuZXJhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIybGxhdmEtaGYlMkZsbGF2YS0xLjUtN2ItaGYlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybGxhdmEtaGYlMkZsbGF2YS0xLjUtN2ItaGYlMjIpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIyJTNDaW1hZ2UlM0UlNUNuVVNFUiUzQSUyMFdoYXQncyUyMHRoZSUyMGNvbnRlbnQlMjBvZiUyMHRoZSUyMGltYWdlJTNGJTVDbkFTU0lTVEFOVCUzQSUyMiUwQXVybCUyMCUzRCUyMCUyMmh0dHBzJTNBJTJGJTJGd3d3LmlsYW5rZWxtYW4ub3JnJTJGc3RvcHNpZ25zJTJGYXVzdHJhbGlhLmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEcHJvbXB0JTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQSUyMyUyMEdlbmVyYXRlJTBBZ2VuZXJhdGVfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbGVuZ3RoJTNEMzApJTBBcHJvY2Vzc29yLmJhdGNoX2RlY29kZShnZW5lcmF0ZV9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSUyQyUyMGNsZWFuX3VwX3Rva2VuaXphdGlvbl9zcGFjZXMlM0RGYWxzZSklNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, LlavaForConditionalGeneration

<span class="hljs-meta">&gt;&gt;&gt; </span>model = LlavaForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;llava-hf/llava-1.5-7b-hf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;llava-hf/llava-1.5-7b-hf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;&lt;image&gt;\\nUSER: What&#x27;s the content of the image?\\nASSISTANT:&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://www.ilankelman.org/stopsigns/australia.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=prompt, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(**inputs, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;\\nUSER: What&#x27;s the content of the image?\\nASSISTANT: The image features a stop sign on a street corner&quot;</span>`,wrap:!1}}),{c(){d=i("p"),d.textContent=L,b=a(),f(p.$$.fragment)},l(r){d=l(r,"P",{"data-svelte-h":!0}),m(d)!=="svelte-11lpom8"&&(d.textContent=L),b=s(r),h(p.$$.fragment,r)},m(r,y){o(r,d,y),o(r,b,y),g(p,r,y),T=!0},p:ut,i(r){T||(u(p.$$.fragment,r),T=!0)},o(r){_(p.$$.fragment,r),T=!1},d(r){r&&(n(d),n(b)),v(p,r)}}}function Dt(z){let d,L,b,p,T,r,y,Ce,B,_t="LLaVa is an open-source chatbot trained by fine-tuning LlamA/Vicuna on GPT-generated multimodal instruction-following data. It is an auto-regressive language model, based on the transformer architecture. In other words, it is an multi-modal version of LLMs fine-tuned for chat / instructions.",xe,S,vt='The LLaVa model was proposed in <a href="https://arxiv.org/abs/2304.08485" rel="nofollow">Visual Instruction Tuning</a> and improved in <a href="https://arxiv.org/pdf/2310.03744" rel="nofollow">Improved Baselines with Visual Instruction Tuning</a> by Haotian Liu, Chunyuan Li, Yuheng Li and Yong Jae Lee.',ke,E,bt="The abstract from the paper is the following:",Je,H,Tt="<em>Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in âˆ¼1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available</em>",ze,F,yt,Fe,A,$t='LLaVa architecture. Taken from the <a href="https://arxiv.org/abs/2304.08485">original paper.</a>',Ie,q,wt=`This model was contributed by <a href="https://huggingface.co/ArthurZ" rel="nofollow">ArthurZ</a> and <a href="https://huggingface.co/ybelkada" rel="nofollow">ybelkada</a>.
The original code can be found <a href="https://github.com/haotian-liu/LLaVA/tree/main/llava" rel="nofollow">here</a>.`,Ue,Q,je,R,Lt="<li><p>We advise users to use <code>padding_side=&quot;left&quot;</code> when computing batched generation as it leads to more accurate results. Simply make sure to call <code>processor.tokenizer.padding_side = &quot;left&quot;</code> before generating.</p></li> <li><p>Note the model has not been explicitly trained to process multiple images in the same prompt, although this is technically possible, you may experience inaccurate results.</p></li> <li><p>For better results, we recommend users to prompt the model with the correct prompt format:</p></li>",Pe,X,Ge,Y,Mt="For multiple turns conversation:",We,D,Ne,O,Ze,K,Ct='Flash Attention 2 is an even faster, optimized version of the previous optimization, please refer to the <a href="https://huggingface.co/docs/transformers/perf_infer_gpu_one" rel="nofollow">Flash Attention 2 section of performance docs</a>.',Ve,ee,Be,te,xt="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with BEiT.",Se,ne,Ee,oe,kt='<li>A <a href="https://colab.research.google.com/drive/1qsl6cd2c8gGtEW1xV5io7S8NHh-Cp1TV?usp=sharing" rel="nofollow">Google Colab demo</a> on how to run Llava on a free-tier Google colab instance leveraging 4-bit inference.</li> <li>A <a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LLaVa/Inference_with_LLaVa_for_multimodal_generation.ipynb" rel="nofollow">similar notebook</a> showcasing batched inference. ðŸŒŽ</li>',He,ae,Ae,$,se,et,he,Jt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a>. It is used to instantiate an
Llava model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Llava-9B.`,tt,ge,zt='e.g. <a href="https://huggingface.co/llava-hf/llava-9b" rel="nofollow">llava-hf/llava-9b</a>',nt,ue,Ft=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ot,I,qe,re,Qe,w,ie,at,_e,It="Constructs a Llava processor which wraps a Llava image processor and a Llava tokenizer into a single processor.",st,ve,Ut=`<a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaProcessor">LlavaProcessor</a> offers all the functionalities of <a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> and <a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaProcessor.decode">decode()</a> for more information.`,rt,U,le,it,be,jt=`This method forwards all its arguments to LlamaTokenizerFastâ€™s <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.batch_decode">batch_decode()</a>. Please
refer to the docstring of this method for more information.`,lt,j,de,dt,Te,Pt=`This method forwards all its arguments to LlamaTokenizerFastâ€™s <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Tokenizer.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,Re,ce,Xe,M,me,ct,ye,Gt=`The LLAVA model which consists of a vision backbone and a language model.
This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,mt,$e,Wt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,pt,k,pe,ft,we,Nt='The <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration">LlavaForConditionalGeneration</a> forward method, overrides the <code>__call__</code> special method.',ht,P,gt,G,Ye,Me,De;return T=new V({props:{title:"LLaVa",local:"llava",headingTag:"h1"}}),y=new V({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Q=new V({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),X=new Ke({props:{code:"JTIyVVNFUiUzQSUyMCUzQ2ltYWdlJTNFJTVDbiUzQ3Byb21wdCUzRUFTU0lTVEFOVCUzQSUyMg==",highlighted:'<span class="hljs-string">&quot;USER: &lt;image&gt;\\n&lt;prompt&gt;ASSISTANT:&quot;</span>',wrap:!1}}),D=new Ke({props:{code:"JTIyVVNFUiUzQSUyMCUzQ2ltYWdlJTNFJTVDbiUzQ3Byb21wdDElM0VBU1NJU1RBTlQlM0ElMjAlM0NhbnN3ZXIxJTNFVVNFUiUzQSUyMCUzQ3Byb21wdDIlM0VBU1NJU1RBTlQlM0ElMjAlM0NhbnN3ZXIyJTNFVVNFUiUzQSUyMCUzQ3Byb21wdDMlM0VBU1NJU1RBTlQlM0ElMjI=",highlighted:'<span class="hljs-string">&quot;USER: &lt;image&gt;\\n&lt;prompt1&gt;ASSISTANT: &lt;answer1&gt;USER: &lt;prompt2&gt;ASSISTANT: &lt;answer2&gt;USER: &lt;prompt3&gt;ASSISTANT:&quot;</span>',wrap:!1}}),O=new V({props:{title:"Using Flash Attention 2",local:"using-flash-attention-2",headingTag:"h3"}}),ee=new V({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ne=new Qt({props:{pipeline:"image-to-text"}}),ae=new V({props:{title:"LlavaConfig",local:"transformers.LlavaConfig",headingTag:"h2"}}),se=new Le({props:{name:"class transformers.LlavaConfig",anchor:"transformers.LlavaConfig",parameters:[{name:"vision_config",val:" = None"},{name:"text_config",val:" = None"},{name:"ignore_index",val:" = -100"},{name:"image_token_index",val:" = 32000"},{name:"projector_hidden_act",val:" = 'gelu'"},{name:"vision_feature_select_strategy",val:" = 'default'"},{name:"vision_feature_layer",val:" = -2"},{name:"vocab_size",val:" = 32000"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.LlavaConfig.vision_config",description:`<strong>vision_config</strong> (<code>LlavaVisionConfig</code>,  <em>optional</em>) &#x2014;
Custom vision config or dict`,name:"vision_config"},{anchor:"transformers.LlavaConfig.text_config",description:`<strong>text_config</strong> (<code>Union[AutoConfig, dict]</code>, <em>optional</em>) &#x2014;
The config object of the text backbone. Can be any of <code>LlamaConfig</code> or <code>MistralConfig</code>.`,name:"text_config"},{anchor:"transformers.LlavaConfig.ignore_index",description:`<strong>ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
The ignore index for the loss function.`,name:"ignore_index"},{anchor:"transformers.LlavaConfig.image_token_index",description:`<strong>image_token_index</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
The image token index to encode the image prompt.`,name:"image_token_index"},{anchor:"transformers.LlavaConfig.projector_hidden_act",description:`<strong>projector_hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The activation function used by the multimodal projector.`,name:"projector_hidden_act"},{anchor:"transformers.LlavaConfig.vision_feature_select_strategy",description:`<strong>vision_feature_select_strategy</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;default&quot;</code>) &#x2014;
The feature selection strategy used to select the vision feature from the CLIP backbone.`,name:"vision_feature_select_strategy"},{anchor:"transformers.LlavaConfig.vision_feature_layer",description:`<strong>vision_feature_layer</strong> (<code>int</code>, <em>optional</em>, defaults to -2) &#x2014;
The index of the layer to select the vision feature.`,name:"vision_feature_layer"},{anchor:"transformers.LlavaConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
Vocabulary size of the Llava model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaForConditionalGeneration">~LlavaForConditionalGeneration</a>`,name:"vocab_size"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llava/configuration_llava.py#L28"}}),I=new Zt({props:{anchor:"transformers.LlavaConfig.example",$$slots:{default:[Rt]},$$scope:{ctx:z}}}),re=new V({props:{title:"LlavaProcessor",local:"transformers.LlavaProcessor",headingTag:"h2"}}),ie=new Le({props:{name:"class transformers.LlavaProcessor",anchor:"transformers.LlavaProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"}],parametersDescription:[{anchor:"transformers.LlavaProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>, <em>optional</em>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.LlavaProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/model_doc/llama2#transformers.LlamaTokenizerFast">LlamaTokenizerFast</a>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llava/processing_llava.py#L29"}}),le=new Le({props:{name:"batch_decode",anchor:"transformers.LlavaProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llava/processing_llava.py#L116"}}),de=new Le({props:{name:"decode",anchor:"transformers.LlavaProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llava/processing_llava.py#L124"}}),ce=new V({props:{title:"LlavaForConditionalGeneration",local:"transformers.LlavaForConditionalGeneration",headingTag:"h2"}}),me=new Le({props:{name:"class transformers.LlavaForConditionalGeneration",anchor:"transformers.LlavaForConditionalGeneration",parameters:[{name:"config",val:": LlavaConfig"}],parametersDescription:[{anchor:"transformers.LlavaForConditionalGeneration.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig">LlavaConfig</a> or <code>LlavaVisionConfig</code>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llava/modeling_llava.py#L233"}}),pe=new Le({props:{name:"forward",anchor:"transformers.LlavaForConditionalGeneration.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"pixel_values",val:": FloatTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"vision_feature_layer",val:": Optional = None"},{name:"vision_feature_select_strategy",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.LlavaForConditionalGeneration.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.LlavaForConditionalGeneration.forward.pixel_values",description:'<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, image_size, image_size)) -- The tensors corresponding to the input images. Pixel values can be obtained using [AutoImageProcessor](/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor). See [CLIPImageProcessor.__call__()](/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__) for details ([]</code>LlavaProcessor`] uses\n<a href="/docs/transformers/main/en/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> for processing images).',name:"pixel_values"},{anchor:"transformers.LlavaForConditionalGeneration.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.LlavaForConditionalGeneration.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>. <a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.LlavaForConditionalGeneration.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used (see <code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.LlavaForConditionalGeneration.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.LlavaForConditionalGeneration.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.LlavaForConditionalGeneration.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.LlavaForConditionalGeneration.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.LlavaForConditionalGeneration.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.</p>
<p>Args &#x2014;
labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/llava/modeling_llava.py#L348",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/llava#transformers.LlavaConfig"
>LlavaConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) â€” Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) â€” Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>image_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the image embeddings, <code>(batch_size, num_images, sequence_length, hidden_size)</code>.</p>
<p>image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),P=new qt({props:{$$slots:{default:[Xt]},$$scope:{ctx:z}}}),G=new Zt({props:{anchor:"transformers.LlavaForConditionalGeneration.forward.example",$$slots:{default:[Yt]},$$scope:{ctx:z}}}),{c(){d=i("meta"),L=a(),b=i("p"),p=a(),f(T.$$.fragment),r=a(),f(y.$$.fragment),Ce=a(),B=i("p"),B.textContent=_t,xe=a(),S=i("p"),S.innerHTML=vt,ke=a(),E=i("p"),E.textContent=bt,Je=a(),H=i("p"),H.innerHTML=Tt,ze=a(),F=i("img"),Fe=a(),A=i("small"),A.innerHTML=$t,Ie=a(),q=i("p"),q.innerHTML=wt,Ue=a(),f(Q.$$.fragment),je=a(),R=i("ul"),R.innerHTML=Lt,Pe=a(),f(X.$$.fragment),Ge=a(),Y=i("p"),Y.textContent=Mt,We=a(),f(D.$$.fragment),Ne=a(),f(O.$$.fragment),Ze=a(),K=i("p"),K.innerHTML=Ct,Ve=a(),f(ee.$$.fragment),Be=a(),te=i("p"),te.textContent=xt,Se=a(),f(ne.$$.fragment),Ee=a(),oe=i("ul"),oe.innerHTML=kt,He=a(),f(ae.$$.fragment),Ae=a(),$=i("div"),f(se.$$.fragment),et=a(),he=i("p"),he.innerHTML=Jt,tt=a(),ge=i("p"),ge.innerHTML=zt,nt=a(),ue=i("p"),ue.innerHTML=Ft,ot=a(),f(I.$$.fragment),qe=a(),f(re.$$.fragment),Qe=a(),w=i("div"),f(ie.$$.fragment),at=a(),_e=i("p"),_e.textContent=It,st=a(),ve=i("p"),ve.innerHTML=Ut,rt=a(),U=i("div"),f(le.$$.fragment),it=a(),be=i("p"),be.innerHTML=jt,lt=a(),j=i("div"),f(de.$$.fragment),dt=a(),Te=i("p"),Te.innerHTML=Pt,Re=a(),f(ce.$$.fragment),Xe=a(),M=i("div"),f(me.$$.fragment),ct=a(),ye=i("p"),ye.innerHTML=Gt,mt=a(),$e=i("p"),$e.innerHTML=Wt,pt=a(),k=i("div"),f(pe.$$.fragment),ft=a(),we=i("p"),we.innerHTML=Nt,ht=a(),f(P.$$.fragment),gt=a(),f(G.$$.fragment),Ye=a(),Me=i("p"),this.h()},l(e){const t=At("svelte-u9bgzb",document.head);d=l(t,"META",{name:!0,content:!0}),t.forEach(n),L=s(e),b=l(e,"P",{}),Z(b).forEach(n),p=s(e),h(T.$$.fragment,e),r=s(e),h(y.$$.fragment,e),Ce=s(e),B=l(e,"P",{"data-svelte-h":!0}),m(B)!=="svelte-xhhjdz"&&(B.textContent=_t),xe=s(e),S=l(e,"P",{"data-svelte-h":!0}),m(S)!=="svelte-iilyg8"&&(S.innerHTML=vt),ke=s(e),E=l(e,"P",{"data-svelte-h":!0}),m(E)!=="svelte-vfdo9a"&&(E.textContent=bt),Je=s(e),H=l(e,"P",{"data-svelte-h":!0}),m(H)!=="svelte-a2g4ox"&&(H.innerHTML=Tt),ze=s(e),F=l(e,"IMG",{src:!0,alt:!0,width:!0}),Fe=s(e),A=l(e,"SMALL",{"data-svelte-h":!0}),m(A)!=="svelte-16ml59q"&&(A.innerHTML=$t),Ie=s(e),q=l(e,"P",{"data-svelte-h":!0}),m(q)!=="svelte-3u37is"&&(q.innerHTML=wt),Ue=s(e),h(Q.$$.fragment,e),je=s(e),R=l(e,"UL",{"data-svelte-h":!0}),m(R)!=="svelte-5bb4g0"&&(R.innerHTML=Lt),Pe=s(e),h(X.$$.fragment,e),Ge=s(e),Y=l(e,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-mnlzle"&&(Y.textContent=Mt),We=s(e),h(D.$$.fragment,e),Ne=s(e),h(O.$$.fragment,e),Ze=s(e),K=l(e,"P",{"data-svelte-h":!0}),m(K)!=="svelte-15nzlnu"&&(K.innerHTML=Ct),Ve=s(e),h(ee.$$.fragment,e),Be=s(e),te=l(e,"P",{"data-svelte-h":!0}),m(te)!=="svelte-1bkfbxv"&&(te.textContent=xt),Se=s(e),h(ne.$$.fragment,e),Ee=s(e),oe=l(e,"UL",{"data-svelte-h":!0}),m(oe)!=="svelte-qnejrc"&&(oe.innerHTML=kt),He=s(e),h(ae.$$.fragment,e),Ae=s(e),$=l(e,"DIV",{class:!0});var C=Z($);h(se.$$.fragment,C),et=s(C),he=l(C,"P",{"data-svelte-h":!0}),m(he)!=="svelte-1r3a7z"&&(he.innerHTML=Jt),tt=s(C),ge=l(C,"P",{"data-svelte-h":!0}),m(ge)!=="svelte-d825u1"&&(ge.innerHTML=zt),nt=s(C),ue=l(C,"P",{"data-svelte-h":!0}),m(ue)!=="svelte-o55m63"&&(ue.innerHTML=Ft),ot=s(C),h(I.$$.fragment,C),C.forEach(n),qe=s(e),h(re.$$.fragment,e),Qe=s(e),w=l(e,"DIV",{class:!0});var x=Z(w);h(ie.$$.fragment,x),at=s(x),_e=l(x,"P",{"data-svelte-h":!0}),m(_e)!=="svelte-api29h"&&(_e.textContent=It),st=s(x),ve=l(x,"P",{"data-svelte-h":!0}),m(ve)!=="svelte-1qpkvbj"&&(ve.innerHTML=Ut),rt=s(x),U=l(x,"DIV",{class:!0});var fe=Z(U);h(le.$$.fragment,fe),it=s(fe),be=l(fe,"P",{"data-svelte-h":!0}),m(be)!=="svelte-1rpwwnm"&&(be.innerHTML=jt),fe.forEach(n),lt=s(x),j=l(x,"DIV",{class:!0});var Oe=Z(j);h(de.$$.fragment,Oe),dt=s(Oe),Te=l(Oe,"P",{"data-svelte-h":!0}),m(Te)!=="svelte-1flpnnd"&&(Te.innerHTML=Pt),Oe.forEach(n),x.forEach(n),Re=s(e),h(ce.$$.fragment,e),Xe=s(e),M=l(e,"DIV",{class:!0});var W=Z(M);h(me.$$.fragment,W),ct=s(W),ye=l(W,"P",{"data-svelte-h":!0}),m(ye)!=="svelte-1ox7dlx"&&(ye.innerHTML=Gt),mt=s(W),$e=l(W,"P",{"data-svelte-h":!0}),m($e)!=="svelte-hswkmf"&&($e.innerHTML=Wt),pt=s(W),k=l(W,"DIV",{class:!0});var N=Z(k);h(pe.$$.fragment,N),ft=s(N),we=l(N,"P",{"data-svelte-h":!0}),m(we)!=="svelte-1n3tg9h"&&(we.innerHTML=Nt),ht=s(N),h(P.$$.fragment,N),gt=s(N),h(G.$$.fragment,N),N.forEach(n),W.forEach(n),Ye=s(e),Me=l(e,"P",{}),Z(Me).forEach(n),this.h()},h(){J(d,"name","hf:doc:metadata"),J(d,"content",Ot),Bt(F.src,yt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_architecture.jpg")||J(F,"src",yt),J(F,"alt","drawing"),J(F,"width","600"),J($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){c(document.head,d),o(e,L,t),o(e,b,t),o(e,p,t),g(T,e,t),o(e,r,t),g(y,e,t),o(e,Ce,t),o(e,B,t),o(e,xe,t),o(e,S,t),o(e,ke,t),o(e,E,t),o(e,Je,t),o(e,H,t),o(e,ze,t),o(e,F,t),o(e,Fe,t),o(e,A,t),o(e,Ie,t),o(e,q,t),o(e,Ue,t),g(Q,e,t),o(e,je,t),o(e,R,t),o(e,Pe,t),g(X,e,t),o(e,Ge,t),o(e,Y,t),o(e,We,t),g(D,e,t),o(e,Ne,t),g(O,e,t),o(e,Ze,t),o(e,K,t),o(e,Ve,t),g(ee,e,t),o(e,Be,t),o(e,te,t),o(e,Se,t),g(ne,e,t),o(e,Ee,t),o(e,oe,t),o(e,He,t),g(ae,e,t),o(e,Ae,t),o(e,$,t),g(se,$,null),c($,et),c($,he),c($,tt),c($,ge),c($,nt),c($,ue),c($,ot),g(I,$,null),o(e,qe,t),g(re,e,t),o(e,Qe,t),o(e,w,t),g(ie,w,null),c(w,at),c(w,_e),c(w,st),c(w,ve),c(w,rt),c(w,U),g(le,U,null),c(U,it),c(U,be),c(w,lt),c(w,j),g(de,j,null),c(j,dt),c(j,Te),o(e,Re,t),g(ce,e,t),o(e,Xe,t),o(e,M,t),g(me,M,null),c(M,ct),c(M,ye),c(M,mt),c(M,$e),c(M,pt),c(M,k),g(pe,k,null),c(k,ft),c(k,we),c(k,ht),g(P,k,null),c(k,gt),g(G,k,null),o(e,Ye,t),o(e,Me,t),De=!0},p(e,[t]){const C={};t&2&&(C.$$scope={dirty:t,ctx:e}),I.$set(C);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),P.$set(x);const fe={};t&2&&(fe.$$scope={dirty:t,ctx:e}),G.$set(fe)},i(e){De||(u(T.$$.fragment,e),u(y.$$.fragment,e),u(Q.$$.fragment,e),u(X.$$.fragment,e),u(D.$$.fragment,e),u(O.$$.fragment,e),u(ee.$$.fragment,e),u(ne.$$.fragment,e),u(ae.$$.fragment,e),u(se.$$.fragment,e),u(I.$$.fragment,e),u(re.$$.fragment,e),u(ie.$$.fragment,e),u(le.$$.fragment,e),u(de.$$.fragment,e),u(ce.$$.fragment,e),u(me.$$.fragment,e),u(pe.$$.fragment,e),u(P.$$.fragment,e),u(G.$$.fragment,e),De=!0)},o(e){_(T.$$.fragment,e),_(y.$$.fragment,e),_(Q.$$.fragment,e),_(X.$$.fragment,e),_(D.$$.fragment,e),_(O.$$.fragment,e),_(ee.$$.fragment,e),_(ne.$$.fragment,e),_(ae.$$.fragment,e),_(se.$$.fragment,e),_(I.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(P.$$.fragment,e),_(G.$$.fragment,e),De=!1},d(e){e&&(n(L),n(b),n(p),n(r),n(Ce),n(B),n(xe),n(S),n(ke),n(E),n(Je),n(H),n(ze),n(F),n(Fe),n(A),n(Ie),n(q),n(Ue),n(je),n(R),n(Pe),n(Ge),n(Y),n(We),n(Ne),n(Ze),n(K),n(Ve),n(Be),n(te),n(Se),n(Ee),n(oe),n(He),n(Ae),n($),n(qe),n(Qe),n(w),n(Re),n(Xe),n(M),n(Ye),n(Me)),n(d),v(T,e),v(y,e),v(Q,e),v(X,e),v(D,e),v(O,e),v(ee,e),v(ne,e),v(ae,e),v(se),v(I),v(re,e),v(ie),v(le),v(de),v(ce,e),v(me),v(pe),v(P),v(G)}}}const Ot='{"title":"LLaVa","local":"llava","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[{"title":"Using Flash Attention 2","local":"using-flash-attention-2","sections":[],"depth":3}],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"LlavaConfig","local":"transformers.LlavaConfig","sections":[],"depth":2},{"title":"LlavaProcessor","local":"transformers.LlavaProcessor","sections":[],"depth":2},{"title":"LlavaForConditionalGeneration","local":"transformers.LlavaForConditionalGeneration","sections":[],"depth":2}],"depth":1}';function Kt(z){return St(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class dn extends Et{constructor(d){super(),Ht(this,d,Kt,Dt,Vt,{})}}export{dn as component};
