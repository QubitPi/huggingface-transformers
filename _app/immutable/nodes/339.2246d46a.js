import{s as Vn,o as xn,n as I}from"../chunks/scheduler.9bc65507.js";import{S as zn,i as Xn,g as M,s as i,r as d,A as Nn,h as b,f as n,c as p,j as z,u,x as T,k as Fn,y as _,a as r,v as h,d as $,t as g,w as y}from"../chunks/index.707bf1b6.js";import{T as Dt}from"../chunks/Tip.c2ecdbf4.js";import{Y as Hn}from"../chunks/Youtube.e1129c6f.js";import{C as k}from"../chunks/CodeBlock.54a9f38d.js";import{D as An}from"../chunks/DocNotebookDropdown.41f65cb5.js";import{F as zt,M as A}from"../chunks/Markdown.fef84341.js";import{H as L}from"../chunks/Heading.342b1fa6.js";function Yn(j){let s,o;return s=new k({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRvcmNo",highlighted:"pip install torch",wrap:!1}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p:I,i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function En(j){let s,o;return s=new A({props:{$$slots:{default:[Yn]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function Bn(j){let s,o;return s=new k({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRlbnNvcmZsb3c=",highlighted:"pip install tensorflow",wrap:!1}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p:I,i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function Ln(j){let s,o;return s=new A({props:{$$slots:{default:[Bn]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function qn(j){let s,o='For a complete list of available tasks, check out the <a href="./main_classes/pipelines">pipeline API reference</a>.';return{c(){s=M("p"),s.innerHTML=o},l(t){s=b(t,"P",{"data-svelte-h":!0}),T(s)!=="svelte-yyszjd"&&(s.innerHTML=o)},m(t,l){r(t,s,l)},p:I,d(t){t&&n(s)}}}function Sn(j){let s,o='Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification">AutoModelForSequenceClassification</a> and <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> to load the pretrained model and it’s associated tokenizer (more on an <code>AutoClass</code> in the next section):',t,l,f;return l=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`,wrap:!1}}),{c(){s=M("p"),s.innerHTML=o,t=i(),d(l.$$.fragment)},l(m){s=b(m,"P",{"data-svelte-h":!0}),T(s)!=="svelte-1magt38"&&(s.innerHTML=o),t=p(m),u(l.$$.fragment,m)},m(m,J){r(m,s,J),r(m,t,J),h(l,m,J),f=!0},p:I,i(m){f||($(l.$$.fragment,m),f=!0)},o(m){g(l.$$.fragment,m),f=!1},d(m){m&&(n(s),n(t)),y(l,m)}}}function Qn(j){let s,o;return s=new A({props:{$$slots:{default:[Sn]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function Pn(j){let s,o='Use <a href="/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification">TFAutoModelForSequenceClassification</a> and <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> to load the pretrained model and it’s associated tokenizer (more on an <code>TFAutoClass</code> in the next section):',t,l,f;return l=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQobW9kZWxfbmFtZSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`,wrap:!1}}),{c(){s=M("p"),s.innerHTML=o,t=i(),d(l.$$.fragment)},l(m){s=b(m,"P",{"data-svelte-h":!0}),T(s)!=="svelte-ucahb2"&&(s.innerHTML=o),t=p(m),u(l.$$.fragment,m)},m(m,J){r(m,s,J),r(m,t,J),h(l,m,J),f=!0},p:I,i(m){f||($(l.$$.fragment,m),f=!0)},o(m){g(l.$$.fragment,m),f=!1},d(m){m&&(n(s),n(t)),y(l,m)}}}function On(j){let s,o;return s=new A({props:{$$slots:{default:[Pn]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function Dn(j){let s,o;return s=new k({props:{code:"cHRfYmF0Y2glMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwJTVCJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiUyQyUyMCUyMldlJTIwaG9wZSUyMHlvdSUyMGRvbid0JTIwaGF0ZSUyMGl0LiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHBhZGRpbmclM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwdHJ1bmNhdGlvbiUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBtYXhfbGVuZ3RoJTNENTEyJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p:I,i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function Kn(j){let s,o;return s=new A({props:{$$slots:{default:[Dn]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function el(j){let s,o;return s=new k({props:{code:"dGZfYmF0Y2glMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwJTVCJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiUyQyUyMCUyMldlJTIwaG9wZSUyMHlvdSUyMGRvbid0JTIwaGF0ZSUyMGl0LiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHBhZGRpbmclM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwdHJ1bmNhdGlvbiUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBtYXhfbGVuZ3RoJTNENTEyJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMiUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p:I,i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function tl(j){let s,o;return s=new A({props:{$$slots:{default:[el]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function sl(j){let s,o='Check out the <a href="./preprocessing">preprocess</a> tutorial for more details about tokenization, and how to use an <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>, <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a> and <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoProcessor">AutoProcessor</a> to preprocess image, audio, and multimodal inputs.';return{c(){s=M("p"),s.innerHTML=o},l(t){s=b(t,"P",{"data-svelte-h":!0}),T(s)!=="svelte-1xk5c2w"&&(s.innerHTML=o)},m(t,l){r(t,s,l)},p:I,d(t){t&&n(s)}}}function al(j){let s,o='See the <a href="./task_summary">task summary</a> for tasks supported by an <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.';return{c(){s=M("p"),s.innerHTML=o},l(t){s=b(t,"P",{"data-svelte-h":!0}),T(s)!=="svelte-11k4zea"&&(s.innerHTML=o)},m(t,l){r(t,s,l)},p:I,d(t){t&&n(s)}}}function nl(j){let s,o='🤗 Transformers provides a simple and unified way to load pretrained instances. This means you can load an <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModel">AutoModel</a> like you would load an <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. The only difference is selecting the correct <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModel">AutoModel</a> for the task. For text (or sequence) classification, you should load <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification">AutoModelForSequenceClassification</a>:',t,l,f,m,J,Z,C="Now pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding <code>**</code>:",G,w,v,R,q="The model outputs the final activations in the <code>logits</code> attribute. Apply the softmax function to the <code>logits</code> to retrieve the probabilities:",F,W,H;return l=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbF9uYW1lJTIwJTNEJTIwJTIybmxwdG93biUyRmJlcnQtYmFzZS1tdWx0aWxpbmd1YWwtdW5jYXNlZC1zZW50aW1lbnQlMjIlMEFwdF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,wrap:!1}}),m=new Dt({props:{$$slots:{default:[al]},$$scope:{ctx:j}}}),w=new k({props:{code:"cHRfb3V0cHV0cyUyMCUzRCUyMHB0X21vZGVsKCoqcHRfYmF0Y2gp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)',wrap:!1}}),W=new k({props:{code:"ZnJvbSUyMHRvcmNoJTIwaW1wb3J0JTIwbm4lMEElMEFwdF9wcmVkaWN0aW9ucyUyMCUzRCUyMG5uLmZ1bmN0aW9uYWwuc29mdG1heChwdF9vdXRwdXRzLmxvZ2l0cyUyQyUyMGRpbSUzRC0xKSUwQXByaW50KHB0X3ByZWRpY3Rpb25zKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`,wrap:!1}}),{c(){s=M("p"),s.innerHTML=o,t=i(),d(l.$$.fragment),f=i(),d(m.$$.fragment),J=i(),Z=M("p"),Z.innerHTML=C,G=i(),d(w.$$.fragment),v=i(),R=M("p"),R.innerHTML=q,F=i(),d(W.$$.fragment)},l(c){s=b(c,"P",{"data-svelte-h":!0}),T(s)!=="svelte-1l97ihu"&&(s.innerHTML=o),t=p(c),u(l.$$.fragment,c),f=p(c),u(m.$$.fragment,c),J=p(c),Z=b(c,"P",{"data-svelte-h":!0}),T(Z)!=="svelte-wsvj9w"&&(Z.innerHTML=C),G=p(c),u(w.$$.fragment,c),v=p(c),R=b(c,"P",{"data-svelte-h":!0}),T(R)!=="svelte-17tdpgw"&&(R.innerHTML=q),F=p(c),u(W.$$.fragment,c)},m(c,U){r(c,s,U),r(c,t,U),h(l,c,U),r(c,f,U),h(m,c,U),r(c,J,U),r(c,Z,U),r(c,G,U),h(w,c,U),r(c,v,U),r(c,R,U),r(c,F,U),h(W,c,U),H=!0},p(c,U){const X={};U&2&&(X.$$scope={dirty:U,ctx:c}),m.$set(X)},i(c){H||($(l.$$.fragment,c),$(m.$$.fragment,c),$(w.$$.fragment,c),$(W.$$.fragment,c),H=!0)},o(c){g(l.$$.fragment,c),g(m.$$.fragment,c),g(w.$$.fragment,c),g(W.$$.fragment,c),H=!1},d(c){c&&(n(s),n(t),n(f),n(J),n(Z),n(G),n(v),n(R),n(F)),y(l,c),y(m,c),y(w,c),y(W,c)}}}function ll(j){let s,o;return s=new A({props:{$$slots:{default:[nl]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function rl(j){let s,o='See the <a href="./task_summary">task summary</a> for tasks supported by an <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModel">AutoModel</a> class.';return{c(){s=M("p"),s.innerHTML=o},l(t){s=b(t,"P",{"data-svelte-h":!0}),T(s)!=="svelte-11k4zea"&&(s.innerHTML=o)},m(t,l){r(t,s,l)},p:I,d(t){t&&n(s)}}}function ol(j){let s,o='🤗 Transformers provides a simple and unified way to load pretrained instances. This means you can load an <a href="/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModel">TFAutoModel</a> like you would load an <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. The only difference is selecting the correct <a href="/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModel">TFAutoModel</a> for the task. For text (or sequence) classification, you should load <a href="/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification">TFAutoModelForSequenceClassification</a>:',t,l,f,m,J,Z,C="Now pass your preprocessed batch of inputs directly to the model. You can pass the tensors as-is:",G,w,v,R,q="The model outputs the final activations in the <code>logits</code> attribute. Apply the softmax function to the <code>logits</code> to retrieve the probabilities:",F,W,H;return l=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsX25hbWUlMjAlM0QlMjAlMjJubHB0b3duJTJGYmVydC1iYXNlLW11bHRpbGluZ3VhbC11bmNhc2VkLXNlbnRpbWVudCUyMiUwQXRmX21vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,wrap:!1}}),m=new Dt({props:{$$slots:{default:[rl]},$$scope:{ctx:j}}}),w=new k({props:{code:"dGZfb3V0cHV0cyUyMCUzRCUyMHRmX21vZGVsKHRmX2JhdGNoKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)',wrap:!1}}),W=new k({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEF0Zl9wcmVkaWN0aW9ucyUyMCUzRCUyMHRmLm5uLnNvZnRtYXgodGZfb3V0cHV0cy5sb2dpdHMlMkMlMjBheGlzJTNELTEpJTBBdGZfcHJlZGljdGlvbnM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`,wrap:!1}}),{c(){s=M("p"),s.innerHTML=o,t=i(),d(l.$$.fragment),f=i(),d(m.$$.fragment),J=i(),Z=M("p"),Z.textContent=C,G=i(),d(w.$$.fragment),v=i(),R=M("p"),R.innerHTML=q,F=i(),d(W.$$.fragment)},l(c){s=b(c,"P",{"data-svelte-h":!0}),T(s)!=="svelte-1cx03ie"&&(s.innerHTML=o),t=p(c),u(l.$$.fragment,c),f=p(c),u(m.$$.fragment,c),J=p(c),Z=b(c,"P",{"data-svelte-h":!0}),T(Z)!=="svelte-19ydqo0"&&(Z.textContent=C),G=p(c),u(w.$$.fragment,c),v=p(c),R=b(c,"P",{"data-svelte-h":!0}),T(R)!=="svelte-17tdpgw"&&(R.innerHTML=q),F=p(c),u(W.$$.fragment,c)},m(c,U){r(c,s,U),r(c,t,U),h(l,c,U),r(c,f,U),h(m,c,U),r(c,J,U),r(c,Z,U),r(c,G,U),h(w,c,U),r(c,v,U),r(c,R,U),r(c,F,U),h(W,c,U),H=!0},p(c,U){const X={};U&2&&(X.$$scope={dirty:U,ctx:c}),m.$set(X)},i(c){H||($(l.$$.fragment,c),$(m.$$.fragment,c),$(w.$$.fragment,c),$(W.$$.fragment,c),H=!0)},o(c){g(l.$$.fragment,c),g(m.$$.fragment,c),g(w.$$.fragment,c),g(W.$$.fragment,c),H=!1},d(c){c&&(n(s),n(t),n(f),n(J),n(Z),n(G),n(v),n(R),n(F)),y(l,c),y(m,c),y(w,c),y(W,c)}}}function il(j){let s,o;return s=new A({props:{$$slots:{default:[ol]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function pl(j){let s,o=`All 🤗 Transformers models (PyTorch or TensorFlow) output the tensors <em>before</em> the final activation
function (like softmax) because the final activation function is often fused with the loss. Model outputs are special dataclasses so their attributes are autocompleted in an IDE. The model outputs behave like a tuple or a dictionary (you can index with an integer, a slice or a string) in which case, attributes that are None are ignored.`;return{c(){s=M("p"),s.innerHTML=o},l(t){s=b(t,"P",{"data-svelte-h":!0}),T(s)!=="svelte-3ji870"&&(s.innerHTML=o)},m(t,l){r(t,s,l)},p:I,d(t){t&&n(s)}}}function ml(j){let s,o='Once your model is fine-tuned, you can save it with its tokenizer using <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">PreTrainedModel.save_pretrained()</a>:',t,l,f,m,J='When you are ready to use the model again, reload it with <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">PreTrainedModel.from_pretrained()</a>:',Z,C,G;return l=new k({props:{code:"cHRfc2F2ZV9kaXJlY3RvcnklMjAlM0QlMjAlMjIuJTJGcHRfc2F2ZV9wcmV0cmFpbmVkJTIyJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZChwdF9zYXZlX2RpcmVjdG9yeSklMEFwdF9tb2RlbC5zYXZlX3ByZXRyYWluZWQocHRfc2F2ZV9kaXJlY3Rvcnkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`,wrap:!1}}),C=new k({props:{code:"cHRfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfc2F2ZV9wcmV0cmFpbmVkJTIyKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)',wrap:!1}}),{c(){s=M("p"),s.innerHTML=o,t=i(),d(l.$$.fragment),f=i(),m=M("p"),m.innerHTML=J,Z=i(),d(C.$$.fragment)},l(w){s=b(w,"P",{"data-svelte-h":!0}),T(s)!=="svelte-1dfj42p"&&(s.innerHTML=o),t=p(w),u(l.$$.fragment,w),f=p(w),m=b(w,"P",{"data-svelte-h":!0}),T(m)!=="svelte-1h7e1vx"&&(m.innerHTML=J),Z=p(w),u(C.$$.fragment,w)},m(w,v){r(w,s,v),r(w,t,v),h(l,w,v),r(w,f,v),r(w,m,v),r(w,Z,v),h(C,w,v),G=!0},p:I,i(w){G||($(l.$$.fragment,w),$(C.$$.fragment,w),G=!0)},o(w){g(l.$$.fragment,w),g(C.$$.fragment,w),G=!1},d(w){w&&(n(s),n(t),n(f),n(m),n(Z)),y(l,w),y(C,w)}}}function cl(j){let s,o;return s=new A({props:{$$slots:{default:[ml]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function fl(j){let s,o='Once your model is fine-tuned, you can save it with its tokenizer using <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained">TFPreTrainedModel.save_pretrained()</a>:',t,l,f,m,J='When you are ready to use the model again, reload it with <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">TFPreTrainedModel.from_pretrained()</a>:',Z,C,G;return l=new k({props:{code:"dGZfc2F2ZV9kaXJlY3RvcnklMjAlM0QlMjAlMjIuJTJGdGZfc2F2ZV9wcmV0cmFpbmVkJTIyJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZCh0Zl9zYXZlX2RpcmVjdG9yeSklMEF0Zl9tb2RlbC5zYXZlX3ByZXRyYWluZWQodGZfc2F2ZV9kaXJlY3Rvcnkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`,wrap:!1}}),C=new k({props:{code:"dGZfbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0Zl9zYXZlX3ByZXRyYWluZWQlMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)',wrap:!1}}),{c(){s=M("p"),s.innerHTML=o,t=i(),d(l.$$.fragment),f=i(),m=M("p"),m.innerHTML=J,Z=i(),d(C.$$.fragment)},l(w){s=b(w,"P",{"data-svelte-h":!0}),T(s)!=="svelte-1hprd39"&&(s.innerHTML=o),t=p(w),u(l.$$.fragment,w),f=p(w),m=b(w,"P",{"data-svelte-h":!0}),T(m)!=="svelte-6vlxmh"&&(m.innerHTML=J),Z=p(w),u(C.$$.fragment,w)},m(w,v){r(w,s,v),r(w,t,v),h(l,w,v),r(w,f,v),r(w,m,v),r(w,Z,v),h(C,w,v),G=!0},p:I,i(w){G||($(l.$$.fragment,w),$(C.$$.fragment,w),G=!0)},o(w){g(l.$$.fragment,w),g(C.$$.fragment,w),G=!1},d(w){w&&(n(s),n(t),n(f),n(m),n(Z)),y(l,w),y(C,w)}}}function dl(j){let s,o;return s=new A({props:{$$slots:{default:[fl]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function ul(j){let s,o;return s=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKHRmX3NhdmVfZGlyZWN0b3J5KSUwQXB0X21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQodGZfc2F2ZV9kaXJlY3RvcnklMkMlMjBmcm9tX3RmJTNEVHJ1ZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`,wrap:!1}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p:I,i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function hl(j){let s,o;return s=new A({props:{$$slots:{default:[ul]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function $l(j){let s,o;return s=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQocHRfc2F2ZV9kaXJlY3RvcnkpJTBBdGZfbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKHB0X3NhdmVfZGlyZWN0b3J5JTJDJTIwZnJvbV9wdCUzRFRydWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`,wrap:!1}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p:I,i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function gl(j){let s,o;return s=new A({props:{$$slots:{default:[$l]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function yl(j){let s,o='Create a model from your custom configuration with <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">AutoModel.from_config()</a>:',t,l,f;return l=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUwQSUwQW15X21vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fY29uZmlnKG15X2NvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = AutoModel.from_config(my_config)`,wrap:!1}}),{c(){s=M("p"),s.innerHTML=o,t=i(),d(l.$$.fragment)},l(m){s=b(m,"P",{"data-svelte-h":!0}),T(s)!=="svelte-s9ywww"&&(s.innerHTML=o),t=p(m),u(l.$$.fragment,m)},m(m,J){r(m,s,J),r(m,t,J),h(l,m,J),f=!0},p:I,i(m){f||($(l.$$.fragment,m),f=!0)},o(m){g(l.$$.fragment,m),f=!1},d(m){m&&(n(s),n(t)),y(l,m)}}}function Ml(j){let s,o;return s=new A({props:{$$slots:{default:[yl]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function bl(j){let s,o='Create a model from your custom configuration with <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">TFAutoModel.from_config()</a>:',t,l,f;return l=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsJTBBJTBBbXlfbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbC5mcm9tX2NvbmZpZyhteV9jb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = TFAutoModel.from_config(my_config)`,wrap:!1}}),{c(){s=M("p"),s.innerHTML=o,t=i(),d(l.$$.fragment)},l(m){s=b(m,"P",{"data-svelte-h":!0}),T(s)!=="svelte-ga8nfe"&&(s.innerHTML=o),t=p(m),u(l.$$.fragment,m)},m(m,J){r(m,s,J),r(m,t,J),h(l,m,J),f=!0},p:I,i(m){f||($(l.$$.fragment,m),f=!0)},o(m){g(l.$$.fragment,m),f=!1},d(m){m&&(n(s),n(t)),y(l,m)}}}function wl(j){let s,o;return s=new A({props:{$$slots:{default:[bl]},$$scope:{ctx:j}}}),{c(){d(s.$$.fragment)},l(t){u(s.$$.fragment,t)},m(t,l){h(s,t,l),o=!0},p(t,l){const f={};l&2&&(f.$$scope={dirty:l,ctx:t}),s.$set(f)},i(t){o||($(s.$$.fragment,t),o=!0)},o(t){g(s.$$.fragment,t),o=!1},d(t){y(s,t)}}}function Tl(j){let s,o='For tasks - like translation or summarization - that use a sequence-to-sequence model, use the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainer">Seq2SeqTrainer</a> and <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments">Seq2SeqTrainingArguments</a> classes instead.';return{c(){s=M("p"),s.innerHTML=o},l(t){s=b(t,"P",{"data-svelte-h":!0}),T(s)!=="svelte-1q421e1"&&(s.innerHTML=o)},m(t,l){r(t,s,l)},p:I,d(t){t&&n(s)}}}function jl(j){let s,o,t,l,f,m,J,Z,C,G='Get up and running with 🤗 Transformers! Whether you’re a developer or an everyday user, this quick tour will help you get started and show you how to use the <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> for inference, load a pretrained model and preprocessor with an <a href="./model_doc/auto">AutoClass</a>, and quickly train a model with PyTorch or TensorFlow. If you’re a beginner, we recommend checking out our tutorials or <a href="https://huggingface.co/course/chapter1/1" rel="nofollow">course</a> next for more in-depth explanations of the concepts introduced here.',w,v,R="Before you begin, make sure you have all the necessary libraries installed:",q,F,W,H,c="You’ll also need to install your preferred machine learning framework:",U,X,es,ne,ts,le,ss,re,Ya='The <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> is the easiest and fastest way to use a pretrained model for inference. You can use the <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> out-of-the-box for many tasks across different modalities, some of which are shown in the table below:',as,S,ns,oe,Ea="<thead><tr><th><strong>Task</strong></th> <th><strong>Description</strong></th> <th><strong>Modality</strong></th> <th><strong>Pipeline identifier</strong></th></tr></thead> <tbody><tr><td>Text classification</td> <td>assign a label to a given sequence of text</td> <td>NLP</td> <td>pipeline(task=“sentiment-analysis”)</td></tr> <tr><td>Text generation</td> <td>generate text given a prompt</td> <td>NLP</td> <td>pipeline(task=“text-generation”)</td></tr> <tr><td>Summarization</td> <td>generate a summary of a sequence of text or document</td> <td>NLP</td> <td>pipeline(task=“summarization”)</td></tr> <tr><td>Image classification</td> <td>assign a label to an image</td> <td>Computer vision</td> <td>pipeline(task=“image-classification”)</td></tr> <tr><td>Image segmentation</td> <td>assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation)</td> <td>Computer vision</td> <td>pipeline(task=“image-segmentation”)</td></tr> <tr><td>Object detection</td> <td>predict the bounding boxes and classes of objects in an image</td> <td>Computer vision</td> <td>pipeline(task=“object-detection”)</td></tr> <tr><td>Audio classification</td> <td>assign a label to some audio data</td> <td>Audio</td> <td>pipeline(task=“audio-classification”)</td></tr> <tr><td>Automatic speech recognition</td> <td>transcribe speech into text</td> <td>Audio</td> <td>pipeline(task=“automatic-speech-recognition”)</td></tr> <tr><td>Visual question answering</td> <td>answer a question about the image, given an image and a question</td> <td>Multimodal</td> <td>pipeline(task=“vqa”)</td></tr> <tr><td>Document question answering</td> <td>answer a question about the document, given a document and a question</td> <td>Multimodal</td> <td>pipeline(task=“document-question-answering”)</td></tr> <tr><td>Image captioning</td> <td>generate a caption for a given image</td> <td>Multimodal</td> <td>pipeline(task=“image-to-text”)</td></tr></tbody>",ls,ie,Ba='Start by creating an instance of <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> and specifying a task you want to use it for. In this guide, you’ll use the <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> for sentiment analysis as an example:',rs,pe,os,me,La='The <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> downloads and caches a default <a href="https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english" rel="nofollow">pretrained model</a> and tokenizer for sentiment analysis. Now you can use the <code>classifier</code> on your target text:',is,ce,ps,fe,qa='If you have more than one input, pass your inputs as a list to the <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> to return a list of dictionaries:',ms,de,cs,ue,Sa='The <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> can also iterate over an entire dataset for any task you like. For this example, let’s choose automatic speech recognition as our task:',fs,he,ds,$e,Qa='Load an audio dataset (see the 🤗 Datasets <a href="https://huggingface.co/docs/datasets/quickstart#audio" rel="nofollow">Quick Start</a> for more details) you’d like to iterate over. For example, load the <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">MInDS-14</a> dataset:',us,ge,hs,ye,Pa=`You need to make sure the sampling rate of the dataset matches the sampling
rate <a href="https://huggingface.co/facebook/wav2vec2-base-960h" rel="nofollow"><code>facebook/wav2vec2-base-960h</code></a> was trained on:`,$s,Me,gs,be,Oa=`The audio files are automatically loaded and resampled when calling the <code>&quot;audio&quot;</code> column.
Extract the raw waveform arrays from the first 4 samples and pass it as a list to the pipeline:`,ys,we,Ms,Te,Da='For larger datasets where the inputs are big (like in speech or vision), you’ll want to pass a generator instead of a list to load all the inputs in memory. Take a look at the <a href="./main_classes/pipelines">pipeline API reference</a> for more information.',bs,je,ws,ke,Ka='The <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> can accommodate any model from the <a href="https://huggingface.co/models" rel="nofollow">Hub</a>, making it easy to adapt the <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> for other use-cases. For example, if you’d like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual <a href="https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment" rel="nofollow">BERT model</a> finetuned for sentiment analysis you can use for French text:',Ts,_e,js,Q,ks,Je,en='Specify the model and tokenizer in the <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a>, and now you can apply the <code>classifier</code> on French text:',_s,Ue,Js,ve,tn='If you can’t find a model for your use-case, you’ll need to finetune a pretrained model on your data. Take a look at our <a href="./training">finetuning tutorial</a> to learn how. Finally, after you’ve finetuned your pretrained model, please consider <a href="./model_sharing">sharing</a> the model with the community on the Hub to democratize machine learning for everyone! 🤗',Us,Ze,vs,Ce,Zs,Ge,sn='Under the hood, the <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification">AutoModelForSequenceClassification</a> and <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> classes work together to power the <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a> you used above. An <a href="./model_doc/auto">AutoClass</a> is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriate <code>AutoClass</code> for your task and it’s associated preprocessing class.',Cs,We,an='Let’s return to the example from the previous section and see how you can use the <code>AutoClass</code> to replicate the results of the <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a>.',Gs,Re,Ws,Ie,nn='A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the <a href="./tokenizer_summary">tokenizer summary</a>). The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you’re using the same tokenization rules a model was pretrained with.',Rs,Fe,ln='Load a tokenizer with <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>:',Is,He,Fs,Ve,rn="Pass your text to the tokenizer:",Hs,xe,Vs,ze,on="The tokenizer returns a dictionary containing:",xs,Xe,pn='<li><a href="./glossary#input-ids">input_ids</a>: numerical representations of your tokens.</li> <li><a href=".glossary#attention-mask">attention_mask</a>: indicates which tokens should be attended to.</li>',zs,Ne,mn="A tokenizer can also accept a list of inputs, and pad and truncate the text to return a batch with uniform length:",Xs,P,Ns,O,As,Ae,Ys,D,Es,K,Bs,Ye,Ls,ee,qs,Ee,cn="One particularly cool 🤗 Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The <code>from_pt</code> or <code>from_tf</code> parameter can convert the model from one framework to the other:",Ss,te,Qs,Be,Ps,Le,fn="You can modify the model’s configuration class to change how a model is built. The configuration specifies a model’s attributes, such as the number of hidden layers or attention heads. You start from scratch when you initialize a model from a custom configuration class. The model attributes are randomly initialized, and you’ll need to train the model before you can use it to get meaningful results.",Os,qe,dn='Start by importing <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig">AutoConfig</a>, and then load the pretrained model you want to modify. Within <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig.from_pretrained">AutoConfig.from_pretrained()</a>, you can specify the attribute you want to change, such as the number of attention heads:',Ds,Se,Ks,se,ea,Qe,un='Take a look at the <a href="./create_a_model">Create a custom architecture</a> guide for more information about building custom configurations.',ta,Pe,sa,Oe,hn='All models are a standard <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow"><code>torch.nn.Module</code></a> so you can use them in any typical training loop. While you can write your own training loop, 🤗 Transformers provides a <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> class for PyTorch, which contains the basic training loop and adds additional functionality for features like distributed training, mixed precision, and more.',aa,De,$n='Depending on your task, you’ll typically pass the following parameters to <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a>:',na,V,Ke,Xt,gn='You’ll start with a <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a> or a <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow"><code>torch.nn.Module</code></a>:',wa,et,Ta,tt,Nt,yn='<a href="/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> contains the model hyperparameters you can change like learning rate, batch size, and the number of epochs to train for. The default values are used if you don’t specify any training arguments:',ja,st,ka,at,At,Mn="Load a preprocessing class like a tokenizer, image processor, feature extractor, or processor:",_a,nt,Ja,lt,Yt,bn="Load a dataset:",Ua,rt,va,E,Et,wn="Create a function to tokenize the dataset:",Za,ot,Ca,Bt,Tn='Then apply it over the entire dataset with <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map" rel="nofollow">map</a>:',Ga,it,Wa,pt,Lt,jn='A <a href="/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding">DataCollatorWithPadding</a> to create a batch of examples from your dataset:',Ra,mt,la,ct,kn='Now gather all these classes in <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a>:',ra,ft,oa,dt,_n='When you’re ready, call <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train">train()</a> to start training:',ia,ut,pa,ae,ma,ht,Jn='You can customize the training loop behavior by subclassing the methods inside <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a>. This allows you to customize features such as the loss function, optimizer, and scheduler. Take a look at the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> reference for which methods can be subclassed.',ca,$t,Un='The other way to customize the training loop is by using <a href="./main_classes/callbacks">Callbacks</a>. You can use callbacks to integrate with other libraries and inspect the training loop to report on progress or stop the training early. Callbacks do not modify anything in the training loop itself. To customize something like the loss function, you need to subclass the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> instead.',fa,gt,da,yt,vn='All models are a standard <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow"><code>tf.keras.Model</code></a> so they can be trained in TensorFlow with the <a href="https://keras.io/" rel="nofollow">Keras</a> API. 🤗 Transformers provides the <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset">prepare_tf_dataset()</a> method to easily load your dataset as a <code>tf.data.Dataset</code> so you can start training right away with Keras’ <a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a> and <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a> methods.',ua,N,Mt,qt,Zn='You’ll start with a <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a> or a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow"><code>tf.keras.Model</code></a>:',Ia,bt,Fa,wt,St,Cn="Load a preprocessing class like a tokenizer, image processor, feature extractor, or processor:",Ha,Tt,Va,jt,Qt,Gn="Create a function to tokenize the dataset:",xa,kt,za,_t,Pt,Wn='Apply the tokenizer over the entire dataset with <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map" rel="nofollow">map</a> and then pass the dataset and tokenizer to <a href="/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset">prepare_tf_dataset()</a>. You can also change the batch size and shuffle the dataset here if you’d like:',Xa,Jt,Na,Ut,Ot,Rn="When you’re ready, you can call <code>compile</code> and <code>fit</code> to start training. Note that Transformers models all have a default task-relevant loss function, so you don’t need to specify one unless you want to:",Aa,vt,ha,Zt,$a,Ct,In="Now that you’ve completed the 🤗 Transformers quick tour, check out our guides and learn how to do more specific things like writing a custom model, fine-tuning a model for a task, and how to train a model with a script. If you’re interested in learning more about 🤗 Transformers core concepts, grab a cup of coffee and take a look at our Conceptual Guides!",ga,Kt,ya;return f=new L({props:{title:"Quick tour",local:"quick-tour",headingTag:"h1"}}),J=new An({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/quicktour.ipynb"}]}}),F=new k({props:{code:"IXBpcCUyMGluc3RhbGwlMjB0cmFuc2Zvcm1lcnMlMjBkYXRhc2V0cw==",highlighted:"!pip install transformers datasets",wrap:!1}}),X=new zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ln],pytorch:[En]},$$scope:{ctx:j}}}),ne=new L({props:{title:"Pipeline",local:"pipeline",headingTag:"h2"}}),le=new Hn({props:{id:"tiZFewofSLM"}}),S=new Dt({props:{$$slots:{default:[qn]},$$scope:{ctx:j}}}),pe=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMnNlbnRpbWVudC1hbmFseXNpcyUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`,wrap:!1}}),ce=new k({props:{code:"Y2xhc3NpZmllciglMjJXZSUyMGFyZSUyMHZlcnklMjBoYXBweSUyMHRvJTIwc2hvdyUyMHlvdSUyMHRoZSUyMCVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycyUyMGxpYnJhcnkuJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`,wrap:!1}}),de=new k({props:{code:"cmVzdWx0cyUyMCUzRCUyMGNsYXNzaWZpZXIoJTVCJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiUyQyUyMCUyMldlJTIwaG9wZSUyMHlvdSUyMGRvbid0JTIwaGF0ZSUyMGl0LiUyMiU1RCklMEFmb3IlMjByZXN1bHQlMjBpbiUyMHJlc3VsdHMlM0ElMEElMjAlMjAlMjAlMjBwcmludChmJTIybGFiZWwlM0ElMjAlN0JyZXN1bHQlNUInbGFiZWwnJTVEJTdEJTJDJTIwd2l0aCUyMHNjb3JlJTNBJTIwJTdCcm91bmQocmVzdWx0JTVCJ3Njb3JlJyU1RCUyQyUyMDQpJTdEJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`,wrap:!1}}),he=new k({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFzcGVlY2hfcmVjb2duaXplciUyMCUzRCUyMHBpcGVsaW5lKCUyMmF1dG9tYXRpYy1zcGVlY2gtcmVjb2duaXRpb24lMjIlMkMlMjBtb2RlbCUzRCUyMmZhY2Vib29rJTJGd2F2MnZlYzItYmFzZS05NjBoJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`,wrap:!1}}),ge=new k({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMlBvbHlBSSUyRm1pbmRzMTQlMjIlMkMlMjBuYW1lJTNEJTIyZW4tVVMlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`,wrap:!1}}),Me=new k({props:{code:"ZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQuY2FzdF9jb2x1bW4oJTIyYXVkaW8lMjIlMkMlMjBBdWRpbyhzYW1wbGluZ19yYXRlJTNEc3BlZWNoX3JlY29nbml6ZXIuZmVhdHVyZV9leHRyYWN0b3Iuc2FtcGxpbmdfcmF0ZSkp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))',wrap:!1}}),we=new k({props:{code:"cmVzdWx0JTIwJTNEJTIwc3BlZWNoX3JlY29nbml6ZXIoZGF0YXNldCU1QiUzQTQlNUQlNUIlMjJhdWRpbyUyMiU1RCklMEFwcmludCglNUJkJTVCJTIydGV4dCUyMiU1RCUyMGZvciUyMGQlMjBpbiUyMHJlc3VsdCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>result = speech_recognizer(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([d[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> result])
[<span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>, <span class="hljs-string">&quot;FONDERING HOW I&#x27;D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>, <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>, <span class="hljs-string">&#x27;HOW DO I FURN A JOINA COUT&#x27;</span>]`,wrap:!1}}),je=new L({props:{title:"Use another model and tokenizer in the pipeline",local:"use-another-model-and-tokenizer-in-the-pipeline",headingTag:"h3"}}),_e=new k({props:{code:"bW9kZWxfbmFtZSUyMCUzRCUyMCUyMm5scHRvd24lMkZiZXJ0LWJhc2UtbXVsdGlsaW5ndWFsLXVuY2FzZWQtc2VudGltZW50JTIy",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>',wrap:!1}}),Q=new zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[On],pytorch:[Qn]},$$scope:{ctx:j}}}),Ue=new k({props:{code:"Y2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMnNlbnRpbWVudC1hbmFseXNpcyUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIpJTBBY2xhc3NpZmllciglMjJOb3VzJTIwc29tbWVzJTIwdHIlQzMlQThzJTIwaGV1cmV1eCUyMGRlJTIwdm91cyUyMHByJUMzJUE5c2VudGVyJTIwbGElMjBiaWJsaW90aCVDMyVBOHF1ZSUyMCVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycy4lMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`,wrap:!1}}),Ze=new L({props:{title:"AutoClass",local:"autoclass",headingTag:"h2"}}),Ce=new Hn({props:{id:"AhChOFRegn4"}}),Re=new L({props:{title:"AutoTokenizer",local:"autotokenizer",headingTag:"h3"}}),He=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEFtb2RlbF9uYW1lJTIwJTNEJTIwJTIybmxwdG93biUyRmJlcnQtYmFzZS1tdWx0aWxpbmd1YWwtdW5jYXNlZC1zZW50aW1lbnQlMjIlMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`,wrap:!1}}),xe=new k({props:{code:"ZW5jb2RpbmclMjAlM0QlMjB0b2tlbml6ZXIoJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiklMEFwcmludChlbmNvZGluZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the 🤗 Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`,wrap:!1}}),P=new zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[tl],pytorch:[Kn]},$$scope:{ctx:j}}}),O=new Dt({props:{$$slots:{default:[sl]},$$scope:{ctx:j}}}),Ae=new L({props:{title:"AutoModel",local:"automodel",headingTag:"h3"}}),D=new zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[il],pytorch:[ll]},$$scope:{ctx:j}}}),K=new Dt({props:{$$slots:{default:[pl]},$$scope:{ctx:j}}}),Ye=new L({props:{title:"Save a model",local:"save-a-model",headingTag:"h3"}}),ee=new zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[dl],pytorch:[cl]},$$scope:{ctx:j}}}),te=new zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[gl],pytorch:[hl]},$$scope:{ctx:j}}}),Be=new L({props:{title:"Custom model builds",local:"custom-model-builds",headingTag:"h2"}}),Se=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMEElMEFteV9jb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIlMkMlMjBuX2hlYWRzJTNEMTIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>my_config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>, n_heads=<span class="hljs-number">12</span>)`,wrap:!1}}),se=new zt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[wl],pytorch:[Ml]},$$scope:{ctx:j}}}),Pe=new L({props:{title:"Trainer - a PyTorch optimized training loop",local:"trainer---a-pytorch-optimized-training-loop",headingTag:"h2"}}),et=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),st=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluaW5nQXJndW1lbnRzJTBBJTBBdHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJwYXRoJTJGdG8lMkZzYXZlJTJGZm9sZGVyJTJGJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlM0Q4JTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV9ldmFsX2JhdGNoX3NpemUlM0Q4JTJDJTBBJTIwJTIwJTIwJTIwbnVtX3RyYWluX2Vwb2NocyUzRDIlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;path/to/save/folder/&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">8</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">8</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),nt=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),rt=new k({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJyb3R0ZW5fdG9tYXRvZXMlMjIpJTIwJTIwJTIzJTIwZG9jdGVzdCUzQSUyMCUyQklHTk9SRV9SRVNVTFQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;rotten_tomatoes&quot;</span>)  <span class="hljs-comment"># doctest: +IGNORE_RESULT</span>`,wrap:!1}}),ot=new k({props:{code:"ZGVmJTIwdG9rZW5pemVfZGF0YXNldChkYXRhc2V0KSUzQSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMHRva2VuaXplcihkYXRhc2V0JTVCJTIydGV4dCUyMiU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_dataset</span>(<span class="hljs-params">dataset</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(dataset[<span class="hljs-string">&quot;text&quot;</span>])`,wrap:!1}}),it=new k({props:{code:"ZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQubWFwKHRva2VuaXplX2RhdGFzZXQlMkMlMjBiYXRjaGVkJTNEVHJ1ZSk=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(tokenize_dataset, batched=<span class="hljs-literal">True</span>)',wrap:!1}}),mt=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nJTBBJTBBZGF0YV9jb2xsYXRvciUyMCUzRCUyMERhdGFDb2xsYXRvcldpdGhQYWRkaW5nKHRva2VuaXplciUzRHRva2VuaXplcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorWithPadding

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorWithPadding(tokenizer=tokenizer)`,wrap:!1}}),ft=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluZXIlMEElMEF0cmFpbmVyJTIwJTNEJTIwVHJhaW5lciglMEElMjAlMjAlMjAlMjBtb2RlbCUzRG1vZGVsJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjB0cmFpbl9kYXRhc2V0JTNEZGF0YXNldCU1QiUyMnRyYWluJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9kYXRhc2V0JTNEZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlMkMlMEElMjAlMjAlMjAlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMEElMjAlMjAlMjAlMjBkYXRhX2NvbGxhdG9yJTNEZGF0YV9jb2xsYXRvciUyQyUwQSklMjAlMjAlMjMlMjBkb2N0ZXN0JTNBJTIwJTJCU0tJUA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=dataset[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=dataset[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>)  <span class="hljs-comment"># doctest: +SKIP</span>`,wrap:!1}}),ut=new k({props:{code:"dHJhaW5lci50cmFpbigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()',wrap:!1}}),ae=new Dt({props:{$$slots:{default:[Tl]},$$scope:{ctx:j}}}),gt=new L({props:{title:"Train with TensorFlow",local:"train-with-tensorflow",headingTag:"h2"}}),bt=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),Tt=new k({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)`,wrap:!1}}),kt=new k({props:{code:"ZGVmJTIwdG9rZW5pemVfZGF0YXNldChkYXRhc2V0KSUzQSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMHRva2VuaXplcihkYXRhc2V0JTVCJTIydGV4dCUyMiU1RCklMjAlMjAlMjMlMjBkb2N0ZXN0JTNBJTIwJTJCU0tJUA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_dataset</span>(<span class="hljs-params">dataset</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(dataset[<span class="hljs-string">&quot;text&quot;</span>])  <span class="hljs-comment"># doctest: +SKIP</span>`,wrap:!1}}),Jt=new k({props:{code:"ZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQubWFwKHRva2VuaXplX2RhdGFzZXQpJTIwJTIwJTIzJTIwZG9jdGVzdCUzQSUyMCUyQlNLSVAlMEF0Zl9kYXRhc2V0JTIwJTNEJTIwbW9kZWwucHJlcGFyZV90Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMGRhdGFzZXQlNUIlMjJ0cmFpbiUyMiU1RCUyQyUyMGJhdGNoX3NpemUlM0QxNiUyQyUyMHNodWZmbGUlM0RUcnVlJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyJTBBKSUyMCUyMCUyMyUyMGRvY3Rlc3QlM0ElMjAlMkJTS0lQ",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.<span class="hljs-built_in">map</span>(tokenize_dataset)  <span class="hljs-comment"># doctest: +SKIP</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_dataset = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    dataset[<span class="hljs-string">&quot;train&quot;</span>], batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>, tokenizer=tokenizer
<span class="hljs-meta">... </span>)  <span class="hljs-comment"># doctest: +SKIP</span>`,wrap:!1}}),vt=new k({props:{code:"ZnJvbSUyMHRlbnNvcmZsb3cua2VyYXMub3B0aW1pemVycyUyMGltcG9ydCUyMEFkYW0lMEElMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzREFkYW0oM2UtNSkpJTIwJTIwJTIzJTIwTm8lMjBsb3NzJTIwYXJndW1lbnQhJTBBbW9kZWwuZml0KHRmX2RhdGFzZXQpJTIwJTIwJTIzJTIwZG9jdGVzdCUzQSUyMCUyQlNLSVA=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">3e-5</span>))  <span class="hljs-comment"># No loss argument!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tf_dataset)  <span class="hljs-comment"># doctest: +SKIP</span>`,wrap:!1}}),Zt=new L({props:{title:"What’s next?",local:"whats-next",headingTag:"h2"}}),{c(){s=M("meta"),o=i(),t=M("p"),l=i(),d(f.$$.fragment),m=i(),d(J.$$.fragment),Z=i(),C=M("p"),C.innerHTML=G,w=i(),v=M("p"),v.textContent=R,q=i(),d(F.$$.fragment),W=i(),H=M("p"),H.textContent=c,U=i(),d(X.$$.fragment),es=i(),d(ne.$$.fragment),ts=i(),d(le.$$.fragment),ss=i(),re=M("p"),re.innerHTML=Ya,as=i(),d(S.$$.fragment),ns=i(),oe=M("table"),oe.innerHTML=Ea,ls=i(),ie=M("p"),ie.innerHTML=Ba,rs=i(),d(pe.$$.fragment),os=i(),me=M("p"),me.innerHTML=La,is=i(),d(ce.$$.fragment),ps=i(),fe=M("p"),fe.innerHTML=qa,ms=i(),d(de.$$.fragment),cs=i(),ue=M("p"),ue.innerHTML=Sa,fs=i(),d(he.$$.fragment),ds=i(),$e=M("p"),$e.innerHTML=Qa,us=i(),d(ge.$$.fragment),hs=i(),ye=M("p"),ye.innerHTML=Pa,$s=i(),d(Me.$$.fragment),gs=i(),be=M("p"),be.innerHTML=Oa,ys=i(),d(we.$$.fragment),Ms=i(),Te=M("p"),Te.innerHTML=Da,bs=i(),d(je.$$.fragment),ws=i(),ke=M("p"),ke.innerHTML=Ka,Ts=i(),d(_e.$$.fragment),js=i(),d(Q.$$.fragment),ks=i(),Je=M("p"),Je.innerHTML=en,_s=i(),d(Ue.$$.fragment),Js=i(),ve=M("p"),ve.innerHTML=tn,Us=i(),d(Ze.$$.fragment),vs=i(),d(Ce.$$.fragment),Zs=i(),Ge=M("p"),Ge.innerHTML=sn,Cs=i(),We=M("p"),We.innerHTML=an,Gs=i(),d(Re.$$.fragment),Ws=i(),Ie=M("p"),Ie.innerHTML=nn,Rs=i(),Fe=M("p"),Fe.innerHTML=ln,Is=i(),d(He.$$.fragment),Fs=i(),Ve=M("p"),Ve.textContent=rn,Hs=i(),d(xe.$$.fragment),Vs=i(),ze=M("p"),ze.textContent=on,xs=i(),Xe=M("ul"),Xe.innerHTML=pn,zs=i(),Ne=M("p"),Ne.textContent=mn,Xs=i(),d(P.$$.fragment),Ns=i(),d(O.$$.fragment),As=i(),d(Ae.$$.fragment),Ys=i(),d(D.$$.fragment),Es=i(),d(K.$$.fragment),Bs=i(),d(Ye.$$.fragment),Ls=i(),d(ee.$$.fragment),qs=i(),Ee=M("p"),Ee.innerHTML=cn,Ss=i(),d(te.$$.fragment),Qs=i(),d(Be.$$.fragment),Ps=i(),Le=M("p"),Le.textContent=fn,Os=i(),qe=M("p"),qe.innerHTML=dn,Ds=i(),d(Se.$$.fragment),Ks=i(),d(se.$$.fragment),ea=i(),Qe=M("p"),Qe.innerHTML=un,ta=i(),d(Pe.$$.fragment),sa=i(),Oe=M("p"),Oe.innerHTML=hn,aa=i(),De=M("p"),De.innerHTML=$n,na=i(),V=M("ol"),Ke=M("li"),Xt=M("p"),Xt.innerHTML=gn,wa=i(),d(et.$$.fragment),Ta=i(),tt=M("li"),Nt=M("p"),Nt.innerHTML=yn,ja=i(),d(st.$$.fragment),ka=i(),at=M("li"),At=M("p"),At.textContent=Mn,_a=i(),d(nt.$$.fragment),Ja=i(),lt=M("li"),Yt=M("p"),Yt.textContent=bn,Ua=i(),d(rt.$$.fragment),va=i(),E=M("li"),Et=M("p"),Et.textContent=wn,Za=i(),d(ot.$$.fragment),Ca=i(),Bt=M("p"),Bt.innerHTML=Tn,Ga=i(),d(it.$$.fragment),Wa=i(),pt=M("li"),Lt=M("p"),Lt.innerHTML=jn,Ra=i(),d(mt.$$.fragment),la=i(),ct=M("p"),ct.innerHTML=kn,ra=i(),d(ft.$$.fragment),oa=i(),dt=M("p"),dt.innerHTML=_n,ia=i(),d(ut.$$.fragment),pa=i(),d(ae.$$.fragment),ma=i(),ht=M("p"),ht.innerHTML=Jn,ca=i(),$t=M("p"),$t.innerHTML=Un,fa=i(),d(gt.$$.fragment),da=i(),yt=M("p"),yt.innerHTML=vn,ua=i(),N=M("ol"),Mt=M("li"),qt=M("p"),qt.innerHTML=Zn,Ia=i(),d(bt.$$.fragment),Fa=i(),wt=M("li"),St=M("p"),St.textContent=Cn,Ha=i(),d(Tt.$$.fragment),Va=i(),jt=M("li"),Qt=M("p"),Qt.textContent=Gn,xa=i(),d(kt.$$.fragment),za=i(),_t=M("li"),Pt=M("p"),Pt.innerHTML=Wn,Xa=i(),d(Jt.$$.fragment),Na=i(),Ut=M("li"),Ot=M("p"),Ot.innerHTML=Rn,Aa=i(),d(vt.$$.fragment),ha=i(),d(Zt.$$.fragment),$a=i(),Ct=M("p"),Ct.textContent=In,ga=i(),Kt=M("p"),this.h()},l(e){const a=Nn("svelte-u9bgzb",document.head);s=b(a,"META",{name:!0,content:!0}),a.forEach(n),o=p(e),t=b(e,"P",{}),z(t).forEach(n),l=p(e),u(f.$$.fragment,e),m=p(e),u(J.$$.fragment,e),Z=p(e),C=b(e,"P",{"data-svelte-h":!0}),T(C)!=="svelte-qu8i7k"&&(C.innerHTML=G),w=p(e),v=b(e,"P",{"data-svelte-h":!0}),T(v)!=="svelte-1c9nexd"&&(v.textContent=R),q=p(e),u(F.$$.fragment,e),W=p(e),H=b(e,"P",{"data-svelte-h":!0}),T(H)!=="svelte-1nw764u"&&(H.textContent=c),U=p(e),u(X.$$.fragment,e),es=p(e),u(ne.$$.fragment,e),ts=p(e),u(le.$$.fragment,e),ss=p(e),re=b(e,"P",{"data-svelte-h":!0}),T(re)!=="svelte-1dtnm1c"&&(re.innerHTML=Ya),as=p(e),u(S.$$.fragment,e),ns=p(e),oe=b(e,"TABLE",{"data-svelte-h":!0}),T(oe)!=="svelte-x282na"&&(oe.innerHTML=Ea),ls=p(e),ie=b(e,"P",{"data-svelte-h":!0}),T(ie)!=="svelte-1s3aj34"&&(ie.innerHTML=Ba),rs=p(e),u(pe.$$.fragment,e),os=p(e),me=b(e,"P",{"data-svelte-h":!0}),T(me)!=="svelte-kx5i7j"&&(me.innerHTML=La),is=p(e),u(ce.$$.fragment,e),ps=p(e),fe=b(e,"P",{"data-svelte-h":!0}),T(fe)!=="svelte-144so8s"&&(fe.innerHTML=qa),ms=p(e),u(de.$$.fragment,e),cs=p(e),ue=b(e,"P",{"data-svelte-h":!0}),T(ue)!=="svelte-9fh64d"&&(ue.innerHTML=Sa),fs=p(e),u(he.$$.fragment,e),ds=p(e),$e=b(e,"P",{"data-svelte-h":!0}),T($e)!=="svelte-1gp521x"&&($e.innerHTML=Qa),us=p(e),u(ge.$$.fragment,e),hs=p(e),ye=b(e,"P",{"data-svelte-h":!0}),T(ye)!=="svelte-8z5qlz"&&(ye.innerHTML=Pa),$s=p(e),u(Me.$$.fragment,e),gs=p(e),be=b(e,"P",{"data-svelte-h":!0}),T(be)!=="svelte-ue0ub3"&&(be.innerHTML=Oa),ys=p(e),u(we.$$.fragment,e),Ms=p(e),Te=b(e,"P",{"data-svelte-h":!0}),T(Te)!=="svelte-1q1ewi4"&&(Te.innerHTML=Da),bs=p(e),u(je.$$.fragment,e),ws=p(e),ke=b(e,"P",{"data-svelte-h":!0}),T(ke)!=="svelte-11eqp1z"&&(ke.innerHTML=Ka),Ts=p(e),u(_e.$$.fragment,e),js=p(e),u(Q.$$.fragment,e),ks=p(e),Je=b(e,"P",{"data-svelte-h":!0}),T(Je)!=="svelte-z8qwnn"&&(Je.innerHTML=en),_s=p(e),u(Ue.$$.fragment,e),Js=p(e),ve=b(e,"P",{"data-svelte-h":!0}),T(ve)!=="svelte-1tkpcvw"&&(ve.innerHTML=tn),Us=p(e),u(Ze.$$.fragment,e),vs=p(e),u(Ce.$$.fragment,e),Zs=p(e),Ge=b(e,"P",{"data-svelte-h":!0}),T(Ge)!=="svelte-13zqyls"&&(Ge.innerHTML=sn),Cs=p(e),We=b(e,"P",{"data-svelte-h":!0}),T(We)!=="svelte-vgd6up"&&(We.innerHTML=an),Gs=p(e),u(Re.$$.fragment,e),Ws=p(e),Ie=b(e,"P",{"data-svelte-h":!0}),T(Ie)!=="svelte-1s6s1n"&&(Ie.innerHTML=nn),Rs=p(e),Fe=b(e,"P",{"data-svelte-h":!0}),T(Fe)!=="svelte-1ce7t4m"&&(Fe.innerHTML=ln),Is=p(e),u(He.$$.fragment,e),Fs=p(e),Ve=b(e,"P",{"data-svelte-h":!0}),T(Ve)!=="svelte-1c0z2pg"&&(Ve.textContent=rn),Hs=p(e),u(xe.$$.fragment,e),Vs=p(e),ze=b(e,"P",{"data-svelte-h":!0}),T(ze)!=="svelte-19i2zcw"&&(ze.textContent=on),xs=p(e),Xe=b(e,"UL",{"data-svelte-h":!0}),T(Xe)!=="svelte-5dpeuy"&&(Xe.innerHTML=pn),zs=p(e),Ne=b(e,"P",{"data-svelte-h":!0}),T(Ne)!=="svelte-3sx4bx"&&(Ne.textContent=mn),Xs=p(e),u(P.$$.fragment,e),Ns=p(e),u(O.$$.fragment,e),As=p(e),u(Ae.$$.fragment,e),Ys=p(e),u(D.$$.fragment,e),Es=p(e),u(K.$$.fragment,e),Bs=p(e),u(Ye.$$.fragment,e),Ls=p(e),u(ee.$$.fragment,e),qs=p(e),Ee=b(e,"P",{"data-svelte-h":!0}),T(Ee)!=="svelte-10grpgy"&&(Ee.innerHTML=cn),Ss=p(e),u(te.$$.fragment,e),Qs=p(e),u(Be.$$.fragment,e),Ps=p(e),Le=b(e,"P",{"data-svelte-h":!0}),T(Le)!=="svelte-8iah3b"&&(Le.textContent=fn),Os=p(e),qe=b(e,"P",{"data-svelte-h":!0}),T(qe)!=="svelte-1c0rueq"&&(qe.innerHTML=dn),Ds=p(e),u(Se.$$.fragment,e),Ks=p(e),u(se.$$.fragment,e),ea=p(e),Qe=b(e,"P",{"data-svelte-h":!0}),T(Qe)!=="svelte-1ld89qp"&&(Qe.innerHTML=un),ta=p(e),u(Pe.$$.fragment,e),sa=p(e),Oe=b(e,"P",{"data-svelte-h":!0}),T(Oe)!=="svelte-urakba"&&(Oe.innerHTML=hn),aa=p(e),De=b(e,"P",{"data-svelte-h":!0}),T(De)!=="svelte-1b7ydqw"&&(De.innerHTML=$n),na=p(e),V=b(e,"OL",{});var x=z(V);Ke=b(x,"LI",{});var Gt=z(Ke);Xt=b(Gt,"P",{"data-svelte-h":!0}),T(Xt)!=="svelte-1c8pjui"&&(Xt.innerHTML=gn),wa=p(Gt),u(et.$$.fragment,Gt),Gt.forEach(n),Ta=p(x),tt=b(x,"LI",{});var Wt=z(tt);Nt=b(Wt,"P",{"data-svelte-h":!0}),T(Nt)!=="svelte-18xr32e"&&(Nt.innerHTML=yn),ja=p(Wt),u(st.$$.fragment,Wt),Wt.forEach(n),ka=p(x),at=b(x,"LI",{});var Rt=z(at);At=b(Rt,"P",{"data-svelte-h":!0}),T(At)!=="svelte-1ov2exk"&&(At.textContent=Mn),_a=p(Rt),u(nt.$$.fragment,Rt),Rt.forEach(n),Ja=p(x),lt=b(x,"LI",{});var It=z(lt);Yt=b(It,"P",{"data-svelte-h":!0}),T(Yt)!=="svelte-1u8gaxj"&&(Yt.textContent=bn),Ua=p(It),u(rt.$$.fragment,It),It.forEach(n),va=p(x),E=b(x,"LI",{});var B=z(E);Et=b(B,"P",{"data-svelte-h":!0}),T(Et)!=="svelte-1u6z95a"&&(Et.textContent=wn),Za=p(B),u(ot.$$.fragment,B),Ca=p(B),Bt=b(B,"P",{"data-svelte-h":!0}),T(Bt)!=="svelte-1fz1ica"&&(Bt.innerHTML=Tn),Ga=p(B),u(it.$$.fragment,B),B.forEach(n),Wa=p(x),pt=b(x,"LI",{});var Ft=z(pt);Lt=b(Ft,"P",{"data-svelte-h":!0}),T(Lt)!=="svelte-uat4v0"&&(Lt.innerHTML=jn),Ra=p(Ft),u(mt.$$.fragment,Ft),Ft.forEach(n),x.forEach(n),la=p(e),ct=b(e,"P",{"data-svelte-h":!0}),T(ct)!=="svelte-1a9nun4"&&(ct.innerHTML=kn),ra=p(e),u(ft.$$.fragment,e),oa=p(e),dt=b(e,"P",{"data-svelte-h":!0}),T(dt)!=="svelte-1rn5goj"&&(dt.innerHTML=_n),ia=p(e),u(ut.$$.fragment,e),pa=p(e),u(ae.$$.fragment,e),ma=p(e),ht=b(e,"P",{"data-svelte-h":!0}),T(ht)!=="svelte-11fl78x"&&(ht.innerHTML=Jn),ca=p(e),$t=b(e,"P",{"data-svelte-h":!0}),T($t)!=="svelte-422c8p"&&($t.innerHTML=Un),fa=p(e),u(gt.$$.fragment,e),da=p(e),yt=b(e,"P",{"data-svelte-h":!0}),T(yt)!=="svelte-gae27c"&&(yt.innerHTML=vn),ua=p(e),N=b(e,"OL",{});var Y=z(N);Mt=b(Y,"LI",{});var Ht=z(Mt);qt=b(Ht,"P",{"data-svelte-h":!0}),T(qt)!=="svelte-137rzzn"&&(qt.innerHTML=Zn),Ia=p(Ht),u(bt.$$.fragment,Ht),Ht.forEach(n),Fa=p(Y),wt=b(Y,"LI",{});var Vt=z(wt);St=b(Vt,"P",{"data-svelte-h":!0}),T(St)!=="svelte-1ov2exk"&&(St.textContent=Cn),Ha=p(Vt),u(Tt.$$.fragment,Vt),Vt.forEach(n),Va=p(Y),jt=b(Y,"LI",{});var xt=z(jt);Qt=b(xt,"P",{"data-svelte-h":!0}),T(Qt)!=="svelte-1u6z95a"&&(Qt.textContent=Gn),xa=p(xt),u(kt.$$.fragment,xt),xt.forEach(n),za=p(Y),_t=b(Y,"LI",{});var Ma=z(_t);Pt=b(Ma,"P",{"data-svelte-h":!0}),T(Pt)!=="svelte-11qq3xg"&&(Pt.innerHTML=Wn),Xa=p(Ma),u(Jt.$$.fragment,Ma),Ma.forEach(n),Na=p(Y),Ut=b(Y,"LI",{});var ba=z(Ut);Ot=b(ba,"P",{"data-svelte-h":!0}),T(Ot)!=="svelte-1map0wi"&&(Ot.innerHTML=Rn),Aa=p(ba),u(vt.$$.fragment,ba),ba.forEach(n),Y.forEach(n),ha=p(e),u(Zt.$$.fragment,e),$a=p(e),Ct=b(e,"P",{"data-svelte-h":!0}),T(Ct)!=="svelte-gn9in9"&&(Ct.textContent=In),ga=p(e),Kt=b(e,"P",{}),z(Kt).forEach(n),this.h()},h(){Fn(s,"name","hf:doc:metadata"),Fn(s,"content",kl)},m(e,a){_(document.head,s),r(e,o,a),r(e,t,a),r(e,l,a),h(f,e,a),r(e,m,a),h(J,e,a),r(e,Z,a),r(e,C,a),r(e,w,a),r(e,v,a),r(e,q,a),h(F,e,a),r(e,W,a),r(e,H,a),r(e,U,a),h(X,e,a),r(e,es,a),h(ne,e,a),r(e,ts,a),h(le,e,a),r(e,ss,a),r(e,re,a),r(e,as,a),h(S,e,a),r(e,ns,a),r(e,oe,a),r(e,ls,a),r(e,ie,a),r(e,rs,a),h(pe,e,a),r(e,os,a),r(e,me,a),r(e,is,a),h(ce,e,a),r(e,ps,a),r(e,fe,a),r(e,ms,a),h(de,e,a),r(e,cs,a),r(e,ue,a),r(e,fs,a),h(he,e,a),r(e,ds,a),r(e,$e,a),r(e,us,a),h(ge,e,a),r(e,hs,a),r(e,ye,a),r(e,$s,a),h(Me,e,a),r(e,gs,a),r(e,be,a),r(e,ys,a),h(we,e,a),r(e,Ms,a),r(e,Te,a),r(e,bs,a),h(je,e,a),r(e,ws,a),r(e,ke,a),r(e,Ts,a),h(_e,e,a),r(e,js,a),h(Q,e,a),r(e,ks,a),r(e,Je,a),r(e,_s,a),h(Ue,e,a),r(e,Js,a),r(e,ve,a),r(e,Us,a),h(Ze,e,a),r(e,vs,a),h(Ce,e,a),r(e,Zs,a),r(e,Ge,a),r(e,Cs,a),r(e,We,a),r(e,Gs,a),h(Re,e,a),r(e,Ws,a),r(e,Ie,a),r(e,Rs,a),r(e,Fe,a),r(e,Is,a),h(He,e,a),r(e,Fs,a),r(e,Ve,a),r(e,Hs,a),h(xe,e,a),r(e,Vs,a),r(e,ze,a),r(e,xs,a),r(e,Xe,a),r(e,zs,a),r(e,Ne,a),r(e,Xs,a),h(P,e,a),r(e,Ns,a),h(O,e,a),r(e,As,a),h(Ae,e,a),r(e,Ys,a),h(D,e,a),r(e,Es,a),h(K,e,a),r(e,Bs,a),h(Ye,e,a),r(e,Ls,a),h(ee,e,a),r(e,qs,a),r(e,Ee,a),r(e,Ss,a),h(te,e,a),r(e,Qs,a),h(Be,e,a),r(e,Ps,a),r(e,Le,a),r(e,Os,a),r(e,qe,a),r(e,Ds,a),h(Se,e,a),r(e,Ks,a),h(se,e,a),r(e,ea,a),r(e,Qe,a),r(e,ta,a),h(Pe,e,a),r(e,sa,a),r(e,Oe,a),r(e,aa,a),r(e,De,a),r(e,na,a),r(e,V,a),_(V,Ke),_(Ke,Xt),_(Ke,wa),h(et,Ke,null),_(V,Ta),_(V,tt),_(tt,Nt),_(tt,ja),h(st,tt,null),_(V,ka),_(V,at),_(at,At),_(at,_a),h(nt,at,null),_(V,Ja),_(V,lt),_(lt,Yt),_(lt,Ua),h(rt,lt,null),_(V,va),_(V,E),_(E,Et),_(E,Za),h(ot,E,null),_(E,Ca),_(E,Bt),_(E,Ga),h(it,E,null),_(V,Wa),_(V,pt),_(pt,Lt),_(pt,Ra),h(mt,pt,null),r(e,la,a),r(e,ct,a),r(e,ra,a),h(ft,e,a),r(e,oa,a),r(e,dt,a),r(e,ia,a),h(ut,e,a),r(e,pa,a),h(ae,e,a),r(e,ma,a),r(e,ht,a),r(e,ca,a),r(e,$t,a),r(e,fa,a),h(gt,e,a),r(e,da,a),r(e,yt,a),r(e,ua,a),r(e,N,a),_(N,Mt),_(Mt,qt),_(Mt,Ia),h(bt,Mt,null),_(N,Fa),_(N,wt),_(wt,St),_(wt,Ha),h(Tt,wt,null),_(N,Va),_(N,jt),_(jt,Qt),_(jt,xa),h(kt,jt,null),_(N,za),_(N,_t),_(_t,Pt),_(_t,Xa),h(Jt,_t,null),_(N,Na),_(N,Ut),_(Ut,Ot),_(Ut,Aa),h(vt,Ut,null),r(e,ha,a),h(Zt,e,a),r(e,$a,a),r(e,Ct,a),r(e,ga,a),r(e,Kt,a),ya=!0},p(e,[a]){const x={};a&2&&(x.$$scope={dirty:a,ctx:e}),X.$set(x);const Gt={};a&2&&(Gt.$$scope={dirty:a,ctx:e}),S.$set(Gt);const Wt={};a&2&&(Wt.$$scope={dirty:a,ctx:e}),Q.$set(Wt);const Rt={};a&2&&(Rt.$$scope={dirty:a,ctx:e}),P.$set(Rt);const It={};a&2&&(It.$$scope={dirty:a,ctx:e}),O.$set(It);const B={};a&2&&(B.$$scope={dirty:a,ctx:e}),D.$set(B);const Ft={};a&2&&(Ft.$$scope={dirty:a,ctx:e}),K.$set(Ft);const Y={};a&2&&(Y.$$scope={dirty:a,ctx:e}),ee.$set(Y);const Ht={};a&2&&(Ht.$$scope={dirty:a,ctx:e}),te.$set(Ht);const Vt={};a&2&&(Vt.$$scope={dirty:a,ctx:e}),se.$set(Vt);const xt={};a&2&&(xt.$$scope={dirty:a,ctx:e}),ae.$set(xt)},i(e){ya||($(f.$$.fragment,e),$(J.$$.fragment,e),$(F.$$.fragment,e),$(X.$$.fragment,e),$(ne.$$.fragment,e),$(le.$$.fragment,e),$(S.$$.fragment,e),$(pe.$$.fragment,e),$(ce.$$.fragment,e),$(de.$$.fragment,e),$(he.$$.fragment,e),$(ge.$$.fragment,e),$(Me.$$.fragment,e),$(we.$$.fragment,e),$(je.$$.fragment,e),$(_e.$$.fragment,e),$(Q.$$.fragment,e),$(Ue.$$.fragment,e),$(Ze.$$.fragment,e),$(Ce.$$.fragment,e),$(Re.$$.fragment,e),$(He.$$.fragment,e),$(xe.$$.fragment,e),$(P.$$.fragment,e),$(O.$$.fragment,e),$(Ae.$$.fragment,e),$(D.$$.fragment,e),$(K.$$.fragment,e),$(Ye.$$.fragment,e),$(ee.$$.fragment,e),$(te.$$.fragment,e),$(Be.$$.fragment,e),$(Se.$$.fragment,e),$(se.$$.fragment,e),$(Pe.$$.fragment,e),$(et.$$.fragment,e),$(st.$$.fragment,e),$(nt.$$.fragment,e),$(rt.$$.fragment,e),$(ot.$$.fragment,e),$(it.$$.fragment,e),$(mt.$$.fragment,e),$(ft.$$.fragment,e),$(ut.$$.fragment,e),$(ae.$$.fragment,e),$(gt.$$.fragment,e),$(bt.$$.fragment,e),$(Tt.$$.fragment,e),$(kt.$$.fragment,e),$(Jt.$$.fragment,e),$(vt.$$.fragment,e),$(Zt.$$.fragment,e),ya=!0)},o(e){g(f.$$.fragment,e),g(J.$$.fragment,e),g(F.$$.fragment,e),g(X.$$.fragment,e),g(ne.$$.fragment,e),g(le.$$.fragment,e),g(S.$$.fragment,e),g(pe.$$.fragment,e),g(ce.$$.fragment,e),g(de.$$.fragment,e),g(he.$$.fragment,e),g(ge.$$.fragment,e),g(Me.$$.fragment,e),g(we.$$.fragment,e),g(je.$$.fragment,e),g(_e.$$.fragment,e),g(Q.$$.fragment,e),g(Ue.$$.fragment,e),g(Ze.$$.fragment,e),g(Ce.$$.fragment,e),g(Re.$$.fragment,e),g(He.$$.fragment,e),g(xe.$$.fragment,e),g(P.$$.fragment,e),g(O.$$.fragment,e),g(Ae.$$.fragment,e),g(D.$$.fragment,e),g(K.$$.fragment,e),g(Ye.$$.fragment,e),g(ee.$$.fragment,e),g(te.$$.fragment,e),g(Be.$$.fragment,e),g(Se.$$.fragment,e),g(se.$$.fragment,e),g(Pe.$$.fragment,e),g(et.$$.fragment,e),g(st.$$.fragment,e),g(nt.$$.fragment,e),g(rt.$$.fragment,e),g(ot.$$.fragment,e),g(it.$$.fragment,e),g(mt.$$.fragment,e),g(ft.$$.fragment,e),g(ut.$$.fragment,e),g(ae.$$.fragment,e),g(gt.$$.fragment,e),g(bt.$$.fragment,e),g(Tt.$$.fragment,e),g(kt.$$.fragment,e),g(Jt.$$.fragment,e),g(vt.$$.fragment,e),g(Zt.$$.fragment,e),ya=!1},d(e){e&&(n(o),n(t),n(l),n(m),n(Z),n(C),n(w),n(v),n(q),n(W),n(H),n(U),n(es),n(ts),n(ss),n(re),n(as),n(ns),n(oe),n(ls),n(ie),n(rs),n(os),n(me),n(is),n(ps),n(fe),n(ms),n(cs),n(ue),n(fs),n(ds),n($e),n(us),n(hs),n(ye),n($s),n(gs),n(be),n(ys),n(Ms),n(Te),n(bs),n(ws),n(ke),n(Ts),n(js),n(ks),n(Je),n(_s),n(Js),n(ve),n(Us),n(vs),n(Zs),n(Ge),n(Cs),n(We),n(Gs),n(Ws),n(Ie),n(Rs),n(Fe),n(Is),n(Fs),n(Ve),n(Hs),n(Vs),n(ze),n(xs),n(Xe),n(zs),n(Ne),n(Xs),n(Ns),n(As),n(Ys),n(Es),n(Bs),n(Ls),n(qs),n(Ee),n(Ss),n(Qs),n(Ps),n(Le),n(Os),n(qe),n(Ds),n(Ks),n(ea),n(Qe),n(ta),n(sa),n(Oe),n(aa),n(De),n(na),n(V),n(la),n(ct),n(ra),n(oa),n(dt),n(ia),n(pa),n(ma),n(ht),n(ca),n($t),n(fa),n(da),n(yt),n(ua),n(N),n(ha),n($a),n(Ct),n(ga),n(Kt)),n(s),y(f,e),y(J,e),y(F,e),y(X,e),y(ne,e),y(le,e),y(S,e),y(pe,e),y(ce,e),y(de,e),y(he,e),y(ge,e),y(Me,e),y(we,e),y(je,e),y(_e,e),y(Q,e),y(Ue,e),y(Ze,e),y(Ce,e),y(Re,e),y(He,e),y(xe,e),y(P,e),y(O,e),y(Ae,e),y(D,e),y(K,e),y(Ye,e),y(ee,e),y(te,e),y(Be,e),y(Se,e),y(se,e),y(Pe,e),y(et),y(st),y(nt),y(rt),y(ot),y(it),y(mt),y(ft,e),y(ut,e),y(ae,e),y(gt,e),y(bt),y(Tt),y(kt),y(Jt),y(vt),y(Zt,e)}}}const kl='{"title":"Quick tour","local":"quick-tour","sections":[{"title":"Pipeline","local":"pipeline","sections":[{"title":"Use another model and tokenizer in the pipeline","local":"use-another-model-and-tokenizer-in-the-pipeline","sections":[],"depth":3}],"depth":2},{"title":"AutoClass","local":"autoclass","sections":[{"title":"AutoTokenizer","local":"autotokenizer","sections":[],"depth":3},{"title":"AutoModel","local":"automodel","sections":[],"depth":3},{"title":"Save a model","local":"save-a-model","sections":[],"depth":3}],"depth":2},{"title":"Custom model builds","local":"custom-model-builds","sections":[],"depth":2},{"title":"Trainer - a PyTorch optimized training loop","local":"trainer---a-pytorch-optimized-training-loop","sections":[],"depth":2},{"title":"Train with TensorFlow","local":"train-with-tensorflow","sections":[],"depth":2},{"title":"What’s next?","local":"whats-next","sections":[],"depth":2}],"depth":1}';function _l(j){return xn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Il extends zn{constructor(s){super(),Xn(this,s,_l,jl,Vn,{})}}export{Il as component};
