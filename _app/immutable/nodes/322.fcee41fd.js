import{s as Ae,o as De,n as ke}from"../chunks/scheduler.9bc65507.js";import{S as Ke,i as et,g as a,s as r,r as d,A as tt,h as s,f as n,c as l,j as Qe,u as h,x as p,k as Oe,y as nt,a as o,v as T,d as y,t as w,w as $}from"../chunks/index.707bf1b6.js";import{T as Ce}from"../chunks/Tip.c2ecdbf4.js";import{C as A}from"../chunks/CodeBlock.54a9f38d.js";import{H as D}from"../chunks/Heading.342b1fa6.js";function ot(g){let i,c='BetterTransformer is not supported for all models. Check this <a href="https://huggingface.co/docs/optimum/bettertransformer/overview#supported-models" rel="nofollow">list</a> to see if a model supports BetterTransformer.';return{c(){i=a("p"),i.innerHTML=c},l(m){i=s(m,"P",{"data-svelte-h":!0}),p(i)!=="svelte-scbfw6"&&(i.innerHTML=c)},m(m,f){o(m,i,f)},p:ke,d(m){m&&n(i)}}}function it(g){let i,c="For PyTorch &gt;= 1.14.0, JIT-mode could benefit any model for prediction and evaluation since the dict input is supported in <code>jit.trace</code>.",m,f,b="For PyTorch &lt; 1.14.0, JIT-mode could benefit a model if its forward parameter order matches the tuple input order in <code>jit.trace</code>, such as a question-answering model. If the forward parameter order does not match the tuple input order in <code>jit.trace</code>, like a text classification model, <code>jit.trace</code> will fail and we are capturing this with the exception here to make it fallback. Logging is used to notify users.";return{c(){i=a("p"),i.innerHTML=c,m=r(),f=a("p"),f.innerHTML=b},l(u){i=s(u,"P",{"data-svelte-h":!0}),p(i)!=="svelte-t0nelt"&&(i.innerHTML=c),m=l(u),f=s(u,"P",{"data-svelte-h":!0}),p(f)!=="svelte-t4iueh"&&(f.innerHTML=b)},m(u,M){o(u,i,M),o(u,m,M),o(u,f,M)},p:ke,d(u){u&&(n(i),n(m),n(f))}}}function rt(g){let i,c='Learn more details about using ORT with ü§ó Optimum in the <a href="https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models" rel="nofollow">Optimum Inference with ONNX Runtime</a> guide. This section only provides a brief and simple example.';return{c(){i=a("p"),i.innerHTML=c},l(m){i=s(m,"P",{"data-svelte-h":!0}),p(i)!=="svelte-awtbyy"&&(i.innerHTML=c)},m(m,f){o(m,i,f)},p:ke,d(m){m&&n(i)}}}function lt(g){let i,c,m,f,b,u,M,Ie="With some optimizations, it is possible to efficiently run large model inference on a CPU. One of these optimization techniques involves compiling the PyTorch code into an intermediate format for high-performance environments like C++. The other technique fuses multiple operations into one kernel to reduce the overhead of running each operation separately.",K,x,Xe='You‚Äôll learn how to use <a href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/" rel="nofollow">BetterTransformer</a> for faster inference, and how to convert your PyTorch code to <a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html" rel="nofollow">TorchScript</a>. If you‚Äôre using an Intel CPU, you can also use <a href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features.html#graph-optimization" rel="nofollow">graph optimizations</a> from <a href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/index.html" rel="nofollow">Intel Extension for PyTorch</a> to boost inference speed even more. Finally, learn how to use ü§ó Optimum to accelerate inference with ONNX Runtime or OpenVINO (if you‚Äôre using an Intel CPU).',ee,J,te,C,Le="BetterTransformer accelerates inference with its fastpath (native PyTorch specialized implementation of Transformer functions) execution. The two optimizations in the fastpath execution are:",ne,k,Be="<li>fusion, which combines multiple sequential operations into a single ‚Äúkernel‚Äù to reduce the number of computation steps</li> <li>skipping the inherent sparsity of padding tokens to avoid unnecessary computation with nested tensors</li>",oe,I,Pe='BetterTransformer also converts all attention operations to use the more memory-efficient <a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention" rel="nofollow">scaled dot product attention</a>.',ie,_,re,X,je='Before you start, make sure you have ü§ó Optimum <a href="https://huggingface.co/docs/optimum/installation" rel="nofollow">installed</a>.',le,L,We='Enable BetterTransformer with the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.to_bettertransformer">PreTrainedModel.to_bettertransformer()</a> method:',ae,B,se,P,me,j,Ze='TorchScript is an intermediate PyTorch model representation that can be run in production environments where performance is important. You can train a model in PyTorch and then export it to TorchScript to free the model from Python performance constraints. PyTorch <a href="https://pytorch.org/docs/stable/generated/torch.jit.trace.html" rel="nofollow">traces</a> a model to return a <code>ScriptFunction</code> that is optimized with just-in-time compilation (JIT). Compared to the default eager mode, JIT mode in PyTorch typically yields better performance for inference using optimization techniques like operator fusion.',pe,W,He='For a gentle introduction to TorchScript, see the <a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html" rel="nofollow">Introduction to PyTorch TorchScript</a> tutorial.',fe,Z,Se='With the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> class, you can enable JIT mode for CPU inference by setting the <code>--jit_mode_eval</code> flag:',ue,H,ce,v,de,S,he,V,Ve='Intel¬Æ Extension for PyTorch (IPEX) provides further optimizations in JIT mode for Intel CPUs, and we recommend combining it with TorchScript for even faster performance. The IPEX <a href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/graph_optimization.html" rel="nofollow">graph optimization</a> fuses operations like Multi-head attention, Concat Linear, Linear + Add, Linear + Gelu, Add + LayerNorm, and more.',Te,R,Re='To take advantage of these graph optimizations, make sure you have IPEX <a href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html" rel="nofollow">installed</a>:',ye,F,we,G,Fe='Set the <code>--use_ipex</code> and <code>--jit_mode_eval</code> flags in the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> class to enable JIT mode with the graph optimizations:',$e,z,Me,q,be,U,ge,Y,Ge='ONNX Runtime (ORT) is a model accelerator that runs inference on CPUs by default. ORT is supported by ü§ó Optimum which can be used in ü§ó Transformers, without making too many changes to your code. You only need to replace the ü§ó Transformers <code>AutoClass</code> with its equivalent <a href="https://huggingface.co/docs/optimum/main/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel" rel="nofollow">ORTModel</a> for the task you‚Äôre solving, and load a checkpoint in the ONNX format.',_e,E,ze='For example, if you‚Äôre running inference on a question answering task, load the <a href="https://huggingface.co/optimum/roberta-base-squad2" rel="nofollow">optimum/roberta-base-squad2</a> checkpoint which contains a <code>model.onnx</code> file:',ve,N,Ue,Q,qe='If you have an Intel CPU, take a look at ü§ó <a href="https://huggingface.co/docs/optimum/intel/index" rel="nofollow">Optimum Intel</a> which supports a variety of compression techniques (quantization, pruning, knowledge distillation) and tools for converting models to the <a href="https://huggingface.co/docs/optimum/intel/inference" rel="nofollow">OpenVINO</a> format for higher performance inference.',xe,O,Je;return b=new D({props:{title:"CPU inference",local:"cpu-inference",headingTag:"h1"}}),J=new D({props:{title:"BetterTransformer",local:"bettertransformer",headingTag:"h2"}}),_=new Ce({props:{$$slots:{default:[ot]},$$scope:{ctx:g}}}),B=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyYmlnY29kZSUyRnN0YXJjb2RlciUyMiklMEFtb2RlbC50b19iZXR0ZXJ0cmFuc2Zvcm1lcigp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigcode/starcoder&quot;</span>)
model.to_bettertransformer()`,wrap:!1}}),P=new D({props:{title:"TorchScript",local:"torchscript",headingTag:"h2"}}),H=new A({props:{code:"cHl0aG9uJTIwcnVuX3FhLnB5JTIwJTVDJTBBLS1tb2RlbF9uYW1lX29yX3BhdGglMjBjc2Fycm9uJTJGYmVydC1iYXNlLXVuY2FzZWQtc3F1YWQtdjElMjAlNUMlMEEtLWRhdGFzZXRfbmFtZSUyMHNxdWFkJTIwJTVDJTBBLS1kb19ldmFsJTIwJTVDJTBBLS1tYXhfc2VxX2xlbmd0aCUyMDM4NCUyMCU1QyUwQS0tZG9jX3N0cmlkZSUyMDEyOCUyMCU1QyUwQS0tb3V0cHV0X2RpciUyMCUyRnRtcCUyRiUyMCU1QyUwQS0tbm9fY3VkYSUyMCU1QyUwQS0taml0X21vZGVfZXZhbA==",highlighted:`python run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
--jit_mode_eval`,wrap:!1}}),v=new Ce({props:{warning:!0,$$slots:{default:[it]},$$scope:{ctx:g}}}),S=new D({props:{title:"IPEX graph optimization",local:"ipex-graph-optimization",headingTag:"h2"}}),F=new A({props:{code:"cGlwJTIwaW5zdGFsbCUyMGludGVsX2V4dGVuc2lvbl9mb3JfcHl0b3JjaA==",highlighted:"pip install intel_extension_for_pytorch",wrap:!1}}),z=new A({props:{code:"cHl0aG9uJTIwcnVuX3FhLnB5JTIwJTVDJTBBLS1tb2RlbF9uYW1lX29yX3BhdGglMjBjc2Fycm9uJTJGYmVydC1iYXNlLXVuY2FzZWQtc3F1YWQtdjElMjAlNUMlMEEtLWRhdGFzZXRfbmFtZSUyMHNxdWFkJTIwJTVDJTBBLS1kb19ldmFsJTIwJTVDJTBBLS1tYXhfc2VxX2xlbmd0aCUyMDM4NCUyMCU1QyUwQS0tZG9jX3N0cmlkZSUyMDEyOCUyMCU1QyUwQS0tb3V0cHV0X2RpciUyMCUyRnRtcCUyRiUyMCU1QyUwQS0tbm9fY3VkYSUyMCU1QyUwQS0tdXNlX2lwZXglMjAlNUMlMEEtLWppdF9tb2RlX2V2YWw=",highlighted:`python run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
--use_ipex \\
--jit_mode_eval`,wrap:!1}}),q=new D({props:{title:"ü§ó Optimum",local:"-optimum",headingTag:"h2"}}),U=new Ce({props:{$$slots:{default:[rt]},$$scope:{ctx:g}}}),N=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBwaXBlbGluZSUwQWZyb20lMjBvcHRpbXVtLm9ubnhydW50aW1lJTIwaW1wb3J0JTIwT1JUTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQW1vZGVsJTIwJTNEJTIwT1JUTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyb3B0aW11bSUyRnJvYmVydGEtYmFzZS1zcXVhZDIlMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZGVlcHNldCUyRnJvYmVydGEtYmFzZS1zcXVhZDIlMjIpJTBBJTBBb25ueF9xYSUyMCUzRCUyMHBpcGVsaW5lKCUyMnF1ZXN0aW9uLWFuc3dlcmluZyUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIpJTBBJTBBcXVlc3Rpb24lMjAlM0QlMjAlMjJXaGF0J3MlMjBteSUyMG5hbWUlM0YlMjIlMEFjb250ZXh0JTIwJTNEJTIwJTIyTXklMjBuYW1lJTIwaXMlMjBQaGlsaXBwJTIwYW5kJTIwSSUyMGxpdmUlMjBpbiUyME51cmVtYmVyZy4lMjIlMEFwcmVkJTIwJTNEJTIwb25ueF9xYShxdWVzdGlvbiUyQyUyMGNvbnRleHQp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, pipeline
<span class="hljs-keyword">from</span> optimum.onnxruntime <span class="hljs-keyword">import</span> ORTModelForQuestionAnswering

model = ORTModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;optimum/roberta-base-squad2&quot;</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;deepset/roberta-base-squad2&quot;</span>)

onnx_qa = pipeline(<span class="hljs-string">&quot;question-answering&quot;</span>, model=model, tokenizer=tokenizer)

question = <span class="hljs-string">&quot;What&#x27;s my name?&quot;</span>
context = <span class="hljs-string">&quot;My name is Philipp and I live in Nuremberg.&quot;</span>
pred = onnx_qa(question, context)`,wrap:!1}}),{c(){i=a("meta"),c=r(),m=a("p"),f=r(),d(b.$$.fragment),u=r(),M=a("p"),M.textContent=Ie,K=r(),x=a("p"),x.innerHTML=Xe,ee=r(),d(J.$$.fragment),te=r(),C=a("p"),C.textContent=Le,ne=r(),k=a("ol"),k.innerHTML=Be,oe=r(),I=a("p"),I.innerHTML=Pe,ie=r(),d(_.$$.fragment),re=r(),X=a("p"),X.innerHTML=je,le=r(),L=a("p"),L.innerHTML=We,ae=r(),d(B.$$.fragment),se=r(),d(P.$$.fragment),me=r(),j=a("p"),j.innerHTML=Ze,pe=r(),W=a("p"),W.innerHTML=He,fe=r(),Z=a("p"),Z.innerHTML=Se,ue=r(),d(H.$$.fragment),ce=r(),d(v.$$.fragment),de=r(),d(S.$$.fragment),he=r(),V=a("p"),V.innerHTML=Ve,Te=r(),R=a("p"),R.innerHTML=Re,ye=r(),d(F.$$.fragment),we=r(),G=a("p"),G.innerHTML=Fe,$e=r(),d(z.$$.fragment),Me=r(),d(q.$$.fragment),be=r(),d(U.$$.fragment),ge=r(),Y=a("p"),Y.innerHTML=Ge,_e=r(),E=a("p"),E.innerHTML=ze,ve=r(),d(N.$$.fragment),Ue=r(),Q=a("p"),Q.innerHTML=qe,xe=r(),O=a("p"),this.h()},l(e){const t=tt("svelte-u9bgzb",document.head);i=s(t,"META",{name:!0,content:!0}),t.forEach(n),c=l(e),m=s(e,"P",{}),Qe(m).forEach(n),f=l(e),h(b.$$.fragment,e),u=l(e),M=s(e,"P",{"data-svelte-h":!0}),p(M)!=="svelte-1i824r1"&&(M.textContent=Ie),K=l(e),x=s(e,"P",{"data-svelte-h":!0}),p(x)!=="svelte-18xf8m9"&&(x.innerHTML=Xe),ee=l(e),h(J.$$.fragment,e),te=l(e),C=s(e,"P",{"data-svelte-h":!0}),p(C)!=="svelte-mytrcu"&&(C.textContent=Le),ne=l(e),k=s(e,"OL",{"data-svelte-h":!0}),p(k)!=="svelte-1b2ln7l"&&(k.innerHTML=Be),oe=l(e),I=s(e,"P",{"data-svelte-h":!0}),p(I)!=="svelte-1ntl6a3"&&(I.innerHTML=Pe),ie=l(e),h(_.$$.fragment,e),re=l(e),X=s(e,"P",{"data-svelte-h":!0}),p(X)!=="svelte-1qwvxdv"&&(X.innerHTML=je),le=l(e),L=s(e,"P",{"data-svelte-h":!0}),p(L)!=="svelte-1ufbxrn"&&(L.innerHTML=We),ae=l(e),h(B.$$.fragment,e),se=l(e),h(P.$$.fragment,e),me=l(e),j=s(e,"P",{"data-svelte-h":!0}),p(j)!=="svelte-kgrwv1"&&(j.innerHTML=Ze),pe=l(e),W=s(e,"P",{"data-svelte-h":!0}),p(W)!=="svelte-i0x3az"&&(W.innerHTML=He),fe=l(e),Z=s(e,"P",{"data-svelte-h":!0}),p(Z)!=="svelte-sano8o"&&(Z.innerHTML=Se),ue=l(e),h(H.$$.fragment,e),ce=l(e),h(v.$$.fragment,e),de=l(e),h(S.$$.fragment,e),he=l(e),V=s(e,"P",{"data-svelte-h":!0}),p(V)!=="svelte-1yofu8o"&&(V.innerHTML=Ve),Te=l(e),R=s(e,"P",{"data-svelte-h":!0}),p(R)!=="svelte-1xouf9s"&&(R.innerHTML=Re),ye=l(e),h(F.$$.fragment,e),we=l(e),G=s(e,"P",{"data-svelte-h":!0}),p(G)!=="svelte-1mtujyw"&&(G.innerHTML=Fe),$e=l(e),h(z.$$.fragment,e),Me=l(e),h(q.$$.fragment,e),be=l(e),h(U.$$.fragment,e),ge=l(e),Y=s(e,"P",{"data-svelte-h":!0}),p(Y)!=="svelte-flw3r7"&&(Y.innerHTML=Ge),_e=l(e),E=s(e,"P",{"data-svelte-h":!0}),p(E)!=="svelte-1khkfx8"&&(E.innerHTML=ze),ve=l(e),h(N.$$.fragment,e),Ue=l(e),Q=s(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-1ko1nlh"&&(Q.innerHTML=qe),xe=l(e),O=s(e,"P",{}),Qe(O).forEach(n),this.h()},h(){Oe(i,"name","hf:doc:metadata"),Oe(i,"content",at)},m(e,t){nt(document.head,i),o(e,c,t),o(e,m,t),o(e,f,t),T(b,e,t),o(e,u,t),o(e,M,t),o(e,K,t),o(e,x,t),o(e,ee,t),T(J,e,t),o(e,te,t),o(e,C,t),o(e,ne,t),o(e,k,t),o(e,oe,t),o(e,I,t),o(e,ie,t),T(_,e,t),o(e,re,t),o(e,X,t),o(e,le,t),o(e,L,t),o(e,ae,t),T(B,e,t),o(e,se,t),T(P,e,t),o(e,me,t),o(e,j,t),o(e,pe,t),o(e,W,t),o(e,fe,t),o(e,Z,t),o(e,ue,t),T(H,e,t),o(e,ce,t),T(v,e,t),o(e,de,t),T(S,e,t),o(e,he,t),o(e,V,t),o(e,Te,t),o(e,R,t),o(e,ye,t),T(F,e,t),o(e,we,t),o(e,G,t),o(e,$e,t),T(z,e,t),o(e,Me,t),T(q,e,t),o(e,be,t),T(U,e,t),o(e,ge,t),o(e,Y,t),o(e,_e,t),o(e,E,t),o(e,ve,t),T(N,e,t),o(e,Ue,t),o(e,Q,t),o(e,xe,t),o(e,O,t),Je=!0},p(e,[t]){const Ye={};t&2&&(Ye.$$scope={dirty:t,ctx:e}),_.$set(Ye);const Ee={};t&2&&(Ee.$$scope={dirty:t,ctx:e}),v.$set(Ee);const Ne={};t&2&&(Ne.$$scope={dirty:t,ctx:e}),U.$set(Ne)},i(e){Je||(y(b.$$.fragment,e),y(J.$$.fragment,e),y(_.$$.fragment,e),y(B.$$.fragment,e),y(P.$$.fragment,e),y(H.$$.fragment,e),y(v.$$.fragment,e),y(S.$$.fragment,e),y(F.$$.fragment,e),y(z.$$.fragment,e),y(q.$$.fragment,e),y(U.$$.fragment,e),y(N.$$.fragment,e),Je=!0)},o(e){w(b.$$.fragment,e),w(J.$$.fragment,e),w(_.$$.fragment,e),w(B.$$.fragment,e),w(P.$$.fragment,e),w(H.$$.fragment,e),w(v.$$.fragment,e),w(S.$$.fragment,e),w(F.$$.fragment,e),w(z.$$.fragment,e),w(q.$$.fragment,e),w(U.$$.fragment,e),w(N.$$.fragment,e),Je=!1},d(e){e&&(n(c),n(m),n(f),n(u),n(M),n(K),n(x),n(ee),n(te),n(C),n(ne),n(k),n(oe),n(I),n(ie),n(re),n(X),n(le),n(L),n(ae),n(se),n(me),n(j),n(pe),n(W),n(fe),n(Z),n(ue),n(ce),n(de),n(he),n(V),n(Te),n(R),n(ye),n(we),n(G),n($e),n(Me),n(be),n(ge),n(Y),n(_e),n(E),n(ve),n(Ue),n(Q),n(xe),n(O)),n(i),$(b,e),$(J,e),$(_,e),$(B,e),$(P,e),$(H,e),$(v,e),$(S,e),$(F,e),$(z,e),$(q,e),$(U,e),$(N,e)}}}const at='{"title":"CPU inference","local":"cpu-inference","sections":[{"title":"BetterTransformer","local":"bettertransformer","sections":[],"depth":2},{"title":"TorchScript","local":"torchscript","sections":[],"depth":2},{"title":"IPEX graph optimization","local":"ipex-graph-optimization","sections":[],"depth":2},{"title":"ü§ó Optimum","local":"-optimum","sections":[],"depth":2}],"depth":1}';function st(g){return De(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class dt extends Ke{constructor(i){super(),et(this,i,st,lt,Ae,{})}}export{dt as component};
