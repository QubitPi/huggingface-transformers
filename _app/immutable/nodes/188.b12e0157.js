import{s as ia,o as la,n as N}from"../chunks/scheduler.9bc65507.js";import{S as da,i as ca,g as d,s as a,r as u,A as pa,h as c,f as n,c as s,j as P,u as h,x as p,k as B,l as ma,y as i,a as r,v as f,d as g,t as _,w as M}from"../chunks/index.707bf1b6.js";import{T as at}from"../chunks/Tip.c2ecdbf4.js";import{D as S}from"../chunks/Docstring.17db21ae.js";import{C as Y}from"../chunks/CodeBlock.54a9f38d.js";import{E as $n}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as U}from"../chunks/Heading.342b1fa6.js";function ua(v){let o,b;return o=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1pc3RyYWxNb2RlbCUyQyUyME1pc3RyYWxDb25maWclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwTWlzdHJhbCUyMDdCJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyME1pc3RyYWxDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMGZyb20lMjB0aGUlMjBNaXN0cmFsJTIwN0IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyME1pc3RyYWxNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MistralModel, MistralConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Mistral 7B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = MistralConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the Mistral 7B style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = MistralModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){u(o.$$.fragment)},l(l){h(o.$$.fragment,l)},m(l,y){f(o,l,y),b=!0},p:N,i(l){b||(g(o.$$.fragment,l),b=!0)},o(l){_(o.$$.fragment,l),b=!1},d(l){M(o,l)}}}function ha(v){let o,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=b},l(l){o=c(l,"P",{"data-svelte-h":!0}),p(o)!=="svelte-fincs2"&&(o.innerHTML=b)},m(l,y){r(l,o,y)},p:N,d(l){l&&n(o)}}}function fa(v){let o,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=b},l(l){o=c(l,"P",{"data-svelte-h":!0}),p(o)!=="svelte-fincs2"&&(o.innerHTML=b)},m(l,y){r(l,o,y)},p:N,d(l){l&&n(o)}}}function ga(v){let o,b="Example:",l,y,T;return y=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBNaXN0cmFsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbCUyMCUzRCUyME1pc3RyYWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWlzdHJhbC03Qi12MC4xJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm1pc3RyYWxhaSUyRk1pc3RyYWwtN0ItdjAuMSUyMiklMEElMEFwcm9tcHQlMjAlM0QlMjAlMjJIZXklMkMlMjBhcmUlMjB5b3UlMjBjb25zY2lvdXMlM0YlMjBDYW4lMjB5b3UlMjB0YWxrJTIwdG8lMjBtZSUzRiUyMiUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQSUyMyUyMEdlbmVyYXRlJTBBZ2VuZXJhdGVfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoaW5wdXRzLmlucHV0X2lkcyUyQyUyMG1heF9sZW5ndGglM0QzMCklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlJTJDJTIwY2xlYW5fdXBfdG9rZW5pemF0aW9uX3NwYWNlcyUzREZhbHNlKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, MistralForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = MistralForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;Hey, are you conscious? Can you talk to me?\\nI&#x27;m not conscious, but I can talk to you.&quot;</span>`,wrap:!1}}),{c(){o=d("p"),o.textContent=b,l=a(),u(y.$$.fragment)},l(m){o=c(m,"P",{"data-svelte-h":!0}),p(o)!=="svelte-11lpom8"&&(o.textContent=b),l=s(m),h(y.$$.fragment,m)},m(m,k){r(m,o,k),r(m,l,k),f(y,m,k),T=!0},p:N,i(m){T||(g(y.$$.fragment,m),T=!0)},o(m){_(y.$$.fragment,m),T=!1},d(m){m&&(n(o),n(l)),M(y,m)}}}function _a(v){let o,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=b},l(l){o=c(l,"P",{"data-svelte-h":!0}),p(o)!=="svelte-fincs2"&&(o.innerHTML=b)},m(l,y){r(l,o,y)},p:N,d(l){l&&n(o)}}}function Ma(v){let o,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=b},l(l){o=c(l,"P",{"data-svelte-h":!0}),p(o)!=="svelte-fincs2"&&(o.innerHTML=b)},m(l,y){r(l,o,y)},p:N,d(l){l&&n(o)}}}function ya(v){let o,b=`This example uses a random model as the real ones are all very big. To get proper results, you should use
mistralai/Mistral-7B-v0.1 instead of ksmcg/Mistral-tiny. If you get out-of-memory when loading that checkpoint, you can try
adding <code>device_map=&quot;auto&quot;</code> in the <code>from_pretrained</code> call.`;return{c(){o=d("p"),o.innerHTML=b},l(l){o=c(l,"P",{"data-svelte-h":!0}),p(o)!=="svelte-p3ymc5"&&(o.innerHTML=b)},m(l,y){r(l,o,y)},p:N,d(l){l&&n(o)}}}function ba(v){let o,b="Example:",l,y,T;return y=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBGbGF4TWlzdHJhbE1vZGVsJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIya3NtY2clMkZNaXN0cmFsLXRpbnklMjIpJTBBbW9kZWwlMjAlM0QlMjBGbGF4TWlzdHJhbE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJrc21jZyUyRk1pc3RyYWwtdGlueSUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySGVsbG8lMkMlMjBteSUyMGRvZyUyMGlzJTIwY3V0ZSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIyamF4JTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxMistralModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;ksmcg/Mistral-tiny&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMistralModel.from_pretrained(<span class="hljs-string">&quot;ksmcg/Mistral-tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;jax&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){o=d("p"),o.textContent=b,l=a(),u(y.$$.fragment)},l(m){o=c(m,"P",{"data-svelte-h":!0}),p(o)!=="svelte-11lpom8"&&(o.textContent=b),l=s(m),h(y.$$.fragment,m)},m(m,k){r(m,o,k),r(m,l,k),f(y,m,k),T=!0},p:N,i(m){T||(g(y.$$.fragment,m),T=!0)},o(m){_(y.$$.fragment,m),T=!1},d(m){m&&(n(o),n(l)),M(y,m)}}}function va(v){let o,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=b},l(l){o=c(l,"P",{"data-svelte-h":!0}),p(o)!=="svelte-fincs2"&&(o.innerHTML=b)},m(l,y){r(l,o,y)},p:N,d(l){l&&n(o)}}}function Ta(v){let o,b=`This example uses a random model as the real ones are all very big. To get proper results, you should use
mistralai/Mistral-7B-v0.1 instead of ksmcg/Mistral-tiny. If you get out-of-memory when loading that checkpoint, you can try
adding <code>device_map=&quot;auto&quot;</code> in the <code>from_pretrained</code> call.`;return{c(){o=d("p"),o.innerHTML=b},l(l){o=c(l,"P",{"data-svelte-h":!0}),p(o)!=="svelte-p3ymc5"&&(o.innerHTML=b)},m(l,y){r(l,o,y)},p:N,d(l){l&&n(o)}}}function ka(v){let o,b="Example:",l,y,T;return y=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBGbGF4TWlzdHJhbEZvckNhdXNhbExNJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIya3NtY2clMkZNaXN0cmFsLXRpbnklMjIpJTBBbW9kZWwlMjAlM0QlMjBGbGF4TWlzdHJhbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJrc21jZyUyRk1pc3RyYWwtdGlueSUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySGVsbG8lMkMlMjBteSUyMGRvZyUyMGlzJTIwY3V0ZSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIybnAlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQSUyMyUyMHJldHJpZXZlJTIwbG9ndHMlMjBmb3IlMjBuZXh0JTIwdG9rZW4lMEFuZXh0X3Rva2VuX2xvZ2l0cyUyMCUzRCUyMG91dHB1dHMubG9naXRzJTVCJTNBJTJDJTIwLTElNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxMistralForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;ksmcg/Mistral-tiny&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxMistralForCausalLM.from_pretrained(<span class="hljs-string">&quot;ksmcg/Mistral-tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># retrieve logts for next token</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>next_token_logits = outputs.logits[:, -<span class="hljs-number">1</span>]`,wrap:!1}}),{c(){o=d("p"),o.textContent=b,l=a(),u(y.$$.fragment)},l(m){o=c(m,"P",{"data-svelte-h":!0}),p(o)!=="svelte-11lpom8"&&(o.textContent=b),l=s(m),h(y.$$.fragment,m)},m(m,k){r(m,o,k),r(m,l,k),f(y,m,k),T=!0},p:N,i(m){T||(g(y.$$.fragment,m),T=!0)},o(m){_(y.$$.fragment,m),T=!1},d(m){m&&(n(o),n(l)),M(y,m)}}}function wa(v){let o,b,l,y,T,m,k,Ut,de,fo="Mistral-7B-v0.1 is Mistral AI’s first Large Language Model (LLM).",Jt,ce,It,pe,go="Mistral-7B-v0.1 is a decoder-based LM with the following architectural choices:",Wt,me,_o="<li>Sliding Window Attention - Trained with 8k context length and fixed cache size, with a theoretical attention span of 128K tokens</li> <li>GQA (Grouped Query Attention) - allowing faster inference and lower cache size.</li> <li>Byte-fallback BPE tokenizer - ensures that characters are never mapped to out of vocabulary tokens.</li>",Ht,ue,Mo="We also provide an instruction fine-tuned model: <code>Mistral-7B-Instruct-v0.1</code> which can be used for chat-based inference.",Zt,he,yo='For more details please read our <a href="https://mistral.ai/news/announcing-mistral-7b/" rel="nofollow">release blog post</a>',qt,fe,Pt,ge,bo="Both <code>Mistral-7B-v0.1</code> and <code>Mistral-7B-Instruct-v0.1</code> are released under the Apache 2.0 license.",Bt,_e,Gt,Me,vo='<code>Mistral-7B-v0.1</code> and <code>Mistral-7B-Instruct-v0.1</code> can be found on the <a href="https://huggingface.co/mistralai" rel="nofollow">Huggingface Hub</a>',Rt,ye,To="These ready-to-use checkpoints can be downloaded and used via the HuggingFace Hub:",St,be,Nt,ve,ko="Raw weights for <code>Mistral-7B-v0.1</code> and <code>Mistral-7B-Instruct-v0.1</code> can be downloaded from:",Xt,Te,wo='<thead><tr><th>Model Name</th> <th>Checkpoint</th></tr></thead> <tbody><tr><td><code>Mistral-7B-v0.1</code></td> <td><a href="https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-v0.1.tar" rel="nofollow">Raw Checkpoint</a></td></tr> <tr><td><code>Mistral-7B-Instruct-v0.1</code></td> <td><a href="https://files.mistral-7b-v0-1.mistral.ai/mistral-7B-instruct-v0.1.tar" rel="nofollow">Raw Checkpoint</a></td></tr></tbody>',Vt,ke,$o="To use these raw checkpoints with HuggingFace you can use the <code>convert_mistral_weights_to_hf.py</code> script to convert them to the HuggingFace format:",Et,we,At,$e,xo="You can then load the converted model from the <code>output/path</code>:",Qt,xe,Yt,Ce,Dt,Fe,Co="First, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.",Ot,ze,Kt,je,Fo='Make also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow"><code>flash-attn</code></a> repository. Make also sure to load your model in half-precision (e.g. <code>torch.float16</code>)',en,Le,zo="To load and run a model using Flash Attention 2, refer to the snippet below:",tn,Ue,nn,Je,on,Ie,jo="Below is a expected speedup diagram that compares pure inference time between the native implementation in transformers using <code>mistralai/Mistral-7B-v0.1</code> checkpoint and the Flash Attention 2 version of the model.",an,D,Lo='<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/mistral-7b-inference-large-seqlen.png"/>',sn,We,rn,He,Uo=`The current implementation supports the sliding window attention mechanism and memory efficient cache management.
To enable sliding window attention, just make sure to have a <code>flash-attn</code> version that is compatible with sliding window attention (<code>&gt;=2.3.0</code>).`,ln,Ze,Jo="The Flash Attention-2 model uses also a more memory efficient cache slicing mechanism - as recommended per the official implementation of Mistral model that use rolling cache mechanism we keep the cache size fixed (<code>self.config.sliding_window</code>), support batched generation only for <code>padding_side=&quot;left&quot;</code> and use the absolute position of the current token to compute the positional embedding.",dn,qe,cn,Pe,Io="Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed.",pn,Be,mn,j,Ge,xn,st,Wo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralModel">MistralModel</a>. It is used to instantiate an
Mistral model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the Mistral-7B-v0.1 or Mistral-7B-Instruct-v0.1.`,Cn,rt,Ho='<a href="https://huggingface.co/mistralai/Mistral-7B-v0.1" rel="nofollow">mistralai/Mistral-7B-v0.1</a> <a href="https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1" rel="nofollow">mistralai/Mistral-7B-Instruct-v0.1</a>',Fn,it,Zo=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,zn,O,un,Re,hn,L,Se,jn,lt,qo=`The bare Mistral Model outputting raw hidden-states without any specific head on top.
This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Ln,dt,Po=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Un,ct,Bo="Transformer decoder consisting of <em>config.num_hidden_layers</em> layers. Each layer is a <code>MistralDecoderLayer</code>",Jn,X,Ne,In,pt,Go='The <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralModel">MistralModel</a> forward method, overrides the <code>__call__</code> special method.',Wn,K,fn,Xe,gn,E,Ve,Hn,G,Ee,Zn,mt,Ro='The <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralForCausalLM">MistralForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',qn,ee,Pn,te,_n,Ae,Mn,w,Qe,Bn,ut,So="The Mistral Model transformer with a sequence classification head on top (linear layer).",Gn,ht,No=`<a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralForSequenceClassification">MistralForSequenceClassification</a> uses the last token in order to do the classification, as other causal models
(e.g. GPT-2) do.`,Rn,ft,Xo=`Since it does classification on the last token, it requires to know the position of the last token. If a
<code>pad_token_id</code> is defined in the configuration, it finds the last token that is not a padding token in each row. If
no <code>pad_token_id</code> is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when <code>inputs_embeds</code> are passed instead of <code>input_ids</code>, it does the same (take the last value in
each row of the batch).`,Sn,gt,Vo=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Nn,_t,Eo=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Xn,V,Ye,Vn,Mt,Ao='The <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralForSequenceClassification">MistralForSequenceClassification</a> forward method, overrides the <code>__call__</code> special method.',En,ne,yn,De,bn,$,Oe,An,yt,Qo="The bare Mistral Model transformer outputting raw hidden-states without any specific head on top.",Qn,bt,Yo=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Yn,vt,Do=`This model is also a Flax Linen
<a href="https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html" rel="nofollow">flax.nn.Module</a> subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`,Dn,Tt,Oo="Finally, this model supports inherent JAX features such as:",On,kt,Ko='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',Kn,J,Ke,eo,wt,ea="The <code>FlaxMistralPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",to,oe,no,ae,oo,se,vn,et,Tn,x,tt,ao,$t,ta="The Mistral Model transformer with a language modeling head (linear layer) on top.",so,xt,na=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ro,Ct,oa=`This model is also a Flax Linen
<a href="https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html" rel="nofollow">flax.nn.Module</a> subclass. Use it as a
regular Flax Module and refer to the Flax documentation for all matter related to general usage and behavior.`,io,Ft,aa="Finally, this model supports inherent JAX features such as:",lo,zt,sa='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',co,I,nt,po,jt,ra="The <code>FlaxMistralPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",mo,re,uo,ie,ho,le,kn,Lt,wn;return T=new U({props:{title:"Mistral",local:"mistral",headingTag:"h1"}}),k=new U({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ce=new U({props:{title:"Model Details",local:"model-details",headingTag:"h3"}}),fe=new U({props:{title:"License",local:"license",headingTag:"h3"}}),_e=new U({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),be=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWRldmljZSUyMCUzRCUyMCUyMmN1ZGElMjIlMjAlMjMlMjB0aGUlMjBkZXZpY2UlMjB0byUyMGxvYWQlMjB0aGUlMjBtb2RlbCUyMG9udG8lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtaXN0cmFsYWklMkZNaXN0cmFsLTdCLXYwLjElMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIybWlzdHJhbGFpJTJGTWlzdHJhbC03Qi12MC4xJTIyKSUwQSUwQXByb21wdCUyMCUzRCUyMCUyMk15JTIwZmF2b3VyaXRlJTIwY29uZGltZW50JTIwaXMlMjIlMEElMEFtb2RlbF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTVCcHJvbXB0JTVEJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oZGV2aWNlKSUwQW1vZGVsLnRvKGRldmljZSklMEElMEFnZW5lcmF0ZWRfaWRzJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKiptb2RlbF9pbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDEwMCUyQyUyMGRvX3NhbXBsZSUzRFRydWUpJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-comment"># the device to load the model onto</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;My favourite condiment is&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer([prompt], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs, max_new_tokens=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;The expected output&quot;</span>`,wrap:!1}}),we=new Y({props:{code:"cHl0aG9uJTIwc3JjJTJGdHJhbnNmb3JtZXJzJTJGbW9kZWxzJTJGbWlzdHJhbCUyRmNvbnZlcnRfbWlzdHJhbF93ZWlnaHRzX3RvX2hmLnB5JTIwJTVDJTBBJTIwJTIwJTIwJTIwLS1pbnB1dF9kaXIlMjAlMkZwYXRoJTJGdG8lMkZkb3dubG9hZGVkJTJGbWlzdHJhbCUyRndlaWdodHMlMjAtLW1vZGVsX3NpemUlMjA3QiUyMC0tb3V0cHV0X2RpciUyMCUyRm91dHB1dCUyRnBhdGg=",highlighted:`python src/transformers/models/mistral/convert_mistral_weights_to_hf.py \\
    --input_dir /path/to/downloaded/mistral/weights --model_size 7B --output_dir /output/path`,wrap:!1}}),xe=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME1pc3RyYWxGb3JDYXVzYWxMTSUyQyUyMExsYW1hVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwTGxhbWFUb2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMiUyRm91dHB1dCUyRnBhdGglMjIpJTBBbW9kZWwlMjAlM0QlMjBNaXN0cmFsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMiUyRm91dHB1dCUyRnBhdGglMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> MistralForCausalLM, LlamaTokenizer

tokenizer = LlamaTokenizer.from_pretrained(<span class="hljs-string">&quot;/output/path&quot;</span>)
model = MistralForCausalLM.from_pretrained(<span class="hljs-string">&quot;/output/path&quot;</span>)`,wrap:!1}}),Ce=new U({props:{title:"Combining Mistral and Flash Attention 2",local:"combining-mistral-and-flash-attention-2",headingTag:"h2"}}),ze=new Y({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1VJTIwZmxhc2gtYXR0biUyMC0tbm8tYnVpbGQtaXNvbGF0aW9u",highlighted:"pip install -U flash-attn --no-build-isolation",wrap:!1}}),Ue=new Y({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMCUyMyUyMHRoZSUyMGRldmljZSUyMHRvJTIwbG9hZCUyMHRoZSUyMG1vZGVsJTIwb250byUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMm1pc3RyYWxhaSUyRk1pc3RyYWwtN0ItdjAuMSUyMiUyQyUyMHRvcmNoX2R0eXBlJTNEdG9yY2guZmxvYXQxNiUyQyUyMGF0dG5faW1wbGVtZW50YXRpb24lM0QlMjJmbGFzaF9hdHRlbnRpb25fMiUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJtaXN0cmFsYWklMkZNaXN0cmFsLTdCLXYwLjElMjIpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIyTXklMjBmYXZvdXJpdGUlMjBjb25kaW1lbnQlMjBpcyUyMiUwQSUwQW1vZGVsX2lucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUJwcm9tcHQlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhkZXZpY2UpJTBBbW9kZWwudG8oZGV2aWNlKSUwQSUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKm1vZGVsX2lucHV0cyUyQyUyMG1heF9uZXdfdG9rZW5zJTNEMTAwJTJDJTIwZG9fc2FtcGxlJTNEVHJ1ZSklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMpJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-comment"># the device to load the model onto</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>, torch_dtype=torch.float16, attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;mistralai/Mistral-7B-v0.1&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;My favourite condiment is&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer([prompt], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(device)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs, max_new_tokens=<span class="hljs-number">100</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids)[<span class="hljs-number">0</span>]
<span class="hljs-string">&quot;The expected output&quot;</span>`,wrap:!1}}),Je=new U({props:{title:"Expected speedups",local:"expected-speedups",headingTag:"h3"}}),We=new U({props:{title:"Sliding window Attention",local:"sliding-window-attention",headingTag:"h3"}}),qe=new U({props:{title:"The Mistral Team",local:"the-mistral-team",headingTag:"h2"}}),Be=new U({props:{title:"MistralConfig",local:"transformers.MistralConfig",headingTag:"h2"}}),Ge=new S({props:{name:"class transformers.MistralConfig",anchor:"transformers.MistralConfig",parameters:[{name:"vocab_size",val:" = 32000"},{name:"hidden_size",val:" = 4096"},{name:"intermediate_size",val:" = 14336"},{name:"num_hidden_layers",val:" = 32"},{name:"num_attention_heads",val:" = 32"},{name:"num_key_value_heads",val:" = 8"},{name:"hidden_act",val:" = 'silu'"},{name:"max_position_embeddings",val:" = 131072"},{name:"initializer_range",val:" = 0.02"},{name:"rms_norm_eps",val:" = 1e-06"},{name:"use_cache",val:" = True"},{name:"pad_token_id",val:" = None"},{name:"bos_token_id",val:" = 1"},{name:"eos_token_id",val:" = 2"},{name:"tie_word_embeddings",val:" = False"},{name:"rope_theta",val:" = 10000.0"},{name:"sliding_window",val:" = 4096"},{name:"attention_dropout",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.MistralConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
Vocabulary size of the Mistral model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralModel">MistralModel</a>`,name:"vocab_size"},{anchor:"transformers.MistralConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimension of the hidden representations.`,name:"hidden_size"},{anchor:"transformers.MistralConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 14336) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.MistralConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.MistralConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.MistralConfig.num_key_value_heads",description:`<strong>num_key_value_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
This is the number of key_value heads that should be used to implement Grouped Query Attention. If
<code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if
<code>num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group. For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to </code>8\`.`,name:"num_key_value_heads"},{anchor:"transformers.MistralConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the decoder.`,name:"hidden_act"},{anchor:"transformers.MistralConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to <code>4096*32</code>) &#x2014;
The maximum sequence length that this model might ever be used with. Mistral&#x2019;s sliding window attention
allows sequence of up to 4096*32 tokens.`,name:"max_position_embeddings"},{anchor:"transformers.MistralConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.MistralConfig.rms_norm_eps",description:`<strong>rms_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the rms normalization layers.`,name:"rms_norm_eps"},{anchor:"transformers.MistralConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.MistralConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The id of the padding token.`,name:"pad_token_id"},{anchor:"transformers.MistralConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The id of the &#x201C;beginning-of-sequence&#x201D; token.`,name:"bos_token_id"},{anchor:"transformers.MistralConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The id of the &#x201C;end-of-sequence&#x201D; token.`,name:"eos_token_id"},{anchor:"transformers.MistralConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the model&#x2019;s input and output word embeddings should be tied.`,name:"tie_word_embeddings"},{anchor:"transformers.MistralConfig.rope_theta",description:`<strong>rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to 10000.0) &#x2014;
The base period of the RoPE embeddings.`,name:"rope_theta"},{anchor:"transformers.MistralConfig.sliding_window",description:`<strong>sliding_window</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Sliding window attention window size. If not specified, will default to <code>4096</code>.`,name:"sliding_window"},{anchor:"transformers.MistralConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/configuration_mistral.py#L29"}}),O=new $n({props:{anchor:"transformers.MistralConfig.example",$$slots:{default:[ua]},$$scope:{ctx:v}}}),Re=new U({props:{title:"MistralModel",local:"transformers.MistralModel",headingTag:"h2"}}),Se=new S({props:{name:"class transformers.MistralModel",anchor:"transformers.MistralModel",parameters:[{name:"config",val:": MistralConfig"}],parametersDescription:[{anchor:"transformers.MistralModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.
config &#x2014; MistralConfig`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L897"}}),Ne=new S({props:{name:"forward",anchor:"transformers.MistralModel.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.MistralModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MistralModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.MistralModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MistralModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <a href="/docs/transformers/main/en/internal/generation_utils#transformers.Cache">Cache</a> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.MistralModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MistralModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.MistralModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MistralModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MistralModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L931"}}),K=new at({props:{$$slots:{default:[ha]},$$scope:{ctx:v}}}),Xe=new U({props:{title:"MistralForCausalLM",local:"transformers.MistralForCausalLM",headingTag:"h2"}}),Ve=new S({props:{name:"class transformers.MistralForCausalLM",anchor:"transformers.MistralForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L1079"}}),Ee=new S({props:{name:"forward",anchor:"transformers.MistralForCausalLM.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.MistralForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MistralForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.MistralForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MistralForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <a href="/docs/transformers/main/en/internal/generation_utils#transformers.Cache">Cache</a> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.MistralForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MistralForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.MistralForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MistralForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MistralForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.</p>
<p>Args &#x2014;
labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L1109",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig"
>MistralConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ee=new at({props:{$$slots:{default:[fa]},$$scope:{ctx:v}}}),te=new $n({props:{anchor:"transformers.MistralForCausalLM.forward.example",$$slots:{default:[ga]},$$scope:{ctx:v}}}),Ae=new U({props:{title:"MistralForSequenceClassification",local:"transformers.MistralForSequenceClassification",headingTag:"h2"}}),Qe=new S({props:{name:"class transformers.MistralForSequenceClassification",anchor:"transformers.MistralForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.MistralForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L1265"}}),Ye=new S({props:{name:"forward",anchor:"transformers.MistralForSequenceClassification.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.MistralForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.MistralForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.MistralForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.MistralForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <a href="/docs/transformers/main/en/internal/generation_utils#transformers.Cache">Cache</a> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.MistralForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.MistralForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.MistralForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.MistralForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.MistralForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.MistralForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/modeling_mistral.py#L1297"}}),ne=new at({props:{$$slots:{default:[_a]},$$scope:{ctx:v}}}),De=new U({props:{title:"FlaxMistralModel",local:"transformers.FlaxMistralModel",headingTag:"h2"}}),Oe=new S({props:{name:"class transformers.FlaxMistralModel",anchor:"transformers.FlaxMistralModel",parameters:[{name:"config",val:": MistralConfig"},{name:"input_shape",val:": Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxMistralModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxMistralModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code>, or
<code>jax.numpy.bfloat16</code>.</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/modeling_flax_mistral.py#L635"}}),Ke=new S({props:{name:"__call__",anchor:"transformers.FlaxMistralModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlaxMistralModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxMistralModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlaxMistralModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlaxMistralModel.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxMistralModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxMistralModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxMistralModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/modeling_flax_mistral.py#L457",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast"
>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig"
>MistralConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>Dict[str, jnp.ndarray]</code>) — Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast"
>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oe=new at({props:{$$slots:{default:[Ma]},$$scope:{ctx:v}}}),ae=new at({props:{warning:!0,$$slots:{default:[ya]},$$scope:{ctx:v}}}),se=new $n({props:{anchor:"transformers.FlaxMistralModel.__call__.example",$$slots:{default:[ba]},$$scope:{ctx:v}}}),et=new U({props:{title:"FlaxMistralForCausalLM",local:"transformers.FlaxMistralForCausalLM",headingTag:"h2"}}),tt=new S({props:{name:"class transformers.FlaxMistralForCausalLM",anchor:"transformers.FlaxMistralForCausalLM",parameters:[{name:"config",val:": MistralConfig"},{name:"input_shape",val:": Tuple = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxMistralForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig">MistralConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxMistralForCausalLM.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code>, or
<code>jax.numpy.bfloat16</code>.</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/en/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/modeling_flax_mistral.py#L697"}}),nt=new S({props:{name:"__call__",anchor:"transformers.FlaxMistralForCausalLM.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"past_key_values",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlaxMistralForCausalLM.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, input_ids_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxMistralForCausalLM.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlaxMistralForCausalLM.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlaxMistralForCausalLM.__call__.past_key_values",description:`<strong>past_key_values</strong> (<code>Dict[str, np.ndarray]</code>, <em>optional</em>, returned by <code>init_cache</code> or when passing previous <code>past_key_values</code>) &#x2014;
Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
auto-regressive decoding. Pre-computed key and value hidden-states are of shape <em>[batch_size, max_length]</em>.`,name:"past_key_values"},{anchor:"transformers.FlaxMistralForCausalLM.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxMistralForCausalLM.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxMistralForCausalLM.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/mistral/modeling_flax_mistral.py#L457",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions"
>transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/mistral#transformers.MistralConfig"
>MistralConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Cross attentions weights after the attention softmax, used to compute the weighted average in the
cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(jnp.ndarray))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Tuple of <code>jnp.ndarray</code> tuples of length <code>config.n_layers</code>, with each tuple containing the cached key, value
states of the self-attention and the cross-attention layers if model is used in encoder-decoder setting.
Only relevant if <code>config.is_decoder = True</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions"
>transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),re=new at({props:{$$slots:{default:[va]},$$scope:{ctx:v}}}),ie=new at({props:{warning:!0,$$slots:{default:[Ta]},$$scope:{ctx:v}}}),le=new $n({props:{anchor:"transformers.FlaxMistralForCausalLM.__call__.example",$$slots:{default:[ka]},$$scope:{ctx:v}}}),{c(){o=d("meta"),b=a(),l=d("p"),y=a(),u(T.$$.fragment),m=a(),u(k.$$.fragment),Ut=a(),de=d("p"),de.textContent=fo,Jt=a(),u(ce.$$.fragment),It=a(),pe=d("p"),pe.textContent=go,Wt=a(),me=d("ul"),me.innerHTML=_o,Ht=a(),ue=d("p"),ue.innerHTML=Mo,Zt=a(),he=d("p"),he.innerHTML=yo,qt=a(),u(fe.$$.fragment),Pt=a(),ge=d("p"),ge.innerHTML=bo,Bt=a(),u(_e.$$.fragment),Gt=a(),Me=d("p"),Me.innerHTML=vo,Rt=a(),ye=d("p"),ye.textContent=To,St=a(),u(be.$$.fragment),Nt=a(),ve=d("p"),ve.innerHTML=ko,Xt=a(),Te=d("table"),Te.innerHTML=wo,Vt=a(),ke=d("p"),ke.innerHTML=$o,Et=a(),u(we.$$.fragment),At=a(),$e=d("p"),$e.innerHTML=xo,Qt=a(),u(xe.$$.fragment),Yt=a(),u(Ce.$$.fragment),Dt=a(),Fe=d("p"),Fe.textContent=Co,Ot=a(),u(ze.$$.fragment),Kt=a(),je=d("p"),je.innerHTML=Fo,en=a(),Le=d("p"),Le.textContent=zo,tn=a(),u(Ue.$$.fragment),nn=a(),u(Je.$$.fragment),on=a(),Ie=d("p"),Ie.innerHTML=jo,an=a(),D=d("div"),D.innerHTML=Lo,sn=a(),u(We.$$.fragment),rn=a(),He=d("p"),He.innerHTML=Uo,ln=a(),Ze=d("p"),Ze.innerHTML=Jo,dn=a(),u(qe.$$.fragment),cn=a(),Pe=d("p"),Pe.textContent=Io,pn=a(),u(Be.$$.fragment),mn=a(),j=d("div"),u(Ge.$$.fragment),xn=a(),st=d("p"),st.innerHTML=Wo,Cn=a(),rt=d("p"),rt.innerHTML=Ho,Fn=a(),it=d("p"),it.innerHTML=Zo,zn=a(),u(O.$$.fragment),un=a(),u(Re.$$.fragment),hn=a(),L=d("div"),u(Se.$$.fragment),jn=a(),lt=d("p"),lt.innerHTML=qo,Ln=a(),dt=d("p"),dt.innerHTML=Po,Un=a(),ct=d("p"),ct.innerHTML=Bo,Jn=a(),X=d("div"),u(Ne.$$.fragment),In=a(),pt=d("p"),pt.innerHTML=Go,Wn=a(),u(K.$$.fragment),fn=a(),u(Xe.$$.fragment),gn=a(),E=d("div"),u(Ve.$$.fragment),Hn=a(),G=d("div"),u(Ee.$$.fragment),Zn=a(),mt=d("p"),mt.innerHTML=Ro,qn=a(),u(ee.$$.fragment),Pn=a(),u(te.$$.fragment),_n=a(),u(Ae.$$.fragment),Mn=a(),w=d("div"),u(Qe.$$.fragment),Bn=a(),ut=d("p"),ut.textContent=So,Gn=a(),ht=d("p"),ht.innerHTML=No,Rn=a(),ft=d("p"),ft.innerHTML=Xo,Sn=a(),gt=d("p"),gt.innerHTML=Vo,Nn=a(),_t=d("p"),_t.innerHTML=Eo,Xn=a(),V=d("div"),u(Ye.$$.fragment),Vn=a(),Mt=d("p"),Mt.innerHTML=Ao,En=a(),u(ne.$$.fragment),yn=a(),u(De.$$.fragment),bn=a(),$=d("div"),u(Oe.$$.fragment),An=a(),yt=d("p"),yt.textContent=Qo,Qn=a(),bt=d("p"),bt.innerHTML=Yo,Yn=a(),vt=d("p"),vt.innerHTML=Do,Dn=a(),Tt=d("p"),Tt.textContent=Oo,On=a(),kt=d("ul"),kt.innerHTML=Ko,Kn=a(),J=d("div"),u(Ke.$$.fragment),eo=a(),wt=d("p"),wt.innerHTML=ea,to=a(),u(oe.$$.fragment),no=a(),u(ae.$$.fragment),oo=a(),u(se.$$.fragment),vn=a(),u(et.$$.fragment),Tn=a(),x=d("div"),u(tt.$$.fragment),ao=a(),$t=d("p"),$t.textContent=ta,so=a(),xt=d("p"),xt.innerHTML=na,ro=a(),Ct=d("p"),Ct.innerHTML=oa,io=a(),Ft=d("p"),Ft.textContent=aa,lo=a(),zt=d("ul"),zt.innerHTML=sa,co=a(),I=d("div"),u(nt.$$.fragment),po=a(),jt=d("p"),jt.innerHTML=ra,mo=a(),u(re.$$.fragment),uo=a(),u(ie.$$.fragment),ho=a(),u(le.$$.fragment),kn=a(),Lt=d("p"),this.h()},l(e){const t=pa("svelte-u9bgzb",document.head);o=c(t,"META",{name:!0,content:!0}),t.forEach(n),b=s(e),l=c(e,"P",{}),P(l).forEach(n),y=s(e),h(T.$$.fragment,e),m=s(e),h(k.$$.fragment,e),Ut=s(e),de=c(e,"P",{"data-svelte-h":!0}),p(de)!=="svelte-1nseuow"&&(de.textContent=fo),Jt=s(e),h(ce.$$.fragment,e),It=s(e),pe=c(e,"P",{"data-svelte-h":!0}),p(pe)!=="svelte-unnu27"&&(pe.textContent=go),Wt=s(e),me=c(e,"UL",{"data-svelte-h":!0}),p(me)!=="svelte-hqpplt"&&(me.innerHTML=_o),Ht=s(e),ue=c(e,"P",{"data-svelte-h":!0}),p(ue)!=="svelte-1w3a9um"&&(ue.innerHTML=Mo),Zt=s(e),he=c(e,"P",{"data-svelte-h":!0}),p(he)!=="svelte-o5qoei"&&(he.innerHTML=yo),qt=s(e),h(fe.$$.fragment,e),Pt=s(e),ge=c(e,"P",{"data-svelte-h":!0}),p(ge)!=="svelte-14253l8"&&(ge.innerHTML=bo),Bt=s(e),h(_e.$$.fragment,e),Gt=s(e),Me=c(e,"P",{"data-svelte-h":!0}),p(Me)!=="svelte-u5jvf7"&&(Me.innerHTML=vo),Rt=s(e),ye=c(e,"P",{"data-svelte-h":!0}),p(ye)!=="svelte-a5tlwb"&&(ye.textContent=To),St=s(e),h(be.$$.fragment,e),Nt=s(e),ve=c(e,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-1jktxis"&&(ve.innerHTML=ko),Xt=s(e),Te=c(e,"TABLE",{"data-svelte-h":!0}),p(Te)!=="svelte-azzqtn"&&(Te.innerHTML=wo),Vt=s(e),ke=c(e,"P",{"data-svelte-h":!0}),p(ke)!=="svelte-192zt76"&&(ke.innerHTML=$o),Et=s(e),h(we.$$.fragment,e),At=s(e),$e=c(e,"P",{"data-svelte-h":!0}),p($e)!=="svelte-1svz3xz"&&($e.innerHTML=xo),Qt=s(e),h(xe.$$.fragment,e),Yt=s(e),h(Ce.$$.fragment,e),Dt=s(e),Fe=c(e,"P",{"data-svelte-h":!0}),p(Fe)!=="svelte-o3pzzu"&&(Fe.textContent=Co),Ot=s(e),h(ze.$$.fragment,e),Kt=s(e),je=c(e,"P",{"data-svelte-h":!0}),p(je)!=="svelte-qk7tod"&&(je.innerHTML=Fo),en=s(e),Le=c(e,"P",{"data-svelte-h":!0}),p(Le)!=="svelte-14hchid"&&(Le.textContent=zo),tn=s(e),h(Ue.$$.fragment,e),nn=s(e),h(Je.$$.fragment,e),on=s(e),Ie=c(e,"P",{"data-svelte-h":!0}),p(Ie)!=="svelte-lhkvj8"&&(Ie.innerHTML=jo),an=s(e),D=c(e,"DIV",{style:!0,"data-svelte-h":!0}),p(D)!=="svelte-18u0t8d"&&(D.innerHTML=Lo),sn=s(e),h(We.$$.fragment,e),rn=s(e),He=c(e,"P",{"data-svelte-h":!0}),p(He)!=="svelte-10i6fhp"&&(He.innerHTML=Uo),ln=s(e),Ze=c(e,"P",{"data-svelte-h":!0}),p(Ze)!=="svelte-1bvsrfr"&&(Ze.innerHTML=Jo),dn=s(e),h(qe.$$.fragment,e),cn=s(e),Pe=c(e,"P",{"data-svelte-h":!0}),p(Pe)!=="svelte-15xllx1"&&(Pe.textContent=Io),pn=s(e),h(Be.$$.fragment,e),mn=s(e),j=c(e,"DIV",{class:!0});var W=P(j);h(Ge.$$.fragment,W),xn=s(W),st=c(W,"P",{"data-svelte-h":!0}),p(st)!=="svelte-14v0v0n"&&(st.innerHTML=Wo),Cn=s(W),rt=c(W,"P",{"data-svelte-h":!0}),p(rt)!=="svelte-28p57e"&&(rt.innerHTML=Ho),Fn=s(W),it=c(W,"P",{"data-svelte-h":!0}),p(it)!=="svelte-o55m63"&&(it.innerHTML=Zo),zn=s(W),h(O.$$.fragment,W),W.forEach(n),un=s(e),h(Re.$$.fragment,e),hn=s(e),L=c(e,"DIV",{class:!0});var H=P(L);h(Se.$$.fragment,H),jn=s(H),lt=c(H,"P",{"data-svelte-h":!0}),p(lt)!=="svelte-tk9y80"&&(lt.innerHTML=qo),Ln=s(H),dt=c(H,"P",{"data-svelte-h":!0}),p(dt)!=="svelte-hswkmf"&&(dt.innerHTML=Po),Un=s(H),ct=c(H,"P",{"data-svelte-h":!0}),p(ct)!=="svelte-1oenqzd"&&(ct.innerHTML=Bo),Jn=s(H),X=c(H,"DIV",{class:!0});var A=P(X);h(Ne.$$.fragment,A),In=s(A),pt=c(A,"P",{"data-svelte-h":!0}),p(pt)!=="svelte-3rq1wp"&&(pt.innerHTML=Go),Wn=s(A),h(K.$$.fragment,A),A.forEach(n),H.forEach(n),fn=s(e),h(Xe.$$.fragment,e),gn=s(e),E=c(e,"DIV",{class:!0});var ot=P(E);h(Ve.$$.fragment,ot),Hn=s(ot),G=c(ot,"DIV",{class:!0});var R=P(G);h(Ee.$$.fragment,R),Zn=s(R),mt=c(R,"P",{"data-svelte-h":!0}),p(mt)!=="svelte-1bgwutx"&&(mt.innerHTML=Ro),qn=s(R),h(ee.$$.fragment,R),Pn=s(R),h(te.$$.fragment,R),R.forEach(n),ot.forEach(n),_n=s(e),h(Ae.$$.fragment,e),Mn=s(e),w=c(e,"DIV",{class:!0});var C=P(w);h(Qe.$$.fragment,C),Bn=s(C),ut=c(C,"P",{"data-svelte-h":!0}),p(ut)!=="svelte-1r7clz4"&&(ut.textContent=So),Gn=s(C),ht=c(C,"P",{"data-svelte-h":!0}),p(ht)!=="svelte-oiiwva"&&(ht.innerHTML=No),Rn=s(C),ft=c(C,"P",{"data-svelte-h":!0}),p(ft)!=="svelte-10ugs3m"&&(ft.innerHTML=Xo),Sn=s(C),gt=c(C,"P",{"data-svelte-h":!0}),p(gt)!=="svelte-6pahdo"&&(gt.innerHTML=Vo),Nn=s(C),_t=c(C,"P",{"data-svelte-h":!0}),p(_t)!=="svelte-hswkmf"&&(_t.innerHTML=Eo),Xn=s(C),V=c(C,"DIV",{class:!0});var Q=P(V);h(Ye.$$.fragment,Q),Vn=s(Q),Mt=c(Q,"P",{"data-svelte-h":!0}),p(Mt)!=="svelte-1bksa9b"&&(Mt.innerHTML=Ao),En=s(Q),h(ne.$$.fragment,Q),Q.forEach(n),C.forEach(n),yn=s(e),h(De.$$.fragment,e),bn=s(e),$=c(e,"DIV",{class:!0});var F=P($);h(Oe.$$.fragment,F),An=s(F),yt=c(F,"P",{"data-svelte-h":!0}),p(yt)!=="svelte-1vqhmd7"&&(yt.textContent=Qo),Qn=s(F),bt=c(F,"P",{"data-svelte-h":!0}),p(bt)!=="svelte-18ki9f4"&&(bt.innerHTML=Yo),Yn=s(F),vt=c(F,"P",{"data-svelte-h":!0}),p(vt)!=="svelte-idybz1"&&(vt.innerHTML=Do),Dn=s(F),Tt=c(F,"P",{"data-svelte-h":!0}),p(Tt)!=="svelte-1pplc4a"&&(Tt.textContent=Oo),On=s(F),kt=c(F,"UL",{"data-svelte-h":!0}),p(kt)!=="svelte-1w7z84m"&&(kt.innerHTML=Ko),Kn=s(F),J=c(F,"DIV",{class:!0});var Z=P(J);h(Ke.$$.fragment,Z),eo=s(Z),wt=c(Z,"P",{"data-svelte-h":!0}),p(wt)!=="svelte-dti72j"&&(wt.innerHTML=ea),to=s(Z),h(oe.$$.fragment,Z),no=s(Z),h(ae.$$.fragment,Z),oo=s(Z),h(se.$$.fragment,Z),Z.forEach(n),F.forEach(n),vn=s(e),h(et.$$.fragment,e),Tn=s(e),x=c(e,"DIV",{class:!0});var z=P(x);h(tt.$$.fragment,z),ao=s(z),$t=c(z,"P",{"data-svelte-h":!0}),p($t)!=="svelte-lgxco8"&&($t.textContent=ta),so=s(z),xt=c(z,"P",{"data-svelte-h":!0}),p(xt)!=="svelte-18ki9f4"&&(xt.innerHTML=na),ro=s(z),Ct=c(z,"P",{"data-svelte-h":!0}),p(Ct)!=="svelte-idybz1"&&(Ct.innerHTML=oa),io=s(z),Ft=c(z,"P",{"data-svelte-h":!0}),p(Ft)!=="svelte-1pplc4a"&&(Ft.textContent=aa),lo=s(z),zt=c(z,"UL",{"data-svelte-h":!0}),p(zt)!=="svelte-1w7z84m"&&(zt.innerHTML=sa),co=s(z),I=c(z,"DIV",{class:!0});var q=P(I);h(nt.$$.fragment,q),po=s(q),jt=c(q,"P",{"data-svelte-h":!0}),p(jt)!=="svelte-dti72j"&&(jt.innerHTML=ra),mo=s(q),h(re.$$.fragment,q),uo=s(q),h(ie.$$.fragment,q),ho=s(q),h(le.$$.fragment,q),q.forEach(n),z.forEach(n),kn=s(e),Lt=c(e,"P",{}),P(Lt).forEach(n),this.h()},h(){B(o,"name","hf:doc:metadata"),B(o,"content",$a),ma(D,"text-align","center"),B(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),B(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){i(document.head,o),r(e,b,t),r(e,l,t),r(e,y,t),f(T,e,t),r(e,m,t),f(k,e,t),r(e,Ut,t),r(e,de,t),r(e,Jt,t),f(ce,e,t),r(e,It,t),r(e,pe,t),r(e,Wt,t),r(e,me,t),r(e,Ht,t),r(e,ue,t),r(e,Zt,t),r(e,he,t),r(e,qt,t),f(fe,e,t),r(e,Pt,t),r(e,ge,t),r(e,Bt,t),f(_e,e,t),r(e,Gt,t),r(e,Me,t),r(e,Rt,t),r(e,ye,t),r(e,St,t),f(be,e,t),r(e,Nt,t),r(e,ve,t),r(e,Xt,t),r(e,Te,t),r(e,Vt,t),r(e,ke,t),r(e,Et,t),f(we,e,t),r(e,At,t),r(e,$e,t),r(e,Qt,t),f(xe,e,t),r(e,Yt,t),f(Ce,e,t),r(e,Dt,t),r(e,Fe,t),r(e,Ot,t),f(ze,e,t),r(e,Kt,t),r(e,je,t),r(e,en,t),r(e,Le,t),r(e,tn,t),f(Ue,e,t),r(e,nn,t),f(Je,e,t),r(e,on,t),r(e,Ie,t),r(e,an,t),r(e,D,t),r(e,sn,t),f(We,e,t),r(e,rn,t),r(e,He,t),r(e,ln,t),r(e,Ze,t),r(e,dn,t),f(qe,e,t),r(e,cn,t),r(e,Pe,t),r(e,pn,t),f(Be,e,t),r(e,mn,t),r(e,j,t),f(Ge,j,null),i(j,xn),i(j,st),i(j,Cn),i(j,rt),i(j,Fn),i(j,it),i(j,zn),f(O,j,null),r(e,un,t),f(Re,e,t),r(e,hn,t),r(e,L,t),f(Se,L,null),i(L,jn),i(L,lt),i(L,Ln),i(L,dt),i(L,Un),i(L,ct),i(L,Jn),i(L,X),f(Ne,X,null),i(X,In),i(X,pt),i(X,Wn),f(K,X,null),r(e,fn,t),f(Xe,e,t),r(e,gn,t),r(e,E,t),f(Ve,E,null),i(E,Hn),i(E,G),f(Ee,G,null),i(G,Zn),i(G,mt),i(G,qn),f(ee,G,null),i(G,Pn),f(te,G,null),r(e,_n,t),f(Ae,e,t),r(e,Mn,t),r(e,w,t),f(Qe,w,null),i(w,Bn),i(w,ut),i(w,Gn),i(w,ht),i(w,Rn),i(w,ft),i(w,Sn),i(w,gt),i(w,Nn),i(w,_t),i(w,Xn),i(w,V),f(Ye,V,null),i(V,Vn),i(V,Mt),i(V,En),f(ne,V,null),r(e,yn,t),f(De,e,t),r(e,bn,t),r(e,$,t),f(Oe,$,null),i($,An),i($,yt),i($,Qn),i($,bt),i($,Yn),i($,vt),i($,Dn),i($,Tt),i($,On),i($,kt),i($,Kn),i($,J),f(Ke,J,null),i(J,eo),i(J,wt),i(J,to),f(oe,J,null),i(J,no),f(ae,J,null),i(J,oo),f(se,J,null),r(e,vn,t),f(et,e,t),r(e,Tn,t),r(e,x,t),f(tt,x,null),i(x,ao),i(x,$t),i(x,so),i(x,xt),i(x,ro),i(x,Ct),i(x,io),i(x,Ft),i(x,lo),i(x,zt),i(x,co),i(x,I),f(nt,I,null),i(I,po),i(I,jt),i(I,mo),f(re,I,null),i(I,uo),f(ie,I,null),i(I,ho),f(le,I,null),r(e,kn,t),r(e,Lt,t),wn=!0},p(e,[t]){const W={};t&2&&(W.$$scope={dirty:t,ctx:e}),O.$set(W);const H={};t&2&&(H.$$scope={dirty:t,ctx:e}),K.$set(H);const A={};t&2&&(A.$$scope={dirty:t,ctx:e}),ee.$set(A);const ot={};t&2&&(ot.$$scope={dirty:t,ctx:e}),te.$set(ot);const R={};t&2&&(R.$$scope={dirty:t,ctx:e}),ne.$set(R);const C={};t&2&&(C.$$scope={dirty:t,ctx:e}),oe.$set(C);const Q={};t&2&&(Q.$$scope={dirty:t,ctx:e}),ae.$set(Q);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),se.$set(F);const Z={};t&2&&(Z.$$scope={dirty:t,ctx:e}),re.$set(Z);const z={};t&2&&(z.$$scope={dirty:t,ctx:e}),ie.$set(z);const q={};t&2&&(q.$$scope={dirty:t,ctx:e}),le.$set(q)},i(e){wn||(g(T.$$.fragment,e),g(k.$$.fragment,e),g(ce.$$.fragment,e),g(fe.$$.fragment,e),g(_e.$$.fragment,e),g(be.$$.fragment,e),g(we.$$.fragment,e),g(xe.$$.fragment,e),g(Ce.$$.fragment,e),g(ze.$$.fragment,e),g(Ue.$$.fragment,e),g(Je.$$.fragment,e),g(We.$$.fragment,e),g(qe.$$.fragment,e),g(Be.$$.fragment,e),g(Ge.$$.fragment,e),g(O.$$.fragment,e),g(Re.$$.fragment,e),g(Se.$$.fragment,e),g(Ne.$$.fragment,e),g(K.$$.fragment,e),g(Xe.$$.fragment,e),g(Ve.$$.fragment,e),g(Ee.$$.fragment,e),g(ee.$$.fragment,e),g(te.$$.fragment,e),g(Ae.$$.fragment,e),g(Qe.$$.fragment,e),g(Ye.$$.fragment,e),g(ne.$$.fragment,e),g(De.$$.fragment,e),g(Oe.$$.fragment,e),g(Ke.$$.fragment,e),g(oe.$$.fragment,e),g(ae.$$.fragment,e),g(se.$$.fragment,e),g(et.$$.fragment,e),g(tt.$$.fragment,e),g(nt.$$.fragment,e),g(re.$$.fragment,e),g(ie.$$.fragment,e),g(le.$$.fragment,e),wn=!0)},o(e){_(T.$$.fragment,e),_(k.$$.fragment,e),_(ce.$$.fragment,e),_(fe.$$.fragment,e),_(_e.$$.fragment,e),_(be.$$.fragment,e),_(we.$$.fragment,e),_(xe.$$.fragment,e),_(Ce.$$.fragment,e),_(ze.$$.fragment,e),_(Ue.$$.fragment,e),_(Je.$$.fragment,e),_(We.$$.fragment,e),_(qe.$$.fragment,e),_(Be.$$.fragment,e),_(Ge.$$.fragment,e),_(O.$$.fragment,e),_(Re.$$.fragment,e),_(Se.$$.fragment,e),_(Ne.$$.fragment,e),_(K.$$.fragment,e),_(Xe.$$.fragment,e),_(Ve.$$.fragment,e),_(Ee.$$.fragment,e),_(ee.$$.fragment,e),_(te.$$.fragment,e),_(Ae.$$.fragment,e),_(Qe.$$.fragment,e),_(Ye.$$.fragment,e),_(ne.$$.fragment,e),_(De.$$.fragment,e),_(Oe.$$.fragment,e),_(Ke.$$.fragment,e),_(oe.$$.fragment,e),_(ae.$$.fragment,e),_(se.$$.fragment,e),_(et.$$.fragment,e),_(tt.$$.fragment,e),_(nt.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),wn=!1},d(e){e&&(n(b),n(l),n(y),n(m),n(Ut),n(de),n(Jt),n(It),n(pe),n(Wt),n(me),n(Ht),n(ue),n(Zt),n(he),n(qt),n(Pt),n(ge),n(Bt),n(Gt),n(Me),n(Rt),n(ye),n(St),n(Nt),n(ve),n(Xt),n(Te),n(Vt),n(ke),n(Et),n(At),n($e),n(Qt),n(Yt),n(Dt),n(Fe),n(Ot),n(Kt),n(je),n(en),n(Le),n(tn),n(nn),n(on),n(Ie),n(an),n(D),n(sn),n(rn),n(He),n(ln),n(Ze),n(dn),n(cn),n(Pe),n(pn),n(mn),n(j),n(un),n(hn),n(L),n(fn),n(gn),n(E),n(_n),n(Mn),n(w),n(yn),n(bn),n($),n(vn),n(Tn),n(x),n(kn),n(Lt)),n(o),M(T,e),M(k,e),M(ce,e),M(fe,e),M(_e,e),M(be,e),M(we,e),M(xe,e),M(Ce,e),M(ze,e),M(Ue,e),M(Je,e),M(We,e),M(qe,e),M(Be,e),M(Ge),M(O),M(Re,e),M(Se),M(Ne),M(K),M(Xe,e),M(Ve),M(Ee),M(ee),M(te),M(Ae,e),M(Qe),M(Ye),M(ne),M(De,e),M(Oe),M(Ke),M(oe),M(ae),M(se),M(et,e),M(tt),M(nt),M(re),M(ie),M(le)}}}const $a='{"title":"Mistral","local":"mistral","sections":[{"title":"Overview","local":"overview","sections":[{"title":"Model Details","local":"model-details","sections":[],"depth":3},{"title":"License","local":"license","sections":[],"depth":3}],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Combining Mistral and Flash Attention 2","local":"combining-mistral-and-flash-attention-2","sections":[{"title":"Expected speedups","local":"expected-speedups","sections":[],"depth":3},{"title":"Sliding window Attention","local":"sliding-window-attention","sections":[],"depth":3}],"depth":2},{"title":"The Mistral Team","local":"the-mistral-team","sections":[],"depth":2},{"title":"MistralConfig","local":"transformers.MistralConfig","sections":[],"depth":2},{"title":"MistralModel","local":"transformers.MistralModel","sections":[],"depth":2},{"title":"MistralForCausalLM","local":"transformers.MistralForCausalLM","sections":[],"depth":2},{"title":"MistralForSequenceClassification","local":"transformers.MistralForSequenceClassification","sections":[],"depth":2},{"title":"FlaxMistralModel","local":"transformers.FlaxMistralModel","sections":[],"depth":2},{"title":"FlaxMistralForCausalLM","local":"transformers.FlaxMistralForCausalLM","sections":[],"depth":2}],"depth":1}';function xa(v){return la(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ia extends da{constructor(o){super(),ca(this,o,xa,wa,ia,{})}}export{Ia as component};
