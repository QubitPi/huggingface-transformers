import{s as Ke,o as et,n as ce}from"../chunks/scheduler.9bc65507.js";import{S as tt,i as ot,g as c,s as l,r as g,A as st,h as d,f as a,c as m,j as O,u as w,x as v,k as K,y as f,a as n,v as _,d as b,t as y,w as M}from"../chunks/index.707bf1b6.js";import{T as Oe}from"../chunks/Tip.c2ecdbf4.js";import{D as le}from"../chunks/Docstring.17db21ae.js";import{C as Ne}from"../chunks/CodeBlock.54a9f38d.js";import{E as ke}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as me}from"../chunks/Heading.342b1fa6.js";function at(F){let t,h="Example:",r,i,p;return i=new Ne({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFN3aWZ0Rm9ybWVyQ29uZmlnJTJDJTIwU3dpZnRGb3JtZXJNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBTd2lmdEZvcm1lciUyMHN3aWZ0Zm9ybWVyLWJhc2UtcGF0Y2gxNi0yMjQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwU3dpZnRGb3JtZXJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwc3dpZnRmb3JtZXItYmFzZS1wYXRjaDE2LTIyNCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwU3dpZnRGb3JtZXJNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SwiftFormerConfig, SwiftFormerModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a SwiftFormer swiftformer-base-patch16-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = SwiftFormerConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the swiftformer-base-patch16-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SwiftFormerModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=c("p"),t.textContent=h,r=l(),g(i.$$.fragment)},l(o){t=d(o,"P",{"data-svelte-h":!0}),v(t)!=="svelte-11lpom8"&&(t.textContent=h),r=m(o),w(i.$$.fragment,o)},m(o,u){n(o,t,u),n(o,r,u),_(i,o,u),p=!0},p:ce,i(o){p||(b(i.$$.fragment,o),p=!0)},o(o){y(i.$$.fragment,o),p=!1},d(o){o&&(a(t),a(r)),M(i,o)}}}function nt(F){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=h},l(r){t=d(r,"P",{"data-svelte-h":!0}),v(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(r,i){n(r,t,i)},p:ce,d(r){r&&a(t)}}}function rt(F){let t,h="Example:",r,i,p;return i=new Ne({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN3aWZ0Rm9ybWVyTW9kZWwlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMk1CWlVBSSUyRnN3aWZ0Zm9ybWVyLXhzJTIyKSUwQW1vZGVsJTIwJTNEJTIwU3dpZnRGb3JtZXJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyTUJaVUFJJTJGc3dpZnRmb3JtZXIteHMlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, SwiftFormerModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;MBZUAI/swiftformer-xs&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SwiftFormerModel.from_pretrained(<span class="hljs-string">&quot;MBZUAI/swiftformer-xs&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">220</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`,wrap:!1}}),{c(){t=c("p"),t.textContent=h,r=l(),g(i.$$.fragment)},l(o){t=d(o,"P",{"data-svelte-h":!0}),v(t)!=="svelte-11lpom8"&&(t.textContent=h),r=m(o),w(i.$$.fragment,o)},m(o,u){n(o,t,u),n(o,r,u),_(i,o,u),p=!0},p:ce,i(o){p||(b(i.$$.fragment,o),p=!0)},o(o){y(i.$$.fragment,o),p=!1},d(o){o&&(a(t),a(r)),M(i,o)}}}function it(F){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=h},l(r){t=d(r,"P",{"data-svelte-h":!0}),v(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(r,i){n(r,t,i)},p:ce,d(r){r&&a(t)}}}function lt(F){let t,h="Example:",r,i,p;return i=new Ne({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN3aWZ0Rm9ybWVyRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyTUJaVUFJJTJGc3dpZnRmb3JtZXIteHMlMjIpJTBBbW9kZWwlMjAlM0QlMjBTd2lmdEZvcm1lckZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMk1CWlVBSSUyRnN3aWZ0Zm9ybWVyLXhzJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, SwiftFormerForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;MBZUAI/swiftformer-xs&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = SwiftFormerForImageClassification.from_pretrained(<span class="hljs-string">&quot;MBZUAI/swiftformer-xs&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){t=c("p"),t.textContent=h,r=l(),g(i.$$.fragment)},l(o){t=d(o,"P",{"data-svelte-h":!0}),v(t)!=="svelte-11lpom8"&&(t.textContent=h),r=m(o),w(i.$$.fragment,o)},m(o,u){n(o,t,u),n(o,r,u),_(i,o,u),p=!0},p:ce,i(o){p||(b(i.$$.fragment,o),p=!0)},o(o){y(i.$$.fragment,o),p=!1},d(o){o&&(a(t),a(r)),M(i,o)}}}function mt(F){let t,h,r,i,p,o,u,de,N,Be='The SwiftFormer model was proposed in <a href="https://arxiv.org/abs/2303.15446" rel="nofollow">SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications</a> by Abdelrahman Shaker, Muhammad Maaz, Hanoona Rasheed, Salman Khan, Ming-Hsuan Yang, Fahad Shahbaz Khan.',fe,B,ze="The SwiftFormer paper introduces a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations in the self-attention computation with linear element-wise multiplications. A series of models called ‘SwiftFormer’ is built based on this, which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Even their small variant achieves 78.5% top-1 ImageNet1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2× faster compared to MobileViT-v2.",pe,z,Pe="The abstract from the paper is the following:",he,P,Xe="<em>Self-attention has become a defacto choice for capturing global context in various vision applications. However, its quadratic computational complexity with respect to image resolution limits its use in real-time applications, especially for deployment on resource-constrained mobile devices. Although hybrid approaches have been proposed to combine the advantages of convolutions and self-attention for a better speed-accuracy trade-off, the expensive matrix multiplication operations in self-attention remain a bottleneck. In this work, we introduce a novel efficient additive attention mechanism that effectively replaces the quadratic matrix multiplication operations with linear element-wise multiplications. Our design shows that the key-value interaction can be replaced with a linear layer without sacrificing any accuracy. Unlike previous state-of-the-art methods, our efficient formulation of self-attention enables its usage at all stages of the network. Using our proposed efficient additive attention, we build a series of models called “SwiftFormer” which achieves state-of-the-art performance in terms of both accuracy and mobile inference speed. Our small variant achieves 78.5% top-1 ImageNet-1K accuracy with only 0.8 ms latency on iPhone 14, which is more accurate and 2x faster compared to MobileViT-v2.</em>",ue,X,Qe=`This model was contributed by <a href="https://huggingface.co/shehan97" rel="nofollow">shehan97</a>.
The original code can be found <a href="https://github.com/Amshaker/SwiftFormer" rel="nofollow">here</a>.`,ge,Q,we,$,H,Te,ee,He=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerModel">SwiftFormerModel</a>. It is used to instantiate an
SwiftFormer model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the SwiftFormer
<a href="https://huggingface.co/MBZUAI/swiftformer-xs" rel="nofollow">MBZUAI/swiftformer-xs</a> architecture.`,Fe,te,Ee=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ce,W,_e,E,be,x,L,Se,oe,Le=`The bare SwiftFormer Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,xe,C,Y,Ue,se,Ye='The <a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerModel">SwiftFormerModel</a> forward method, overrides the <code>__call__</code> special method.',je,R,Je,V,ye,q,Me,T,A,Ze,ae,qe="SwiftFormer Model transformer with an image classification head on top (e.g. for ImageNet).",Ie,ne,Ae=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,We,S,D,Re,re,De='The <a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerForImageClassification">SwiftFormerForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',Ve,G,Ge,k,ve,ie,$e;return p=new me({props:{title:"SwiftFormer",local:"swiftformer",headingTag:"h1"}}),u=new me({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Q=new me({props:{title:"SwiftFormerConfig",local:"transformers.SwiftFormerConfig",headingTag:"h2"}}),H=new le({props:{name:"class transformers.SwiftFormerConfig",anchor:"transformers.SwiftFormerConfig",parameters:[{name:"num_channels",val:" = 3"},{name:"depths",val:" = [3, 3, 6, 4]"},{name:"embed_dims",val:" = [48, 56, 112, 220]"},{name:"mlp_ratio",val:" = 4"},{name:"downsamples",val:" = [True, True, True, True]"},{name:"hidden_act",val:" = 'gelu'"},{name:"down_patch_size",val:" = 3"},{name:"down_stride",val:" = 2"},{name:"down_pad",val:" = 1"},{name:"drop_path_rate",val:" = 0.0"},{name:"use_layer_scale",val:" = True"},{name:"layer_scale_init_value",val:" = 1e-05"},{name:"batch_norm_eps",val:" = 1e-05"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.SwiftFormerConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels`,name:"num_channels"},{anchor:"transformers.SwiftFormerConfig.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 6, 4]</code>) &#x2014;
Depth of each stage`,name:"depths"},{anchor:"transformers.SwiftFormerConfig.embed_dims",description:`<strong>embed_dims</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[48, 56, 112, 220]</code>) &#x2014;
The embedding dimension at each stage`,name:"embed_dims"},{anchor:"transformers.SwiftFormerConfig.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Ratio of size of the hidden dimensionality of an MLP to the dimensionality of its input.`,name:"mlp_ratio"},{anchor:"transformers.SwiftFormerConfig.downsamples",description:`<strong>downsamples</strong> (<code>List[bool]</code>, <em>optional</em>, defaults to <code>[True, True, True, True]</code>) &#x2014;
Whether or not to downsample inputs between two stages.`,name:"downsamples"},{anchor:"transformers.SwiftFormerConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (string). <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.SwiftFormerConfig.down_patch_size",description:`<strong>down_patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The size of patches in downsampling layers.`,name:"down_patch_size"},{anchor:"transformers.SwiftFormerConfig.down_stride",description:`<strong>down_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The stride of convolution kernels in downsampling layers.`,name:"down_stride"},{anchor:"transformers.SwiftFormerConfig.down_pad",description:`<strong>down_pad</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding in downsampling layers.`,name:"down_pad"},{anchor:"transformers.SwiftFormerConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Rate at which to increase dropout probability in DropPath.`,name:"drop_path_rate"},{anchor:"transformers.SwiftFormerConfig.use_layer_scale",description:`<strong>use_layer_scale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to scale outputs from token mixers.`,name:"use_layer_scale"},{anchor:"transformers.SwiftFormerConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
Factor by which outputs from token mixers are scaled.`,name:"layer_scale_init_value"},{anchor:"transformers.SwiftFormerConfig.batch_norm_eps",description:`<strong>batch_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the batch normalization layers.`,name:"batch_norm_eps"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swiftformer/configuration_swiftformer.py#L34"}}),W=new ke({props:{anchor:"transformers.SwiftFormerConfig.example",$$slots:{default:[at]},$$scope:{ctx:F}}}),E=new me({props:{title:"SwiftFormerModel",local:"transformers.SwiftFormerModel",headingTag:"h2"}}),L=new le({props:{name:"class transformers.SwiftFormerModel",anchor:"transformers.SwiftFormerModel",parameters:[{name:"config",val:": SwiftFormerConfig"}],parametersDescription:[{anchor:"transformers.SwiftFormerModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig">SwiftFormerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swiftformer/modeling_swiftformer.py#L471"}}),Y=new le({props:{name:"forward",anchor:"transformers.SwiftFormerModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.SwiftFormerModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.SwiftFormerModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.SwiftFormerModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swiftformer/modeling_swiftformer.py#L486",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig"
>SwiftFormerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new Oe({props:{$$slots:{default:[nt]},$$scope:{ctx:F}}}),V=new ke({props:{anchor:"transformers.SwiftFormerModel.forward.example",$$slots:{default:[rt]},$$scope:{ctx:F}}}),q=new me({props:{title:"SwiftFormerForImageClassification",local:"transformers.SwiftFormerForImageClassification",headingTag:"h2"}}),A=new le({props:{name:"class transformers.SwiftFormerForImageClassification",anchor:"transformers.SwiftFormerForImageClassification",parameters:[{name:"config",val:": SwiftFormerConfig"}],parametersDescription:[{anchor:"transformers.SwiftFormerForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig">SwiftFormerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swiftformer/modeling_swiftformer.py#L526"}}),D=new le({props:{name:"forward",anchor:"transformers.SwiftFormerForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.SwiftFormerForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.SwiftFormerForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.SwiftFormerForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.SwiftFormerForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swiftformer/modeling_swiftformer.py#L549",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/swiftformer#transformers.SwiftFormerConfig"
>SwiftFormerConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),G=new Oe({props:{$$slots:{default:[it]},$$scope:{ctx:F}}}),k=new ke({props:{anchor:"transformers.SwiftFormerForImageClassification.forward.example",$$slots:{default:[lt]},$$scope:{ctx:F}}}),{c(){t=c("meta"),h=l(),r=c("p"),i=l(),g(p.$$.fragment),o=l(),g(u.$$.fragment),de=l(),N=c("p"),N.innerHTML=Be,fe=l(),B=c("p"),B.textContent=ze,pe=l(),z=c("p"),z.textContent=Pe,he=l(),P=c("p"),P.innerHTML=Xe,ue=l(),X=c("p"),X.innerHTML=Qe,ge=l(),g(Q.$$.fragment),we=l(),$=c("div"),g(H.$$.fragment),Te=l(),ee=c("p"),ee.innerHTML=He,Fe=l(),te=c("p"),te.innerHTML=Ee,Ce=l(),g(W.$$.fragment),_e=l(),g(E.$$.fragment),be=l(),x=c("div"),g(L.$$.fragment),Se=l(),oe=c("p"),oe.innerHTML=Le,xe=l(),C=c("div"),g(Y.$$.fragment),Ue=l(),se=c("p"),se.innerHTML=Ye,je=l(),g(R.$$.fragment),Je=l(),g(V.$$.fragment),ye=l(),g(q.$$.fragment),Me=l(),T=c("div"),g(A.$$.fragment),Ze=l(),ae=c("p"),ae.textContent=qe,Ie=l(),ne=c("p"),ne.innerHTML=Ae,We=l(),S=c("div"),g(D.$$.fragment),Re=l(),re=c("p"),re.innerHTML=De,Ve=l(),g(G.$$.fragment),Ge=l(),g(k.$$.fragment),ve=l(),ie=c("p"),this.h()},l(e){const s=st("svelte-u9bgzb",document.head);t=d(s,"META",{name:!0,content:!0}),s.forEach(a),h=m(e),r=d(e,"P",{}),O(r).forEach(a),i=m(e),w(p.$$.fragment,e),o=m(e),w(u.$$.fragment,e),de=m(e),N=d(e,"P",{"data-svelte-h":!0}),v(N)!=="svelte-1xrpfhe"&&(N.innerHTML=Be),fe=m(e),B=d(e,"P",{"data-svelte-h":!0}),v(B)!=="svelte-q1qv2z"&&(B.textContent=ze),pe=m(e),z=d(e,"P",{"data-svelte-h":!0}),v(z)!=="svelte-vfdo9a"&&(z.textContent=Pe),he=m(e),P=d(e,"P",{"data-svelte-h":!0}),v(P)!=="svelte-1hgv92f"&&(P.innerHTML=Xe),ue=m(e),X=d(e,"P",{"data-svelte-h":!0}),v(X)!=="svelte-18uisef"&&(X.innerHTML=Qe),ge=m(e),w(Q.$$.fragment,e),we=m(e),$=d(e,"DIV",{class:!0});var U=O($);w(H.$$.fragment,U),Te=m(U),ee=d(U,"P",{"data-svelte-h":!0}),v(ee)!=="svelte-1rvnpyv"&&(ee.innerHTML=He),Fe=m(U),te=d(U,"P",{"data-svelte-h":!0}),v(te)!=="svelte-o55m63"&&(te.innerHTML=Ee),Ce=m(U),w(W.$$.fragment,U),U.forEach(a),_e=m(e),w(E.$$.fragment,e),be=m(e),x=d(e,"DIV",{class:!0});var I=O(x);w(L.$$.fragment,I),Se=m(I),oe=d(I,"P",{"data-svelte-h":!0}),v(oe)!=="svelte-12h2za7"&&(oe.innerHTML=Le),xe=m(I),C=d(I,"DIV",{class:!0});var j=O(C);w(Y.$$.fragment,j),Ue=m(j),se=d(j,"P",{"data-svelte-h":!0}),v(se)!=="svelte-1lpypw3"&&(se.innerHTML=Ye),je=m(j),w(R.$$.fragment,j),Je=m(j),w(V.$$.fragment,j),j.forEach(a),I.forEach(a),ye=m(e),w(q.$$.fragment,e),Me=m(e),T=d(e,"DIV",{class:!0});var J=O(T);w(A.$$.fragment,J),Ze=m(J),ae=d(J,"P",{"data-svelte-h":!0}),v(ae)!=="svelte-cokk7a"&&(ae.textContent=qe),Ie=m(J),ne=d(J,"P",{"data-svelte-h":!0}),v(ne)!=="svelte-1gjh92c"&&(ne.innerHTML=Ae),We=m(J),S=d(J,"DIV",{class:!0});var Z=O(S);w(D.$$.fragment,Z),Re=m(Z),re=d(Z,"P",{"data-svelte-h":!0}),v(re)!=="svelte-ut67bx"&&(re.innerHTML=De),Ve=m(Z),w(G.$$.fragment,Z),Ge=m(Z),w(k.$$.fragment,Z),Z.forEach(a),J.forEach(a),ve=m(e),ie=d(e,"P",{}),O(ie).forEach(a),this.h()},h(){K(t,"name","hf:doc:metadata"),K(t,"content",ct),K($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),K(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),K(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),K(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),K(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){f(document.head,t),n(e,h,s),n(e,r,s),n(e,i,s),_(p,e,s),n(e,o,s),_(u,e,s),n(e,de,s),n(e,N,s),n(e,fe,s),n(e,B,s),n(e,pe,s),n(e,z,s),n(e,he,s),n(e,P,s),n(e,ue,s),n(e,X,s),n(e,ge,s),_(Q,e,s),n(e,we,s),n(e,$,s),_(H,$,null),f($,Te),f($,ee),f($,Fe),f($,te),f($,Ce),_(W,$,null),n(e,_e,s),_(E,e,s),n(e,be,s),n(e,x,s),_(L,x,null),f(x,Se),f(x,oe),f(x,xe),f(x,C),_(Y,C,null),f(C,Ue),f(C,se),f(C,je),_(R,C,null),f(C,Je),_(V,C,null),n(e,ye,s),_(q,e,s),n(e,Me,s),n(e,T,s),_(A,T,null),f(T,Ze),f(T,ae),f(T,Ie),f(T,ne),f(T,We),f(T,S),_(D,S,null),f(S,Re),f(S,re),f(S,Ve),_(G,S,null),f(S,Ge),_(k,S,null),n(e,ve,s),n(e,ie,s),$e=!0},p(e,[s]){const U={};s&2&&(U.$$scope={dirty:s,ctx:e}),W.$set(U);const I={};s&2&&(I.$$scope={dirty:s,ctx:e}),R.$set(I);const j={};s&2&&(j.$$scope={dirty:s,ctx:e}),V.$set(j);const J={};s&2&&(J.$$scope={dirty:s,ctx:e}),G.$set(J);const Z={};s&2&&(Z.$$scope={dirty:s,ctx:e}),k.$set(Z)},i(e){$e||(b(p.$$.fragment,e),b(u.$$.fragment,e),b(Q.$$.fragment,e),b(H.$$.fragment,e),b(W.$$.fragment,e),b(E.$$.fragment,e),b(L.$$.fragment,e),b(Y.$$.fragment,e),b(R.$$.fragment,e),b(V.$$.fragment,e),b(q.$$.fragment,e),b(A.$$.fragment,e),b(D.$$.fragment,e),b(G.$$.fragment,e),b(k.$$.fragment,e),$e=!0)},o(e){y(p.$$.fragment,e),y(u.$$.fragment,e),y(Q.$$.fragment,e),y(H.$$.fragment,e),y(W.$$.fragment,e),y(E.$$.fragment,e),y(L.$$.fragment,e),y(Y.$$.fragment,e),y(R.$$.fragment,e),y(V.$$.fragment,e),y(q.$$.fragment,e),y(A.$$.fragment,e),y(D.$$.fragment,e),y(G.$$.fragment,e),y(k.$$.fragment,e),$e=!1},d(e){e&&(a(h),a(r),a(i),a(o),a(de),a(N),a(fe),a(B),a(pe),a(z),a(he),a(P),a(ue),a(X),a(ge),a(we),a($),a(_e),a(be),a(x),a(ye),a(Me),a(T),a(ve),a(ie)),a(t),M(p,e),M(u,e),M(Q,e),M(H),M(W),M(E,e),M(L),M(Y),M(R),M(V),M(q,e),M(A),M(D),M(G),M(k)}}}const ct='{"title":"SwiftFormer","local":"swiftformer","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"SwiftFormerConfig","local":"transformers.SwiftFormerConfig","sections":[],"depth":2},{"title":"SwiftFormerModel","local":"transformers.SwiftFormerModel","sections":[],"depth":2},{"title":"SwiftFormerForImageClassification","local":"transformers.SwiftFormerForImageClassification","sections":[],"depth":2}],"depth":1}';function dt(F){return et(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bt extends tt{constructor(t){super(),ot(this,t,dt,mt,Ke,{})}}export{bt as component};
