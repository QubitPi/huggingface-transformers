import{s as st,o as at,n as nt}from"../chunks/scheduler.9bc65507.js";import{S as ot,i as rt,g as l,s,r as h,A as lt,h as i,f as n,c as a,j as W,u as g,x as m,k as G,y as r,a as o,v as k,d as b,t as T,w}from"../chunks/index.707bf1b6.js";import{T as it}from"../chunks/Tip.c2ecdbf4.js";import{D as le}from"../chunks/Docstring.17db21ae.js";import{C as Pe}from"../chunks/CodeBlock.54a9f38d.js";import{E as pt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as Re}from"../chunks/Heading.342b1fa6.js";function ct(F){let p,j=`This implementation is the same as BERT, except for tokenization method. Refer to <a href="bert">BERT documentation</a> for
API reference information.`;return{c(){p=l("p"),p.innerHTML=j},l(f){p=i(f,"P",{"data-svelte-h":!0}),m(p)!=="svelte-wqppg4"&&(p.innerHTML=j)},m(f,_){o(f,p,_)},p:nt,d(f){f&&n(p)}}}function dt(F){let p,j="pair mask has the following format:",f,_,M;return _=new Pe({props:{code:"MCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMEElN0MlMjBmaXJzdCUyMHNlcXVlbmNlJTIwJTIwJTIwJTIwJTdDJTIwc2Vjb25kJTIwc2VxdWVuY2UlMjAlN0M=",highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`,wrap:!1}}),{c(){p=l("p"),p.textContent=j,f=s(),h(_.$$.fragment)},l(c){p=i(c,"P",{"data-svelte-h":!0}),m(p)!=="svelte-qjgeij"&&(p.textContent=j),f=a(c),g(_.$$.fragment,c)},m(c,v){o(c,p,v),o(c,f,v),k(_,c,v),M=!0},p:nt,i(c){M||(b(_.$$.fragment,c),M=!0)},o(c){T(_.$$.fragment,c),M=!1},d(c){c&&(n(p),n(f)),w(_,c)}}}function mt(F){let p,j,f,_,M,c,v,ie,B,Qe="The BERT models trained on Japanese text.",pe,q,He="There are models with two different tokenization methods:",ce,U,Ze='<li>Tokenize with MeCab and WordPiece. This requires some extra dependencies, <a href="https://github.com/polm/fugashi" rel="nofollow">fugashi</a> which is a wrapper around <a href="https://taku910.github.io/mecab/" rel="nofollow">MeCab</a>.</li> <li>Tokenize into characters.</li>',de,D,Ae=`To use <em>MecabTokenizer</em>, you should <code>pip install transformers[&quot;ja&quot;]</code> (or <code>pip install -e .[&quot;ja&quot;]</code> if you install
from source) to install dependencies.`,me,L,Ve='See <a href="https://github.com/cl-tohoku/bert-japanese" rel="nofollow">details on cl-tohoku repository</a>.',ue,R,Ne="Example of using a model with MeCab and WordPiece tokenization:",fe,P,_e,Q,Xe="Example of using a model with Character tokenization:",he,H,ge,Z,Se='This model was contributed by <a href="https://huggingface.co/cl-tohoku" rel="nofollow">cl-tohoku</a>.',ke,y,be,A,Te,d,V,je,Y,Oe="Construct a BERT tokenizer for Japanese text.",Je,K,We=`This tokenizer inherits from <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer
to: this superclass for more information regarding those methods.`,xe,J,N,ye,ee,Ge=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`,ze,te,Fe="<li>single sequence: <code>[CLS] X [SEP]</code></li> <li>pair of sequences: <code>[CLS] A [SEP] B [SEP]</code></li>",Ce,z,X,Ee,ne,Ye="Converts a sequence of tokens (string) in a single string.",Ie,$,S,Be,se,Ke="Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence",qe,C,Ue,ae,et="If <code>token_ids_1</code> is <code>None</code>, this method only returns the first portion of the mask (0s).",De,E,O,Le,oe,tt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,we,re,Me;return M=new Re({props:{title:"BertJapanese",local:"bertjapanese",headingTag:"h1"}}),v=new Re({props:{title:"Overview",local:"overview",headingTag:"h2"}}),P=new Pe({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQWJlcnRqYXBhbmVzZSUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyY2wtdG9ob2t1JTJGYmVydC1iYXNlLWphcGFuZXNlJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmNsLXRvaG9rdSUyRmJlcnQtYmFzZS1qYXBhbmVzZSUyMiklMEElMEElMjMlMjMlMjBJbnB1dCUyMEphcGFuZXNlJTIwVGV4dCUwQWxpbmUlMjAlM0QlMjAlMjIlRTUlOTAlQkUlRTglQkMlQTklRTMlODElQUYlRTclOEMlQUIlRTMlODElQTclRTMlODElODIlRTMlODIlOEIlRTMlODAlODIlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIobGluZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShpbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQlNUIwJTVEKSklMEElMEFvdXRwdXRzJTIwJTNEJTIwYmVydGphcGFuZXNlKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>bertjapanese = AutoModel.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">## Input Japanese Text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;吾輩は猫である。&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(line, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]))
[CLS] 吾輩 は 猫 で ある 。 [SEP]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = bertjapanese(**inputs)`,wrap:!1}}),H=new Pe({props:{code:"YmVydGphcGFuZXNlJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJjbC10b2hva3UlMkZiZXJ0LWJhc2UtamFwYW5lc2UtY2hhciUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJjbC10b2hva3UlMkZiZXJ0LWJhc2UtamFwYW5lc2UtY2hhciUyMiklMEElMEElMjMlMjMlMjBJbnB1dCUyMEphcGFuZXNlJTIwVGV4dCUwQWxpbmUlMjAlM0QlMjAlMjIlRTUlOTAlQkUlRTglQkMlQTklRTMlODElQUYlRTclOEMlQUIlRTMlODElQTclRTMlODElODIlRTMlODIlOEIlRTMlODAlODIlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIobGluZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShpbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQlNUIwJTVEKSklMEElMEFvdXRwdXRzJTIwJTNEJTIwYmVydGphcGFuZXNlKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>bertjapanese = AutoModel.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese-char&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese-char&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">## Input Japanese Text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;吾輩は猫である。&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(line, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]))
[CLS] 吾 輩 は 猫 で あ る 。 [SEP]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = bertjapanese(**inputs)`,wrap:!1}}),y=new it({props:{$$slots:{default:[ct]},$$scope:{ctx:F}}}),A=new Re({props:{title:"BertJapaneseTokenizer",local:"transformers.BertJapaneseTokenizer",headingTag:"h2"}}),V=new le({props:{name:"class transformers.BertJapaneseTokenizer",anchor:"transformers.BertJapaneseTokenizer",parameters:[{name:"vocab_file",val:""},{name:"spm_file",val:" = None"},{name:"do_lower_case",val:" = False"},{name:"do_word_tokenize",val:" = True"},{name:"do_subword_tokenize",val:" = True"},{name:"word_tokenizer_type",val:" = 'basic'"},{name:"subword_tokenizer_type",val:" = 'wordpiece'"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"mecab_kwargs",val:" = None"},{name:"sudachi_kwargs",val:" = None"},{name:"jumanpp_kwargs",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to a one-wordpiece-per-line vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertJapaneseTokenizer.spm_file",description:`<strong>spm_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to <a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a .spm or .model
extension) that contains the vocabulary.`,name:"spm_file"},{anchor:"transformers.BertJapaneseTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to lower case the input. Only has an effect when do_basic_tokenize=True.`,name:"do_lower_case"},{anchor:"transformers.BertJapaneseTokenizer.do_word_tokenize",description:`<strong>do_word_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to do word tokenization.`,name:"do_word_tokenize"},{anchor:"transformers.BertJapaneseTokenizer.do_subword_tokenize",description:`<strong>do_subword_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to do subword tokenization.`,name:"do_subword_tokenize"},{anchor:"transformers.BertJapaneseTokenizer.word_tokenizer_type",description:`<strong>word_tokenizer_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;basic&quot;</code>) &#x2014;
Type of word tokenizer. Choose from [&#x201C;basic&#x201D;, &#x201C;mecab&#x201D;, &#x201C;sudachi&#x201D;, &#x201C;jumanpp&#x201D;].`,name:"word_tokenizer_type"},{anchor:"transformers.BertJapaneseTokenizer.subword_tokenizer_type",description:`<strong>subword_tokenizer_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;wordpiece&quot;</code>) &#x2014;
Type of subword tokenizer. Choose from [&#x201C;wordpiece&#x201D;, &#x201C;character&#x201D;, &#x201C;sentencepiece&#x201D;,].`,name:"subword_tokenizer_type"},{anchor:"transformers.BertJapaneseTokenizer.mecab_kwargs",description:`<strong>mecab_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary passed to the <code>MecabTokenizer</code> constructor.`,name:"mecab_kwargs"},{anchor:"transformers.BertJapaneseTokenizer.sudachi_kwargs",description:`<strong>sudachi_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary passed to the <code>SudachiTokenizer</code> constructor.`,name:"sudachi_kwargs"},{anchor:"transformers.BertJapaneseTokenizer.jumanpp_kwargs",description:`<strong>jumanpp_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary passed to the <code>JumanppTokenizer</code> constructor.`,name:"jumanpp_kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L107"}}),N=new le({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertJapaneseTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertJapaneseTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L307",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),X=new le({props:{name:"convert_tokens_to_string",anchor:"transformers.BertJapaneseTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L299"}}),S=new le({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BertJapaneseTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertJapaneseTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L362",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),C=new pt({props:{anchor:"transformers.BertJapaneseTokenizer.create_token_type_ids_from_sequences.example",$$slots:{default:[dt]},$$scope:{ctx:F}}}),O=new le({props:{name:"get_special_tokens_mask",anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L333",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),{c(){p=l("meta"),j=s(),f=l("p"),_=s(),h(M.$$.fragment),c=s(),h(v.$$.fragment),ie=s(),B=l("p"),B.textContent=Qe,pe=s(),q=l("p"),q.textContent=He,ce=s(),U=l("ul"),U.innerHTML=Ze,de=s(),D=l("p"),D.innerHTML=Ae,me=s(),L=l("p"),L.innerHTML=Ve,ue=s(),R=l("p"),R.textContent=Ne,fe=s(),h(P.$$.fragment),_e=s(),Q=l("p"),Q.textContent=Xe,he=s(),h(H.$$.fragment),ge=s(),Z=l("p"),Z.innerHTML=Se,ke=s(),h(y.$$.fragment),be=s(),h(A.$$.fragment),Te=s(),d=l("div"),h(V.$$.fragment),je=s(),Y=l("p"),Y.textContent=Oe,Je=s(),K=l("p"),K.innerHTML=We,xe=s(),J=l("div"),h(N.$$.fragment),ye=s(),ee=l("p"),ee.textContent=Ge,ze=s(),te=l("ul"),te.innerHTML=Fe,Ce=s(),z=l("div"),h(X.$$.fragment),Ee=s(),ne=l("p"),ne.textContent=Ye,Ie=s(),$=l("div"),h(S.$$.fragment),Be=s(),se=l("p"),se.textContent=Ke,qe=s(),h(C.$$.fragment),Ue=s(),ae=l("p"),ae.innerHTML=et,De=s(),E=l("div"),h(O.$$.fragment),Le=s(),oe=l("p"),oe.innerHTML=tt,we=s(),re=l("p"),this.h()},l(e){const t=lt("svelte-u9bgzb",document.head);p=i(t,"META",{name:!0,content:!0}),t.forEach(n),j=a(e),f=i(e,"P",{}),W(f).forEach(n),_=a(e),g(M.$$.fragment,e),c=a(e),g(v.$$.fragment,e),ie=a(e),B=i(e,"P",{"data-svelte-h":!0}),m(B)!=="svelte-q6gk8k"&&(B.textContent=Qe),pe=a(e),q=i(e,"P",{"data-svelte-h":!0}),m(q)!=="svelte-130rfh4"&&(q.textContent=He),ce=a(e),U=i(e,"UL",{"data-svelte-h":!0}),m(U)!=="svelte-1nk5iha"&&(U.innerHTML=Ze),de=a(e),D=i(e,"P",{"data-svelte-h":!0}),m(D)!=="svelte-1dsac8d"&&(D.innerHTML=Ae),me=a(e),L=i(e,"P",{"data-svelte-h":!0}),m(L)!=="svelte-u7nt0b"&&(L.innerHTML=Ve),ue=a(e),R=i(e,"P",{"data-svelte-h":!0}),m(R)!=="svelte-vqnu8f"&&(R.textContent=Ne),fe=a(e),g(P.$$.fragment,e),_e=a(e),Q=i(e,"P",{"data-svelte-h":!0}),m(Q)!=="svelte-m6bjin"&&(Q.textContent=Xe),he=a(e),g(H.$$.fragment,e),ge=a(e),Z=i(e,"P",{"data-svelte-h":!0}),m(Z)!=="svelte-1ykd1l9"&&(Z.innerHTML=Se),ke=a(e),g(y.$$.fragment,e),be=a(e),g(A.$$.fragment,e),Te=a(e),d=i(e,"DIV",{class:!0});var u=W(d);g(V.$$.fragment,u),je=a(u),Y=i(u,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-1tm7ou1"&&(Y.textContent=Oe),Je=a(u),K=i(u,"P",{"data-svelte-h":!0}),m(K)!=="svelte-1ss6yiy"&&(K.innerHTML=We),xe=a(u),J=i(u,"DIV",{class:!0});var x=W(J);g(N.$$.fragment,x),ye=a(x),ee=i(x,"P",{"data-svelte-h":!0}),m(ee)!=="svelte-t7qurq"&&(ee.textContent=Ge),ze=a(x),te=i(x,"UL",{"data-svelte-h":!0}),m(te)!=="svelte-xi6653"&&(te.innerHTML=Fe),x.forEach(n),Ce=a(u),z=i(u,"DIV",{class:!0});var $e=W(z);g(X.$$.fragment,$e),Ee=a($e),ne=i($e,"P",{"data-svelte-h":!0}),m(ne)!=="svelte-b3k2yi"&&(ne.textContent=Ye),$e.forEach(n),Ie=a(u),$=i(u,"DIV",{class:!0});var I=W($);g(S.$$.fragment,I),Be=a(I),se=i(I,"P",{"data-svelte-h":!0}),m(se)!=="svelte-gn6wi7"&&(se.textContent=Ke),qe=a(I),g(C.$$.fragment,I),Ue=a(I),ae=i(I,"P",{"data-svelte-h":!0}),m(ae)!=="svelte-owoxgn"&&(ae.innerHTML=et),I.forEach(n),De=a(u),E=i(u,"DIV",{class:!0});var ve=W(E);g(O.$$.fragment,ve),Le=a(ve),oe=i(ve,"P",{"data-svelte-h":!0}),m(oe)!=="svelte-1f4f5kp"&&(oe.innerHTML=tt),ve.forEach(n),u.forEach(n),we=a(e),re=i(e,"P",{}),W(re).forEach(n),this.h()},h(){G(p,"name","hf:doc:metadata"),G(p,"content",ut),G(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){r(document.head,p),o(e,j,t),o(e,f,t),o(e,_,t),k(M,e,t),o(e,c,t),k(v,e,t),o(e,ie,t),o(e,B,t),o(e,pe,t),o(e,q,t),o(e,ce,t),o(e,U,t),o(e,de,t),o(e,D,t),o(e,me,t),o(e,L,t),o(e,ue,t),o(e,R,t),o(e,fe,t),k(P,e,t),o(e,_e,t),o(e,Q,t),o(e,he,t),k(H,e,t),o(e,ge,t),o(e,Z,t),o(e,ke,t),k(y,e,t),o(e,be,t),k(A,e,t),o(e,Te,t),o(e,d,t),k(V,d,null),r(d,je),r(d,Y),r(d,Je),r(d,K),r(d,xe),r(d,J),k(N,J,null),r(J,ye),r(J,ee),r(J,ze),r(J,te),r(d,Ce),r(d,z),k(X,z,null),r(z,Ee),r(z,ne),r(d,Ie),r(d,$),k(S,$,null),r($,Be),r($,se),r($,qe),k(C,$,null),r($,Ue),r($,ae),r(d,De),r(d,E),k(O,E,null),r(E,Le),r(E,oe),o(e,we,t),o(e,re,t),Me=!0},p(e,[t]){const u={};t&2&&(u.$$scope={dirty:t,ctx:e}),y.$set(u);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),C.$set(x)},i(e){Me||(b(M.$$.fragment,e),b(v.$$.fragment,e),b(P.$$.fragment,e),b(H.$$.fragment,e),b(y.$$.fragment,e),b(A.$$.fragment,e),b(V.$$.fragment,e),b(N.$$.fragment,e),b(X.$$.fragment,e),b(S.$$.fragment,e),b(C.$$.fragment,e),b(O.$$.fragment,e),Me=!0)},o(e){T(M.$$.fragment,e),T(v.$$.fragment,e),T(P.$$.fragment,e),T(H.$$.fragment,e),T(y.$$.fragment,e),T(A.$$.fragment,e),T(V.$$.fragment,e),T(N.$$.fragment,e),T(X.$$.fragment,e),T(S.$$.fragment,e),T(C.$$.fragment,e),T(O.$$.fragment,e),Me=!1},d(e){e&&(n(j),n(f),n(_),n(c),n(ie),n(B),n(pe),n(q),n(ce),n(U),n(de),n(D),n(me),n(L),n(ue),n(R),n(fe),n(_e),n(Q),n(he),n(ge),n(Z),n(ke),n(be),n(Te),n(d),n(we),n(re)),n(p),w(M,e),w(v,e),w(P,e),w(H,e),w(y,e),w(A,e),w(V),w(N),w(X),w(S),w(C),w(O)}}}const ut='{"title":"BertJapanese","local":"bertjapanese","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"BertJapaneseTokenizer","local":"transformers.BertJapaneseTokenizer","sections":[],"depth":2}],"depth":1}';function ft(F){return at(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Mt extends ot{constructor(p){super(),rt(this,p,ft,mt,st,{})}}export{Mt as component};
