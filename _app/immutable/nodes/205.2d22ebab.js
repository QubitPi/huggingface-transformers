import{s as Gn,o as Ln,n as Cn}from"../chunks/scheduler.9bc65507.js";import{S as Zn,i as In,g as r,s,r as m,m as Un,A as Wn,h as i,f as n,c as a,j as C,u as g,x as c,n as Nn,k as G,y as l,a as o,v as u,d as h,t as f,w as b}from"../chunks/index.707bf1b6.js";import{D as ye}from"../chunks/Docstring.17db21ae.js";import{C as Me}from"../chunks/CodeBlock.54a9f38d.js";import{E as qn}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as W}from"../chunks/Heading.342b1fa6.js";function Rn(Te){let p,z="Examples:",w,_,M;return _=new Me({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME5sbGJUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBObGxiVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyJTJDJTIwc3JjX2xhbmclM0QlMjJlbmdfTGF0biUyMiUyQyUyMHRndF9sYW5nJTNEJTIyZnJhX0xhdG4lMjIlMEEpJTBBZXhhbXBsZV9lbmdsaXNoX3BocmFzZSUyMCUzRCUyMCUyMiUyMFVOJTIwQ2hpZWYlMjBTYXlzJTIwVGhlcmUlMjBJcyUyME5vJTIwTWlsaXRhcnklMjBTb2x1dGlvbiUyMGluJTIwU3lyaWElMjIlMEFleHBlY3RlZF90cmFuc2xhdGlvbl9mcmVuY2glMjAlM0QlMjAlMjJMZSUyMGNoZWYlMjBkZSUyMGwnT05VJTIwYWZmaXJtZSUyMHF1J2lsJTIwbid5JTIwYSUyMHBhcyUyMGRlJTIwc29sdXRpb24lMjBtaWxpdGFpcmUlMjBlbiUyMFN5cmllLiUyMiUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihleGFtcGxlX2VuZ2xpc2hfcGhyYXNlJTJDJTIwdGV4dF90YXJnZXQlM0RleHBlY3RlZF90cmFuc2xhdGlvbl9mcmVuY2glMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizer.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, src_lang=<span class="hljs-string">&quot;eng_Latn&quot;</span>, tgt_lang=<span class="hljs-string">&quot;fra_Latn&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example_english_phrase = <span class="hljs-string">&quot; UN Chief Says There Is No Military Solution in Syria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>expected_translation_french = <span class="hljs-string">&quot;Le chef de l&#x27;ONU affirme qu&#x27;il n&#x27;y a pas de solution militaire en Syrie.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),{c(){p=r("p"),p.textContent=z,w=s(),m(_.$$.fragment)},l(d){p=i(d,"P",{"data-svelte-h":!0}),c(p)!=="svelte-kvfsh7"&&(p.textContent=z),w=a(d),g(_.$$.fragment,d)},m(d,v){o(d,p,v),o(d,w,v),u(_,d,v),M=!0},p:Cn,i(d){M||(h(_.$$.fragment,d),M=!0)},o(d){f(_.$$.fragment,d),M=!1},d(d){d&&(n(p),n(w)),b(_,d)}}}function Fn(Te){let p,z="Examples:",w,_,M;return _=new Me({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME5sbGJUb2tlbml6ZXJGYXN0JTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwTmxsYlRva2VuaXplckZhc3QuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMmZhY2Vib29rJTJGbmxsYi0yMDAtZGlzdGlsbGVkLTYwME0lMjIlMkMlMjBzcmNfbGFuZyUzRCUyMmVuZ19MYXRuJTIyJTJDJTIwdGd0X2xhbmclM0QlMjJmcmFfTGF0biUyMiUwQSklMEFleGFtcGxlX2VuZ2xpc2hfcGhyYXNlJTIwJTNEJTIwJTIyJTIwVU4lMjBDaGllZiUyMFNheXMlMjBUaGVyZSUyMElzJTIwTm8lMjBNaWxpdGFyeSUyMFNvbHV0aW9uJTIwaW4lMjBTeXJpYSUyMiUwQWV4cGVjdGVkX3RyYW5zbGF0aW9uX2ZyZW5jaCUyMCUzRCUyMCUyMkxlJTIwY2hlZiUyMGRlJTIwbCdPTlUlMjBhZmZpcm1lJTIwcXUnaWwlMjBuJ3klMjBhJTIwcGFzJTIwZGUlMjBzb2x1dGlvbiUyMG1pbGl0YWlyZSUyMGVuJTIwU3lyaWUuJTIyJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKGV4YW1wbGVfZW5nbGlzaF9waHJhc2UlMkMlMjB0ZXh0X3RhcmdldCUzRGV4cGVjdGVkX3RyYW5zbGF0aW9uX2ZyZW5jaCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizerFast.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, src_lang=<span class="hljs-string">&quot;eng_Latn&quot;</span>, tgt_lang=<span class="hljs-string">&quot;fra_Latn&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>example_english_phrase = <span class="hljs-string">&quot; UN Chief Says There Is No Military Solution in Syria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>expected_translation_french = <span class="hljs-string">&quot;Le chef de l&#x27;ONU affirme qu&#x27;il n&#x27;y a pas de solution militaire en Syrie.&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(example_english_phrase, text_target=expected_translation_french, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),{c(){p=r("p"),p.textContent=z,w=s(),m(_.$$.fragment)},l(d){p=i(d,"P",{"data-svelte-h":!0}),c(p)!=="svelte-kvfsh7"&&(p.textContent=z),w=a(d),g(_.$$.fragment,d)},m(d,v){o(d,p,v),o(d,w,v),u(_,d,v),M=!0},p:Cn,i(d){M||(h(_.$$.fragment,d),M=!0)},o(d){f(_.$$.fragment,d),M=!1},d(d){d&&(n(p),n(w)),b(_,d)}}}function Vn(Te){let p,z,w,_,M,d,v,Xe,R,Pt=`<strong>DISCLAIMER:</strong> The default behaviour for the tokenizer was fixed and thus changed in April 2023.
The previous version adds <code>[self.eos_token_id, self.cur_lang_code]</code> at the end of the token sequence for both target and source tokenization. This is wrong as the NLLB paper mentions (page 48, 6.1.1. Model Architecture) :`,He,F,Dt=`<em>Note that we prefix the source sequence with the source language, as opposed to the target
language as previously done in several works (Arivazhagan et al., 2019; Johnson et al.,
2017). This is primarily because we prioritize optimizing zero-shot performance of our
model on any pair of 200 languages at a minor cost to supervised performance.</em>`,Se,V,At="Previous behaviour:",Ye,B,Pe,E,Qt="New behaviour",De,X,Ae,H,Ot="Enabling the old behaviour can be done as follows:",Qe,S,Oe,Y,Kt='For more details, feel free to check the linked <a href="https://github.com/huggingface/transformers/pull/22313" rel="nofollow">PR</a> and <a href="https://github.com/huggingface/transformers/issues/19943" rel="nofollow">Issue</a>.',Ke,P,et,D,en=`The NLLB model was presented in <a href="https://arxiv.org/abs/2207.04672" rel="nofollow">No Language Left Behind: Scaling Human-Centered Machine Translation</a> by Marta R. Costa-jussà, James Cross, Onur Çelebi,
Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula,
Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,
Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,
Safiyyah Saleem, Holger Schwenk, and Jeff Wang.`,tt,A,tn="The abstract of the paper is the following:",nt,Q,nn=`<em>Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today.
However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the
200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by
first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.</em>`,st,O,sn="This implementation contains the dense models available on release.",at,K,an='<strong>The sparse model NLLB-MoE (Mixture of Expert) is now available! More details <a href="nllb-moe">here</a></strong>',ot,ee,on='This model was contributed by <a href="https://huggingface.co/lysandre" rel="nofollow">Lysandre</a>. The authors’ code can be found <a href="https://github.com/facebookresearch/fairseq/tree/nllb" rel="nofollow">here</a>.',lt,te,rt,ne,ln=`While generating the target text set the <code>forced_bos_token_id</code> to the target language id. The following
example shows how to translate English to French using the <em>facebook/nllb-200-distilled-600M</em> model.`,it,se,rn=`Note that we’re using the BCP-47 code for French <code>fra_Latn</code>. See <a href="https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200" rel="nofollow">here</a>
for the list of all BCP-47 in the Flores 200 dataset.`,ct,ae,dt,oe,pt,le,cn=`English (<code>eng_Latn</code>) is set as the default language from which to translate. In order to specify that you’d like to translate from a different language,
you should specify the BCP-47 code in the <code>src_lang</code> keyword argument of the tokenizer initialization.`,mt,re,dn="See example below for a translation from romanian to german:",gt,ie,ut,ce,ht,de,pn='<li><a href="../tasks/translation">Translation task guide</a></li> <li><a href="../tasks/summarization">Summarization task guide</a></li>',ft,pe,bt,T,me,wt,we,mn="Construct an NLLB tokenizer.",vt,ve,gn=`Adapted from <a href="/docs/transformers/main/en/model_doc/roberta#transformers.RobertaTokenizer">RobertaTokenizer</a> and <a href="/docs/transformers/main/en/model_doc/xlnet#transformers.XLNetTokenizer">XLNetTokenizer</a>. Based on
<a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a>.`,xt,xe,un="The tokenization method is <code>&lt;tokens&gt; &lt;eos&gt; &lt;language code&gt;</code> for source language documents, and `&lt;language code&gt;",jt,L,Jt,J,ge,$t,je,hn=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. An NLLB sequence has the following format, where <code>X</code> represents the sequence:`,zt,Je,fn="<li><code>input_ids</code> (for encoder) <code>X [eos, src_lang_code]</code></li> <li><code>decoder_input_ids</code>: (for decoder) <code>X [eos, tgt_lang_code]</code></li>",Ut,$e,bn=`BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a
separator.`,kt,ue,_t,k,he,Nt,ze,kn=`Construct a “fast” NLLB tokenizer (backed by HuggingFace’s <em>tokenizers</em> library). Based on
<a href="https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models" rel="nofollow">BPE</a>.`,qt,Ue,_n=`This tokenizer inherits from <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`,Ct,Ne,yn="The tokenization method is <code>&lt;tokens&gt; &lt;eos&gt; &lt;language code&gt;</code> for source language documents, and `&lt;language code&gt;",Gt,Z,Lt,j,fe,Zt,qe,Mn=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. The special tokens depend on calling set_lang.`,It,Ce,Tn="An NLLB sequence has the following format, where <code>X</code> represents the sequence:",Wt,Ge,wn="<li><code>input_ids</code> (for encoder) <code>X [eos, src_lang_code]</code></li> <li><code>decoder_input_ids</code>: (for decoder) <code>X [eos, tgt_lang_code]</code></li>",Rt,Le,vn=`BOS is never used. Pairs of sequences are not the expected use case, but they will be handled without a
separator.`,Ft,I,be,Vt,Ze,xn=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. nllb does not
make use of token type ids, therefore a list of zeros is returned.`,Bt,U,ke,Et,Ie,jn="Reset the special tokens to the source lang setting.",Xt,We,Jn="<li>In legacy mode: No prefix and suffix=[eos, src_lang_code].</li> <li>In default mode: Prefix=[src_lang_code], suffix = [eos]</li>",Ht,N,_e,St,Re,$n="Reset the special tokens to the target lang setting.",Yt,Fe,zn="<li>In legacy mode: No prefix and suffix=[eos, tgt_lang_code].</li> <li>In default mode: Prefix=[tgt_lang_code], suffix = [eos]</li>",yt,Ee,Mt;return M=new W({props:{title:"NLLB",local:"nllb",headingTag:"h1"}}),v=new W({props:{title:"Updated tokenizer behavior",local:"transformers.NllbTokenizer",headingTag:"h2"}}),B=new Me({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME5sbGJUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBObGxiVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyKSUwQXRva2VuaXplciglMjJIb3clMjB3YXMlMjB5b3VyJTIwZGF5JTNGJTIyKS5pbnB1dF9pZHMlMEElMEElMjMlMjAyJTNBJTIwJyUzQyUyRnMlM0UnJTBBJTIzJTIwMjU2MDQ3JTIwJTNBJTIwJ2VuZ19MYXRuJw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer(<span class="hljs-string">&quot;How was your day?&quot;</span>).input_ids
[<span class="hljs-number">13374</span>, <span class="hljs-number">1398</span>, <span class="hljs-number">4260</span>, <span class="hljs-number">4039</span>, <span class="hljs-number">248130</span>, <span class="hljs-number">2</span>, <span class="hljs-number">256047</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 2: &#x27;&lt;/s&gt;&#x27;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># 256047 : &#x27;eng_Latn&#x27;</span>`,wrap:!1}}),X=new Me({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME5sbGJUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBObGxiVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyKSUwQXRva2VuaXplciglMjJIb3clMjB3YXMlMjB5b3VyJTIwZGF5JTNGJTIyKS5pbnB1dF9pZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer(<span class="hljs-string">&quot;How was your day?&quot;</span>).input_ids
[<span class="hljs-number">256047</span>, <span class="hljs-number">13374</span>, <span class="hljs-number">1398</span>, <span class="hljs-number">4260</span>, <span class="hljs-number">4039</span>, <span class="hljs-number">248130</span>, <span class="hljs-number">2</span>]`,wrap:!1}}),S=new Me({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyME5sbGJUb2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBObGxiVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyJTJDJTIwbGVnYWN5X2JlaGF2aW91ciUzRFRydWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> NllbTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = NllbTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, legacy_behaviour=<span class="hljs-literal">True</span>)`,wrap:!1}}),P=new W({props:{title:"Overview",local:"overview",headingTag:"h2"}}),te=new W({props:{title:"Generating with NLLB",local:"generating-with-nllb",headingTag:"h2"}}),ae=new Me({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUyQyUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyKSUwQSUwQWFydGljbGUlMjAlM0QlMjAlMjJVTiUyMENoaWVmJTIwc2F5cyUyMHRoZXJlJTIwaXMlMjBubyUyMG1pbGl0YXJ5JTIwc29sdXRpb24lMjBpbiUyMFN5cmlhJTIyJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKGFydGljbGUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXRyYW5zbGF0ZWRfdG9rZW5zJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoJTBBJTIwJTIwJTIwJTIwKippbnB1dHMlMkMlMjBmb3JjZWRfYm9zX3Rva2VuX2lkJTNEdG9rZW5pemVyLmxhbmdfY29kZV90b19pZCU1QiUyMmZyYV9MYXRuJTIyJTVEJTJDJTIwbWF4X2xlbmd0aCUzRDMwJTBBKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUodHJhbnNsYXRlZF90b2tlbnMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;UN Chief says there is no military solution in Syria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>translated_tokens = model.generate(
<span class="hljs-meta">... </span>    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[<span class="hljs-string">&quot;fra_Latn&quot;</span>], max_length=<span class="hljs-number">30</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(translated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
Le chef de l<span class="hljs-string">&#x27;ONU dit qu&#x27;</span>il n<span class="hljs-string">&#x27;y a pas de solution militaire en Syrie</span>`,wrap:!1}}),oe=new W({props:{title:"Generating from any other language than English",local:"generating-from-any-other-language-than-english",headingTag:"h3"}}),ie=new Me({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUyQyUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJmYWNlYm9vayUyRm5sbGItMjAwLWRpc3RpbGxlZC02MDBNJTIyJTJDJTIwdG9rZW4lM0RUcnVlJTJDJTIwc3JjX2xhbmclM0QlMjJyb25fTGF0biUyMiUwQSklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZubGxiLTIwMC1kaXN0aWxsZWQtNjAwTSUyMiUyQyUyMHRva2VuJTNEVHJ1ZSklMEElMEFhcnRpY2xlJTIwJTNEJTIwJTIyJUM1JTlFZWZ1bCUyME9OVSUyMHNwdW5lJTIwYyVDNCU4MyUyMG51JTIwZXhpc3QlQzQlODMlMjBvJTIwc29sdSVDNSVBM2llJTIwbWlsaXRhciVDNCU4MyUyMCVDMyVBRW4lMjBTaXJpYSUyMiUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihhcnRpY2xlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF0cmFuc2xhdGVkX3Rva2VucyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCUwQSUyMCUyMCUyMCUyMCoqaW5wdXRzJTJDJTIwZm9yY2VkX2Jvc190b2tlbl9pZCUzRHRva2VuaXplci5sYW5nX2NvZGVfdG9faWQlNUIlMjJkZXVfTGF0biUyMiU1RCUyQyUyMG1heF9sZW5ndGglM0QzMCUwQSklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKHRyYW5zbGF0ZWRfdG9rZW5zJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, token=<span class="hljs-literal">True</span>, src_lang=<span class="hljs-string">&quot;ron_Latn&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;facebook/nllb-200-distilled-600M&quot;</span>, token=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>article = <span class="hljs-string">&quot;Şeful ONU spune că nu există o soluţie militară în Siria&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(article, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>translated_tokens = model.generate(
<span class="hljs-meta">... </span>    **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[<span class="hljs-string">&quot;deu_Latn&quot;</span>], max_length=<span class="hljs-number">30</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(translated_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
UN-Chef sagt, es gibt keine militärische Lösung <span class="hljs-keyword">in</span> Syrien`,wrap:!1}}),ce=new W({props:{title:"Resources",local:"resources",headingTag:"h2"}}),pe=new W({props:{title:"NllbTokenizer",local:"nllbtokenizer",headingTag:"h2"}}),me=new ye({props:{name:"class transformers.NllbTokenizer",anchor:"transformers.NllbTokenizer",parameters:[{name:"vocab_file",val:""},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"tokenizer_file",val:" = None"},{name:"src_lang",val:" = None"},{name:"tgt_lang",val:" = None"},{name:"sp_model_kwargs",val:": Optional = None"},{name:"additional_special_tokens",val:" = None"},{name:"legacy_behaviour",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.NllbTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.NllbTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.NllbTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.NllbTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.NllbTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.NllbTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.NllbTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.NllbTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.NllbTokenizer.tokenizer_file",description:`<strong>tokenizer_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The path to a tokenizer file to use instead of the vocab file.`,name:"tokenizer_file"},{anchor:"transformers.NllbTokenizer.src_lang",description:`<strong>src_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The language to use as source language for translation.`,name:"src_lang"},{anchor:"transformers.NllbTokenizer.tgt_lang",description:`<strong>tgt_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The language to use as target language for translation.`,name:"tgt_lang"},{anchor:"transformers.NllbTokenizer.sp_model_kwargs",description:`<strong>sp_model_kwargs</strong> (<code>Dict[str, str]</code>) &#x2014;
Additional keyword arguments to pass to the model initialization.`,name:"sp_model_kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nllb/tokenization_nllb.py#L47"}}),L=new qn({props:{anchor:"transformers.NllbTokenizer.example",$$slots:{default:[Rn]},$$scope:{ctx:Te}}}),ge=new ye({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.NllbTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.NllbTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.NllbTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nllb/tokenization_nllb.py#L300",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),ue=new W({props:{title:"NllbTokenizerFast",local:"nllbtokenizerfast",headingTag:"h2"}}),he=new ye({props:{name:"class transformers.NllbTokenizerFast",anchor:"transformers.NllbTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"src_lang",val:" = None"},{name:"tgt_lang",val:" = None"},{name:"additional_special_tokens",val:" = None"},{name:"legacy_behaviour",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.NllbTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.NllbTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.NllbTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.NllbTokenizerFast.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.NllbTokenizerFast.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.NllbTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.NllbTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.NllbTokenizerFast.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"},{anchor:"transformers.NllbTokenizerFast.tokenizer_file",description:`<strong>tokenizer_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The path to a tokenizer file to use instead of the vocab file.`,name:"tokenizer_file"},{anchor:"transformers.NllbTokenizerFast.src_lang",description:`<strong>src_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The language to use as source language for translation.`,name:"src_lang"},{anchor:"transformers.NllbTokenizerFast.tgt_lang",description:`<strong>tgt_lang</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The language to use as target language for translation.`,name:"tgt_lang"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nllb/tokenization_nllb_fast.py#L59"}}),Z=new qn({props:{anchor:"transformers.NllbTokenizerFast.example",$$slots:{default:[Fn]},$$scope:{ctx:Te}}}),fe=new ye({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.NllbTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.NllbTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.NllbTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nllb/tokenization_nllb_fast.py#L213",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>list of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),be=new ye({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.NllbTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.NllbTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.NllbTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nllb/tokenization_nllb_fast.py#L242",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),ke=new ye({props:{name:"set_src_lang_special_tokens",anchor:"transformers.NllbTokenizerFast.set_src_lang_special_tokens",parameters:[{name:"src_lang",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nllb/tokenization_nllb_fast.py#L297"}}),_e=new ye({props:{name:"set_tgt_lang_special_tokens",anchor:"transformers.NllbTokenizerFast.set_tgt_lang_special_tokens",parameters:[{name:"lang",val:": str"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/nllb/tokenization_nllb_fast.py#L320"}}),{c(){p=r("meta"),z=s(),w=r("p"),_=s(),m(M.$$.fragment),d=s(),m(v.$$.fragment),Xe=s(),R=r("p"),R.innerHTML=Pt,He=s(),F=r("p"),F.innerHTML=Dt,Se=s(),V=r("p"),V.textContent=At,Ye=s(),m(B.$$.fragment),Pe=s(),E=r("p"),E.textContent=Qt,De=s(),m(X.$$.fragment),Ae=s(),H=r("p"),H.textContent=Ot,Qe=s(),m(S.$$.fragment),Oe=s(),Y=r("p"),Y.innerHTML=Kt,Ke=s(),m(P.$$.fragment),et=s(),D=r("p"),D.innerHTML=en,tt=s(),A=r("p"),A.textContent=tn,nt=s(),Q=r("p"),Q.innerHTML=nn,st=s(),O=r("p"),O.textContent=sn,at=s(),K=r("p"),K.innerHTML=an,ot=s(),ee=r("p"),ee.innerHTML=on,lt=s(),m(te.$$.fragment),rt=s(),ne=r("p"),ne.innerHTML=ln,it=s(),se=r("p"),se.innerHTML=rn,ct=s(),m(ae.$$.fragment),dt=s(),m(oe.$$.fragment),pt=s(),le=r("p"),le.innerHTML=cn,mt=s(),re=r("p"),re.textContent=dn,gt=s(),m(ie.$$.fragment),ut=s(),m(ce.$$.fragment),ht=s(),de=r("ul"),de.innerHTML=pn,ft=s(),m(pe.$$.fragment),bt=s(),T=r("div"),m(me.$$.fragment),wt=s(),we=r("p"),we.textContent=mn,vt=s(),ve=r("p"),ve.innerHTML=gn,xt=s(),xe=r("p"),xe.innerHTML=un,jt=Un(`
<tokens> <eos>\` for target language documents.
`),m(L.$$.fragment),Jt=s(),J=r("div"),m(ge.$$.fragment),$t=s(),je=r("p"),je.innerHTML=hn,zt=s(),Je=r("ul"),Je.innerHTML=fn,Ut=s(),$e=r("p"),$e.textContent=bn,kt=s(),m(ue.$$.fragment),_t=s(),k=r("div"),m(he.$$.fragment),Nt=s(),ze=r("p"),ze.innerHTML=kn,qt=s(),Ue=r("p"),Ue.innerHTML=_n,Ct=s(),Ne=r("p"),Ne.innerHTML=yn,Gt=Un(`
<tokens> <eos>\` for target language documents.
`),m(Z.$$.fragment),Lt=s(),j=r("div"),m(fe.$$.fragment),Zt=s(),qe=r("p"),qe.textContent=Mn,It=s(),Ce=r("p"),Ce.innerHTML=Tn,Wt=s(),Ge=r("ul"),Ge.innerHTML=wn,Rt=s(),Le=r("p"),Le.textContent=vn,Ft=s(),I=r("div"),m(be.$$.fragment),Vt=s(),Ze=r("p"),Ze.textContent=xn,Bt=s(),U=r("div"),m(ke.$$.fragment),Et=s(),Ie=r("p"),Ie.textContent=jn,Xt=s(),We=r("ul"),We.innerHTML=Jn,Ht=s(),N=r("div"),m(_e.$$.fragment),St=s(),Re=r("p"),Re.textContent=$n,Yt=s(),Fe=r("ul"),Fe.innerHTML=zn,yt=s(),Ee=r("p"),this.h()},l(e){const t=Wn("svelte-u9bgzb",document.head);p=i(t,"META",{name:!0,content:!0}),t.forEach(n),z=a(e),w=i(e,"P",{}),C(w).forEach(n),_=a(e),g(M.$$.fragment,e),d=a(e),g(v.$$.fragment,e),Xe=a(e),R=i(e,"P",{"data-svelte-h":!0}),c(R)!=="svelte-1bqcebz"&&(R.innerHTML=Pt),He=a(e),F=i(e,"P",{"data-svelte-h":!0}),c(F)!=="svelte-vrv0c7"&&(F.innerHTML=Dt),Se=a(e),V=i(e,"P",{"data-svelte-h":!0}),c(V)!=="svelte-vmuleo"&&(V.textContent=At),Ye=a(e),g(B.$$.fragment,e),Pe=a(e),E=i(e,"P",{"data-svelte-h":!0}),c(E)!=="svelte-1qx2sah"&&(E.textContent=Qt),De=a(e),g(X.$$.fragment,e),Ae=a(e),H=i(e,"P",{"data-svelte-h":!0}),c(H)!=="svelte-obd9iw"&&(H.textContent=Ot),Qe=a(e),g(S.$$.fragment,e),Oe=a(e),Y=i(e,"P",{"data-svelte-h":!0}),c(Y)!=="svelte-ib1bh"&&(Y.innerHTML=Kt),Ke=a(e),g(P.$$.fragment,e),et=a(e),D=i(e,"P",{"data-svelte-h":!0}),c(D)!=="svelte-12x4cpm"&&(D.innerHTML=en),tt=a(e),A=i(e,"P",{"data-svelte-h":!0}),c(A)!=="svelte-wu27l3"&&(A.textContent=tn),nt=a(e),Q=i(e,"P",{"data-svelte-h":!0}),c(Q)!=="svelte-1j7uypk"&&(Q.innerHTML=nn),st=a(e),O=i(e,"P",{"data-svelte-h":!0}),c(O)!=="svelte-1ayd4bm"&&(O.textContent=sn),at=a(e),K=i(e,"P",{"data-svelte-h":!0}),c(K)!=="svelte-qp2ubt"&&(K.innerHTML=an),ot=a(e),ee=i(e,"P",{"data-svelte-h":!0}),c(ee)!=="svelte-ywl52z"&&(ee.innerHTML=on),lt=a(e),g(te.$$.fragment,e),rt=a(e),ne=i(e,"P",{"data-svelte-h":!0}),c(ne)!=="svelte-gz9jek"&&(ne.innerHTML=ln),it=a(e),se=i(e,"P",{"data-svelte-h":!0}),c(se)!=="svelte-c2ujwv"&&(se.innerHTML=rn),ct=a(e),g(ae.$$.fragment,e),dt=a(e),g(oe.$$.fragment,e),pt=a(e),le=i(e,"P",{"data-svelte-h":!0}),c(le)!=="svelte-ets0jr"&&(le.innerHTML=cn),mt=a(e),re=i(e,"P",{"data-svelte-h":!0}),c(re)!=="svelte-14hvt1f"&&(re.textContent=dn),gt=a(e),g(ie.$$.fragment,e),ut=a(e),g(ce.$$.fragment,e),ht=a(e),de=i(e,"UL",{"data-svelte-h":!0}),c(de)!=="svelte-6ej6p2"&&(de.innerHTML=pn),ft=a(e),g(pe.$$.fragment,e),bt=a(e),T=i(e,"DIV",{class:!0});var x=C(T);g(me.$$.fragment,x),wt=a(x),we=i(x,"P",{"data-svelte-h":!0}),c(we)!=="svelte-7mmiyn"&&(we.textContent=mn),vt=a(x),ve=i(x,"P",{"data-svelte-h":!0}),c(ve)!=="svelte-1serjol"&&(ve.innerHTML=gn),xt=a(x),xe=i(x,"P",{"data-svelte-h":!0}),c(xe)!=="svelte-1i8rh37"&&(xe.innerHTML=un),jt=Nn(x,`
<tokens> <eos>\` for target language documents.
`),g(L.$$.fragment,x),Jt=a(x),J=i(x,"DIV",{class:!0});var $=C(J);g(ge.$$.fragment,$),$t=a($),je=i($,"P",{"data-svelte-h":!0}),c(je)!=="svelte-nkpot4"&&(je.innerHTML=hn),zt=a($),Je=i($,"UL",{"data-svelte-h":!0}),c(Je)!=="svelte-mlrsks"&&(Je.innerHTML=fn),Ut=a($),$e=i($,"P",{"data-svelte-h":!0}),c($e)!=="svelte-46aam0"&&($e.textContent=bn),$.forEach(n),x.forEach(n),kt=a(e),g(ue.$$.fragment,e),_t=a(e),k=i(e,"DIV",{class:!0});var y=C(k);g(he.$$.fragment,y),Nt=a(y),ze=i(y,"P",{"data-svelte-h":!0}),c(ze)!=="svelte-g5z8ln"&&(ze.innerHTML=kn),qt=a(y),Ue=i(y,"P",{"data-svelte-h":!0}),c(Ue)!=="svelte-fh0aq"&&(Ue.innerHTML=_n),Ct=a(y),Ne=i(y,"P",{"data-svelte-h":!0}),c(Ne)!=="svelte-1i8rh37"&&(Ne.innerHTML=yn),Gt=Nn(y,`
<tokens> <eos>\` for target language documents.
`),g(Z.$$.fragment,y),Lt=a(y),j=i(y,"DIV",{class:!0});var q=C(j);g(fe.$$.fragment,q),Zt=a(q),qe=i(q,"P",{"data-svelte-h":!0}),c(qe)!=="svelte-1vll0v2"&&(qe.textContent=Mn),It=a(q),Ce=i(q,"P",{"data-svelte-h":!0}),c(Ce)!=="svelte-90np8u"&&(Ce.innerHTML=Tn),Wt=a(q),Ge=i(q,"UL",{"data-svelte-h":!0}),c(Ge)!=="svelte-mlrsks"&&(Ge.innerHTML=wn),Rt=a(q),Le=i(q,"P",{"data-svelte-h":!0}),c(Le)!=="svelte-46aam0"&&(Le.textContent=vn),q.forEach(n),Ft=a(y),I=i(y,"DIV",{class:!0});var Tt=C(I);g(be.$$.fragment,Tt),Vt=a(Tt),Ze=i(Tt,"P",{"data-svelte-h":!0}),c(Ze)!=="svelte-1bcwf97"&&(Ze.textContent=xn),Tt.forEach(n),Bt=a(y),U=i(y,"DIV",{class:!0});var Ve=C(U);g(ke.$$.fragment,Ve),Et=a(Ve),Ie=i(Ve,"P",{"data-svelte-h":!0}),c(Ie)!=="svelte-1euodjq"&&(Ie.textContent=jn),Xt=a(Ve),We=i(Ve,"UL",{"data-svelte-h":!0}),c(We)!=="svelte-rrgzfj"&&(We.innerHTML=Jn),Ve.forEach(n),Ht=a(y),N=i(y,"DIV",{class:!0});var Be=C(N);g(_e.$$.fragment,Be),St=a(Be),Re=i(Be,"P",{"data-svelte-h":!0}),c(Re)!=="svelte-1i6tlcm"&&(Re.textContent=$n),Yt=a(Be),Fe=i(Be,"UL",{"data-svelte-h":!0}),c(Fe)!=="svelte-1durotj"&&(Fe.innerHTML=zn),Be.forEach(n),y.forEach(n),yt=a(e),Ee=i(e,"P",{}),C(Ee).forEach(n),this.h()},h(){G(p,"name","hf:doc:metadata"),G(p,"content",Bn),G(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){l(document.head,p),o(e,z,t),o(e,w,t),o(e,_,t),u(M,e,t),o(e,d,t),u(v,e,t),o(e,Xe,t),o(e,R,t),o(e,He,t),o(e,F,t),o(e,Se,t),o(e,V,t),o(e,Ye,t),u(B,e,t),o(e,Pe,t),o(e,E,t),o(e,De,t),u(X,e,t),o(e,Ae,t),o(e,H,t),o(e,Qe,t),u(S,e,t),o(e,Oe,t),o(e,Y,t),o(e,Ke,t),u(P,e,t),o(e,et,t),o(e,D,t),o(e,tt,t),o(e,A,t),o(e,nt,t),o(e,Q,t),o(e,st,t),o(e,O,t),o(e,at,t),o(e,K,t),o(e,ot,t),o(e,ee,t),o(e,lt,t),u(te,e,t),o(e,rt,t),o(e,ne,t),o(e,it,t),o(e,se,t),o(e,ct,t),u(ae,e,t),o(e,dt,t),u(oe,e,t),o(e,pt,t),o(e,le,t),o(e,mt,t),o(e,re,t),o(e,gt,t),u(ie,e,t),o(e,ut,t),u(ce,e,t),o(e,ht,t),o(e,de,t),o(e,ft,t),u(pe,e,t),o(e,bt,t),o(e,T,t),u(me,T,null),l(T,wt),l(T,we),l(T,vt),l(T,ve),l(T,xt),l(T,xe),l(T,jt),u(L,T,null),l(T,Jt),l(T,J),u(ge,J,null),l(J,$t),l(J,je),l(J,zt),l(J,Je),l(J,Ut),l(J,$e),o(e,kt,t),u(ue,e,t),o(e,_t,t),o(e,k,t),u(he,k,null),l(k,Nt),l(k,ze),l(k,qt),l(k,Ue),l(k,Ct),l(k,Ne),l(k,Gt),u(Z,k,null),l(k,Lt),l(k,j),u(fe,j,null),l(j,Zt),l(j,qe),l(j,It),l(j,Ce),l(j,Wt),l(j,Ge),l(j,Rt),l(j,Le),l(k,Ft),l(k,I),u(be,I,null),l(I,Vt),l(I,Ze),l(k,Bt),l(k,U),u(ke,U,null),l(U,Et),l(U,Ie),l(U,Xt),l(U,We),l(k,Ht),l(k,N),u(_e,N,null),l(N,St),l(N,Re),l(N,Yt),l(N,Fe),o(e,yt,t),o(e,Ee,t),Mt=!0},p(e,[t]){const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),L.$set(x);const $={};t&2&&($.$$scope={dirty:t,ctx:e}),Z.$set($)},i(e){Mt||(h(M.$$.fragment,e),h(v.$$.fragment,e),h(B.$$.fragment,e),h(X.$$.fragment,e),h(S.$$.fragment,e),h(P.$$.fragment,e),h(te.$$.fragment,e),h(ae.$$.fragment,e),h(oe.$$.fragment,e),h(ie.$$.fragment,e),h(ce.$$.fragment,e),h(pe.$$.fragment,e),h(me.$$.fragment,e),h(L.$$.fragment,e),h(ge.$$.fragment,e),h(ue.$$.fragment,e),h(he.$$.fragment,e),h(Z.$$.fragment,e),h(fe.$$.fragment,e),h(be.$$.fragment,e),h(ke.$$.fragment,e),h(_e.$$.fragment,e),Mt=!0)},o(e){f(M.$$.fragment,e),f(v.$$.fragment,e),f(B.$$.fragment,e),f(X.$$.fragment,e),f(S.$$.fragment,e),f(P.$$.fragment,e),f(te.$$.fragment,e),f(ae.$$.fragment,e),f(oe.$$.fragment,e),f(ie.$$.fragment,e),f(ce.$$.fragment,e),f(pe.$$.fragment,e),f(me.$$.fragment,e),f(L.$$.fragment,e),f(ge.$$.fragment,e),f(ue.$$.fragment,e),f(he.$$.fragment,e),f(Z.$$.fragment,e),f(fe.$$.fragment,e),f(be.$$.fragment,e),f(ke.$$.fragment,e),f(_e.$$.fragment,e),Mt=!1},d(e){e&&(n(z),n(w),n(_),n(d),n(Xe),n(R),n(He),n(F),n(Se),n(V),n(Ye),n(Pe),n(E),n(De),n(Ae),n(H),n(Qe),n(Oe),n(Y),n(Ke),n(et),n(D),n(tt),n(A),n(nt),n(Q),n(st),n(O),n(at),n(K),n(ot),n(ee),n(lt),n(rt),n(ne),n(it),n(se),n(ct),n(dt),n(pt),n(le),n(mt),n(re),n(gt),n(ut),n(ht),n(de),n(ft),n(bt),n(T),n(kt),n(_t),n(k),n(yt),n(Ee)),n(p),b(M,e),b(v,e),b(B,e),b(X,e),b(S,e),b(P,e),b(te,e),b(ae,e),b(oe,e),b(ie,e),b(ce,e),b(pe,e),b(me),b(L),b(ge),b(ue,e),b(he),b(Z),b(fe),b(be),b(ke),b(_e)}}}const Bn='{"title":"NLLB","local":"nllb","sections":[{"title":"Updated tokenizer behavior","local":"transformers.NllbTokenizer","sections":[],"depth":2},{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Generating with NLLB","local":"generating-with-nllb","sections":[{"title":"Generating from any other language than English","local":"generating-from-any-other-language-than-english","sections":[],"depth":3}],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"NllbTokenizer","local":"nllbtokenizer","sections":[],"depth":2},{"title":"NllbTokenizerFast","local":"nllbtokenizerfast","sections":[],"depth":2}],"depth":1}';function En(Te){return Ln(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class An extends Zn{constructor(p){super(),In(this,p,En,Vn,Gn,{})}}export{An as component};
