import{s as bt,f as wt,o as vt,n as Me}from"../chunks/scheduler.9bc65507.js";import{S as yt,i as $t,g as m,s as r,r as h,A as Nt,h as c,f as n,c as i,j as ie,u,x as b,k as I,y as p,a,v as g,d as _,t as M,w as T}from"../chunks/index.707bf1b6.js";import{T as Tt}from"../chunks/Tip.c2ecdbf4.js";import{D as _e}from"../chunks/Docstring.17db21ae.js";import{C as tt}from"../chunks/CodeBlock.54a9f38d.js";import{E as et}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as Vt}from"../chunks/PipelineTag.44585822.js";import{H as le}from"../chunks/Heading.342b1fa6.js";function Ct(N){let o,w="Example:",l,d,f;return d=new tt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpVE1TTk1vZGVsJTJDJTIwVmlUTVNOQ29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFZpVCUyME1TTiUyMHZpdC1tc24tYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBWaVRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMGZyb20lMjB0aGUlMjB2aXQtbXNuLWJhc2UlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFZpVE1TTk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ViTMSNModel, ViTMSNConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ViT MSN vit-msn-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ViTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the vit-msn-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ViTMSNModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=m("p"),o.textContent=w,l=r(),h(d.$$.fragment)},l(s){o=c(s,"P",{"data-svelte-h":!0}),b(o)!=="svelte-11lpom8"&&(o.textContent=w),l=i(s),u(d.$$.fragment,s)},m(s,v){a(s,o,v),a(s,l,v),g(d,s,v),f=!0},p:Me,i(s){f||(_(d.$$.fragment,s),f=!0)},o(s){M(d.$$.fragment,s),f=!1},d(s){s&&(n(o),n(l)),T(d,s)}}}function St(N){let o,w=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=w},l(l){o=c(l,"P",{"data-svelte-h":!0}),b(o)!=="svelte-fincs2"&&(o.innerHTML=w)},m(l,d){a(l,o,d)},p:Me,d(l){l&&n(o)}}}function xt(N){let o,w="Examples:",l,d,f;return d=new tt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFZpVE1TTk1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGdml0LW1zbi1zbWFsbCUyMiklMEFtb2RlbCUyMCUzRCUyMFZpVE1TTk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tc24tc21hbGwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGU=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ViTMSNModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/vit-msn-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ViTMSNModel.from_pretrained(<span class="hljs-string">&quot;facebook/vit-msn-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){o=m("p"),o.textContent=w,l=r(),h(d.$$.fragment)},l(s){o=c(s,"P",{"data-svelte-h":!0}),b(o)!=="svelte-kvfsh7"&&(o.textContent=w),l=i(s),u(d.$$.fragment,s)},m(s,v){a(s,o,v),a(s,l,v),g(d,s,v),f=!0},p:Me,i(s){f||(_(d.$$.fragment,s),f=!0)},o(s){M(d.$$.fragment,s),f=!1},d(s){s&&(n(o),n(l)),T(d,s)}}}function kt(N){let o,w=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=w},l(l){o=c(l,"P",{"data-svelte-h":!0}),b(o)!=="svelte-fincs2"&&(o.innerHTML=w)},m(l,d){a(l,o,d)},p:Me,d(l){l&&n(o)}}}function jt(N){let o,w="Examples:",l,d,f;return d=new tt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFZpVE1TTkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdG9yY2gubWFudWFsX3NlZWQoMiklMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tc24tc21hbGwlMjIpJTBBbW9kZWwlMjAlM0QlMjBWaVRNU05Gb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRnZpdC1tc24tc21hbGwlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cyUwQSUyMyUyMG1vZGVsJTIwcHJlZGljdHMlMjBvbmUlMjBvZiUyMHRoZSUyMDEwMDAlMjBJbWFnZU5ldCUyMGNsYXNzZXMlMEFwcmVkaWN0ZWRfbGFiZWwlMjAlM0QlMjBsb2dpdHMuYXJnbWF4KC0xKS5pdGVtKCklMEFwcmludChtb2RlbC5jb25maWcuaWQybGFiZWwlNUJwcmVkaWN0ZWRfbGFiZWwlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ViTMSNForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">2</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/vit-msn-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ViTMSNForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/vit-msn-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
Kerry blue terrier`,wrap:!1}}),{c(){o=m("p"),o.textContent=w,l=r(),h(d.$$.fragment)},l(s){o=c(s,"P",{"data-svelte-h":!0}),b(o)!=="svelte-kvfsh7"&&(o.textContent=w),l=i(s),u(d.$$.fragment,s)},m(s,v){a(s,o,v),a(s,l,v),g(d,s,v),f=!0},p:Me,i(s){f||(_(d.$$.fragment,s),f=!0)},o(s){M(d.$$.fragment,s),f=!1},d(s){s&&(n(o),n(l)),T(d,s)}}}function Jt(N){let o,w,l,d,f,s,v,Te,P,nt=`The ViTMSN model was proposed in <a href="https://arxiv.org/abs/2204.07141" rel="nofollow">Masked Siamese Networks for Label-Efficient Learning</a> by Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Florian Bordes,
Pascal Vincent, Armand Joulin, Michael Rabbat, Nicolas Ballas. The paper presents a joint-embedding architecture to match the prototypes
of masked patches with that of the unmasked patches. With this setup, their method yields excellent performance in the low-shot and extreme low-shot
regimes.`,be,G,ot="The abstract from the paper is the following:",we,L,st=`<em>We propose Masked Siamese Networks (MSN), a self-supervised learning framework for learning image representations. Our
approach matches the representation of an image view containing randomly masked patches to the representation of the original
unmasked image. This self-supervised pre-training strategy is particularly scalable when applied to Vision Transformers since only the
unmasked patches are processed by the network. As a result, MSNs improve the scalability of joint-embedding architectures,
while producing representations of a high semantic level that perform competitively on low-shot image classification. For instance,
on ImageNet-1K, with only 5,000 annotated images, our base MSN model achieves 72.4% top-1 accuracy,
and with 1% of ImageNet-1K labels, we achieve 75.7% top-1 accuracy, setting a new state-of-the-art for self-supervised learning on this benchmark.</em>`,ve,U,at,ye,R,rt='MSN architecture. Taken from the <a href="https://arxiv.org/abs/2204.07141">original paper.</a>',$e,q,it='This model was contributed by <a href="https://huggingface.co/sayakpaul" rel="nofollow">sayakpaul</a>. The original code can be found <a href="https://github.com/facebookresearch/msn" rel="nofollow">here</a>.',Ne,E,Ve,Y,lt=`<li>MSN (masked siamese networks) is a method for self-supervised pre-training of Vision Transformers (ViTs). The pre-training
objective is to match the prototypes assigned to the unmasked views of the images to that of the masked views of the same images.</li> <li>The authors have only released pre-trained weights of the backbone (ImageNet-1k pre-training). So, to use that on your own image classification dataset,
use the <a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification">ViTMSNForImageClassification</a> class which is initialized from <a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNModel">ViTMSNModel</a>. Follow
<a href="https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">this notebook</a> for a detailed tutorial on fine-tuning.</li> <li>MSN is particularly useful in the low-shot and extreme low-shot regimes. Notably, it achieves 75.7% top-1 accuracy with only 1% of ImageNet-1K
labels when fine-tuned.</li>`,Ce,X,Se,Q,dt="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with ViT MSN.",xe,A,ke,O,mt='<li><a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification">ViTMSNForImageClassification</a> is supported by this <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">example script</a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">notebook</a>.</li> <li>See also: <a href="../tasks/image_classification">Image classification task guide</a></li>',je,D,ct="If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",Je,K,Ie,y,ee,He,de,pt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNModel">ViTMSNModel</a>. It is used to instantiate an ViT
MSN model according to the specified arguments, defining the model architecture. Instantiating a configuration with
the defaults will yield a similar configuration to that of the ViT
<a href="https://huggingface.co/facebook/vit_msn_base" rel="nofollow">facebook/vit_msn_base</a> architecture.`,Pe,me,ft=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ge,W,Fe,te,Ue,S,ne,Le,ce,ht=`The bare ViTMSN Model outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Re,V,oe,qe,pe,ut='The <a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNModel">ViTMSNModel</a> forward method, overrides the <code>__call__</code> special method.',Ee,Z,Ye,z,We,se,Ze,$,ae,Xe,fe,gt="ViTMSN Model with an image classification head on top e.g. for ImageNet.",Qe,he,_t=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Ae,C,re,Oe,ue,Mt='The <a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNForImageClassification">ViTMSNForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',De,B,Ke,H,ze,ge,Be;return f=new le({props:{title:"ViTMSN",local:"vitmsn",headingTag:"h1"}}),v=new le({props:{title:"Overview",local:"overview",headingTag:"h2"}}),E=new le({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),X=new le({props:{title:"Resources",local:"resources",headingTag:"h2"}}),A=new Vt({props:{pipeline:"image-classification"}}),K=new le({props:{title:"ViTMSNConfig",local:"transformers.ViTMSNConfig",headingTag:"h2"}}),ee=new _e({props:{name:"class transformers.ViTMSNConfig",anchor:"transformers.ViTMSNConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ViTMSNConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.ViTMSNConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.ViTMSNConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.ViTMSNConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.ViTMSNConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.ViTMSNConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.ViTMSNConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.ViTMSNConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ViTMSNConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.ViTMSNConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.ViTMSNConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.ViTMSNConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.ViTMSNConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/vit_msn/configuration_vit_msn.py#L30"}}),W=new et({props:{anchor:"transformers.ViTMSNConfig.example",$$slots:{default:[Ct]},$$scope:{ctx:N}}}),te=new le({props:{title:"ViTMSNModel",local:"transformers.ViTMSNModel",headingTag:"h2"}}),ne=new _e({props:{name:"class transformers.ViTMSNModel",anchor:"transformers.ViTMSNModel",parameters:[{name:"config",val:": ViTMSNConfig"},{name:"use_mask_token",val:": bool = False"}],parametersDescription:[{anchor:"transformers.ViTMSNModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig">ViTMSNConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/vit_msn/modeling_vit_msn.py#L478"}}),oe=new _e({props:{name:"forward",anchor:"transformers.ViTMSNModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"interpolate_pos_encoding",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ViTMSNModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.ViTMSNModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ViTMSNModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ViTMSNModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ViTMSNModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.ViTMSNModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ViTMSNModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>, <em>optional</em>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/vit_msn/modeling_vit_msn.py#L506",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig"
>ViTMSNConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Z=new Tt({props:{$$slots:{default:[St]},$$scope:{ctx:N}}}),z=new et({props:{anchor:"transformers.ViTMSNModel.forward.example",$$slots:{default:[xt]},$$scope:{ctx:N}}}),se=new le({props:{title:"ViTMSNForImageClassification",local:"transformers.ViTMSNForImageClassification",headingTag:"h2"}}),ae=new _e({props:{name:"class transformers.ViTMSNForImageClassification",anchor:"transformers.ViTMSNForImageClassification",parameters:[{name:"config",val:": ViTMSNConfig"}],parametersDescription:[{anchor:"transformers.ViTMSNForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig">ViTMSNConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/vit_msn/modeling_vit_msn.py#L585"}}),re=new _e({props:{name:"forward",anchor:"transformers.ViTMSNForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"interpolate_pos_encoding",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ViTMSNForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.ViTMSNForImageClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ViTMSNForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ViTMSNForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ViTMSNForImageClassification.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.ViTMSNForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/vit_msn/modeling_vit_msn.py#L604",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/vit_msn#transformers.ViTMSNConfig"
>ViTMSNConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),B=new Tt({props:{$$slots:{default:[kt]},$$scope:{ctx:N}}}),H=new et({props:{anchor:"transformers.ViTMSNForImageClassification.forward.example",$$slots:{default:[jt]},$$scope:{ctx:N}}}),{c(){o=m("meta"),w=r(),l=m("p"),d=r(),h(f.$$.fragment),s=r(),h(v.$$.fragment),Te=r(),P=m("p"),P.innerHTML=nt,be=r(),G=m("p"),G.textContent=ot,we=r(),L=m("p"),L.innerHTML=st,ve=r(),U=m("img"),ye=r(),R=m("small"),R.innerHTML=rt,$e=r(),q=m("p"),q.innerHTML=it,Ne=r(),h(E.$$.fragment),Ve=r(),Y=m("ul"),Y.innerHTML=lt,Ce=r(),h(X.$$.fragment),Se=r(),Q=m("p"),Q.textContent=dt,xe=r(),h(A.$$.fragment),ke=r(),O=m("ul"),O.innerHTML=mt,je=r(),D=m("p"),D.textContent=ct,Je=r(),h(K.$$.fragment),Ie=r(),y=m("div"),h(ee.$$.fragment),He=r(),de=m("p"),de.innerHTML=pt,Pe=r(),me=m("p"),me.innerHTML=ft,Ge=r(),h(W.$$.fragment),Fe=r(),h(te.$$.fragment),Ue=r(),S=m("div"),h(ne.$$.fragment),Le=r(),ce=m("p"),ce.innerHTML=ht,Re=r(),V=m("div"),h(oe.$$.fragment),qe=r(),pe=m("p"),pe.innerHTML=ut,Ee=r(),h(Z.$$.fragment),Ye=r(),h(z.$$.fragment),We=r(),h(se.$$.fragment),Ze=r(),$=m("div"),h(ae.$$.fragment),Xe=r(),fe=m("p"),fe.textContent=gt,Qe=r(),he=m("p"),he.innerHTML=_t,Ae=r(),C=m("div"),h(re.$$.fragment),Oe=r(),ue=m("p"),ue.innerHTML=Mt,De=r(),h(B.$$.fragment),Ke=r(),h(H.$$.fragment),ze=r(),ge=m("p"),this.h()},l(e){const t=Nt("svelte-u9bgzb",document.head);o=c(t,"META",{name:!0,content:!0}),t.forEach(n),w=i(e),l=c(e,"P",{}),ie(l).forEach(n),d=i(e),u(f.$$.fragment,e),s=i(e),u(v.$$.fragment,e),Te=i(e),P=c(e,"P",{"data-svelte-h":!0}),b(P)!=="svelte-lk3nii"&&(P.innerHTML=nt),be=i(e),G=c(e,"P",{"data-svelte-h":!0}),b(G)!=="svelte-vfdo9a"&&(G.textContent=ot),we=i(e),L=c(e,"P",{"data-svelte-h":!0}),b(L)!=="svelte-abxoxf"&&(L.innerHTML=st),ve=i(e),U=c(e,"IMG",{src:!0,alt:!0,width:!0}),ye=i(e),R=c(e,"SMALL",{"data-svelte-h":!0}),b(R)!=="svelte-1iymhb9"&&(R.innerHTML=rt),$e=i(e),q=c(e,"P",{"data-svelte-h":!0}),b(q)!=="svelte-ig50j8"&&(q.innerHTML=it),Ne=i(e),u(E.$$.fragment,e),Ve=i(e),Y=c(e,"UL",{"data-svelte-h":!0}),b(Y)!=="svelte-bydp6v"&&(Y.innerHTML=lt),Ce=i(e),u(X.$$.fragment,e),Se=i(e),Q=c(e,"P",{"data-svelte-h":!0}),b(Q)!=="svelte-s3w6c0"&&(Q.textContent=dt),xe=i(e),u(A.$$.fragment,e),ke=i(e),O=c(e,"UL",{"data-svelte-h":!0}),b(O)!=="svelte-13ucqrn"&&(O.innerHTML=mt),je=i(e),D=c(e,"P",{"data-svelte-h":!0}),b(D)!=="svelte-1xesile"&&(D.textContent=ct),Je=i(e),u(K.$$.fragment,e),Ie=i(e),y=c(e,"DIV",{class:!0});var x=ie(y);u(ee.$$.fragment,x),He=i(x),de=c(x,"P",{"data-svelte-h":!0}),b(de)!=="svelte-15tozut"&&(de.innerHTML=pt),Pe=i(x),me=c(x,"P",{"data-svelte-h":!0}),b(me)!=="svelte-o55m63"&&(me.innerHTML=ft),Ge=i(x),u(W.$$.fragment,x),x.forEach(n),Fe=i(e),u(te.$$.fragment,e),Ue=i(e),S=c(e,"DIV",{class:!0});var F=ie(S);u(ne.$$.fragment,F),Le=i(F),ce=c(F,"P",{"data-svelte-h":!0}),b(ce)!=="svelte-f45q49"&&(ce.innerHTML=ht),Re=i(F),V=c(F,"DIV",{class:!0});var k=ie(V);u(oe.$$.fragment,k),qe=i(k),pe=c(k,"P",{"data-svelte-h":!0}),b(pe)!=="svelte-1csk01x"&&(pe.innerHTML=ut),Ee=i(k),u(Z.$$.fragment,k),Ye=i(k),u(z.$$.fragment,k),k.forEach(n),F.forEach(n),We=i(e),u(se.$$.fragment,e),Ze=i(e),$=c(e,"DIV",{class:!0});var j=ie($);u(ae.$$.fragment,j),Xe=i(j),fe=c(j,"P",{"data-svelte-h":!0}),b(fe)!=="svelte-73jzbh"&&(fe.textContent=gt),Qe=i(j),he=c(j,"P",{"data-svelte-h":!0}),b(he)!=="svelte-1gjh92c"&&(he.innerHTML=_t),Ae=i(j),C=c(j,"DIV",{class:!0});var J=ie(C);u(re.$$.fragment,J),Oe=i(J),ue=c(J,"P",{"data-svelte-h":!0}),b(ue)!=="svelte-1pk8ttz"&&(ue.innerHTML=Mt),De=i(J),u(B.$$.fragment,J),Ke=i(J),u(H.$$.fragment,J),J.forEach(n),j.forEach(n),ze=i(e),ge=c(e,"P",{}),ie(ge).forEach(n),this.h()},h(){I(o,"name","hf:doc:metadata"),I(o,"content",It),wt(U.src,at="https://i.ibb.co/W6PQMdC/Screenshot-2022-09-13-at-9-08-40-AM.png")||I(U,"src",at),I(U,"alt","drawing"),I(U,"width","600"),I(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){p(document.head,o),a(e,w,t),a(e,l,t),a(e,d,t),g(f,e,t),a(e,s,t),g(v,e,t),a(e,Te,t),a(e,P,t),a(e,be,t),a(e,G,t),a(e,we,t),a(e,L,t),a(e,ve,t),a(e,U,t),a(e,ye,t),a(e,R,t),a(e,$e,t),a(e,q,t),a(e,Ne,t),g(E,e,t),a(e,Ve,t),a(e,Y,t),a(e,Ce,t),g(X,e,t),a(e,Se,t),a(e,Q,t),a(e,xe,t),g(A,e,t),a(e,ke,t),a(e,O,t),a(e,je,t),a(e,D,t),a(e,Je,t),g(K,e,t),a(e,Ie,t),a(e,y,t),g(ee,y,null),p(y,He),p(y,de),p(y,Pe),p(y,me),p(y,Ge),g(W,y,null),a(e,Fe,t),g(te,e,t),a(e,Ue,t),a(e,S,t),g(ne,S,null),p(S,Le),p(S,ce),p(S,Re),p(S,V),g(oe,V,null),p(V,qe),p(V,pe),p(V,Ee),g(Z,V,null),p(V,Ye),g(z,V,null),a(e,We,t),g(se,e,t),a(e,Ze,t),a(e,$,t),g(ae,$,null),p($,Xe),p($,fe),p($,Qe),p($,he),p($,Ae),p($,C),g(re,C,null),p(C,Oe),p(C,ue),p(C,De),g(B,C,null),p(C,Ke),g(H,C,null),a(e,ze,t),a(e,ge,t),Be=!0},p(e,[t]){const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),W.$set(x);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),Z.$set(F);const k={};t&2&&(k.$$scope={dirty:t,ctx:e}),z.$set(k);const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),B.$set(j);const J={};t&2&&(J.$$scope={dirty:t,ctx:e}),H.$set(J)},i(e){Be||(_(f.$$.fragment,e),_(v.$$.fragment,e),_(E.$$.fragment,e),_(X.$$.fragment,e),_(A.$$.fragment,e),_(K.$$.fragment,e),_(ee.$$.fragment,e),_(W.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(oe.$$.fragment,e),_(Z.$$.fragment,e),_(z.$$.fragment,e),_(se.$$.fragment,e),_(ae.$$.fragment,e),_(re.$$.fragment,e),_(B.$$.fragment,e),_(H.$$.fragment,e),Be=!0)},o(e){M(f.$$.fragment,e),M(v.$$.fragment,e),M(E.$$.fragment,e),M(X.$$.fragment,e),M(A.$$.fragment,e),M(K.$$.fragment,e),M(ee.$$.fragment,e),M(W.$$.fragment,e),M(te.$$.fragment,e),M(ne.$$.fragment,e),M(oe.$$.fragment,e),M(Z.$$.fragment,e),M(z.$$.fragment,e),M(se.$$.fragment,e),M(ae.$$.fragment,e),M(re.$$.fragment,e),M(B.$$.fragment,e),M(H.$$.fragment,e),Be=!1},d(e){e&&(n(w),n(l),n(d),n(s),n(Te),n(P),n(be),n(G),n(we),n(L),n(ve),n(U),n(ye),n(R),n($e),n(q),n(Ne),n(Ve),n(Y),n(Ce),n(Se),n(Q),n(xe),n(ke),n(O),n(je),n(D),n(Je),n(Ie),n(y),n(Fe),n(Ue),n(S),n(We),n(Ze),n($),n(ze),n(ge)),n(o),T(f,e),T(v,e),T(E,e),T(X,e),T(A,e),T(K,e),T(ee),T(W),T(te,e),T(ne),T(oe),T(Z),T(z),T(se,e),T(ae),T(re),T(B),T(H)}}}const It='{"title":"ViTMSN","local":"vitmsn","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"ViTMSNConfig","local":"transformers.ViTMSNConfig","sections":[],"depth":2},{"title":"ViTMSNModel","local":"transformers.ViTMSNModel","sections":[],"depth":2},{"title":"ViTMSNForImageClassification","local":"transformers.ViTMSNForImageClassification","sections":[],"depth":2}],"depth":1}';function Ft(N){return vt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Lt extends yt{constructor(o){super(),$t(this,o,Ft,Jt,bt,{})}}export{Lt as component};
