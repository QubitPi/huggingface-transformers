import{s as jt,o as bt,n as Hs}from"../chunks/scheduler.9bc65507.js";import{S as wt,i as Jt,g as c,s as n,r as h,A as Tt,h as m,f as t,c as p,j as Mt,u as d,x as b,k as ft,y as Ut,a,v as u,d as M,t as f,w as y,m as $t,n as _t}from"../chunks/index.707bf1b6.js";import{T as He}from"../chunks/Tip.c2ecdbf4.js";import{Y as Ct}from"../chunks/Youtube.e1129c6f.js";import{C as T}from"../chunks/CodeBlock.54a9f38d.js";import{D as vt}from"../chunks/DocNotebookDropdown.41f65cb5.js";import{F as yt,M as gt}from"../chunks/Markdown.fef84341.js";import{H as Bs}from"../chunks/Heading.342b1fa6.js";function kt(v){let l,g,r='<a href="../model_doc/data2vec-audio">Data2VecAudio</a>, <a href="../model_doc/hubert">Hubert</a>, <a href="../model_doc/mctct">M-CTC-T</a>, <a href="../model_doc/sew">SEW</a>, <a href="../model_doc/sew-d">SEW-D</a>, <a href="../model_doc/unispeech">UniSpeech</a>, <a href="../model_doc/unispeech-sat">UniSpeechSat</a>, <a href="../model_doc/wav2vec2">Wav2Vec2</a>, <a href="../model_doc/wav2vec2-bert">Wav2Vec2-BERT</a>, <a href="../model_doc/wav2vec2-conformer">Wav2Vec2-Conformer</a>, <a href="../model_doc/wavlm">WavLM</a>';return{c(){l=$t(`The task illustrated in this tutorial is supported by the following model architectures:

`),g=c("p"),g.innerHTML=r},l(j){l=_t(j,`The task illustrated in this tutorial is supported by the following model architectures:

`),g=m(j,"P",{"data-svelte-h":!0}),b(g)!=="svelte-qabixr"&&(g.innerHTML=r)},m(j,w){a(j,l,w),a(j,g,w)},p:Hs,d(j){j&&(t(l),t(g))}}}function It(v){let l,g='If you arenâ€™t familiar with finetuning a model with the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a>, take a look at the basic tutorial <a href="../training#train-with-pytorch-trainer">here</a>!';return{c(){l=c("p"),l.innerHTML=g},l(r){l=m(r,"P",{"data-svelte-h":!0}),b(l)!=="svelte-15s4um0"&&(l.innerHTML=g)},m(r,j){a(r,l,j)},p:Hs,d(r){r&&t(l)}}}function Rt(v){let l,g,r,j='Youâ€™re ready to start training your model now! Load Wav2Vec2 with <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForCTC">AutoModelForCTC</a>. Specify the reduction to apply with the <code>ctc_loss_reduction</code> parameter. It is often better to use the average instead of the default summation:',w,C,R,I,k="At this point, only three steps remain:",Z,U,X='<li>Define your training hyperparameters in <a href="/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>. The only required parameter is <code>output_dir</code> which specifies where to save your model. Youâ€™ll push this model to the Hub by setting <code>push_to_hub=True</code> (you need to be signed in to Hugging Face to upload your model). At the end of each epoch, the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> will evaluate the WER and save the training checkpoint.</li> <li>Pass the training arguments to <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a> along with the model, dataset, tokenizer, data collator, and <code>compute_metrics</code> function.</li> <li>Call <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train">train()</a> to finetune your model.</li>',W,$,G,i,_='Once training is completed, share your model to the Hub with the <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub">push_to_hub()</a> method so everyone can use your model:',E,B,x;return l=new He({props:{$$slots:{default:[It]},$$scope:{ctx:v}}}),C=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNUQyUyQyUyMFRyYWluaW5nQXJndW1lbnRzJTJDJTIwVHJhaW5lciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ1RDLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJmYWNlYm9vayUyRndhdjJ2ZWMyLWJhc2UlMjIlMkMlMEElMjAlMjAlMjAlMjBjdGNfbG9zc19yZWR1Y3Rpb24lM0QlMjJtZWFuJTIyJTJDJTBBJTIwJTIwJTIwJTIwcGFkX3Rva2VuX2lkJTNEcHJvY2Vzc29yLnRva2VuaXplci5wYWRfdG9rZW5faWQlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCTC, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>,
<span class="hljs-meta">... </span>    ctc_loss_reduction=<span class="hljs-string">&quot;mean&quot;</span>,
<span class="hljs-meta">... </span>    pad_token_id=processor.tokenizer.pad_token_id,
<span class="hljs-meta">... </span>)`,wrap:!1}}),$=new T({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJteV9hd2Vzb21lX2Fzcl9taW5kX21vZGVsJTIyJTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV90cmFpbl9iYXRjaF9zaXplJTNEOCUyQyUwQSUyMCUyMCUyMCUyMGdyYWRpZW50X2FjY3VtdWxhdGlvbl9zdGVwcyUzRDIlMkMlMEElMjAlMjAlMjAlMjBsZWFybmluZ19yYXRlJTNEMWUtNSUyQyUwQSUyMCUyMCUyMCUyMHdhcm11cF9zdGVwcyUzRDUwMCUyQyUwQSUyMCUyMCUyMCUyMG1heF9zdGVwcyUzRDIwMDAlMkMlMEElMjAlMjAlMjAlMjBncmFkaWVudF9jaGVja3BvaW50aW5nJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGZwMTYlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwZ3JvdXBfYnlfbGVuZ3RoJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGV2YWx1YXRpb25fc3RyYXRlZ3klM0QlMjJzdGVwcyUyMiUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEOCUyQyUwQSUyMCUyMCUyMCUyMHNhdmVfc3RlcHMlM0QxMDAwJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9zdGVwcyUzRDEwMDAlMkMlMEElMjAlMjAlMjAlMjBsb2dnaW5nX3N0ZXBzJTNEMjUlMkMlMEElMjAlMjAlMjAlMjBsb2FkX2Jlc3RfbW9kZWxfYXRfZW5kJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMG1ldHJpY19mb3JfYmVzdF9tb2RlbCUzRCUyMndlciUyMiUyQyUwQSUyMCUyMCUyMCUyMGdyZWF0ZXJfaXNfYmV0dGVyJTNERmFsc2UlMkMlMEElMjAlMjAlMjAlMjBwdXNoX3RvX2h1YiUzRFRydWUlMkMlMEEpJTBBJTBBdHJhaW5lciUyMCUzRCUyMFRyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0Rtb2RlbCUyQyUwQSUyMCUyMCUyMCUyMGFyZ3MlM0R0cmFpbmluZ19hcmdzJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRGVuY29kZWRfbWluZHMlNUIlMjJ0cmFpbiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMGV2YWxfZGF0YXNldCUzRGVuY29kZWRfbWluZHMlNUIlMjJ0ZXN0JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwdG9rZW5pemVyJTNEcHJvY2Vzc29yJTJDJTBBJTIwJTIwJTIwJTIwZGF0YV9jb2xsYXRvciUzRGRhdGFfY29sbGF0b3IlMkMlMEElMjAlMjAlMjAlMjBjb21wdXRlX21ldHJpY3MlM0Rjb21wdXRlX21ldHJpY3MlMkMlMEEpJTBBJTBBdHJhaW5lci50cmFpbigp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_asr_mind_model&quot;</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">8</span>,
<span class="hljs-meta">... </span>    gradient_accumulation_steps=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">1e-5</span>,
<span class="hljs-meta">... </span>    warmup_steps=<span class="hljs-number">500</span>,
<span class="hljs-meta">... </span>    max_steps=<span class="hljs-number">2000</span>,
<span class="hljs-meta">... </span>    gradient_checkpointing=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    group_by_length=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;steps&quot;</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">8</span>,
<span class="hljs-meta">... </span>    save_steps=<span class="hljs-number">1000</span>,
<span class="hljs-meta">... </span>    eval_steps=<span class="hljs-number">1000</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">25</span>,
<span class="hljs-meta">... </span>    load_best_model_at_end=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    metric_for_best_model=<span class="hljs-string">&quot;wer&quot;</span>,
<span class="hljs-meta">... </span>    greater_is_better=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=encoded_minds[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=encoded_minds[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=processor,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),B=new T({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),{c(){h(l.$$.fragment),g=n(),r=c("p"),r.innerHTML=j,w=n(),h(C.$$.fragment),R=n(),I=c("p"),I.textContent=k,Z=n(),U=c("ol"),U.innerHTML=X,W=n(),h($.$$.fragment),G=n(),i=c("p"),i.innerHTML=_,E=n(),h(B.$$.fragment)},l(o){d(l.$$.fragment,o),g=p(o),r=m(o,"P",{"data-svelte-h":!0}),b(r)!=="svelte-x5yuc0"&&(r.innerHTML=j),w=p(o),d(C.$$.fragment,o),R=p(o),I=m(o,"P",{"data-svelte-h":!0}),b(I)!=="svelte-l42k0i"&&(I.textContent=k),Z=p(o),U=m(o,"OL",{"data-svelte-h":!0}),b(U)!=="svelte-1v24u9e"&&(U.innerHTML=X),W=p(o),d($.$$.fragment,o),G=p(o),i=m(o,"P",{"data-svelte-h":!0}),b(i)!=="svelte-1v13hlo"&&(i.innerHTML=_),E=p(o),d(B.$$.fragment,o)},m(o,J){u(l,o,J),a(o,g,J),a(o,r,J),a(o,w,J),u(C,o,J),a(o,R,J),a(o,I,J),a(o,Z,J),a(o,U,J),a(o,W,J),u($,o,J),a(o,G,J),a(o,i,J),a(o,E,J),u(B,o,J),x=!0},p(o,J){const Xs={};J&2&&(Xs.$$scope={dirty:J,ctx:o}),l.$set(Xs)},i(o){x||(M(l.$$.fragment,o),M(C.$$.fragment,o),M($.$$.fragment,o),M(B.$$.fragment,o),x=!0)},o(o){f(l.$$.fragment,o),f(C.$$.fragment,o),f($.$$.fragment,o),f(B.$$.fragment,o),x=!1},d(o){o&&(t(g),t(r),t(w),t(R),t(I),t(Z),t(U),t(W),t(G),t(i),t(E)),y(l,o),y(C,o),y($,o),y(B,o)}}}function Zt(v){let l,g;return l=new gt({props:{$$slots:{default:[Rt]},$$scope:{ctx:v}}}),{c(){h(l.$$.fragment)},l(r){d(l.$$.fragment,r)},m(r,j){u(l,r,j),g=!0},p(r,j){const w={};j&2&&(w.$$scope={dirty:j,ctx:r}),l.$set(w)},i(r){g||(M(l.$$.fragment,r),g=!0)},o(r){f(l.$$.fragment,r),g=!1},d(r){y(l,r)}}}function Wt(v){let l,g='For a more in-depth example of how to finetune a model for automatic speech recognition, take a look at this blog <a href="https://huggingface.co/blog/fine-tune-wav2vec2-english" rel="nofollow">post</a> for English ASR and this <a href="https://huggingface.co/blog/fine-tune-xlsr-wav2vec2" rel="nofollow">post</a> for multilingual ASR.';return{c(){l=c("p"),l.innerHTML=g},l(r){l=m(r,"P",{"data-svelte-h":!0}),b(l)!=="svelte-1jxkvde"&&(l.innerHTML=g)},m(r,j){a(r,l,j)},p:Hs,d(r){r&&t(l)}}}function xt(v){let l,g="The transcription is decent, but it could be better! Try finetuning your model on more examples to get even better results!";return{c(){l=c("p"),l.textContent=g},l(r){l=m(r,"P",{"data-svelte-h":!0}),b(l)!=="svelte-1a2d12i"&&(l.textContent=g)},m(r,j){a(r,l,j)},p:Hs,d(r){r&&t(l)}}}function Gt(v){let l,g="Load a processor to preprocess the audio file and transcription and return the <code>input</code> as PyTorch tensors:",r,j,w,C,R="Pass your inputs to the model and return the logits:",I,k,Z,U,X="Get the predicted <code>input_ids</code> with the highest probability, and use the processor to decode the predicted <code>input_ids</code> back into text:",W,$,G;return j=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfYXNyX21pbmRfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGRhdGFzZXQlNUIwJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUIlMjJhcnJheSUyMiU1RCUyQyUyMHNhbXBsaW5nX3JhdGUlM0RzYW1wbGluZ19yYXRlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_asr_mind_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),k=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNUQyUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ1RDLmZyb21fcHJldHJhaW5lZCglMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfYXNyX21pbmRfbW9kZWwlMjIpJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_asr_mind_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits`,wrap:!1}}),$=new T({props:{code:"aW1wb3J0JTIwdG9yY2glMEElMEFwcmVkaWN0ZWRfaWRzJTIwJTNEJTIwdG9yY2guYXJnbWF4KGxvZ2l0cyUyQyUyMGRpbSUzRC0xKSUwQXRyYW5zY3JpcHRpb24lMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKHByZWRpY3RlZF9pZHMpJTBBdHJhbnNjcmlwdGlvbg==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription = processor.batch_decode(predicted_ids)
<span class="hljs-meta">&gt;&gt;&gt; </span>transcription
[<span class="hljs-string">&#x27;I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER&#x27;</span>]`,wrap:!1}}),{c(){l=c("p"),l.innerHTML=g,r=n(),h(j.$$.fragment),w=n(),C=c("p"),C.textContent=R,I=n(),h(k.$$.fragment),Z=n(),U=c("p"),U.innerHTML=X,W=n(),h($.$$.fragment)},l(i){l=m(i,"P",{"data-svelte-h":!0}),b(l)!=="svelte-apwwja"&&(l.innerHTML=g),r=p(i),d(j.$$.fragment,i),w=p(i),C=m(i,"P",{"data-svelte-h":!0}),b(C)!=="svelte-1at92g"&&(C.textContent=R),I=p(i),d(k.$$.fragment,i),Z=p(i),U=m(i,"P",{"data-svelte-h":!0}),b(U)!=="svelte-1hxzj8m"&&(U.innerHTML=X),W=p(i),d($.$$.fragment,i)},m(i,_){a(i,l,_),a(i,r,_),u(j,i,_),a(i,w,_),a(i,C,_),a(i,I,_),u(k,i,_),a(i,Z,_),a(i,U,_),a(i,W,_),u($,i,_),G=!0},p:Hs,i(i){G||(M(j.$$.fragment,i),M(k.$$.fragment,i),M($.$$.fragment,i),G=!0)},o(i){f(j.$$.fragment,i),f(k.$$.fragment,i),f($.$$.fragment,i),G=!1},d(i){i&&(t(l),t(r),t(w),t(C),t(I),t(Z),t(U),t(W)),y(j,i),y(k,i),y($,i)}}}function Bt(v){let l,g;return l=new gt({props:{$$slots:{default:[Gt]},$$scope:{ctx:v}}}),{c(){h(l.$$.fragment)},l(r){d(l.$$.fragment,r)},m(r,j){u(l,r,j),g=!0},p(r,j){const w={};j&2&&(w.$$scope={dirty:j,ctx:r}),l.$set(w)},i(r){g||(M(l.$$.fragment,r),g=!0)},o(r){f(l.$$.fragment,r),g=!1},d(r){y(l,r)}}}function Xt(v){let l,g,r,j,w,C,R,I,k,Z,U,X="Automatic speech recognition (ASR) converts a speech signal to text, mapping a sequence of audio inputs to text outputs. Virtual assistants like Siri and Alexa use ASR models to help users everyday, and there are many other useful user-facing applications like live captioning and note-taking during meetings.",W,$,G="This guide will show you how to:",i,_,E='<li>Finetune <a href="https://huggingface.co/facebook/wav2vec2-base" rel="nofollow">Wav2Vec2</a> on the <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">MInDS-14</a> dataset to transcribe audio to text.</li> <li>Use your finetuned model for inference.</li>',B,x,o,J,Xs="Before you begin, make sure you have all the necessary libraries installed:",Ys,Q,Ns,z,Ye="We encourage you to login to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to login:",Es,A,Qs,F,zs,q,Ne='Start by loading a smaller subset of the <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">MInDS-14</a> dataset from the ðŸ¤— Datasets library. Thisâ€™ll give you a chance to experiment and make sure everything works before spending more time training on the full dataset.',As,S,Fs,L,Ee="Split the datasetâ€™s <code>train</code> split into a train and test set with the <code>~Dataset.train_test_split</code> method:",qs,P,Ss,D,Qe="Then take a look at the dataset:",Ls,K,Ps,O,ze='While the dataset contains a lot of useful information, like <code>lang_id</code> and <code>english_transcription</code>, youâ€™ll focus on the <code>audio</code> and <code>transcription</code> in this guide. Remove the other columns with the <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.remove_columns" rel="nofollow">remove_columns</a> method:',Ds,ss,Ks,es,Ae="Take a look at the example again:",Os,ts,se,as,Fe="There are two fields:",ee,ls,qe="<li><code>audio</code>: a 1-dimensional <code>array</code> of the speech signal that must be called to load and resample the audio file.</li> <li><code>transcription</code>: the target text.</li>",te,ns,ae,ps,Se="The next step is to load a Wav2Vec2 processor to process the audio signal:",le,rs,ne,os,Le='The MInDS-14 dataset has a sampling rate of 8000kHz (you can find this information in its <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">dataset card</a>), which means youâ€™ll need to resample the dataset to 16000kHz to use the pretrained Wav2Vec2 model:',pe,is,re,cs,Pe="As you can see in the <code>transcription</code> above, the text contains a mix of upper and lowercase characters. The Wav2Vec2 tokenizer is only trained on uppercase characters so youâ€™ll need to make sure the text matches the tokenizerâ€™s vocabulary:",oe,ms,ie,hs,De="Now create a preprocessing function that:",ce,ds,Ke="<li>Calls the <code>audio</code> column to load and resample the audio file.</li> <li>Extracts the <code>input_values</code> from the audio file and tokenize the <code>transcription</code> column with the processor.</li>",me,us,he,Ms,Oe='To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map" rel="nofollow">map</a> function. You can speed up <code>map</code> by increasing the number of processes with the <code>num_proc</code> parameter. Remove the columns you donâ€™t need with the <a href="https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.remove_columns" rel="nofollow">remove_columns</a> method:',de,fs,ue,ys,st='ðŸ¤— Transformers doesnâ€™t have a data collator for ASR, so youâ€™ll need to adapt the <a href="/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding">DataCollatorWithPadding</a> to create a batch of examples. Itâ€™ll also dynamically pad your text and labels to the length of the longest element in its batch (instead of the entire dataset) so they are a uniform length. While it is possible to pad your text in the <code>tokenizer</code> function by setting <code>padding=True</code>, dynamic padding is more efficient.',Me,gs,et="Unlike other data collators, this specific data collator needs to apply a different padding method to <code>input_values</code> and <code>labels</code>:",fe,js,ye,bs,tt="Now instantiate your <code>DataCollatorForCTCWithPadding</code>:",ge,ws,je,Js,be,Ts,at='Including a metric during training is often helpful for evaluating your modelâ€™s performance. You can quickly load a evaluation method with the ðŸ¤— <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> library. For this task, load the <a href="https://huggingface.co/spaces/evaluate-metric/wer" rel="nofollow">word error rate</a> (WER) metric (see the ðŸ¤— Evaluate <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">quick tour</a> to learn more about how to load and compute a metric):',we,Us,Je,$s,lt='Then create a function that passes your predictions and labels to <a href="https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute" rel="nofollow">compute</a> to calculate the WER:',Te,_s,Ue,Cs,nt="Your <code>compute_metrics</code> function is ready to go now, and youâ€™ll return to it when you setup your training.",$e,vs,_e,V,Ce,H,ve,ks,ke,Is,pt="Great, now that youâ€™ve finetuned a model, you can use it for inference!",Ie,Rs,rt="Load an audio file youâ€™d like to run inference on. Remember to resample the sampling rate of the audio file to match the sampling rate of the model if you need to!",Re,Zs,Ze,Ws,ot='The simplest way to try out your finetuned model for inference is to use it in a <a href="/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline">pipeline()</a>. Instantiate a <code>pipeline</code> for automatic speech recognition with your model, and pass your audio file to it:',We,xs,xe,Y,Ge,Gs,it="You can also manually replicate the results of the <code>pipeline</code> if youâ€™d like:",Be,N,Xe,Vs,Ve;return w=new Bs({props:{title:"Automatic speech recognition",local:"automatic-speech-recognition",headingTag:"h1"}}),R=new vt({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/asr.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/asr.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/asr.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/asr.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/pytorch/asr.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/en/tensorflow/asr.ipynb"}]}}),k=new Ct({props:{id:"TksaY_FDgnk"}}),x=new He({props:{$$slots:{default:[kt]},$$scope:{ctx:v}}}),Q=new T({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGUlMjBqaXdlcg==",highlighted:"pip install transformers datasets evaluate jiwer",wrap:!1}}),A=new T({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),F=new Bs({props:{title:"Load MInDS-14 dataset",local:"load-minds-14-dataset",headingTag:"h2"}}),S=new T({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFtaW5kcyUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJQb2x5QUklMkZtaW5kczE0JTIyJTJDJTIwbmFtZSUzRCUyMmVuLVVTJTIyJTJDJTIwc3BsaXQlM0QlMjJ0cmFpbiU1QiUzQTEwMCU1RCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>minds = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train[:100]&quot;</span>)`,wrap:!1}}),P=new T({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy50cmFpbl90ZXN0X3NwbGl0KHRlc3Rfc2l6ZSUzRDAuMik=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.train_test_split(test_size=<span class="hljs-number">0.2</span>)',wrap:!1}}),K=new T({props:{code:"bWluZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds
DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;transcription&#x27;</span>, <span class="hljs-string">&#x27;english_transcription&#x27;</span>, <span class="hljs-string">&#x27;intent_class&#x27;</span>, <span class="hljs-string">&#x27;lang_id&#x27;</span>],
        num_rows: <span class="hljs-number">16</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;transcription&#x27;</span>, <span class="hljs-string">&#x27;english_transcription&#x27;</span>, <span class="hljs-string">&#x27;intent_class&#x27;</span>, <span class="hljs-string">&#x27;lang_id&#x27;</span>],
        num_rows: <span class="hljs-number">4</span>
    })
})`,wrap:!1}}),ss=new T({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy5yZW1vdmVfY29sdW1ucyglNUIlMjJlbmdsaXNoX3RyYW5zY3JpcHRpb24lMjIlMkMlMjAlMjJpbnRlbnRfY2xhc3MlMjIlMkMlMjAlMjJsYW5nX2lkJTIyJTVEKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.remove_columns([<span class="hljs-string">&quot;english_transcription&quot;</span>, <span class="hljs-string">&quot;intent_class&quot;</span>, <span class="hljs-string">&quot;lang_id&quot;</span>])',wrap:!1}}),ts=new T({props:{code:"bWluZHMlNUIlMjJ0cmFpbiUyMiU1RCU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">0.00024414</span>,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        , ...,  <span class="hljs-number">0.00024414</span>,
          <span class="hljs-number">0.00024414</span>,  <span class="hljs-number">0.00024414</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>},
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
 <span class="hljs-string">&#x27;transcription&#x27;</span>: <span class="hljs-string">&quot;hi I&#x27;m trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing&quot;</span>}`,wrap:!1}}),ns=new Bs({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),rs=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRndhdjJ2ZWMyLWJhc2UlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`,wrap:!1}}),is=new T({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy5jYXN0X2NvbHVtbiglMjJhdWRpbyUyMiUyQyUyMEF1ZGlvKHNhbXBsaW5nX3JhdGUlM0QxNl8wMDApKSUwQW1pbmRzJTVCJTIydHJhaW4lMjIlNUQlNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>minds[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([-<span class="hljs-number">2.38064706e-04</span>, -<span class="hljs-number">1.58618059e-04</span>, -<span class="hljs-number">5.43987835e-06</span>, ...,
          <span class="hljs-number">2.78103951e-04</span>,  <span class="hljs-number">2.38446111e-04</span>,  <span class="hljs-number">1.18740834e-04</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>},
 <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602ba9e2963e11ccd901cd4f.wav&#x27;</span>,
 <span class="hljs-string">&#x27;transcription&#x27;</span>: <span class="hljs-string">&quot;hi I&#x27;m trying to use the banking app on my phone and currently my checking and savings account balance is not refreshing&quot;</span>}`,wrap:!1}}),ms=new T({props:{code:"ZGVmJTIwdXBwZXJjYXNlKGV4YW1wbGUpJTNBJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwJTdCJTIydHJhbnNjcmlwdGlvbiUyMiUzQSUyMGV4YW1wbGUlNUIlMjJ0cmFuc2NyaXB0aW9uJTIyJTVELnVwcGVyKCklN0QlMEElMEElMEFtaW5kcyUyMCUzRCUyMG1pbmRzLm1hcCh1cHBlcmNhc2Up",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">uppercase</span>(<span class="hljs-params">example</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;transcription&quot;</span>: example[<span class="hljs-string">&quot;transcription&quot;</span>].upper()}


<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.<span class="hljs-built_in">map</span>(uppercase)`,wrap:!1}}),us=new T({props:{code:"ZGVmJTIwcHJlcGFyZV9kYXRhc2V0KGJhdGNoKSUzQSUwQSUyMCUyMCUyMCUyMGF1ZGlvJTIwJTNEJTIwYmF0Y2glNUIlMjJhdWRpbyUyMiU1RCUwQSUyMCUyMCUyMCUyMGJhdGNoJTIwJTNEJTIwcHJvY2Vzc29yKGF1ZGlvJTVCJTIyYXJyYXklMjIlNUQlMkMlMjBzYW1wbGluZ19yYXRlJTNEYXVkaW8lNUIlMjJzYW1wbGluZ19yYXRlJTIyJTVEJTJDJTIwdGV4dCUzRGJhdGNoJTVCJTIydHJhbnNjcmlwdGlvbiUyMiU1RCklMEElMjAlMjAlMjAlMjBiYXRjaCU1QiUyMmlucHV0X2xlbmd0aCUyMiU1RCUyMCUzRCUyMGxlbihiYXRjaCU1QiUyMmlucHV0X3ZhbHVlcyUyMiU1RCU1QjAlNUQpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwYmF0Y2g=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">prepare_dataset</span>(<span class="hljs-params">batch</span>):
<span class="hljs-meta">... </span>    audio = batch[<span class="hljs-string">&quot;audio&quot;</span>]
<span class="hljs-meta">... </span>    batch = processor(audio[<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=audio[<span class="hljs-string">&quot;sampling_rate&quot;</span>], text=batch[<span class="hljs-string">&quot;transcription&quot;</span>])
<span class="hljs-meta">... </span>    batch[<span class="hljs-string">&quot;input_length&quot;</span>] = <span class="hljs-built_in">len</span>(batch[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">0</span>])
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> batch`,wrap:!1}}),fs=new T({props:{code:"ZW5jb2RlZF9taW5kcyUyMCUzRCUyMG1pbmRzLm1hcChwcmVwYXJlX2RhdGFzZXQlMkMlMjByZW1vdmVfY29sdW1ucyUzRG1pbmRzLmNvbHVtbl9uYW1lcyU1QiUyMnRyYWluJTIyJTVEJTJDJTIwbnVtX3Byb2MlM0Q0KQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_minds = minds.<span class="hljs-built_in">map</span>(prepare_dataset, remove_columns=minds.column_names[<span class="hljs-string">&quot;train&quot;</span>], num_proc=<span class="hljs-number">4</span>)',wrap:!1}}),js=new T({props:{code:"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",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> dataclasses <span class="hljs-keyword">import</span> dataclass, field
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> <span class="hljs-type">Any</span>, <span class="hljs-type">Dict</span>, <span class="hljs-type">List</span>, <span class="hljs-type">Optional</span>, <span class="hljs-type">Union</span>


<span class="hljs-meta">&gt;&gt;&gt; </span>@dataclass
<span class="hljs-meta">... </span><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataCollatorCTCWithPadding</span>:
<span class="hljs-meta">... </span>    processor: AutoProcessor
<span class="hljs-meta">... </span>    padding: <span class="hljs-type">Union</span>[<span class="hljs-built_in">bool</span>, <span class="hljs-built_in">str</span>] = <span class="hljs-string">&quot;longest&quot;</span>

<span class="hljs-meta">... </span>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__call__</span>(<span class="hljs-params">self, features: <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Union</span>[<span class="hljs-type">List</span>[<span class="hljs-built_in">int</span>], torch.Tensor]]]</span>) -&gt; <span class="hljs-type">Dict</span>[<span class="hljs-built_in">str</span>, torch.Tensor]:
<span class="hljs-meta">... </span>        <span class="hljs-comment"># split inputs and labels since they have to be of different lengths and need</span>
<span class="hljs-meta">... </span>        <span class="hljs-comment"># different padding methods</span>
<span class="hljs-meta">... </span>        input_features = [{<span class="hljs-string">&quot;input_values&quot;</span>: feature[<span class="hljs-string">&quot;input_values&quot;</span>][<span class="hljs-number">0</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]
<span class="hljs-meta">... </span>        label_features = [{<span class="hljs-string">&quot;input_ids&quot;</span>: feature[<span class="hljs-string">&quot;labels&quot;</span>]} <span class="hljs-keyword">for</span> feature <span class="hljs-keyword">in</span> features]

<span class="hljs-meta">... </span>        batch = self.processor.pad(input_features, padding=self.padding, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">... </span>        labels_batch = self.processor.pad(labels=label_features, padding=self.padding, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">... </span>        <span class="hljs-comment"># replace padding with -100 to ignore loss correctly</span>
<span class="hljs-meta">... </span>        labels = labels_batch[<span class="hljs-string">&quot;input_ids&quot;</span>].masked_fill(labels_batch.attention_mask.ne(<span class="hljs-number">1</span>), -<span class="hljs-number">100</span>)

<span class="hljs-meta">... </span>        batch[<span class="hljs-string">&quot;labels&quot;</span>] = labels

<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> batch`,wrap:!1}}),ws=new T({props:{code:"ZGF0YV9jb2xsYXRvciUyMCUzRCUyMERhdGFDb2xsYXRvckNUQ1dpdGhQYWRkaW5nKHByb2Nlc3NvciUzRHByb2Nlc3NvciUyQyUyMHBhZGRpbmclM0QlMjJsb25nZXN0JTIyKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorCTCWithPadding(processor=processor, padding=<span class="hljs-string">&quot;longest&quot;</span>)',wrap:!1}}),Js=new Bs({props:{title:"Evaluate",local:"evaluate",headingTag:"h2"}}),Us=new T({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEF3ZXIlMjAlM0QlMjBldmFsdWF0ZS5sb2FkKCUyMndlciUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>wer = evaluate.load(<span class="hljs-string">&quot;wer&quot;</span>)`,wrap:!1}}),_s=new T({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwY29tcHV0ZV9tZXRyaWNzKHByZWQpJTNBJTBBJTIwJTIwJTIwJTIwcHJlZF9sb2dpdHMlMjAlM0QlMjBwcmVkLnByZWRpY3Rpb25zJTBBJTIwJTIwJTIwJTIwcHJlZF9pZHMlMjAlM0QlMjBucC5hcmdtYXgocHJlZF9sb2dpdHMlMkMlMjBheGlzJTNELTEpJTBBJTBBJTIwJTIwJTIwJTIwcHJlZC5sYWJlbF9pZHMlNUJwcmVkLmxhYmVsX2lkcyUyMCUzRCUzRCUyMC0xMDAlNUQlMjAlM0QlMjBwcm9jZXNzb3IudG9rZW5pemVyLnBhZF90b2tlbl9pZCUwQSUwQSUyMCUyMCUyMCUyMHByZWRfc3RyJTIwJTNEJTIwcHJvY2Vzc29yLmJhdGNoX2RlY29kZShwcmVkX2lkcyklMEElMjAlMjAlMjAlMjBsYWJlbF9zdHIlMjAlM0QlMjBwcm9jZXNzb3IuYmF0Y2hfZGVjb2RlKHByZWQubGFiZWxfaWRzJTJDJTIwZ3JvdXBfdG9rZW5zJTNERmFsc2UpJTBBJTBBJTIwJTIwJTIwJTIwd2VyJTIwJTNEJTIwd2VyLmNvbXB1dGUocHJlZGljdGlvbnMlM0RwcmVkX3N0ciUyQyUyMHJlZmVyZW5jZXMlM0RsYWJlbF9zdHIpJTBBJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwJTdCJTIyd2VyJTIyJTNBJTIwd2VyJTdE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">pred</span>):
<span class="hljs-meta">... </span>    pred_logits = pred.predictions
<span class="hljs-meta">... </span>    pred_ids = np.argmax(pred_logits, axis=-<span class="hljs-number">1</span>)

<span class="hljs-meta">... </span>    pred.label_ids[pred.label_ids == -<span class="hljs-number">100</span>] = processor.tokenizer.pad_token_id

<span class="hljs-meta">... </span>    pred_str = processor.batch_decode(pred_ids)
<span class="hljs-meta">... </span>    label_str = processor.batch_decode(pred.label_ids, group_tokens=<span class="hljs-literal">False</span>)

<span class="hljs-meta">... </span>    wer = wer.compute(predictions=pred_str, references=label_str)

<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> {<span class="hljs-string">&quot;wer&quot;</span>: wer}`,wrap:!1}}),vs=new Bs({props:{title:"Train",local:"train",headingTag:"h2"}}),V=new yt({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Zt]},$$scope:{ctx:v}}}),H=new He({props:{$$slots:{default:[Wt]},$$scope:{ctx:v}}}),ks=new Bs({props:{title:"Inference",local:"inference",headingTag:"h2"}}),Zs=new T({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMlBvbHlBSSUyRm1pbmRzMTQlMjIlMkMlMjAlMjJlbi1VUyUyMiUyQyUyMHNwbGl0JTNEJTIydHJhaW4lMjIpJTBBZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQuY2FzdF9jb2x1bW4oJTIyYXVkaW8lMjIlMkMlMjBBdWRpbyhzYW1wbGluZ19yYXRlJTNEMTYwMDApKSUwQXNhbXBsaW5nX3JhdGUlMjAlM0QlMjBkYXRhc2V0LmZlYXR1cmVzJTVCJTIyYXVkaW8lMjIlNUQuc2FtcGxpbmdfcmF0ZSUwQWF1ZGlvX2ZpbGUlMjAlM0QlMjBkYXRhc2V0JTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIycGF0aCUyMiU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, <span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_file = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;path&quot;</span>]`,wrap:!1}}),xs=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBdHJhbnNjcmliZXIlMjAlM0QlMjBwaXBlbGluZSglMjJhdXRvbWF0aWMtc3BlZWNoLXJlY29nbml0aW9uJTIyJTJDJTIwbW9kZWwlM0QlMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfYXNyX21pbmRzX21vZGVsJTIyKSUwQXRyYW5zY3JpYmVyKGF1ZGlvX2ZpbGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>transcriber = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;stevhliu/my_awesome_asr_minds_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>transcriber(audio_file)
{<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;I WOUD LIKE O SET UP JOINT ACOUNT WTH Y PARTNER&#x27;</span>}`,wrap:!1}}),Y=new He({props:{$$slots:{default:[xt]},$$scope:{ctx:v}}}),N=new yt({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Bt]},$$scope:{ctx:v}}}),{c(){l=c("meta"),g=n(),r=c("p"),j=n(),h(w.$$.fragment),C=n(),h(R.$$.fragment),I=n(),h(k.$$.fragment),Z=n(),U=c("p"),U.textContent=X,W=n(),$=c("p"),$.textContent=G,i=n(),_=c("ol"),_.innerHTML=E,B=n(),h(x.$$.fragment),o=n(),J=c("p"),J.textContent=Xs,Ys=n(),h(Q.$$.fragment),Ns=n(),z=c("p"),z.textContent=Ye,Es=n(),h(A.$$.fragment),Qs=n(),h(F.$$.fragment),zs=n(),q=c("p"),q.innerHTML=Ne,As=n(),h(S.$$.fragment),Fs=n(),L=c("p"),L.innerHTML=Ee,qs=n(),h(P.$$.fragment),Ss=n(),D=c("p"),D.textContent=Qe,Ls=n(),h(K.$$.fragment),Ps=n(),O=c("p"),O.innerHTML=ze,Ds=n(),h(ss.$$.fragment),Ks=n(),es=c("p"),es.textContent=Ae,Os=n(),h(ts.$$.fragment),se=n(),as=c("p"),as.textContent=Fe,ee=n(),ls=c("ul"),ls.innerHTML=qe,te=n(),h(ns.$$.fragment),ae=n(),ps=c("p"),ps.textContent=Se,le=n(),h(rs.$$.fragment),ne=n(),os=c("p"),os.innerHTML=Le,pe=n(),h(is.$$.fragment),re=n(),cs=c("p"),cs.innerHTML=Pe,oe=n(),h(ms.$$.fragment),ie=n(),hs=c("p"),hs.textContent=De,ce=n(),ds=c("ol"),ds.innerHTML=Ke,me=n(),h(us.$$.fragment),he=n(),Ms=c("p"),Ms.innerHTML=Oe,de=n(),h(fs.$$.fragment),ue=n(),ys=c("p"),ys.innerHTML=st,Me=n(),gs=c("p"),gs.innerHTML=et,fe=n(),h(js.$$.fragment),ye=n(),bs=c("p"),bs.innerHTML=tt,ge=n(),h(ws.$$.fragment),je=n(),h(Js.$$.fragment),be=n(),Ts=c("p"),Ts.innerHTML=at,we=n(),h(Us.$$.fragment),Je=n(),$s=c("p"),$s.innerHTML=lt,Te=n(),h(_s.$$.fragment),Ue=n(),Cs=c("p"),Cs.innerHTML=nt,$e=n(),h(vs.$$.fragment),_e=n(),h(V.$$.fragment),Ce=n(),h(H.$$.fragment),ve=n(),h(ks.$$.fragment),ke=n(),Is=c("p"),Is.textContent=pt,Ie=n(),Rs=c("p"),Rs.textContent=rt,Re=n(),h(Zs.$$.fragment),Ze=n(),Ws=c("p"),Ws.innerHTML=ot,We=n(),h(xs.$$.fragment),xe=n(),h(Y.$$.fragment),Ge=n(),Gs=c("p"),Gs.innerHTML=it,Be=n(),h(N.$$.fragment),Xe=n(),Vs=c("p"),this.h()},l(s){const e=Tt("svelte-u9bgzb",document.head);l=m(e,"META",{name:!0,content:!0}),e.forEach(t),g=p(s),r=m(s,"P",{}),Mt(r).forEach(t),j=p(s),d(w.$$.fragment,s),C=p(s),d(R.$$.fragment,s),I=p(s),d(k.$$.fragment,s),Z=p(s),U=m(s,"P",{"data-svelte-h":!0}),b(U)!=="svelte-jaigob"&&(U.textContent=X),W=p(s),$=m(s,"P",{"data-svelte-h":!0}),b($)!=="svelte-1aff4p7"&&($.textContent=G),i=p(s),_=m(s,"OL",{"data-svelte-h":!0}),b(_)!=="svelte-swvwb8"&&(_.innerHTML=E),B=p(s),d(x.$$.fragment,s),o=p(s),J=m(s,"P",{"data-svelte-h":!0}),b(J)!=="svelte-1c9nexd"&&(J.textContent=Xs),Ys=p(s),d(Q.$$.fragment,s),Ns=p(s),z=m(s,"P",{"data-svelte-h":!0}),b(z)!=="svelte-k76o1m"&&(z.textContent=Ye),Es=p(s),d(A.$$.fragment,s),Qs=p(s),d(F.$$.fragment,s),zs=p(s),q=m(s,"P",{"data-svelte-h":!0}),b(q)!=="svelte-17q5vds"&&(q.innerHTML=Ne),As=p(s),d(S.$$.fragment,s),Fs=p(s),L=m(s,"P",{"data-svelte-h":!0}),b(L)!=="svelte-1mnfdd3"&&(L.innerHTML=Ee),qs=p(s),d(P.$$.fragment,s),Ss=p(s),D=m(s,"P",{"data-svelte-h":!0}),b(D)!=="svelte-2twqg0"&&(D.textContent=Qe),Ls=p(s),d(K.$$.fragment,s),Ps=p(s),O=m(s,"P",{"data-svelte-h":!0}),b(O)!=="svelte-1ww49c9"&&(O.innerHTML=ze),Ds=p(s),d(ss.$$.fragment,s),Ks=p(s),es=m(s,"P",{"data-svelte-h":!0}),b(es)!=="svelte-kucdg1"&&(es.textContent=Ae),Os=p(s),d(ts.$$.fragment,s),se=p(s),as=m(s,"P",{"data-svelte-h":!0}),b(as)!=="svelte-bf7elb"&&(as.textContent=Fe),ee=p(s),ls=m(s,"UL",{"data-svelte-h":!0}),b(ls)!=="svelte-k1dj8f"&&(ls.innerHTML=qe),te=p(s),d(ns.$$.fragment,s),ae=p(s),ps=m(s,"P",{"data-svelte-h":!0}),b(ps)!=="svelte-1u4mmu7"&&(ps.textContent=Se),le=p(s),d(rs.$$.fragment,s),ne=p(s),os=m(s,"P",{"data-svelte-h":!0}),b(os)!=="svelte-yrxfgz"&&(os.innerHTML=Le),pe=p(s),d(is.$$.fragment,s),re=p(s),cs=m(s,"P",{"data-svelte-h":!0}),b(cs)!=="svelte-w66yza"&&(cs.innerHTML=Pe),oe=p(s),d(ms.$$.fragment,s),ie=p(s),hs=m(s,"P",{"data-svelte-h":!0}),b(hs)!=="svelte-8cflje"&&(hs.textContent=De),ce=p(s),ds=m(s,"OL",{"data-svelte-h":!0}),b(ds)!=="svelte-1ydcdgg"&&(ds.innerHTML=Ke),me=p(s),d(us.$$.fragment,s),he=p(s),Ms=m(s,"P",{"data-svelte-h":!0}),b(Ms)!=="svelte-6k75jk"&&(Ms.innerHTML=Oe),de=p(s),d(fs.$$.fragment,s),ue=p(s),ys=m(s,"P",{"data-svelte-h":!0}),b(ys)!=="svelte-1xej9zp"&&(ys.innerHTML=st),Me=p(s),gs=m(s,"P",{"data-svelte-h":!0}),b(gs)!=="svelte-ik80vf"&&(gs.innerHTML=et),fe=p(s),d(js.$$.fragment,s),ye=p(s),bs=m(s,"P",{"data-svelte-h":!0}),b(bs)!=="svelte-hven70"&&(bs.innerHTML=tt),ge=p(s),d(ws.$$.fragment,s),je=p(s),d(Js.$$.fragment,s),be=p(s),Ts=m(s,"P",{"data-svelte-h":!0}),b(Ts)!=="svelte-od28ug"&&(Ts.innerHTML=at),we=p(s),d(Us.$$.fragment,s),Je=p(s),$s=m(s,"P",{"data-svelte-h":!0}),b($s)!=="svelte-ss7nbw"&&($s.innerHTML=lt),Te=p(s),d(_s.$$.fragment,s),Ue=p(s),Cs=m(s,"P",{"data-svelte-h":!0}),b(Cs)!=="svelte-183aynn"&&(Cs.innerHTML=nt),$e=p(s),d(vs.$$.fragment,s),_e=p(s),d(V.$$.fragment,s),Ce=p(s),d(H.$$.fragment,s),ve=p(s),d(ks.$$.fragment,s),ke=p(s),Is=m(s,"P",{"data-svelte-h":!0}),b(Is)!=="svelte-633ppb"&&(Is.textContent=pt),Ie=p(s),Rs=m(s,"P",{"data-svelte-h":!0}),b(Rs)!=="svelte-1j24vrm"&&(Rs.textContent=rt),Re=p(s),d(Zs.$$.fragment,s),Ze=p(s),Ws=m(s,"P",{"data-svelte-h":!0}),b(Ws)!=="svelte-19jfw7j"&&(Ws.innerHTML=ot),We=p(s),d(xs.$$.fragment,s),xe=p(s),d(Y.$$.fragment,s),Ge=p(s),Gs=m(s,"P",{"data-svelte-h":!0}),b(Gs)!=="svelte-1njl8vm"&&(Gs.innerHTML=it),Be=p(s),d(N.$$.fragment,s),Xe=p(s),Vs=m(s,"P",{}),Mt(Vs).forEach(t),this.h()},h(){ft(l,"name","hf:doc:metadata"),ft(l,"content",Vt)},m(s,e){Ut(document.head,l),a(s,g,e),a(s,r,e),a(s,j,e),u(w,s,e),a(s,C,e),u(R,s,e),a(s,I,e),u(k,s,e),a(s,Z,e),a(s,U,e),a(s,W,e),a(s,$,e),a(s,i,e),a(s,_,e),a(s,B,e),u(x,s,e),a(s,o,e),a(s,J,e),a(s,Ys,e),u(Q,s,e),a(s,Ns,e),a(s,z,e),a(s,Es,e),u(A,s,e),a(s,Qs,e),u(F,s,e),a(s,zs,e),a(s,q,e),a(s,As,e),u(S,s,e),a(s,Fs,e),a(s,L,e),a(s,qs,e),u(P,s,e),a(s,Ss,e),a(s,D,e),a(s,Ls,e),u(K,s,e),a(s,Ps,e),a(s,O,e),a(s,Ds,e),u(ss,s,e),a(s,Ks,e),a(s,es,e),a(s,Os,e),u(ts,s,e),a(s,se,e),a(s,as,e),a(s,ee,e),a(s,ls,e),a(s,te,e),u(ns,s,e),a(s,ae,e),a(s,ps,e),a(s,le,e),u(rs,s,e),a(s,ne,e),a(s,os,e),a(s,pe,e),u(is,s,e),a(s,re,e),a(s,cs,e),a(s,oe,e),u(ms,s,e),a(s,ie,e),a(s,hs,e),a(s,ce,e),a(s,ds,e),a(s,me,e),u(us,s,e),a(s,he,e),a(s,Ms,e),a(s,de,e),u(fs,s,e),a(s,ue,e),a(s,ys,e),a(s,Me,e),a(s,gs,e),a(s,fe,e),u(js,s,e),a(s,ye,e),a(s,bs,e),a(s,ge,e),u(ws,s,e),a(s,je,e),u(Js,s,e),a(s,be,e),a(s,Ts,e),a(s,we,e),u(Us,s,e),a(s,Je,e),a(s,$s,e),a(s,Te,e),u(_s,s,e),a(s,Ue,e),a(s,Cs,e),a(s,$e,e),u(vs,s,e),a(s,_e,e),u(V,s,e),a(s,Ce,e),u(H,s,e),a(s,ve,e),u(ks,s,e),a(s,ke,e),a(s,Is,e),a(s,Ie,e),a(s,Rs,e),a(s,Re,e),u(Zs,s,e),a(s,Ze,e),a(s,Ws,e),a(s,We,e),u(xs,s,e),a(s,xe,e),u(Y,s,e),a(s,Ge,e),a(s,Gs,e),a(s,Be,e),u(N,s,e),a(s,Xe,e),a(s,Vs,e),Ve=!0},p(s,[e]){const ct={};e&2&&(ct.$$scope={dirty:e,ctx:s}),x.$set(ct);const mt={};e&2&&(mt.$$scope={dirty:e,ctx:s}),V.$set(mt);const ht={};e&2&&(ht.$$scope={dirty:e,ctx:s}),H.$set(ht);const dt={};e&2&&(dt.$$scope={dirty:e,ctx:s}),Y.$set(dt);const ut={};e&2&&(ut.$$scope={dirty:e,ctx:s}),N.$set(ut)},i(s){Ve||(M(w.$$.fragment,s),M(R.$$.fragment,s),M(k.$$.fragment,s),M(x.$$.fragment,s),M(Q.$$.fragment,s),M(A.$$.fragment,s),M(F.$$.fragment,s),M(S.$$.fragment,s),M(P.$$.fragment,s),M(K.$$.fragment,s),M(ss.$$.fragment,s),M(ts.$$.fragment,s),M(ns.$$.fragment,s),M(rs.$$.fragment,s),M(is.$$.fragment,s),M(ms.$$.fragment,s),M(us.$$.fragment,s),M(fs.$$.fragment,s),M(js.$$.fragment,s),M(ws.$$.fragment,s),M(Js.$$.fragment,s),M(Us.$$.fragment,s),M(_s.$$.fragment,s),M(vs.$$.fragment,s),M(V.$$.fragment,s),M(H.$$.fragment,s),M(ks.$$.fragment,s),M(Zs.$$.fragment,s),M(xs.$$.fragment,s),M(Y.$$.fragment,s),M(N.$$.fragment,s),Ve=!0)},o(s){f(w.$$.fragment,s),f(R.$$.fragment,s),f(k.$$.fragment,s),f(x.$$.fragment,s),f(Q.$$.fragment,s),f(A.$$.fragment,s),f(F.$$.fragment,s),f(S.$$.fragment,s),f(P.$$.fragment,s),f(K.$$.fragment,s),f(ss.$$.fragment,s),f(ts.$$.fragment,s),f(ns.$$.fragment,s),f(rs.$$.fragment,s),f(is.$$.fragment,s),f(ms.$$.fragment,s),f(us.$$.fragment,s),f(fs.$$.fragment,s),f(js.$$.fragment,s),f(ws.$$.fragment,s),f(Js.$$.fragment,s),f(Us.$$.fragment,s),f(_s.$$.fragment,s),f(vs.$$.fragment,s),f(V.$$.fragment,s),f(H.$$.fragment,s),f(ks.$$.fragment,s),f(Zs.$$.fragment,s),f(xs.$$.fragment,s),f(Y.$$.fragment,s),f(N.$$.fragment,s),Ve=!1},d(s){s&&(t(g),t(r),t(j),t(C),t(I),t(Z),t(U),t(W),t($),t(i),t(_),t(B),t(o),t(J),t(Ys),t(Ns),t(z),t(Es),t(Qs),t(zs),t(q),t(As),t(Fs),t(L),t(qs),t(Ss),t(D),t(Ls),t(Ps),t(O),t(Ds),t(Ks),t(es),t(Os),t(se),t(as),t(ee),t(ls),t(te),t(ae),t(ps),t(le),t(ne),t(os),t(pe),t(re),t(cs),t(oe),t(ie),t(hs),t(ce),t(ds),t(me),t(he),t(Ms),t(de),t(ue),t(ys),t(Me),t(gs),t(fe),t(ye),t(bs),t(ge),t(je),t(be),t(Ts),t(we),t(Je),t($s),t(Te),t(Ue),t(Cs),t($e),t(_e),t(Ce),t(ve),t(ke),t(Is),t(Ie),t(Rs),t(Re),t(Ze),t(Ws),t(We),t(xe),t(Ge),t(Gs),t(Be),t(Xe),t(Vs)),t(l),y(w,s),y(R,s),y(k,s),y(x,s),y(Q,s),y(A,s),y(F,s),y(S,s),y(P,s),y(K,s),y(ss,s),y(ts,s),y(ns,s),y(rs,s),y(is,s),y(ms,s),y(us,s),y(fs,s),y(js,s),y(ws,s),y(Js,s),y(Us,s),y(_s,s),y(vs,s),y(V,s),y(H,s),y(ks,s),y(Zs,s),y(xs,s),y(Y,s),y(N,s)}}}const Vt='{"title":"Automatic speech recognition","local":"automatic-speech-recognition","sections":[{"title":"Load MInDS-14 dataset","local":"load-minds-14-dataset","sections":[],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[],"depth":2},{"title":"Evaluate","local":"evaluate","sections":[],"depth":2},{"title":"Train","local":"train","sections":[],"depth":2},{"title":"Inference","local":"inference","sections":[],"depth":2}],"depth":1}';function Ht(v){return bt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class St extends wt{constructor(l){super(),Jt(this,l,Ht,Xt,jt,{})}}export{St as component};
