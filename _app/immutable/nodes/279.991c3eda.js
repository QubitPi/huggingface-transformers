import{s as at,f as st,o as rt,n as ze}from"../chunks/scheduler.9bc65507.js";import{S as it,i as lt,g as l,s,r as g,A as mt,h as m,f as n,c as r,j as ie,u,x as d,k as J,y as w,a as o,v as h,d as b,t as _,w as y}from"../chunks/index.707bf1b6.js";import{T as ct}from"../chunks/Tip.c2ecdbf4.js";import{D as Ge}from"../chunks/Docstring.17db21ae.js";import{C as Ze}from"../chunks/CodeBlock.54a9f38d.js";import{E as ot}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as se}from"../chunks/Heading.342b1fa6.js";function pt(C){let i,v="Examples:",p,c,f;return c=new Ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFVwZXJOZXRDb25maWclMkMlMjBVcGVyTmV0Rm9yU2VtYW50aWNTZWdtZW50YXRpb24lMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBVcGVyTmV0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFVwZXJOZXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbihjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> UperNetConfig, UperNetForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = UperNetConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UperNetForSemanticSegmentation(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){i=l("p"),i.textContent=v,p=s(),g(c.$$.fragment)},l(a){i=m(a,"P",{"data-svelte-h":!0}),d(i)!=="svelte-kvfsh7"&&(i.textContent=v),p=r(a),u(c.$$.fragment,a)},m(a,M){o(a,i,M),o(a,p,M),h(c,a,M),f=!0},p:ze,i(a){f||(b(c.$$.fragment,a),f=!0)},o(a){_(c.$$.fragment,a),f=!1},d(a){a&&(n(i),n(p)),y(c,a)}}}function dt(C){let i,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){i=l("p"),i.innerHTML=v},l(p){i=m(p,"P",{"data-svelte-h":!0}),d(i)!=="svelte-fincs2"&&(i.innerHTML=v)},m(p,c){o(p,i,c)},p:ze,d(p){p&&n(i)}}}function ft(C){let i,v="Examples:",p,c,f;return c=new Ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFVwZXJOZXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjBodWdnaW5nZmFjZV9odWIlMjBpbXBvcnQlMjBoZl9odWJfZG93bmxvYWQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5tbWxhYiUyRnVwZXJuZXQtY29udm5leHQtdGlueSUyMiklMEFtb2RlbCUyMCUzRCUyMFVwZXJOZXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyb3Blbm1tbGFiJTJGdXBlcm5ldC1jb252bmV4dC10aW55JTIyKSUwQSUwQWZpbGVwYXRoJTIwJTNEJTIwaGZfaHViX2Rvd25sb2FkKCUwQSUyMCUyMCUyMCUyMHJlcG9faWQlM0QlMjJoZi1pbnRlcm5hbC10ZXN0aW5nJTJGZml4dHVyZXNfYWRlMjBrJTIyJTJDJTIwZmlsZW5hbWUlM0QlMjJBREVfdmFsXzAwMDAwMDAxLmpwZyUyMiUyQyUyMHJlcG9fdHlwZSUzRCUyMmRhdGFzZXQlMjIlMEEpJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKGZpbGVwYXRoKS5jb252ZXJ0KCUyMlJHQiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cyUyMCUyMCUyMyUyMHNoYXBlJTIwKGJhdGNoX3NpemUlMkMlMjBudW1fbGFiZWxzJTJDJTIwaGVpZ2h0JTJDJTIwd2lkdGgpJTBBbGlzdChsb2dpdHMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, UperNetForSemanticSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> hf_hub_download

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;openmmlab/upernet-convnext-tiny&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = UperNetForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;openmmlab/upernet-convnext-tiny&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>filepath = hf_hub_download(
<span class="hljs-meta">... </span>    repo_id=<span class="hljs-string">&quot;hf-internal-testing/fixtures_ade20k&quot;</span>, filename=<span class="hljs-string">&quot;ADE_val_00000001.jpg&quot;</span>, repo_type=<span class="hljs-string">&quot;dataset&quot;</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(filepath).convert(<span class="hljs-string">&quot;RGB&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits  <span class="hljs-comment"># shape (batch_size, num_labels, height, width)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">150</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>]`,wrap:!1}}),{c(){i=l("p"),i.textContent=v,p=s(),g(c.$$.fragment)},l(a){i=m(a,"P",{"data-svelte-h":!0}),d(i)!=="svelte-kvfsh7"&&(i.textContent=v),p=r(a),u(c.$$.fragment,a)},m(a,M){o(a,i,M),o(a,p,M),h(c,a,M),f=!0},p:ze,i(a){f||(b(c.$$.fragment,a),f=!0)},o(a){_(c.$$.fragment,a),f=!1},d(a){a&&(n(i),n(p)),y(c,a)}}}function gt(C){let i,v,p,c,f,a,M,le,W,Xe=`The UPerNet model was proposed in <a href="https://arxiv.org/abs/1807.10221" rel="nofollow">Unified Perceptual Parsing for Scene Understanding</a>
by Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun. UPerNet is a general framework to effectively segment
a wide range of concepts from images, leveraging any vision backbone like <a href="convnext">ConvNeXt</a> or <a href="swin">Swin</a>.`,me,R,Be="The abstract from the paper is the following:",ce,V,Ie="<em>Humans recognize the visual world at multiple levels: we effortlessly categorize scenes and detect objects inside, while also identifying the textures and surfaces of the objects along with their different compositional parts. In this paper, we study a new task called Unified Perceptual Parsing, which requires the machine vision systems to recognize as many visual concepts as possible from a given image. A multi-task framework called UPerNet and a training strategy are developed to learn from heterogeneous image annotations. We benchmark our framework on Unified Perceptual Parsing and show that it is able to effectively segment a wide range of concepts from images. The trained networks are further applied to discover visual knowledge in natural scenes.</em>",pe,Z,Qe,de,P,Ee='UPerNet framework. Taken from the <a href="https://arxiv.org/abs/1807.10221">original paper</a>.',fe,G,He='This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>. The original code is based on OpenMMLabâ€™s mmsegmentation <a href="https://github.com/open-mmlab/mmsegmentation/blob/master/mmseg/models/decode_heads/uper_head.py" rel="nofollow">here</a>.',ge,z,ue,X,Ye="UPerNet is a general framework for semantic segmentation. It can be used with any vision backbone, like so:",he,B,be,I,qe='To use another vision backbone, like <a href="convnext">ConvNeXt</a>, simply instantiate the model with the appropriate backbone:',_e,Q,ye,E,Le="Note that this will randomly initialize all the weights of the model.",Me,H,we,Y,Ae="A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with UPerNet.",ve,q,Oe='<li>Demo notebooks for UPerNet can be found <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/UPerNet" rel="nofollow">here</a>.</li> <li><a href="/docs/transformers/main/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a> is supported by this <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/semantic-segmentation" rel="nofollow">example script</a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/semantic_segmentation.ipynb" rel="nofollow">notebook</a>.</li> <li>See also: <a href="../tasks/semantic_segmentation">Semantic segmentation task guide</a></li>',Ue,L,De="If youâ€™re interested in submitting a resource to be included here, please feel free to open a Pull Request and weâ€™ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",$e,A,xe,U,O,ke,te,Ke=`This is the configuration class to store the configuration of an <a href="/docs/transformers/main/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a>. It is used to
instantiate an UperNet model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the UperNet
<a href="https://huggingface.co/openmmlab/upernet-convnext-tiny" rel="nofollow">openmmlab/upernet-convnext-tiny</a> architecture.`,Se,ne,et=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Fe,k,Ne,D,Te,x,K,Je,oe,tt="UperNet framework leveraging any vision backbone e.g. for ADE20k, CityScapes.",We,$,ee,Re,ae,nt='The <a href="/docs/transformers/main/en/model_doc/upernet#transformers.UperNetForSemanticSegmentation">UperNetForSemanticSegmentation</a> forward method, overrides the <code>__call__</code> special method.',Ve,S,Pe,F,je,re,Ce;return f=new se({props:{title:"UPerNet",local:"upernet",headingTag:"h1"}}),M=new se({props:{title:"Overview",local:"overview",headingTag:"h2"}}),z=new se({props:{title:"Usage examples",local:"usage-examples",headingTag:"h2"}}),B=new Ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFN3aW5Db25maWclMkMlMjBVcGVyTmV0Q29uZmlnJTJDJTIwVXBlck5ldEZvclNlbWFudGljU2VnbWVudGF0aW9uJTBBJTBBYmFja2JvbmVfY29uZmlnJTIwJTNEJTIwU3dpbkNvbmZpZyhvdXRfZmVhdHVyZXMlM0QlNUIlMjJzdGFnZTElMjIlMkMlMjAlMjJzdGFnZTIlMjIlMkMlMjAlMjJzdGFnZTMlMjIlMkMlMjAlMjJzdGFnZTQlMjIlNUQpJTBBJTBBY29uZmlnJTIwJTNEJTIwVXBlck5ldENvbmZpZyhiYWNrYm9uZV9jb25maWclM0RiYWNrYm9uZV9jb25maWcpJTBBbW9kZWwlMjAlM0QlMjBVcGVyTmV0Rm9yU2VtYW50aWNTZWdtZW50YXRpb24oY29uZmlnKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> SwinConfig, UperNetConfig, UperNetForSemanticSegmentation

backbone_config = SwinConfig(out_features=[<span class="hljs-string">&quot;stage1&quot;</span>, <span class="hljs-string">&quot;stage2&quot;</span>, <span class="hljs-string">&quot;stage3&quot;</span>, <span class="hljs-string">&quot;stage4&quot;</span>])

config = UperNetConfig(backbone_config=backbone_config)
model = UperNetForSemanticSegmentation(config)`,wrap:!1}}),Q=new Ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENvbnZOZXh0Q29uZmlnJTJDJTIwVXBlck5ldENvbmZpZyUyQyUyMFVwZXJOZXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQSUwQWJhY2tib25lX2NvbmZpZyUyMCUzRCUyMENvbnZOZXh0Q29uZmlnKG91dF9mZWF0dXJlcyUzRCU1QiUyMnN0YWdlMSUyMiUyQyUyMCUyMnN0YWdlMiUyMiUyQyUyMCUyMnN0YWdlMyUyMiUyQyUyMCUyMnN0YWdlNCUyMiU1RCklMEElMEFjb25maWclMjAlM0QlMjBVcGVyTmV0Q29uZmlnKGJhY2tib25lX2NvbmZpZyUzRGJhY2tib25lX2NvbmZpZyklMEFtb2RlbCUyMCUzRCUyMFVwZXJOZXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbihjb25maWcp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ConvNextConfig, UperNetConfig, UperNetForSemanticSegmentation

backbone_config = ConvNextConfig(out_features=[<span class="hljs-string">&quot;stage1&quot;</span>, <span class="hljs-string">&quot;stage2&quot;</span>, <span class="hljs-string">&quot;stage3&quot;</span>, <span class="hljs-string">&quot;stage4&quot;</span>])

config = UperNetConfig(backbone_config=backbone_config)
model = UperNetForSemanticSegmentation(config)`,wrap:!1}}),H=new se({props:{title:"Resources",local:"resources",headingTag:"h2"}}),A=new se({props:{title:"UperNetConfig",local:"transformers.UperNetConfig",headingTag:"h2"}}),O=new Ge({props:{name:"class transformers.UperNetConfig",anchor:"transformers.UperNetConfig",parameters:[{name:"backbone_config",val:" = None"},{name:"backbone",val:" = None"},{name:"use_pretrained_backbone",val:" = False"},{name:"use_timm_backbone",val:" = False"},{name:"backbone_kwargs",val:" = None"},{name:"hidden_size",val:" = 512"},{name:"initializer_range",val:" = 0.02"},{name:"pool_scales",val:" = [1, 2, 3, 6]"},{name:"use_auxiliary_head",val:" = True"},{name:"auxiliary_loss_weight",val:" = 0.4"},{name:"auxiliary_in_channels",val:" = 384"},{name:"auxiliary_channels",val:" = 256"},{name:"auxiliary_num_convs",val:" = 1"},{name:"auxiliary_concat_input",val:" = False"},{name:"loss_ignore_index",val:" = 255"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.UperNetConfig.backbone_config",description:`<strong>backbone_config</strong> (<code>PretrainedConfig</code> or <code>dict</code>, <em>optional</em>, defaults to <code>ResNetConfig()</code>) &#x2014;
The configuration of the backbone model.`,name:"backbone_config"},{anchor:"transformers.UperNetConfig.backbone",description:`<strong>backbone</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Name of backbone to use when <code>backbone_config</code> is <code>None</code>. If <code>use_pretrained_backbone</code> is <code>True</code>, this
will load the corresponding pretrained weights from the timm or transformers library. If <code>use_pretrained_backbone</code>
is <code>False</code>, this loads the backbone&#x2019;s config and uses that to initialize the backbone with random weights.`,name:"backbone"},{anchor:"transformers.UperNetConfig.use_pretrained_backbone",description:`<strong>use_pretrained_backbone</strong> (<code>bool</code>, <em>optional</em>, <code>False</code>) &#x2014;
Whether to use pretrained weights for the backbone.`,name:"use_pretrained_backbone"},{anchor:"transformers.UperNetConfig.use_timm_backbone",description:`<strong>use_timm_backbone</strong> (<code>bool</code>, <em>optional</em>, <code>False</code>) &#x2014;
Whether to load <code>backbone</code> from the timm library. If <code>False</code>, the backbone is loaded from the transformers
library.`,name:"use_timm_backbone"},{anchor:"transformers.UperNetConfig.backbone_kwargs",description:`<strong>backbone_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Keyword arguments to be passed to AutoBackbone when loading from a checkpoint
e.g. <code>{&apos;out_indices&apos;: (0, 1, 2, 3)}</code>. Cannot be specified if <code>backbone_config</code> is set.`,name:"backbone_kwargs"},{anchor:"transformers.UperNetConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The number of hidden units in the convolutional layers.`,name:"hidden_size"},{anchor:"transformers.UperNetConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.UperNetConfig.pool_scales",description:`<strong>pool_scales</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 3, 6]</code>) &#x2014;
Pooling scales used in Pooling Pyramid Module applied on the last feature map.`,name:"pool_scales"},{anchor:"transformers.UperNetConfig.use_auxiliary_head",description:`<strong>use_auxiliary_head</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an auxiliary head during training.`,name:"use_auxiliary_head"},{anchor:"transformers.UperNetConfig.auxiliary_loss_weight",description:`<strong>auxiliary_loss_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 0.4) &#x2014;
Weight of the cross-entropy loss of the auxiliary head.`,name:"auxiliary_loss_weight"},{anchor:"transformers.UperNetConfig.auxiliary_channels",description:`<strong>auxiliary_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Number of channels to use in the auxiliary head.`,name:"auxiliary_channels"},{anchor:"transformers.UperNetConfig.auxiliary_num_convs",description:`<strong>auxiliary_num_convs</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of convolutional layers to use in the auxiliary head.`,name:"auxiliary_num_convs"},{anchor:"transformers.UperNetConfig.auxiliary_concat_input",description:`<strong>auxiliary_concat_input</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to concatenate the output of the auxiliary head with the input before the classification layer.`,name:"auxiliary_concat_input"},{anchor:"transformers.UperNetConfig.loss_ignore_index",description:`<strong>loss_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to 255) &#x2014;
The index that is ignored by the loss function.`,name:"loss_ignore_index"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/upernet/configuration_upernet.py#L26"}}),k=new ot({props:{anchor:"transformers.UperNetConfig.example",$$slots:{default:[pt]},$$scope:{ctx:C}}}),D=new se({props:{title:"UperNetForSemanticSegmentation",local:"transformers.UperNetForSemanticSegmentation",headingTag:"h2"}}),K=new Ge({props:{name:"class transformers.UperNetForSemanticSegmentation",anchor:"transformers.UperNetForSemanticSegmentation",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.UperNetForSemanticSegmentation.This",description:"<strong>This</strong> model is a PyTorch [torch.nn.Module](https &#x2014;//pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use",name:"This"},{anchor:"transformers.UperNetForSemanticSegmentation.it",description:`<strong>it</strong> as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and &#x2014;
behavior. &#x2014;
config (<a href="/docs/transformers/main/en/model_doc/upernet#transformers.UperNetConfig">UperNetConfig</a>): Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"it"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/upernet/modeling_upernet.py#L343"}}),ee=new Ge({props:{name:"forward",anchor:"transformers.UperNetForSemanticSegmentation.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.UperNetForSemanticSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/segformer#transformers.SegformerFeatureExtractor.__call__">SegformerImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.UperNetForSemanticSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers in case the backbone has them. See
<code>attentions</code> under returned tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.UperNetForSemanticSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers of the backbone. See <code>hidden_states</code> under
returned tensors for more detail.`,name:"output_hidden_states"},{anchor:"transformers.UperNetForSemanticSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.UperNetForSemanticSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Ground truth semantic segmentation maps for computing the loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels &gt; 1</code>, a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/upernet/modeling_upernet.py#L360",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/upernet#transformers.UperNetConfig"
>UperNetConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels, logits_height, logits_width)</code>) â€” Classification scores for each pixel.</p>
<Tip warning={true}>
<p>The logits returned do not necessarily have the same size as the <code>pixel_values</code> passed as inputs. This is
to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the
original image size as post-processing. You should always check your logits shape and resize as needed.</p>
</Tip>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, patch_size, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),S=new ct({props:{$$slots:{default:[dt]},$$scope:{ctx:C}}}),F=new ot({props:{anchor:"transformers.UperNetForSemanticSegmentation.forward.example",$$slots:{default:[ft]},$$scope:{ctx:C}}}),{c(){i=l("meta"),v=s(),p=l("p"),c=s(),g(f.$$.fragment),a=s(),g(M.$$.fragment),le=s(),W=l("p"),W.innerHTML=Xe,me=s(),R=l("p"),R.textContent=Be,ce=s(),V=l("p"),V.innerHTML=Ie,pe=s(),Z=l("img"),de=s(),P=l("small"),P.innerHTML=Ee,fe=s(),G=l("p"),G.innerHTML=He,ge=s(),g(z.$$.fragment),ue=s(),X=l("p"),X.textContent=Ye,he=s(),g(B.$$.fragment),be=s(),I=l("p"),I.innerHTML=qe,_e=s(),g(Q.$$.fragment),ye=s(),E=l("p"),E.textContent=Le,Me=s(),g(H.$$.fragment),we=s(),Y=l("p"),Y.textContent=Ae,ve=s(),q=l("ul"),q.innerHTML=Oe,Ue=s(),L=l("p"),L.textContent=De,$e=s(),g(A.$$.fragment),xe=s(),U=l("div"),g(O.$$.fragment),ke=s(),te=l("p"),te.innerHTML=Ke,Se=s(),ne=l("p"),ne.innerHTML=et,Fe=s(),g(k.$$.fragment),Ne=s(),g(D.$$.fragment),Te=s(),x=l("div"),g(K.$$.fragment),Je=s(),oe=l("p"),oe.textContent=tt,We=s(),$=l("div"),g(ee.$$.fragment),Re=s(),ae=l("p"),ae.innerHTML=nt,Ve=s(),g(S.$$.fragment),Pe=s(),g(F.$$.fragment),je=s(),re=l("p"),this.h()},l(e){const t=mt("svelte-u9bgzb",document.head);i=m(t,"META",{name:!0,content:!0}),t.forEach(n),v=r(e),p=m(e,"P",{}),ie(p).forEach(n),c=r(e),u(f.$$.fragment,e),a=r(e),u(M.$$.fragment,e),le=r(e),W=m(e,"P",{"data-svelte-h":!0}),d(W)!=="svelte-1c4g37v"&&(W.innerHTML=Xe),me=r(e),R=m(e,"P",{"data-svelte-h":!0}),d(R)!=="svelte-vfdo9a"&&(R.textContent=Be),ce=r(e),V=m(e,"P",{"data-svelte-h":!0}),d(V)!=="svelte-1k7ta1y"&&(V.innerHTML=Ie),pe=r(e),Z=m(e,"IMG",{src:!0,alt:!0,width:!0}),de=r(e),P=m(e,"SMALL",{"data-svelte-h":!0}),d(P)!=="svelte-vh2z7s"&&(P.innerHTML=Ee),fe=r(e),G=m(e,"P",{"data-svelte-h":!0}),d(G)!=="svelte-1xqy3xr"&&(G.innerHTML=He),ge=r(e),u(z.$$.fragment,e),ue=r(e),X=m(e,"P",{"data-svelte-h":!0}),d(X)!=="svelte-gv2geu"&&(X.textContent=Ye),he=r(e),u(B.$$.fragment,e),be=r(e),I=m(e,"P",{"data-svelte-h":!0}),d(I)!=="svelte-1ffmd3f"&&(I.innerHTML=qe),_e=r(e),u(Q.$$.fragment,e),ye=r(e),E=m(e,"P",{"data-svelte-h":!0}),d(E)!=="svelte-a05sd1"&&(E.textContent=Le),Me=r(e),u(H.$$.fragment,e),we=r(e),Y=m(e,"P",{"data-svelte-h":!0}),d(Y)!=="svelte-108fh4w"&&(Y.textContent=Ae),ve=r(e),q=m(e,"UL",{"data-svelte-h":!0}),d(q)!=="svelte-17s05lc"&&(q.innerHTML=Oe),Ue=r(e),L=m(e,"P",{"data-svelte-h":!0}),d(L)!=="svelte-1xesile"&&(L.textContent=De),$e=r(e),u(A.$$.fragment,e),xe=r(e),U=m(e,"DIV",{class:!0});var N=ie(U);u(O.$$.fragment,N),ke=r(N),te=m(N,"P",{"data-svelte-h":!0}),d(te)!=="svelte-10jovpc"&&(te.innerHTML=Ke),Se=r(N),ne=m(N,"P",{"data-svelte-h":!0}),d(ne)!=="svelte-o55m63"&&(ne.innerHTML=et),Fe=r(N),u(k.$$.fragment,N),N.forEach(n),Ne=r(e),u(D.$$.fragment,e),Te=r(e),x=m(e,"DIV",{class:!0});var j=ie(x);u(K.$$.fragment,j),Je=r(j),oe=m(j,"P",{"data-svelte-h":!0}),d(oe)!=="svelte-ohcumq"&&(oe.textContent=tt),We=r(j),$=m(j,"DIV",{class:!0});var T=ie($);u(ee.$$.fragment,T),Re=r(T),ae=m(T,"P",{"data-svelte-h":!0}),d(ae)!=="svelte-1al9aea"&&(ae.innerHTML=nt),Ve=r(T),u(S.$$.fragment,T),Pe=r(T),u(F.$$.fragment,T),T.forEach(n),j.forEach(n),je=r(e),re=m(e,"P",{}),ie(re).forEach(n),this.h()},h(){J(i,"name","hf:doc:metadata"),J(i,"content",ut),st(Z.src,Qe="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/upernet_architecture.jpg")||J(Z,"src",Qe),J(Z,"alt","drawing"),J(Z,"width","600"),J(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){w(document.head,i),o(e,v,t),o(e,p,t),o(e,c,t),h(f,e,t),o(e,a,t),h(M,e,t),o(e,le,t),o(e,W,t),o(e,me,t),o(e,R,t),o(e,ce,t),o(e,V,t),o(e,pe,t),o(e,Z,t),o(e,de,t),o(e,P,t),o(e,fe,t),o(e,G,t),o(e,ge,t),h(z,e,t),o(e,ue,t),o(e,X,t),o(e,he,t),h(B,e,t),o(e,be,t),o(e,I,t),o(e,_e,t),h(Q,e,t),o(e,ye,t),o(e,E,t),o(e,Me,t),h(H,e,t),o(e,we,t),o(e,Y,t),o(e,ve,t),o(e,q,t),o(e,Ue,t),o(e,L,t),o(e,$e,t),h(A,e,t),o(e,xe,t),o(e,U,t),h(O,U,null),w(U,ke),w(U,te),w(U,Se),w(U,ne),w(U,Fe),h(k,U,null),o(e,Ne,t),h(D,e,t),o(e,Te,t),o(e,x,t),h(K,x,null),w(x,Je),w(x,oe),w(x,We),w(x,$),h(ee,$,null),w($,Re),w($,ae),w($,Ve),h(S,$,null),w($,Pe),h(F,$,null),o(e,je,t),o(e,re,t),Ce=!0},p(e,[t]){const N={};t&2&&(N.$$scope={dirty:t,ctx:e}),k.$set(N);const j={};t&2&&(j.$$scope={dirty:t,ctx:e}),S.$set(j);const T={};t&2&&(T.$$scope={dirty:t,ctx:e}),F.$set(T)},i(e){Ce||(b(f.$$.fragment,e),b(M.$$.fragment,e),b(z.$$.fragment,e),b(B.$$.fragment,e),b(Q.$$.fragment,e),b(H.$$.fragment,e),b(A.$$.fragment,e),b(O.$$.fragment,e),b(k.$$.fragment,e),b(D.$$.fragment,e),b(K.$$.fragment,e),b(ee.$$.fragment,e),b(S.$$.fragment,e),b(F.$$.fragment,e),Ce=!0)},o(e){_(f.$$.fragment,e),_(M.$$.fragment,e),_(z.$$.fragment,e),_(B.$$.fragment,e),_(Q.$$.fragment,e),_(H.$$.fragment,e),_(A.$$.fragment,e),_(O.$$.fragment,e),_(k.$$.fragment,e),_(D.$$.fragment,e),_(K.$$.fragment,e),_(ee.$$.fragment,e),_(S.$$.fragment,e),_(F.$$.fragment,e),Ce=!1},d(e){e&&(n(v),n(p),n(c),n(a),n(le),n(W),n(me),n(R),n(ce),n(V),n(pe),n(Z),n(de),n(P),n(fe),n(G),n(ge),n(ue),n(X),n(he),n(be),n(I),n(_e),n(ye),n(E),n(Me),n(we),n(Y),n(ve),n(q),n(Ue),n(L),n($e),n(xe),n(U),n(Ne),n(Te),n(x),n(je),n(re)),n(i),y(f,e),y(M,e),y(z,e),y(B,e),y(Q,e),y(H,e),y(A,e),y(O),y(k),y(D,e),y(K),y(ee),y(S),y(F)}}}const ut='{"title":"UPerNet","local":"upernet","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage examples","local":"usage-examples","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"UperNetConfig","local":"transformers.UperNetConfig","sections":[],"depth":2},{"title":"UperNetForSemanticSegmentation","local":"transformers.UperNetForSemanticSegmentation","sections":[],"depth":2}],"depth":1}';function ht(C){return rt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $t extends it{constructor(i){super(),lt(this,i,ht,gt,at,{})}}export{$t as component};
