import{s as Yn,o as Qn,n as U}from"../chunks/scheduler.9bc65507.js";import{S as Kn,i as er,g as d,s,r as f,A as or,h as c,f as a,c as i,j as x,u as g,x as b,k as y,y as n,a as p,v as h,d as u,t as _,w as v}from"../chunks/index.707bf1b6.js";import{T as Co}from"../chunks/Tip.c2ecdbf4.js";import{D as $}from"../chunks/Docstring.17db21ae.js";import{C as ze}from"../chunks/CodeBlock.54a9f38d.js";import{E as we}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as z}from"../chunks/Heading.342b1fa6.js";function tr(F){let o,k="Example:",l,m,T;return m=new ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEZsYXZhQ29uZmlnJTJDJTIwRmxhdmFNb2RlbCUyQyUyMEZsYXZhRm9yUHJlVHJhaW5pbmclMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwRmxhdmFDb25maWclMjB3aXRoJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEZsYXZhQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwRmxhdmFNb2RlbCUyMGFuZCUyMEZsYXZhRm9yUHJlVHJhaW5pbmclMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBGbGF2YU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBbW9kZWxfcHJlJTIwJTNEJTIwRmxhdmFGb3JQcmVUcmFpbmluZyhjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWclMEFjb25maWd1cmF0aW9uX3ByZSUyMCUzRCUyMG1vZGVsX3ByZS5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaConfig, FlavaModel, FlavaForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaConfig with style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaModel and FlavaForPreTraining model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel(configuration)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_pre = FlavaForPreTraining(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration_pre = model_pre.config`,wrap:!1}}),{c(){o=d("p"),o.textContent=k,l=s(),f(m.$$.fragment)},l(t){o=c(t,"P",{"data-svelte-h":!0}),b(o)!=="svelte-11lpom8"&&(o.textContent=k),l=i(t),g(m.$$.fragment,t)},m(t,M){p(t,o,M),p(t,l,M),h(m,t,M),T=!0},p:U,i(t){T||(u(m.$$.fragment,t),T=!0)},o(t){_(m.$$.fragment,t),T=!1},d(t){t&&(a(o),a(l)),v(m,t)}}}function ar(F){let o,k="Example:",l,m,T;return m=new ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEZsYXZhVGV4dENvbmZpZyUyQyUyMEZsYXZhVGV4dE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEZsYXZhVGV4dE1vZGVsJTIwd2l0aCUyMCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBGbGF2YVRleHRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBGbGF2YVRleHRNb2RlbCUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEZsYXZhVGV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaTextConfig, FlavaTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaTextModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaTextModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=d("p"),o.textContent=k,l=s(),f(m.$$.fragment)},l(t){o=c(t,"P",{"data-svelte-h":!0}),b(o)!=="svelte-11lpom8"&&(o.textContent=k),l=i(t),g(m.$$.fragment,t)},m(t,M){p(t,o,M),p(t,l,M),h(m,t,M),T=!0},p:U,i(t){T||(u(m.$$.fragment,t),T=!0)},o(t){_(m.$$.fragment,t),T=!1},d(t){t&&(a(o),a(l)),v(m,t)}}}function nr(F){let o,k="Example:",l,m,T;return m=new ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEZsYXZhSW1hZ2VDb25maWclMkMlMjBGbGF2YUltYWdlTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwRmxhdmFJbWFnZU1vZGVsJTIwd2l0aCUyMCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBGbGF2YUltYWdlQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwRmxhdmFJbWFnZU1vZGVsJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwRmxhdmFJbWFnZU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaImageConfig, FlavaImageModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaImageModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaImageConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaImageModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaImageModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=d("p"),o.textContent=k,l=s(),f(m.$$.fragment)},l(t){o=c(t,"P",{"data-svelte-h":!0}),b(o)!=="svelte-11lpom8"&&(o.textContent=k),l=i(t),g(m.$$.fragment,t)},m(t,M){p(t,o,M),p(t,l,M),h(m,t,M),T=!0},p:U,i(t){T||(u(m.$$.fragment,t),T=!0)},o(t){_(m.$$.fragment,t),T=!1},d(t){t&&(a(o),a(l)),v(m,t)}}}function rr(F){let o,k="Example:",l,m,T;return m=new ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEZsYXZhTXVsdGltb2RhbENvbmZpZyUyQyUyMEZsYXZhTXVsdGltb2RhbE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEZsYXZhTXVsdGltb2RhbE1vZGVsJTIwd2l0aCUyMCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBGbGF2YU11bHRpbW9kYWxDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBGbGF2YU11bHRpbW9kYWxNb2RlbCUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEZsYXZhTXVsdGltb2RhbE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> FlavaMultimodalConfig, FlavaMultimodalModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaMultimodalModel with  style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = FlavaMultimodalConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a FlavaMultimodalModel model (with random weights) from the style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaMultimodalModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=d("p"),o.textContent=k,l=s(),f(m.$$.fragment)},l(t){o=c(t,"P",{"data-svelte-h":!0}),b(o)!=="svelte-11lpom8"&&(o.textContent=k),l=i(t),g(m.$$.fragment,t)},m(t,M){p(t,o,M),p(t,l,M),h(m,t,M),T=!0},p:U,i(t){T||(u(m.$$.fragment,t),T=!0)},o(t){_(m.$$.fragment,t),T=!1},d(t){t&&(a(o),a(l)),v(m,t)}}}function sr(F){let o,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=k},l(l){o=c(l,"P",{"data-svelte-h":!0}),b(o)!=="svelte-fincs2"&&(o.innerHTML=k)},m(l,m){p(l,o,m)},p:U,d(l){l&&a(o)}}}function ir(F){let o,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=k},l(l){o=c(l,"P",{"data-svelte-h":!0}),b(o)!=="svelte-fincs2"&&(o.innerHTML=k)},m(l,m){p(l,o,m)},p:U,d(l){l&&a(o)}}}function lr(F){let o,k="Examples:",l,m,T;return m=new ze({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEZsYXZhTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEZsYXZhTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZmxhdmEtZnVsbCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmZsYXZhLWZ1bGwlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTVEJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcGFkZGluZyUzRFRydWUpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWltYWdlX2VtYmVkZGluZ3MlMjAlM0QlMjBvdXRwdXRzLmltYWdlX2VtYmVkZGluZ3MlMEF0ZXh0X2VtYmVkZGluZ3MlMjAlM0QlMjBvdXRwdXRzLnRleHRfZW1iZWRkaW5ncyUwQW11bHRpbW9kYWxfZW1iZWRkaW5ncyUyMCUzRCUyMG91dHB1dHMubXVsdGltb2RhbF9lbWJlZGRpbmdzJTBBJTBBb3V0cHV0cy5pbWFnZV9lbWJlZGRpbmdzLnNoYXBlJTBBJTBBdGV4dF9lbWJlZGRpbmdzLnNoYXBlJTBBJTBBbXVsdGltb2RhbF9lbWJlZGRpbmdzLnNoYXBl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, FlavaModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_embeddings = outputs.image_embeddings
<span class="hljs-meta">&gt;&gt;&gt; </span>text_embeddings = outputs.text_embeddings
<span class="hljs-meta">&gt;&gt;&gt; </span>multimodal_embeddings = outputs.multimodal_embeddings

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs.image_embeddings.shape
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>text_embeddings.shape
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">7</span>, <span class="hljs-number">768</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>multimodal_embeddings.shape
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">205</span>, <span class="hljs-number">768</span>])`,wrap:!1}}),{c(){o=d("p"),o.textContent=k,l=s(),f(m.$$.fragment)},l(t){o=c(t,"P",{"data-svelte-h":!0}),b(o)!=="svelte-kvfsh7"&&(o.textContent=k),l=i(t),g(m.$$.fragment,t)},m(t,M){p(t,o,M),p(t,l,M),h(m,t,M),T=!0},p:U,i(t){T||(u(m.$$.fragment,t),T=!0)},o(t){_(m.$$.fragment,t),T=!1},d(t){t&&(a(o),a(l)),v(m,t)}}}function dr(F){let o,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=k},l(l){o=c(l,"P",{"data-svelte-h":!0}),b(o)!=="svelte-fincs2"&&(o.innerHTML=k)},m(l,m){p(l,o,m)},p:U,d(l){l&&a(o)}}}function cr(F){let o,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=k},l(l){o=c(l,"P",{"data-svelte-h":!0}),b(o)!=="svelte-fincs2"&&(o.innerHTML=k)},m(l,m){p(l,o,m)},p:U,d(l){l&&a(o)}}}function mr(F){let o,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=k},l(l){o=c(l,"P",{"data-svelte-h":!0}),b(o)!=="svelte-fincs2"&&(o.innerHTML=k)},m(l,m){p(l,o,m)},p:U,d(l){l&&a(o)}}}function pr(F){let o,k="Example:",l,m,T;return m=new ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBGbGF2YVRleHRNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZmbGF2YS1mdWxsJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxhdmFUZXh0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZmxhdmEtZnVsbCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySGVsbG8lMkMlMjBteSUyMGRvZyUyMGlzJTIwY3V0ZSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGU=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlavaTextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaTextModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){o=d("p"),o.textContent=k,l=s(),f(m.$$.fragment)},l(t){o=c(t,"P",{"data-svelte-h":!0}),b(o)!=="svelte-11lpom8"&&(o.textContent=k),l=i(t),g(m.$$.fragment,t)},m(t,M){p(t,o,M),p(t,l,M),h(m,t,M),T=!0},p:U,i(t){T||(u(m.$$.fragment,t),T=!0)},o(t){_(m.$$.fragment,t),T=!1},d(t){t&&(a(o),a(l)),v(m,t)}}}function fr(F){let o,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=k},l(l){o=c(l,"P",{"data-svelte-h":!0}),b(o)!=="svelte-fincs2"&&(o.innerHTML=k)},m(l,m){p(l,o,m)},p:U,d(l){l&&a(o)}}}function gr(F){let o,k="Example:",l,m,T;return m=new ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEZsYXZhSW1hZ2VNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZmbGF2YS1mdWxsJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxhdmFJbWFnZU1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmZsYXZhLWZ1bGwlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, FlavaImageModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaImageModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>]`,wrap:!1}}),{c(){o=d("p"),o.textContent=k,l=s(),f(m.$$.fragment)},l(t){o=c(t,"P",{"data-svelte-h":!0}),b(o)!=="svelte-11lpom8"&&(o.textContent=k),l=i(t),g(m.$$.fragment,t)},m(t,M){p(t,o,M),p(t,l,M),h(m,t,M),T=!0},p:U,i(t){T||(u(m.$$.fragment,t),T=!0)},o(t){_(m.$$.fragment,t),T=!1},d(t){t&&(a(o),a(l)),v(m,t)}}}function hr(F){let o,k=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=d("p"),o.innerHTML=k},l(l){o=c(l,"P",{"data-svelte-h":!0}),b(o)!=="svelte-fincs2"&&(o.innerHTML=k)},m(l,m){p(l,o,m)},p:U,d(l){l&&a(o)}}}function ur(F){let o,k="Example:",l,m,T;return m=new ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBGbGF2YU11bHRpbW9kYWxNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZmbGF2YS1mdWxsJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxhdmFNdWx0aW1vZGFsTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZmxhdmEtZnVsbCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIySGVsbG8lMkMlMjBteSUyMGRvZyUyMGlzJTIwY3V0ZSUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGU=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlavaMultimodalModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlavaMultimodalModel.from_pretrained(<span class="hljs-string">&quot;facebook/flava-full&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){o=d("p"),o.textContent=k,l=s(),f(m.$$.fragment)},l(t){o=c(t,"P",{"data-svelte-h":!0}),b(o)!=="svelte-11lpom8"&&(o.textContent=k),l=i(t),g(m.$$.fragment,t)},m(t,M){p(t,o,M),p(t,l,M),h(m,t,M),T=!0},p:U,i(t){T||(u(m.$$.fragment,t),T=!0)},o(t){_(m.$$.fragment,t),T=!1},d(t){t&&(a(o),a(l)),v(m,t)}}}function _r(F){let o,k,l,m,T,t,M,kt,Ce,cn='The FLAVA model was proposed in <a href="https://arxiv.org/abs/2112.04482" rel="nofollow">FLAVA: A Foundational Language And Vision Alignment Model</a> by Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela and is accepted at CVPR 2022.',Tt,Ie,mn=`The paper aims at creating a single unified foundation model which can work across vision, language
as well as vision-and-language multimodal tasks.`,Mt,Pe,pn="The abstract from the paper is the following:",Ft,We,fn=`<em>State-of-the-art vision and vision-and-language models rely on large-scale visio-linguistic pretraining for obtaining good performance on a variety
of downstream tasks. Generally, such models are often either cross-modal (contrastive) or multi-modal
(with earlier fusion) but not both; and they often only target specific modalities or tasks. A promising
direction would be to use a single holistic universal model, as a “foundation”, that targets all modalities
at once — a true vision and language foundation model should be good at vision tasks, language tasks, and
cross- and multi-modal vision and language tasks. We introduce FLAVA as such a model and demonstrate
impressive performance on a wide range of 35 tasks spanning these target modalities.</em>`,xt,je,gn='This model was contributed by <a href="https://huggingface.co/aps" rel="nofollow">aps</a>. The original code can be found <a href="https://github.com/facebookresearch/multimodal/tree/main/examples/flava" rel="nofollow">here</a>.',yt,Ze,$t,C,Ue,oa,Io,hn=`<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a> is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel">FlavaModel</a>. It is used to
instantiate FLAVA model according to the specified arguments, defining the text model, image model, image codebook
and multimodal model configs. Instantiating a configuration with the defaults will yield a similar configuration to
that of the FLAVA <a href="https://huggingface.co/facebook/flava-full" rel="nofollow">facebook/flava-full</a> architecture.`,ta,Po,un=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,aa,ne,na,re,Je,ra,Wo,_n=`Instantiate a <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a> (or a derived class) from flava text model configuration, flava image model
configuration, flava multimodal model and flava codebook model configuration.`,wt,Ne,zt,I,Le,sa,jo,vn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`,ia,Zo,bn=`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
<a href="https://huggingface.co/facebook/flava-full" rel="nofollow">facebook/flava-full</a> architecture.`,la,Uo,kn=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,da,se,Ct,Ve,It,P,Be,ca,Jo,Tn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel">FlavaImageModel</a>. It is used to instantiate an
FLAVA model according to the specified arguments, defining the model architecture.`,ma,No,Mn=`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
<a href="https://huggingface.co/facebook/flava-full" rel="nofollow">facebook/flava-full</a> architecture.`,pa,Lo,Fn=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,fa,ie,Pt,Re,Wt,W,Ge,ga,Vo,xn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel">FlavaMultimodalModel</a>. It is used to instantiate
an FLAVA model according to the specified arguments, defining the model architecture.`,ha,Bo,yn=`Instantiating a configuration with the defaults will yield a similar configuration to that of the FLAVA
<a href="https://huggingface.co/facebook/flava-full" rel="nofollow">facebook/flava-full</a> architecture.`,ua,Ro,$n=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,_a,le,jt,qe,Zt,Ee,He,Ut,Ae,Jt,j,Oe,va,Go,wn="Constructs a FLAVA processor which wraps a FLAVA image processor and a FLAVA tokenizer into a single processor.",ba,qo,zn=`<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaProcessor">FlavaProcessor</a> offers all the functionalities of <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a> and <a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaProcessor.decode">decode()</a> for more information.`,ka,de,Se,Ta,Eo,Cn=`This method forwards all its arguments to BertTokenizerFast’s <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.batch_decode">batch_decode()</a>. Please
refer to the docstring of this method for more information.`,Ma,ce,De,Fa,Ho,In=`This method forwards all its arguments to BertTokenizerFast’s <a href="/docs/transformers/main/en/model_doc/speecht5#transformers.SpeechT5Tokenizer.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,Nt,Xe,Lt,Ye,Qe,Vt,Ke,Bt,S,eo,xa,Ao,Pn="Constructs a Flava image processor.",ya,me,oo,$a,Oo,Wn="Preprocess an image or batch of images.",Rt,to,Gt,J,ao,wa,So,jn="The FLAVA model for pretraining which outputs losses, embeddings, logits and transformer outputs.",za,Do,Zn=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Ca,K,no,Ia,Xo,Un='The <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaForPreTraining">FlavaForPreTraining</a> forward method, overrides the <code>__call__</code> special method.',Pa,pe,qt,ro,Et,Z,so,Wa,Yo,Jn=`The bare FLAVA Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,ja,q,io,Za,Qo,Nn='The <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel">FlavaModel</a> forward method, overrides the <code>__call__</code> special method.',Ua,fe,Ja,ge,Na,ee,lo,La,Ko,Ln='The <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel">FlavaModel</a> forward method, overrides the <code>__call__</code> special method.',Va,he,Ba,oe,co,Ra,et,Vn='The <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaModel">FlavaModel</a> forward method, overrides the <code>__call__</code> special method.',Ga,ue,Ht,mo,At,w,po,qa,ot,Bn=`The FLAVA’s image codebook model inspired from DALL-E’s original encoder. Outputs raw hidden states and can be used
to generate image tokens for an image based on DALL-E’s vocab. Used to generate labels for MIM. Use
<code>get_codebook_indices</code> to get image tokens for an image.`,Ea,tt,Rn=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Ha,at,fo,Aa,nt,go,Oa,rt,ho,Ot,uo,St,D,_o,Sa,st,Gn=`The bare FLAVA Text Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Da,E,vo,Xa,it,qn='The <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a> forward method, overrides the <code>__call__</code> special method.',Ya,_e,Qa,ve,Dt,bo,Xt,X,ko,Ka,lt,En=`The bare FLAVA Image Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,en,H,To,on,dt,Hn='The <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel">FlavaImageModel</a> forward method, overrides the <code>__call__</code> special method.',tn,be,an,ke,Yt,Mo,Qt,Y,Fo,nn,ct,An=`The bare FLAVA Multimodal Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,rn,A,xo,sn,mt,On='The <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel">FlavaMultimodalModel</a> forward method, overrides the <code>__call__</code> special method.',ln,Te,dn,Me,Kt,_t,ea;return T=new z({props:{title:"FLAVA",local:"flava",headingTag:"h1"}}),M=new z({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Ze=new z({props:{title:"FlavaConfig",local:"transformers.FlavaConfig",headingTag:"h2"}}),Ue=new $({props:{name:"class transformers.FlavaConfig",anchor:"transformers.FlavaConfig",parameters:[{name:"image_config",val:": Dict = None"},{name:"text_config",val:": Dict = None"},{name:"multimodal_config",val:": Dict = None"},{name:"image_codebook_config",val:": Dict = None"},{name:"hidden_size",val:": int = 768"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"projection_dim",val:": int = 768"},{name:"init_codebook",val:": bool = True"},{name:"logit_scale_init_value",val:": float = 2.6592"},{name:"initializer_range",val:": float = 0.02"},{name:"ce_ignore_index",val:": int = -100"},{name:"mim_weight",val:": float = 1.0"},{name:"mlm_weight",val:": float = 1.0"},{name:"global_contrastive_weight",val:": float = 1.0"},{name:"itm_weight",val:": float = 1.0"},{name:"mmm_image_weight",val:": float = 1.0"},{name:"mmm_text_weight",val:": float = 1.0"},{name:"global_backprop_contrastive",val:": bool = True"},{name:"skip_unmasked_multimodal_encoder",val:": bool = True"},{name:"return_loss",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextConfig">FlavaTextConfig</a>.`,name:"text_config"},{anchor:"transformers.FlavaConfig.image_config",description:`<strong>image_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageConfig">FlavaImageConfig</a>.`,name:"image_config"},{anchor:"transformers.FlavaConfig.multimodal_config",description:`<strong>multimodal_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalConfig">FlavaMultimodalConfig</a>.`,name:"multimodal_config"},{anchor:"transformers.FlavaConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and image projection layers.`,name:"projection_dim"},{anchor:"transformers.FlavaConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original FLAVA/CLIP
implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.FlavaConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaConfig.ce_ignore_index",description:`<strong>ce_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to -100) &#x2014;
Cross entropy index to ignore.`,name:"ce_ignore_index"},{anchor:"transformers.FlavaConfig.mim_weight",description:`<strong>mim_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MIM (Masked Image Modeling) unimodal loss`,name:"mim_weight"},{anchor:"transformers.FlavaConfig.mlm_weight",description:`<strong>mlm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MLM (Masked Language Modeling) unimodal loss`,name:"mlm_weight"},{anchor:"transformers.FlavaConfig.global_contrastive_weight",description:`<strong>global_contrastive_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to global contrastive cross-alignment loss.`,name:"global_contrastive_weight"},{anchor:"transformers.FlavaConfig.itm_weight",description:`<strong>itm_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to image-text matching multimodal loss.`,name:"itm_weight"},{anchor:"transformers.FlavaConfig.mmm_image_weight",description:`<strong>mmm_image_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s image part.`,name:"mmm_image_weight"},{anchor:"transformers.FlavaConfig.mmm_text_weight",description:`<strong>mmm_text_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Weight to be assigned to MMM loss&#x2019;s text part.`,name:"mmm_text_weight"},{anchor:"transformers.FlavaConfig.global_backprop_contrastive",description:`<strong>global_backprop_contrastive</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use global backpropgation through all workers in contrastive loss.`,name:"global_backprop_contrastive"},{anchor:"transformers.FlavaConfig.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to skip running unmasked multimodal encoder whose outputs are not used by FLAVA losses.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FlavaConfig.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to return loss or not`,name:"return_loss"},{anchor:"transformers.FlavaConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L468"}}),ne=new we({props:{anchor:"transformers.FlavaConfig.example",$$slots:{default:[tr]},$$scope:{ctx:F}}}),Je=new $({props:{name:"from_configs",anchor:"transformers.FlavaConfig.from_configs",parameters:[{name:"image_config",val:": FlavaImageConfig"},{name:"text_config",val:": FlavaTextConfig"},{name:"multimodal_config",val:": FlavaMultimodalConfig"},{name:"image_codebook_config",val:": FlavaImageCodebookConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L742",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig"
>FlavaConfig</a></p>
`}}),Ne=new z({props:{title:"FlavaTextConfig",local:"transformers.FlavaTextConfig",headingTag:"h2"}}),Le=new $({props:{name:"class transformers.FlavaTextConfig",anchor:"transformers.FlavaTextConfig",parameters:[{name:"vocab_size",val:": int = 30522"},{name:"type_vocab_size",val:": int = 2"},{name:"max_position_embeddings",val:": int = 512"},{name:"position_embedding_type",val:": str = 'absolute'"},{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 12"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": str = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"pad_token_id",val:": int = 0"},{name:"qkv_bias",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the BERT model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.FlavaTextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel">FlavaTextModel</a>. Note that even though
text encoder allows <code>token_type_ids</code>&#x2019;s value as 2, for text-only pretraining and fine-tuning, only 1 is
used similar to RoBERTa.`,name:"type_vocab_size"},{anchor:"transformers.FlavaTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048). For VL, max_length passed to model is 77.`,name:"max_position_embeddings"},{anchor:"transformers.FlavaTextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.FlavaTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaTextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaTextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaTextConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FlavaTextConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FlavaTextConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FlavaTextConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L150"}}),se=new we({props:{anchor:"transformers.FlavaTextConfig.example",$$slots:{default:[ar]},$$scope:{ctx:F}}}),Ve=new z({props:{title:"FlavaImageConfig",local:"transformers.FlavaImageConfig",headingTag:"h2"}}),Be=new $({props:{name:"class transformers.FlavaImageConfig",anchor:"transformers.FlavaImageConfig",parameters:[{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 12"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": int = 'gelu'"},{name:"hidden_dropout_prob",val:": float = 0.0"},{name:"attention_probs_dropout_prob",val:": float = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"image_size",val:": int = 224"},{name:"patch_size",val:": int = 16"},{name:"num_channels",val:": int = 3"},{name:"qkv_bias",val:": bool = True"},{name:"mask_token",val:": bool = True"},{name:"vocab_size",val:": int = 8192"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaImageConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaImageConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaImageConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaImageConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaImageConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaImageConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaImageConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaImageConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaImageConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.FlavaImageConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.FlavaImageConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.FlavaImageConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FlavaImageConfig.mask_token",description:`<strong>mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use a mask token or not. Used in MIM (Masked Image Modeling) loss for FLAVA.`,name:"mask_token"},{anchor:"transformers.FlavaImageConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a> used in conjunction with <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel">FlavaImageModel</a> for MIM (Masked
Image Modeling) loss for FLAVA.`,name:"vocab_size"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L31"}}),ie=new we({props:{anchor:"transformers.FlavaImageConfig.example",$$slots:{default:[nr]},$$scope:{ctx:F}}}),Re=new z({props:{title:"FlavaMultimodalConfig",local:"transformers.FlavaMultimodalConfig",headingTag:"h2"}}),Ge=new $({props:{name:"class transformers.FlavaMultimodalConfig",anchor:"transformers.FlavaMultimodalConfig",parameters:[{name:"hidden_size",val:": int = 768"},{name:"num_hidden_layers",val:": int = 6"},{name:"num_attention_heads",val:": int = 12"},{name:"intermediate_size",val:": int = 3072"},{name:"hidden_act",val:": int = 'gelu'"},{name:"hidden_dropout_prob",val:": int = 0.0"},{name:"attention_probs_dropout_prob",val:": int = 0.0"},{name:"initializer_range",val:": float = 0.02"},{name:"layer_norm_eps",val:": float = 1e-12"},{name:"qkv_bias",val:": bool = True"},{name:"use_cls_token",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaMultimodalConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.FlavaMultimodalConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.FlavaMultimodalConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.FlavaMultimodalConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.FlavaMultimodalConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.FlavaMultimodalConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.FlavaMultimodalConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.FlavaMultimodalConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.FlavaMultimodalConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.FlavaMultimodalConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.FlavaMultimodalConfig.use_cls_token",description:`<strong>use_cls_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an extra CLS token for multimodal settings. Usually needed by the FLAVA model.`,name:"use_cls_token"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L280"}}),le=new we({props:{anchor:"transformers.FlavaMultimodalConfig.example",$$slots:{default:[rr]},$$scope:{ctx:F}}}),qe=new z({props:{title:"FlavaImageCodebookConfig",local:"transformers.FlavaImageCodebookConfig",headingTag:"h2"}}),He=new $({props:{name:"class transformers.FlavaImageCodebookConfig",anchor:"transformers.FlavaImageCodebookConfig",parameters:[{name:"num_groups",val:": int = 4"},{name:"input_channels",val:": int = 3"},{name:"num_blocks_per_group",val:": int = 2"},{name:"hidden_size",val:": int = 256"},{name:"vocab_size",val:": int = 8192"},{name:"freeze",val:": int = True"},{name:"initializer_range",val:": float = 0.02"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/configuration_flava.py#L383"}}),Ae=new z({props:{title:"FlavaProcessor",local:"transformers.FlavaProcessor",headingTag:"h2"}}),Oe=new $({props:{name:"class transformers.FlavaProcessor",anchor:"transformers.FlavaProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaProcessor.image_processor",description:'<strong>image_processor</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageProcessor">FlavaImageProcessor</a>, <em>optional</em>) &#x2014; The image processor is a required input.',name:"image_processor"},{anchor:"transformers.FlavaProcessor.tokenizer",description:'<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a>, <em>optional</em>) &#x2014; The tokenizer is a required input.',name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/processing_flava.py#L28"}}),Se=new $({props:{name:"batch_decode",anchor:"transformers.FlavaProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/processing_flava.py#L131"}}),De=new $({props:{name:"decode",anchor:"transformers.FlavaProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/processing_flava.py#L138"}}),Xe=new z({props:{title:"FlavaFeatureExtractor",local:"transformers.FlavaFeatureExtractor",headingTag:"h2"}}),Qe=new $({props:{name:"class transformers.FlavaFeatureExtractor",anchor:"transformers.FlavaFeatureExtractor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/feature_extraction_flava.py#L26"}}),Ke=new z({props:{title:"FlavaImageProcessor",local:"transformers.FlavaImageProcessor",headingTag:"h2"}}),eo=new $({props:{name:"class transformers.FlavaImageProcessor",anchor:"transformers.FlavaImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": Dict = None"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": Union = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"return_image_mask",val:": bool = False"},{name:"input_size_patches",val:": int = 14"},{name:"total_mask_patches",val:": int = 75"},{name:"mask_group_min_patches",val:": int = 16"},{name:"mask_group_max_patches",val:": Optional = None"},{name:"mask_group_min_aspect_ratio",val:": float = 0.3"},{name:"mask_group_max_aspect_ratio",val:": Optional = None"},{name:"return_codebook_pixels",val:": bool = False"},{name:"codebook_do_resize",val:": bool = True"},{name:"codebook_size",val:": bool = None"},{name:"codebook_resample",val:": int = <Resampling.LANCZOS: 1>"},{name:"codebook_do_center_crop",val:": bool = True"},{name:"codebook_crop_size",val:": int = None"},{name:"codebook_do_rescale",val:": bool = True"},{name:"codebook_rescale_factor",val:": Union = 0.00392156862745098"},{name:"codebook_do_map_pixels",val:": bool = True"},{name:"codebook_do_normalize",val:": bool = True"},{name:"codebook_image_mean",val:": Union = None"},{name:"codebook_image_std",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in <code>preprocess</code>.`,name:"do_resize"},{anchor:"transformers.FlavaImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Size of the image after resizing. Can be overridden by the <code>size</code> parameter in <code>preprocess</code>.`,name:"size"},{anchor:"transformers.FlavaImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in
<code>preprocess</code>.`,name:"resample"},{anchor:"transformers.FlavaImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the images. Can be overridden by the <code>do_center_crop</code> parameter in <code>preprocess</code>.`,name:"do_center_crop"},{anchor:"transformers.FlavaImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Size of image after the center crop <code>(crop_size[&quot;height&quot;], crop_size[&quot;width&quot;])</code>. Can be overridden by the
<code>crop_size</code> parameter in <code>preprocess</code>.`,name:"crop_size"},{anchor:"transformers.FlavaImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in <code>preprocess</code>.`,name:"do_rescale"},{anchor:"transformers.FlavaImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in
<code>preprocess</code>.`,name:"rescale_factor"},{anchor:"transformers.FlavaImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in <code>preprocess</code>.`,name:"do_normalize"},{anchor:"transformers.FlavaImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.FlavaImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.FlavaImageProcessor.return_image_mask",description:`<strong>return_image_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to return the image mask. Can be overridden by the <code>return_image_mask</code> parameter in <code>preprocess</code>.`,name:"return_image_mask"},{anchor:"transformers.FlavaImageProcessor.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 14) &#x2014;
Number of patches in the image in height and width direction. 14x14 = 196 total patches. Can be overridden
by the <code>input_size_patches</code> parameter in <code>preprocess</code>.`,name:"input_size_patches"},{anchor:"transformers.FlavaImageProcessor.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 75) &#x2014;
Total number of patches that should be masked. Can be overridden by the <code>total_mask_patches</code> parameter in
<code>preprocess</code>.`,name:"total_mask_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Minimum number of patches that should be masked. Can be overridden by the <code>mask_group_min_patches</code>
parameter in <code>preprocess</code>.`,name:"mask_group_min_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Maximum number of patches that should be masked. Can be overridden by the <code>mask_group_max_patches</code>
parameter in <code>preprocess</code>.`,name:"mask_group_max_patches"},{anchor:"transformers.FlavaImageProcessor.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0.3) &#x2014;
Minimum aspect ratio of the mask window. Can be overridden by the <code>mask_group_min_aspect_ratio</code> parameter
in <code>preprocess</code>.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Maximum aspect ratio of the mask window. Can be overridden by the <code>mask_group_max_aspect_ratio</code> parameter
in <code>preprocess</code>.`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the input for codebook to a certain. Can be overridden by the <code>codebook_do_resize</code>
parameter in <code>preprocess</code>. <code>codebook_size</code>.`,name:"codebook_do_resize"},{anchor:"transformers.FlavaImageProcessor.codebook_size",description:`<strong>codebook_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Resize the input for codebook to the given size. Can be overridden by the <code>codebook_size</code> parameter in
<code>preprocess</code>.`,name:"codebook_size"},{anchor:"transformers.FlavaImageProcessor.codebook_resample",description:`<strong>codebook_resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>PILImageResampling.LANCZOS</code>) &#x2014;
Resampling filter to use if resizing the codebook image. Can be overridden by the <code>codebook_resample</code>
parameter in <code>preprocess</code>.`,name:"codebook_resample"},{anchor:"transformers.FlavaImageProcessor.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to crop the input for codebook at the center. If the input size is smaller than
<code>codebook_crop_size</code> along any edge, the image is padded with 0&#x2019;s and then center cropped. Can be
overridden by the <code>codebook_do_center_crop</code> parameter in <code>preprocess</code>.`,name:"codebook_do_center_crop"},{anchor:"transformers.FlavaImageProcessor.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Desired output size for codebook input when applying center-cropping. Can be overridden by the
<code>codebook_crop_size</code> parameter in <code>preprocess</code>.`,name:"codebook_crop_size"},{anchor:"transformers.FlavaImageProcessor.codebook_do_rescale",description:`<strong>codebook_do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the input for codebook by the specified scale <code>codebook_rescale_factor</code>. Can be
overridden by the <code>codebook_do_rescale</code> parameter in <code>preprocess</code>.`,name:"codebook_do_rescale"},{anchor:"transformers.FlavaImageProcessor.codebook_rescale_factor",description:`<strong>codebook_rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Defines the scale factor to use if rescaling the codebook image. Can be overridden by the
<code>codebook_rescale_factor</code> parameter in <code>preprocess</code>.`,name:"codebook_rescale_factor"},{anchor:"transformers.FlavaImageProcessor.codebook_do_map_pixels",description:`<strong>codebook_do_map_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to map the pixel values of the codebook input to (1 - 2e)x + e. Can be overridden by the
<code>codebook_do_map_pixels</code> parameter in <code>preprocess</code>.`,name:"codebook_do_map_pixels"},{anchor:"transformers.FlavaImageProcessor.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the input for codebook with <code>codebook_image_mean</code> and <code>codebook_image_std</code>. Can
be overridden by the <code>codebook_do_normalize</code> parameter in <code>preprocess</code>.`,name:"codebook_do_normalize"},{anchor:"transformers.FlavaImageProcessor.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>Optional[Union[float, Iterable[float]]]</code>, <em>optional</em>, defaults to <code>[0, 0, 0]</code>) &#x2014;
The sequence of means for each channel, to be used when normalizing images for codebook. Can be overridden
by the <code>codebook_image_mean</code> parameter in <code>preprocess</code>.`,name:"codebook_image_mean"},{anchor:"transformers.FlavaImageProcessor.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>Optional[Union[float, Iterable[float]]]</code>, <em>optional</em>, defaults to <code>[0.5, 0.5, 0.5]</code>) &#x2014;
The sequence of standard deviations for each channel, to be used when normalizing images for codebook. Can
be overridden by the <code>codebook_image_std</code> parameter in <code>preprocess</code>.`,name:"codebook_image_std"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L135"}}),oo=new $({props:{name:"preprocess",anchor:"transformers.FlavaImageProcessor.preprocess",parameters:[{name:"images",val:": Union"},{name:"do_resize",val:": Optional = None"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = None"},{name:"do_center_crop",val:": Optional = None"},{name:"crop_size",val:": Optional = None"},{name:"do_rescale",val:": Optional = None"},{name:"rescale_factor",val:": Optional = None"},{name:"do_normalize",val:": Optional = None"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"return_image_mask",val:": Optional = None"},{name:"input_size_patches",val:": Optional = None"},{name:"total_mask_patches",val:": Optional = None"},{name:"mask_group_min_patches",val:": Optional = None"},{name:"mask_group_max_patches",val:": Optional = None"},{name:"mask_group_min_aspect_ratio",val:": Optional = None"},{name:"mask_group_max_aspect_ratio",val:": Optional = None"},{name:"return_codebook_pixels",val:": Optional = None"},{name:"codebook_do_resize",val:": Optional = None"},{name:"codebook_size",val:": Optional = None"},{name:"codebook_resample",val:": Optional = None"},{name:"codebook_do_center_crop",val:": Optional = None"},{name:"codebook_crop_size",val:": Optional = None"},{name:"codebook_do_rescale",val:": Optional = None"},{name:"codebook_rescale_factor",val:": Optional = None"},{name:"codebook_do_map_pixels",val:": Optional = None"},{name:"codebook_do_normalize",val:": Optional = None"},{name:"codebook_image_mean",val:": Optional = None"},{name:"codebook_image_std",val:": Optional = None"},{name:"return_tensors",val:": Union = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlavaImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.FlavaImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image.`,name:"size"},{anchor:"transformers.FlavaImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>, Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.FlavaImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the center crop. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.FlavaImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.FlavaImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.FlavaImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.FlavaImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_image_mask",description:`<strong>return_image_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.return_image_mask</code>) &#x2014;
Whether to return the image mask.`,name:"return_image_mask"},{anchor:"transformers.FlavaImageProcessor.preprocess.input_size_patches",description:`<strong>input_size_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.input_size_patches</code>) &#x2014;
Size of the patches to extract from the image.`,name:"input_size_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.total_mask_patches",description:`<strong>total_mask_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.total_mask_patches</code>) &#x2014;
Total number of patches to extract from the image.`,name:"total_mask_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_min_patches",description:`<strong>mask_group_min_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.mask_group_min_patches</code>) &#x2014;
Minimum number of patches to extract from the image.`,name:"mask_group_min_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_max_patches",description:`<strong>mask_group_max_patches</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.mask_group_max_patches</code>) &#x2014;
Maximum number of patches to extract from the image.`,name:"mask_group_max_patches"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_min_aspect_ratio",description:`<strong>mask_group_min_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.mask_group_min_aspect_ratio</code>) &#x2014;
Minimum aspect ratio of the patches to extract from the image.`,name:"mask_group_min_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.preprocess.mask_group_max_aspect_ratio",description:`<strong>mask_group_max_aspect_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.mask_group_max_aspect_ratio</code>) &#x2014;
Maximum aspect ratio of the patches to extract from the image.`,name:"mask_group_max_aspect_ratio"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_codebook_pixels",description:`<strong>return_codebook_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.return_codebook_pixels</code>) &#x2014;
Whether to return the codebook pixels.`,name:"return_codebook_pixels"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_resize",description:`<strong>codebook_do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_resize</code>) &#x2014;
Whether to resize the codebook pixels.`,name:"codebook_do_resize"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_size",description:`<strong>codebook_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.codebook_size</code>) &#x2014;
Size of the codebook pixels.`,name:"codebook_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_resample",description:`<strong>codebook_resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.codebook_resample</code>) &#x2014;
Resampling filter to use if resizing the codebook pixels. This can be one of the enum
<code>PILImageResampling</code>, Only has an effect if <code>codebook_do_resize</code> is set to <code>True</code>.`,name:"codebook_resample"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_center_crop",description:`<strong>codebook_do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_center_crop</code>) &#x2014;
Whether to center crop the codebook pixels.`,name:"codebook_do_center_crop"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_crop_size",description:`<strong>codebook_crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.codebook_crop_size</code>) &#x2014;
Size of the center crop of the codebook pixels. Only has an effect if <code>codebook_do_center_crop</code> is set
to <code>True</code>.`,name:"codebook_crop_size"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_rescale",description:`<strong>codebook_do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_rescale</code>) &#x2014;
Whether to rescale the codebook pixels values between [0 - 1].`,name:"codebook_do_rescale"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_rescale_factor",description:`<strong>codebook_rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.codebook_rescale_factor</code>) &#x2014;
Rescale factor to rescale the codebook pixels by if <code>codebook_do_rescale</code> is set to <code>True</code>.`,name:"codebook_rescale_factor"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_map_pixels",description:`<strong>codebook_do_map_pixels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_map_pixels</code>) &#x2014;
Whether to map the codebook pixels values.`,name:"codebook_do_map_pixels"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_do_normalize",description:`<strong>codebook_do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.codebook_do_normalize</code>) &#x2014;
Whether to normalize the codebook pixels.`,name:"codebook_do_normalize"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_image_mean",description:`<strong>codebook_image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.codebook_image_mean</code>) &#x2014;
Codebook pixels mean to normalize the codebook pixels by if <code>codebook_do_normalize</code> is set to <code>True</code>.`,name:"codebook_image_mean"},{anchor:"transformers.FlavaImageProcessor.preprocess.codebook_image_std",description:`<strong>codebook_image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.codebook_image_std</code>) &#x2014;
Codebook pixels standard deviation to normalize the codebook pixels by if <code>codebook_do_normalize</code> is
set to <code>True</code>.`,name:"codebook_image_std"},{anchor:"transformers.FlavaImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.FlavaImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.FlavaImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/image_processing_flava.py#L447"}}),to=new z({props:{title:"FlavaForPreTraining",local:"transformers.FlavaForPreTraining",headingTag:"h2"}}),ao=new $({props:{name:"class transformers.FlavaForPreTraining",anchor:"transformers.FlavaForPreTraining",parameters:[{name:"config",val:": FlavaConfig"},{name:"image_codebook",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlavaForPreTraining.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlavaForPreTraining.image_codebook",description:`<strong>image_codebook</strong> (<code>nn.Module</code>) &#x2014; If passed, the image codebook will be set to this. Otherwise. it will
be initialized using the image_codebook_config defined in the config first as the first parameter.`,name:"image_codebook"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1714"}}),no=new $({props:{name:"forward",anchor:"transformers.FlavaForPreTraining.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"input_ids_masked",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"codebook_pixel_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"image_attention_mask",val:": Optional = None"},{name:"skip_unmasked_multimodal_encoder",val:": bool = None"},{name:"mlm_labels",val:": Optional = None"},{name:"mim_labels",val:": Optional = None"},{name:"itm_labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": Optional = None"},{name:"return_loss",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlavaForPreTraining.forward.input_ids_masked",description:`<strong>input_ids_masked</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. These ones are the masked version of the original task
to be used with MLM. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a> along with
<code>DataCollatorForMaskedLanguageModeling</code>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids_masked"},{anchor:"transformers.FlavaForPreTraining.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaForPreTraining.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaForPreTraining.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">FlavaImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaForPreTraining.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaForPreTraining.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaForPreTraining.forward.image_attention_mask",description:`<strong>image_attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices specifically for images. Mask values selected
in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"image_attention_mask"},{anchor:"transformers.FlavaForPreTraining.forward.skip_unmasked_multimodal_encoder",description:`<strong>skip_unmasked_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder for unmasked inputs. FLAVA pretraining doesn&#x2019;t need unmasked
multimodal embeddings or outputs as of now.`,name:"skip_unmasked_multimodal_encoder"},{anchor:"transformers.FlavaForPreTraining.forward.mlm_labels",description:`<strong>mlm_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language and multimodal masked modeling loss (next word prediction).
Indices should be in <code>[-100, 0, ..., text_config.vocab_size - 1]</code> (see <code>input_ids</code> docstring). Tokens with
indices set to <code>-100</code> are ignored (masked), the loss is only computed for the tokens with labels in <code>[0, ..., text_config.vocab_size - 1]</code>.`,name:"mlm_labels"},{anchor:"transformers.FlavaForPreTraining.forward.mim_labels",description:`<strong>mim_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Labels for computing the image and multimodal masked modeling loss. Indices should be in <code>[-100, 0, ..., image_config.vocab_size - 1]</code>. Tokens with indices set to <code>-100</code> are ignored (masked), the loss is only
computed for the tokens with labels in <code>[0, ..., image_config.vocab_size - 1]</code>. If not passed, they are
generated automatically using the image codebook assigned to the model. By default, it uses
<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a>. See <a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebook">FlavaImageCodebook</a> to understand how to generate mim_labels.`,name:"mim_labels"},{anchor:"transformers.FlavaForPreTraining.forward.itm_labels",description:`<strong>itm_labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, 1)</code>, <em>optional</em>) &#x2014;
Labels for computing the image-text matching loss. 0 means the pairs don&#x2019;t match and 1 means they match.
The pairs with 0 will be skipped for calculation of MMM and global contrastive losses as well.`,name:"itm_labels"},{anchor:"transformers.FlavaForPreTraining.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>, default to None) &#x2014;
Whether to return calculated loss or not.`,name:"return_loss"},{anchor:"transformers.FlavaForPreTraining.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaForPreTraining.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaForPreTraining.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaForPreTraining.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaForPreTraining.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.</p>
<p>Examples &#x2014;`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1764",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FlavaConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code>, <em>optional</em>, returned when <code>return_loss</code> is True) — Total loss calculated for this model.</p>
</li>
<li>
<p><strong>loss_info</strong> (<code>FlavaLosses</code>) — Detailed info for FLAVA Pretraining losses. Check <code>FlavaLosses</code> class description for the information on
the keys.</p>
</li>
<li>
<p><strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) — The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</p>
</li>
<li>
<p><strong>image_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) — The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</p>
</li>
<li>
<p><strong>text_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> are present) — The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>text_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids</code> are present) — The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) — The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_unmasked_multimodal_encoder</code> is <code>None</code> or <code>False</code>) — The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</p>
</li>
<li>
<p><strong>image_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) — The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>. Uses <code>bool_masked_pos</code>
to create masked images.</p>
</li>
<li>
<p><strong>image_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) — The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>. Uses <code>bool_masked_pos</code> to create masked images.</p>
</li>
<li>
<p><strong>text_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present) — The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>text_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present) — The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present) — The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</p>
</li>
<li>
<p><strong>multimodal_masked_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) — The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</p>
</li>
<li>
<p><strong>mim_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape <code>(total_masked_patches, image_vocab_size)</code> , <em>optional</em>, returned when <code>pixel_values</code> are present and <code>input_ids_masked</code> are not) — The logits for MIM unimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The flattened output is
returned when <code>bool_masked_pos</code> has some of the patches masked.</p>
</li>
<li>
<p><strong>mlm_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape <code>(total_masked_seq_length, text_vocab_size)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> are present and <code>pixel_values</code> are not) — The logits for MLM unimodal loss. The flattened output is returned when <code>input_ids_masked</code> has some of
the tokens masked.</p>
</li>
<li>
<p><strong>itm_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 2)</code>, <em>optional</em>, returned when <code>input_ids_masked</code> and <code>pixel_values</code> are present) — The logits for ITM loss. Note that ITM loss is calculated on masked pairs in FLAVA.</p>
</li>
<li>
<p><strong>mmm_image_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_image_patches, image_vocab_size)</code> or of shape<code>(total_masked_patches, image_vocab_size)</code>, <em>optional</em>, returned when <code>pixel_values</code> and <code>input_ids_masked</code> are present) — The logits for MMM image multimodal loss. Uses <code>book_masked_pos</code> to get masked patches. The flattened
output is returned when <code>bool_masked_pos</code> has some of the patches masked.</p>
</li>
<li>
<p><strong>mmm_text_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length, text_vocab_size)</code> or of shape <code>(</code>(total_masked_seq_length, text_vocab_size)<code>), *optional*, returned when </code>pixel_values<code>and</code>input_ids_masked<code>are present) -- The logits for MMM text multimodal loss. The flattened output is returned when</code>input_ids_masked\` has
some of the tokens masked.</p>
</li>
<li>
<p><strong>contrastive_logits_per_image</strong> (<code>torch.FloatTensor</code> of shape <code>(image_batch_size, text_batch_size)</code>) — The scaled dot product scores between <code>image_embeddings</code> and <code>text_embeddings</code> but passed through FLAVA’s
<code>image_projection</code> and <code>text_projection</code> layers respectively. This represents the image-text similarity
scores. This is calculated on unmasked images and texts.</p>
</li>
<li>
<p><strong>contrastive_logits_per_text</strong> (<code>torch.FloatTensor</code> of shape <code>(text_batch_size, image_batch_size)</code>) — The scaled dot product scores between <code>text_embeddings</code> and <code>image_embeddings</code> but passed through FLAVA’s
<code>text_projection</code> and <code>image_projection</code> layers respectively. This is calculated on unmasked images and
texts.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.flava.modeling_flava.FlavaForPreTrainingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pe=new Co({props:{$$slots:{default:[sr]},$$scope:{ctx:F}}}),ro=new z({props:{title:"FlavaModel",local:"transformers.FlavaModel",headingTag:"h2"}}),so=new $({props:{name:"class transformers.FlavaModel",anchor:"transformers.FlavaModel",parameters:[{name:"config",val:": FlavaConfig"}],parametersDescription:[{anchor:"transformers.FlavaModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaConfig">FlavaConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1179"}}),io=new $({props:{name:"forward",anchor:"transformers.FlavaModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"image_attention_mask",val:": Optional = None"},{name:"skip_multimodal_encoder",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": bool = True"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">FlavaImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.FlavaModel.forward.skip_multimodal_encoder",description:`<strong>skip_multimodal_encoder</strong> (<em>bool</em>, <em>optional</em>) &#x2014;
Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.`,name:"skip_multimodal_encoder"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1323",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.flava.modeling_flava.FlavaModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.flava.configuration_flava.FlavaConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>image_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) — The image embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</li>
<li><strong>image_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>pixel_values</code> are present) — The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageModel"
>FlavaImageModel</a>.</li>
<li><strong>text_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> are present) — The text embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>text_output</strong> (<code>BaseModelOutputWithPooling</code>, <em>optional</em>, returned when <code>input_ids</code> are present) — The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_embeddings</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code>, <em>optional</em>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) — The multimodal embeddings which are basically the pooled output of <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextModel"
>FlavaTextModel</a>.</li>
<li><strong>multimodal_output</strong> (<code>BaseModelOutputWithPooling</code>, returned when <code>input_ids</code> and <code>pixel_values</code> are present and <code>skip_multimodal_encoder</code> is <code>None</code> or <code>False</code>) — The output of the <a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalModel"
>FlavaMultimodalModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.flava.modeling_flava.FlavaModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new Co({props:{$$slots:{default:[ir]},$$scope:{ctx:F}}}),ge=new we({props:{anchor:"transformers.FlavaModel.forward.example",$$slots:{default:[lr]},$$scope:{ctx:F}}}),lo=new $({props:{name:"get_text_features",anchor:"transformers.FlavaModel.get_text_features",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.get_text_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1229"}}),he=new Co({props:{$$slots:{default:[dr]},$$scope:{ctx:F}}}),co=new $({props:{name:"get_image_features",anchor:"transformers.FlavaModel.get_image_features",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"interpolate_pos_encoding",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlavaModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">FlavaImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaModel.get_image_features.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaModel.get_image_features.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaModel.get_image_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaModel.get_image_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1273"}}),ue=new Co({props:{$$slots:{default:[cr]},$$scope:{ctx:F}}}),mo=new z({props:{title:"FlavaImageCodebook",local:"transformers.FlavaImageCodebook",headingTag:"h2"}}),po=new $({props:{name:"class transformers.FlavaImageCodebook",anchor:"transformers.FlavaImageCodebook",parameters:[{name:"config",val:": FlavaImageCodebookConfig"},{name:"**kwargs",val:": Any"}],parametersDescription:[{anchor:"transformers.FlavaImageCodebook.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageCodebookConfig">FlavaImageCodebookConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1500"}}),fo=new $({props:{name:"forward",anchor:"transformers.FlavaImageCodebook.forward",parameters:[{name:"pixel_values",val:": FloatTensor"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1590"}}),go=new $({props:{name:"get_codebook_indices",anchor:"transformers.FlavaImageCodebook.get_codebook_indices",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1558"}}),ho=new $({props:{name:"get_codebook_probs",anchor:"transformers.FlavaImageCodebook.get_codebook_probs",parameters:[{name:"pixel_values",val:": Tensor"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1586"}}),uo=new z({props:{title:"FlavaTextModel",local:"transformers.FlavaTextModel",headingTag:"h2"}}),_o=new $({props:{name:"class transformers.FlavaTextModel",anchor:"transformers.FlavaTextModel",parameters:[{name:"config",val:": FlavaTextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FlavaTextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextConfig">FlavaTextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L976"}}),vo=new $({props:{name:"forward",anchor:"transformers.FlavaTextModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlavaTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlavaTextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.FlavaTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_seq_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaTextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1011",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaTextConfig"
>FlavaTextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),_e=new Co({props:{$$slots:{default:[mr]},$$scope:{ctx:F}}}),ve=new we({props:{anchor:"transformers.FlavaTextModel.forward.example",$$slots:{default:[pr]},$$scope:{ctx:F}}}),bo=new z({props:{title:"FlavaImageModel",local:"transformers.FlavaImageModel",headingTag:"h2"}}),ko=new $({props:{name:"class transformers.FlavaImageModel",anchor:"transformers.FlavaImageModel",parameters:[{name:"config",val:": FlavaImageConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.FlavaImageModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageConfig">FlavaImageConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L877"}}),To=new $({props:{name:"forward",anchor:"transformers.FlavaImageModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"interpolate_pos_encoding",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlavaImageModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">FlavaImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlavaImageModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, image_num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.FlavaImageModel.forward.interpolate_pos_encoding",description:`<strong>interpolate_pos_encoding</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to interpolate the pre-trained position encodings.`,name:"interpolate_pos_encoding"},{anchor:"transformers.FlavaImageModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaImageModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaImageModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaImageModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaImageModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L914",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaImageConfig"
>FlavaImageConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),be=new Co({props:{$$slots:{default:[fr]},$$scope:{ctx:F}}}),ke=new we({props:{anchor:"transformers.FlavaImageModel.forward.example",$$slots:{default:[gr]},$$scope:{ctx:F}}}),Mo=new z({props:{title:"FlavaMultimodalModel",local:"transformers.FlavaMultimodalModel",headingTag:"h2"}}),Fo=new $({props:{name:"class transformers.FlavaMultimodalModel",anchor:"transformers.FlavaMultimodalModel",parameters:[{name:"config",val:": FlavaMultimodalConfig"},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.FlavaMultimodalModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalConfig">FlavaMultimodalConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1081"}}),xo=new $({props:{name:"forward",anchor:"transformers.FlavaMultimodalModel.forward",parameters:[{name:"hidden_states",val:": Tensor"},{name:"attention_mask",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlavaMultimodalModel.forward.hidden_states",description:`<strong>hidden_states</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len, hidden_size)</code>) &#x2014;
The concatenated hidden states of unimodal encoders.`,name:"hidden_states"},{anchor:"transformers.FlavaMultimodalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_num_patches + text_seq_len)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.FlavaMultimodalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.FlavaMultimodalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlavaMultimodalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlavaMultimodalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/flava/modeling_flava.py#L1113",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/flava#transformers.FlavaMultimodalConfig"
>FlavaMultimodalConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Te=new Co({props:{$$slots:{default:[hr]},$$scope:{ctx:F}}}),Me=new we({props:{anchor:"transformers.FlavaMultimodalModel.forward.example",$$slots:{default:[ur]},$$scope:{ctx:F}}}),{c(){o=d("meta"),k=s(),l=d("p"),m=s(),f(T.$$.fragment),t=s(),f(M.$$.fragment),kt=s(),Ce=d("p"),Ce.innerHTML=cn,Tt=s(),Ie=d("p"),Ie.textContent=mn,Mt=s(),Pe=d("p"),Pe.textContent=pn,Ft=s(),We=d("p"),We.innerHTML=fn,xt=s(),je=d("p"),je.innerHTML=gn,yt=s(),f(Ze.$$.fragment),$t=s(),C=d("div"),f(Ue.$$.fragment),oa=s(),Io=d("p"),Io.innerHTML=hn,ta=s(),Po=d("p"),Po.innerHTML=un,aa=s(),f(ne.$$.fragment),na=s(),re=d("div"),f(Je.$$.fragment),ra=s(),Wo=d("p"),Wo.innerHTML=_n,wt=s(),f(Ne.$$.fragment),zt=s(),I=d("div"),f(Le.$$.fragment),sa=s(),jo=d("p"),jo.innerHTML=vn,ia=s(),Zo=d("p"),Zo.innerHTML=bn,la=s(),Uo=d("p"),Uo.innerHTML=kn,da=s(),f(se.$$.fragment),Ct=s(),f(Ve.$$.fragment),It=s(),P=d("div"),f(Be.$$.fragment),ca=s(),Jo=d("p"),Jo.innerHTML=Tn,ma=s(),No=d("p"),No.innerHTML=Mn,pa=s(),Lo=d("p"),Lo.innerHTML=Fn,fa=s(),f(ie.$$.fragment),Pt=s(),f(Re.$$.fragment),Wt=s(),W=d("div"),f(Ge.$$.fragment),ga=s(),Vo=d("p"),Vo.innerHTML=xn,ha=s(),Bo=d("p"),Bo.innerHTML=yn,ua=s(),Ro=d("p"),Ro.innerHTML=$n,_a=s(),f(le.$$.fragment),jt=s(),f(qe.$$.fragment),Zt=s(),Ee=d("div"),f(He.$$.fragment),Ut=s(),f(Ae.$$.fragment),Jt=s(),j=d("div"),f(Oe.$$.fragment),va=s(),Go=d("p"),Go.textContent=wn,ba=s(),qo=d("p"),qo.innerHTML=zn,ka=s(),de=d("div"),f(Se.$$.fragment),Ta=s(),Eo=d("p"),Eo.innerHTML=Cn,Ma=s(),ce=d("div"),f(De.$$.fragment),Fa=s(),Ho=d("p"),Ho.innerHTML=In,Nt=s(),f(Xe.$$.fragment),Lt=s(),Ye=d("div"),f(Qe.$$.fragment),Vt=s(),f(Ke.$$.fragment),Bt=s(),S=d("div"),f(eo.$$.fragment),xa=s(),Ao=d("p"),Ao.textContent=Pn,ya=s(),me=d("div"),f(oo.$$.fragment),$a=s(),Oo=d("p"),Oo.textContent=Wn,Rt=s(),f(to.$$.fragment),Gt=s(),J=d("div"),f(ao.$$.fragment),wa=s(),So=d("p"),So.textContent=jn,za=s(),Do=d("p"),Do.innerHTML=Zn,Ca=s(),K=d("div"),f(no.$$.fragment),Ia=s(),Xo=d("p"),Xo.innerHTML=Un,Pa=s(),f(pe.$$.fragment),qt=s(),f(ro.$$.fragment),Et=s(),Z=d("div"),f(so.$$.fragment),Wa=s(),Yo=d("p"),Yo.innerHTML=Jn,ja=s(),q=d("div"),f(io.$$.fragment),Za=s(),Qo=d("p"),Qo.innerHTML=Nn,Ua=s(),f(fe.$$.fragment),Ja=s(),f(ge.$$.fragment),Na=s(),ee=d("div"),f(lo.$$.fragment),La=s(),Ko=d("p"),Ko.innerHTML=Ln,Va=s(),f(he.$$.fragment),Ba=s(),oe=d("div"),f(co.$$.fragment),Ra=s(),et=d("p"),et.innerHTML=Vn,Ga=s(),f(ue.$$.fragment),Ht=s(),f(mo.$$.fragment),At=s(),w=d("div"),f(po.$$.fragment),qa=s(),ot=d("p"),ot.innerHTML=Bn,Ea=s(),tt=d("p"),tt.innerHTML=Rn,Ha=s(),at=d("div"),f(fo.$$.fragment),Aa=s(),nt=d("div"),f(go.$$.fragment),Oa=s(),rt=d("div"),f(ho.$$.fragment),Ot=s(),f(uo.$$.fragment),St=s(),D=d("div"),f(_o.$$.fragment),Sa=s(),st=d("p"),st.innerHTML=Gn,Da=s(),E=d("div"),f(vo.$$.fragment),Xa=s(),it=d("p"),it.innerHTML=qn,Ya=s(),f(_e.$$.fragment),Qa=s(),f(ve.$$.fragment),Dt=s(),f(bo.$$.fragment),Xt=s(),X=d("div"),f(ko.$$.fragment),Ka=s(),lt=d("p"),lt.innerHTML=En,en=s(),H=d("div"),f(To.$$.fragment),on=s(),dt=d("p"),dt.innerHTML=Hn,tn=s(),f(be.$$.fragment),an=s(),f(ke.$$.fragment),Yt=s(),f(Mo.$$.fragment),Qt=s(),Y=d("div"),f(Fo.$$.fragment),nn=s(),ct=d("p"),ct.innerHTML=An,rn=s(),A=d("div"),f(xo.$$.fragment),sn=s(),mt=d("p"),mt.innerHTML=On,ln=s(),f(Te.$$.fragment),dn=s(),f(Me.$$.fragment),Kt=s(),_t=d("p"),this.h()},l(e){const r=or("svelte-u9bgzb",document.head);o=c(r,"META",{name:!0,content:!0}),r.forEach(a),k=i(e),l=c(e,"P",{}),x(l).forEach(a),m=i(e),g(T.$$.fragment,e),t=i(e),g(M.$$.fragment,e),kt=i(e),Ce=c(e,"P",{"data-svelte-h":!0}),b(Ce)!=="svelte-dkrobq"&&(Ce.innerHTML=cn),Tt=i(e),Ie=c(e,"P",{"data-svelte-h":!0}),b(Ie)!=="svelte-d5xhfu"&&(Ie.textContent=mn),Mt=i(e),Pe=c(e,"P",{"data-svelte-h":!0}),b(Pe)!=="svelte-vfdo9a"&&(Pe.textContent=pn),Ft=i(e),We=c(e,"P",{"data-svelte-h":!0}),b(We)!=="svelte-32wzl0"&&(We.innerHTML=fn),xt=i(e),je=c(e,"P",{"data-svelte-h":!0}),b(je)!=="svelte-zltroy"&&(je.innerHTML=gn),yt=i(e),g(Ze.$$.fragment,e),$t=i(e),C=c(e,"DIV",{class:!0});var N=x(C);g(Ue.$$.fragment,N),oa=i(N),Io=c(N,"P",{"data-svelte-h":!0}),b(Io)!=="svelte-1g1kjl5"&&(Io.innerHTML=hn),ta=i(N),Po=c(N,"P",{"data-svelte-h":!0}),b(Po)!=="svelte-o55m63"&&(Po.innerHTML=un),aa=i(N),g(ne.$$.fragment,N),na=i(N),re=c(N,"DIV",{class:!0});var yo=x(re);g(Je.$$.fragment,yo),ra=i(yo),Wo=c(yo,"P",{"data-svelte-h":!0}),b(Wo)!=="svelte-rsw387"&&(Wo.innerHTML=_n),yo.forEach(a),N.forEach(a),wt=i(e),g(Ne.$$.fragment,e),zt=i(e),I=c(e,"DIV",{class:!0});var L=x(I);g(Le.$$.fragment,L),sa=i(L),jo=c(L,"P",{"data-svelte-h":!0}),b(jo)!=="svelte-1l8b18b"&&(jo.innerHTML=vn),ia=i(L),Zo=c(L,"P",{"data-svelte-h":!0}),b(Zo)!=="svelte-mil706"&&(Zo.innerHTML=bn),la=i(L),Uo=c(L,"P",{"data-svelte-h":!0}),b(Uo)!=="svelte-o55m63"&&(Uo.innerHTML=kn),da=i(L),g(se.$$.fragment,L),L.forEach(a),Ct=i(e),g(Ve.$$.fragment,e),It=i(e),P=c(e,"DIV",{class:!0});var V=x(P);g(Be.$$.fragment,V),ca=i(V),Jo=c(V,"P",{"data-svelte-h":!0}),b(Jo)!=="svelte-1la2ttf"&&(Jo.innerHTML=Tn),ma=i(V),No=c(V,"P",{"data-svelte-h":!0}),b(No)!=="svelte-mil706"&&(No.innerHTML=Mn),pa=i(V),Lo=c(V,"P",{"data-svelte-h":!0}),b(Lo)!=="svelte-o55m63"&&(Lo.innerHTML=Fn),fa=i(V),g(ie.$$.fragment,V),V.forEach(a),Pt=i(e),g(Re.$$.fragment,e),Wt=i(e),W=c(e,"DIV",{class:!0});var B=x(W);g(Ge.$$.fragment,B),ga=i(B),Vo=c(B,"P",{"data-svelte-h":!0}),b(Vo)!=="svelte-1hopzs9"&&(Vo.innerHTML=xn),ha=i(B),Bo=c(B,"P",{"data-svelte-h":!0}),b(Bo)!=="svelte-mil706"&&(Bo.innerHTML=yn),ua=i(B),Ro=c(B,"P",{"data-svelte-h":!0}),b(Ro)!=="svelte-o55m63"&&(Ro.innerHTML=$n),_a=i(B),g(le.$$.fragment,B),B.forEach(a),jt=i(e),g(qe.$$.fragment,e),Zt=i(e),Ee=c(e,"DIV",{class:!0});var vt=x(Ee);g(He.$$.fragment,vt),vt.forEach(a),Ut=i(e),g(Ae.$$.fragment,e),Jt=i(e),j=c(e,"DIV",{class:!0});var R=x(j);g(Oe.$$.fragment,R),va=i(R),Go=c(R,"P",{"data-svelte-h":!0}),b(Go)!=="svelte-6nczxn"&&(Go.textContent=wn),ba=i(R),qo=c(R,"P",{"data-svelte-h":!0}),b(qo)!=="svelte-l4mt6h"&&(qo.innerHTML=zn),ka=i(R),de=c(R,"DIV",{class:!0});var $o=x(de);g(Se.$$.fragment,$o),Ta=i($o),Eo=c($o,"P",{"data-svelte-h":!0}),b(Eo)!=="svelte-aes5em"&&(Eo.innerHTML=Cn),$o.forEach(a),Ma=i(R),ce=c(R,"DIV",{class:!0});var wo=x(ce);g(De.$$.fragment,wo),Fa=i(wo),Ho=c(wo,"P",{"data-svelte-h":!0}),b(Ho)!=="svelte-12js55v"&&(Ho.innerHTML=In),wo.forEach(a),R.forEach(a),Nt=i(e),g(Xe.$$.fragment,e),Lt=i(e),Ye=c(e,"DIV",{class:!0});var bt=x(Ye);g(Qe.$$.fragment,bt),bt.forEach(a),Vt=i(e),g(Ke.$$.fragment,e),Bt=i(e),S=c(e,"DIV",{class:!0});var te=x(S);g(eo.$$.fragment,te),xa=i(te),Ao=c(te,"P",{"data-svelte-h":!0}),b(Ao)!=="svelte-18n2ywm"&&(Ao.textContent=Pn),ya=i(te),me=c(te,"DIV",{class:!0});var zo=x(me);g(oo.$$.fragment,zo),$a=i(zo),Oo=c(zo,"P",{"data-svelte-h":!0}),b(Oo)!=="svelte-1x3yxsa"&&(Oo.textContent=Wn),zo.forEach(a),te.forEach(a),Rt=i(e),g(to.$$.fragment,e),Gt=i(e),J=c(e,"DIV",{class:!0});var Q=x(J);g(ao.$$.fragment,Q),wa=i(Q),So=c(Q,"P",{"data-svelte-h":!0}),b(So)!=="svelte-1ishsx8"&&(So.textContent=jn),za=i(Q),Do=c(Q,"P",{"data-svelte-h":!0}),b(Do)!=="svelte-1gjh92c"&&(Do.innerHTML=Zn),Ca=i(Q),K=c(Q,"DIV",{class:!0});var ae=x(K);g(no.$$.fragment,ae),Ia=i(ae),Xo=c(ae,"P",{"data-svelte-h":!0}),b(Xo)!=="svelte-5xikln"&&(Xo.innerHTML=Un),Pa=i(ae),g(pe.$$.fragment,ae),ae.forEach(a),Q.forEach(a),qt=i(e),g(ro.$$.fragment,e),Et=i(e),Z=c(e,"DIV",{class:!0});var G=x(Z);g(so.$$.fragment,G),Wa=i(G),Yo=c(G,"P",{"data-svelte-h":!0}),b(Yo)!=="svelte-t6qnc9"&&(Yo.innerHTML=Jn),ja=i(G),q=c(G,"DIV",{class:!0});var Fe=x(q);g(io.$$.fragment,Fe),Za=i(Fe),Qo=c(Fe,"P",{"data-svelte-h":!0}),b(Qo)!=="svelte-elgcix"&&(Qo.innerHTML=Nn),Ua=i(Fe),g(fe.$$.fragment,Fe),Ja=i(Fe),g(ge.$$.fragment,Fe),Fe.forEach(a),Na=i(G),ee=c(G,"DIV",{class:!0});var pt=x(ee);g(lo.$$.fragment,pt),La=i(pt),Ko=c(pt,"P",{"data-svelte-h":!0}),b(Ko)!=="svelte-elgcix"&&(Ko.innerHTML=Ln),Va=i(pt),g(he.$$.fragment,pt),pt.forEach(a),Ba=i(G),oe=c(G,"DIV",{class:!0});var ft=x(oe);g(co.$$.fragment,ft),Ra=i(ft),et=c(ft,"P",{"data-svelte-h":!0}),b(et)!=="svelte-elgcix"&&(et.innerHTML=Vn),Ga=i(ft),g(ue.$$.fragment,ft),ft.forEach(a),G.forEach(a),Ht=i(e),g(mo.$$.fragment,e),At=i(e),w=c(e,"DIV",{class:!0});var O=x(w);g(po.$$.fragment,O),qa=i(O),ot=c(O,"P",{"data-svelte-h":!0}),b(ot)!=="svelte-8jac64"&&(ot.innerHTML=Bn),Ea=i(O),tt=c(O,"P",{"data-svelte-h":!0}),b(tt)!=="svelte-1gjh92c"&&(tt.innerHTML=Rn),Ha=i(O),at=c(O,"DIV",{class:!0});var Sn=x(at);g(fo.$$.fragment,Sn),Sn.forEach(a),Aa=i(O),nt=c(O,"DIV",{class:!0});var Dn=x(nt);g(go.$$.fragment,Dn),Dn.forEach(a),Oa=i(O),rt=c(O,"DIV",{class:!0});var Xn=x(rt);g(ho.$$.fragment,Xn),Xn.forEach(a),O.forEach(a),Ot=i(e),g(uo.$$.fragment,e),St=i(e),D=c(e,"DIV",{class:!0});var gt=x(D);g(_o.$$.fragment,gt),Sa=i(gt),st=c(gt,"P",{"data-svelte-h":!0}),b(st)!=="svelte-1f7fy72"&&(st.innerHTML=Gn),Da=i(gt),E=c(gt,"DIV",{class:!0});var xe=x(E);g(vo.$$.fragment,xe),Xa=i(xe),it=c(xe,"P",{"data-svelte-h":!0}),b(it)!=="svelte-kjg9cr"&&(it.innerHTML=qn),Ya=i(xe),g(_e.$$.fragment,xe),Qa=i(xe),g(ve.$$.fragment,xe),xe.forEach(a),gt.forEach(a),Dt=i(e),g(bo.$$.fragment,e),Xt=i(e),X=c(e,"DIV",{class:!0});var ht=x(X);g(ko.$$.fragment,ht),Ka=i(ht),lt=c(ht,"P",{"data-svelte-h":!0}),b(lt)!=="svelte-7canni"&&(lt.innerHTML=En),en=i(ht),H=c(ht,"DIV",{class:!0});var ye=x(H);g(To.$$.fragment,ye),on=i(ye),dt=c(ye,"P",{"data-svelte-h":!0}),b(dt)!=="svelte-177grmz"&&(dt.innerHTML=Hn),tn=i(ye),g(be.$$.fragment,ye),an=i(ye),g(ke.$$.fragment,ye),ye.forEach(a),ht.forEach(a),Yt=i(e),g(Mo.$$.fragment,e),Qt=i(e),Y=c(e,"DIV",{class:!0});var ut=x(Y);g(Fo.$$.fragment,ut),nn=i(ut),ct=c(ut,"P",{"data-svelte-h":!0}),b(ct)!=="svelte-1646u0b"&&(ct.innerHTML=An),rn=i(ut),A=c(ut,"DIV",{class:!0});var $e=x(A);g(xo.$$.fragment,$e),sn=i($e),mt=c($e,"P",{"data-svelte-h":!0}),b(mt)!=="svelte-1fvs26t"&&(mt.innerHTML=On),ln=i($e),g(Te.$$.fragment,$e),dn=i($e),g(Me.$$.fragment,$e),$e.forEach(a),ut.forEach(a),Kt=i(e),_t=c(e,"P",{}),x(_t).forEach(a),this.h()},h(){y(o,"name","hf:doc:metadata"),y(o,"content",vr),y(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(at,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),y(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,r){n(document.head,o),p(e,k,r),p(e,l,r),p(e,m,r),h(T,e,r),p(e,t,r),h(M,e,r),p(e,kt,r),p(e,Ce,r),p(e,Tt,r),p(e,Ie,r),p(e,Mt,r),p(e,Pe,r),p(e,Ft,r),p(e,We,r),p(e,xt,r),p(e,je,r),p(e,yt,r),h(Ze,e,r),p(e,$t,r),p(e,C,r),h(Ue,C,null),n(C,oa),n(C,Io),n(C,ta),n(C,Po),n(C,aa),h(ne,C,null),n(C,na),n(C,re),h(Je,re,null),n(re,ra),n(re,Wo),p(e,wt,r),h(Ne,e,r),p(e,zt,r),p(e,I,r),h(Le,I,null),n(I,sa),n(I,jo),n(I,ia),n(I,Zo),n(I,la),n(I,Uo),n(I,da),h(se,I,null),p(e,Ct,r),h(Ve,e,r),p(e,It,r),p(e,P,r),h(Be,P,null),n(P,ca),n(P,Jo),n(P,ma),n(P,No),n(P,pa),n(P,Lo),n(P,fa),h(ie,P,null),p(e,Pt,r),h(Re,e,r),p(e,Wt,r),p(e,W,r),h(Ge,W,null),n(W,ga),n(W,Vo),n(W,ha),n(W,Bo),n(W,ua),n(W,Ro),n(W,_a),h(le,W,null),p(e,jt,r),h(qe,e,r),p(e,Zt,r),p(e,Ee,r),h(He,Ee,null),p(e,Ut,r),h(Ae,e,r),p(e,Jt,r),p(e,j,r),h(Oe,j,null),n(j,va),n(j,Go),n(j,ba),n(j,qo),n(j,ka),n(j,de),h(Se,de,null),n(de,Ta),n(de,Eo),n(j,Ma),n(j,ce),h(De,ce,null),n(ce,Fa),n(ce,Ho),p(e,Nt,r),h(Xe,e,r),p(e,Lt,r),p(e,Ye,r),h(Qe,Ye,null),p(e,Vt,r),h(Ke,e,r),p(e,Bt,r),p(e,S,r),h(eo,S,null),n(S,xa),n(S,Ao),n(S,ya),n(S,me),h(oo,me,null),n(me,$a),n(me,Oo),p(e,Rt,r),h(to,e,r),p(e,Gt,r),p(e,J,r),h(ao,J,null),n(J,wa),n(J,So),n(J,za),n(J,Do),n(J,Ca),n(J,K),h(no,K,null),n(K,Ia),n(K,Xo),n(K,Pa),h(pe,K,null),p(e,qt,r),h(ro,e,r),p(e,Et,r),p(e,Z,r),h(so,Z,null),n(Z,Wa),n(Z,Yo),n(Z,ja),n(Z,q),h(io,q,null),n(q,Za),n(q,Qo),n(q,Ua),h(fe,q,null),n(q,Ja),h(ge,q,null),n(Z,Na),n(Z,ee),h(lo,ee,null),n(ee,La),n(ee,Ko),n(ee,Va),h(he,ee,null),n(Z,Ba),n(Z,oe),h(co,oe,null),n(oe,Ra),n(oe,et),n(oe,Ga),h(ue,oe,null),p(e,Ht,r),h(mo,e,r),p(e,At,r),p(e,w,r),h(po,w,null),n(w,qa),n(w,ot),n(w,Ea),n(w,tt),n(w,Ha),n(w,at),h(fo,at,null),n(w,Aa),n(w,nt),h(go,nt,null),n(w,Oa),n(w,rt),h(ho,rt,null),p(e,Ot,r),h(uo,e,r),p(e,St,r),p(e,D,r),h(_o,D,null),n(D,Sa),n(D,st),n(D,Da),n(D,E),h(vo,E,null),n(E,Xa),n(E,it),n(E,Ya),h(_e,E,null),n(E,Qa),h(ve,E,null),p(e,Dt,r),h(bo,e,r),p(e,Xt,r),p(e,X,r),h(ko,X,null),n(X,Ka),n(X,lt),n(X,en),n(X,H),h(To,H,null),n(H,on),n(H,dt),n(H,tn),h(be,H,null),n(H,an),h(ke,H,null),p(e,Yt,r),h(Mo,e,r),p(e,Qt,r),p(e,Y,r),h(Fo,Y,null),n(Y,nn),n(Y,ct),n(Y,rn),n(Y,A),h(xo,A,null),n(A,sn),n(A,mt),n(A,ln),h(Te,A,null),n(A,dn),h(Me,A,null),p(e,Kt,r),p(e,_t,r),ea=!0},p(e,[r]){const N={};r&2&&(N.$$scope={dirty:r,ctx:e}),ne.$set(N);const yo={};r&2&&(yo.$$scope={dirty:r,ctx:e}),se.$set(yo);const L={};r&2&&(L.$$scope={dirty:r,ctx:e}),ie.$set(L);const V={};r&2&&(V.$$scope={dirty:r,ctx:e}),le.$set(V);const B={};r&2&&(B.$$scope={dirty:r,ctx:e}),pe.$set(B);const vt={};r&2&&(vt.$$scope={dirty:r,ctx:e}),fe.$set(vt);const R={};r&2&&(R.$$scope={dirty:r,ctx:e}),ge.$set(R);const $o={};r&2&&($o.$$scope={dirty:r,ctx:e}),he.$set($o);const wo={};r&2&&(wo.$$scope={dirty:r,ctx:e}),ue.$set(wo);const bt={};r&2&&(bt.$$scope={dirty:r,ctx:e}),_e.$set(bt);const te={};r&2&&(te.$$scope={dirty:r,ctx:e}),ve.$set(te);const zo={};r&2&&(zo.$$scope={dirty:r,ctx:e}),be.$set(zo);const Q={};r&2&&(Q.$$scope={dirty:r,ctx:e}),ke.$set(Q);const ae={};r&2&&(ae.$$scope={dirty:r,ctx:e}),Te.$set(ae);const G={};r&2&&(G.$$scope={dirty:r,ctx:e}),Me.$set(G)},i(e){ea||(u(T.$$.fragment,e),u(M.$$.fragment,e),u(Ze.$$.fragment,e),u(Ue.$$.fragment,e),u(ne.$$.fragment,e),u(Je.$$.fragment,e),u(Ne.$$.fragment,e),u(Le.$$.fragment,e),u(se.$$.fragment,e),u(Ve.$$.fragment,e),u(Be.$$.fragment,e),u(ie.$$.fragment,e),u(Re.$$.fragment,e),u(Ge.$$.fragment,e),u(le.$$.fragment,e),u(qe.$$.fragment,e),u(He.$$.fragment,e),u(Ae.$$.fragment,e),u(Oe.$$.fragment,e),u(Se.$$.fragment,e),u(De.$$.fragment,e),u(Xe.$$.fragment,e),u(Qe.$$.fragment,e),u(Ke.$$.fragment,e),u(eo.$$.fragment,e),u(oo.$$.fragment,e),u(to.$$.fragment,e),u(ao.$$.fragment,e),u(no.$$.fragment,e),u(pe.$$.fragment,e),u(ro.$$.fragment,e),u(so.$$.fragment,e),u(io.$$.fragment,e),u(fe.$$.fragment,e),u(ge.$$.fragment,e),u(lo.$$.fragment,e),u(he.$$.fragment,e),u(co.$$.fragment,e),u(ue.$$.fragment,e),u(mo.$$.fragment,e),u(po.$$.fragment,e),u(fo.$$.fragment,e),u(go.$$.fragment,e),u(ho.$$.fragment,e),u(uo.$$.fragment,e),u(_o.$$.fragment,e),u(vo.$$.fragment,e),u(_e.$$.fragment,e),u(ve.$$.fragment,e),u(bo.$$.fragment,e),u(ko.$$.fragment,e),u(To.$$.fragment,e),u(be.$$.fragment,e),u(ke.$$.fragment,e),u(Mo.$$.fragment,e),u(Fo.$$.fragment,e),u(xo.$$.fragment,e),u(Te.$$.fragment,e),u(Me.$$.fragment,e),ea=!0)},o(e){_(T.$$.fragment,e),_(M.$$.fragment,e),_(Ze.$$.fragment,e),_(Ue.$$.fragment,e),_(ne.$$.fragment,e),_(Je.$$.fragment,e),_(Ne.$$.fragment,e),_(Le.$$.fragment,e),_(se.$$.fragment,e),_(Ve.$$.fragment,e),_(Be.$$.fragment,e),_(ie.$$.fragment,e),_(Re.$$.fragment,e),_(Ge.$$.fragment,e),_(le.$$.fragment,e),_(qe.$$.fragment,e),_(He.$$.fragment,e),_(Ae.$$.fragment,e),_(Oe.$$.fragment,e),_(Se.$$.fragment,e),_(De.$$.fragment,e),_(Xe.$$.fragment,e),_(Qe.$$.fragment,e),_(Ke.$$.fragment,e),_(eo.$$.fragment,e),_(oo.$$.fragment,e),_(to.$$.fragment,e),_(ao.$$.fragment,e),_(no.$$.fragment,e),_(pe.$$.fragment,e),_(ro.$$.fragment,e),_(so.$$.fragment,e),_(io.$$.fragment,e),_(fe.$$.fragment,e),_(ge.$$.fragment,e),_(lo.$$.fragment,e),_(he.$$.fragment,e),_(co.$$.fragment,e),_(ue.$$.fragment,e),_(mo.$$.fragment,e),_(po.$$.fragment,e),_(fo.$$.fragment,e),_(go.$$.fragment,e),_(ho.$$.fragment,e),_(uo.$$.fragment,e),_(_o.$$.fragment,e),_(vo.$$.fragment,e),_(_e.$$.fragment,e),_(ve.$$.fragment,e),_(bo.$$.fragment,e),_(ko.$$.fragment,e),_(To.$$.fragment,e),_(be.$$.fragment,e),_(ke.$$.fragment,e),_(Mo.$$.fragment,e),_(Fo.$$.fragment,e),_(xo.$$.fragment,e),_(Te.$$.fragment,e),_(Me.$$.fragment,e),ea=!1},d(e){e&&(a(k),a(l),a(m),a(t),a(kt),a(Ce),a(Tt),a(Ie),a(Mt),a(Pe),a(Ft),a(We),a(xt),a(je),a(yt),a($t),a(C),a(wt),a(zt),a(I),a(Ct),a(It),a(P),a(Pt),a(Wt),a(W),a(jt),a(Zt),a(Ee),a(Ut),a(Jt),a(j),a(Nt),a(Lt),a(Ye),a(Vt),a(Bt),a(S),a(Rt),a(Gt),a(J),a(qt),a(Et),a(Z),a(Ht),a(At),a(w),a(Ot),a(St),a(D),a(Dt),a(Xt),a(X),a(Yt),a(Qt),a(Y),a(Kt),a(_t)),a(o),v(T,e),v(M,e),v(Ze,e),v(Ue),v(ne),v(Je),v(Ne,e),v(Le),v(se),v(Ve,e),v(Be),v(ie),v(Re,e),v(Ge),v(le),v(qe,e),v(He),v(Ae,e),v(Oe),v(Se),v(De),v(Xe,e),v(Qe),v(Ke,e),v(eo),v(oo),v(to,e),v(ao),v(no),v(pe),v(ro,e),v(so),v(io),v(fe),v(ge),v(lo),v(he),v(co),v(ue),v(mo,e),v(po),v(fo),v(go),v(ho),v(uo,e),v(_o),v(vo),v(_e),v(ve),v(bo,e),v(ko),v(To),v(be),v(ke),v(Mo,e),v(Fo),v(xo),v(Te),v(Me)}}}const vr='{"title":"FLAVA","local":"flava","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"FlavaConfig","local":"transformers.FlavaConfig","sections":[],"depth":2},{"title":"FlavaTextConfig","local":"transformers.FlavaTextConfig","sections":[],"depth":2},{"title":"FlavaImageConfig","local":"transformers.FlavaImageConfig","sections":[],"depth":2},{"title":"FlavaMultimodalConfig","local":"transformers.FlavaMultimodalConfig","sections":[],"depth":2},{"title":"FlavaImageCodebookConfig","local":"transformers.FlavaImageCodebookConfig","sections":[],"depth":2},{"title":"FlavaProcessor","local":"transformers.FlavaProcessor","sections":[],"depth":2},{"title":"FlavaFeatureExtractor","local":"transformers.FlavaFeatureExtractor","sections":[],"depth":2},{"title":"FlavaImageProcessor","local":"transformers.FlavaImageProcessor","sections":[],"depth":2},{"title":"FlavaForPreTraining","local":"transformers.FlavaForPreTraining","sections":[],"depth":2},{"title":"FlavaModel","local":"transformers.FlavaModel","sections":[],"depth":2},{"title":"FlavaImageCodebook","local":"transformers.FlavaImageCodebook","sections":[],"depth":2},{"title":"FlavaTextModel","local":"transformers.FlavaTextModel","sections":[],"depth":2},{"title":"FlavaImageModel","local":"transformers.FlavaImageModel","sections":[],"depth":2},{"title":"FlavaMultimodalModel","local":"transformers.FlavaMultimodalModel","sections":[],"depth":2}],"depth":1}';function br(F){return Qn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class wr extends Kn{constructor(o){super(),er(this,o,br,_r,Yn,{})}}export{wr as component};
