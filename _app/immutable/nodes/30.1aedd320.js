import{s as Or,o as Kr,n as xr}from"../chunks/scheduler.9bc65507.js";import{S as eo,i as to,g as a,s as o,r as g,A as ro,h as s,f as r,c as n,j as y,u as f,x as p,k as x,y as t,a as m,v as h,d as u,t as _,w as b}from"../chunks/index.707bf1b6.js";import{T as oo}from"../chunks/Tip.c2ecdbf4.js";import{D as M}from"../chunks/Docstring.17db21ae.js";import{C as Ar}from"../chunks/CodeBlock.54a9f38d.js";import{E as Gr}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as yr}from"../chunks/Heading.342b1fa6.js";function no(Q){let d,P="Examples:",w,$,I;return $=new Ar({props:{code:"JTIzJTIwV2UlMjBjYW4ndCUyMGluc3RhbnRpYXRlJTIwZGlyZWN0bHklMjB0aGUlMjBiYXNlJTIwY2xhc3MlMjAqSW1hZ2VQcm9jZXNzaW5nTWl4aW4qJTIwc28lMjBsZXQncyUyMHNob3clMjB0aGUlMjBleGFtcGxlcyUyMG9uJTIwYSUwQSUyMyUyMGRlcml2ZWQlMjBjbGFzcyUzQSUyMCpDTElQSW1hZ2VQcm9jZXNzb3IqJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQ0xJUEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIlMEEpJTIwJTIwJTIzJTIwRG93bmxvYWQlMjBpbWFnZV9wcm9jZXNzaW5nX2NvbmZpZyUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMENMSVBJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRiUyMiUwQSklMjAlMjAlMjMlMjBFLmcuJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjAob3IlMjBtb2RlbCklMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMENMSVBJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRnByZXByb2Nlc3Nvcl9jb25maWcuanNvbiUyMiklMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBDTElQSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiUyQyUyMGRvX25vcm1hbGl6ZSUzREZhbHNlJTJDJTIwZm9vJTNERmFsc2UlMEEpJTBBYXNzZXJ0JTIwaW1hZ2VfcHJvY2Vzc29yLmRvX25vcm1hbGl6ZSUyMGlzJTIwRmFsc2UlMEFpbWFnZV9wcm9jZXNzb3IlMkMlMjB1bnVzZWRfa3dhcmdzJTIwJTNEJTIwQ0xJUEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIlMkMlMjBkb19ub3JtYWxpemUlM0RGYWxzZSUyQyUyMGZvbyUzREZhbHNlJTJDJTIwcmV0dXJuX3VudXNlZF9rd2FyZ3MlM0RUcnVlJTBBKSUwQWFzc2VydCUyMGltYWdlX3Byb2Nlc3Nvci5kb19ub3JtYWxpemUlMjBpcyUyMEZhbHNlJTBBYXNzZXJ0JTIwdW51c2VkX2t3YXJncyUyMCUzRCUzRCUyMCU3QiUyMmZvbyUyMiUzQSUyMEZhbHNlJTdE",highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *ImageProcessingMixin* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *CLIPImageProcessor*</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>
)  <span class="hljs-comment"># Download image_processing_config from huggingface.co and cache.</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. image processor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
image_processor = CLIPImageProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){d=a("p"),d.textContent=P,w=o(),g($.$$.fragment)},l(c){d=s(c,"P",{"data-svelte-h":!0}),p(d)!=="svelte-kvfsh7"&&(d.textContent=P),w=n(c),f($.$$.fragment,c)},m(c,T){m(c,d,T),m(c,w,T),h($,c,T),I=!0},p:xr,i(c){I||(u($.$$.fragment,c),I=!0)},o(c){_($.$$.fragment,c),I=!1},d(c){c&&(r(d),r(w)),b($,c)}}}function ao(Q){let d,P="Examples:",w,$,I;return $=new Ar({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQWltYWdlJTIwcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBQdXNoJTIwdGhlJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjB0byUyMHlvdXIlMjBuYW1lc3BhY2UlMjB3aXRoJTIwdGhlJTIwbmFtZSUyMCUyMm15LWZpbmV0dW5lZC1iZXJ0JTIyLiUwQWltYWdlJTIwcHJvY2Vzc29yLnB1c2hfdG9faHViKCUyMm15LWZpbmV0dW5lZC1iZXJ0JTIyKSUwQSUwQSUyMyUyMFB1c2glMjB0aGUlMjBpbWFnZSUyMHByb2Nlc3NvciUyMHRvJTIwYW4lMjBvcmdhbml6YXRpb24lMjB3aXRoJTIwdGhlJTIwbmFtZSUyMCUyMm15LWZpbmV0dW5lZC1iZXJ0JTIyLiUwQWltYWdlJTIwcHJvY2Vzc29yLnB1c2hfdG9faHViKCUyMmh1Z2dpbmdmYWNlJTJGbXktZmluZXR1bmVkLWJlcnQlMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

image processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-comment"># Push the image processor to your namespace with the name &quot;my-finetuned-bert&quot;.</span>
image processor.push_to_hub(<span class="hljs-string">&quot;my-finetuned-bert&quot;</span>)

<span class="hljs-comment"># Push the image processor to an organization with the name &quot;my-finetuned-bert&quot;.</span>
image processor.push_to_hub(<span class="hljs-string">&quot;huggingface/my-finetuned-bert&quot;</span>)`,wrap:!1}}),{c(){d=a("p"),d.textContent=P,w=o(),g($.$$.fragment)},l(c){d=s(c,"P",{"data-svelte-h":!0}),p(d)!=="svelte-kvfsh7"&&(d.textContent=P),w=n(c),f($.$$.fragment,c)},m(c,T){m(c,d,T),m(c,w,T),h($,c,T),I=!0},p:xr,i(c){I||(u($.$$.fragment,c),I=!0)},o(c){_($.$$.fragment,c),I=!1},d(c){c&&(r(d),r(w)),b($,c)}}}function so(Q){let d,P="This API is experimental and may have some slight breaking changes in the next releases.";return{c(){d=a("p"),d.textContent=P},l(w){d=s(w,"P",{"data-svelte-h":!0}),p(d)!=="svelte-15rpg4"&&(d.textContent=P)},m(w,$){m(w,d,$)},p:xr,d(w){w&&r(d)}}}function io(Q){let d,P,w,$,I,c,T,$r=`This page lists all the utility functions used by the image processors, mainly the functional
transformations used to process the images.`,at,te,Mr="Most of those are only useful if you are studying the code of the image processors in the library.",st,re,it,N,oe,Zt,Pe,wr=`Crops the <code>image</code> to the specified <code>size</code> using a center crop. Note that if the image is too small to be cropped to
the size given, it will be padded (so the returned result will always be of size <code>size</code>).`,mt,C,ne,Nt,Ce,Ir="Converts bounding boxes from center format to corners format.",kt,je,Tr=`center format: contains the coordinate for the center of the box and its width, height dimensions
(center_x, center_y, width, height)
corners format: contains the coodinates for the top-left and bottom-right corners of the box
(top_left_x, top_left_y, bottom_right_x, bottom_right_y)`,ct,j,ae,Dt,Ue,Pr="Converts bounding boxes from corners format to center format.",Wt,ze,Cr=`corners format: contains the coordinates for the top-left and bottom-right corners of the box
(top_left_x, top_left_y, bottom_right_x, bottom_right_y)
center format: contains the coordinate for the center of the box and its the width, height dimensions
(center_x, center_y, width, height)`,dt,k,se,qt,Je,jr="Converts unique ID to RGB color.",lt,U,ie,Bt,Le,Ur="Normalizes <code>image</code> using the mean and standard deviation specified by <code>mean</code> and <code>std</code>.",Vt,Ze,zr="image = (image - mean) / std",pt,D,me,Ht,Ne,Jr="Pads the <code>image</code> with the specified (height, width) <code>padding</code> and <code>mode</code>.",gt,W,ce,Et,ke,Lr="Converts RGB color to unique ID.",ft,q,de,Qt,De,Zr="Rescales <code>image</code> by <code>scale</code>.",ht,B,le,Ft,We,Nr="Resizes <code>image</code> to <code>(height, width)</code> specified by <code>size</code> using the PIL library.",ut,V,pe,Rt,qe,kr=`Converts <code>image</code> to a PIL Image. Optionally rescales it and puts the channel dimension back as the last axis if
needed.`,_t,ge,bt,l,fe,Xt,Be,Dr=`This is an image processor mixin used to provide saving/loading functionality for sequential and image feature
extractors.`,St,z,he,Yt,Ve,Wr="Convert a single or a list of urls into the corresponding <code>PIL.Image</code> objects.",Gt,He,qr=`If a single url is passed, the return value will be a single object. If a list is passed a list of objects is
returned.`,At,F,ue,Ot,Ee,Br='Instantiates a type of <a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> from a Python dictionary of parameters.',Kt,R,_e,er,Qe,Vr=`Instantiates a image processor of type <a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> from the path to a JSON
file of parameters.`,tr,J,be,rr,Fe,Hr='Instantiate a type of <a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> from an image processor.',or,X,nr,S,ve,ar,Re,Er=`From a <code>pretrained_model_name_or_path</code>, resolve to a dictionary of parameters, to be used for instantiating a
image processor of type <code>~image_processor_utils.ImageProcessingMixin</code> using <code>from_dict</code>.`,sr,L,ye,ir,Xe,Qr="Upload the image processor file to the ðŸ¤— Model Hub.",mr,Y,cr,Z,xe,dr,Se,Fr=`Register this class with a given auto class. This should only be used for custom image processors as the ones
in the library are already mapped with <code>AutoImageProcessor </code>.`,lr,G,pr,A,$e,gr,Ye,Rr=`Save an image processor object to the directory <code>save_directory</code>, so that it can be re-loaded using the
<a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin.from_pretrained">from_pretrained()</a> class method.`,fr,O,Me,hr,Ge,Xr="Serializes this instance to a Python dictionary.",ur,K,we,_r,Ae,Sr="Save this instance to a JSON file.",br,ee,Ie,vr,Oe,Yr="Serializes this instance to a JSON string.",vt,nt,yt;return I=new yr({props:{title:"Utilities for Image Processors",local:"utilities-for-image-processors",headingTag:"h1"}}),re=new yr({props:{title:"Image Transformations",local:"transformers.image_transforms.center_crop",headingTag:"h2"}}),oe=new M({props:{name:"transformers.image_transforms.center_crop",anchor:"transformers.image_transforms.center_crop",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": Tuple"},{name:"data_format",val:": Union = None"},{name:"input_data_format",val:": Union = None"},{name:"return_numpy",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.image_transforms.center_crop.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to crop.`,name:"image"},{anchor:"transformers.image_transforms.center_crop.size",description:`<strong>size</strong> (<code>Tuple[int, int]</code>) &#x2014;
The target size for the cropped image.`,name:"size"},{anchor:"transformers.image_transforms.center_crop.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use the inferred format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_transforms.center_crop.input_data_format",description:`<strong>input_data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use the inferred format of the input image.</li>
</ul>`,name:"input_data_format"},{anchor:"transformers.image_transforms.center_crop.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the cropped image as a numpy array. Used for backwards compatibility with the
previous ImageFeatureExtractionMixin method.<ul>
<li>Unset: will return the same type as the input image.</li>
<li><code>True</code>: will return a numpy array.</li>
<li><code>False</code>: will return a <code>PIL.Image.Image</code> object.</li>
</ul>`,name:"return_numpy"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_transforms.py#L407",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The cropped image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),ne=new M({props:{name:"transformers.image_transforms.center_to_corners_format",anchor:"transformers.image_transforms.center_to_corners_format",parameters:[{name:"bboxes_center",val:": TensorType"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_transforms.py#L537"}}),ae=new M({props:{name:"transformers.image_transforms.corners_to_center_format",anchor:"transformers.image_transforms.corners_to_center_format",parameters:[{name:"bboxes_corners",val:": TensorType"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_transforms.py#L597"}}),se=new M({props:{name:"transformers.image_transforms.id_to_rgb",anchor:"transformers.image_transforms.id_to_rgb",parameters:[{name:"id_map",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_transforms.py#L631"}}),ie=new M({props:{name:"transformers.image_transforms.normalize",anchor:"transformers.image_transforms.normalize",parameters:[{name:"image",val:": ndarray"},{name:"mean",val:": Union"},{name:"std",val:": Union"},{name:"data_format",val:": Optional = None"},{name:"input_data_format",val:": Union = None"}],parametersDescription:[{anchor:"transformers.image_transforms.normalize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to normalize.`,name:"image"},{anchor:"transformers.image_transforms.normalize.mean",description:`<strong>mean</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
The mean to use for normalization.`,name:"mean"},{anchor:"transformers.image_transforms.normalize.std",description:`<strong>std</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
The standard deviation to use for normalization.`,name:"std"},{anchor:"transformers.image_transforms.normalize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If unset, will use the inferred format from the input.`,name:"data_format"},{anchor:"transformers.image_transforms.normalize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If unset, will use the inferred format from the input.`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_transforms.py#L347"}}),me=new M({props:{name:"transformers.image_transforms.pad",anchor:"transformers.image_transforms.pad",parameters:[{name:"image",val:": ndarray"},{name:"padding",val:": Union"},{name:"mode",val:": PaddingMode = <PaddingMode.CONSTANT: 'constant'>"},{name:"constant_values",val:": Union = 0.0"},{name:"data_format",val:": Union = None"},{name:"input_data_format",val:": Union = None"}],parametersDescription:[{anchor:"transformers.image_transforms.pad.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to pad.`,name:"image"},{anchor:"transformers.image_transforms.pad.padding",description:`<strong>padding</strong> (<code>int</code> or <code>Tuple[int, int]</code> or <code>Iterable[Tuple[int, int]]</code>) &#x2014;
Padding to apply to the edges of the height, width axes. Can be one of three formats:<ul>
<li><code>((before_height, after_height), (before_width, after_width))</code> unique pad widths for each axis.</li>
<li><code>((before, after),)</code> yields same before and after pad for height and width.</li>
<li><code>(pad,)</code> or int is a shortcut for before = after = pad width for all axes.</li>
</ul>`,name:"padding"},{anchor:"transformers.image_transforms.pad.mode",description:`<strong>mode</strong> (<code>PaddingMode</code>) &#x2014;
The padding mode to use. Can be one of:<ul>
<li><code>&quot;constant&quot;</code>: pads with a constant value.</li>
<li><code>&quot;reflect&quot;</code>: pads with the reflection of the vector mirrored on the first and last values of the
vector along each axis.</li>
<li><code>&quot;replicate&quot;</code>: pads with the replication of the last value on the edge of the array along each axis.</li>
<li><code>&quot;symmetric&quot;</code>: pads with the reflection of the vector mirrored along the edge of the array.</li>
</ul>`,name:"mode"},{anchor:"transformers.image_transforms.pad.constant_values",description:`<strong>constant_values</strong> (<code>float</code> or <code>Iterable[float]</code>, <em>optional</em>) &#x2014;
The value to use for the padding if <code>mode</code> is <code>&quot;constant&quot;</code>.`,name:"constant_values"},{anchor:"transformers.image_transforms.pad.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use same as the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_transforms.pad.input_data_format",description:`<strong>input_data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.
If unset, will use the inferred format of the input image.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_transforms.py#L661",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The padded image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),ce=new M({props:{name:"transformers.image_transforms.rgb_to_id",anchor:"transformers.image_transforms.rgb_to_id",parameters:[{name:"color",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_transforms.py#L620"}}),de=new M({props:{name:"transformers.image_transforms.rescale",anchor:"transformers.image_transforms.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": float"},{name:"data_format",val:": Optional = None"},{name:"dtype",val:": dtype = <class 'numpy.float32'>"},{name:"input_data_format",val:": Union = None"}],parametersDescription:[{anchor:"transformers.image_transforms.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to rescale.`,name:"image"},{anchor:"transformers.image_transforms.rescale.scale",description:`<strong>scale</strong> (<code>float</code>) &#x2014;
The scale to use for rescaling the image.`,name:"scale"},{anchor:"transformers.image_transforms.rescale.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the image. If not provided, it will be the same as the input image.`,name:"data_format"},{anchor:"transformers.image_transforms.rescale.dtype",description:`<strong>dtype</strong> (<code>np.dtype</code>, <em>optional</em>, defaults to <code>np.float32</code>) &#x2014;
The dtype of the output image. Defaults to <code>np.float32</code>. Used for backwards compatibility with feature
extractors.`,name:"dtype"},{anchor:"transformers.image_transforms.rescale.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If not provided, it will be inferred from the input image.`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_transforms.py#L92",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The rescaled image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),le=new M({props:{name:"transformers.image_transforms.resize",anchor:"transformers.image_transforms.resize",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": Tuple"},{name:"resample",val:": PILImageResampling = None"},{name:"reducing_gap",val:": Optional = None"},{name:"data_format",val:": Optional = None"},{name:"return_numpy",val:": bool = True"},{name:"input_data_format",val:": Union = None"}],parametersDescription:[{anchor:"transformers.image_transforms.resize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
The image to resize.`,name:"image"},{anchor:"transformers.image_transforms.resize.size",description:`<strong>size</strong> (<code>Tuple[int, int]</code>) &#x2014;
The size to use for resizing the image.`,name:"size"},{anchor:"transformers.image_transforms.resize.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>PILImageResampling.BILINEAR</code>) &#x2014;
The filter to user for resampling.`,name:"resample"},{anchor:"transformers.image_transforms.resize.reducing_gap",description:`<strong>reducing_gap</strong> (<code>int</code>, <em>optional</em>) &#x2014;
Apply optimization by resizing the image in two steps. The bigger <code>reducing_gap</code>, the closer the result to
the fair resampling. See corresponding Pillow documentation for more details.`,name:"reducing_gap"},{anchor:"transformers.image_transforms.resize.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the output image. If unset, will use the inferred format from the input.`,name:"data_format"},{anchor:"transformers.image_transforms.resize.return_numpy",description:`<strong>return_numpy</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to return the resized image as a numpy array. If False a <code>PIL.Image.Image</code> object is
returned.`,name:"return_numpy"},{anchor:"transformers.image_transforms.resize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If unset, will use the inferred format from the input.`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_transforms.py#L276",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The resized image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),pe=new M({props:{name:"transformers.image_transforms.to_pil_image",anchor:"transformers.image_transforms.to_pil_image",parameters:[{name:"image",val:": Union"},{name:"do_rescale",val:": Optional = None"},{name:"input_data_format",val:": Union = None"}],parametersDescription:[{anchor:"transformers.image_transforms.to_pil_image.image",description:`<strong>image</strong> (<code>PIL.Image.Image</code> or <code>numpy.ndarray</code> or <code>torch.Tensor</code> or <code>tf.Tensor</code>) &#x2014;
The image to convert to the <code>PIL.Image</code> format.`,name:"image"},{anchor:"transformers.image_transforms.to_pil_image.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to apply the scaling factor (to make pixel values integers between 0 and 255). Will default
to <code>True</code> if the image type is a floating type and casting to <code>int</code> would result in a loss of precision,
and <code>False</code> otherwise.`,name:"do_rescale"},{anchor:"transformers.image_transforms.to_pil_image.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format of the input image. If unset, will use the inferred format from the input.`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_transforms.py#L157",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The converted image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>PIL.Image.Image</code></p>
`}}),ge=new yr({props:{title:"ImageProcessingMixin",local:"transformers.ImageProcessingMixin",headingTag:"h2"}}),fe=new M({props:{name:"class transformers.ImageProcessingMixin",anchor:"transformers.ImageProcessingMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L68"}}),he=new M({props:{name:"fetch_images",anchor:"transformers.ImageProcessingMixin.fetch_images",parameters:[{name:"image_url_or_urls",val:": Union"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L522"}}),ue=new M({props:{name:"from_dict",anchor:"transformers.ImageProcessingMixin.from_dict",parameters:[{name:"image_processor_dict",val:": Dict"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_dict.image_processor_dict",description:`<strong>image_processor_dict</strong> (<code>Dict[str, Any]</code>) &#x2014;
Dictionary that will be used to instantiate the image processor object. Such a dictionary can be
retrieved from a pretrained checkpoint by leveraging the
<a href="/docs/transformers/main/en/internal/image_processing_utils#transformers.ImageProcessingMixin.to_dict">to_dict()</a> method.`,name:"image_processor_dict"},{anchor:"transformers.ImageProcessingMixin.from_dict.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>) &#x2014;
Additional parameters from which to initialize the image processor object.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L385",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image processor object instantiated from those
parameters.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a></p>
`}}),_e=new M({props:{name:"from_json_file",anchor:"transformers.ImageProcessingMixin.from_json_file",parameters:[{name:"json_file",val:": Union"}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_json_file.json_file",description:`<strong>json_file</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file containing the parameters.`,name:"json_file"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L442",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image_processor object
instantiated from that JSON file.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A image processor of type <a
  href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a></p>
`}}),be=new M({props:{name:"from_pretrained",anchor:"transformers.ImageProcessingMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": Union"},{name:"cache_dir",val:": Union = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": Union = None"},{name:"revision",val:": str = 'main'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained image_processor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a image processor file saved using the
<a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved image processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model image processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the image processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L95",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A image processor of type <a
  href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a>.</p>
`}}),X=new Gr({props:{anchor:"transformers.ImageProcessingMixin.from_pretrained.example",$$slots:{default:[no]},$$scope:{ctx:Q}}}),ve=new M({props:{name:"get_image_processor_dict",anchor:"transformers.ImageProcessingMixin.get_image_processor_dict",parameters:[{name:"pretrained_model_name_or_path",val:": Union"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.get_image_processor_dict.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.`,name:"pretrained_model_name_or_path"},{anchor:"transformers.ImageProcessingMixin.get_image_processor_dict.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&quot;</code>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can
specify the folder name here.`,name:"subfolder"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L270",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The dictionary(ies) that will be used to instantiate the image processor object.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Tuple[Dict, Dict]</code></p>
`}}),ye=new M({props:{name:"push_to_hub",anchor:"transformers.ImageProcessingMixin.push_to_hub",parameters:[{name:"repo_id",val:": str"},{name:"use_temp_dir",val:": Optional = None"},{name:"commit_message",val:": Optional = None"},{name:"private",val:": Optional = None"},{name:"token",val:": Union = None"},{name:"max_shard_size",val:": Union = '5GB'"},{name:"create_pr",val:": bool = False"},{name:"safe_serialization",val:": bool = True"},{name:"revision",val:": str = None"},{name:"commit_description",val:": str = None"},{name:"tags",val:": Optional = None"},{name:"**deprecated_kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.push_to_hub.repo_id",description:`<strong>repo_id</strong> (<code>str</code>) &#x2014;
The name of the repository you want to push your image processor to. It should contain your organization name
when pushing to a given organization.`,name:"repo_id"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.use_temp_dir",description:`<strong>use_temp_dir</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.
Will default to <code>True</code> if there is no directory named like <code>repo_id</code>, <code>False</code> otherwise.`,name:"use_temp_dir"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.commit_message",description:`<strong>commit_message</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Message to commit while pushing. Will default to <code>&quot;Upload image processor&quot;</code>.`,name:"commit_message"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.private",description:`<strong>private</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not the repository created should be private.`,name:"private"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.token",description:`<strong>token</strong> (<code>bool</code> or <code>str</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>). Will default to <code>True</code> if <code>repo_url</code>
is not specified.`,name:"token"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.max_shard_size",description:`<strong>max_shard_size</strong> (<code>int</code> or <code>str</code>, <em>optional</em>, defaults to <code>&quot;5GB&quot;</code>) &#x2014;
Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard
will then be each of size lower than this size. If expressed as a string, needs to be digits followed
by a unit (like <code>&quot;5MB&quot;</code>). We default it to <code>&quot;5GB&quot;</code> so that users can easily load models on free-tier
Google Colab instances without any CPU OOM issues.`,name:"max_shard_size"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.create_pr",description:`<strong>create_pr</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to create a PR with the uploaded files or directly commit.`,name:"create_pr"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.safe_serialization",description:`<strong>safe_serialization</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to convert the model weights in safetensors format for safer serialization.`,name:"safe_serialization"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Branch to push the uploaded files to.`,name:"revision"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.commit_description",description:`<strong>commit_description</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The description of the commit that will be created`,name:"commit_description"},{anchor:"transformers.ImageProcessingMixin.push_to_hub.tags",description:`<strong>tags</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of tags to push on the Hub.`,name:"tags"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/hub.py#L768"}}),Y=new Gr({props:{anchor:"transformers.ImageProcessingMixin.push_to_hub.example",$$slots:{default:[ao]},$$scope:{ctx:Q}}}),xe=new M({props:{name:"register_for_auto_class",anchor:"transformers.ImageProcessingMixin.register_for_auto_class",parameters:[{name:"auto_class",val:" = 'AutoImageProcessor'"}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.register_for_auto_class.auto_class",description:`<strong>auto_class</strong> (<code>str</code> or <code>type</code>, <em>optional</em>, defaults to <code>&quot;AutoImageProcessor &quot;</code>) &#x2014;
The auto class to register this new image processor with.`,name:"auto_class"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L496"}}),G=new oo({props:{warning:!0,$$slots:{default:[so]},$$scope:{ctx:Q}}}),$e=new M({props:{name:"save_pretrained",anchor:"transformers.ImageProcessingMixin.save_pretrained",parameters:[{name:"save_directory",val:": Union"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the image processor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L209"}}),Me=new M({props:{name:"to_dict",anchor:"transformers.ImageProcessingMixin.to_dict",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L430",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Dictionary of all the attributes that make up this image processor instance.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Dict[str, Any]</code></p>
`}}),we=new M({props:{name:"to_json_file",anchor:"transformers.ImageProcessingMixin.to_json_file",parameters:[{name:"json_file_path",val:": Union"}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.to_json_file.json_file_path",description:`<strong>json_file_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Path to the JSON file in which this image_processor instance&#x2019;s parameters will be saved.`,name:"json_file_path"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L482"}}),Ie=new M({props:{name:"to_json_string",anchor:"transformers.ImageProcessingMixin.to_json_string",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L461",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>String containing all the attributes that make up this feature_extractor instance in JSON format.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>str</code></p>
`}}),{c(){d=a("meta"),P=o(),w=a("p"),$=o(),g(I.$$.fragment),c=o(),T=a("p"),T.textContent=$r,at=o(),te=a("p"),te.textContent=Mr,st=o(),g(re.$$.fragment),it=o(),N=a("div"),g(oe.$$.fragment),Zt=o(),Pe=a("p"),Pe.innerHTML=wr,mt=o(),C=a("div"),g(ne.$$.fragment),Nt=o(),Ce=a("p"),Ce.textContent=Ir,kt=o(),je=a("p"),je.textContent=Tr,ct=o(),j=a("div"),g(ae.$$.fragment),Dt=o(),Ue=a("p"),Ue.textContent=Pr,Wt=o(),ze=a("p"),ze.textContent=Cr,dt=o(),k=a("div"),g(se.$$.fragment),qt=o(),Je=a("p"),Je.textContent=jr,lt=o(),U=a("div"),g(ie.$$.fragment),Bt=o(),Le=a("p"),Le.innerHTML=Ur,Vt=o(),Ze=a("p"),Ze.textContent=zr,pt=o(),D=a("div"),g(me.$$.fragment),Ht=o(),Ne=a("p"),Ne.innerHTML=Jr,gt=o(),W=a("div"),g(ce.$$.fragment),Et=o(),ke=a("p"),ke.textContent=Lr,ft=o(),q=a("div"),g(de.$$.fragment),Qt=o(),De=a("p"),De.innerHTML=Zr,ht=o(),B=a("div"),g(le.$$.fragment),Ft=o(),We=a("p"),We.innerHTML=Nr,ut=o(),V=a("div"),g(pe.$$.fragment),Rt=o(),qe=a("p"),qe.innerHTML=kr,_t=o(),g(ge.$$.fragment),bt=o(),l=a("div"),g(fe.$$.fragment),Xt=o(),Be=a("p"),Be.textContent=Dr,St=o(),z=a("div"),g(he.$$.fragment),Yt=o(),Ve=a("p"),Ve.innerHTML=Wr,Gt=o(),He=a("p"),He.textContent=qr,At=o(),F=a("div"),g(ue.$$.fragment),Ot=o(),Ee=a("p"),Ee.innerHTML=Br,Kt=o(),R=a("div"),g(_e.$$.fragment),er=o(),Qe=a("p"),Qe.innerHTML=Vr,tr=o(),J=a("div"),g(be.$$.fragment),rr=o(),Fe=a("p"),Fe.innerHTML=Hr,or=o(),g(X.$$.fragment),nr=o(),S=a("div"),g(ve.$$.fragment),ar=o(),Re=a("p"),Re.innerHTML=Er,sr=o(),L=a("div"),g(ye.$$.fragment),ir=o(),Xe=a("p"),Xe.textContent=Qr,mr=o(),g(Y.$$.fragment),cr=o(),Z=a("div"),g(xe.$$.fragment),dr=o(),Se=a("p"),Se.innerHTML=Fr,lr=o(),g(G.$$.fragment),pr=o(),A=a("div"),g($e.$$.fragment),gr=o(),Ye=a("p"),Ye.innerHTML=Rr,fr=o(),O=a("div"),g(Me.$$.fragment),hr=o(),Ge=a("p"),Ge.textContent=Xr,ur=o(),K=a("div"),g(we.$$.fragment),_r=o(),Ae=a("p"),Ae.textContent=Sr,br=o(),ee=a("div"),g(Ie.$$.fragment),vr=o(),Oe=a("p"),Oe.textContent=Yr,vt=o(),nt=a("p"),this.h()},l(e){const i=ro("svelte-u9bgzb",document.head);d=s(i,"META",{name:!0,content:!0}),i.forEach(r),P=n(e),w=s(e,"P",{}),y(w).forEach(r),$=n(e),f(I.$$.fragment,e),c=n(e),T=s(e,"P",{"data-svelte-h":!0}),p(T)!=="svelte-nip4ii"&&(T.textContent=$r),at=n(e),te=s(e,"P",{"data-svelte-h":!0}),p(te)!=="svelte-60lkqv"&&(te.textContent=Mr),st=n(e),f(re.$$.fragment,e),it=n(e),N=s(e,"DIV",{class:!0});var Te=y(N);f(oe.$$.fragment,Te),Zt=n(Te),Pe=s(Te,"P",{"data-svelte-h":!0}),p(Pe)!=="svelte-2qw57k"&&(Pe.innerHTML=wr),Te.forEach(r),mt=n(e),C=s(e,"DIV",{class:!0});var H=y(C);f(ne.$$.fragment,H),Nt=n(H),Ce=s(H,"P",{"data-svelte-h":!0}),p(Ce)!=="svelte-kjzox5"&&(Ce.textContent=Ir),kt=n(H),je=s(H,"P",{"data-svelte-h":!0}),p(je)!=="svelte-fnusyy"&&(je.textContent=Tr),H.forEach(r),ct=n(e),j=s(e,"DIV",{class:!0});var E=y(j);f(ae.$$.fragment,E),Dt=n(E),Ue=s(E,"P",{"data-svelte-h":!0}),p(Ue)!=="svelte-m5ejz9"&&(Ue.textContent=Pr),Wt=n(E),ze=s(E,"P",{"data-svelte-h":!0}),p(ze)!=="svelte-1isflmb"&&(ze.textContent=Cr),E.forEach(r),dt=n(e),k=s(e,"DIV",{class:!0});var xt=y(k);f(se.$$.fragment,xt),qt=n(xt),Je=s(xt,"P",{"data-svelte-h":!0}),p(Je)!=="svelte-19ts50v"&&(Je.textContent=jr),xt.forEach(r),lt=n(e),U=s(e,"DIV",{class:!0});var Ke=y(U);f(ie.$$.fragment,Ke),Bt=n(Ke),Le=s(Ke,"P",{"data-svelte-h":!0}),p(Le)!=="svelte-cgeryj"&&(Le.innerHTML=Ur),Vt=n(Ke),Ze=s(Ke,"P",{"data-svelte-h":!0}),p(Ze)!=="svelte-1vabayc"&&(Ze.textContent=zr),Ke.forEach(r),pt=n(e),D=s(e,"DIV",{class:!0});var $t=y(D);f(me.$$.fragment,$t),Ht=n($t),Ne=s($t,"P",{"data-svelte-h":!0}),p(Ne)!=="svelte-1v3jx5j"&&(Ne.innerHTML=Jr),$t.forEach(r),gt=n(e),W=s(e,"DIV",{class:!0});var Mt=y(W);f(ce.$$.fragment,Mt),Et=n(Mt),ke=s(Mt,"P",{"data-svelte-h":!0}),p(ke)!=="svelte-78trhr"&&(ke.textContent=Lr),Mt.forEach(r),ft=n(e),q=s(e,"DIV",{class:!0});var wt=y(q);f(de.$$.fragment,wt),Qt=n(wt),De=s(wt,"P",{"data-svelte-h":!0}),p(De)!=="svelte-skn2h6"&&(De.innerHTML=Zr),wt.forEach(r),ht=n(e),B=s(e,"DIV",{class:!0});var It=y(B);f(le.$$.fragment,It),Ft=n(It),We=s(It,"P",{"data-svelte-h":!0}),p(We)!=="svelte-avham3"&&(We.innerHTML=Nr),It.forEach(r),ut=n(e),V=s(e,"DIV",{class:!0});var Tt=y(V);f(pe.$$.fragment,Tt),Rt=n(Tt),qe=s(Tt,"P",{"data-svelte-h":!0}),p(qe)!=="svelte-e557ju"&&(qe.innerHTML=kr),Tt.forEach(r),_t=n(e),f(ge.$$.fragment,e),bt=n(e),l=s(e,"DIV",{class:!0});var v=y(l);f(fe.$$.fragment,v),Xt=n(v),Be=s(v,"P",{"data-svelte-h":!0}),p(Be)!=="svelte-16ht4m3"&&(Be.textContent=Dr),St=n(v),z=s(v,"DIV",{class:!0});var et=y(z);f(he.$$.fragment,et),Yt=n(et),Ve=s(et,"P",{"data-svelte-h":!0}),p(Ve)!=="svelte-2773v6"&&(Ve.innerHTML=Wr),Gt=n(et),He=s(et,"P",{"data-svelte-h":!0}),p(He)!=="svelte-1q8uymn"&&(He.textContent=qr),et.forEach(r),At=n(v),F=s(v,"DIV",{class:!0});var Pt=y(F);f(ue.$$.fragment,Pt),Ot=n(Pt),Ee=s(Pt,"P",{"data-svelte-h":!0}),p(Ee)!=="svelte-1ojiicg"&&(Ee.innerHTML=Br),Pt.forEach(r),Kt=n(v),R=s(v,"DIV",{class:!0});var Ct=y(R);f(_e.$$.fragment,Ct),er=n(Ct),Qe=s(Ct,"P",{"data-svelte-h":!0}),p(Qe)!=="svelte-1nh5w7q"&&(Qe.innerHTML=Vr),Ct.forEach(r),tr=n(v),J=s(v,"DIV",{class:!0});var tt=y(J);f(be.$$.fragment,tt),rr=n(tt),Fe=s(tt,"P",{"data-svelte-h":!0}),p(Fe)!=="svelte-p0wgo9"&&(Fe.innerHTML=Hr),or=n(tt),f(X.$$.fragment,tt),tt.forEach(r),nr=n(v),S=s(v,"DIV",{class:!0});var jt=y(S);f(ve.$$.fragment,jt),ar=n(jt),Re=s(jt,"P",{"data-svelte-h":!0}),p(Re)!=="svelte-d2g2ab"&&(Re.innerHTML=Er),jt.forEach(r),sr=n(v),L=s(v,"DIV",{class:!0});var rt=y(L);f(ye.$$.fragment,rt),ir=n(rt),Xe=s(rt,"P",{"data-svelte-h":!0}),p(Xe)!=="svelte-1qlyz3u"&&(Xe.textContent=Qr),mr=n(rt),f(Y.$$.fragment,rt),rt.forEach(r),cr=n(v),Z=s(v,"DIV",{class:!0});var ot=y(Z);f(xe.$$.fragment,ot),dr=n(ot),Se=s(ot,"P",{"data-svelte-h":!0}),p(Se)!=="svelte-ofyqve"&&(Se.innerHTML=Fr),lr=n(ot),f(G.$$.fragment,ot),ot.forEach(r),pr=n(v),A=s(v,"DIV",{class:!0});var Ut=y(A);f($e.$$.fragment,Ut),gr=n(Ut),Ye=s(Ut,"P",{"data-svelte-h":!0}),p(Ye)!=="svelte-67v12q"&&(Ye.innerHTML=Rr),Ut.forEach(r),fr=n(v),O=s(v,"DIV",{class:!0});var zt=y(O);f(Me.$$.fragment,zt),hr=n(zt),Ge=s(zt,"P",{"data-svelte-h":!0}),p(Ge)!=="svelte-1ww3wqq"&&(Ge.textContent=Xr),zt.forEach(r),ur=n(v),K=s(v,"DIV",{class:!0});var Jt=y(K);f(we.$$.fragment,Jt),_r=n(Jt),Ae=s(Jt,"P",{"data-svelte-h":!0}),p(Ae)!=="svelte-1g70y32"&&(Ae.textContent=Sr),Jt.forEach(r),br=n(v),ee=s(v,"DIV",{class:!0});var Lt=y(ee);f(Ie.$$.fragment,Lt),vr=n(Lt),Oe=s(Lt,"P",{"data-svelte-h":!0}),p(Oe)!=="svelte-5ayq1f"&&(Oe.textContent=Yr),Lt.forEach(r),v.forEach(r),vt=n(e),nt=s(e,"P",{}),y(nt).forEach(r),this.h()},h(){x(d,"name","hf:doc:metadata"),x(d,"content",mo),x(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(l,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,i){t(document.head,d),m(e,P,i),m(e,w,i),m(e,$,i),h(I,e,i),m(e,c,i),m(e,T,i),m(e,at,i),m(e,te,i),m(e,st,i),h(re,e,i),m(e,it,i),m(e,N,i),h(oe,N,null),t(N,Zt),t(N,Pe),m(e,mt,i),m(e,C,i),h(ne,C,null),t(C,Nt),t(C,Ce),t(C,kt),t(C,je),m(e,ct,i),m(e,j,i),h(ae,j,null),t(j,Dt),t(j,Ue),t(j,Wt),t(j,ze),m(e,dt,i),m(e,k,i),h(se,k,null),t(k,qt),t(k,Je),m(e,lt,i),m(e,U,i),h(ie,U,null),t(U,Bt),t(U,Le),t(U,Vt),t(U,Ze),m(e,pt,i),m(e,D,i),h(me,D,null),t(D,Ht),t(D,Ne),m(e,gt,i),m(e,W,i),h(ce,W,null),t(W,Et),t(W,ke),m(e,ft,i),m(e,q,i),h(de,q,null),t(q,Qt),t(q,De),m(e,ht,i),m(e,B,i),h(le,B,null),t(B,Ft),t(B,We),m(e,ut,i),m(e,V,i),h(pe,V,null),t(V,Rt),t(V,qe),m(e,_t,i),h(ge,e,i),m(e,bt,i),m(e,l,i),h(fe,l,null),t(l,Xt),t(l,Be),t(l,St),t(l,z),h(he,z,null),t(z,Yt),t(z,Ve),t(z,Gt),t(z,He),t(l,At),t(l,F),h(ue,F,null),t(F,Ot),t(F,Ee),t(l,Kt),t(l,R),h(_e,R,null),t(R,er),t(R,Qe),t(l,tr),t(l,J),h(be,J,null),t(J,rr),t(J,Fe),t(J,or),h(X,J,null),t(l,nr),t(l,S),h(ve,S,null),t(S,ar),t(S,Re),t(l,sr),t(l,L),h(ye,L,null),t(L,ir),t(L,Xe),t(L,mr),h(Y,L,null),t(l,cr),t(l,Z),h(xe,Z,null),t(Z,dr),t(Z,Se),t(Z,lr),h(G,Z,null),t(l,pr),t(l,A),h($e,A,null),t(A,gr),t(A,Ye),t(l,fr),t(l,O),h(Me,O,null),t(O,hr),t(O,Ge),t(l,ur),t(l,K),h(we,K,null),t(K,_r),t(K,Ae),t(l,br),t(l,ee),h(Ie,ee,null),t(ee,vr),t(ee,Oe),m(e,vt,i),m(e,nt,i),yt=!0},p(e,[i]){const Te={};i&2&&(Te.$$scope={dirty:i,ctx:e}),X.$set(Te);const H={};i&2&&(H.$$scope={dirty:i,ctx:e}),Y.$set(H);const E={};i&2&&(E.$$scope={dirty:i,ctx:e}),G.$set(E)},i(e){yt||(u(I.$$.fragment,e),u(re.$$.fragment,e),u(oe.$$.fragment,e),u(ne.$$.fragment,e),u(ae.$$.fragment,e),u(se.$$.fragment,e),u(ie.$$.fragment,e),u(me.$$.fragment,e),u(ce.$$.fragment,e),u(de.$$.fragment,e),u(le.$$.fragment,e),u(pe.$$.fragment,e),u(ge.$$.fragment,e),u(fe.$$.fragment,e),u(he.$$.fragment,e),u(ue.$$.fragment,e),u(_e.$$.fragment,e),u(be.$$.fragment,e),u(X.$$.fragment,e),u(ve.$$.fragment,e),u(ye.$$.fragment,e),u(Y.$$.fragment,e),u(xe.$$.fragment,e),u(G.$$.fragment,e),u($e.$$.fragment,e),u(Me.$$.fragment,e),u(we.$$.fragment,e),u(Ie.$$.fragment,e),yt=!0)},o(e){_(I.$$.fragment,e),_(re.$$.fragment,e),_(oe.$$.fragment,e),_(ne.$$.fragment,e),_(ae.$$.fragment,e),_(se.$$.fragment,e),_(ie.$$.fragment,e),_(me.$$.fragment,e),_(ce.$$.fragment,e),_(de.$$.fragment,e),_(le.$$.fragment,e),_(pe.$$.fragment,e),_(ge.$$.fragment,e),_(fe.$$.fragment,e),_(he.$$.fragment,e),_(ue.$$.fragment,e),_(_e.$$.fragment,e),_(be.$$.fragment,e),_(X.$$.fragment,e),_(ve.$$.fragment,e),_(ye.$$.fragment,e),_(Y.$$.fragment,e),_(xe.$$.fragment,e),_(G.$$.fragment,e),_($e.$$.fragment,e),_(Me.$$.fragment,e),_(we.$$.fragment,e),_(Ie.$$.fragment,e),yt=!1},d(e){e&&(r(P),r(w),r($),r(c),r(T),r(at),r(te),r(st),r(it),r(N),r(mt),r(C),r(ct),r(j),r(dt),r(k),r(lt),r(U),r(pt),r(D),r(gt),r(W),r(ft),r(q),r(ht),r(B),r(ut),r(V),r(_t),r(bt),r(l),r(vt),r(nt)),r(d),b(I,e),b(re,e),b(oe),b(ne),b(ae),b(se),b(ie),b(me),b(ce),b(de),b(le),b(pe),b(ge,e),b(fe),b(he),b(ue),b(_e),b(be),b(X),b(ve),b(ye),b(Y),b(xe),b(G),b($e),b(Me),b(we),b(Ie)}}}const mo='{"title":"Utilities for Image Processors","local":"utilities-for-image-processors","sections":[{"title":"Image Transformations","local":"transformers.image_transforms.center_crop","sections":[],"depth":2},{"title":"ImageProcessingMixin","local":"transformers.ImageProcessingMixin","sections":[],"depth":2}],"depth":1}';function co(Q){return Kr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bo extends eo{constructor(d){super(),to(this,d,co,io,Or,{})}}export{bo as component};
