import{s as Qt,o as Dt,n as Je}from"../chunks/scheduler.9bc65507.js";import{S as Kt,i as eo,g as l,s as n,r as u,A as to,h as d,f as o,c as a,j as W,u as f,x as p,k as q,y as i,a as r,v as g,d as _,t as b,w as y}from"../chunks/index.707bf1b6.js";import{T as St}from"../chunks/Tip.c2ecdbf4.js";import{D as _e}from"../chunks/Docstring.17db21ae.js";import{C as Ze}from"../chunks/CodeBlock.54a9f38d.js";import{E as Ot}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as U}from"../chunks/Heading.342b1fa6.js";function oo(C){let s,T="Example:",m,h,M;return h=new Ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFN0YWJsZUxtTW9kZWwlMkMlMjBTdGFibGVMbUNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBTdGFibGVMTSUyMHN0YWJsZWxtLTNiJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMFN0YWJsZUxtQ29uZmlnKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> StableLmModel, StableLmConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a StableLM stablelm-3b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = StableLmConfig()`,wrap:!1}}),{c(){s=l("p"),s.textContent=T,m=n(),u(h.$$.fragment)},l(c){s=d(c,"P",{"data-svelte-h":!0}),p(s)!=="svelte-11lpom8"&&(s.textContent=T),m=a(c),f(h.$$.fragment,c)},m(c,k){r(c,s,k),r(c,m,k),g(h,c,k),M=!0},p:Je,i(c){M||(_(h.$$.fragment,c),M=!0)},o(c){b(h.$$.fragment,c),M=!1},d(c){c&&(o(s),o(m)),y(h,c)}}}function no(C){let s,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=l("p"),s.innerHTML=T},l(m){s=d(m,"P",{"data-svelte-h":!0}),p(s)!=="svelte-fincs2"&&(s.innerHTML=T)},m(m,h){r(m,s,h)},p:Je,d(m){m&&o(s)}}}function ao(C){let s,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=l("p"),s.innerHTML=T},l(m){s=d(m,"P",{"data-svelte-h":!0}),p(s)!=="svelte-fincs2"&&(s.innerHTML=T)},m(m,h){r(m,s,h)},p:Je,d(m){m&&o(s)}}}function so(C){let s,T="Example:",m,h,M;return h=new Ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBTdGFibGVMbUZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBTdGFibGVMbUZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJzdGFiaWxpdHlhaSUyRnN0YWJsZWxtLTNiLTRlMXQlMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyc3RhYmlsaXR5YWklMkZzdGFibGVsbS0zYi00ZTF0JTIyKSUwQSUwQXByb21wdCUyMCUzRCUyMCUyMlRoZSUyMHdlYXRoZXIlMjBpcyUyMGFsd2F5cyUyMHdvbmRlcmZ1bCUyMGluJTIyJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHByb21wdCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBJTIzJTIwR2VuZXJhdGUlMEFnZW5lcmF0ZV9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZShpbnB1dHMuaW5wdXRfaWRzJTJDJTIwbWF4X2xlbmd0aCUzRDMwKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUlMkMlMjBjbGVhbl91cF90b2tlbml6YXRpb25fc3BhY2VzJTNERmFsc2UpJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, StableLmForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = StableLmForCausalLM.from_pretrained(<span class="hljs-string">&quot;stabilityai/stablelm-3b-4e1t&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;stabilityai/stablelm-3b-4e1t&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;The weather is always wonderful in&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Generate</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generate_ids = model.generate(inputs.input_ids, max_length=<span class="hljs-number">30</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generate_ids, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;The weather is always wonderful in the summer in the city of San Diego. The city is located on the coast of the Pacific Ocean and is surrounded by&#x27;</span>`,wrap:!1}}),{c(){s=l("p"),s.textContent=T,m=n(),u(h.$$.fragment)},l(c){s=d(c,"P",{"data-svelte-h":!0}),p(s)!=="svelte-11lpom8"&&(s.textContent=T),m=a(c),f(h.$$.fragment,c)},m(c,k){r(c,s,k),r(c,m,k),g(h,c,k),M=!0},p:Je,i(c){M||(_(h.$$.fragment,c),M=!0)},o(c){b(h.$$.fragment,c),M=!1},d(c){c&&(o(s),o(m)),y(h,c)}}}function ro(C){let s,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){s=l("p"),s.innerHTML=T},l(m){s=d(m,"P",{"data-svelte-h":!0}),p(s)!=="svelte-fincs2"&&(s.innerHTML=T)},m(m,h){r(m,s,h)},p:Je,d(m){m&&o(s)}}}function io(C){let s,T,m,h,M,c,k,Ie,X,Ct='<code>StableLM 3B 4E1T</code> was proposed in <a href="https://stability.wandb.io/stability-llm/stable-lm/reports/StableLM-3B-4E1T--VmlldzoyMjU4?accessToken=u3zujipenkx5g7rtcj9qojjgxpconyjktjkli2po09nffrffdhhchq045vp0wyfo" rel="nofollow"><code>StableLM 3B 4E1T</code>: Technical Report</a> by Stability AI and is the first model in a series of multi-epoch pre-trained language models.',We,V,qe,B,xt=`<code>StableLM 3B 4E1T</code> is a decoder-only base language model pre-trained on 1 trillion tokens of diverse English and code datasets for four epochs.
The model architecture is transformer-based with partial Rotary Position Embeddings, SwiGLU activation, LayerNorm, etc.`,Ue,E,zt="We also provide <code>StableLM Zephyr 3B</code>, an instruction fine-tuned version of the model that can be used for chat-based applications.",Ge,A,Pe,Y,Ft='<li>The architecture is similar to LLaMA but with RoPE applied to 25% of head embedding dimensions, LayerNorm instead of RMSNorm, and optional QKV bias terms.</li> <li><code>StableLM 3B 4E1T</code>-based models uses the same tokenizer as <a href="/docs/transformers/main/en/model_doc/gpt_neox#transformers.GPTNeoXTokenizerFast">GPTNeoXTokenizerFast</a>.</li>',Re,O,jt='<code>StableLM 3B 4E1T</code> and <code>StableLM Zephyr 3B</code> can be found on the <a href="https://huggingface.co/stabilityai" rel="nofollow">Huggingface Hub</a>',Ne,Q,Zt="The following code snippet demonstrates how to use <code>StableLM 3B 4E1T</code> for inference:",He,D,Xe,K,Ve,ee,Jt="First, make sure to install the latest version of Flash Attention v2.",Be,te,Ee,oe,It='Also make sure that your hardware is compatible with Flash-Attention 2. Read more about it in the official documentation of the <a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow"><code>flash-attn</code></a> repository. Note: you must load your model in half-precision (e.g. <code>torch.bfloat16</code>).',Ae,ne,Wt="Now, to run the model with Flash Attention 2, refer to the snippet below:",Ye,ae,Oe,se,Qe,$,re,rt,be,qt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmModel">~StableLmModel</a>.
It is used to instantiate an StableLM model according to the specified arguments, defining the model
architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of
the StableLM <a href="https://huggingface.co/stabilityai/stablelm-3b-4e1t" rel="nofollow">stabilityai/stablelm-3b-4e1t</a> architecture.`,it,ye,Ut=`Configuration objects inherit from  <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used
to control the model outputs. Read the documentation from  <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>
for more information.`,lt,G,De,ie,Ke,w,le,dt,Te,Gt=`The bare StableLm Model outputting raw hidden-states without any specific head on top.
This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ct,ve,Pt=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,mt,Me,Rt="Transformer decoder consisting of <em>config.num_hidden_layers</em> layers. Each layer is a <code>StableLmDecoderLayer</code>",pt,j,de,ht,ke,Nt='The <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmModel">StableLmModel</a> forward method, overrides the <code>__call__</code> special method.',ut,P,et,ce,tt,J,me,ft,x,pe,gt,we,Ht='The <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForCausalLM">StableLmForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',_t,R,bt,N,ot,he,nt,v,ue,yt,Le,Xt="The StableLm transformer with a sequence classification head on top (linear layer).",Tt,$e,Vt=`<a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForSequenceClassification">StableLmForSequenceClassification</a> uses the last token in order to do the classification, as other causal
models (e.g. GPT-2) do.`,vt,Se,Bt=`Since it does classification on the last token, it requires to know the position of the last token. If a
<code>pad_token_id</code> is defined in the configuration, it finds the last token that is not a padding token in each row. If
no <code>pad_token_id</code> is defined, it simply takes the last value in each row of the batch. Since it cannot guess the
padding tokens when <code>inputs_embeds</code> are passed instead of <code>input_ids</code>, it does the same (take the last value in
each row of the batch).`,Mt,Ce,Et=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,kt,xe,At=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,wt,Z,fe,Lt,ze,Yt='The <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmForSequenceClassification">StableLmForSequenceClassification</a> forward method, overrides the <code>__call__</code> special method.',$t,H,at,je,st;return M=new U({props:{title:"StableLM",local:"stablelm",headingTag:"h1"}}),k=new U({props:{title:"Overview",local:"overview",headingTag:"h2"}}),V=new U({props:{title:"Model Details",local:"model-details",headingTag:"h3"}}),A=new U({props:{title:"Usage Tips",local:"usage-tips",headingTag:"h3"}}),D=new Ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQWRldmljZSUyMCUzRCUyMCUyMmN1ZGElMjIlMjAlMjMlMjB0aGUlMjBkZXZpY2UlMjB0byUyMGxvYWQlMjB0aGUlMjBtb2RlbCUyMG9udG8lMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJzdGFiaWxpdHlhaSUyRnN0YWJsZWxtLTNiLTRlMXQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyc3RhYmlsaXR5YWklMkZzdGFibGVsbS0zYi00ZTF0JTIyKSUwQW1vZGVsLnRvKGRldmljZSklMEElMEFtb2RlbF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyVGhlJTIwd2VhdGhlciUyMGlzJTIwYWx3YXlzJTIwd29uZGVyZnVsJTIwaW4lMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzJTJDJTIwbWF4X2xlbmd0aCUzRDMyJTJDJTIwZG9fc2FtcGxlJTNEVHJ1ZSklMEFyZXNwb25zZXMlMjAlM0QlMjB0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEFyZXNwb25zZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-comment"># the device to load the model onto</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;stabilityai/stablelm-3b-4e1t&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;stabilityai/stablelm-3b-4e1t&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer(<span class="hljs-string">&quot;The weather is always wonderful in&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs, max_length=<span class="hljs-number">32</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>responses
[<span class="hljs-string">&#x27;The weather is always wonderful in Santa Barbara and, for visitors hoping to make the move to our beautiful seaside city, this town offers plenty of great places to...&#x27;</span>]`,wrap:!1}}),K=new U({props:{title:"Combining StableLM and Flash Attention 2",local:"combining-stablelm-and-flash-attention-2",headingTag:"h2"}}),te=new Ze({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1VJTIwZmxhc2gtYXR0biUyMC0tbm8tYnVpbGQtaXNvbGF0aW9u",highlighted:"pip install -U flash-attn --no-build-isolation",wrap:!1}}),ae=new Ze({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMCUyMyUyMHRoZSUyMGRldmljZSUyMHRvJTIwbG9hZCUyMHRoZSUyMG1vZGVsJTIwb250byUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMnN0YWJpbGl0eWFpJTJGc3RhYmxlbG0tM2ItNGUxdCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJzdGFiaWxpdHlhaSUyRnN0YWJsZWxtLTNiLTRlMXQlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmJmbG9hdDE2JTJDJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMmZsYXNoX2F0dGVudGlvbl8yJTIyKSUwQW1vZGVsLnRvKGRldmljZSklMEElMEFtb2RlbF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyVGhlJTIwd2VhdGhlciUyMGlzJTIwYWx3YXlzJTIwd29uZGVyZnVsJTIwaW4lMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byhtb2RlbC5kZXZpY2UpJTBBJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzJTJDJTIwbWF4X2xlbmd0aCUzRDMyJTJDJTIwZG9fc2FtcGxlJTNEVHJ1ZSklMEFyZXNwb25zZXMlMjAlM0QlMjB0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEFyZXNwb25zZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span>device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-comment"># the device to load the model onto</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;stabilityai/stablelm-3b-4e1t&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;stabilityai/stablelm-3b-4e1t&quot;</span>, torch_dtype=torch.bfloat16, attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)

<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer(<span class="hljs-string">&quot;The weather is always wonderful in&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(model.device)

<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs, max_length=<span class="hljs-number">32</span>, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>responses = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>responses
[<span class="hljs-string">&#x27;The weather is always wonderful in Santa Barbara and, for visitors hoping to make the move to our beautiful seaside city, this town offers plenty of great places to...&#x27;</span>]`,wrap:!1}}),se=new U({props:{title:"StableLmConfig",local:"transformers.StableLmConfig",headingTag:"h2"}}),re=new _e({props:{name:"class transformers.StableLmConfig",anchor:"transformers.StableLmConfig",parameters:[{name:"vocab_size",val:" = 50304"},{name:"intermediate_size",val:" = 6912"},{name:"hidden_size",val:" = 2560"},{name:"num_hidden_layers",val:" = 32"},{name:"num_attention_heads",val:" = 32"},{name:"num_key_value_heads",val:" = 32"},{name:"hidden_act",val:" = 'silu'"},{name:"max_position_embeddings",val:" = 4096"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"use_cache",val:" = True"},{name:"tie_word_embeddings",val:" = False"},{name:"rope_theta",val:" = 10000"},{name:"rope_scaling",val:" = None"},{name:"use_qkv_bias",val:" = False"},{name:"hidden_dropout",val:" = 0.0"},{name:"attention_dropout",val:" = 0.0"},{name:"partial_rotary_factor",val:" = 0.25"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.StableLmConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50304) &#x2014;
Vocabulary size of the StableLM model. Defines the number of different tokens that
can be represented by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmModel">StableLmModel</a>.`,name:"vocab_size"},{anchor:"transformers.StableLmConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 6912) &#x2014;
Dimension of the MLP representations.`,name:"intermediate_size"},{anchor:"transformers.StableLmConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2560) &#x2014;
Number of hidden layers in the Transformer decoder.`,name:"hidden_size"},{anchor:"transformers.StableLmConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the Transformer decoder.`,name:"num_hidden_layers"},{anchor:"transformers.StableLmConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.StableLmConfig.num_key_value_heads",description:`<strong>num_key_value_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
This is the number of key_value heads that should be used to implement Grouped Query Attention. If
<code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if
<code>num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed by meanpooling all the original heads within that group. For more details checkout [this paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to </code>num_attention_heads\`.`,name:"num_key_value_heads"},{anchor:"transformers.StableLmConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string).`,name:"hidden_act"},{anchor:"transformers.StableLmConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
The maximum sequence length that this model might ever be used with.
Typically set this to something large just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.StableLmConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing
all weight matrices.`,name:"initializer_range"},{anchor:"transformers.StableLmConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.StableLmConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions
(not used by all models). Only relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.StableLmConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the model&#x2019;s input and output word embeddings should be tied.`,name:"tie_word_embeddings"},{anchor:"transformers.StableLmConfig.rope_theta",description:`<strong>rope_theta</strong> (<code>float</code>, <em>optional</em>, defaults to <code>10000.0</code>) &#x2014;
The base period of the RoPE embeddings.`,name:"rope_theta"},{anchor:"transformers.StableLmConfig.rope_scaling",description:`<strong>rope_scaling</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling
strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is
<code>{&quot;type&quot;: strategy name, &quot;factor&quot;: scaling factor}</code>. When using this flag, don&#x2019;t update
<code>max_position_embeddings</code> to the expected new maximum. See the following thread for more information on how
these scaling strategies behave:
<a href="https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/" rel="nofollow">https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/</a>. This
is an experimental feature, subject to breaking API changes in future versions.`,name:"rope_scaling"},{anchor:"transformers.StableLmConfig.use_qkv_bias",description:`<strong>use_qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the model should use bias for qkv layers.`,name:"use_qkv_bias"},{anchor:"transformers.StableLmConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio after applying the MLP to the hidden states.`,name:"hidden_dropout"},{anchor:"transformers.StableLmConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.StableLmConfig.partial_rotary_factor",description:`<strong>partial_rotary_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 0.25) &#x2014;
Percentage of the query and keys which will have rotary embedding.`,name:"partial_rotary_factor"},{anchor:"transformers.StableLmConfig.bos_token_id",description:`<strong>bos_token_id</strong> (int, <em>optional</em>, defaults to 0) &#x2014;
The id of the <code>BOS</code> token in the vocabulary.`,name:"bos_token_id"},{anchor:"transformers.StableLmConfig.eos_token_id",description:`<strong>eos_token_id</strong> (int, <em>optional</em>, defaults to 0) &#x2014;
The id of the <code>EOS</code> token in the vocabulary.`,name:"eos_token_id"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/stablelm/configuration_stablelm.py#L29"}}),G=new Ot({props:{anchor:"transformers.StableLmConfig.example",$$slots:{default:[oo]},$$scope:{ctx:C}}}),ie=new U({props:{title:"StableLmModel",local:"transformers.StableLmModel",headingTag:"h2"}}),le=new _e({props:{name:"class transformers.StableLmModel",anchor:"transformers.StableLmModel",parameters:[{name:"config",val:": StableLmConfig"}],parametersDescription:[{anchor:"transformers.StableLmModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.
config &#x2014; StableLmConfig`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/stablelm/modeling_stablelm.py#L766"}}),de=new _e({props:{name:"forward",anchor:"transformers.StableLmModel.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.StableLmModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.StableLmModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.StableLmModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.StableLmModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <a href="/docs/transformers/main/en/internal/generation_utils#transformers.Cache">Cache</a> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.StableLmModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.StableLmModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.StableLmModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.StableLmModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.StableLmModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/stablelm/modeling_stablelm.py#L800"}}),P=new St({props:{$$slots:{default:[no]},$$scope:{ctx:C}}}),ce=new U({props:{title:"StableLmForCausalLM",local:"transformers.StableLmForCausalLM",headingTag:"h2"}}),me=new _e({props:{name:"class transformers.StableLmForCausalLM",anchor:"transformers.StableLmForCausalLM",parameters:[{name:"config",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/stablelm/modeling_stablelm.py#L926"}}),pe=new _e({props:{name:"forward",anchor:"transformers.StableLmForCausalLM.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.StableLmForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.StableLmForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.StableLmForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.StableLmForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <a href="/docs/transformers/main/en/internal/generation_utils#transformers.Cache">Cache</a> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.StableLmForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.StableLmForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.StableLmForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.StableLmForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.StableLmForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.</p>
<p>Args &#x2014;
labels (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Labels for computing the masked language modeling loss. Indices should either be in <code>[0, ..., config.vocab_size]</code> or -100 (see <code>input_ids</code> docstring). Tokens with indices set to <code>-100</code> are ignored
(masked), the loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/stablelm/modeling_stablelm.py#L963",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig"
>StableLmConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new St({props:{$$slots:{default:[ao]},$$scope:{ctx:C}}}),N=new Ot({props:{anchor:"transformers.StableLmForCausalLM.forward.example",$$slots:{default:[so]},$$scope:{ctx:C}}}),he=new U({props:{title:"StableLmForSequenceClassification",local:"transformers.StableLmForSequenceClassification",headingTag:"h2"}}),ue=new _e({props:{name:"class transformers.StableLmForSequenceClassification",anchor:"transformers.StableLmForSequenceClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.StableLmForSequenceClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/stablelm#transformers.StableLmConfig">StableLmConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/stablelm/modeling_stablelm.py#L1117"}}),fe=new _e({props:{name:"forward",anchor:"transformers.StableLmForSequenceClassification.forward",parameters:[{name:"input_ids",val:": LongTensor = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.StableLmForSequenceClassification.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.StableLmForSequenceClassification.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a></p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p>If <code>past_key_values</code> is used, optionally only the last <code>decoder_input_ids</code> have to be input (see
<code>past_key_values</code>).</p>
<p>If you want to change padding behavior, you should read <code>modeling_opt._prepare_decoder_attention_mask</code>
and modify to your needs. See diagram 1 in <a href="https://arxiv.org/abs/1910.13461" rel="nofollow">the paper</a> for more
information on the default strategy.</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.StableLmForSequenceClassification.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.n_positions - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.StableLmForSequenceClassification.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>Cache</code> or <code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>) &#x2014;
Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
blocks) that can be used to speed up sequential decoding. This typically consists in the <code>past_key_values</code>
returned by the model at a previous stage of decoding, when <code>use_cache=True</code> or <code>config.use_cache=True</code>.</p>
<p>Two formats are allowed:</p>
<ul>
<li>a <a href="/docs/transformers/main/en/internal/generation_utils#transformers.Cache">Cache</a> instance;</li>
<li>Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of
shape <code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>). This is also known as the legacy
cache format.</li>
</ul>
<p>The model will output the same cache format that is fed as input. If no <code>past_key_values</code> are passed, the
legacy cache format will be returned.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>input_ids</code> (those that don&#x2019;t
have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all <code>input_ids</code>
of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.StableLmForSequenceClassification.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.StableLmForSequenceClassification.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.StableLmForSequenceClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.StableLmForSequenceClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.StableLmForSequenceClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.StableLmForSequenceClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/stablelm/modeling_stablelm.py#L1149"}}),H=new St({props:{$$slots:{default:[ro]},$$scope:{ctx:C}}}),{c(){s=l("meta"),T=n(),m=l("p"),h=n(),u(M.$$.fragment),c=n(),u(k.$$.fragment),Ie=n(),X=l("p"),X.innerHTML=Ct,We=n(),u(V.$$.fragment),qe=n(),B=l("p"),B.innerHTML=xt,Ue=n(),E=l("p"),E.innerHTML=zt,Ge=n(),u(A.$$.fragment),Pe=n(),Y=l("ul"),Y.innerHTML=Ft,Re=n(),O=l("p"),O.innerHTML=jt,Ne=n(),Q=l("p"),Q.innerHTML=Zt,He=n(),u(D.$$.fragment),Xe=n(),u(K.$$.fragment),Ve=n(),ee=l("p"),ee.textContent=Jt,Be=n(),u(te.$$.fragment),Ee=n(),oe=l("p"),oe.innerHTML=It,Ae=n(),ne=l("p"),ne.textContent=Wt,Ye=n(),u(ae.$$.fragment),Oe=n(),u(se.$$.fragment),Qe=n(),$=l("div"),u(re.$$.fragment),rt=n(),be=l("p"),be.innerHTML=qt,it=n(),ye=l("p"),ye.innerHTML=Ut,lt=n(),u(G.$$.fragment),De=n(),u(ie.$$.fragment),Ke=n(),w=l("div"),u(le.$$.fragment),dt=n(),Te=l("p"),Te.innerHTML=Gt,ct=n(),ve=l("p"),ve.innerHTML=Pt,mt=n(),Me=l("p"),Me.innerHTML=Rt,pt=n(),j=l("div"),u(de.$$.fragment),ht=n(),ke=l("p"),ke.innerHTML=Nt,ut=n(),u(P.$$.fragment),et=n(),u(ce.$$.fragment),tt=n(),J=l("div"),u(me.$$.fragment),ft=n(),x=l("div"),u(pe.$$.fragment),gt=n(),we=l("p"),we.innerHTML=Ht,_t=n(),u(R.$$.fragment),bt=n(),u(N.$$.fragment),ot=n(),u(he.$$.fragment),nt=n(),v=l("div"),u(ue.$$.fragment),yt=n(),Le=l("p"),Le.textContent=Xt,Tt=n(),$e=l("p"),$e.innerHTML=Vt,vt=n(),Se=l("p"),Se.innerHTML=Bt,Mt=n(),Ce=l("p"),Ce.innerHTML=Et,kt=n(),xe=l("p"),xe.innerHTML=At,wt=n(),Z=l("div"),u(fe.$$.fragment),Lt=n(),ze=l("p"),ze.innerHTML=Yt,$t=n(),u(H.$$.fragment),at=n(),je=l("p"),this.h()},l(e){const t=to("svelte-u9bgzb",document.head);s=d(t,"META",{name:!0,content:!0}),t.forEach(o),T=a(e),m=d(e,"P",{}),W(m).forEach(o),h=a(e),f(M.$$.fragment,e),c=a(e),f(k.$$.fragment,e),Ie=a(e),X=d(e,"P",{"data-svelte-h":!0}),p(X)!=="svelte-p70wk3"&&(X.innerHTML=Ct),We=a(e),f(V.$$.fragment,e),qe=a(e),B=d(e,"P",{"data-svelte-h":!0}),p(B)!=="svelte-13e6wpg"&&(B.innerHTML=xt),Ue=a(e),E=d(e,"P",{"data-svelte-h":!0}),p(E)!=="svelte-ecv3xy"&&(E.innerHTML=zt),Ge=a(e),f(A.$$.fragment,e),Pe=a(e),Y=d(e,"UL",{"data-svelte-h":!0}),p(Y)!=="svelte-1lxeakf"&&(Y.innerHTML=Ft),Re=a(e),O=d(e,"P",{"data-svelte-h":!0}),p(O)!=="svelte-1a9jefj"&&(O.innerHTML=jt),Ne=a(e),Q=d(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-udtc57"&&(Q.innerHTML=Zt),He=a(e),f(D.$$.fragment,e),Xe=a(e),f(K.$$.fragment,e),Ve=a(e),ee=d(e,"P",{"data-svelte-h":!0}),p(ee)!=="svelte-pstkbw"&&(ee.textContent=Jt),Be=a(e),f(te.$$.fragment,e),Ee=a(e),oe=d(e,"P",{"data-svelte-h":!0}),p(oe)!=="svelte-8b9qy3"&&(oe.innerHTML=It),Ae=a(e),ne=d(e,"P",{"data-svelte-h":!0}),p(ne)!=="svelte-1w2ttra"&&(ne.textContent=Wt),Ye=a(e),f(ae.$$.fragment,e),Oe=a(e),f(se.$$.fragment,e),Qe=a(e),$=d(e,"DIV",{class:!0});var z=W($);f(re.$$.fragment,z),rt=a(z),be=d(z,"P",{"data-svelte-h":!0}),p(be)!=="svelte-m2vol3"&&(be.innerHTML=qt),it=a(z),ye=d(z,"P",{"data-svelte-h":!0}),p(ye)!=="svelte-thm8xj"&&(ye.innerHTML=Ut),lt=a(z),f(G.$$.fragment,z),z.forEach(o),De=a(e),f(ie.$$.fragment,e),Ke=a(e),w=d(e,"DIV",{class:!0});var S=W(w);f(le.$$.fragment,S),dt=a(S),Te=d(S,"P",{"data-svelte-h":!0}),p(Te)!=="svelte-1gvw3z2"&&(Te.innerHTML=Gt),ct=a(S),ve=d(S,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-hswkmf"&&(ve.innerHTML=Pt),mt=a(S),Me=d(S,"P",{"data-svelte-h":!0}),p(Me)!=="svelte-12wkov7"&&(Me.innerHTML=Rt),pt=a(S),j=d(S,"DIV",{class:!0});var I=W(j);f(de.$$.fragment,I),ht=a(I),ke=d(I,"P",{"data-svelte-h":!0}),p(ke)!=="svelte-nx0vhf"&&(ke.innerHTML=Nt),ut=a(I),f(P.$$.fragment,I),I.forEach(o),S.forEach(o),et=a(e),f(ce.$$.fragment,e),tt=a(e),J=d(e,"DIV",{class:!0});var ge=W(J);f(me.$$.fragment,ge),ft=a(ge),x=d(ge,"DIV",{class:!0});var F=W(x);f(pe.$$.fragment,F),gt=a(F),we=d(F,"P",{"data-svelte-h":!0}),p(we)!=="svelte-jv1ouv"&&(we.innerHTML=Ht),_t=a(F),f(R.$$.fragment,F),bt=a(F),f(N.$$.fragment,F),F.forEach(o),ge.forEach(o),ot=a(e),f(he.$$.fragment,e),nt=a(e),v=d(e,"DIV",{class:!0});var L=W(v);f(ue.$$.fragment,L),yt=a(L),Le=d(L,"P",{"data-svelte-h":!0}),p(Le)!=="svelte-1r4bxkh"&&(Le.textContent=Xt),Tt=a(L),$e=d(L,"P",{"data-svelte-h":!0}),p($e)!=="svelte-y2qvum"&&($e.innerHTML=Vt),vt=a(L),Se=d(L,"P",{"data-svelte-h":!0}),p(Se)!=="svelte-10ugs3m"&&(Se.innerHTML=Bt),Mt=a(L),Ce=d(L,"P",{"data-svelte-h":!0}),p(Ce)!=="svelte-6pahdo"&&(Ce.innerHTML=Et),kt=a(L),xe=d(L,"P",{"data-svelte-h":!0}),p(xe)!=="svelte-hswkmf"&&(xe.innerHTML=At),wt=a(L),Z=d(L,"DIV",{class:!0});var Fe=W(Z);f(fe.$$.fragment,Fe),Lt=a(Fe),ze=d(Fe,"P",{"data-svelte-h":!0}),p(ze)!=="svelte-3mka3v"&&(ze.innerHTML=Yt),$t=a(Fe),f(H.$$.fragment,Fe),Fe.forEach(o),L.forEach(o),at=a(e),je=d(e,"P",{}),W(je).forEach(o),this.h()},h(){q(s,"name","hf:doc:metadata"),q(s,"content",lo),q($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),q(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){i(document.head,s),r(e,T,t),r(e,m,t),r(e,h,t),g(M,e,t),r(e,c,t),g(k,e,t),r(e,Ie,t),r(e,X,t),r(e,We,t),g(V,e,t),r(e,qe,t),r(e,B,t),r(e,Ue,t),r(e,E,t),r(e,Ge,t),g(A,e,t),r(e,Pe,t),r(e,Y,t),r(e,Re,t),r(e,O,t),r(e,Ne,t),r(e,Q,t),r(e,He,t),g(D,e,t),r(e,Xe,t),g(K,e,t),r(e,Ve,t),r(e,ee,t),r(e,Be,t),g(te,e,t),r(e,Ee,t),r(e,oe,t),r(e,Ae,t),r(e,ne,t),r(e,Ye,t),g(ae,e,t),r(e,Oe,t),g(se,e,t),r(e,Qe,t),r(e,$,t),g(re,$,null),i($,rt),i($,be),i($,it),i($,ye),i($,lt),g(G,$,null),r(e,De,t),g(ie,e,t),r(e,Ke,t),r(e,w,t),g(le,w,null),i(w,dt),i(w,Te),i(w,ct),i(w,ve),i(w,mt),i(w,Me),i(w,pt),i(w,j),g(de,j,null),i(j,ht),i(j,ke),i(j,ut),g(P,j,null),r(e,et,t),g(ce,e,t),r(e,tt,t),r(e,J,t),g(me,J,null),i(J,ft),i(J,x),g(pe,x,null),i(x,gt),i(x,we),i(x,_t),g(R,x,null),i(x,bt),g(N,x,null),r(e,ot,t),g(he,e,t),r(e,nt,t),r(e,v,t),g(ue,v,null),i(v,yt),i(v,Le),i(v,Tt),i(v,$e),i(v,vt),i(v,Se),i(v,Mt),i(v,Ce),i(v,kt),i(v,xe),i(v,wt),i(v,Z),g(fe,Z,null),i(Z,Lt),i(Z,ze),i(Z,$t),g(H,Z,null),r(e,at,t),r(e,je,t),st=!0},p(e,[t]){const z={};t&2&&(z.$$scope={dirty:t,ctx:e}),G.$set(z);const S={};t&2&&(S.$$scope={dirty:t,ctx:e}),P.$set(S);const I={};t&2&&(I.$$scope={dirty:t,ctx:e}),R.$set(I);const ge={};t&2&&(ge.$$scope={dirty:t,ctx:e}),N.$set(ge);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),H.$set(F)},i(e){st||(_(M.$$.fragment,e),_(k.$$.fragment,e),_(V.$$.fragment,e),_(A.$$.fragment,e),_(D.$$.fragment,e),_(K.$$.fragment,e),_(te.$$.fragment,e),_(ae.$$.fragment,e),_(se.$$.fragment,e),_(re.$$.fragment,e),_(G.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(P.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(R.$$.fragment,e),_(N.$$.fragment,e),_(he.$$.fragment,e),_(ue.$$.fragment,e),_(fe.$$.fragment,e),_(H.$$.fragment,e),st=!0)},o(e){b(M.$$.fragment,e),b(k.$$.fragment,e),b(V.$$.fragment,e),b(A.$$.fragment,e),b(D.$$.fragment,e),b(K.$$.fragment,e),b(te.$$.fragment,e),b(ae.$$.fragment,e),b(se.$$.fragment,e),b(re.$$.fragment,e),b(G.$$.fragment,e),b(ie.$$.fragment,e),b(le.$$.fragment,e),b(de.$$.fragment,e),b(P.$$.fragment,e),b(ce.$$.fragment,e),b(me.$$.fragment,e),b(pe.$$.fragment,e),b(R.$$.fragment,e),b(N.$$.fragment,e),b(he.$$.fragment,e),b(ue.$$.fragment,e),b(fe.$$.fragment,e),b(H.$$.fragment,e),st=!1},d(e){e&&(o(T),o(m),o(h),o(c),o(Ie),o(X),o(We),o(qe),o(B),o(Ue),o(E),o(Ge),o(Pe),o(Y),o(Re),o(O),o(Ne),o(Q),o(He),o(Xe),o(Ve),o(ee),o(Be),o(Ee),o(oe),o(Ae),o(ne),o(Ye),o(Oe),o(Qe),o($),o(De),o(Ke),o(w),o(et),o(tt),o(J),o(ot),o(nt),o(v),o(at),o(je)),o(s),y(M,e),y(k,e),y(V,e),y(A,e),y(D,e),y(K,e),y(te,e),y(ae,e),y(se,e),y(re),y(G),y(ie,e),y(le),y(de),y(P),y(ce,e),y(me),y(pe),y(R),y(N),y(he,e),y(ue),y(fe),y(H)}}}const lo='{"title":"StableLM","local":"stablelm","sections":[{"title":"Overview","local":"overview","sections":[{"title":"Model Details","local":"model-details","sections":[],"depth":3},{"title":"Usage Tips","local":"usage-tips","sections":[],"depth":3}],"depth":2},{"title":"Combining StableLM and Flash Attention 2","local":"combining-stablelm-and-flash-attention-2","sections":[],"depth":2},{"title":"StableLmConfig","local":"transformers.StableLmConfig","sections":[],"depth":2},{"title":"StableLmModel","local":"transformers.StableLmModel","sections":[],"depth":2},{"title":"StableLmForCausalLM","local":"transformers.StableLmForCausalLM","sections":[],"depth":2},{"title":"StableLmForSequenceClassification","local":"transformers.StableLmForSequenceClassification","sections":[],"depth":2}],"depth":1}';function co(C){return Dt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bo extends Kt{constructor(s){super(),eo(this,s,co,io,Qt,{})}}export{bo as component};
