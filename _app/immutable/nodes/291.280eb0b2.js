import{s as Ze,o as Ee,n as ve}from"../chunks/scheduler.9bc65507.js";import{S as We,i as Je,g as c,s as i,r as v,A as Ie,h as m,f as n,c as r,j as X,u as w,x as _,k as O,y as u,a as s,v as $,d as M,t as T,w as y}from"../chunks/index.707bf1b6.js";import{T as Ue}from"../chunks/Tip.c2ecdbf4.js";import{D as be}from"../chunks/Docstring.17db21ae.js";import{C as ze}from"../chunks/CodeBlock.54a9f38d.js";import{E as je}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as de}from"../chunks/Heading.342b1fa6.js";function Pe(k){let a,h="Example:",d,l,p;return l=new ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpdERldENvbmZpZyUyQyUyMFZpdERldE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFZpdERldCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwVml0RGV0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMFZpdERldE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VitDetConfig, VitDetModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a VitDet configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = VitDetConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VitDetModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){a=c("p"),a.textContent=h,d=i(),v(l.$$.fragment)},l(o){a=m(o,"P",{"data-svelte-h":!0}),_(a)!=="svelte-11lpom8"&&(a.textContent=h),d=r(o),w(l.$$.fragment,o)},m(o,f){s(o,a,f),s(o,d,f),$(l,o,f),p=!0},p:ve,i(o){p||(M(l.$$.fragment,o),p=!0)},o(o){T(l.$$.fragment,o),p=!1},d(o){o&&(n(a),n(d)),y(l,o)}}}function Re(k){let a,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){a=c("p"),a.innerHTML=h},l(d){a=m(d,"P",{"data-svelte-h":!0}),_(a)!=="svelte-fincs2"&&(a.innerHTML=h)},m(d,l){s(d,a,l)},p:ve,d(d){d&&n(a)}}}function Le(k){let a,h="Examples:",d,l,p;return l=new ze({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFZpdERldENvbmZpZyUyQyUyMFZpdERldE1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEElMEFjb25maWclMjAlM0QlMjBWaXREZXRDb25maWcoKSUwQW1vZGVsJTIwJTNEJTIwVml0RGV0TW9kZWwoY29uZmlnKSUwQSUwQXBpeGVsX3ZhbHVlcyUyMCUzRCUyMHRvcmNoLnJhbmRuKDElMkMlMjAzJTJDJTIwMjI0JTJDJTIwMjI0KSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwocGl4ZWxfdmFsdWVzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> VitDetConfig, VitDetModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>config = VitDetConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>model = VitDetModel(config)

<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(pixel_values)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">768</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>]`,wrap:!1}}),{c(){a=c("p"),a.textContent=h,d=i(),v(l.$$.fragment)},l(o){a=m(o,"P",{"data-svelte-h":!0}),_(a)!=="svelte-kvfsh7"&&(a.textContent=h),d=r(o),w(l.$$.fragment,o)},m(o,f){s(o,a,f),s(o,d,f),$(l,o,f),p=!0},p:ve,i(o){p||(M(l.$$.fragment,o),p=!0)},o(o){T(l.$$.fragment,o),p=!1},d(o){o&&(n(a),n(d)),y(l,o)}}}function Fe(k){let a,h,d,l,p,o,f,Q,E,we=`The ViTDet model was proposed in <a href="https://arxiv.org/abs/2203.16527" rel="nofollow">Exploring Plain Vision Transformer Backbones for Object Detection</a> by Yanghao Li, Hanzi Mao, Ross Girshick, Kaiming He.
VitDet leverages the plain <a href="vit">Vision Transformer</a> for the task of object detection.`,Y,W,$e="The abstract from the paper is the following:",K,J,Me="<em>We explore the plain, non-hierarchical Vision Transformer (ViT) as a backbone network for object detection. This design enables the original ViT architecture to be fine-tuned for object detection without needing to redesign a hierarchical backbone for pre-training. With minimal adaptations for fine-tuning, our plain-backbone detector can achieve competitive results. Surprisingly, we observe: (i) it is sufficient to build a simple feature pyramid from a single-scale feature map (without the common FPN design) and (ii) it is sufficient to use window attention (without shifting) aided with very few cross-window propagation blocks. With plain ViT backbones pre-trained as Masked Autoencoders (MAE), our detector, named ViTDet, can compete with the previous leading methods that were all based on hierarchical backbones, reaching up to 61.3 AP_box on the COCO dataset using only ImageNet-1K pre-training. We hope our study will draw attention to research on plain-backbone detectors.</em>",ee,I,Te=`This model was contributed by <a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>.
The original code can be found <a href="https://github.com/facebookresearch/detectron2/tree/main/projects/ViTDet" rel="nofollow">here</a>.`,te,U,ye="Tips:",ne,P,Ve="<li>At the moment, only the backbone is available.</li>",oe,R,ae,g,L,ce,q,Ce=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetModel">VitDetModel</a>. It is used to instantiate an
VitDet model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the VitDet
<a href="https://huggingface.co/google/vitdet-base-patch16-224" rel="nofollow">google/vitdet-base-patch16-224</a> architecture.`,me,B,xe=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,pe,j,se,F,ie,V,G,fe,N,De=`The bare VitDet Transformer model outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,ue,b,H,he,S,ke='The <a href="/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetModel">VitDetModel</a> forward method, overrides the <code>__call__</code> special method.',ge,z,_e,Z,re,A,le;return p=new de({props:{title:"ViTDet",local:"vitdet",headingTag:"h1"}}),f=new de({props:{title:"Overview",local:"overview",headingTag:"h2"}}),R=new de({props:{title:"VitDetConfig",local:"transformers.VitDetConfig",headingTag:"h2"}}),L=new be({props:{name:"class transformers.VitDetConfig",anchor:"transformers.VitDetConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"mlp_ratio",val:" = 4"},{name:"hidden_act",val:" = 'gelu'"},{name:"dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-06"},{name:"image_size",val:" = 224"},{name:"pretrain_image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"drop_path_rate",val:" = 0.0"},{name:"window_block_indices",val:" = []"},{name:"residual_block_indices",val:" = []"},{name:"use_absolute_position_embeddings",val:" = True"},{name:"use_relative_position_embeddings",val:" = False"},{name:"window_size",val:" = 0"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.VitDetConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.VitDetConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.VitDetConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.VitDetConfig.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Ratio of mlp hidden dim to embedding dim.`,name:"mlp_ratio"},{anchor:"transformers.VitDetConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.VitDetConfig.dropout_prob",description:`<strong>dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout_prob"},{anchor:"transformers.VitDetConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.VitDetConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.VitDetConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.VitDetConfig.pretrain_image_size",description:`<strong>pretrain_image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image during pretraining.`,name:"pretrain_image_size"},{anchor:"transformers.VitDetConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.VitDetConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.VitDetConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.VitDetConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Stochastic depth rate.`,name:"drop_path_rate"},{anchor:"transformers.VitDetConfig.window_block_indices",description:`<strong>window_block_indices</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[]</code>) &#x2014;
List of indices of blocks that should have window attention instead of regular global self-attention.`,name:"window_block_indices"},{anchor:"transformers.VitDetConfig.residual_block_indices",description:`<strong>residual_block_indices</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[]</code>) &#x2014;
List of indices of blocks that should have an extra residual block after the MLP.`,name:"residual_block_indices"},{anchor:"transformers.VitDetConfig.use_absolute_position_embeddings",description:`<strong>use_absolute_position_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add absolute position embeddings to the patch embeddings.`,name:"use_absolute_position_embeddings"},{anchor:"transformers.VitDetConfig.use_relative_position_embeddings",description:`<strong>use_relative_position_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to add relative position embeddings to the attention maps.`,name:"use_relative_position_embeddings"},{anchor:"transformers.VitDetConfig.window_size",description:`<strong>window_size</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The size of the attention window.`,name:"window_size"},{anchor:"transformers.VitDetConfig.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.VitDetConfig.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/vitdet/configuration_vitdet.py#L30"}}),j=new je({props:{anchor:"transformers.VitDetConfig.example",$$slots:{default:[Pe]},$$scope:{ctx:k}}}),F=new de({props:{title:"VitDetModel",local:"transformers.VitDetModel",headingTag:"h2"}}),G=new be({props:{name:"class transformers.VitDetModel",anchor:"transformers.VitDetModel",parameters:[{name:"config",val:": VitDetConfig"}],parametersDescription:[{anchor:"transformers.VitDetModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetConfig">VitDetConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/vitdet/modeling_vitdet.py#L698"}}),H=new be({props:{name:"forward",anchor:"transformers.VitDetModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.VitDetModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.VitDetModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.VitDetModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.VitDetModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.VitDetModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/vitdet/modeling_vitdet.py#L724",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/vitdet#transformers.VitDetConfig"
>VitDetConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput"
>transformers.modeling_outputs.BaseModelOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),z=new Ue({props:{$$slots:{default:[Re]},$$scope:{ctx:k}}}),Z=new je({props:{anchor:"transformers.VitDetModel.forward.example",$$slots:{default:[Le]},$$scope:{ctx:k}}}),{c(){a=c("meta"),h=i(),d=c("p"),l=i(),v(p.$$.fragment),o=i(),v(f.$$.fragment),Q=i(),E=c("p"),E.innerHTML=we,Y=i(),W=c("p"),W.textContent=$e,K=i(),J=c("p"),J.innerHTML=Me,ee=i(),I=c("p"),I.innerHTML=Te,te=i(),U=c("p"),U.textContent=ye,ne=i(),P=c("ul"),P.innerHTML=Ve,oe=i(),v(R.$$.fragment),ae=i(),g=c("div"),v(L.$$.fragment),ce=i(),q=c("p"),q.innerHTML=Ce,me=i(),B=c("p"),B.innerHTML=xe,pe=i(),v(j.$$.fragment),se=i(),v(F.$$.fragment),ie=i(),V=c("div"),v(G.$$.fragment),fe=i(),N=c("p"),N.innerHTML=De,ue=i(),b=c("div"),v(H.$$.fragment),he=i(),S=c("p"),S.innerHTML=ke,ge=i(),v(z.$$.fragment),_e=i(),v(Z.$$.fragment),re=i(),A=c("p"),this.h()},l(e){const t=Ie("svelte-u9bgzb",document.head);a=m(t,"META",{name:!0,content:!0}),t.forEach(n),h=r(e),d=m(e,"P",{}),X(d).forEach(n),l=r(e),w(p.$$.fragment,e),o=r(e),w(f.$$.fragment,e),Q=r(e),E=m(e,"P",{"data-svelte-h":!0}),_(E)!=="svelte-brze68"&&(E.innerHTML=we),Y=r(e),W=m(e,"P",{"data-svelte-h":!0}),_(W)!=="svelte-vfdo9a"&&(W.textContent=$e),K=r(e),J=m(e,"P",{"data-svelte-h":!0}),_(J)!=="svelte-dasg8s"&&(J.innerHTML=Me),ee=r(e),I=m(e,"P",{"data-svelte-h":!0}),_(I)!=="svelte-14aeur3"&&(I.innerHTML=Te),te=r(e),U=m(e,"P",{"data-svelte-h":!0}),_(U)!=="svelte-axv494"&&(U.textContent=ye),ne=r(e),P=m(e,"UL",{"data-svelte-h":!0}),_(P)!=="svelte-1cc6n3i"&&(P.innerHTML=Ve),oe=r(e),w(R.$$.fragment,e),ae=r(e),g=m(e,"DIV",{class:!0});var C=X(g);w(L.$$.fragment,C),ce=r(C),q=m(C,"P",{"data-svelte-h":!0}),_(q)!=="svelte-3q709r"&&(q.innerHTML=Ce),me=r(C),B=m(C,"P",{"data-svelte-h":!0}),_(B)!=="svelte-o55m63"&&(B.innerHTML=xe),pe=r(C),w(j.$$.fragment,C),C.forEach(n),se=r(e),w(F.$$.fragment,e),ie=r(e),V=m(e,"DIV",{class:!0});var D=X(V);w(G.$$.fragment,D),fe=r(D),N=m(D,"P",{"data-svelte-h":!0}),_(N)!=="svelte-1cui7vp"&&(N.innerHTML=De),ue=r(D),b=m(D,"DIV",{class:!0});var x=X(b);w(H.$$.fragment,x),he=r(x),S=m(x,"P",{"data-svelte-h":!0}),_(S)!=="svelte-1ita1f7"&&(S.innerHTML=ke),ge=r(x),w(z.$$.fragment,x),_e=r(x),w(Z.$$.fragment,x),x.forEach(n),D.forEach(n),re=r(e),A=m(e,"P",{}),X(A).forEach(n),this.h()},h(){O(a,"name","hf:doc:metadata"),O(a,"content",Ge),O(g,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){u(document.head,a),s(e,h,t),s(e,d,t),s(e,l,t),$(p,e,t),s(e,o,t),$(f,e,t),s(e,Q,t),s(e,E,t),s(e,Y,t),s(e,W,t),s(e,K,t),s(e,J,t),s(e,ee,t),s(e,I,t),s(e,te,t),s(e,U,t),s(e,ne,t),s(e,P,t),s(e,oe,t),$(R,e,t),s(e,ae,t),s(e,g,t),$(L,g,null),u(g,ce),u(g,q),u(g,me),u(g,B),u(g,pe),$(j,g,null),s(e,se,t),$(F,e,t),s(e,ie,t),s(e,V,t),$(G,V,null),u(V,fe),u(V,N),u(V,ue),u(V,b),$(H,b,null),u(b,he),u(b,S),u(b,ge),$(z,b,null),u(b,_e),$(Z,b,null),s(e,re,t),s(e,A,t),le=!0},p(e,[t]){const C={};t&2&&(C.$$scope={dirty:t,ctx:e}),j.$set(C);const D={};t&2&&(D.$$scope={dirty:t,ctx:e}),z.$set(D);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),Z.$set(x)},i(e){le||(M(p.$$.fragment,e),M(f.$$.fragment,e),M(R.$$.fragment,e),M(L.$$.fragment,e),M(j.$$.fragment,e),M(F.$$.fragment,e),M(G.$$.fragment,e),M(H.$$.fragment,e),M(z.$$.fragment,e),M(Z.$$.fragment,e),le=!0)},o(e){T(p.$$.fragment,e),T(f.$$.fragment,e),T(R.$$.fragment,e),T(L.$$.fragment,e),T(j.$$.fragment,e),T(F.$$.fragment,e),T(G.$$.fragment,e),T(H.$$.fragment,e),T(z.$$.fragment,e),T(Z.$$.fragment,e),le=!1},d(e){e&&(n(h),n(d),n(l),n(o),n(Q),n(E),n(Y),n(W),n(K),n(J),n(ee),n(I),n(te),n(U),n(ne),n(P),n(oe),n(ae),n(g),n(se),n(ie),n(V),n(re),n(A)),n(a),y(p,e),y(f,e),y(R,e),y(L),y(j),y(F,e),y(G),y(H),y(z),y(Z)}}}const Ge='{"title":"ViTDet","local":"vitdet","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"VitDetConfig","local":"transformers.VitDetConfig","sections":[],"depth":2},{"title":"VitDetModel","local":"transformers.VitDetModel","sections":[],"depth":2}],"depth":1}';function He(k){return Ee(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Qe extends We{constructor(a){super(),Je(this,a,He,Fe,Ze,{})}}export{Qe as component};
