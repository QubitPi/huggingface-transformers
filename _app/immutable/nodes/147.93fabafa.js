import{s as Jt,o as yt,n as ye}from"../chunks/scheduler.9bc65507.js";import{S as Mt,i as Ut,g as p,s as r,r as h,A as kt,h as m,f as a,c as i,j as P,u,x as M,k as V,y as c,a as l,v as g,d as f,t as _,w as T}from"../chunks/index.707bf1b6.js";import{T as bt}from"../chunks/Tip.c2ecdbf4.js";import{D as pe}from"../chunks/Docstring.17db21ae.js";import{C as Ue}from"../chunks/CodeBlock.54a9f38d.js";import{E as Ie}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as S}from"../chunks/Heading.342b1fa6.js";function vt(k){let t,b;return t=new Ue({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVE5lb1hKYXBhbmVzZUNvbmZpZyUyQyUyMEdQVE5lb1hKYXBhbmVzZU1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEdQVE5lb1hKYXBhbmVzZSUyMGdwdC1uZW94LWphcGFuZXNlLTIuN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwR1BUTmVvWEphcGFuZXNlQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGdwdC1uZW94LWphcGFuZXNlLTIuN2IlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEdQVE5lb1hKYXBhbmVzZU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoXJapaneseConfig, GPTNeoXJapaneseModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a GPTNeoXJapanese gpt-neox-japanese-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = GPTNeoXJapaneseConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the gpt-neox-japanese-2.7b style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){h(t.$$.fragment)},l(s){u(t.$$.fragment,s)},m(s,d){g(t,s,d),b=!0},p:ye,i(s){b||(f(t.$$.fragment,s),b=!0)},o(s){_(t.$$.fragment,s),b=!1},d(s){T(t,s)}}}function wt(k){let t,b="Example:",s,d,J;return d=new Ue({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVE5lb1hKYXBhbmVzZVRva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEdQVE5lb1hKYXBhbmVzZVRva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQSUyMyUyMFlvdSUyMGNhbiUyMGNvbmZpcm0lMjBib3RoJTIwJUU2JTg1JUI2JUU1JUJGJTlDJTIwYW5kJTIwJUU2JTg1JUI2JUU2JTg3JTg5JTIwYXJlJTIwZW5jb2RlZCUyMHRvJTIwMTc3NDklMEF0b2tlbml6ZXIoJTIyJUU1JTkwJUJFJUU4JUJDJUE5JUUzJTgxJUFGJUU3JThDJUFCJUUzJTgxJUE3JUUzJTgxJTgyJUUzJTgyJThCJUYwJTlGJTkwJUFGJUUzJTgwJTgyJUU1JUFFJTlGJUUzJTgxJUFGJUU2JTg1JUI2JUU1JUJGJTlDKCVFNiU4NSVCNiVFNiU4NyU4OSklRTUlQTQlQTclRTUlQUQlQTYlRTUlODclQkElRTglQkElQUIlMjIpJTVCJTIyaW5wdXRfaWRzJTIyJTVEJTBBJTBBJTIzJTIwQm90aCUyMCVFNiU4NSVCNiVFNSVCRiU5QyUyMGFuZCUyMCVFNiU4NSVCNiVFNiU4NyU4OSUyMGFyZSUyMGRlY29kZWQlMjB0byUyMCVFNiU4NSVCNiVFNSVCRiU5QyUwQXRva2VuaXplci5kZWNvZGUodG9rZW5pemVyKCUyMiVFNSU5MCVCRSVFOCVCQyVBOSVFMyU4MSVBRiVFNyU4QyVBQiVFMyU4MSVBNyVFMyU4MSU4MiVFMyU4MiU4QiVGMCU5RiU5MCVBRiVFMyU4MCU4MiVFNSVBRSU5RiVFMyU4MSVBRiVFNiU4NSVCNiVFNSVCRiU5QyglRTYlODUlQjYlRTYlODclODkpJUU1JUE0JUE3JUU1JUFEJUE2JUU1JTg3JUJBJUU4JUJBJUFCJTIyKSU1QiUyMmlucHV0X2lkcyUyMiU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoXJapaneseTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># You can confirm both ÊÖ∂Âøú and ÊÖ∂Êáâ are encoded to 17749</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer(<span class="hljs-string">&quot;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]
[<span class="hljs-number">30014</span>, <span class="hljs-number">26883</span>, <span class="hljs-number">26638</span>, <span class="hljs-number">27228</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26650</span>, <span class="hljs-number">31732</span>, <span class="hljs-number">31679</span>, <span class="hljs-number">27809</span>, <span class="hljs-number">26638</span>, <span class="hljs-number">17749</span>, <span class="hljs-number">31592</span>, <span class="hljs-number">17749</span>, <span class="hljs-number">31593</span>, <span class="hljs-number">321</span>, <span class="hljs-number">1281</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Both ÊÖ∂Âøú and ÊÖ∂Êáâ are decoded to ÊÖ∂Âøú</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(tokenizer(<span class="hljs-string">&quot;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Êáâ)Â§ßÂ≠¶Âá∫Ë∫´&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>])
<span class="hljs-string">&#x27;ÂêæËº©„ÅØÁå´„Åß„ÅÇ„ÇãüêØ„ÄÇÂÆü„ÅØÊÖ∂Âøú(ÊÖ∂Âøú)Â§ßÂ≠¶Âá∫Ë∫´&#x27;</span>`,wrap:!1}}),{c(){t=p("p"),t.textContent=b,s=r(),h(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=b),s=i(o),u(d.$$.fragment,o)},m(o,y){l(o,t,y),l(o,s,y),g(d,o,y),J=!0},p:ye,i(o){J||(f(d.$$.fragment,o),J=!0)},o(o){_(d.$$.fragment,o),J=!1},d(o){o&&(a(t),a(s)),T(d,o)}}}function $t(k){let t,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=p("p"),t.innerHTML=b},l(s){t=m(s,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=b)},m(s,d){l(s,t,d)},p:ye,d(s){s&&a(t)}}}function Nt(k){let t,b="Example:",s,d,J;return d=new Ue({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBHUFROZW9YSmFwYW5lc2VNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQW1vZGVsJTIwJTNEJTIwR1BUTmVvWEphcGFuZXNlTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmFiZWphJTJGZ3B0LW5lb3gtamFwYW5lc2UtMi43YiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyJUU2JTk3JUE1JUU2JTlDJUFDJUU4JUFBJTlFJUUzJTgxJUFFR1BULW5lb3glRTMlODElOENIdWdnaW5nJTIwRmFjZSVFMyU4MSVBNyVFNCVCRCVCRiVFMyU4MSU4OCVFMyU4MSVCRSVFMyU4MSU5OSVGMCU5RiU5OCU4MCUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGU=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, GPTNeoXJapaneseModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseModel.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Êó•Êú¨Ë™û„ÅÆGPT-neox„ÅåHugging Face„Åß‰Ωø„Åà„Åæ„ÅôüòÄ&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){t=p("p"),t.textContent=b,s=r(),h(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=b),s=i(o),u(d.$$.fragment,o)},m(o,y){l(o,t,y),l(o,s,y),g(d,o,y),J=!0},p:ye,i(o){J||(f(d.$$.fragment,o),J=!0)},o(o){_(d.$$.fragment,o),J=!1},d(o){o&&(a(t),a(s)),T(d,o)}}}function jt(k){let t,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=p("p"),t.innerHTML=b},l(s){t=m(s,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=b)},m(s,d){l(s,t,d)},p:ye,d(s){s&&a(t)}}}function Ct(k){let t,b="Example:",s,d,J;return d=new Ue({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBHUFROZW9YSmFwYW5lc2VGb3JDYXVzYWxMTSUyQyUyMEdQVE5lb1hKYXBhbmVzZUNvbmZpZyUwQWltcG9ydCUyMHRvcmNoJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQWNvbmZpZyUyMCUzRCUyMEdQVE5lb1hKYXBhbmVzZUNvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQWNvbmZpZy5pc19kZWNvZGVyJTIwJTNEJTIwVHJ1ZSUwQW1vZGVsJTIwJTNEJTIwR1BUTmVvWEphcGFuZXNlRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmFiZWphJTJGZ3B0LW5lb3gtamFwYW5lc2UtMi43YiUyMiUyQyUyMGNvbmZpZyUzRGNvbmZpZyklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTIyJUU2JTk3JUE1JUU2JTlDJUFDJUU4JUFBJTlFJUUzJTgxJUFFR1BULW5lb3glRTMlODElOENIdWdnaW5nJTIwRmFjZSVFMyU4MSVBNyVFNCVCRCVCRiVFMyU4MSU4OCVFMyU4MSVCRSVFMyU4MSU5OSVGMCU5RiU5OCU4MCUyMiUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQXByZWRpY3Rpb25fbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseConfig
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config = GPTNeoXJapaneseConfig.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.is_decoder = <span class="hljs-literal">True</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseForCausalLM.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>, config=config)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Êó•Êú¨Ë™û„ÅÆGPT-neox„ÅåHugging Face„Åß‰Ωø„Åà„Åæ„ÅôüòÄ&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>prediction_logits = outputs.logits`,wrap:!1}}),{c(){t=p("p"),t.textContent=b,s=r(),h(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=b),s=i(o),u(d.$$.fragment,o)},m(o,y){l(o,t,y),l(o,s,y),g(d,o,y),J=!0},p:ye,i(o){J||(f(d.$$.fragment,o),J=!0)},o(o){_(d.$$.fragment,o),J=!1},d(o){o&&(a(t),a(s)),T(d,o)}}}function Gt(k){let t,b,s,d,J,o,y,ke,L,rt=`We introduce GPT-NeoX-Japanese, which is an autoregressive language model for Japanese, trained on top of <a href="https://github.com/EleutherAI/gpt-neox" rel="nofollow">https://github.com/EleutherAI/gpt-neox</a>.
Japanese is a unique language with its large vocabulary and a combination of hiragana, katakana, and kanji writing scripts.
To address this distinct structure of the Japanese language, we use a <a href="https://github.com/tanreinama/Japanese-BPEEncoder_V2" rel="nofollow">special sub-word tokenizer</a>. We are very grateful to <em>tanreinama</em> for open-sourcing this incredibly helpful tokenizer.
Following the recommendations from Google‚Äôs research on <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html" rel="nofollow">PaLM</a>, we have removed bias parameters from transformer blocks, achieving better model performance. Please refer <a href="https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4" rel="nofollow">this article</a> in detail.`,ve,Q,it='Development of the model was led by <a href="https://github.com/SO0529" rel="nofollow">Shinya Otani</a>, <a href="https://github.com/spider-man-tm" rel="nofollow">Takayoshi Makabe</a>, <a href="https://github.com/Anuj040" rel="nofollow">Anuj Arora</a>, and <a href="https://github.com/go5paopao" rel="nofollow">Kyo Hattori</a> from <a href="https://www.abejainc.com/" rel="nofollow">ABEJA, Inc.</a>. For more information on this model-building activity, please refer <a href="https://tech-blog.abeja.asia/entry/abeja-gpt-project-202207" rel="nofollow">here (ja)</a>.',we,H,$e,Y,lt="The <code>generate()</code> method can be used to generate text using GPT NeoX Japanese model.",Ne,O,je,A,Ce,D,dt='<li><a href="../tasks/language_modeling">Causal language modeling task guide</a></li>',Ge,K,xe,v,ee,Ee,me,ct=`This is the configuration class to store the configuration of a <code>GPTNeoXModelJapanese</code>. It is used to instantiate
a GPTNeoX model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the GPTNeoXJapanese
<a href="https://huggingface.co/abeja/gpt-neox-japanese-2.7b" rel="nofollow">abeja/gpt-neox-japanese-2.7b</a> architecture.`,Re,he,pt=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information. Default configs is set as 2.7B model`,qe,F,Xe,te,ze,U,ne,Se,ue,mt=`This tokenizer inherits from <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> and is based on Japanese special Sub-Word-Encoding that is
used in this repository (<a href="https://github.com/tanreinama/Japanese-BPEEncoder_V2" rel="nofollow">https://github.com/tanreinama/Japanese-BPEEncoder_V2</a>). Check the repository for details.
Japanese has a relatively large vocabulary and there is no separation between words. Furthermore, the language is a
combination of hiragana, katakana, and kanji, and variants such as ‚Äú1‚Äù and ‚Äú‚ë†‚Äù are often used. In order to cope
with these, this tokenizer has the following features`,Le,ge,ht=`<li>Subword-by-subword segmentation, which is intermediate between byte strings and morphological analysis.</li> <li>BPEs are created for each Kanji, Hiragana, and Katakana character, and there are no BPEs that cross character
types, such as Kanji + Hiragana or Hiragana + Katakana.</li> <li>All-byte encoding that does not require &lt;unk&gt;.</li> <li>Independent of UTF codes such as 2-byte and 3-byte characters</li> <li>Conversion of heterographs to the same token_id</li> <li>Emoji and Emoticon are grouped into 12 types as special tags.</li>`,Qe,Z,He,W,oe,Ye,fe,ut="Converts a sequence of tokens (string) in a single string.",Pe,ae,Ve,j,se,Oe,_e,gt=`The bare GPTNeoXJapanese Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Ae,$,re,De,Te,ft='The <a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseModel">GPTNeoXJapaneseModel</a> forward method, overrides the <code>__call__</code> special method.',Ke,B,et,I,Fe,ie,Ze,C,le,tt,be,_t=`GPTNeoXJapanese Model with a <code>language modeling</code> head on top for Classifier Model fine-tuning.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,nt,N,de,ot,Je,Tt='The <a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseForCausalLM">GPTNeoXJapaneseForCausalLM</a> forward method, overrides the <code>__call__</code> special method.',at,E,st,R,We,Me,Be;return J=new S({props:{title:"GPT-NeoX-Japanese",local:"gpt-neox-japanese",headingTag:"h1"}}),y=new S({props:{title:"Overview",local:"overview",headingTag:"h2"}}),H=new S({props:{title:"Usage example",local:"usage-example",headingTag:"h3"}}),O=new Ue({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEdQVE5lb1hKYXBhbmVzZUZvckNhdXNhbExNJTJDJTIwR1BUTmVvWEphcGFuZXNlVG9rZW5pemVyJTBBJTBBbW9kZWwlMjAlM0QlMjBHUFROZW9YSmFwYW5lc2VGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEdQVE5lb1hKYXBhbmVzZVRva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyYWJlamElMkZncHQtbmVveC1qYXBhbmVzZS0yLjdiJTIyKSUwQSUwQXByb21wdCUyMCUzRCUyMCUyMiVFNCVCQSVCQSVFMyU4MSVBOEFJJUUzJTgxJThDJUU1JThEJTk0JUU4JUFBJUJGJUUzJTgxJTk5JUUzJTgyJThCJUUzJTgxJTlGJUUzJTgyJTgxJUUzJTgxJUFCJUUzJTgxJUFGJUUzJTgwJTgxJTIyJTBBJTBBaW5wdXRfaWRzJTIwJTNEJTIwdG9rZW5pemVyKHByb21wdCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLmlucHV0X2lkcyUwQSUwQWdlbl90b2tlbnMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSglMEElMjAlMjAlMjAlMjBpbnB1dF9pZHMlMkMlMEElMjAlMjAlMjAlMjBkb19zYW1wbGUlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwdGVtcGVyYXR1cmUlM0QwLjklMkMlMEElMjAlMjAlMjAlMjBtYXhfbGVuZ3RoJTNEMTAwJTJDJTBBKSUwQWdlbl90ZXh0JTIwJTNEJTIwdG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5fdG9rZW5zJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RCUwQSUwQXByaW50KGdlbl90ZXh0KQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = GPTNeoXJapaneseForCausalLM.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(<span class="hljs-string">&quot;abeja/gpt-neox-japanese-2.7b&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>prompt = <span class="hljs-string">&quot;‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids

<span class="hljs-meta">&gt;&gt;&gt; </span>gen_tokens = model.generate(
<span class="hljs-meta">... </span>    input_ids,
<span class="hljs-meta">... </span>    do_sample=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    temperature=<span class="hljs-number">0.9</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">100</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(gen_text)
‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅAI„Å®‰∫∫„ÅåÂÖ±Â≠ò„Åó„ÄÅAI„ÇíÊ≠£„Åó„ÅèÁêÜËß£„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ`,wrap:!1}}),A=new S({props:{title:"Resources",local:"resources",headingTag:"h2"}}),K=new S({props:{title:"GPTNeoXJapaneseConfig",local:"transformers.GPTNeoXJapaneseConfig",headingTag:"h2"}}),ee=new pe({props:{name:"class transformers.GPTNeoXJapaneseConfig",anchor:"transformers.GPTNeoXJapaneseConfig",parameters:[{name:"vocab_size",val:" = 32000"},{name:"hidden_size",val:" = 2560"},{name:"num_hidden_layers",val:" = 32"},{name:"num_attention_heads",val:" = 32"},{name:"intermediate_multiple_size",val:" = 4"},{name:"hidden_act",val:" = 'gelu'"},{name:"rotary_pct",val:" = 1.0"},{name:"rotary_emb_base",val:" = 10000"},{name:"max_position_embeddings",val:" = 2048"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"use_cache",val:" = True"},{name:"bos_token_id",val:" = 31996"},{name:"eos_token_id",val:" = 31999"},{name:"attention_dropout",val:" = 0.1"},{name:"hidden_dropout",val:" = 0.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32000) &#x2014;
Vocabulary size of the GPTNeoXJapanese model. Defines the number of different tokens that can be
represented by the <code>inputs_ids</code> passed when calling <code>GPTNeoXJapanese</code>.`,name:"vocab_size"},{anchor:"transformers.GPTNeoXJapaneseConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2560) &#x2014;
Dimension of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.GPTNeoXJapaneseConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.GPTNeoXJapaneseConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.GPTNeoXJapaneseConfig.intermediate_multiple_size",description:`<strong>intermediate_multiple_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Dimension of the &#x201C;intermediate&#x201D; layer in the Transformer encoder is calculated by hidden_size *
intermediate_multiple_size.`,name:"intermediate_multiple_size"},{anchor:"transformers.GPTNeoXJapaneseConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler.`,name:"hidden_act"},{anchor:"transformers.GPTNeoXJapaneseConfig.rotary_pct",description:`<strong>rotary_pct</strong> (<code>float</code>, <em>optional</em>, defaults to 1.00) &#x2014;
percentage of hidden dimensions to allocate to rotary embeddings`,name:"rotary_pct"},{anchor:"transformers.GPTNeoXJapaneseConfig.rotary_emb_base",description:`<strong>rotary_emb_base</strong> (<code>int</code>, <em>optional</em>, defaults to 10000) &#x2014;
base for computing rotary embeddings frequency`,name:"rotary_emb_base"},{anchor:"transformers.GPTNeoXJapaneseConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
The maximum sequence length that this model might ever be used with.`,name:"max_position_embeddings"},{anchor:"transformers.GPTNeoXJapaneseConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.GPTNeoXJapaneseConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-5) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.GPTNeoXJapaneseConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.GPTNeoXJapaneseConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention.`,name:"attention_dropout"},{anchor:"transformers.GPTNeoXJapaneseConfig.hidden_dropout",description:`<strong>hidden_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the hidden layer.
Example &#x2014;`,name:"hidden_dropout"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gpt_neox_japanese/configuration_gpt_neox_japanese.py#L28"}}),F=new Ie({props:{anchor:"transformers.GPTNeoXJapaneseConfig.example",$$slots:{default:[vt]},$$scope:{ctx:k}}}),te=new S({props:{title:"GPTNeoXJapaneseTokenizer",local:"transformers.GPTNeoXJapaneseTokenizer",headingTag:"h2"}}),ne=new pe({props:{name:"class transformers.GPTNeoXJapaneseTokenizer",anchor:"transformers.GPTNeoXJapaneseTokenizer",parameters:[{name:"vocab_file",val:""},{name:"emoji_file",val:""},{name:"unk_token",val:" = '<|endoftext|>'"},{name:"pad_token",val:" = '<|endoftext|>'"},{name:"bos_token",val:" = '<|startoftext|>'"},{name:"eos_token",val:" = '<|endoftext|>'"},{name:"do_clean_text",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
File containing the vocabulary.`,name:"vocab_file"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.emoji_file",description:`<strong>emoji_file</strong> (<code>str</code>) &#x2014;
File containing the emoji.`,name:"emoji_file"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The token used for padding`,name:"pad_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|startoftext|&gt;&quot;</code>) &#x2014;
The beginning of sequence token.`,name:"bos_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.GPTNeoXJapaneseTokenizer.do_clean_text",description:`<strong>do_clean_text</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to clean text for URL, EMAIL, TEL, Japanese DATE and Japanese PRICE.`,name:"do_clean_text"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py#L66"}}),Z=new Ie({props:{anchor:"transformers.GPTNeoXJapaneseTokenizer.example",$$slots:{default:[wt]},$$scope:{ctx:k}}}),oe=new pe({props:{name:"convert_tokens_to_string",anchor:"transformers.GPTNeoXJapaneseTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py#L173"}}),ae=new S({props:{title:"GPTNeoXJapaneseModel",local:"transformers.GPTNeoXJapaneseModel",headingTag:"h2"}}),se=new pe({props:{name:"class transformers.GPTNeoXJapaneseModel",anchor:"transformers.GPTNeoXJapaneseModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig">~GPTNeoXJapaneseConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L438"}}),re=new pe({props:{name:"forward",anchor:"transformers.GPTNeoXJapaneseModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>.`,name:"input_ids"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code> of length <code>config.n_layers</code> with each tuple having 4 tensors of shape <code>(batch_size, num_heads, sequence_length - 1, embed_size_per_head)</code>) &#x2014;
Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.
If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GPTNeoXJapaneseModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L462",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig"
>GPTNeoXJapaneseConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) ‚Äî Sequence of hidden-states at the output of the last layer of the model.</p>
<p>If <code>past_key_values</code> is used only the last hidden-state of the sequences of shape <code>(batch_size, 1, hidden_size)</code> is output.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) ‚Äî Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPast"
>transformers.modeling_outputs.BaseModelOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),B=new bt({props:{$$slots:{default:[$t]},$$scope:{ctx:k}}}),I=new Ie({props:{anchor:"transformers.GPTNeoXJapaneseModel.forward.example",$$slots:{default:[Nt]},$$scope:{ctx:k}}}),ie=new S({props:{title:"GPTNeoXJapaneseForCausalLM",local:"transformers.GPTNeoXJapaneseForCausalLM",headingTag:"h2"}}),le=new pe({props:{name:"class transformers.GPTNeoXJapaneseForCausalLM",anchor:"transformers.GPTNeoXJapaneseForCausalLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseForCausalLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig">~GPTNeoXJapaneseConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L593"}}),de=new pe({props:{name:"forward",anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>.`,name:"input_ids"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>`,name:"attention_mask"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.`,name:"position_ids"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <em>input_ids</em> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and 2 additional tensors of shape
<code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>. The two additional tensors are
only required when the model is used as a decoder in a Sequence to Sequence model.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>decoder_input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in
<code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are
ignored (masked), the loss is only computed for the tokens with labels n <code>[0, ..., config.vocab_size]</code>.`,name:"labels"},{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py#L616",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/gpt_neox_japanese#transformers.GPTNeoXJapaneseConfig"
>GPTNeoXJapaneseConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) ‚Äî Language modeling loss (for next-token prediction).</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) ‚Äî Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) ‚Äî Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>)</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/output#transformers.modeling_outputs.CausalLMOutputWithPast"
>transformers.modeling_outputs.CausalLMOutputWithPast</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),E=new bt({props:{$$slots:{default:[jt]},$$scope:{ctx:k}}}),R=new Ie({props:{anchor:"transformers.GPTNeoXJapaneseForCausalLM.forward.example",$$slots:{default:[Ct]},$$scope:{ctx:k}}}),{c(){t=p("meta"),b=r(),s=p("p"),d=r(),h(J.$$.fragment),o=r(),h(y.$$.fragment),ke=r(),L=p("p"),L.innerHTML=rt,ve=r(),Q=p("p"),Q.innerHTML=it,we=r(),h(H.$$.fragment),$e=r(),Y=p("p"),Y.innerHTML=lt,Ne=r(),h(O.$$.fragment),je=r(),h(A.$$.fragment),Ce=r(),D=p("ul"),D.innerHTML=dt,Ge=r(),h(K.$$.fragment),xe=r(),v=p("div"),h(ee.$$.fragment),Ee=r(),me=p("p"),me.innerHTML=ct,Re=r(),he=p("p"),he.innerHTML=pt,qe=r(),h(F.$$.fragment),Xe=r(),h(te.$$.fragment),ze=r(),U=p("div"),h(ne.$$.fragment),Se=r(),ue=p("p"),ue.innerHTML=mt,Le=r(),ge=p("ul"),ge.innerHTML=ht,Qe=r(),h(Z.$$.fragment),He=r(),W=p("div"),h(oe.$$.fragment),Ye=r(),fe=p("p"),fe.textContent=ut,Pe=r(),h(ae.$$.fragment),Ve=r(),j=p("div"),h(se.$$.fragment),Oe=r(),_e=p("p"),_e.innerHTML=gt,Ae=r(),$=p("div"),h(re.$$.fragment),De=r(),Te=p("p"),Te.innerHTML=ft,Ke=r(),h(B.$$.fragment),et=r(),h(I.$$.fragment),Fe=r(),h(ie.$$.fragment),Ze=r(),C=p("div"),h(le.$$.fragment),tt=r(),be=p("p"),be.innerHTML=_t,nt=r(),N=p("div"),h(de.$$.fragment),ot=r(),Je=p("p"),Je.innerHTML=Tt,at=r(),h(E.$$.fragment),st=r(),h(R.$$.fragment),We=r(),Me=p("p"),this.h()},l(e){const n=kt("svelte-u9bgzb",document.head);t=m(n,"META",{name:!0,content:!0}),n.forEach(a),b=i(e),s=m(e,"P",{}),P(s).forEach(a),d=i(e),u(J.$$.fragment,e),o=i(e),u(y.$$.fragment,e),ke=i(e),L=m(e,"P",{"data-svelte-h":!0}),M(L)!=="svelte-vnhg7c"&&(L.innerHTML=rt),ve=i(e),Q=m(e,"P",{"data-svelte-h":!0}),M(Q)!=="svelte-18p40hn"&&(Q.innerHTML=it),we=i(e),u(H.$$.fragment,e),$e=i(e),Y=m(e,"P",{"data-svelte-h":!0}),M(Y)!=="svelte-znvtjx"&&(Y.innerHTML=lt),Ne=i(e),u(O.$$.fragment,e),je=i(e),u(A.$$.fragment,e),Ce=i(e),D=m(e,"UL",{"data-svelte-h":!0}),M(D)!=="svelte-162aebv"&&(D.innerHTML=dt),Ge=i(e),u(K.$$.fragment,e),xe=i(e),v=m(e,"DIV",{class:!0});var G=P(v);u(ee.$$.fragment,G),Ee=i(G),me=m(G,"P",{"data-svelte-h":!0}),M(me)!=="svelte-6p1ri7"&&(me.innerHTML=ct),Re=i(G),he=m(G,"P",{"data-svelte-h":!0}),M(he)!=="svelte-1srd3n9"&&(he.innerHTML=pt),qe=i(G),u(F.$$.fragment,G),G.forEach(a),Xe=i(e),u(te.$$.fragment,e),ze=i(e),U=m(e,"DIV",{class:!0});var w=P(U);u(ne.$$.fragment,w),Se=i(w),ue=m(w,"P",{"data-svelte-h":!0}),M(ue)!=="svelte-kc5s8e"&&(ue.innerHTML=mt),Le=i(w),ge=m(w,"UL",{"data-svelte-h":!0}),M(ge)!=="svelte-9px4ih"&&(ge.innerHTML=ht),Qe=i(w),u(Z.$$.fragment,w),He=i(w),W=m(w,"DIV",{class:!0});var ce=P(W);u(oe.$$.fragment,ce),Ye=i(ce),fe=m(ce,"P",{"data-svelte-h":!0}),M(fe)!=="svelte-b3k2yi"&&(fe.textContent=ut),ce.forEach(a),w.forEach(a),Pe=i(e),u(ae.$$.fragment,e),Ve=i(e),j=m(e,"DIV",{class:!0});var X=P(j);u(se.$$.fragment,X),Oe=i(X),_e=m(X,"P",{"data-svelte-h":!0}),M(_e)!=="svelte-17on1em"&&(_e.innerHTML=gt),Ae=i(X),$=m(X,"DIV",{class:!0});var x=P($);u(re.$$.fragment,x),De=i(x),Te=m(x,"P",{"data-svelte-h":!0}),M(Te)!=="svelte-gtg2df"&&(Te.innerHTML=ft),Ke=i(x),u(B.$$.fragment,x),et=i(x),u(I.$$.fragment,x),x.forEach(a),X.forEach(a),Fe=i(e),u(ie.$$.fragment,e),Ze=i(e),C=m(e,"DIV",{class:!0});var z=P(C);u(le.$$.fragment,z),tt=i(z),be=m(z,"P",{"data-svelte-h":!0}),M(be)!=="svelte-1ycm5h2"&&(be.innerHTML=_t),nt=i(z),N=m(z,"DIV",{class:!0});var q=P(N);u(de.$$.fragment,q),ot=i(q),Je=m(q,"P",{"data-svelte-h":!0}),M(Je)!=="svelte-1sv0k6n"&&(Je.innerHTML=Tt),at=i(q),u(E.$$.fragment,q),st=i(q),u(R.$$.fragment,q),q.forEach(a),z.forEach(a),We=i(e),Me=m(e,"P",{}),P(Me).forEach(a),this.h()},h(){V(t,"name","hf:doc:metadata"),V(t,"content",xt),V(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),V(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){c(document.head,t),l(e,b,n),l(e,s,n),l(e,d,n),g(J,e,n),l(e,o,n),g(y,e,n),l(e,ke,n),l(e,L,n),l(e,ve,n),l(e,Q,n),l(e,we,n),g(H,e,n),l(e,$e,n),l(e,Y,n),l(e,Ne,n),g(O,e,n),l(e,je,n),g(A,e,n),l(e,Ce,n),l(e,D,n),l(e,Ge,n),g(K,e,n),l(e,xe,n),l(e,v,n),g(ee,v,null),c(v,Ee),c(v,me),c(v,Re),c(v,he),c(v,qe),g(F,v,null),l(e,Xe,n),g(te,e,n),l(e,ze,n),l(e,U,n),g(ne,U,null),c(U,Se),c(U,ue),c(U,Le),c(U,ge),c(U,Qe),g(Z,U,null),c(U,He),c(U,W),g(oe,W,null),c(W,Ye),c(W,fe),l(e,Pe,n),g(ae,e,n),l(e,Ve,n),l(e,j,n),g(se,j,null),c(j,Oe),c(j,_e),c(j,Ae),c(j,$),g(re,$,null),c($,De),c($,Te),c($,Ke),g(B,$,null),c($,et),g(I,$,null),l(e,Fe,n),g(ie,e,n),l(e,Ze,n),l(e,C,n),g(le,C,null),c(C,tt),c(C,be),c(C,nt),c(C,N),g(de,N,null),c(N,ot),c(N,Je),c(N,at),g(E,N,null),c(N,st),g(R,N,null),l(e,We,n),l(e,Me,n),Be=!0},p(e,[n]){const G={};n&2&&(G.$$scope={dirty:n,ctx:e}),F.$set(G);const w={};n&2&&(w.$$scope={dirty:n,ctx:e}),Z.$set(w);const ce={};n&2&&(ce.$$scope={dirty:n,ctx:e}),B.$set(ce);const X={};n&2&&(X.$$scope={dirty:n,ctx:e}),I.$set(X);const x={};n&2&&(x.$$scope={dirty:n,ctx:e}),E.$set(x);const z={};n&2&&(z.$$scope={dirty:n,ctx:e}),R.$set(z)},i(e){Be||(f(J.$$.fragment,e),f(y.$$.fragment,e),f(H.$$.fragment,e),f(O.$$.fragment,e),f(A.$$.fragment,e),f(K.$$.fragment,e),f(ee.$$.fragment,e),f(F.$$.fragment,e),f(te.$$.fragment,e),f(ne.$$.fragment,e),f(Z.$$.fragment,e),f(oe.$$.fragment,e),f(ae.$$.fragment,e),f(se.$$.fragment,e),f(re.$$.fragment,e),f(B.$$.fragment,e),f(I.$$.fragment,e),f(ie.$$.fragment,e),f(le.$$.fragment,e),f(de.$$.fragment,e),f(E.$$.fragment,e),f(R.$$.fragment,e),Be=!0)},o(e){_(J.$$.fragment,e),_(y.$$.fragment,e),_(H.$$.fragment,e),_(O.$$.fragment,e),_(A.$$.fragment,e),_(K.$$.fragment,e),_(ee.$$.fragment,e),_(F.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(Z.$$.fragment,e),_(oe.$$.fragment,e),_(ae.$$.fragment,e),_(se.$$.fragment,e),_(re.$$.fragment,e),_(B.$$.fragment,e),_(I.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(E.$$.fragment,e),_(R.$$.fragment,e),Be=!1},d(e){e&&(a(b),a(s),a(d),a(o),a(ke),a(L),a(ve),a(Q),a(we),a($e),a(Y),a(Ne),a(je),a(Ce),a(D),a(Ge),a(xe),a(v),a(Xe),a(ze),a(U),a(Pe),a(Ve),a(j),a(Fe),a(Ze),a(C),a(We),a(Me)),a(t),T(J,e),T(y,e),T(H,e),T(O,e),T(A,e),T(K,e),T(ee),T(F),T(te,e),T(ne),T(Z),T(oe),T(ae,e),T(se),T(re),T(B),T(I),T(ie,e),T(le),T(de),T(E),T(R)}}}const xt='{"title":"GPT-NeoX-Japanese","local":"gpt-neox-japanese","sections":[{"title":"Overview","local":"overview","sections":[{"title":"Usage example","local":"usage-example","sections":[],"depth":3}],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"GPTNeoXJapaneseConfig","local":"transformers.GPTNeoXJapaneseConfig","sections":[],"depth":2},{"title":"GPTNeoXJapaneseTokenizer","local":"transformers.GPTNeoXJapaneseTokenizer","sections":[],"depth":2},{"title":"GPTNeoXJapaneseModel","local":"transformers.GPTNeoXJapaneseModel","sections":[],"depth":2},{"title":"GPTNeoXJapaneseForCausalLM","local":"transformers.GPTNeoXJapaneseForCausalLM","sections":[],"depth":2}],"depth":1}';function Xt(k){return yt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class It extends Mt{constructor(t){super(),Ut(this,t,Xt,Gt,Jt,{})}}export{It as component};
