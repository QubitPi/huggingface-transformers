import{s as Hr,o as Sr,n as he}from"../chunks/scheduler.9bc65507.js";import{S as Vr,i as Lr,g as i,s as o,r as p,A as Yr,h as l,f as n,c as a,j as w,u as f,x as c,k as $,l as Xr,y as s,a as r,v as h,d as u,t as g,w as _}from"../chunks/index.707bf1b6.js";import{T as Ho}from"../chunks/Tip.c2ecdbf4.js";import{D as C}from"../chunks/Docstring.17db21ae.js";import{C as U}from"../chunks/CodeBlock.54a9f38d.js";import{E as So}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as T}from"../chunks/Heading.342b1fa6.js";function qr(J){let d,y="Example:",b,k,v;return k=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBCYXJrTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzdW5vJTJGYmFyay1zbWFsbCUyMiklMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmstc21hbGwlMjIpJTBBJTBBJTIzJTIwVG8lMjBhZGQlMjBhJTIwdm9pY2UlMjBwcmVzZXQlMkMlMjB5b3UlMjBjYW4lMjBwYXNzJTIwJTYwdm9pY2VfcHJlc2V0JTYwJTIwdG8lMjAlNjBCYXJrUHJvY2Vzc29yLl9fY2FsbF9fKC4uLiklNjAlMEF2b2ljZV9wcmVzZXQlMjAlM0QlMjAlMjJ2MiUyRmVuX3NwZWFrZXJfNiUyMiUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTJDJTIwSSUyMG5lZWQlMjBoaW0lMjBpbiUyMG15JTIwbGlmZSUyMiUyQyUyMHZvaWNlX3ByZXNldCUzRHZvaWNlX3ByZXNldCklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwc2VtYW50aWNfbWF4X25ld190b2tlbnMlM0QxMDApJTBBYXVkaW9fYXJyYXklMjAlM0QlMjBhdWRpb19hcnJheS5jcHUoKS5udW1weSgpLnNxdWVlemUoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, BarkModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To add a voice preset, you can pass \`voice_preset\` to \`BarkProcessor.__call__(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>voice_preset = <span class="hljs-string">&quot;v2/en_speaker_6&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello, my dog is cute, I need him in my life&quot;</span>, voice_preset=voice_preset)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs, semantic_max_new_tokens=<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=o(),p(k.$$.fragment)},l(m){d=l(m,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(m),f(k.$$.fragment,m)},m(m,M){r(m,d,M),r(m,b,M),h(k,m,M),v=!0},p:he,i(m){v||(u(k.$$.fragment,m),v=!0)},o(m){g(k.$$.fragment,m),v=!1},d(m){m&&(n(d),n(b)),_(k,m)}}}function Er(J){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:he,d(b){b&&n(d)}}}function Rr(J){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:he,d(b){b&&n(d)}}}function Dr(J){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:he,d(b){b&&n(d)}}}function Qr(J){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),c(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:he,d(b){b&&n(d)}}}function Or(J){let d,y="Example:",b,k,v;return k=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtDb2Fyc2VDb25maWclMkMlMjBCYXJrQ29hcnNlTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQmFyayUyMHN1Yi1tb2R1bGUlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQmFya0NvYXJzZUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya0NvYXJzZU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkCoarseConfig, BarkCoarseModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkCoarseConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkCoarseModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=o(),p(k.$$.fragment)},l(m){d=l(m,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(m),f(k.$$.fragment,m)},m(m,M){r(m,d,M),r(m,b,M),h(k,m,M),v=!0},p:he,i(m){v||(u(k.$$.fragment,m),v=!0)},o(m){g(k.$$.fragment,m),v=!1},d(m){m&&(n(d),n(b)),_(k,m)}}}function Ar(J){let d,y="Example:",b,k,v;return k=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtGaW5lQ29uZmlnJTJDJTIwQmFya0ZpbmVNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCYXJrJTIwc3ViLW1vZHVsZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCYXJrRmluZUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya0ZpbmVNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkFineConfig, BarkFineModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkFineConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkFineModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=o(),p(k.$$.fragment)},l(m){d=l(m,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(m),f(k.$$.fragment,m)},m(m,M){r(m,d,M),r(m,b,M),h(k,m,M),v=!0},p:he,i(m){v||(u(k.$$.fragment,m),v=!0)},o(m){g(k.$$.fragment,m),v=!1},d(m){m&&(n(d),n(b)),_(k,m)}}}function Kr(J){let d,y="Example:",b,k,v;return k=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtTZW1hbnRpY0NvbmZpZyUyQyUyMEJhcmtTZW1hbnRpY01vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJhcmslMjBzdWItbW9kdWxlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEJhcmtTZW1hbnRpY0NvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya1NlbWFudGljTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkSemanticConfig, BarkSemanticModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkSemanticConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkSemanticModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=o(),p(k.$$.fragment)},l(m){d=l(m,"P",{"data-svelte-h":!0}),c(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(m),f(k.$$.fragment,m)},m(m,M){r(m,d,M),r(m,b,M),h(k,m,M),v=!0},p:he,i(m){v||(u(k.$$.fragment,m),v=!0)},o(m){g(k.$$.fragment,m),v=!1},d(m){m&&(n(d),n(b)),_(k,m)}}}function es(J){let d,y,b,k,v,m,M,wn,ue,Na='Bark is a transformer-based text-to-speech model proposed by Suno AI in <a href="https://github.com/suno-ai/bark" rel="nofollow">suno-ai/bark</a>.',$n,ge,Fa="Bark is made of 4 main models:",Tn,_e,Ga='<li><a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> (also referred to as the ‘text’ model): a causal auto-regressive transformer model that takes as input tokenized text, and predicts semantic text tokens that capture the meaning of the text.</li> <li><a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a> (also referred to as the ‘coarse acoustics’ model): a causal autoregressive transformer, that takes as input the results of the <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> model. It aims at predicting the first two audio codebooks necessary for EnCodec.</li> <li><a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> (the ‘fine acoustics’ model), this time a non-causal autoencoder transformer, which iteratively predicts the last codebooks based on the sum of the previous codebooks embeddings.</li> <li>having predicted all the codebook channels from the <a href="/docs/transformers/main/en/model_doc/encodec#transformers.EncodecModel">EncodecModel</a>, Bark uses it to decode the output audio array.</li>',Bn,be,Ha="It should be noted that each of the first three modules can support conditional speaker embeddings to condition the output sound according to specific predefined voice.",Cn,ke,Sa=`This model was contributed by <a href="https://huggingface.co/ylacombe" rel="nofollow">Yoach Lacombe (ylacombe)</a> and <a href="https://github.com/sanchit-gandhi" rel="nofollow">Sanchit Gandhi (sanchit-gandhi)</a>.
The original code can be found <a href="https://github.com/suno-ai/bark" rel="nofollow">here</a>.`,xn,ye,Jn,ve,Va="Bark can be optimized with just a few extra lines of code, which <strong>significantly reduces its memory footprint</strong> and <strong>accelerates inference</strong>.",jn,Me,zn,we,La="You can speed up inference and reduce memory footprint by 50% simply by loading the model in half-precision.",Un,$e,In,Te,Wn,Be,Ya="As mentioned above, Bark is made up of 4 sub-models, which are called up sequentially during audio generation. In other words, while one sub-model is in use, the other sub-models are idle.",Zn,Ce,Xa="If you’re using a CUDA device, a simple solution to benefit from an 80% reduction in memory footprint is to offload the submodels from GPU to CPU when they’re idle. This operation is called <em>CPU offloading</em>. You can use it with one line of code as follows:",Pn,xe,Nn,Je,qa='Note that 🤗 Accelerate must be installed before using this feature. <a href="https://huggingface.co/docs/accelerate/basic_tutorials/install" rel="nofollow">Here’s how to install it.</a>',Fn,je,Gn,ze,Ea="Better Transformer is an 🤗 Optimum feature that performs kernel fusion under the hood. You can gain 20% to 30% in speed with zero performance degradation. It only requires one line of code to export the model to 🤗 Better Transformer:",Hn,Ue,Sn,Ie,Ra='Note that 🤗 Optimum must be installed before using this feature. <a href="https://huggingface.co/docs/optimum/installation" rel="nofollow">Here’s how to install it.</a>',Vn,We,Ln,Ze,Da="Flash Attention 2 is an even faster, optimized version of the previous optimization.",Yn,Pe,Xn,Ne,Qa='First, check whether your hardware is compatible with Flash Attention 2. The latest list of compatible hardware can be found in the <a href="https://github.com/Dao-AILab/flash-attention#installation-and-features" rel="nofollow">official documentation</a>. If your hardware is not compatible with Flash Attention 2, you can still benefit from attention kernel optimisations through Better Transformer support covered <a href="https://huggingface.co/docs/transformers/main/en/model_doc/bark#using-better-transformer" rel="nofollow">above</a>.',qn,Fe,Oa='Next, <a href="https://github.com/Dao-AILab/flash-attention#installation-and-features" rel="nofollow">install</a> the latest version of Flash Attention 2:',En,Ge,Rn,He,Dn,Se,Aa='To load a model using Flash Attention 2, we can pass the <code>attn_implementation=&quot;flash_attention_2&quot;</code> flag to <a href="https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained" rel="nofollow"><code>.from_pretrained</code></a>. We’ll also load the model in half-precision (e.g. <code>torch.float16</code>), since it results in almost no degradation to audio quality but significantly lower memory usage and faster inference:',Qn,Ve,On,Le,An,Ye,Ka="The following diagram shows the latency for the native attention implementation (no optimisation) against Better Transformer and Flash Attention 2. In all cases, we generate 400 semantic tokens on a 40GB A100 GPU with PyTorch 2.1. Flash Attention 2 is also consistently faster than Better Transformer, and its performance improves even more as batch sizes increase:",Kn,R,er='<img src="https://huggingface.co/datasets/ylacombe/benchmark-comparison/resolve/main/Bark%20Optimization%20Benchmark.png"/>',eo,Xe,tr='To put this into perspective, on an NVIDIA A100 and when generating 400 semantic tokens with a batch size of 16, you can get 17 times the <a href="https://huggingface.co/blog/optimizing-bark#throughput" rel="nofollow">throughput</a> and still be 2 seconds faster than generating sentences one by one with the native model implementation. In other words, all the samples will be generated 17 times faster.',to,qe,nr="At batch size 8, on an NVIDIA A100, Flash Attention 2 is also 10% faster than Better Transformer, and at batch size 16, 25%.",no,Ee,oo,Re,or="You can combine optimization techniques, and use CPU offload, half-precision and Flash Attention 2 (or 🤗 Better Transformer) all at once.",ao,De,ro,Qe,ar='Find out more on inference optimization techniques <a href="https://huggingface.co/docs/transformers/perf_infer_gpu_one" rel="nofollow">here</a>.',so,Oe,io,Ae,rr=`Suno offers a library of voice presets in a number of languages <a href="https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c" rel="nofollow">here</a>.
These presets are also uploaded in the hub <a href="https://huggingface.co/suno/bark-small/tree/main/speaker_embeddings" rel="nofollow">here</a> or <a href="https://huggingface.co/suno/bark/tree/main/speaker_embeddings" rel="nofollow">here</a>.`,lo,Ke,co,et,sr="Bark can generate highly realistic, <strong>multilingual</strong> speech as well as other audio - including music, background noise and simple sound effects.",mo,tt,po,nt,ir="The model can also produce <strong>nonverbal communications</strong> like laughing, sighing and crying.",fo,ot,ho,at,lr="To save the audio, simply take the sample rate from the model config and some scipy utility:",uo,rt,go,st,_o,j,it,Vo,Ht,dr=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkModel">BarkModel</a>. It is used to instantiate a Bark
model according to the specified sub-models configurations, defining the model architecture.`,Lo,St,cr=`Instantiating a configuration with the defaults will yield a similar configuration to that of the Bark
<a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a> architecture.`,Yo,Vt,mr=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Xo,D,lt,qo,Lt,pr='Instantiate a <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkConfig">BarkConfig</a> (or a derived class) from bark sub-models configuration.',bo,dt,ko,z,ct,Eo,Yt,fr="Constructs a Bark processor which wraps a text tokenizer and optional Bark voice presets into a single processor.",Ro,Q,mt,Do,Xt,hr=`Main method to prepare for the model one or several sequences(s). This method forwards the <code>text</code> and <code>kwargs</code>
arguments to the AutoTokenizer’s <code>__call__()</code> to encode the text. The method also proposes a
voice preset which is a dictionary of arrays that conditions <code>Bark</code>’s output. <code>kwargs</code> arguments are forwarded
to the tokenizer and to <code>cached_file</code> method if <code>voice_preset</code> is a valid filename.`,Qo,O,pt,Oo,qt,ur="Instantiate a Bark processor associated with a pretrained model.",Ao,A,ft,Ko,Et,gr=`Saves the attributes of this processor (tokenizer…) in the specified directory so that it can be reloaded
using the <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkProcessor.from_pretrained">from_pretrained()</a> method.`,yo,ht,vo,B,ut,ea,Rt,_r="The full Bark model, a text-to-speech model composed of 4 sub-models:",ta,Dt,br=`<li><a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> (also referred to as the ‘text’ model): a causal auto-regressive transformer model that
takes
as input tokenized text, and predicts semantic text tokens that capture the meaning of the text.</li> <li><a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a> (also refered to as the ‘coarse acoustics’ model), also a causal autoregressive transformer,
that takes into input the results of the last model. It aims at regressing the first two audio codebooks necessary
to <code>encodec</code>.</li> <li><a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> (the ‘fine acoustics’ model), this time a non-causal autoencoder transformer, which iteratively
predicts the last codebooks based on the sum of the previous codebooks embeddings.</li> <li>having predicted all the codebook channels from the <a href="/docs/transformers/main/en/model_doc/encodec#transformers.EncodecModel">EncodecModel</a>, Bark uses it to decode the output audio
array.</li>`,na,Qt,kr=`It should be noted that each of the first three modules can support conditional speaker embeddings to condition the
output sound according to specific predefined voice.`,oa,Ot,yr=`This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,aa,At,vr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,ra,S,gt,sa,Kt,Mr="Generates audio from an input prompt and an additional optional <code>Bark</code> speaker prompt.",ia,K,la,ee,_t,da,en,wr=`Offloads all sub-models to CPU using accelerate, reducing memory usage with a low impact on performance. This
method moves one whole sub-model at a time to the GPU when it is used, and the sub-model remains in GPU until
the next sub-model runs.`,Mo,bt,wo,I,kt,ca,tn,$r=`Bark semantic (or text) model. It shares the same architecture as the coarse model.
It is a GPT-2 like autoregressive model with a language modeling head on top.
This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ma,nn,Tr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,pa,V,yt,fa,on,Br='The <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',ha,te,$o,vt,To,W,Mt,ua,an,Cr=`Bark coarse acoustics model.
It shares the same architecture as the semantic (or text) model. It is a GPT-2 like autoregressive model with a
language modeling head on top.
This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ga,rn,xr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,_a,L,wt,ba,sn,Jr='The <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',ka,ne,Bo,$t,Co,Z,Tt,ya,ln,jr=`Bark fine acoustics model. It is a non-causal GPT-like model with <code>config.n_codes_total</code> embedding layers and
language modeling heads, one for each codebook.
This model inherits from <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,va,dn,zr=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ma,Y,Bt,wa,cn,Ur='The <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> forward method, overrides the <code>__call__</code> special method.',$a,oe,xo,Ct,Jo,q,xt,Ta,X,Jt,Ba,mn,Ir='The <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',Ca,ae,jo,jt,zo,P,zt,xa,pn,Wr=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,Ja,fn,Zr=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ja,re,Uo,Ut,Io,N,It,za,hn,Pr=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,Ua,un,Nr=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ia,se,Wo,Wt,Zo,F,Zt,Wa,gn,Fr=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,Za,_n,Gr=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Pa,ie,Po,Mn,No;return v=new T({props:{title:"Bark",local:"bark",headingTag:"h1"}}),M=new T({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ye=new T({props:{title:"Optimizing Bark",local:"optimizing-bark",headingTag:"h3"}}),Me=new T({props:{title:"Using half-precision",local:"using-half-precision",headingTag:"h4"}}),$e=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMGlmJTIwdG9yY2guY3VkYS5pc19hdmFpbGFibGUoKSUyMGVsc2UlMjAlMjJjcHUlMjIlMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmstc21hbGwlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYpLnRvKGRldmljZSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkModel
<span class="hljs-keyword">import</span> torch

device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>
model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>, torch_dtype=torch.float16).to(device)`,wrap:!1}}),Te=new T({props:{title:"Using CPU offload",local:"using-cpu-offload",headingTag:"h4"}}),xe=new U({props:{code:"bW9kZWwuZW5hYmxlX2NwdV9vZmZsb2FkKCk=",highlighted:"model.enable_cpu_offload()",wrap:!1}}),je=new T({props:{title:"Using Better Transformer",local:"using-better-transformer",headingTag:"h4"}}),Ue=new U({props:{code:"bW9kZWwlMjAlM0QlMjAlMjBtb2RlbC50b19iZXR0ZXJ0cmFuc2Zvcm1lcigp",highlighted:"model =  model.to_bettertransformer()",wrap:!1}}),We=new T({props:{title:"Using Flash Attention 2",local:"using-flash-attention-2",headingTag:"h4"}}),Pe=new T({props:{title:"Installation",local:"installation",headingTag:"h5"}}),Ge=new U({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1VJTIwZmxhc2gtYXR0biUyMC0tbm8tYnVpbGQtaXNvbGF0aW9u",highlighted:"pip install -U flash-attn --no-build-isolation",wrap:!1}}),He=new T({props:{title:"Usage",local:"usage",headingTag:"h5"}}),Ve=new U({props:{code:"bW9kZWwlMjAlM0QlMjBCYXJrTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnN1bm8lMkZiYXJrLXNtYWxsJTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMmZsYXNoX2F0dGVudGlvbl8yJTIyKS50byhkZXZpY2Up",highlighted:'model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>, torch_dtype=torch.float16, attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>).to(device)',wrap:!1}}),Le=new T({props:{title:"Performance comparison",local:"performance-comparison",headingTag:"h5"}}),Ee=new T({props:{title:"Combining optimization techniques",local:"combining-optimization-techniques",headingTag:"h4"}}),De=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMGlmJTIwdG9yY2guY3VkYS5pc19hdmFpbGFibGUoKSUyMGVsc2UlMjAlMjJjcHUlMjIlMEElMEElMjMlMjBsb2FkJTIwaW4lMjBmcDE2JTIwYW5kJTIwdXNlJTIwRmxhc2glMjBBdHRlbnRpb24lMjAyJTBBbW9kZWwlMjAlM0QlMjBCYXJrTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnN1bm8lMkZiYXJrLXNtYWxsJTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2JTJDJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMmZsYXNoX2F0dGVudGlvbl8yJTIyKS50byhkZXZpY2UpJTBBJTBBJTIzJTIwZW5hYmxlJTIwQ1BVJTIwb2ZmbG9hZCUwQW1vZGVsLmVuYWJsZV9jcHVfb2ZmbG9hZCgp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkModel
<span class="hljs-keyword">import</span> torch

device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

<span class="hljs-comment"># load in fp16 and use Flash Attention 2</span>
model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>, torch_dtype=torch.float16, attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>).to(device)

<span class="hljs-comment"># enable CPU offload</span>
model.enable_cpu_offload()`,wrap:!1}}),Oe=new T({props:{title:"Usage tips",local:"usage-tips",headingTag:"h3"}}),Ke=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBCYXJrTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzdW5vJTJGYmFyayUyMiklMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmslMjIpJTBBJTBBdm9pY2VfcHJlc2V0JTIwJTNEJTIwJTIydjIlMkZlbl9zcGVha2VyXzYlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoJTIySGVsbG8lMkMlMjBteSUyMGRvZyUyMGlzJTIwY3V0ZSUyMiUyQyUyMHZvaWNlX3ByZXNldCUzRHZvaWNlX3ByZXNldCklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwYXVkaW9fYXJyYXkuY3B1KCkubnVtcHkoKS5zcXVlZXplKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, BarkModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;suno/bark&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>voice_preset = <span class="hljs-string">&quot;v2/en_speaker_6&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, voice_preset=voice_preset)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),tt=new U({props:{code:"JTIzJTIwTXVsdGlsaW5ndWFsJTIwc3BlZWNoJTIwLSUyMHNpbXBsaWZpZWQlMjBDaGluZXNlJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKCUyMiVFNiU4MyU4QSVFNCVCQSVCQSVFNyU5QSU4NCVFRiVCQyU4MSVFNiU4OCU5MSVFNCVCQyU5QSVFOCVBRiVCNCVFNCVCOCVBRCVFNiU5NiU4NyUyMiklMEElMEElMjMlMjBNdWx0aWxpbmd1YWwlMjBzcGVlY2glMjAtJTIwRnJlbmNoJTIwLSUyMGxldCdzJTIwdXNlJTIwYSUyMHZvaWNlX3ByZXNldCUyMGFzJTIwd2VsbCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJJbmNyb3lhYmxlISUyMEplJTIwcGV1eCUyMGclQzMlQTluJUMzJUE5cmVyJTIwZHUlMjBzb24uJTIyJTJDJTIwdm9pY2VfcHJlc2V0JTNEJTIyZnJfc3BlYWtlcl81JTIyKSUwQSUwQSUyMyUyMEJhcmslMjBjYW4lMjBhbHNvJTIwZ2VuZXJhdGUlMjBtdXNpYy4lMjBZb3UlMjBjYW4lMjBoZWxwJTIwaXQlMjBvdXQlMjBieSUyMGFkZGluZyUyMG11c2ljJTIwbm90ZXMlMjBhcm91bmQlMjB5b3VyJTIwbHlyaWNzLiUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjIlRTIlOTklQUElMjBIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTIwJUUyJTk5JUFBJTIyKSUwQSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMpJTBBYXVkaW9fYXJyYXklMjAlM0QlMjBhdWRpb19hcnJheS5jcHUoKS5udW1weSgpLnNxdWVlemUoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multilingual speech - simplified Chinese</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;惊人的！我会说中文&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multilingual speech - French - let&#x27;s use a voice_preset as well</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Incroyable! Je peux générer du son.&quot;</span>, voice_preset=<span class="hljs-string">&quot;fr_speaker_5&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Bark can also generate music. You can help it out by adding music notes around your lyrics.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;♪ Hello, my dog is cute ♪&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),ot=new U({props:{code:"JTIzJTIwQWRkaW5nJTIwbm9uLXNwZWVjaCUyMGN1ZXMlMjB0byUyMHRoZSUyMGlucHV0JTIwdGV4dCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJIZWxsbyUyMHVoJTIwLi4uJTIwJTVCY2xlYXJzJTIwdGhyb2F0JTVEJTJDJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjAlNUJsYXVnaHRlciU1RCUyMiklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwYXVkaW9fYXJyYXkuY3B1KCkubnVtcHkoKS5zcXVlZXplKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Adding non-speech cues to the input text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello uh ... [clears throat], my dog is cute [laughter]&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),rt=new U({props:{code:"ZnJvbSUyMHNjaXB5LmlvLndhdmZpbGUlMjBpbXBvcnQlMjB3cml0ZSUyMGFzJTIwd3JpdGVfd2F2JTBBJTBBJTIzJTIwc2F2ZSUyMGF1ZGlvJTIwdG8lMjBkaXNrJTJDJTIwYnV0JTIwZmlyc3QlMjB0YWtlJTIwdGhlJTIwc2FtcGxlJTIwcmF0ZSUyMGZyb20lMjB0aGUlMjBtb2RlbCUyMGNvbmZpZyUwQXNhbXBsZV9yYXRlJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGlvbl9jb25maWcuc2FtcGxlX3JhdGUlMEF3cml0ZV93YXYoJTIyYmFya19nZW5lcmF0aW9uLndhdiUyMiUyQyUyMHNhbXBsZV9yYXRlJTJDJTIwYXVkaW9fYXJyYXkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> scipy.io.wavfile <span class="hljs-keyword">import</span> write <span class="hljs-keyword">as</span> write_wav

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save audio to disk, but first take the sample rate from the model config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sample_rate = model.generation_config.sample_rate
<span class="hljs-meta">&gt;&gt;&gt; </span>write_wav(<span class="hljs-string">&quot;bark_generation.wav&quot;</span>, sample_rate, audio_array)`,wrap:!1}}),st=new T({props:{title:"BarkConfig",local:"transformers.BarkConfig",headingTag:"h2"}}),it=new C({props:{name:"class transformers.BarkConfig",anchor:"transformers.BarkConfig",parameters:[{name:"semantic_config",val:": Dict = None"},{name:"coarse_acoustics_config",val:": Dict = None"},{name:"fine_acoustics_config",val:": Dict = None"},{name:"codec_config",val:": Dict = None"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkConfig.semantic_config",description:`<strong>semantic_config</strong> (<a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkSemanticConfig">BarkSemanticConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying semantic sub-model.`,name:"semantic_config"},{anchor:"transformers.BarkConfig.coarse_acoustics_config",description:`<strong>coarse_acoustics_config</strong> (<a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkCoarseConfig">BarkCoarseConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying coarse acoustics sub-model.`,name:"coarse_acoustics_config"},{anchor:"transformers.BarkConfig.fine_acoustics_config",description:`<strong>fine_acoustics_config</strong> (<a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkFineConfig">BarkFineConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying fine acoustics sub-model.`,name:"fine_acoustics_config"},{anchor:"transformers.BarkConfig.codec_config",description:`<strong>codec_config</strong> (<a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoConfig">AutoConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying codec sub-model.</p>
<p>Example &#x2014;`,name:"codec_config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L219"}}),lt=new C({props:{name:"from_sub_model_configs",anchor:"transformers.BarkConfig.from_sub_model_configs",parameters:[{name:"semantic_config",val:": BarkSemanticConfig"},{name:"coarse_acoustics_config",val:": BarkCoarseConfig"},{name:"fine_acoustics_config",val:": BarkFineConfig"},{name:"codec_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L309",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/model_doc/bark#transformers.BarkConfig"
>BarkConfig</a></p>
`}}),dt=new T({props:{title:"BarkProcessor",local:"transformers.BarkProcessor",headingTag:"h2"}}),ct=new C({props:{name:"class transformers.BarkProcessor",anchor:"transformers.BarkProcessor",parameters:[{name:"tokenizer",val:""},{name:"speaker_embeddings",val:" = None"}],parametersDescription:[{anchor:"transformers.BarkProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>) &#x2014;
An instance of <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>.`,name:"tokenizer"},{anchor:"transformers.BarkProcessor.speaker_embeddings",description:`<strong>speaker_embeddings</strong> (<code>Dict[Dict[str]]</code>, <em>optional</em>) &#x2014;
Optional nested speaker embeddings dictionary. The first level contains voice preset names (e.g
<code>&quot;en_speaker_4&quot;</code>). The second level contains <code>&quot;semantic_prompt&quot;</code>, <code>&quot;coarse_prompt&quot;</code> and <code>&quot;fine_prompt&quot;</code>
embeddings. The values correspond to the path of the corresponding <code>np.ndarray</code>. See
<a href="https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c" rel="nofollow">here</a> for
a list of <code>voice_preset_names</code>.`,name:"speaker_embeddings"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L34"}}),mt=new C({props:{name:"__call__",anchor:"transformers.BarkProcessor.__call__",parameters:[{name:"text",val:" = None"},{name:"voice_preset",val:" = None"},{name:"return_tensors",val:" = 'pt'"},{name:"max_length",val:" = 256"},{name:"add_special_tokens",val:" = False"},{name:"return_attention_mask",val:" = True"},{name:"return_token_type_ids",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.__call__.text",description:`<strong>text</strong> (<code>str</code>, <code>List[str]</code>, <code>List[List[str]]</code>) &#x2014;
The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).`,name:"text"},{anchor:"transformers.BarkProcessor.__call__.voice_preset",description:`<strong>voice_preset</strong> (<code>str</code>, <code>Dict[np.ndarray]</code>) &#x2014;
The voice preset, i.e the speaker embeddings. It can either be a valid voice_preset name, e.g
<code>&quot;en_speaker_1&quot;</code>, or directly a dictionnary of <code>np.ndarray</code> embeddings for each submodel of <code>Bark</code>. Or
it can be a valid file name of a local <code>.npz</code> single voice preset.`,name:"voice_preset"},{anchor:"transformers.BarkProcessor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L218",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A tuple composed of a <a
  href="/docs/transformers/main/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a>, i.e the output of the
<code>tokenizer</code> and a <a
  href="/docs/transformers/main/en/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a>, i.e the voice preset with the right tensors type.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Tuple(<a
  href="/docs/transformers/main/en/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a>, <a
  href="/docs/transformers/main/en/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a>)</p>
`}}),pt=new C({props:{name:"from_pretrained",anchor:"transformers.BarkProcessor.from_pretrained",parameters:[{name:"pretrained_processor_name_or_path",val:""},{name:"speaker_embeddings_dict_path",val:" = 'speaker_embeddings_path.json'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkProcessor">BarkProcessor</a> hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a processor saved using the <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkProcessor.save_pretrained">save_pretrained()</a>
method, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.BarkProcessor.from_pretrained.speaker_embeddings_dict_path",description:`<strong>speaker_embeddings_dict_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings_path.json&quot;</code>) &#x2014;
The name of the <code>.json</code> file containing the speaker_embeddings dictionnary located in
<code>pretrained_model_name_or_path</code>. If <code>None</code>, no speaker_embeddings is loaded.
**kwargs &#x2014;
Additional keyword arguments passed along to both
<code>~tokenization_utils_base.PreTrainedTokenizer.from_pretrained</code>.`,name:"speaker_embeddings_dict_path"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L64"}}),ft=new C({props:{name:"save_pretrained",anchor:"transformers.BarkProcessor.save_pretrained",parameters:[{name:"save_directory",val:""},{name:"speaker_embeddings_dict_path",val:" = 'speaker_embeddings_path.json'"},{name:"speaker_embeddings_directory",val:" = 'speaker_embeddings'"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the tokenizer files and the speaker embeddings will be saved (directory will be created
if it does not exist).`,name:"save_directory"},{anchor:"transformers.BarkProcessor.save_pretrained.speaker_embeddings_dict_path",description:`<strong>speaker_embeddings_dict_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings_path.json&quot;</code>) &#x2014;
The name of the <code>.json</code> file that will contains the speaker_embeddings nested path dictionnary, if it
exists, and that will be located in <code>pretrained_model_name_or_path/speaker_embeddings_directory</code>.`,name:"speaker_embeddings_dict_path"},{anchor:"transformers.BarkProcessor.save_pretrained.speaker_embeddings_directory",description:`<strong>speaker_embeddings_directory</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings/&quot;</code>) &#x2014;
The name of the folder in which the speaker_embeddings arrays will be saved.`,name:"speaker_embeddings_directory"},{anchor:"transformers.BarkProcessor.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L117"}}),ht=new T({props:{title:"BarkModel",local:"transformers.BarkModel",headingTag:"h2"}}),ut=new C({props:{name:"class transformers.BarkModel",anchor:"transformers.BarkModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkConfig">BarkConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1629"}}),gt=new C({props:{name:"generate",anchor:"transformers.BarkModel.generate",parameters:[{name:"input_ids",val:": Optional = None"},{name:"history_prompt",val:": Optional = None"},{name:"return_output_lengths",val:": Optional = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkModel.generate.input_ids",description:`<strong>input_ids</strong> (<code>Optional[torch.Tensor]</code> of shape (batch_size, seq_len), <em>optional</em>) &#x2014;
Input ids. Will be truncated up to 256 tokens. Note that the output audios will be as long as the
longest generation among the batch.`,name:"input_ids"},{anchor:"transformers.BarkModel.generate.history_prompt",description:`<strong>history_prompt</strong> (<code>Optional[Dict[str,torch.Tensor]]</code>, <em>optional</em>) &#x2014;
Optional <code>Bark</code> speaker prompt. Note that for now, this model takes only one speaker prompt per batch.`,name:"history_prompt"},{anchor:"transformers.BarkModel.generate.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014; Remaining dictionary of keyword arguments. Keyword arguments are of two types:</p>
<ul>
<li>Without a prefix, they will be entered as <code>**kwargs</code> for the <code>generate</code> method of each sub-model.</li>
<li>With a <em>semantic_</em>, <em>coarse_</em>, <em>fine_</em> prefix, they will be input for the <code>generate</code> method of the
semantic, coarse and fine respectively. It has the priority over the keywords without a prefix.</li>
</ul>
<p>This means you can, for example, specify a generation strategy for all sub-models except one.`,name:"kwargs"},{anchor:"transformers.BarkModel.generate.return_output_lengths",description:`<strong>return_output_lengths</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the waveform lengths. Useful when batching.`,name:"return_output_lengths"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1737",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>audio_waveform</strong> (<code>torch.Tensor</code> of shape (batch_size, seq_len)): Generated audio waveform.
When <code>return_output_lengths=True</code>:
Returns a tuple made of:</li>
<li><strong>audio_waveform</strong> (<code>torch.Tensor</code> of shape (batch_size, seq_len)): Generated audio waveform.</li>
<li><strong>output_lengths</strong> (<code>torch.Tensor</code> of shape (batch_size)): The length of each waveform in the batch</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>By default</p>
`}}),K=new So({props:{anchor:"transformers.BarkModel.generate.example",$$slots:{default:[qr]},$$scope:{ctx:J}}}),_t=new C({props:{name:"enable_cpu_offload",anchor:"transformers.BarkModel.enable_cpu_offload",parameters:[{name:"gpu_id",val:": Optional = 0"}],parametersDescription:[{anchor:"transformers.BarkModel.enable_cpu_offload.gpu_id",description:`<strong>gpu_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
GPU id on which the sub-models will be loaded and offloaded.`,name:"gpu_id"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1680"}}),bt=new T({props:{title:"BarkSemanticModel",local:"transformers.BarkSemanticModel",headingTag:"h2"}}),kt=new C({props:{name:"class transformers.BarkSemanticModel",anchor:"transformers.BarkSemanticModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkSemanticModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkSemanticConfig">BarkSemanticConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L913"}}),yt=new C({props:{name:"forward",anchor:"transformers.BarkSemanticModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkSemanticModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkSemanticModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkSemanticModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkSemanticModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkSemanticModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkSemanticModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkSemanticModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkSemanticModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkSemanticModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkSemanticModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L749"}}),te=new Ho({props:{$$slots:{default:[Er]},$$scope:{ctx:J}}}),vt=new T({props:{title:"BarkCoarseModel",local:"transformers.BarkCoarseModel",headingTag:"h2"}}),Mt=new C({props:{name:"class transformers.BarkCoarseModel",anchor:"transformers.BarkCoarseModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkCoarseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkCoarseConfig">BarkCoarseConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1022"}}),wt=new C({props:{name:"forward",anchor:"transformers.BarkCoarseModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkCoarseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkCoarseModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkCoarseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkCoarseModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkCoarseModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkCoarseModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkCoarseModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkCoarseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkCoarseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkCoarseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L749"}}),ne=new Ho({props:{$$slots:{default:[Rr]},$$scope:{ctx:J}}}),$t=new T({props:{title:"BarkFineModel",local:"transformers.BarkFineModel",headingTag:"h2"}}),Tt=new C({props:{name:"class transformers.BarkFineModel",anchor:"transformers.BarkFineModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkFineModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkFineConfig">BarkFineConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1243"}}),Bt=new C({props:{name:"forward",anchor:"transformers.BarkFineModel.forward",parameters:[{name:"codebook_idx",val:": int"},{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkFineModel.forward.codebook_idx",description:`<strong>codebook_idx</strong> (<code>int</code>) &#x2014;
Index of the codebook that will be predicted.`,name:"codebook_idx"},{anchor:"transformers.BarkFineModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length, number_of_codebooks)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Initially, indices of the first two codebooks are obtained from the <code>coarse</code> sub-model. The rest is
predicted recursively by attending the previously predicted channels. The model predicts on windows of
length 1024.`,name:"input_ids"},{anchor:"transformers.BarkFineModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkFineModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkFineModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkFineModel.forward.labels",description:"<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014; NOT IMPLEMENTED YET.",name:"labels"},{anchor:"transformers.BarkFineModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. If
<code>past_key_values</code> is used, optionally only the last <code>input_embeds</code> have to be input (see
<code>past_key_values</code>). This is useful if you want more control over how to convert <code>input_ids</code> indices into
associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"input_embeds"},{anchor:"transformers.BarkFineModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkFineModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkFineModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1381"}}),oe=new Ho({props:{$$slots:{default:[Dr]},$$scope:{ctx:J}}}),Ct=new T({props:{title:"BarkCausalModel",local:"transformers.BarkCausalModel",headingTag:"h2"}}),xt=new C({props:{name:"class transformers.BarkCausalModel",anchor:"transformers.BarkCausalModel",parameters:[{name:"config",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L658"}}),Jt=new C({props:{name:"forward",anchor:"transformers.BarkCausalModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkCausalModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Indices can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/en/model_doc/fastspeech2_conformer#transformers.FastSpeech2ConformerTokenizer.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkCausalModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkCausalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkCausalModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkCausalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkCausalModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkCausalModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkCausalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkCausalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkCausalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L749"}}),ae=new Ho({props:{$$slots:{default:[Qr]},$$scope:{ctx:J}}}),jt=new T({props:{title:"BarkCoarseConfig",local:"transformers.BarkCoarseConfig",headingTag:"h2"}}),zt=new C({props:{name:"class transformers.BarkCoarseConfig",anchor:"transformers.BarkCoarseConfig",parameters:[{name:"block_size",val:" = 1024"},{name:"input_vocab_size",val:" = 10048"},{name:"output_vocab_size",val:" = 10048"},{name:"num_layers",val:" = 12"},{name:"num_heads",val:" = 12"},{name:"hidden_size",val:" = 768"},{name:"dropout",val:" = 0.0"},{name:"bias",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkCoarseConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkCoarseConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkCoarseConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkCoarseConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkCoarseConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkCoarseConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkCoarseConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkCoarseConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkCoarseConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkCoarseConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L164"}}),re=new So({props:{anchor:"transformers.BarkCoarseConfig.example",$$slots:{default:[Or]},$$scope:{ctx:J}}}),Ut=new T({props:{title:"BarkFineConfig",local:"transformers.BarkFineConfig",headingTag:"h2"}}),It=new C({props:{name:"class transformers.BarkFineConfig",anchor:"transformers.BarkFineConfig",parameters:[{name:"tie_word_embeddings",val:" = True"},{name:"n_codes_total",val:" = 8"},{name:"n_codes_given",val:" = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkFineConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkFineConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkFineConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkFineConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkFineConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkFineConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkFineConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkFineConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkFineConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkFineConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.BarkFineConfig.n_codes_total",description:`<strong>n_codes_total</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The total number of audio codebooks predicted. Used in the fine acoustics sub-model.`,name:"n_codes_total"},{anchor:"transformers.BarkFineConfig.n_codes_given",description:`<strong>n_codes_given</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of audio codebooks predicted in the coarse acoustics sub-model. Used in the acoustics
sub-models.`,name:"n_codes_given"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L186"}}),se=new So({props:{anchor:"transformers.BarkFineConfig.example",$$slots:{default:[Ar]},$$scope:{ctx:J}}}),Wt=new T({props:{title:"BarkSemanticConfig",local:"transformers.BarkSemanticConfig",headingTag:"h2"}}),Zt=new C({props:{name:"class transformers.BarkSemanticConfig",anchor:"transformers.BarkSemanticConfig",parameters:[{name:"block_size",val:" = 1024"},{name:"input_vocab_size",val:" = 10048"},{name:"output_vocab_size",val:" = 10048"},{name:"num_layers",val:" = 12"},{name:"num_heads",val:" = 12"},{name:"hidden_size",val:" = 768"},{name:"dropout",val:" = 0.0"},{name:"bias",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkSemanticConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkSemanticConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkSemanticConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/main/en/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkSemanticConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkSemanticConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkSemanticConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkSemanticConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkSemanticConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkSemanticConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkSemanticConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L142"}}),ie=new So({props:{anchor:"transformers.BarkSemanticConfig.example",$$slots:{default:[Kr]},$$scope:{ctx:J}}}),{c(){d=i("meta"),y=o(),b=i("p"),k=o(),p(v.$$.fragment),m=o(),p(M.$$.fragment),wn=o(),ue=i("p"),ue.innerHTML=Na,$n=o(),ge=i("p"),ge.textContent=Fa,Tn=o(),_e=i("ul"),_e.innerHTML=Ga,Bn=o(),be=i("p"),be.textContent=Ha,Cn=o(),ke=i("p"),ke.innerHTML=Sa,xn=o(),p(ye.$$.fragment),Jn=o(),ve=i("p"),ve.innerHTML=Va,jn=o(),p(Me.$$.fragment),zn=o(),we=i("p"),we.textContent=La,Un=o(),p($e.$$.fragment),In=o(),p(Te.$$.fragment),Wn=o(),Be=i("p"),Be.textContent=Ya,Zn=o(),Ce=i("p"),Ce.innerHTML=Xa,Pn=o(),p(xe.$$.fragment),Nn=o(),Je=i("p"),Je.innerHTML=qa,Fn=o(),p(je.$$.fragment),Gn=o(),ze=i("p"),ze.textContent=Ea,Hn=o(),p(Ue.$$.fragment),Sn=o(),Ie=i("p"),Ie.innerHTML=Ra,Vn=o(),p(We.$$.fragment),Ln=o(),Ze=i("p"),Ze.textContent=Da,Yn=o(),p(Pe.$$.fragment),Xn=o(),Ne=i("p"),Ne.innerHTML=Qa,qn=o(),Fe=i("p"),Fe.innerHTML=Oa,En=o(),p(Ge.$$.fragment),Rn=o(),p(He.$$.fragment),Dn=o(),Se=i("p"),Se.innerHTML=Aa,Qn=o(),p(Ve.$$.fragment),On=o(),p(Le.$$.fragment),An=o(),Ye=i("p"),Ye.textContent=Ka,Kn=o(),R=i("div"),R.innerHTML=er,eo=o(),Xe=i("p"),Xe.innerHTML=tr,to=o(),qe=i("p"),qe.textContent=nr,no=o(),p(Ee.$$.fragment),oo=o(),Re=i("p"),Re.textContent=or,ao=o(),p(De.$$.fragment),ro=o(),Qe=i("p"),Qe.innerHTML=ar,so=o(),p(Oe.$$.fragment),io=o(),Ae=i("p"),Ae.innerHTML=rr,lo=o(),p(Ke.$$.fragment),co=o(),et=i("p"),et.innerHTML=sr,mo=o(),p(tt.$$.fragment),po=o(),nt=i("p"),nt.innerHTML=ir,fo=o(),p(ot.$$.fragment),ho=o(),at=i("p"),at.textContent=lr,uo=o(),p(rt.$$.fragment),go=o(),p(st.$$.fragment),_o=o(),j=i("div"),p(it.$$.fragment),Vo=o(),Ht=i("p"),Ht.innerHTML=dr,Lo=o(),St=i("p"),St.innerHTML=cr,Yo=o(),Vt=i("p"),Vt.innerHTML=mr,Xo=o(),D=i("div"),p(lt.$$.fragment),qo=o(),Lt=i("p"),Lt.innerHTML=pr,bo=o(),p(dt.$$.fragment),ko=o(),z=i("div"),p(ct.$$.fragment),Eo=o(),Yt=i("p"),Yt.textContent=fr,Ro=o(),Q=i("div"),p(mt.$$.fragment),Do=o(),Xt=i("p"),Xt.innerHTML=hr,Qo=o(),O=i("div"),p(pt.$$.fragment),Oo=o(),qt=i("p"),qt.textContent=ur,Ao=o(),A=i("div"),p(ft.$$.fragment),Ko=o(),Et=i("p"),Et.innerHTML=gr,yo=o(),p(ht.$$.fragment),vo=o(),B=i("div"),p(ut.$$.fragment),ea=o(),Rt=i("p"),Rt.textContent=_r,ta=o(),Dt=i("ul"),Dt.innerHTML=br,na=o(),Qt=i("p"),Qt.textContent=kr,oa=o(),Ot=i("p"),Ot.innerHTML=yr,aa=o(),At=i("p"),At.innerHTML=vr,ra=o(),S=i("div"),p(gt.$$.fragment),sa=o(),Kt=i("p"),Kt.innerHTML=Mr,ia=o(),p(K.$$.fragment),la=o(),ee=i("div"),p(_t.$$.fragment),da=o(),en=i("p"),en.textContent=wr,Mo=o(),p(bt.$$.fragment),wo=o(),I=i("div"),p(kt.$$.fragment),ca=o(),tn=i("p"),tn.innerHTML=$r,ma=o(),nn=i("p"),nn.innerHTML=Tr,pa=o(),V=i("div"),p(yt.$$.fragment),fa=o(),on=i("p"),on.innerHTML=Br,ha=o(),p(te.$$.fragment),$o=o(),p(vt.$$.fragment),To=o(),W=i("div"),p(Mt.$$.fragment),ua=o(),an=i("p"),an.innerHTML=Cr,ga=o(),rn=i("p"),rn.innerHTML=xr,_a=o(),L=i("div"),p(wt.$$.fragment),ba=o(),sn=i("p"),sn.innerHTML=Jr,ka=o(),p(ne.$$.fragment),Bo=o(),p($t.$$.fragment),Co=o(),Z=i("div"),p(Tt.$$.fragment),ya=o(),ln=i("p"),ln.innerHTML=jr,va=o(),dn=i("p"),dn.innerHTML=zr,Ma=o(),Y=i("div"),p(Bt.$$.fragment),wa=o(),cn=i("p"),cn.innerHTML=Ur,$a=o(),p(oe.$$.fragment),xo=o(),p(Ct.$$.fragment),Jo=o(),q=i("div"),p(xt.$$.fragment),Ta=o(),X=i("div"),p(Jt.$$.fragment),Ba=o(),mn=i("p"),mn.innerHTML=Ir,Ca=o(),p(ae.$$.fragment),jo=o(),p(jt.$$.fragment),zo=o(),P=i("div"),p(zt.$$.fragment),xa=o(),pn=i("p"),pn.innerHTML=Wr,Ja=o(),fn=i("p"),fn.innerHTML=Zr,ja=o(),p(re.$$.fragment),Uo=o(),p(Ut.$$.fragment),Io=o(),N=i("div"),p(It.$$.fragment),za=o(),hn=i("p"),hn.innerHTML=Pr,Ua=o(),un=i("p"),un.innerHTML=Nr,Ia=o(),p(se.$$.fragment),Wo=o(),p(Wt.$$.fragment),Zo=o(),F=i("div"),p(Zt.$$.fragment),Wa=o(),gn=i("p"),gn.innerHTML=Fr,Za=o(),_n=i("p"),_n.innerHTML=Gr,Pa=o(),p(ie.$$.fragment),Po=o(),Mn=i("p"),this.h()},l(e){const t=Yr("svelte-u9bgzb",document.head);d=l(t,"META",{name:!0,content:!0}),t.forEach(n),y=a(e),b=l(e,"P",{}),w(b).forEach(n),k=a(e),f(v.$$.fragment,e),m=a(e),f(M.$$.fragment,e),wn=a(e),ue=l(e,"P",{"data-svelte-h":!0}),c(ue)!=="svelte-luezb9"&&(ue.innerHTML=Na),$n=a(e),ge=l(e,"P",{"data-svelte-h":!0}),c(ge)!=="svelte-x1eizn"&&(ge.textContent=Fa),Tn=a(e),_e=l(e,"UL",{"data-svelte-h":!0}),c(_e)!=="svelte-1753ozp"&&(_e.innerHTML=Ga),Bn=a(e),be=l(e,"P",{"data-svelte-h":!0}),c(be)!=="svelte-birews"&&(be.textContent=Ha),Cn=a(e),ke=l(e,"P",{"data-svelte-h":!0}),c(ke)!=="svelte-1m4odbp"&&(ke.innerHTML=Sa),xn=a(e),f(ye.$$.fragment,e),Jn=a(e),ve=l(e,"P",{"data-svelte-h":!0}),c(ve)!=="svelte-d9f63q"&&(ve.innerHTML=Va),jn=a(e),f(Me.$$.fragment,e),zn=a(e),we=l(e,"P",{"data-svelte-h":!0}),c(we)!=="svelte-1c57ze6"&&(we.textContent=La),Un=a(e),f($e.$$.fragment,e),In=a(e),f(Te.$$.fragment,e),Wn=a(e),Be=l(e,"P",{"data-svelte-h":!0}),c(Be)!=="svelte-1gm6phf"&&(Be.textContent=Ya),Zn=a(e),Ce=l(e,"P",{"data-svelte-h":!0}),c(Ce)!=="svelte-kat9bd"&&(Ce.innerHTML=Xa),Pn=a(e),f(xe.$$.fragment,e),Nn=a(e),Je=l(e,"P",{"data-svelte-h":!0}),c(Je)!=="svelte-jn1vz4"&&(Je.innerHTML=qa),Fn=a(e),f(je.$$.fragment,e),Gn=a(e),ze=l(e,"P",{"data-svelte-h":!0}),c(ze)!=="svelte-1r0gpm4"&&(ze.textContent=Ea),Hn=a(e),f(Ue.$$.fragment,e),Sn=a(e),Ie=l(e,"P",{"data-svelte-h":!0}),c(Ie)!=="svelte-bu0gze"&&(Ie.innerHTML=Ra),Vn=a(e),f(We.$$.fragment,e),Ln=a(e),Ze=l(e,"P",{"data-svelte-h":!0}),c(Ze)!=="svelte-ud6z0l"&&(Ze.textContent=Da),Yn=a(e),f(Pe.$$.fragment,e),Xn=a(e),Ne=l(e,"P",{"data-svelte-h":!0}),c(Ne)!=="svelte-ib5l43"&&(Ne.innerHTML=Qa),qn=a(e),Fe=l(e,"P",{"data-svelte-h":!0}),c(Fe)!=="svelte-1pp3dkd"&&(Fe.innerHTML=Oa),En=a(e),f(Ge.$$.fragment,e),Rn=a(e),f(He.$$.fragment,e),Dn=a(e),Se=l(e,"P",{"data-svelte-h":!0}),c(Se)!=="svelte-1qm2lzz"&&(Se.innerHTML=Aa),Qn=a(e),f(Ve.$$.fragment,e),On=a(e),f(Le.$$.fragment,e),An=a(e),Ye=l(e,"P",{"data-svelte-h":!0}),c(Ye)!=="svelte-19fzd9"&&(Ye.textContent=Ka),Kn=a(e),R=l(e,"DIV",{style:!0,"data-svelte-h":!0}),c(R)!=="svelte-odbvi5"&&(R.innerHTML=er),eo=a(e),Xe=l(e,"P",{"data-svelte-h":!0}),c(Xe)!=="svelte-1uahz5n"&&(Xe.innerHTML=tr),to=a(e),qe=l(e,"P",{"data-svelte-h":!0}),c(qe)!=="svelte-106f51x"&&(qe.textContent=nr),no=a(e),f(Ee.$$.fragment,e),oo=a(e),Re=l(e,"P",{"data-svelte-h":!0}),c(Re)!=="svelte-rwb7y1"&&(Re.textContent=or),ao=a(e),f(De.$$.fragment,e),ro=a(e),Qe=l(e,"P",{"data-svelte-h":!0}),c(Qe)!=="svelte-1xrlue6"&&(Qe.innerHTML=ar),so=a(e),f(Oe.$$.fragment,e),io=a(e),Ae=l(e,"P",{"data-svelte-h":!0}),c(Ae)!=="svelte-llmun5"&&(Ae.innerHTML=rr),lo=a(e),f(Ke.$$.fragment,e),co=a(e),et=l(e,"P",{"data-svelte-h":!0}),c(et)!=="svelte-ff55s5"&&(et.innerHTML=sr),mo=a(e),f(tt.$$.fragment,e),po=a(e),nt=l(e,"P",{"data-svelte-h":!0}),c(nt)!=="svelte-sc737i"&&(nt.innerHTML=ir),fo=a(e),f(ot.$$.fragment,e),ho=a(e),at=l(e,"P",{"data-svelte-h":!0}),c(at)!=="svelte-1bkdry6"&&(at.textContent=lr),uo=a(e),f(rt.$$.fragment,e),go=a(e),f(st.$$.fragment,e),_o=a(e),j=l(e,"DIV",{class:!0});var G=w(j);f(it.$$.fragment,G),Vo=a(G),Ht=l(G,"P",{"data-svelte-h":!0}),c(Ht)!=="svelte-xrbchp"&&(Ht.innerHTML=dr),Lo=a(G),St=l(G,"P",{"data-svelte-h":!0}),c(St)!=="svelte-c6ui3q"&&(St.innerHTML=cr),Yo=a(G),Vt=l(G,"P",{"data-svelte-h":!0}),c(Vt)!=="svelte-o55m63"&&(Vt.innerHTML=mr),Xo=a(G),D=l(G,"DIV",{class:!0});var Pt=w(D);f(lt.$$.fragment,Pt),qo=a(Pt),Lt=l(Pt,"P",{"data-svelte-h":!0}),c(Lt)!=="svelte-1ymnxh9"&&(Lt.innerHTML=pr),Pt.forEach(n),G.forEach(n),bo=a(e),f(dt.$$.fragment,e),ko=a(e),z=l(e,"DIV",{class:!0});var H=w(z);f(ct.$$.fragment,H),Eo=a(H),Yt=l(H,"P",{"data-svelte-h":!0}),c(Yt)!=="svelte-1xfrjvw"&&(Yt.textContent=fr),Ro=a(H),Q=l(H,"DIV",{class:!0});var Nt=w(Q);f(mt.$$.fragment,Nt),Do=a(Nt),Xt=l(Nt,"P",{"data-svelte-h":!0}),c(Xt)!=="svelte-1hlbbl8"&&(Xt.innerHTML=hr),Nt.forEach(n),Qo=a(H),O=l(H,"DIV",{class:!0});var Ft=w(O);f(pt.$$.fragment,Ft),Oo=a(Ft),qt=l(Ft,"P",{"data-svelte-h":!0}),c(qt)!=="svelte-1xt1aup"&&(qt.textContent=ur),Ft.forEach(n),Ao=a(H),A=l(H,"DIV",{class:!0});var Gt=w(A);f(ft.$$.fragment,Gt),Ko=a(Gt),Et=l(Gt,"P",{"data-svelte-h":!0}),c(Et)!=="svelte-6jsez3"&&(Et.innerHTML=gr),Gt.forEach(n),H.forEach(n),yo=a(e),f(ht.$$.fragment,e),vo=a(e),B=l(e,"DIV",{class:!0});var x=w(B);f(ut.$$.fragment,x),ea=a(x),Rt=l(x,"P",{"data-svelte-h":!0}),c(Rt)!=="svelte-xp33tl"&&(Rt.textContent=_r),ta=a(x),Dt=l(x,"UL",{"data-svelte-h":!0}),c(Dt)!=="svelte-70o96y"&&(Dt.innerHTML=br),na=a(x),Qt=l(x,"P",{"data-svelte-h":!0}),c(Qt)!=="svelte-beeiv6"&&(Qt.textContent=kr),oa=a(x),Ot=l(x,"P",{"data-svelte-h":!0}),c(Ot)!=="svelte-6pahdo"&&(Ot.innerHTML=yr),aa=a(x),At=l(x,"P",{"data-svelte-h":!0}),c(At)!=="svelte-hswkmf"&&(At.innerHTML=vr),ra=a(x),S=l(x,"DIV",{class:!0});var E=w(S);f(gt.$$.fragment,E),sa=a(E),Kt=l(E,"P",{"data-svelte-h":!0}),c(Kt)!=="svelte-4azpa"&&(Kt.innerHTML=Mr),ia=a(E),f(K.$$.fragment,E),E.forEach(n),la=a(x),ee=l(x,"DIV",{class:!0});var Fo=w(ee);f(_t.$$.fragment,Fo),da=a(Fo),en=l(Fo,"P",{"data-svelte-h":!0}),c(en)!=="svelte-19e6niw"&&(en.textContent=wr),Fo.forEach(n),x.forEach(n),Mo=a(e),f(bt.$$.fragment,e),wo=a(e),I=l(e,"DIV",{class:!0});var le=w(I);f(kt.$$.fragment,le),ca=a(le),tn=l(le,"P",{"data-svelte-h":!0}),c(tn)!=="svelte-17p3s98"&&(tn.innerHTML=$r),ma=a(le),nn=l(le,"P",{"data-svelte-h":!0}),c(nn)!=="svelte-hswkmf"&&(nn.innerHTML=Tr),pa=a(le),V=l(le,"DIV",{class:!0});var bn=w(V);f(yt.$$.fragment,bn),fa=a(bn),on=l(bn,"P",{"data-svelte-h":!0}),c(on)!=="svelte-1nhjvbx"&&(on.innerHTML=Br),ha=a(bn),f(te.$$.fragment,bn),bn.forEach(n),le.forEach(n),$o=a(e),f(vt.$$.fragment,e),To=a(e),W=l(e,"DIV",{class:!0});var de=w(W);f(Mt.$$.fragment,de),ua=a(de),an=l(de,"P",{"data-svelte-h":!0}),c(an)!=="svelte-183clye"&&(an.innerHTML=Cr),ga=a(de),rn=l(de,"P",{"data-svelte-h":!0}),c(rn)!=="svelte-hswkmf"&&(rn.innerHTML=xr),_a=a(de),L=l(de,"DIV",{class:!0});var kn=w(L);f(wt.$$.fragment,kn),ba=a(kn),sn=l(kn,"P",{"data-svelte-h":!0}),c(sn)!=="svelte-1nhjvbx"&&(sn.innerHTML=Jr),ka=a(kn),f(ne.$$.fragment,kn),kn.forEach(n),de.forEach(n),Bo=a(e),f($t.$$.fragment,e),Co=a(e),Z=l(e,"DIV",{class:!0});var ce=w(Z);f(Tt.$$.fragment,ce),ya=a(ce),ln=l(ce,"P",{"data-svelte-h":!0}),c(ln)!=="svelte-1rktjcb"&&(ln.innerHTML=jr),va=a(ce),dn=l(ce,"P",{"data-svelte-h":!0}),c(dn)!=="svelte-hswkmf"&&(dn.innerHTML=zr),Ma=a(ce),Y=l(ce,"DIV",{class:!0});var yn=w(Y);f(Bt.$$.fragment,yn),wa=a(yn),cn=l(yn,"P",{"data-svelte-h":!0}),c(cn)!=="svelte-1jzzrjh"&&(cn.innerHTML=Ur),$a=a(yn),f(oe.$$.fragment,yn),yn.forEach(n),ce.forEach(n),xo=a(e),f(Ct.$$.fragment,e),Jo=a(e),q=l(e,"DIV",{class:!0});var Go=w(q);f(xt.$$.fragment,Go),Ta=a(Go),X=l(Go,"DIV",{class:!0});var vn=w(X);f(Jt.$$.fragment,vn),Ba=a(vn),mn=l(vn,"P",{"data-svelte-h":!0}),c(mn)!=="svelte-1nhjvbx"&&(mn.innerHTML=Ir),Ca=a(vn),f(ae.$$.fragment,vn),vn.forEach(n),Go.forEach(n),jo=a(e),f(jt.$$.fragment,e),zo=a(e),P=l(e,"DIV",{class:!0});var me=w(P);f(zt.$$.fragment,me),xa=a(me),pn=l(me,"P",{"data-svelte-h":!0}),c(pn)!=="svelte-tf2bl9"&&(pn.innerHTML=Wr),Ja=a(me),fn=l(me,"P",{"data-svelte-h":!0}),c(fn)!=="svelte-o55m63"&&(fn.innerHTML=Zr),ja=a(me),f(re.$$.fragment,me),me.forEach(n),Uo=a(e),f(Ut.$$.fragment,e),Io=a(e),N=l(e,"DIV",{class:!0});var pe=w(N);f(It.$$.fragment,pe),za=a(pe),hn=l(pe,"P",{"data-svelte-h":!0}),c(hn)!=="svelte-266kvh"&&(hn.innerHTML=Pr),Ua=a(pe),un=l(pe,"P",{"data-svelte-h":!0}),c(un)!=="svelte-o55m63"&&(un.innerHTML=Nr),Ia=a(pe),f(se.$$.fragment,pe),pe.forEach(n),Wo=a(e),f(Wt.$$.fragment,e),Zo=a(e),F=l(e,"DIV",{class:!0});var fe=w(F);f(Zt.$$.fragment,fe),Wa=a(fe),gn=l(fe,"P",{"data-svelte-h":!0}),c(gn)!=="svelte-uapq2h"&&(gn.innerHTML=Fr),Za=a(fe),_n=l(fe,"P",{"data-svelte-h":!0}),c(_n)!=="svelte-o55m63"&&(_n.innerHTML=Gr),Pa=a(fe),f(ie.$$.fragment,fe),fe.forEach(n),Po=a(e),Mn=l(e,"P",{}),w(Mn).forEach(n),this.h()},h(){$(d,"name","hf:doc:metadata"),$(d,"content",ts),Xr(R,"text-align","center"),$(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){s(document.head,d),r(e,y,t),r(e,b,t),r(e,k,t),h(v,e,t),r(e,m,t),h(M,e,t),r(e,wn,t),r(e,ue,t),r(e,$n,t),r(e,ge,t),r(e,Tn,t),r(e,_e,t),r(e,Bn,t),r(e,be,t),r(e,Cn,t),r(e,ke,t),r(e,xn,t),h(ye,e,t),r(e,Jn,t),r(e,ve,t),r(e,jn,t),h(Me,e,t),r(e,zn,t),r(e,we,t),r(e,Un,t),h($e,e,t),r(e,In,t),h(Te,e,t),r(e,Wn,t),r(e,Be,t),r(e,Zn,t),r(e,Ce,t),r(e,Pn,t),h(xe,e,t),r(e,Nn,t),r(e,Je,t),r(e,Fn,t),h(je,e,t),r(e,Gn,t),r(e,ze,t),r(e,Hn,t),h(Ue,e,t),r(e,Sn,t),r(e,Ie,t),r(e,Vn,t),h(We,e,t),r(e,Ln,t),r(e,Ze,t),r(e,Yn,t),h(Pe,e,t),r(e,Xn,t),r(e,Ne,t),r(e,qn,t),r(e,Fe,t),r(e,En,t),h(Ge,e,t),r(e,Rn,t),h(He,e,t),r(e,Dn,t),r(e,Se,t),r(e,Qn,t),h(Ve,e,t),r(e,On,t),h(Le,e,t),r(e,An,t),r(e,Ye,t),r(e,Kn,t),r(e,R,t),r(e,eo,t),r(e,Xe,t),r(e,to,t),r(e,qe,t),r(e,no,t),h(Ee,e,t),r(e,oo,t),r(e,Re,t),r(e,ao,t),h(De,e,t),r(e,ro,t),r(e,Qe,t),r(e,so,t),h(Oe,e,t),r(e,io,t),r(e,Ae,t),r(e,lo,t),h(Ke,e,t),r(e,co,t),r(e,et,t),r(e,mo,t),h(tt,e,t),r(e,po,t),r(e,nt,t),r(e,fo,t),h(ot,e,t),r(e,ho,t),r(e,at,t),r(e,uo,t),h(rt,e,t),r(e,go,t),h(st,e,t),r(e,_o,t),r(e,j,t),h(it,j,null),s(j,Vo),s(j,Ht),s(j,Lo),s(j,St),s(j,Yo),s(j,Vt),s(j,Xo),s(j,D),h(lt,D,null),s(D,qo),s(D,Lt),r(e,bo,t),h(dt,e,t),r(e,ko,t),r(e,z,t),h(ct,z,null),s(z,Eo),s(z,Yt),s(z,Ro),s(z,Q),h(mt,Q,null),s(Q,Do),s(Q,Xt),s(z,Qo),s(z,O),h(pt,O,null),s(O,Oo),s(O,qt),s(z,Ao),s(z,A),h(ft,A,null),s(A,Ko),s(A,Et),r(e,yo,t),h(ht,e,t),r(e,vo,t),r(e,B,t),h(ut,B,null),s(B,ea),s(B,Rt),s(B,ta),s(B,Dt),s(B,na),s(B,Qt),s(B,oa),s(B,Ot),s(B,aa),s(B,At),s(B,ra),s(B,S),h(gt,S,null),s(S,sa),s(S,Kt),s(S,ia),h(K,S,null),s(B,la),s(B,ee),h(_t,ee,null),s(ee,da),s(ee,en),r(e,Mo,t),h(bt,e,t),r(e,wo,t),r(e,I,t),h(kt,I,null),s(I,ca),s(I,tn),s(I,ma),s(I,nn),s(I,pa),s(I,V),h(yt,V,null),s(V,fa),s(V,on),s(V,ha),h(te,V,null),r(e,$o,t),h(vt,e,t),r(e,To,t),r(e,W,t),h(Mt,W,null),s(W,ua),s(W,an),s(W,ga),s(W,rn),s(W,_a),s(W,L),h(wt,L,null),s(L,ba),s(L,sn),s(L,ka),h(ne,L,null),r(e,Bo,t),h($t,e,t),r(e,Co,t),r(e,Z,t),h(Tt,Z,null),s(Z,ya),s(Z,ln),s(Z,va),s(Z,dn),s(Z,Ma),s(Z,Y),h(Bt,Y,null),s(Y,wa),s(Y,cn),s(Y,$a),h(oe,Y,null),r(e,xo,t),h(Ct,e,t),r(e,Jo,t),r(e,q,t),h(xt,q,null),s(q,Ta),s(q,X),h(Jt,X,null),s(X,Ba),s(X,mn),s(X,Ca),h(ae,X,null),r(e,jo,t),h(jt,e,t),r(e,zo,t),r(e,P,t),h(zt,P,null),s(P,xa),s(P,pn),s(P,Ja),s(P,fn),s(P,ja),h(re,P,null),r(e,Uo,t),h(Ut,e,t),r(e,Io,t),r(e,N,t),h(It,N,null),s(N,za),s(N,hn),s(N,Ua),s(N,un),s(N,Ia),h(se,N,null),r(e,Wo,t),h(Wt,e,t),r(e,Zo,t),r(e,F,t),h(Zt,F,null),s(F,Wa),s(F,gn),s(F,Za),s(F,_n),s(F,Pa),h(ie,F,null),r(e,Po,t),r(e,Mn,t),No=!0},p(e,[t]){const G={};t&2&&(G.$$scope={dirty:t,ctx:e}),K.$set(G);const Pt={};t&2&&(Pt.$$scope={dirty:t,ctx:e}),te.$set(Pt);const H={};t&2&&(H.$$scope={dirty:t,ctx:e}),ne.$set(H);const Nt={};t&2&&(Nt.$$scope={dirty:t,ctx:e}),oe.$set(Nt);const Ft={};t&2&&(Ft.$$scope={dirty:t,ctx:e}),ae.$set(Ft);const Gt={};t&2&&(Gt.$$scope={dirty:t,ctx:e}),re.$set(Gt);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),se.$set(x);const E={};t&2&&(E.$$scope={dirty:t,ctx:e}),ie.$set(E)},i(e){No||(u(v.$$.fragment,e),u(M.$$.fragment,e),u(ye.$$.fragment,e),u(Me.$$.fragment,e),u($e.$$.fragment,e),u(Te.$$.fragment,e),u(xe.$$.fragment,e),u(je.$$.fragment,e),u(Ue.$$.fragment,e),u(We.$$.fragment,e),u(Pe.$$.fragment,e),u(Ge.$$.fragment,e),u(He.$$.fragment,e),u(Ve.$$.fragment,e),u(Le.$$.fragment,e),u(Ee.$$.fragment,e),u(De.$$.fragment,e),u(Oe.$$.fragment,e),u(Ke.$$.fragment,e),u(tt.$$.fragment,e),u(ot.$$.fragment,e),u(rt.$$.fragment,e),u(st.$$.fragment,e),u(it.$$.fragment,e),u(lt.$$.fragment,e),u(dt.$$.fragment,e),u(ct.$$.fragment,e),u(mt.$$.fragment,e),u(pt.$$.fragment,e),u(ft.$$.fragment,e),u(ht.$$.fragment,e),u(ut.$$.fragment,e),u(gt.$$.fragment,e),u(K.$$.fragment,e),u(_t.$$.fragment,e),u(bt.$$.fragment,e),u(kt.$$.fragment,e),u(yt.$$.fragment,e),u(te.$$.fragment,e),u(vt.$$.fragment,e),u(Mt.$$.fragment,e),u(wt.$$.fragment,e),u(ne.$$.fragment,e),u($t.$$.fragment,e),u(Tt.$$.fragment,e),u(Bt.$$.fragment,e),u(oe.$$.fragment,e),u(Ct.$$.fragment,e),u(xt.$$.fragment,e),u(Jt.$$.fragment,e),u(ae.$$.fragment,e),u(jt.$$.fragment,e),u(zt.$$.fragment,e),u(re.$$.fragment,e),u(Ut.$$.fragment,e),u(It.$$.fragment,e),u(se.$$.fragment,e),u(Wt.$$.fragment,e),u(Zt.$$.fragment,e),u(ie.$$.fragment,e),No=!0)},o(e){g(v.$$.fragment,e),g(M.$$.fragment,e),g(ye.$$.fragment,e),g(Me.$$.fragment,e),g($e.$$.fragment,e),g(Te.$$.fragment,e),g(xe.$$.fragment,e),g(je.$$.fragment,e),g(Ue.$$.fragment,e),g(We.$$.fragment,e),g(Pe.$$.fragment,e),g(Ge.$$.fragment,e),g(He.$$.fragment,e),g(Ve.$$.fragment,e),g(Le.$$.fragment,e),g(Ee.$$.fragment,e),g(De.$$.fragment,e),g(Oe.$$.fragment,e),g(Ke.$$.fragment,e),g(tt.$$.fragment,e),g(ot.$$.fragment,e),g(rt.$$.fragment,e),g(st.$$.fragment,e),g(it.$$.fragment,e),g(lt.$$.fragment,e),g(dt.$$.fragment,e),g(ct.$$.fragment,e),g(mt.$$.fragment,e),g(pt.$$.fragment,e),g(ft.$$.fragment,e),g(ht.$$.fragment,e),g(ut.$$.fragment,e),g(gt.$$.fragment,e),g(K.$$.fragment,e),g(_t.$$.fragment,e),g(bt.$$.fragment,e),g(kt.$$.fragment,e),g(yt.$$.fragment,e),g(te.$$.fragment,e),g(vt.$$.fragment,e),g(Mt.$$.fragment,e),g(wt.$$.fragment,e),g(ne.$$.fragment,e),g($t.$$.fragment,e),g(Tt.$$.fragment,e),g(Bt.$$.fragment,e),g(oe.$$.fragment,e),g(Ct.$$.fragment,e),g(xt.$$.fragment,e),g(Jt.$$.fragment,e),g(ae.$$.fragment,e),g(jt.$$.fragment,e),g(zt.$$.fragment,e),g(re.$$.fragment,e),g(Ut.$$.fragment,e),g(It.$$.fragment,e),g(se.$$.fragment,e),g(Wt.$$.fragment,e),g(Zt.$$.fragment,e),g(ie.$$.fragment,e),No=!1},d(e){e&&(n(y),n(b),n(k),n(m),n(wn),n(ue),n($n),n(ge),n(Tn),n(_e),n(Bn),n(be),n(Cn),n(ke),n(xn),n(Jn),n(ve),n(jn),n(zn),n(we),n(Un),n(In),n(Wn),n(Be),n(Zn),n(Ce),n(Pn),n(Nn),n(Je),n(Fn),n(Gn),n(ze),n(Hn),n(Sn),n(Ie),n(Vn),n(Ln),n(Ze),n(Yn),n(Xn),n(Ne),n(qn),n(Fe),n(En),n(Rn),n(Dn),n(Se),n(Qn),n(On),n(An),n(Ye),n(Kn),n(R),n(eo),n(Xe),n(to),n(qe),n(no),n(oo),n(Re),n(ao),n(ro),n(Qe),n(so),n(io),n(Ae),n(lo),n(co),n(et),n(mo),n(po),n(nt),n(fo),n(ho),n(at),n(uo),n(go),n(_o),n(j),n(bo),n(ko),n(z),n(yo),n(vo),n(B),n(Mo),n(wo),n(I),n($o),n(To),n(W),n(Bo),n(Co),n(Z),n(xo),n(Jo),n(q),n(jo),n(zo),n(P),n(Uo),n(Io),n(N),n(Wo),n(Zo),n(F),n(Po),n(Mn)),n(d),_(v,e),_(M,e),_(ye,e),_(Me,e),_($e,e),_(Te,e),_(xe,e),_(je,e),_(Ue,e),_(We,e),_(Pe,e),_(Ge,e),_(He,e),_(Ve,e),_(Le,e),_(Ee,e),_(De,e),_(Oe,e),_(Ke,e),_(tt,e),_(ot,e),_(rt,e),_(st,e),_(it),_(lt),_(dt,e),_(ct),_(mt),_(pt),_(ft),_(ht,e),_(ut),_(gt),_(K),_(_t),_(bt,e),_(kt),_(yt),_(te),_(vt,e),_(Mt),_(wt),_(ne),_($t,e),_(Tt),_(Bt),_(oe),_(Ct,e),_(xt),_(Jt),_(ae),_(jt,e),_(zt),_(re),_(Ut,e),_(It),_(se),_(Wt,e),_(Zt),_(ie)}}}const ts='{"title":"Bark","local":"bark","sections":[{"title":"Overview","local":"overview","sections":[{"title":"Optimizing Bark","local":"optimizing-bark","sections":[{"title":"Using half-precision","local":"using-half-precision","sections":[],"depth":4},{"title":"Using CPU offload","local":"using-cpu-offload","sections":[],"depth":4},{"title":"Using Better Transformer","local":"using-better-transformer","sections":[],"depth":4},{"title":"Using Flash Attention 2","local":"using-flash-attention-2","sections":[{"title":"Installation","local":"installation","sections":[],"depth":5},{"title":"Usage","local":"usage","sections":[],"depth":5},{"title":"Performance comparison","local":"performance-comparison","sections":[],"depth":5}],"depth":4},{"title":"Combining optimization techniques","local":"combining-optimization-techniques","sections":[],"depth":4}],"depth":3},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":3}],"depth":2},{"title":"BarkConfig","local":"transformers.BarkConfig","sections":[],"depth":2},{"title":"BarkProcessor","local":"transformers.BarkProcessor","sections":[],"depth":2},{"title":"BarkModel","local":"transformers.BarkModel","sections":[],"depth":2},{"title":"BarkSemanticModel","local":"transformers.BarkSemanticModel","sections":[],"depth":2},{"title":"BarkCoarseModel","local":"transformers.BarkCoarseModel","sections":[],"depth":2},{"title":"BarkFineModel","local":"transformers.BarkFineModel","sections":[],"depth":2},{"title":"BarkCausalModel","local":"transformers.BarkCausalModel","sections":[],"depth":2},{"title":"BarkCoarseConfig","local":"transformers.BarkCoarseConfig","sections":[],"depth":2},{"title":"BarkFineConfig","local":"transformers.BarkFineConfig","sections":[],"depth":2},{"title":"BarkSemanticConfig","local":"transformers.BarkSemanticConfig","sections":[],"depth":2}],"depth":1}';function ns(J){return Sr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class cs extends Vr{constructor(d){super(),Lr(this,d,ns,es,Hr,{})}}export{cs as component};
