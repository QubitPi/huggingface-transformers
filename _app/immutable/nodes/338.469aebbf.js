import{s as ps,o as ms,n as Q}from"../chunks/scheduler.9bc65507.js";import{S as ds,i as us,g as p,s,r as f,A as fs,h as m,f as l,c as i,j as rs,u as h,x as d,k as Fe,y as hs,a,v as c,d as g,t as b,w as M}from"../chunks/index.707bf1b6.js";import{T as B}from"../chunks/Tip.c2ecdbf4.js";import{C as v}from"../chunks/CodeBlock.54a9f38d.js";import{H as I}from"../chunks/Heading.342b1fa6.js";import{H as Ha,a as xe}from"../chunks/HfOption.6d864328.js";function cs(_){let n,y='Interested in adding a new quantization method to Transformers? Read the <a href="./hf_quantizer">HfQuantizer</a> guide to learn how!';return{c(){n=p("p"),n.innerHTML=y},l(o){n=m(o,"P",{"data-svelte-h":!0}),d(n)!=="svelte-px0yfi"&&(n.innerHTML=y)},m(o,u){a(o,n,u)},p:Q,d(o){o&&l(n)}}}function gs(_){let n,y='Try AWQ quantization with this <a href="https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY" rel="nofollow">notebook</a>!';return{c(){n=p("p"),n.innerHTML=y},l(o){n=m(o,"P",{"data-svelte-h":!0}),d(n)!=="svelte-1yzcfmc"&&(n.innerHTML=y)},m(o,u){a(o,n,u)},p:Q,d(o){o&&l(n)}}}function bs(_){let n,y="Fused modules cannot be combined with other optimization techniques such as FlashAttention-2.";return{c(){n=p("p"),n.textContent=y},l(o){n=m(o,"P",{"data-svelte-h":!0}),d(n)!=="svelte-hwgpn3"&&(n.textContent=y)},m(o,u){a(o,n,u)},p:Q,d(o){o&&l(n)}}}function Ms(_){let n,y='To enable fused modules for supported architectures, create an <a href="/docs/transformers/main/en/main_classes/quantization#transformers.AwqConfig">AwqConfig</a> and set the parameters <code>fuse_max_seq_len</code> and <code>do_fuse=True</code>. The <code>fuse_max_seq_len</code> parameter is the total sequence length and it should include the context length and the expected generation length. You can set it to a larger value to be safe.',o,u,r='For example, to fuse the AWQ modules of the <a href="https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ" rel="nofollow">TheBloke/Mistral-7B-OpenOrca-AWQ</a> model.',T,U,C;return U=new v({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXdxQ29uZmlnJTJDJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMlRoZUJsb2tlJTJGTWlzdHJhbC03Qi1PcGVuT3JjYS1BV1ElMjIlMEElMEFxdWFudGl6YXRpb25fY29uZmlnJTIwJTNEJTIwQXdxQ29uZmlnKCUwQSUyMCUyMCUyMCUyMGJpdHMlM0Q0JTJDJTBBJTIwJTIwJTIwJTIwZnVzZV9tYXhfc2VxX2xlbiUzRDUxMiUyQyUwQSUyMCUyMCUyMCUyMGRvX2Z1c2UlM0RUcnVlJTJDJTBBKSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWcpLnRvKDAp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AwqConfig, AutoModelForCausalLM

model_id = <span class="hljs-string">&quot;TheBloke/Mistral-7B-OpenOrca-AWQ&quot;</span>

quantization_config = AwqConfig(
    bits=<span class="hljs-number">4</span>,
    fuse_max_seq_len=<span class="hljs-number">512</span>,
    do_fuse=<span class="hljs-literal">True</span>,
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config).to(<span class="hljs-number">0</span>)`,wrap:!1}}),{c(){n=p("p"),n.innerHTML=y,o=s(),u=p("p"),u.innerHTML=r,T=s(),f(U.$$.fragment)},l(w){n=m(w,"P",{"data-svelte-h":!0}),d(n)!=="svelte-cvz4de"&&(n.innerHTML=y),o=i(w),u=m(w,"P",{"data-svelte-h":!0}),d(u)!=="svelte-1zxz2i"&&(u.innerHTML=r),T=i(w),h(U.$$.fragment,w)},m(w,q){a(w,n,q),a(w,o,q),a(w,u,q),a(w,T,q),c(U,w,q),C=!0},p:Q,i(w){C||(g(U.$$.fragment,w),C=!0)},o(w){b(U.$$.fragment,w),C=!1},d(w){w&&(l(n),l(o),l(u),l(T)),M(U,w)}}}function ys(_){let n,y='For architectures that don’t support fused modules yet, you need to create a custom fusing mapping to define which modules need to be fused with the <code>modules_to_fuse</code> parameter. For example, to fuse the AWQ modules of the <a href="https://huggingface.co/TheBloke/Yi-34B-AWQ" rel="nofollow">TheBloke/Yi-34B-AWQ</a> model.',o,u,r,T,U="The parameter <code>modules_to_fuse</code> should include:",C,w,q="<li><code>&quot;attention&quot;</code>: The names of the attention layers to fuse in the following order: query, key, value and output projection layer. If you don’t want to fuse these layers, pass an empty list.</li> <li><code>&quot;layernorm&quot;</code>: The names of all the LayerNorm layers you want to replace with a custom fused LayerNorm. If you don’t want to fuse these layers, pass an empty list.</li> <li><code>&quot;mlp&quot;</code>: The names of the MLP layers you want to fuse into a single MLP layer in the order: (gate (dense, layer, post-attention) / up / down layers).</li> <li><code>&quot;use_alibi&quot;</code>: If your model uses ALiBi positional embedding.</li> <li><code>&quot;num_attention_heads&quot;</code>: The number of attention heads.</li> <li><code>&quot;num_key_value_heads&quot;</code>: The number of key value heads that should be used to implement Grouped Query Attention (GQA). If <code>num_key_value_heads=num_attention_heads</code>, the model will use Multi Head Attention (MHA), if <code>num_key_value_heads=1</code> the model will use Multi Query Attention (MQA), otherwise GQA is used.</li> <li><code>&quot;hidden_size&quot;</code>: The dimension of the hidden representations.</li>",Z;return u=new v({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXdxQ29uZmlnJTJDJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMlRoZUJsb2tlJTJGWWktMzRCLUFXUSUyMiUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBBd3FDb25maWcoJTBBJTIwJTIwJTIwJTIwYml0cyUzRDQlMkMlMEElMjAlMjAlMjAlMjBmdXNlX21heF9zZXFfbGVuJTNENTEyJTJDJTBBJTIwJTIwJTIwJTIwbW9kdWxlc190b19mdXNlJTNEJTdCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIyYXR0ZW50aW9uJTIyJTNBJTIwJTVCJTIycV9wcm9qJTIyJTJDJTIwJTIya19wcm9qJTIyJTJDJTIwJTIydl9wcm9qJTIyJTJDJTIwJTIyb19wcm9qJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIybGF5ZXJub3JtJTIyJTNBJTIwJTVCJTIybG4xJTIyJTJDJTIwJTIybG4yJTIyJTJDJTIwJTIybm9ybSUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMm1scCUyMiUzQSUyMCU1QiUyMmdhdGVfcHJvaiUyMiUyQyUyMCUyMnVwX3Byb2olMjIlMkMlMjAlMjJkb3duX3Byb2olMjIlNUQlMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjJ1c2VfYWxpYmklMjIlM0ElMjBGYWxzZSUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMm51bV9hdHRlbnRpb25faGVhZHMlMjIlM0ElMjA1NiUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMm51bV9rZXlfdmFsdWVfaGVhZHMlMjIlM0ElMjA4JTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIyaGlkZGVuX3NpemUlMjIlM0ElMjA3MTY4JTBBJTIwJTIwJTIwJTIwJTdEJTBBKSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWcpLnRvKDAp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AwqConfig, AutoModelForCausalLM

model_id = <span class="hljs-string">&quot;TheBloke/Yi-34B-AWQ&quot;</span>

quantization_config = AwqConfig(
    bits=<span class="hljs-number">4</span>,
    fuse_max_seq_len=<span class="hljs-number">512</span>,
    modules_to_fuse={
        <span class="hljs-string">&quot;attention&quot;</span>: [<span class="hljs-string">&quot;q_proj&quot;</span>, <span class="hljs-string">&quot;k_proj&quot;</span>, <span class="hljs-string">&quot;v_proj&quot;</span>, <span class="hljs-string">&quot;o_proj&quot;</span>],
        <span class="hljs-string">&quot;layernorm&quot;</span>: [<span class="hljs-string">&quot;ln1&quot;</span>, <span class="hljs-string">&quot;ln2&quot;</span>, <span class="hljs-string">&quot;norm&quot;</span>],
        <span class="hljs-string">&quot;mlp&quot;</span>: [<span class="hljs-string">&quot;gate_proj&quot;</span>, <span class="hljs-string">&quot;up_proj&quot;</span>, <span class="hljs-string">&quot;down_proj&quot;</span>],
        <span class="hljs-string">&quot;use_alibi&quot;</span>: <span class="hljs-literal">False</span>,
        <span class="hljs-string">&quot;num_attention_heads&quot;</span>: <span class="hljs-number">56</span>,
        <span class="hljs-string">&quot;num_key_value_heads&quot;</span>: <span class="hljs-number">8</span>,
        <span class="hljs-string">&quot;hidden_size&quot;</span>: <span class="hljs-number">7168</span>
    }
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config).to(<span class="hljs-number">0</span>)`,wrap:!1}}),{c(){n=p("p"),n.innerHTML=y,o=s(),f(u.$$.fragment),r=s(),T=p("p"),T.innerHTML=U,C=s(),w=p("ul"),w.innerHTML=q},l(j){n=m(j,"P",{"data-svelte-h":!0}),d(n)!=="svelte-ecilwg"&&(n.innerHTML=y),o=i(j),h(u.$$.fragment,j),r=i(j),T=m(j,"P",{"data-svelte-h":!0}),d(T)!=="svelte-1divolh"&&(T.innerHTML=U),C=i(j),w=m(j,"UL",{"data-svelte-h":!0}),d(w)!=="svelte-ln3msn"&&(w.innerHTML=q)},m(j,k){a(j,n,k),a(j,o,k),c(u,j,k),a(j,r,k),a(j,T,k),a(j,C,k),a(j,w,k),Z=!0},p:Q,i(j){Z||(g(u.$$.fragment,j),Z=!0)},o(j){b(u.$$.fragment,j),Z=!1},d(j){j&&(l(n),l(o),l(r),l(T),l(C),l(w)),M(u,j)}}}function Ts(_){let n,y,o,u;return n=new xe({props:{id:"fuse",option:"supported architectures",$$slots:{default:[Ms]},$$scope:{ctx:_}}}),o=new xe({props:{id:"fuse",option:"unsupported architectures",$$slots:{default:[ys]},$$scope:{ctx:_}}}),{c(){f(n.$$.fragment),y=s(),f(o.$$.fragment)},l(r){h(n.$$.fragment,r),y=i(r),h(o.$$.fragment,r)},m(r,T){c(n,r,T),a(r,y,T),c(o,r,T),u=!0},p(r,T){const U={};T&2&&(U.$$scope={dirty:T,ctx:r}),n.$set(U);const C={};T&2&&(C.$$scope={dirty:T,ctx:r}),o.$set(C)},i(r){u||(g(n.$$.fragment,r),g(o.$$.fragment,r),u=!0)},o(r){b(n.$$.fragment,r),b(o.$$.fragment,r),u=!1},d(r){r&&l(y),M(n,r),M(o,r)}}}function ws(_){let n,y='Try GPTQ quantization with PEFT in this <a href="https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing" rel="nofollow">notebook</a> and learn more about it’s details in this <a href="https://huggingface.co/blog/gptq-integration" rel="nofollow">blog post</a>!';return{c(){n=p("p"),n.innerHTML=y},l(o){n=m(o,"P",{"data-svelte-h":!0}),d(n)!=="svelte-1tfpiad"&&(n.innerHTML=y)},m(o,u){a(o,n,u)},p:Q,d(o){o&&l(n)}}}function $s(_){let n,y='Depending on your hardware, it can take some time to quantize a model from scratch. It can take ~5 minutes to quantize the <a href="https://huggingface.co/facebook/opt-350m" rel="nofollow">facebook/opt-350m</a> model on a free-tier Google Colab GPU, but it’ll take ~4 hours to quantize a 175B parameter model on a NVIDIA A100. Before you quantize a model, it is a good idea to check the Hub if a GPTQ-quantized version of the model already exists.';return{c(){n=p("p"),n.innerHTML=y},l(o){n=m(o,"P",{"data-svelte-h":!0}),d(n)!=="svelte-jdzahk"&&(n.innerHTML=y)},m(o,u){a(o,n,u)},p:Q,d(o){o&&l(n)}}}function Js(_){let n,y="Only 4-bit models are supported, and we recommend deactivating the ExLlama kernels if you’re finetuning a quantized model with PEFT.";return{c(){n=p("p"),n.textContent=y},l(o){n=m(o,"P",{"data-svelte-h":!0}),d(n)!=="svelte-14ppi1y"&&(n.textContent=y)},m(o,u){a(o,n,u)},p:Q,d(o){o&&l(n)}}}function _s(_){let n,y;return n=new v({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGFjY2VsZXJhdGUlMjBiaXRzYW5kYnl0ZXMlM0UwLjM3LjA=",highlighted:"pip install transformers accelerate bitsandbytes&gt;0.37.0",wrap:!1}}),{c(){f(n.$$.fragment)},l(o){h(n.$$.fragment,o)},m(o,u){c(n,o,u),y=!0},p:Q,i(o){y||(g(n.$$.fragment,o),y=!0)},o(o){b(n.$$.fragment,o),y=!1},d(o){M(n,o)}}}function vs(_){let n,y;return n=new v({props:{code:"cGlwJTIwaW5zdGFsbCUyMGJpdHNhbmRieXRlcyUzRSUzRDAuMzkuMCUwQXBpcCUyMGluc3RhbGwlMjAtLXVwZ3JhZGUlMjBhY2NlbGVyYXRlJTBBcGlwJTIwaW5zdGFsbCUyMC0tdXBncmFkZSUyMHRyYW5zZm9ybWVycw==",highlighted:`pip install bitsandbytes&gt;=0.39.0
pip install --upgrade accelerate
pip install --upgrade transformers`,wrap:!1}}),{c(){f(n.$$.fragment)},l(o){h(n.$$.fragment,o)},m(o,u){c(n,o,u),y=!0},p:Q,i(o){y||(g(n.$$.fragment,o),y=!0)},o(o){b(n.$$.fragment,o),y=!1},d(o){M(n,o)}}}function Us(_){let n,y,o,u;return n=new xe({props:{id:"bnb",option:"8-bit",$$slots:{default:[_s]},$$scope:{ctx:_}}}),o=new xe({props:{id:"bnb",option:"4-bit",$$slots:{default:[vs]},$$scope:{ctx:_}}}),{c(){f(n.$$.fragment),y=s(),f(o.$$.fragment)},l(r){h(n.$$.fragment,r),y=i(r),h(o.$$.fragment,r)},m(r,T){c(n,r,T),a(r,y,T),c(o,r,T),u=!0},p(r,T){const U={};T&2&&(U.$$scope={dirty:T,ctx:r}),n.$set(U);const C={};T&2&&(C.$$scope={dirty:T,ctx:r}),o.$set(C)},i(r){u||(g(n.$$.fragment,r),g(o.$$.fragment,r),u=!0)},o(r){b(n.$$.fragment,r),b(o.$$.fragment,r),u=!1},d(r){r&&l(y),M(n,r),M(o,r)}}}function js(_){let n,y="Quantizing a model in 8-bit halves the memory-usage, and for large models, set <code>device_map=&quot;auto&quot;</code> to efficiently use the GPUs available:",o,u,r,T,U="By default, all the other modules such as <code>torch.nn.LayerNorm</code> are converted to <code>torch.float16</code>. You can change the data type of these modules with the <code>torch_dtype</code> parameter if you want:",C,w,q,Z,j='Once a model is quantized to 8-bit, you can’t push the quantized weights to the Hub unless you’re using the latest version of Transformers and bitsandbytes. If you have the latest versions, then you can push the 8-bit model to the Hub with the <a href="/docs/transformers/main/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method. The quantization config.json file is pushed first, followed by the quantized model weights.',k,$,G;return u=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),w=new v({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGb3B0LTM1MG0lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDMyKSUwQW1vZGVsXzhiaXQubW9kZWwuZGVjb2Rlci5sYXllcnMlNUItMSU1RC5maW5hbF9sYXllcl9ub3JtLndlaWdodC5kdHlwZQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, torch_dtype=torch.float32)
model_8bit.model.decoder.layers[-<span class="hljs-number">1</span>].final_layer_norm.weight.dtype`,wrap:!1}}),$=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmJpZ3NjaWVuY2UlMkZibG9vbS01NjBtJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmJpZ3NjaWVuY2UlMkZibG9vbS01NjBtJTIyKSUwQSUwQW1vZGVsLnB1c2hfdG9faHViKCUyMmJsb29tLTU2MG0tOGJpdCUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom-560m&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom-560m&quot;</span>)

model.push_to_hub(<span class="hljs-string">&quot;bloom-560m-8bit&quot;</span>)`,wrap:!1}}),{c(){n=p("p"),n.innerHTML=y,o=s(),f(u.$$.fragment),r=s(),T=p("p"),T.innerHTML=U,C=s(),f(w.$$.fragment),q=s(),Z=p("p"),Z.innerHTML=j,k=s(),f($.$$.fragment)},l(J){n=m(J,"P",{"data-svelte-h":!0}),d(n)!=="svelte-1dz8xeq"&&(n.innerHTML=y),o=i(J),h(u.$$.fragment,J),r=i(J),T=m(J,"P",{"data-svelte-h":!0}),d(T)!=="svelte-kfzkum"&&(T.innerHTML=U),C=i(J),h(w.$$.fragment,J),q=i(J),Z=m(J,"P",{"data-svelte-h":!0}),d(Z)!=="svelte-951rn"&&(Z.innerHTML=j),k=i(J),h($.$$.fragment,J)},m(J,z){a(J,n,z),a(J,o,z),c(u,J,z),a(J,r,z),a(J,T,z),a(J,C,z),c(w,J,z),a(J,q,z),a(J,Z,z),a(J,k,z),c($,J,z),G=!0},p:Q,i(J){G||(g(u.$$.fragment,J),g(w.$$.fragment,J),g($.$$.fragment,J),G=!0)},o(J){b(u.$$.fragment,J),b(w.$$.fragment,J),b($.$$.fragment,J),G=!1},d(J){J&&(l(n),l(o),l(r),l(T),l(C),l(q),l(Z),l(k)),M(u,J),M(w,J),M($,J)}}}function Cs(_){let n,y="Quantizing a model in 4-bit reduces your memory-usage by 4x, and for large models, set <code>device_map=&quot;auto&quot;</code> to efficiently use the GPUs available:",o,u,r,T,U="By default, all the other modules such as <code>torch.nn.LayerNorm</code> are converted to <code>torch.float16</code>. You can change the data type of these modules with the <code>torch_dtype</code> parameter if you want:",C,w,q,Z,j="If you have <code>bitsandbytes&gt;=0.41.3</code>, you can serialize 4-bit models and push them on Hugging Face Hub. Simply call <code>model.push_to_hub()</code> after loading it in 4-bit precision. You can also save the serialized 4-bit models locally with <code>model.save_pretrained()</code> command.",k;return u=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfNGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBsb2FkX2luXzRiaXQlM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_4bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),w=new v({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbF80Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGb3B0LTM1MG0lMjIlMkMlMjBsb2FkX2luXzRiaXQlM0RUcnVlJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDMyKSUwQW1vZGVsXzRiaXQubW9kZWwuZGVjb2Rlci5sYXllcnMlNUItMSU1RC5maW5hbF9sYXllcl9ub3JtLndlaWdodC5kdHlwZQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_4bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>, torch_dtype=torch.float32)
model_4bit.model.decoder.layers[-<span class="hljs-number">1</span>].final_layer_norm.weight.dtype`,wrap:!1}}),{c(){n=p("p"),n.innerHTML=y,o=s(),f(u.$$.fragment),r=s(),T=p("p"),T.innerHTML=U,C=s(),f(w.$$.fragment),q=s(),Z=p("p"),Z.innerHTML=j},l($){n=m($,"P",{"data-svelte-h":!0}),d(n)!=="svelte-1b6jp1p"&&(n.innerHTML=y),o=i($),h(u.$$.fragment,$),r=i($),T=m($,"P",{"data-svelte-h":!0}),d(T)!=="svelte-kfzkum"&&(T.innerHTML=U),C=i($),h(w.$$.fragment,$),q=i($),Z=m($,"P",{"data-svelte-h":!0}),d(Z)!=="svelte-1pmx2wb"&&(Z.innerHTML=j)},m($,G){a($,n,G),a($,o,G),c(u,$,G),a($,r,G),a($,T,G),a($,C,G),c(w,$,G),a($,q,G),a($,Z,G),k=!0},p:Q,i($){k||(g(u.$$.fragment,$),g(w.$$.fragment,$),k=!0)},o($){b(u.$$.fragment,$),b(w.$$.fragment,$),k=!1},d($){$&&(l(n),l(o),l(r),l(T),l(C),l(q),l(Z)),M(u,$),M(w,$)}}}function qs(_){let n,y,o,u;return n=new xe({props:{id:"bnb",option:"8-bit",$$slots:{default:[js]},$$scope:{ctx:_}}}),o=new xe({props:{id:"bnb",option:"4-bit",$$slots:{default:[Cs]},$$scope:{ctx:_}}}),{c(){f(n.$$.fragment),y=s(),f(o.$$.fragment)},l(r){h(n.$$.fragment,r),y=i(r),h(o.$$.fragment,r)},m(r,T){c(n,r,T),a(r,y,T),c(o,r,T),u=!0},p(r,T){const U={};T&2&&(U.$$scope={dirty:T,ctx:r}),n.$set(U);const C={};T&2&&(C.$$scope={dirty:T,ctx:r}),o.$set(C)},i(r){u||(g(n.$$.fragment,r),g(o.$$.fragment,r),u=!0)},o(r){b(n.$$.fragment,r),b(o.$$.fragment,r),u=!1},d(r){r&&l(y),M(n,r),M(o,r)}}}function ks(_){let n,y="Training with 8-bit and 4-bit weights are only supported for training <em>extra</em> parameters.";return{c(){n=p("p"),n.innerHTML=y},l(o){n=m(o,"P",{"data-svelte-h":!0}),d(n)!=="svelte-of9sym"&&(n.innerHTML=y)},m(o,u){a(o,n,u)},p:Q,d(o){o&&l(n)}}}function Zs(_){let n,y='Learn more about the details of 8-bit quantization in this <a href="https://huggingface.co/blog/hf-bitsandbytes-integration" rel="nofollow">blog post</a>!';return{c(){n=p("p"),n.innerHTML=y},l(o){n=m(o,"P",{"data-svelte-h":!0}),d(n)!=="svelte-1bb05fp"&&(n.innerHTML=y)},m(o,u){a(o,n,u)},p:Q,d(o){o&&l(n)}}}function Gs(_){let n,y='Try 4-bit quantization in this <a href="https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf" rel="nofollow">notebook</a> and learn more about it’s details in this <a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes" rel="nofollow">blog post</a>.';return{c(){n=p("p"),n.innerHTML=y},l(o){n=m(o,"P",{"data-svelte-h":!0}),d(n)!=="svelte-rme94c"&&(n.innerHTML=y)},m(o,u){a(o,n,u)},p:Q,d(o){o&&l(n)}}}function Is(_){let n,y,o,u,r,T,U,C="Quantization techniques focus on representing data with less information while also trying to not lose too much accuracy. This often means converting a data type to represent the same information with fewer bits. For example, if your model weights are stored as 32-bit floating points and they’re quantized to 16-bit floating points, this halves the model size which makes it easier to store and reduces memory-usage. Lower precision can also speedup inference because it takes less time to perform calculations with fewer bits.",w,q,Z="Transformers supports several quantization schemes to help you run inference with large language models (LLMs) and finetune adapters on quantized models. This guide will show you how to use Activation-aware Weight Quantization (AWQ), AutoGPTQ, and bitsandbytes.",j,k,$,G,J,z,Ea='Try AQLM on <a href="https://colab.research.google.com/drive/1-xZmBRXT5Fm3Ghn4Mwa2KRypORXb855X?usp=sharing" rel="nofollow">Google Colab</a>!',Re,O,Ya='Additive Quantization of Language Models (<a href="https://arxiv.org/abs/2401.06118" rel="nofollow">AQLM</a>) is a Large Language Models compression method. It quantizes multiple weights together and take advantage of interdependencies between them. AQLM represents groups of 8-16 weights as a sum of multiple vector codes.',Le,tt,Pa="Inference support for AQLM is realised in the <code>aqlm</code> library. Make sure to install it to run the models (note aqlm works only with python&gt;=3.10):",Ae,et,Ne,lt,Sa="The library provides efficient kernels for both GPU and CPU inference.",He,at,Da='The instructions on how to quantize models yourself, as well as all the relevant code can be found in the corresponding GitHub <a href="https://github.com/Vahe1994/AQLM" rel="nofollow">repository</a>.',Ee,nt,Ye,st,Ka="AQLM quantization setpus vary mainly on the number of codebooks used as well as codebook sizes in bits. The most popular setups, as well as inference kernels they support are:",Pe,it,Oa="<thead><tr><th>Kernel</th> <th>Number of codebooks</th> <th>Codebook size, bits</th> <th>Notation</th> <th>Accuracy</th> <th>Speedup</th> <th>Fast GPU inference</th> <th>Fast CPU inference</th></tr></thead> <tbody><tr><td>Triton</td> <td>K</td> <td>N</td> <td>KxN</td> <td>-</td> <td>Up to ~0.7x</td> <td>✅</td> <td>❌</td></tr> <tr><td>CUDA</td> <td>1</td> <td>16</td> <td>1x16</td> <td>Best</td> <td>Up to ~1.3x</td> <td>✅</td> <td>❌</td></tr> <tr><td>CUDA</td> <td>2</td> <td>8</td> <td>2x8</td> <td>OK</td> <td>Up to ~3.0x</td> <td>✅</td> <td>❌</td></tr> <tr><td>Numba</td> <td>K</td> <td>8</td> <td>Kx8</td> <td>Good</td> <td>Up to ~4.0x</td> <td>❌</td> <td>✅</td></tr></tbody>",Se,ot,De,W,Ke,rt,tn='<a href="https://hf.co/papers/2306.00978" rel="nofollow">Activation-aware Weight Quantization (AWQ)</a> doesn’t quantize all the weights in a model, and instead, it preserves a small percentage of weights that are important for LLM performance. This significantly reduces quantization loss such that you can run models in 4-bit precision without experiencing any performance degradation.',Oe,pt,en='There are several libraries for quantizing models with the AWQ algorithm, such as <a href="https://github.com/mit-han-lab/llm-awq" rel="nofollow">llm-awq</a>, <a href="https://github.com/casper-hansen/AutoAWQ" rel="nofollow">autoawq</a> or <a href="https://huggingface.co/docs/optimum/main/en/intel/optimization_inc" rel="nofollow">optimum-intel</a>. Transformers supports loading models quantized with the llm-awq and autoawq libraries. This guide will show you how to load models quantized with autoawq, but the process is similar for llm-awq quantized models.',tl,mt,ln="Make sure you have autoawq installed:",el,dt,ll,ut,an='AWQ-quantized models can be identified by checking the <code>quantization_config</code> attribute in the model’s <a href="https://huggingface.co/TheBloke/zephyr-7B-alpha-AWQ/blob/main/config.json" rel="nofollow">config.json</a> file:',al,ft,nl,ht,nn='A quantized model is loaded with the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method. If you loaded your model on the CPU, make sure to move it to a GPU device first. Use the <code>device_map</code> parameter to specify where to place the model:',sl,ct,il,gt,sn="Loading an AWQ-quantized model automatically sets other weights to fp16 by default for performance reasons. If you want to load these other weights in a different format, use the <code>torch_dtype</code> parameter:",ol,bt,rl,Mt,on='AWQ quantization can also be combined with <a href="perf_infer_gpu_one#flashattention-2">FlashAttention-2</a> to further accelerate inference:',pl,yt,ml,Tt,dl,wt,rn='Fused modules offers improved accuracy and performance and it is supported out-of-the-box for AWQ modules for <a href="https://huggingface.co/meta-llama" rel="nofollow">Llama</a> and <a href="https://huggingface.co/mistralai/Mistral-7B-v0.1" rel="nofollow">Mistral</a> architectures, but you can also fuse AWQ modules for unsupported architectures.',ul,V,fl,F,hl,$t,cl,x,gl,Jt,pn='The <a href="https://github.com/PanQiWei/AutoGPTQ" rel="nofollow">AutoGPTQ</a> library implements the GPTQ algorithm, a post-training quantization technique where each row of the weight matrix is quantized independently to find a version of the weights that minimizes the error. These weights are quantized to int4, but they’re restored to fp16 on the fly during inference. This can save your memory-usage by 4x because the int4 weights are dequantized in a fused kernel rather than a GPU’s global memory, and you can also expect a speedup in inference because using a lower bitwidth takes less time to communicate.',bl,_t,mn="Before you begin, make sure the following libraries are installed:",Ml,vt,yl,Ut,dn='To quantize a model (currently only supported for text models), you need to create a <a href="/docs/transformers/main/en/main_classes/quantization#transformers.GPTQConfig">GPTQConfig</a> class and set the number of bits to quantize to, a dataset to calibrate the weights for quantization, and a tokenizer to prepare the dataset.',Tl,jt,wl,Ct,un="You could also pass your own dataset as a list of strings, but it is highly recommended to use the same dataset from the GPTQ paper.",$l,qt,Jl,kt,fn='Load a model to quantize and pass the <code>gptq_config</code> to the <a href="/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> method. Set <code>device_map=&quot;auto&quot;</code> to automatically offload the model to a CPU to help fit the model in memory, and allow the model modules to be moved between the CPU and GPU for quantization.',_l,Zt,vl,Gt,hn="If you’re running out of memory because a dataset is too large, disk offloading is not supported. If this is the case, try passing the <code>max_memory</code> parameter to allocate the amount of memory to use on your device (GPU and CPU):",Ul,It,jl,X,Cl,zt,cn='Once your model is quantized, you can push the model and tokenizer to the Hub where it can be easily shared and accessed. Use the <a href="/docs/transformers/main/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method to save the <a href="/docs/transformers/main/en/main_classes/quantization#transformers.GPTQConfig">GPTQConfig</a>:',ql,Qt,kl,Bt,gn='You could also save your quantized model locally with the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method. If the model was quantized with the <code>device_map</code> parameter, make sure to move the entire model to a GPU or CPU before saving it. For example, to save the model on a CPU:',Zl,Wt,Gl,Vt,bn='Reload a quantized model with the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method, and set <code>device_map=&quot;auto&quot;</code> to automatically distribute the model on all available GPUs to load the model faster without using more memory than needed.',Il,Ft,zl,xt,Ql,Xt,Mn='<a href="https://github.com/turboderp/exllama" rel="nofollow">ExLlama</a> is a Python/C++/CUDA implementation of the <a href="model_doc/llama">Llama</a> model that is designed for faster inference with 4-bit GPTQ weights (check out these <a href="https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark" rel="nofollow">benchmarks</a>). The ExLlama kernel is activated by default when you create a <a href="/docs/transformers/main/en/main_classes/quantization#transformers.GPTQConfig">GPTQConfig</a> object. To boost inference speed even further, use the <a href="https://github.com/turboderp/exllamav2" rel="nofollow">ExLlamaV2</a> kernels by configuring the <code>exllama_config</code> parameter:',Bl,Rt,Wl,R,Vl,Lt,yn="The ExLlama kernels are only supported when the entire model is on the GPU. If you’re doing inference on a CPU with AutoGPTQ (version > 0.4.2), then you’ll need to disable the ExLlama kernel. This overwrites the attributes related to the ExLlama kernels in the quantization config of the config.json file.",Fl,At,xl,Nt,Xl,Ht,Tn='<a href="https://github.com/TimDettmers/bitsandbytes" rel="nofollow">bitsandbytes</a> is the easiest option for quantizing a model to 8 and 4-bit. 8-bit quantization multiplies outliers in fp16 with non-outliers in int8, converts the non-outlier values back to fp16, and then adds them together to return the weights in fp16. This reduces the degradative effect outlier values have on a model’s performance. 4-bit quantization compresses a model even further, and it is commonly used with <a href="https://hf.co/papers/2305.14314" rel="nofollow">QLoRA</a> to finetune quantized LLMs.',Rl,Et,wn="To use bitsandbytes, make sure you have the following libraries installed:",Ll,L,Al,Yt,$n='Now you can quantize a model with the <code>load_in_8bit</code> or <code>load_in_4bit</code> parameters in the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method. This works for any model in any modality, as long as it supports loading with Accelerate and contains <code>torch.nn.Linear</code> layers.',Nl,A,Hl,N,El,Pt,Jn="You can check your memory footprint with the <code>get_memory_footprint</code> method:",Yl,St,Pl,Dt,_n='Quantized models can be loaded from the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method without needing to specify the <code>load_in_8bit</code> or <code>load_in_4bit</code> parameters:',Sl,Kt,Dl,Ot,Kl,H,Ol,te,vn="This section explores some of the specific features of 8-bit models, such as offloading, outlier thresholds, skipping module conversion, and finetuning.",ta,ee,ea,le,Un='8-bit models can offload weights between the CPU and GPU to support fitting very large models into memory. The weights dispatched to the CPU are actually stored in <strong>float32</strong>, and aren’t converted to 8-bit. For example, to enable offloading for the <a href="https://huggingface.co/bigscience/bloom-1b7" rel="nofollow">bigscience/bloom-1b7</a> model, start by creating a <a href="/docs/transformers/main/en/main_classes/quantization#transformers.BitsAndBytesConfig">BitsAndBytesConfig</a>:',la,ae,aa,ne,jn="Design a custom device map to fit everything on your GPU except for the <code>lm_head</code>, which you’ll dispatch to the CPU:",na,se,sa,ie,Cn="Now load your model with the custom <code>device_map</code> and <code>quantization_config</code>:",ia,oe,oa,re,ra,pe,qn="An “outlier” is a hidden state value greater than a certain threshold, and these values are computed in fp16. While the values are usually normally distributed ([-3.5, 3.5]), this distribution can be very different for large models ([-60, 6] or [6, 60]). 8-bit quantization works well for values ~5, but beyond that, there is a significant performance penalty. A good default threshold value is 6, but a lower threshold may be needed for more unstable models (small models or finetuning).",pa,me,kn='To find the best threshold for your model, we recommend experimenting with the <code>llm_int8_threshold</code> parameter in <a href="/docs/transformers/main/en/main_classes/quantization#transformers.BitsAndBytesConfig">BitsAndBytesConfig</a>:',ma,de,da,ue,ua,fe,Zn='For some models, like <a href="model_doc/jukebox">Jukebox</a>, you don’t need to quantize every module to 8-bit which can actually cause instability. With Jukebox, there are several <code>lm_head</code> modules that should be skipped using the <code>llm_int8_skip_modules</code> parameter in <a href="/docs/transformers/main/en/main_classes/quantization#transformers.BitsAndBytesConfig">BitsAndBytesConfig</a>:',fa,he,ha,ce,ca,ge,Gn='With the <a href="https://github.com/huggingface/peft" rel="nofollow">PEFT</a> library, you can finetune large models like <a href="https://huggingface.co/google/flan-t5-large" rel="nofollow">flan-t5-large</a> and <a href="https://huggingface.co/facebook/opt-6.7b" rel="nofollow">facebook/opt-6.7b</a> with 8-bit quantization. You don’t need to pass the <code>device_map</code> parameter for training because it’ll automatically load your model on a GPU. However, you can still customize the device map with the <code>device_map</code> parameter if you want to (<code>device_map=&quot;auto&quot;</code> should only be used for inference).',ga,be,ba,E,Ma,Me,In="This section explores some of the specific features of 4-bit models, such as changing the compute data type, using the Normal Float 4 (NF4) data type, and using nested quantization.",ya,ye,Ta,Te,zn='To speedup computation, you can change the data type from float32 (the default value) to bf16 using the <code>bnb_4bit_compute_dtype</code> parameter in <a href="/docs/transformers/main/en/main_classes/quantization#transformers.BitsAndBytesConfig">BitsAndBytesConfig</a>:',wa,we,$a,$e,Ja,Je,Qn='NF4 is a 4-bit data type from the <a href="https://hf.co/papers/2305.14314" rel="nofollow">QLoRA</a> paper, adapted for weights initialized from a normal distribution. You should use NF4 for training 4-bit base models. This can be configured with the <code>bnb_4bit_quant_type</code> parameter in the <a href="/docs/transformers/main/en/main_classes/quantization#transformers.BitsAndBytesConfig">BitsAndBytesConfig</a>:',_a,_e,va,ve,Bn="For inference, the <code>bnb_4bit_quant_type</code> does not have a huge impact on performance. However, to remain consistent with the model weights, you should use the <code>bnb_4bit_compute_dtype</code> and <code>torch_dtype</code> values.",Ua,Ue,ja,je,Wn='Nested quantization is a technique that can save additional memory at no additional performance cost. This feature performs a second quantization of the already quantized weights to save an addition 0.4 bits/parameter. For example, with nested quantization, you can finetune a <a href="https://huggingface.co/meta-llama/Llama-2-13b" rel="nofollow">Llama-13b</a> model on a 16GB NVIDIA T4 GPU with a sequence length of 1024, a batch size of 1, and enabling gradient accumulation with 4 steps.',Ca,Ce,qa,qe,ka,ke,Vn='The <a href="https://huggingface.co/docs/optimum/index" rel="nofollow">Optimum</a> library supports quantization for Intel, Furiosa, ONNX Runtime, GPTQ, and lower-level PyTorch quantization functions. Consider using Optimum for quantization if you’re using specific and optimized hardware like Intel CPUs, Furiosa NPUs or a model accelerator like ONNX Runtime.',Za,Ze,Ga,Ge,Fn='To compare the speed, throughput, and latency of each quantization scheme, check the following benchmarks obtained from the <a href="https://github.com/huggingface/optimum-benchmark" rel="nofollow">optimum-benchmark</a> library. The benchmark was run on a NVIDIA A1000 for the <a href="https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ" rel="nofollow">TheBloke/Mistral-7B-v0.1-AWQ</a> and <a href="https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ" rel="nofollow">TheBloke/Mistral-7B-v0.1-GPTQ</a> models. These were also tested against the bitsandbytes quantization methods as well as a native fp16 model.',Ia,Y,xn='<div><img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_memory_plot.png" alt="forward peak memory per batch size"/> <figcaption class="mt-2 text-center text-sm text-gray-500">forward peak memory/batch size</figcaption></div> <div><img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_memory_plot.png" alt="generate peak memory per batch size"/> <figcaption class="mt-2 text-center text-sm text-gray-500">generate peak memory/batch size</figcaption></div>',za,P,Xn='<div><img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_throughput_plot.png" alt="generate throughput per batch size"/> <figcaption class="mt-2 text-center text-sm text-gray-500">generate throughput/batch size</figcaption></div> <div><img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_latency_plot.png" alt="forward latency per batch size"/> <figcaption class="mt-2 text-center text-sm text-gray-500">forward latency/batch size</figcaption></div>',Qa,Ie,Rn='The benchmarks indicate AWQ quantization is the fastest for inference, text generation, and has the lowest peak memory for text generation. However, AWQ has the largest forward latency per batch size. For a more detailed discussion about the pros and cons of each quantization method, read the <a href="https://huggingface.co/blog/overview-quantization-transformers" rel="nofollow">Overview of natively supported quantization schemes in 🤗 Transformers</a> blog post.',Ba,ze,Wa,Qe,Ln='The <a href="https://huggingface.co/TheBloke/Mistral-7B-OpenOrca-AWQ" rel="nofollow">TheBloke/Mistral-7B-OpenOrca-AWQ</a> model was benchmarked with <code>batch_size=1</code> with and without fused modules.',Va,S,An="Unfused module",Fa,Be,Nn='<thead><tr><th align="right">Batch Size</th> <th align="right">Prefill Length</th> <th align="right">Decode Length</th> <th align="right">Prefill tokens/s</th> <th align="right">Decode tokens/s</th> <th align="left">Memory (VRAM)</th></tr></thead> <tbody><tr><td align="right">1</td> <td align="right">32</td> <td align="right">32</td> <td align="right">60.0984</td> <td align="right">38.4537</td> <td align="left">4.50 GB (5.68%)</td></tr> <tr><td align="right">1</td> <td align="right">64</td> <td align="right">64</td> <td align="right">1333.67</td> <td align="right">31.6604</td> <td align="left">4.50 GB (5.68%)</td></tr> <tr><td align="right">1</td> <td align="right">128</td> <td align="right">128</td> <td align="right">2434.06</td> <td align="right">31.6272</td> <td align="left">4.50 GB (5.68%)</td></tr> <tr><td align="right">1</td> <td align="right">256</td> <td align="right">256</td> <td align="right">3072.26</td> <td align="right">38.1731</td> <td align="left">4.50 GB (5.68%)</td></tr> <tr><td align="right">1</td> <td align="right">512</td> <td align="right">512</td> <td align="right">3184.74</td> <td align="right">31.6819</td> <td align="left">4.59 GB (5.80%)</td></tr> <tr><td align="right">1</td> <td align="right">1024</td> <td align="right">1024</td> <td align="right">3148.18</td> <td align="right">36.8031</td> <td align="left">4.81 GB (6.07%)</td></tr> <tr><td align="right">1</td> <td align="right">2048</td> <td align="right">2048</td> <td align="right">2927.33</td> <td align="right">35.2676</td> <td align="left">5.73 GB (7.23%)</td></tr></tbody>',xa,D,Hn="Fused module",Xa,We,En='<thead><tr><th align="right">Batch Size</th> <th align="right">Prefill Length</th> <th align="right">Decode Length</th> <th align="right">Prefill tokens/s</th> <th align="right">Decode tokens/s</th> <th align="left">Memory (VRAM)</th></tr></thead> <tbody><tr><td align="right">1</td> <td align="right">32</td> <td align="right">32</td> <td align="right">81.4899</td> <td align="right">80.2569</td> <td align="left">4.00 GB (5.05%)</td></tr> <tr><td align="right">1</td> <td align="right">64</td> <td align="right">64</td> <td align="right">1756.1</td> <td align="right">106.26</td> <td align="left">4.00 GB (5.05%)</td></tr> <tr><td align="right">1</td> <td align="right">128</td> <td align="right">128</td> <td align="right">2479.32</td> <td align="right">105.631</td> <td align="left">4.00 GB (5.06%)</td></tr> <tr><td align="right">1</td> <td align="right">256</td> <td align="right">256</td> <td align="right">1813.6</td> <td align="right">85.7485</td> <td align="left">4.01 GB (5.06%)</td></tr> <tr><td align="right">1</td> <td align="right">512</td> <td align="right">512</td> <td align="right">2848.9</td> <td align="right">97.701</td> <td align="left">4.11 GB (5.19%)</td></tr> <tr><td align="right">1</td> <td align="right">1024</td> <td align="right">1024</td> <td align="right">3044.35</td> <td align="right">87.7323</td> <td align="left">4.41 GB (5.57%)</td></tr> <tr><td align="right">1</td> <td align="right">2048</td> <td align="right">2048</td> <td align="right">2715.11</td> <td align="right">89.4709</td> <td align="left">5.57 GB (7.04%)</td></tr></tbody>',Ra,Ve,Yn='The speed and throughput of fused and unfused modules were also tested with the <a href="https://github.com/huggingface/optimum-benchmark" rel="nofollow">optimum-benchmark</a> library.',La,K,Pn='<div><img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/fused_forward_memory_plot.png" alt="generate throughput per batch size"/> <figcaption class="mt-2 text-center text-sm text-gray-500">forward peak memory/batch size</figcaption></div> <div><img class="rounded-xl" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/fused_generate_throughput_plot.png" alt="forward latency per batch size"/> <figcaption class="mt-2 text-center text-sm text-gray-500">generate throughput/batch size</figcaption></div>',Aa,Xe,Na;return r=new I({props:{title:"Quantization",local:"quantization",headingTag:"h1"}}),k=new B({props:{$$slots:{default:[cs]},$$scope:{ctx:_}}}),G=new I({props:{title:"AQLM",local:"aqlm",headingTag:"h2"}}),et=new v({props:{code:"cGlwJTIwaW5zdGFsbCUyMGFxbG0lNUJncHUlMkNjcHUlNUQ=",highlighted:"pip install aqlm[gpu,cpu]",wrap:!1}}),nt=new I({props:{title:"AQLM configurations",local:"aqlm-configurations",headingTag:"h3"}}),ot=new I({props:{title:"AWQ",local:"awq",headingTag:"h2"}}),W=new B({props:{$$slots:{default:[gs]},$$scope:{ctx:_}}}),dt=new v({props:{code:"cGlwJTIwaW5zdGFsbCUyMGF1dG9hd3E=",highlighted:"pip install autoawq",wrap:!1}}),ft=new v({props:{code:"JTdCJTBBJTIwJTIwJTIyX25hbWVfb3JfcGF0aCUyMiUzQSUyMCUyMiUyRndvcmtzcGFjZSUyRnByb2Nlc3MlMkZodWdnaW5nZmFjZWg0X3plcGh5ci03Yi1hbHBoYSUyRnNvdXJjZSUyMiUyQyUwQSUyMCUyMCUyMmFyY2hpdGVjdHVyZXMlMjIlM0ElMjAlNUIlMEElMjAlMjAlMjAlMjAlMjJNaXN0cmFsRm9yQ2F1c2FsTE0lMjIlMEElMjAlMjAlNUQlMkMlMEElMjAlMjAuLi4lMEElMjAlMjAuLi4lMEElMjAlMjAuLi4lMEElMjAlMjAlMjJxdWFudGl6YXRpb25fY29uZmlnJTIyJTNBJTIwJTdCJTBBJTIwJTIwJTIwJTIwJTIycXVhbnRfbWV0aG9kJTIyJTNBJTIwJTIyYXdxJTIyJTJDJTBBJTIwJTIwJTIwJTIwJTIyemVyb19wb2ludCUyMiUzQSUyMHRydWUlMkMlMEElMjAlMjAlMjAlMjAlMjJncm91cF9zaXplJTIyJTNBJTIwMTI4JTJDJTBBJTIwJTIwJTIwJTIwJTIyYml0cyUyMiUzQSUyMDQlMkMlMEElMjAlMjAlMjAlMjAlMjJ2ZXJzaW9uJTIyJTNBJTIwJTIyZ2VtbSUyMiUwQSUyMCUyMCU3RCUwQSU3RA==",highlighted:`<span class="hljs-punctuation">{</span>
  <span class="hljs-attr">&quot;_name_or_path&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;/workspace/process/huggingfaceh4_zephyr-7b-alpha/source&quot;</span><span class="hljs-punctuation">,</span>
  <span class="hljs-attr">&quot;architectures&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span>
    <span class="hljs-string">&quot;MistralForCausalLM&quot;</span>
  <span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span>
  ...
  ...
  ...
  <span class="hljs-attr">&quot;quantization_config&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">&quot;quant_method&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;awq&quot;</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;zero_point&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-literal"><span class="hljs-keyword">true</span></span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;group_size&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">128</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;bits&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-number">4</span><span class="hljs-punctuation">,</span>
    <span class="hljs-attr">&quot;version&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;gemm&quot;</span>
  <span class="hljs-punctuation">}</span>
<span class="hljs-punctuation">}</span>`,wrap:!1}}),ct=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyVGhlQmxva2UlMkZ6ZXBoeXItN0ItYWxwaGEtQVdRJTIyJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyY3VkYSUzQTAlMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span>
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;cuda:0&quot;</span>)`,wrap:!1}}),bt=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyVGhlQmxva2UlMkZ6ZXBoeXItN0ItYWxwaGEtQVdRJTIyJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MzIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span>
model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32)`,wrap:!1}}),yt=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMlRoZUJsb2tlJTJGemVwaHlyLTdCLWFscGhhLUFXUSUyMiUyQyUyMGF0dG5faW1wbGVtZW50YXRpb24lM0QlMjJmbGFzaF9hdHRlbnRpb25fMiUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJjdWRhJTNBMCUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span>, attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>, device_map=<span class="hljs-string">&quot;cuda:0&quot;</span>)`,wrap:!1}}),Tt=new I({props:{title:"Fused modules",local:"fused-modules",headingTag:"h3"}}),V=new B({props:{warning:!0,$$slots:{default:[bs]},$$scope:{ctx:_}}}),F=new Ha({props:{id:"fuse",options:["supported architectures","unsupported architectures"],$$slots:{default:[Ts]},$$scope:{ctx:_}}}),$t=new I({props:{title:"AutoGPTQ",local:"autogptq",headingTag:"h2"}}),x=new B({props:{$$slots:{default:[ws]},$$scope:{ctx:_}}}),vt=new v({props:{code:"cGlwJTIwaW5zdGFsbCUyMGF1dG8tZ3B0cSUwQXBpcCUyMGluc3RhbGwlMjBnaXQlMkJodHRwcyUzQSUyRiUyRmdpdGh1Yi5jb20lMkZodWdnaW5nZmFjZSUyRm9wdGltdW0uZ2l0JTBBcGlwJTIwaW5zdGFsbCUyMGdpdCUyQmh0dHBzJTNBJTJGJTJGZ2l0aHViLmNvbSUyRmh1Z2dpbmdmYWNlJTJGdHJhbnNmb3JtZXJzLmdpdCUwQXBpcCUyMGluc3RhbGwlMjAtLXVwZ3JhZGUlMjBhY2NlbGVyYXRl",highlighted:`pip install auto-gptq
pip install git+https://github.com/huggingface/optimum.git
pip install git+https://github.com/huggingface/transformers.git
pip install --upgrade accelerate`,wrap:!1}}),jt=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEdQVFFDb25maWclMEElMEFtb2RlbF9pZCUyMCUzRCUyMCUyMmZhY2Vib29rJTJGb3B0LTEyNW0lMjIlMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCklMEFncHRxX2NvbmZpZyUyMCUzRCUyMEdQVFFDb25maWcoYml0cyUzRDQlMkMlMjBkYXRhc2V0JTNEJTIyYzQlMjIlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, GPTQConfig

model_id = <span class="hljs-string">&quot;facebook/opt-125m&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>, dataset=<span class="hljs-string">&quot;c4&quot;</span>, tokenizer=tokenizer)`,wrap:!1}}),qt=new v({props:{code:"ZGF0YXNldCUyMCUzRCUyMCU1QiUyMmF1dG8tZ3B0cSUyMGlzJTIwYW4lMjBlYXN5LXRvLXVzZSUyMG1vZGVsJTIwcXVhbnRpemF0aW9uJTIwbGlicmFyeSUyMHdpdGglMjB1c2VyLWZyaWVuZGx5JTIwYXBpcyUyQyUyMGJhc2VkJTIwb24lMjBHUFRRJTIwYWxnb3JpdGhtLiUyMiU1RCUwQWdwdHFfY29uZmlnJTIwJTNEJTIwR1BUUUNvbmZpZyhiaXRzJTNENCUyQyUyMGRhdGFzZXQlM0RkYXRhc2V0JTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKQ==",highlighted:`dataset = [<span class="hljs-string">&quot;auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.&quot;</span>]
gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>, dataset=dataset, tokenizer=tokenizer)`,wrap:!1}}),Zt=new v({props:{code:"cXVhbnRpemVkX21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEZ3B0cV9jb25maWcp",highlighted:'quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config=gptq_config)',wrap:!1}}),It=new v({props:{code:"cXVhbnRpemVkX21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBtYXhfbWVtb3J5JTNEJTdCMCUzQSUyMCUyMjMwR2lCJTIyJTJDJTIwMSUzQSUyMCUyMjQ2R2lCJTIyJTJDJTIwJTIyY3B1JTIyJTNBJTIwJTIyMzBHaUIlMjIlN0QlMkMlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEZ3B0cV9jb25maWcp",highlighted:'quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, max_memory={<span class="hljs-number">0</span>: <span class="hljs-string">&quot;30GiB&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;46GiB&quot;</span>, <span class="hljs-string">&quot;cpu&quot;</span>: <span class="hljs-string">&quot;30GiB&quot;</span>}, quantization_config=gptq_config)',wrap:!1}}),X=new B({props:{warning:!0,$$slots:{default:[$s]},$$scope:{ctx:_}}}),Qt=new v({props:{code:"cXVhbnRpemVkX21vZGVsLnB1c2hfdG9faHViKCUyMm9wdC0xMjVtLWdwdHElMjIpJTBBdG9rZW5pemVyLnB1c2hfdG9faHViKCUyMm9wdC0xMjVtLWdwdHElMjIp",highlighted:`quantized_model.push_to_hub(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)
tokenizer.push_to_hub(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),Wt=new v({props:{code:"cXVhbnRpemVkX21vZGVsLnNhdmVfcHJldHJhaW5lZCglMjJvcHQtMTI1bS1ncHRxJTIyKSUwQXRva2VuaXplci5zYXZlX3ByZXRyYWluZWQoJTIyb3B0LTEyNW0tZ3B0cSUyMiklMEElMEElMjMlMjBpZiUyMHF1YW50aXplZCUyMHdpdGglMjBkZXZpY2VfbWFwJTIwc2V0JTBBcXVhbnRpemVkX21vZGVsLnRvKCUyMmNwdSUyMiklMEFxdWFudGl6ZWRfbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKCUyMm9wdC0xMjVtLWdwdHElMjIp",highlighted:`quantized_model.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)
tokenizer.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)

<span class="hljs-comment"># if quantized with device_map set</span>
quantized_model.to(<span class="hljs-string">&quot;cpu&quot;</span>)
quantized_model.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),Ft=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyJTdCeW91cl91c2VybmFtZSU3RCUyRm9wdC0xMjVtLWdwdHElMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),xt=new I({props:{title:"ExLlama",local:"exllama",headingTag:"h3"}}),Rt=new v({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBHUFRRQ29uZmlnJTBBJTBBZ3B0cV9jb25maWclMjAlM0QlMjBHUFRRQ29uZmlnKGJpdHMlM0Q0JTJDJTIwZXhsbGFtYV9jb25maWclM0QlN0IlMjJ2ZXJzaW9uJTIyJTNBMiU3RCklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjIlN0J5b3VyX3VzZXJuYW1lJTdEJTJGb3B0LTEyNW0tZ3B0cSUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRGdwdHFfY29uZmlnKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, GPTQConfig

gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>, exllama_config={<span class="hljs-string">&quot;version&quot;</span>:<span class="hljs-number">2</span>})
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config=gptq_config)`,wrap:!1}}),R=new B({props:{warning:!0,$$slots:{default:[Js]},$$scope:{ctx:_}}}),At=new v({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBHUFRRQ29uZmlnJTBBZ3B0cV9jb25maWclMjAlM0QlMjBHUFRRQ29uZmlnKGJpdHMlM0Q0JTJDJTIwdXNlX2V4bGxhbWElM0RGYWxzZSklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjIlN0J5b3VyX3VzZXJuYW1lJTdEJTJGb3B0LTEyNW0tZ3B0cSUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJjcHUlMjIlMkMlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEZ3B0cV9jb25maWcp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, GPTQConfig
gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>, use_exllama=<span class="hljs-literal">False</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>, device_map=<span class="hljs-string">&quot;cpu&quot;</span>, quantization_config=gptq_config)`,wrap:!1}}),Nt=new I({props:{title:"bitsandbytes",local:"bitsandbytes",headingTag:"h2"}}),L=new Ha({props:{id:"bnb",options:["8-bit","4-bit"],$$slots:{default:[Us]},$$scope:{ctx:_}}}),A=new Ha({props:{id:"bnb",options:["8-bit","4-bit"],$$slots:{default:[qs]},$$scope:{ctx:_}}}),N=new B({props:{warning:!0,$$slots:{default:[ks]},$$scope:{ctx:_}}}),St=new v({props:{code:"cHJpbnQobW9kZWwuZ2V0X21lbW9yeV9mb290cHJpbnQoKSk=",highlighted:'<span class="hljs-built_in">print</span>(model.get_memory_footprint())',wrap:!1}}),Kt=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMiU3QnlvdXJfdXNlcm5hbWUlN0QlMkZibG9vbS01NjBtLThiaXQlMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/bloom-560m-8bit&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),Ot=new I({props:{title:"8-bit",local:"8-bit",headingTag:"h3"}}),H=new B({props:{$$slots:{default:[Zs]},$$scope:{ctx:_}}}),ee=new I({props:{title:"Offloading",local:"offloading",headingTag:"h4"}}),ae=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyhsbG1faW50OF9lbmFibGVfZnAzMl9jcHVfb2ZmbG9hZCUzRFRydWUp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=<span class="hljs-literal">True</span>)`,wrap:!1}}),se=new v({props:{code:"ZGV2aWNlX21hcCUyMCUzRCUyMCU3QiUwQSUyMCUyMCUyMCUyMCUyMnRyYW5zZm9ybWVyLndvcmRfZW1iZWRkaW5ncyUyMiUzQSUyMDAlMkMlMEElMjAlMjAlMjAlMjAlMjJ0cmFuc2Zvcm1lci53b3JkX2VtYmVkZGluZ3NfbGF5ZXJub3JtJTIyJTNBJTIwMCUyQyUwQSUyMCUyMCUyMCUyMCUyMmxtX2hlYWQlMjIlM0ElMjAlMjJjcHUlMjIlMkMlMEElMjAlMjAlMjAlMjAlMjJ0cmFuc2Zvcm1lci5oJTIyJTNBJTIwMCUyQyUwQSUyMCUyMCUyMCUyMCUyMnRyYW5zZm9ybWVyLmxuX2YlMjIlM0ElMjAwJTJDJTBBJTdE",highlighted:`device_map = {
    <span class="hljs-string">&quot;transformer.word_embeddings&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;transformer.word_embeddings_layernorm&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;lm_head&quot;</span>: <span class="hljs-string">&quot;cpu&quot;</span>,
    <span class="hljs-string">&quot;transformer.h&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;transformer.ln_f&quot;</span>: <span class="hljs-number">0</span>,
}`,wrap:!1}}),oe=new v({props:{code:"bW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRGRldmljZV9tYXAlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUyQyUwQSk=",highlighted:`model_8bit = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>,
    device_map=device_map,
    quantization_config=quantization_config,
)`,wrap:!1}}),re=new I({props:{title:"Outlier threshold",local:"outlier-threshold",headingTag:"h4"}}),de=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyglMEElMjAlMjAlMjAlMjBsbG1faW50OF90aHJlc2hvbGQlM0QxMCUyQyUwQSklMEElMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMG1vZGVsX2lkJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRGRldmljZV9tYXAlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUyQyUwQSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, BitsAndBytesConfig

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=<span class="hljs-number">10</span>,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)`,wrap:!1}}),ue=new I({props:{title:"Skip module conversion",local:"skip-module-conversion",headingTag:"h4"}}),he=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyYmlnc2NpZW5jZSUyRmJsb29tLTFiNyUyMiUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbGxtX2ludDhfc2tpcF9tb2R1bGVzJTNEJTVCJTIybG1faGVhZCUyMiU1RCUyQyUwQSklMEElMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMG1vZGVsX2lkJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUyQyUwQSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=[<span class="hljs-string">&quot;lm_head&quot;</span>],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=<span class="hljs-string">&quot;auto&quot;</span>,
    quantization_config=quantization_config,
)`,wrap:!1}}),ce=new I({props:{title:"Finetuning",local:"finetuning",headingTag:"h4"}}),be=new I({props:{title:"4-bit",local:"4-bit",headingTag:"h3"}}),E=new B({props:{$$slots:{default:[Gs]},$$scope:{ctx:_}}}),ye=new I({props:{title:"Compute data type",local:"compute-data-type",headingTag:"h4"}}),we=new v({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyhsb2FkX2luXzRiaXQlM0RUcnVlJTJDJTIwYm5iXzRiaXRfY29tcHV0ZV9kdHlwZSUzRHRvcmNoLmJmbG9hdDE2KQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=<span class="hljs-literal">True</span>, bnb_4bit_compute_dtype=torch.bfloat16)`,wrap:!1}}),$e=new I({props:{title:"Normal Float 4 (NF4)",local:"normal-float-4-nf4",headingTag:"h4"}}),_e=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW5mNF9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbG9hZF9pbl80Yml0JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGJuYl80Yml0X3F1YW50X3R5cGUlM0QlMjJuZjQlMjIlMkMlMEEpJTBBJTBBbW9kZWxfbmY0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRG5mNF9jb25maWcp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_quant_type=<span class="hljs-string">&quot;nf4&quot;</span>,
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)`,wrap:!1}}),Ue=new I({props:{title:"Nested quantization",local:"nested-quantization",headingTag:"h4"}}),Ce=new v({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQWRvdWJsZV9xdWFudF9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbG9hZF9pbl80Yml0JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGJuYl80Yml0X3VzZV9kb3VibGVfcXVhbnQlM0RUcnVlJTJDJTBBKSUwQSUwQW1vZGVsX2RvdWJsZV9xdWFudCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJtZXRhLWxsYW1hJTJGTGxhbWEtMi0xM2IlMjIlMkMlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEZG91YmxlX3F1YW50X2NvbmZpZyk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_use_double_quant=<span class="hljs-literal">True</span>,
)

model_double_quant = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;meta-llama/Llama-2-13b&quot;</span>, quantization_config=double_quant_config)`,wrap:!1}}),qe=new I({props:{title:"Optimum",local:"optimum",headingTag:"h2"}}),Ze=new I({props:{title:"Benchmarks",local:"benchmarks",headingTag:"h2"}}),ze=new I({props:{title:"Fused AWQ modules",local:"fused-awq-modules",headingTag:"h3"}}),{c(){n=p("meta"),y=s(),o=p("p"),u=s(),f(r.$$.fragment),T=s(),U=p("p"),U.textContent=C,w=s(),q=p("p"),q.textContent=Z,j=s(),f(k.$$.fragment),$=s(),f(G.$$.fragment),J=s(),z=p("p"),z.innerHTML=Ea,Re=s(),O=p("p"),O.innerHTML=Ya,Le=s(),tt=p("p"),tt.innerHTML=Pa,Ae=s(),f(et.$$.fragment),Ne=s(),lt=p("p"),lt.textContent=Sa,He=s(),at=p("p"),at.innerHTML=Da,Ee=s(),f(nt.$$.fragment),Ye=s(),st=p("p"),st.textContent=Ka,Pe=s(),it=p("table"),it.innerHTML=Oa,Se=s(),f(ot.$$.fragment),De=s(),f(W.$$.fragment),Ke=s(),rt=p("p"),rt.innerHTML=tn,Oe=s(),pt=p("p"),pt.innerHTML=en,tl=s(),mt=p("p"),mt.textContent=ln,el=s(),f(dt.$$.fragment),ll=s(),ut=p("p"),ut.innerHTML=an,al=s(),f(ft.$$.fragment),nl=s(),ht=p("p"),ht.innerHTML=nn,sl=s(),f(ct.$$.fragment),il=s(),gt=p("p"),gt.innerHTML=sn,ol=s(),f(bt.$$.fragment),rl=s(),Mt=p("p"),Mt.innerHTML=on,pl=s(),f(yt.$$.fragment),ml=s(),f(Tt.$$.fragment),dl=s(),wt=p("p"),wt.innerHTML=rn,ul=s(),f(V.$$.fragment),fl=s(),f(F.$$.fragment),hl=s(),f($t.$$.fragment),cl=s(),f(x.$$.fragment),gl=s(),Jt=p("p"),Jt.innerHTML=pn,bl=s(),_t=p("p"),_t.textContent=mn,Ml=s(),f(vt.$$.fragment),yl=s(),Ut=p("p"),Ut.innerHTML=dn,Tl=s(),f(jt.$$.fragment),wl=s(),Ct=p("p"),Ct.textContent=un,$l=s(),f(qt.$$.fragment),Jl=s(),kt=p("p"),kt.innerHTML=fn,_l=s(),f(Zt.$$.fragment),vl=s(),Gt=p("p"),Gt.innerHTML=hn,Ul=s(),f(It.$$.fragment),jl=s(),f(X.$$.fragment),Cl=s(),zt=p("p"),zt.innerHTML=cn,ql=s(),f(Qt.$$.fragment),kl=s(),Bt=p("p"),Bt.innerHTML=gn,Zl=s(),f(Wt.$$.fragment),Gl=s(),Vt=p("p"),Vt.innerHTML=bn,Il=s(),f(Ft.$$.fragment),zl=s(),f(xt.$$.fragment),Ql=s(),Xt=p("p"),Xt.innerHTML=Mn,Bl=s(),f(Rt.$$.fragment),Wl=s(),f(R.$$.fragment),Vl=s(),Lt=p("p"),Lt.textContent=yn,Fl=s(),f(At.$$.fragment),xl=s(),f(Nt.$$.fragment),Xl=s(),Ht=p("p"),Ht.innerHTML=Tn,Rl=s(),Et=p("p"),Et.textContent=wn,Ll=s(),f(L.$$.fragment),Al=s(),Yt=p("p"),Yt.innerHTML=$n,Nl=s(),f(A.$$.fragment),Hl=s(),f(N.$$.fragment),El=s(),Pt=p("p"),Pt.innerHTML=Jn,Yl=s(),f(St.$$.fragment),Pl=s(),Dt=p("p"),Dt.innerHTML=_n,Sl=s(),f(Kt.$$.fragment),Dl=s(),f(Ot.$$.fragment),Kl=s(),f(H.$$.fragment),Ol=s(),te=p("p"),te.textContent=vn,ta=s(),f(ee.$$.fragment),ea=s(),le=p("p"),le.innerHTML=Un,la=s(),f(ae.$$.fragment),aa=s(),ne=p("p"),ne.innerHTML=jn,na=s(),f(se.$$.fragment),sa=s(),ie=p("p"),ie.innerHTML=Cn,ia=s(),f(oe.$$.fragment),oa=s(),f(re.$$.fragment),ra=s(),pe=p("p"),pe.textContent=qn,pa=s(),me=p("p"),me.innerHTML=kn,ma=s(),f(de.$$.fragment),da=s(),f(ue.$$.fragment),ua=s(),fe=p("p"),fe.innerHTML=Zn,fa=s(),f(he.$$.fragment),ha=s(),f(ce.$$.fragment),ca=s(),ge=p("p"),ge.innerHTML=Gn,ga=s(),f(be.$$.fragment),ba=s(),f(E.$$.fragment),Ma=s(),Me=p("p"),Me.textContent=In,ya=s(),f(ye.$$.fragment),Ta=s(),Te=p("p"),Te.innerHTML=zn,wa=s(),f(we.$$.fragment),$a=s(),f($e.$$.fragment),Ja=s(),Je=p("p"),Je.innerHTML=Qn,_a=s(),f(_e.$$.fragment),va=s(),ve=p("p"),ve.innerHTML=Bn,Ua=s(),f(Ue.$$.fragment),ja=s(),je=p("p"),je.innerHTML=Wn,Ca=s(),f(Ce.$$.fragment),qa=s(),f(qe.$$.fragment),ka=s(),ke=p("p"),ke.innerHTML=Vn,Za=s(),f(Ze.$$.fragment),Ga=s(),Ge=p("p"),Ge.innerHTML=Fn,Ia=s(),Y=p("div"),Y.innerHTML=xn,za=s(),P=p("div"),P.innerHTML=Xn,Qa=s(),Ie=p("p"),Ie.innerHTML=Rn,Ba=s(),f(ze.$$.fragment),Wa=s(),Qe=p("p"),Qe.innerHTML=Ln,Va=s(),S=p("figcaption"),S.textContent=An,Fa=s(),Be=p("table"),Be.innerHTML=Nn,xa=s(),D=p("figcaption"),D.textContent=Hn,Xa=s(),We=p("table"),We.innerHTML=En,Ra=s(),Ve=p("p"),Ve.innerHTML=Yn,La=s(),K=p("div"),K.innerHTML=Pn,Aa=s(),Xe=p("p"),this.h()},l(t){const e=fs("svelte-u9bgzb",document.head);n=m(e,"META",{name:!0,content:!0}),e.forEach(l),y=i(t),o=m(t,"P",{}),rs(o).forEach(l),u=i(t),h(r.$$.fragment,t),T=i(t),U=m(t,"P",{"data-svelte-h":!0}),d(U)!=="svelte-1euzkei"&&(U.textContent=C),w=i(t),q=m(t,"P",{"data-svelte-h":!0}),d(q)!=="svelte-1f5pv07"&&(q.textContent=Z),j=i(t),h(k.$$.fragment,t),$=i(t),h(G.$$.fragment,t),J=i(t),z=m(t,"P",{"data-svelte-h":!0}),d(z)!=="svelte-vu1jh7"&&(z.innerHTML=Ea),Re=i(t),O=m(t,"P",{"data-svelte-h":!0}),d(O)!=="svelte-11fbd25"&&(O.innerHTML=Ya),Le=i(t),tt=m(t,"P",{"data-svelte-h":!0}),d(tt)!=="svelte-1rdkef8"&&(tt.innerHTML=Pa),Ae=i(t),h(et.$$.fragment,t),Ne=i(t),lt=m(t,"P",{"data-svelte-h":!0}),d(lt)!=="svelte-1pfndoz"&&(lt.textContent=Sa),He=i(t),at=m(t,"P",{"data-svelte-h":!0}),d(at)!=="svelte-mrigan"&&(at.innerHTML=Da),Ee=i(t),h(nt.$$.fragment,t),Ye=i(t),st=m(t,"P",{"data-svelte-h":!0}),d(st)!=="svelte-14j9yls"&&(st.textContent=Ka),Pe=i(t),it=m(t,"TABLE",{"data-svelte-h":!0}),d(it)!=="svelte-mgp7ro"&&(it.innerHTML=Oa),Se=i(t),h(ot.$$.fragment,t),De=i(t),h(W.$$.fragment,t),Ke=i(t),rt=m(t,"P",{"data-svelte-h":!0}),d(rt)!=="svelte-1c5vyni"&&(rt.innerHTML=tn),Oe=i(t),pt=m(t,"P",{"data-svelte-h":!0}),d(pt)!=="svelte-vomabp"&&(pt.innerHTML=en),tl=i(t),mt=m(t,"P",{"data-svelte-h":!0}),d(mt)!=="svelte-1ozln1k"&&(mt.textContent=ln),el=i(t),h(dt.$$.fragment,t),ll=i(t),ut=m(t,"P",{"data-svelte-h":!0}),d(ut)!=="svelte-1lxz9t"&&(ut.innerHTML=an),al=i(t),h(ft.$$.fragment,t),nl=i(t),ht=m(t,"P",{"data-svelte-h":!0}),d(ht)!=="svelte-108eg93"&&(ht.innerHTML=nn),sl=i(t),h(ct.$$.fragment,t),il=i(t),gt=m(t,"P",{"data-svelte-h":!0}),d(gt)!=="svelte-19un5li"&&(gt.innerHTML=sn),ol=i(t),h(bt.$$.fragment,t),rl=i(t),Mt=m(t,"P",{"data-svelte-h":!0}),d(Mt)!=="svelte-ptrt24"&&(Mt.innerHTML=on),pl=i(t),h(yt.$$.fragment,t),ml=i(t),h(Tt.$$.fragment,t),dl=i(t),wt=m(t,"P",{"data-svelte-h":!0}),d(wt)!=="svelte-jywid1"&&(wt.innerHTML=rn),ul=i(t),h(V.$$.fragment,t),fl=i(t),h(F.$$.fragment,t),hl=i(t),h($t.$$.fragment,t),cl=i(t),h(x.$$.fragment,t),gl=i(t),Jt=m(t,"P",{"data-svelte-h":!0}),d(Jt)!=="svelte-sq0x7n"&&(Jt.innerHTML=pn),bl=i(t),_t=m(t,"P",{"data-svelte-h":!0}),d(_t)!=="svelte-13c61n1"&&(_t.textContent=mn),Ml=i(t),h(vt.$$.fragment,t),yl=i(t),Ut=m(t,"P",{"data-svelte-h":!0}),d(Ut)!=="svelte-ur342m"&&(Ut.innerHTML=dn),Tl=i(t),h(jt.$$.fragment,t),wl=i(t),Ct=m(t,"P",{"data-svelte-h":!0}),d(Ct)!=="svelte-1cjo88z"&&(Ct.textContent=un),$l=i(t),h(qt.$$.fragment,t),Jl=i(t),kt=m(t,"P",{"data-svelte-h":!0}),d(kt)!=="svelte-13cn5o4"&&(kt.innerHTML=fn),_l=i(t),h(Zt.$$.fragment,t),vl=i(t),Gt=m(t,"P",{"data-svelte-h":!0}),d(Gt)!=="svelte-1tle3a0"&&(Gt.innerHTML=hn),Ul=i(t),h(It.$$.fragment,t),jl=i(t),h(X.$$.fragment,t),Cl=i(t),zt=m(t,"P",{"data-svelte-h":!0}),d(zt)!=="svelte-jjj96e"&&(zt.innerHTML=cn),ql=i(t),h(Qt.$$.fragment,t),kl=i(t),Bt=m(t,"P",{"data-svelte-h":!0}),d(Bt)!=="svelte-l9o6va"&&(Bt.innerHTML=gn),Zl=i(t),h(Wt.$$.fragment,t),Gl=i(t),Vt=m(t,"P",{"data-svelte-h":!0}),d(Vt)!=="svelte-42jvja"&&(Vt.innerHTML=bn),Il=i(t),h(Ft.$$.fragment,t),zl=i(t),h(xt.$$.fragment,t),Ql=i(t),Xt=m(t,"P",{"data-svelte-h":!0}),d(Xt)!=="svelte-1y1pnbl"&&(Xt.innerHTML=Mn),Bl=i(t),h(Rt.$$.fragment,t),Wl=i(t),h(R.$$.fragment,t),Vl=i(t),Lt=m(t,"P",{"data-svelte-h":!0}),d(Lt)!=="svelte-m9nw5y"&&(Lt.textContent=yn),Fl=i(t),h(At.$$.fragment,t),xl=i(t),h(Nt.$$.fragment,t),Xl=i(t),Ht=m(t,"P",{"data-svelte-h":!0}),d(Ht)!=="svelte-5a92xi"&&(Ht.innerHTML=Tn),Rl=i(t),Et=m(t,"P",{"data-svelte-h":!0}),d(Et)!=="svelte-gf36q7"&&(Et.textContent=wn),Ll=i(t),h(L.$$.fragment,t),Al=i(t),Yt=m(t,"P",{"data-svelte-h":!0}),d(Yt)!=="svelte-1tamrid"&&(Yt.innerHTML=$n),Nl=i(t),h(A.$$.fragment,t),Hl=i(t),h(N.$$.fragment,t),El=i(t),Pt=m(t,"P",{"data-svelte-h":!0}),d(Pt)!=="svelte-1bxp667"&&(Pt.innerHTML=Jn),Yl=i(t),h(St.$$.fragment,t),Pl=i(t),Dt=m(t,"P",{"data-svelte-h":!0}),d(Dt)!=="svelte-17ps4nd"&&(Dt.innerHTML=_n),Sl=i(t),h(Kt.$$.fragment,t),Dl=i(t),h(Ot.$$.fragment,t),Kl=i(t),h(H.$$.fragment,t),Ol=i(t),te=m(t,"P",{"data-svelte-h":!0}),d(te)!=="svelte-1yx1x2g"&&(te.textContent=vn),ta=i(t),h(ee.$$.fragment,t),ea=i(t),le=m(t,"P",{"data-svelte-h":!0}),d(le)!=="svelte-1v0jh01"&&(le.innerHTML=Un),la=i(t),h(ae.$$.fragment,t),aa=i(t),ne=m(t,"P",{"data-svelte-h":!0}),d(ne)!=="svelte-15oy7am"&&(ne.innerHTML=jn),na=i(t),h(se.$$.fragment,t),sa=i(t),ie=m(t,"P",{"data-svelte-h":!0}),d(ie)!=="svelte-k1g8c2"&&(ie.innerHTML=Cn),ia=i(t),h(oe.$$.fragment,t),oa=i(t),h(re.$$.fragment,t),ra=i(t),pe=m(t,"P",{"data-svelte-h":!0}),d(pe)!=="svelte-ur5rgd"&&(pe.textContent=qn),pa=i(t),me=m(t,"P",{"data-svelte-h":!0}),d(me)!=="svelte-v5sezx"&&(me.innerHTML=kn),ma=i(t),h(de.$$.fragment,t),da=i(t),h(ue.$$.fragment,t),ua=i(t),fe=m(t,"P",{"data-svelte-h":!0}),d(fe)!=="svelte-1ba1s3u"&&(fe.innerHTML=Zn),fa=i(t),h(he.$$.fragment,t),ha=i(t),h(ce.$$.fragment,t),ca=i(t),ge=m(t,"P",{"data-svelte-h":!0}),d(ge)!=="svelte-4nvx4q"&&(ge.innerHTML=Gn),ga=i(t),h(be.$$.fragment,t),ba=i(t),h(E.$$.fragment,t),Ma=i(t),Me=m(t,"P",{"data-svelte-h":!0}),d(Me)!=="svelte-7ob7j"&&(Me.textContent=In),ya=i(t),h(ye.$$.fragment,t),Ta=i(t),Te=m(t,"P",{"data-svelte-h":!0}),d(Te)!=="svelte-15j4whk"&&(Te.innerHTML=zn),wa=i(t),h(we.$$.fragment,t),$a=i(t),h($e.$$.fragment,t),Ja=i(t),Je=m(t,"P",{"data-svelte-h":!0}),d(Je)!=="svelte-7qr54g"&&(Je.innerHTML=Qn),_a=i(t),h(_e.$$.fragment,t),va=i(t),ve=m(t,"P",{"data-svelte-h":!0}),d(ve)!=="svelte-1qoc2ct"&&(ve.innerHTML=Bn),Ua=i(t),h(Ue.$$.fragment,t),ja=i(t),je=m(t,"P",{"data-svelte-h":!0}),d(je)!=="svelte-l0u9eq"&&(je.innerHTML=Wn),Ca=i(t),h(Ce.$$.fragment,t),qa=i(t),h(qe.$$.fragment,t),ka=i(t),ke=m(t,"P",{"data-svelte-h":!0}),d(ke)!=="svelte-opujmz"&&(ke.innerHTML=Vn),Za=i(t),h(Ze.$$.fragment,t),Ga=i(t),Ge=m(t,"P",{"data-svelte-h":!0}),d(Ge)!=="svelte-10oh9lm"&&(Ge.innerHTML=Fn),Ia=i(t),Y=m(t,"DIV",{class:!0,"data-svelte-h":!0}),d(Y)!=="svelte-10jjm5l"&&(Y.innerHTML=xn),za=i(t),P=m(t,"DIV",{class:!0,"data-svelte-h":!0}),d(P)!=="svelte-48at03"&&(P.innerHTML=Xn),Qa=i(t),Ie=m(t,"P",{"data-svelte-h":!0}),d(Ie)!=="svelte-1sm7gom"&&(Ie.innerHTML=Rn),Ba=i(t),h(ze.$$.fragment,t),Wa=i(t),Qe=m(t,"P",{"data-svelte-h":!0}),d(Qe)!=="svelte-81zmsw"&&(Qe.innerHTML=Ln),Va=i(t),S=m(t,"FIGCAPTION",{class:!0,"data-svelte-h":!0}),d(S)!=="svelte-grefv0"&&(S.textContent=An),Fa=i(t),Be=m(t,"TABLE",{"data-svelte-h":!0}),d(Be)!=="svelte-19aactm"&&(Be.innerHTML=Nn),xa=i(t),D=m(t,"FIGCAPTION",{class:!0,"data-svelte-h":!0}),d(D)!=="svelte-1r15bg7"&&(D.textContent=Hn),Xa=i(t),We=m(t,"TABLE",{"data-svelte-h":!0}),d(We)!=="svelte-19fczbk"&&(We.innerHTML=En),Ra=i(t),Ve=m(t,"P",{"data-svelte-h":!0}),d(Ve)!=="svelte-gu8e8k"&&(Ve.innerHTML=Yn),La=i(t),K=m(t,"DIV",{class:!0,"data-svelte-h":!0}),d(K)!=="svelte-1ke50ja"&&(K.innerHTML=Pn),Aa=i(t),Xe=m(t,"P",{}),rs(Xe).forEach(l),this.h()},h(){Fe(n,"name","hf:doc:metadata"),Fe(n,"content",zs),Fe(Y,"class","flex gap-4"),Fe(P,"class","flex gap-4"),Fe(S,"class","text-center text-gray-500 text-lg"),Fe(D,"class","text-center text-gray-500 text-lg"),Fe(K,"class","flex gap-4")},m(t,e){hs(document.head,n),a(t,y,e),a(t,o,e),a(t,u,e),c(r,t,e),a(t,T,e),a(t,U,e),a(t,w,e),a(t,q,e),a(t,j,e),c(k,t,e),a(t,$,e),c(G,t,e),a(t,J,e),a(t,z,e),a(t,Re,e),a(t,O,e),a(t,Le,e),a(t,tt,e),a(t,Ae,e),c(et,t,e),a(t,Ne,e),a(t,lt,e),a(t,He,e),a(t,at,e),a(t,Ee,e),c(nt,t,e),a(t,Ye,e),a(t,st,e),a(t,Pe,e),a(t,it,e),a(t,Se,e),c(ot,t,e),a(t,De,e),c(W,t,e),a(t,Ke,e),a(t,rt,e),a(t,Oe,e),a(t,pt,e),a(t,tl,e),a(t,mt,e),a(t,el,e),c(dt,t,e),a(t,ll,e),a(t,ut,e),a(t,al,e),c(ft,t,e),a(t,nl,e),a(t,ht,e),a(t,sl,e),c(ct,t,e),a(t,il,e),a(t,gt,e),a(t,ol,e),c(bt,t,e),a(t,rl,e),a(t,Mt,e),a(t,pl,e),c(yt,t,e),a(t,ml,e),c(Tt,t,e),a(t,dl,e),a(t,wt,e),a(t,ul,e),c(V,t,e),a(t,fl,e),c(F,t,e),a(t,hl,e),c($t,t,e),a(t,cl,e),c(x,t,e),a(t,gl,e),a(t,Jt,e),a(t,bl,e),a(t,_t,e),a(t,Ml,e),c(vt,t,e),a(t,yl,e),a(t,Ut,e),a(t,Tl,e),c(jt,t,e),a(t,wl,e),a(t,Ct,e),a(t,$l,e),c(qt,t,e),a(t,Jl,e),a(t,kt,e),a(t,_l,e),c(Zt,t,e),a(t,vl,e),a(t,Gt,e),a(t,Ul,e),c(It,t,e),a(t,jl,e),c(X,t,e),a(t,Cl,e),a(t,zt,e),a(t,ql,e),c(Qt,t,e),a(t,kl,e),a(t,Bt,e),a(t,Zl,e),c(Wt,t,e),a(t,Gl,e),a(t,Vt,e),a(t,Il,e),c(Ft,t,e),a(t,zl,e),c(xt,t,e),a(t,Ql,e),a(t,Xt,e),a(t,Bl,e),c(Rt,t,e),a(t,Wl,e),c(R,t,e),a(t,Vl,e),a(t,Lt,e),a(t,Fl,e),c(At,t,e),a(t,xl,e),c(Nt,t,e),a(t,Xl,e),a(t,Ht,e),a(t,Rl,e),a(t,Et,e),a(t,Ll,e),c(L,t,e),a(t,Al,e),a(t,Yt,e),a(t,Nl,e),c(A,t,e),a(t,Hl,e),c(N,t,e),a(t,El,e),a(t,Pt,e),a(t,Yl,e),c(St,t,e),a(t,Pl,e),a(t,Dt,e),a(t,Sl,e),c(Kt,t,e),a(t,Dl,e),c(Ot,t,e),a(t,Kl,e),c(H,t,e),a(t,Ol,e),a(t,te,e),a(t,ta,e),c(ee,t,e),a(t,ea,e),a(t,le,e),a(t,la,e),c(ae,t,e),a(t,aa,e),a(t,ne,e),a(t,na,e),c(se,t,e),a(t,sa,e),a(t,ie,e),a(t,ia,e),c(oe,t,e),a(t,oa,e),c(re,t,e),a(t,ra,e),a(t,pe,e),a(t,pa,e),a(t,me,e),a(t,ma,e),c(de,t,e),a(t,da,e),c(ue,t,e),a(t,ua,e),a(t,fe,e),a(t,fa,e),c(he,t,e),a(t,ha,e),c(ce,t,e),a(t,ca,e),a(t,ge,e),a(t,ga,e),c(be,t,e),a(t,ba,e),c(E,t,e),a(t,Ma,e),a(t,Me,e),a(t,ya,e),c(ye,t,e),a(t,Ta,e),a(t,Te,e),a(t,wa,e),c(we,t,e),a(t,$a,e),c($e,t,e),a(t,Ja,e),a(t,Je,e),a(t,_a,e),c(_e,t,e),a(t,va,e),a(t,ve,e),a(t,Ua,e),c(Ue,t,e),a(t,ja,e),a(t,je,e),a(t,Ca,e),c(Ce,t,e),a(t,qa,e),c(qe,t,e),a(t,ka,e),a(t,ke,e),a(t,Za,e),c(Ze,t,e),a(t,Ga,e),a(t,Ge,e),a(t,Ia,e),a(t,Y,e),a(t,za,e),a(t,P,e),a(t,Qa,e),a(t,Ie,e),a(t,Ba,e),c(ze,t,e),a(t,Wa,e),a(t,Qe,e),a(t,Va,e),a(t,S,e),a(t,Fa,e),a(t,Be,e),a(t,xa,e),a(t,D,e),a(t,Xa,e),a(t,We,e),a(t,Ra,e),a(t,Ve,e),a(t,La,e),a(t,K,e),a(t,Aa,e),a(t,Xe,e),Na=!0},p(t,[e]){const Sn={};e&2&&(Sn.$$scope={dirty:e,ctx:t}),k.$set(Sn);const Dn={};e&2&&(Dn.$$scope={dirty:e,ctx:t}),W.$set(Dn);const Kn={};e&2&&(Kn.$$scope={dirty:e,ctx:t}),V.$set(Kn);const On={};e&2&&(On.$$scope={dirty:e,ctx:t}),F.$set(On);const ts={};e&2&&(ts.$$scope={dirty:e,ctx:t}),x.$set(ts);const es={};e&2&&(es.$$scope={dirty:e,ctx:t}),X.$set(es);const ls={};e&2&&(ls.$$scope={dirty:e,ctx:t}),R.$set(ls);const as={};e&2&&(as.$$scope={dirty:e,ctx:t}),L.$set(as);const ns={};e&2&&(ns.$$scope={dirty:e,ctx:t}),A.$set(ns);const ss={};e&2&&(ss.$$scope={dirty:e,ctx:t}),N.$set(ss);const is={};e&2&&(is.$$scope={dirty:e,ctx:t}),H.$set(is);const os={};e&2&&(os.$$scope={dirty:e,ctx:t}),E.$set(os)},i(t){Na||(g(r.$$.fragment,t),g(k.$$.fragment,t),g(G.$$.fragment,t),g(et.$$.fragment,t),g(nt.$$.fragment,t),g(ot.$$.fragment,t),g(W.$$.fragment,t),g(dt.$$.fragment,t),g(ft.$$.fragment,t),g(ct.$$.fragment,t),g(bt.$$.fragment,t),g(yt.$$.fragment,t),g(Tt.$$.fragment,t),g(V.$$.fragment,t),g(F.$$.fragment,t),g($t.$$.fragment,t),g(x.$$.fragment,t),g(vt.$$.fragment,t),g(jt.$$.fragment,t),g(qt.$$.fragment,t),g(Zt.$$.fragment,t),g(It.$$.fragment,t),g(X.$$.fragment,t),g(Qt.$$.fragment,t),g(Wt.$$.fragment,t),g(Ft.$$.fragment,t),g(xt.$$.fragment,t),g(Rt.$$.fragment,t),g(R.$$.fragment,t),g(At.$$.fragment,t),g(Nt.$$.fragment,t),g(L.$$.fragment,t),g(A.$$.fragment,t),g(N.$$.fragment,t),g(St.$$.fragment,t),g(Kt.$$.fragment,t),g(Ot.$$.fragment,t),g(H.$$.fragment,t),g(ee.$$.fragment,t),g(ae.$$.fragment,t),g(se.$$.fragment,t),g(oe.$$.fragment,t),g(re.$$.fragment,t),g(de.$$.fragment,t),g(ue.$$.fragment,t),g(he.$$.fragment,t),g(ce.$$.fragment,t),g(be.$$.fragment,t),g(E.$$.fragment,t),g(ye.$$.fragment,t),g(we.$$.fragment,t),g($e.$$.fragment,t),g(_e.$$.fragment,t),g(Ue.$$.fragment,t),g(Ce.$$.fragment,t),g(qe.$$.fragment,t),g(Ze.$$.fragment,t),g(ze.$$.fragment,t),Na=!0)},o(t){b(r.$$.fragment,t),b(k.$$.fragment,t),b(G.$$.fragment,t),b(et.$$.fragment,t),b(nt.$$.fragment,t),b(ot.$$.fragment,t),b(W.$$.fragment,t),b(dt.$$.fragment,t),b(ft.$$.fragment,t),b(ct.$$.fragment,t),b(bt.$$.fragment,t),b(yt.$$.fragment,t),b(Tt.$$.fragment,t),b(V.$$.fragment,t),b(F.$$.fragment,t),b($t.$$.fragment,t),b(x.$$.fragment,t),b(vt.$$.fragment,t),b(jt.$$.fragment,t),b(qt.$$.fragment,t),b(Zt.$$.fragment,t),b(It.$$.fragment,t),b(X.$$.fragment,t),b(Qt.$$.fragment,t),b(Wt.$$.fragment,t),b(Ft.$$.fragment,t),b(xt.$$.fragment,t),b(Rt.$$.fragment,t),b(R.$$.fragment,t),b(At.$$.fragment,t),b(Nt.$$.fragment,t),b(L.$$.fragment,t),b(A.$$.fragment,t),b(N.$$.fragment,t),b(St.$$.fragment,t),b(Kt.$$.fragment,t),b(Ot.$$.fragment,t),b(H.$$.fragment,t),b(ee.$$.fragment,t),b(ae.$$.fragment,t),b(se.$$.fragment,t),b(oe.$$.fragment,t),b(re.$$.fragment,t),b(de.$$.fragment,t),b(ue.$$.fragment,t),b(he.$$.fragment,t),b(ce.$$.fragment,t),b(be.$$.fragment,t),b(E.$$.fragment,t),b(ye.$$.fragment,t),b(we.$$.fragment,t),b($e.$$.fragment,t),b(_e.$$.fragment,t),b(Ue.$$.fragment,t),b(Ce.$$.fragment,t),b(qe.$$.fragment,t),b(Ze.$$.fragment,t),b(ze.$$.fragment,t),Na=!1},d(t){t&&(l(y),l(o),l(u),l(T),l(U),l(w),l(q),l(j),l($),l(J),l(z),l(Re),l(O),l(Le),l(tt),l(Ae),l(Ne),l(lt),l(He),l(at),l(Ee),l(Ye),l(st),l(Pe),l(it),l(Se),l(De),l(Ke),l(rt),l(Oe),l(pt),l(tl),l(mt),l(el),l(ll),l(ut),l(al),l(nl),l(ht),l(sl),l(il),l(gt),l(ol),l(rl),l(Mt),l(pl),l(ml),l(dl),l(wt),l(ul),l(fl),l(hl),l(cl),l(gl),l(Jt),l(bl),l(_t),l(Ml),l(yl),l(Ut),l(Tl),l(wl),l(Ct),l($l),l(Jl),l(kt),l(_l),l(vl),l(Gt),l(Ul),l(jl),l(Cl),l(zt),l(ql),l(kl),l(Bt),l(Zl),l(Gl),l(Vt),l(Il),l(zl),l(Ql),l(Xt),l(Bl),l(Wl),l(Vl),l(Lt),l(Fl),l(xl),l(Xl),l(Ht),l(Rl),l(Et),l(Ll),l(Al),l(Yt),l(Nl),l(Hl),l(El),l(Pt),l(Yl),l(Pl),l(Dt),l(Sl),l(Dl),l(Kl),l(Ol),l(te),l(ta),l(ea),l(le),l(la),l(aa),l(ne),l(na),l(sa),l(ie),l(ia),l(oa),l(ra),l(pe),l(pa),l(me),l(ma),l(da),l(ua),l(fe),l(fa),l(ha),l(ca),l(ge),l(ga),l(ba),l(Ma),l(Me),l(ya),l(Ta),l(Te),l(wa),l($a),l(Ja),l(Je),l(_a),l(va),l(ve),l(Ua),l(ja),l(je),l(Ca),l(qa),l(ka),l(ke),l(Za),l(Ga),l(Ge),l(Ia),l(Y),l(za),l(P),l(Qa),l(Ie),l(Ba),l(Wa),l(Qe),l(Va),l(S),l(Fa),l(Be),l(xa),l(D),l(Xa),l(We),l(Ra),l(Ve),l(La),l(K),l(Aa),l(Xe)),l(n),M(r,t),M(k,t),M(G,t),M(et,t),M(nt,t),M(ot,t),M(W,t),M(dt,t),M(ft,t),M(ct,t),M(bt,t),M(yt,t),M(Tt,t),M(V,t),M(F,t),M($t,t),M(x,t),M(vt,t),M(jt,t),M(qt,t),M(Zt,t),M(It,t),M(X,t),M(Qt,t),M(Wt,t),M(Ft,t),M(xt,t),M(Rt,t),M(R,t),M(At,t),M(Nt,t),M(L,t),M(A,t),M(N,t),M(St,t),M(Kt,t),M(Ot,t),M(H,t),M(ee,t),M(ae,t),M(se,t),M(oe,t),M(re,t),M(de,t),M(ue,t),M(he,t),M(ce,t),M(be,t),M(E,t),M(ye,t),M(we,t),M($e,t),M(_e,t),M(Ue,t),M(Ce,t),M(qe,t),M(Ze,t),M(ze,t)}}}const zs='{"title":"Quantization","local":"quantization","sections":[{"title":"AQLM","local":"aqlm","sections":[{"title":"AQLM configurations","local":"aqlm-configurations","sections":[],"depth":3}],"depth":2},{"title":"AWQ","local":"awq","sections":[{"title":"Fused modules","local":"fused-modules","sections":[],"depth":3}],"depth":2},{"title":"AutoGPTQ","local":"autogptq","sections":[{"title":"ExLlama","local":"exllama","sections":[],"depth":3}],"depth":2},{"title":"bitsandbytes","local":"bitsandbytes","sections":[{"title":"8-bit","local":"8-bit","sections":[{"title":"Offloading","local":"offloading","sections":[],"depth":4},{"title":"Outlier threshold","local":"outlier-threshold","sections":[],"depth":4},{"title":"Skip module conversion","local":"skip-module-conversion","sections":[],"depth":4},{"title":"Finetuning","local":"finetuning","sections":[],"depth":4}],"depth":3},{"title":"4-bit","local":"4-bit","sections":[{"title":"Compute data type","local":"compute-data-type","sections":[],"depth":4},{"title":"Normal Float 4 (NF4)","local":"normal-float-4-nf4","sections":[],"depth":4},{"title":"Nested quantization","local":"nested-quantization","sections":[],"depth":4}],"depth":3}],"depth":2},{"title":"Optimum","local":"optimum","sections":[],"depth":2},{"title":"Benchmarks","local":"benchmarks","sections":[{"title":"Fused AWQ modules","local":"fused-awq-modules","sections":[],"depth":3}],"depth":2}],"depth":1}';function Qs(_){return ms(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rs extends ds{constructor(n){super(),us(this,n,Qs,Is,ps,{})}}export{Rs as component};
