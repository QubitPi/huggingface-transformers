import{s as nt,o as ot,n as rt}from"../chunks/scheduler.9bc65507.js";import{S as at,i as st,g as s,s as o,r as d,A as it,h as i,f as n,c as r,j as $,u as p,x as M,k as I,y as t,a as m,v as g,d as f,t as h,w as u}from"../chunks/index.707bf1b6.js";import{D as B}from"../chunks/Docstring.17db21ae.js";import{C as ct}from"../chunks/CodeBlock.54a9f38d.js";import{E as mt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as Te}from"../chunks/Heading.342b1fa6.js";function lt(de){let l,W="Examples:",P,y,x;return y=new ct({props:{code:"JTIzJTIwV2UlMjBjYW4ndCUyMGluc3RhbnRpYXRlJTIwZGlyZWN0bHklMjB0aGUlMjBiYXNlJTIwY2xhc3MlMjAqSW1hZ2VQcm9jZXNzaW5nTWl4aW4qJTIwc28lMjBsZXQncyUyMHNob3clMjB0aGUlMjBleGFtcGxlcyUyMG9uJTIwYSUwQSUyMyUyMGRlcml2ZWQlMjBjbGFzcyUzQSUyMCpDTElQSW1hZ2VQcm9jZXNzb3IqJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQ0xJUEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIlMEEpJTIwJTIwJTIzJTIwRG93bmxvYWQlMjBpbWFnZV9wcm9jZXNzaW5nX2NvbmZpZyUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMENMSVBJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRiUyMiUwQSklMjAlMjAlMjMlMjBFLmcuJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjAob3IlMjBtb2RlbCklMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMENMSVBJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRnByZXByb2Nlc3Nvcl9jb25maWcuanNvbiUyMiklMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBDTElQSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiUyQyUyMGRvX25vcm1hbGl6ZSUzREZhbHNlJTJDJTIwZm9vJTNERmFsc2UlMEEpJTBBYXNzZXJ0JTIwaW1hZ2VfcHJvY2Vzc29yLmRvX25vcm1hbGl6ZSUyMGlzJTIwRmFsc2UlMEFpbWFnZV9wcm9jZXNzb3IlMkMlMjB1bnVzZWRfa3dhcmdzJTIwJTNEJTIwQ0xJUEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIlMkMlMjBkb19ub3JtYWxpemUlM0RGYWxzZSUyQyUyMGZvbyUzREZhbHNlJTJDJTIwcmV0dXJuX3VudXNlZF9rd2FyZ3MlM0RUcnVlJTBBKSUwQWFzc2VydCUyMGltYWdlX3Byb2Nlc3Nvci5kb19ub3JtYWxpemUlMjBpcyUyMEZhbHNlJTBBYXNzZXJ0JTIwdW51c2VkX2t3YXJncyUyMCUzRCUzRCUyMCU3QiUyMmZvbyUyMiUzQSUyMEZhbHNlJTdE",highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *ImageProcessingMixin* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *CLIPImageProcessor*</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>
)  <span class="hljs-comment"># Download image_processing_config from huggingface.co and cache.</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. image processor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
image_processor = CLIPImageProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){l=s("p"),l.textContent=W,P=o(),d(y.$$.fragment)},l(c){l=i(c,"P",{"data-svelte-h":!0}),M(l)!=="svelte-kvfsh7"&&(l.textContent=W),P=r(c),p(y.$$.fragment,c)},m(c,w){m(c,l,w),m(c,P,w),g(y,c,w),x=!0},p:rt,i(c){x||(f(y.$$.fragment,c),x=!0)},o(c){h(y.$$.fragment,c),x=!1},d(c){c&&(n(l),n(P)),u(y,c)}}}function dt(de){let l,W,P,y,x,c,w,Ve="An image processor is in charge of preparing input features for vision models and post processing their outputs. This includes transformations such as resizing, normalization, and conversion to PyTorch, TensorFlow, Flax and Numpy tensors. It may also include model specific post-processing such as converting logits to segmentation masks.",pe,q,ge,b,L,Pe,O,Se=`This is an image processor mixin used to provide saving/loading functionality for sequential and image feature
extractors.`,Ce,C,E,Ue,ee,Qe='Instantiate a type of <a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> from an image processor.',Be,j,je,Z,R,Ze,te,Ye=`Save an image processor object to the directory <code>save_directory</code>, so that it can be re-loaded using the
<a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin.from_pretrained">from_pretrained()</a> class method.`,fe,X,he,_,V,ze,ne,He='Holds the output of the <a href="/docs/transformers/main/en/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad">pad()</a> and feature extractor specific <code>__call__</code> methods.',Je,oe,Ae="This class is derived from a python dictionary and can be used as a dictionary.",ke,z,S,Fe,re,Ge="Convert the inner content to tensors.",Ne,J,Q,De,ae,Ke=`Send all values to device by calling <code>v.to(*args, **kwargs)</code> (PyTorch only). This should support casting in
different <code>dtypes</code> and sending the <code>BatchFeature</code> to a different <code>device</code>.`,ue,Y,_e,v,H,We,k,A,qe,se,Oe=`Center crop an image to <code>(size[&quot;height&quot;], size[&quot;width&quot;])</code>. If the input size is smaller than <code>crop_size</code> along
any edge, the image is padded with 0â€™s and then center cropped.`,Le,F,G,Ee,ie,et="Normalize an image. image = (image - image_mean) / image_std.",Re,N,K,Xe,ce,tt="Rescale an image by a scale factor. image = image * scale.",be,le,ve;return x=new Te({props:{title:"Image Processor",local:"image-processor",headingTag:"h1"}}),q=new Te({props:{title:"ImageProcessingMixin",local:"transformers.ImageProcessingMixin",headingTag:"h2"}}),L=new B({props:{name:"class transformers.ImageProcessingMixin",anchor:"transformers.ImageProcessingMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L68"}}),E=new B({props:{name:"from_pretrained",anchor:"transformers.ImageProcessingMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": Union"},{name:"cache_dir",val:": Union = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": Union = None"},{name:"revision",val:": str = 'main'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained image_processor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a image processor file saved using the
<a href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved image processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model image processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the image processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L95",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A image processor of type <a
  href="/docs/transformers/main/en/main_classes/image_processor#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a>.</p>
`}}),j=new mt({props:{anchor:"transformers.ImageProcessingMixin.from_pretrained.example",$$slots:{default:[lt]},$$scope:{ctx:de}}}),R=new B({props:{name:"save_pretrained",anchor:"transformers.ImageProcessingMixin.save_pretrained",parameters:[{name:"save_directory",val:": Union"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the image processor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/en/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L209"}}),X=new Te({props:{title:"BatchFeature",local:"transformers.BatchFeature",headingTag:"h2"}}),V=new B({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": Optional = None"},{name:"tensor_type",val:": Union = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L61"}}),S=new B({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": Union = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/main/en/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L164"}}),Q=new B({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BatchFeature.to.args",description:`<strong>args</strong> (<code>Tuple</code>) &#x2014;
Will be passed to the <code>to(...)</code> function of the tensors.`,name:"args"},{anchor:"transformers.BatchFeature.to.kwargs",description:`<strong>kwargs</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>to(...)</code> function of the tensors.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L195",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The same instance after modification.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/en/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Y=new Te({props:{title:"BaseImageProcessor",local:"transformers.image_processing_utils.BaseImageProcessor",headingTag:"h2"}}),H=new B({props:{name:"class transformers.image_processing_utils.BaseImageProcessor",anchor:"transformers.image_processing_utils.BaseImageProcessor",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L545"}}),A=new B({props:{name:"center_crop",anchor:"transformers.image_processing_utils.BaseImageProcessor.center_crop",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": Dict"},{name:"data_format",val:": Union = None"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.image_processing_utils.BaseImageProcessor.center_crop.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to center crop.`,name:"image"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.center_crop.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>) &#x2014;
Size of the output image.`,name:"size"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.center_crop.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.center_crop.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L625"}}),G=new B({props:{name:"normalize",anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize",parameters:[{name:"image",val:": ndarray"},{name:"mean",val:": Union"},{name:"std",val:": Union"},{name:"data_format",val:": Union = None"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to normalize.`,name:"image"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize.mean",description:`<strong>mean</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
Image mean to use for normalization.`,name:"mean"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize.std",description:`<strong>std</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
Image standard deviation to use for normalization.`,name:"std"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L588",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The normalized image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),K=new B({props:{name:"rescale",anchor:"transformers.image_processing_utils.BaseImageProcessor.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": float"},{name:"data_format",val:": Union = None"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.image_processing_utils.BaseImageProcessor.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to rescale.`,name:"image"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.rescale.scale",description:`<strong>scale</strong> (<code>float</code>) &#x2014;
The scaling factor to rescale pixel values by.`,name:"scale"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.rescale.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.rescale.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L556",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The rescaled image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),{c(){l=s("meta"),W=o(),P=s("p"),y=o(),d(x.$$.fragment),c=o(),w=s("p"),w.textContent=Ve,pe=o(),d(q.$$.fragment),ge=o(),b=s("div"),d(L.$$.fragment),Pe=o(),O=s("p"),O.textContent=Se,Ce=o(),C=s("div"),d(E.$$.fragment),Ue=o(),ee=s("p"),ee.innerHTML=Qe,Be=o(),d(j.$$.fragment),je=o(),Z=s("div"),d(R.$$.fragment),Ze=o(),te=s("p"),te.innerHTML=Ye,fe=o(),d(X.$$.fragment),he=o(),_=s("div"),d(V.$$.fragment),ze=o(),ne=s("p"),ne.innerHTML=He,Je=o(),oe=s("p"),oe.textContent=Ae,ke=o(),z=s("div"),d(S.$$.fragment),Fe=o(),re=s("p"),re.textContent=Ge,Ne=o(),J=s("div"),d(Q.$$.fragment),De=o(),ae=s("p"),ae.innerHTML=Ke,ue=o(),d(Y.$$.fragment),_e=o(),v=s("div"),d(H.$$.fragment),We=o(),k=s("div"),d(A.$$.fragment),qe=o(),se=s("p"),se.innerHTML=Oe,Le=o(),F=s("div"),d(G.$$.fragment),Ee=o(),ie=s("p"),ie.textContent=et,Re=o(),N=s("div"),d(K.$$.fragment),Xe=o(),ce=s("p"),ce.textContent=tt,be=o(),le=s("p"),this.h()},l(e){const a=it("svelte-u9bgzb",document.head);l=i(a,"META",{name:!0,content:!0}),a.forEach(n),W=r(e),P=i(e,"P",{}),$(P).forEach(n),y=r(e),p(x.$$.fragment,e),c=r(e),w=i(e,"P",{"data-svelte-h":!0}),M(w)!=="svelte-l9ii3n"&&(w.textContent=Ve),pe=r(e),p(q.$$.fragment,e),ge=r(e),b=i(e,"DIV",{class:!0});var T=$(b);p(L.$$.fragment,T),Pe=r(T),O=i(T,"P",{"data-svelte-h":!0}),M(O)!=="svelte-16ht4m3"&&(O.textContent=Se),Ce=r(T),C=i(T,"DIV",{class:!0});var me=$(C);p(E.$$.fragment,me),Ue=r(me),ee=i(me,"P",{"data-svelte-h":!0}),M(ee)!=="svelte-p0wgo9"&&(ee.innerHTML=Qe),Be=r(me),p(j.$$.fragment,me),me.forEach(n),je=r(T),Z=i(T,"DIV",{class:!0});var ye=$(Z);p(R.$$.fragment,ye),Ze=r(ye),te=i(ye,"P",{"data-svelte-h":!0}),M(te)!=="svelte-67v12q"&&(te.innerHTML=Ye),ye.forEach(n),T.forEach(n),fe=r(e),p(X.$$.fragment,e),he=r(e),_=i(e,"DIV",{class:!0});var U=$(_);p(V.$$.fragment,U),ze=r(U),ne=i(U,"P",{"data-svelte-h":!0}),M(ne)!=="svelte-an3ndm"&&(ne.innerHTML=He),Je=r(U),oe=i(U,"P",{"data-svelte-h":!0}),M(oe)!=="svelte-saqdtk"&&(oe.textContent=Ae),ke=r(U),z=i(U,"DIV",{class:!0});var xe=$(z);p(S.$$.fragment,xe),Fe=r(xe),re=i(xe,"P",{"data-svelte-h":!0}),M(re)!=="svelte-pxfh9u"&&(re.textContent=Ge),xe.forEach(n),Ne=r(U),J=i(U,"DIV",{class:!0});var $e=$(J);p(Q.$$.fragment,$e),De=r($e),ae=i($e,"P",{"data-svelte-h":!0}),M(ae)!=="svelte-d0cfhs"&&(ae.innerHTML=Ke),$e.forEach(n),U.forEach(n),ue=r(e),p(Y.$$.fragment,e),_e=r(e),v=i(e,"DIV",{class:!0});var D=$(v);p(H.$$.fragment,D),We=r(D),k=i(D,"DIV",{class:!0});var Ie=$(k);p(A.$$.fragment,Ie),qe=r(Ie),se=i(Ie,"P",{"data-svelte-h":!0}),M(se)!=="svelte-193kiu8"&&(se.innerHTML=Oe),Ie.forEach(n),Le=r(D),F=i(D,"DIV",{class:!0});var Me=$(F);p(G.$$.fragment,Me),Ee=r(Me),ie=i(Me,"P",{"data-svelte-h":!0}),M(ie)!=="svelte-1e5okex"&&(ie.textContent=et),Me.forEach(n),Re=r(D),N=i(D,"DIV",{class:!0});var we=$(N);p(K.$$.fragment,we),Xe=r(we),ce=i(we,"P",{"data-svelte-h":!0}),M(ce)!=="svelte-qun0mt"&&(ce.textContent=tt),we.forEach(n),D.forEach(n),be=r(e),le=i(e,"P",{}),$(le).forEach(n),this.h()},h(){I(l,"name","hf:doc:metadata"),I(l,"content",pt),I(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,a){t(document.head,l),m(e,W,a),m(e,P,a),m(e,y,a),g(x,e,a),m(e,c,a),m(e,w,a),m(e,pe,a),g(q,e,a),m(e,ge,a),m(e,b,a),g(L,b,null),t(b,Pe),t(b,O),t(b,Ce),t(b,C),g(E,C,null),t(C,Ue),t(C,ee),t(C,Be),g(j,C,null),t(b,je),t(b,Z),g(R,Z,null),t(Z,Ze),t(Z,te),m(e,fe,a),g(X,e,a),m(e,he,a),m(e,_,a),g(V,_,null),t(_,ze),t(_,ne),t(_,Je),t(_,oe),t(_,ke),t(_,z),g(S,z,null),t(z,Fe),t(z,re),t(_,Ne),t(_,J),g(Q,J,null),t(J,De),t(J,ae),m(e,ue,a),g(Y,e,a),m(e,_e,a),m(e,v,a),g(H,v,null),t(v,We),t(v,k),g(A,k,null),t(k,qe),t(k,se),t(v,Le),t(v,F),g(G,F,null),t(F,Ee),t(F,ie),t(v,Re),t(v,N),g(K,N,null),t(N,Xe),t(N,ce),m(e,be,a),m(e,le,a),ve=!0},p(e,[a]){const T={};a&2&&(T.$$scope={dirty:a,ctx:e}),j.$set(T)},i(e){ve||(f(x.$$.fragment,e),f(q.$$.fragment,e),f(L.$$.fragment,e),f(E.$$.fragment,e),f(j.$$.fragment,e),f(R.$$.fragment,e),f(X.$$.fragment,e),f(V.$$.fragment,e),f(S.$$.fragment,e),f(Q.$$.fragment,e),f(Y.$$.fragment,e),f(H.$$.fragment,e),f(A.$$.fragment,e),f(G.$$.fragment,e),f(K.$$.fragment,e),ve=!0)},o(e){h(x.$$.fragment,e),h(q.$$.fragment,e),h(L.$$.fragment,e),h(E.$$.fragment,e),h(j.$$.fragment,e),h(R.$$.fragment,e),h(X.$$.fragment,e),h(V.$$.fragment,e),h(S.$$.fragment,e),h(Q.$$.fragment,e),h(Y.$$.fragment,e),h(H.$$.fragment,e),h(A.$$.fragment,e),h(G.$$.fragment,e),h(K.$$.fragment,e),ve=!1},d(e){e&&(n(W),n(P),n(y),n(c),n(w),n(pe),n(ge),n(b),n(fe),n(he),n(_),n(ue),n(_e),n(v),n(be),n(le)),n(l),u(x,e),u(q,e),u(L),u(E),u(j),u(R),u(X,e),u(V),u(S),u(Q),u(Y,e),u(H),u(A),u(G),u(K)}}}const pt='{"title":"Image Processor","local":"image-processor","sections":[{"title":"ImageProcessingMixin","local":"transformers.ImageProcessingMixin","sections":[],"depth":2},{"title":"BatchFeature","local":"transformers.BatchFeature","sections":[],"depth":2},{"title":"BaseImageProcessor","local":"transformers.image_processing_utils.BaseImageProcessor","sections":[],"depth":2}],"depth":1}';function gt(de){return ot(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class yt extends at{constructor(l){super(),st(this,l,gt,dt,nt,{})}}export{yt as component};
