import{s as Rt,o as Lt,n as q}from"../chunks/scheduler.9bc65507.js";import{S as Vt,i as Yt,g as c,s as i,r as f,A as Qt,h as m,f as s,c as l,j as B,u as g,x as M,k as G,y as p,a,v as w,d as _,t as v,w as b}from"../chunks/index.707bf1b6.js";import{T as Oe}from"../chunks/Tip.c2ecdbf4.js";import{D as we}from"../chunks/Docstring.17db21ae.js";import{C as Ke}from"../chunks/CodeBlock.54a9f38d.js";import{E as De}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as Ht}from"../chunks/PipelineTag.44585822.js";import{H as _e}from"../chunks/Heading.342b1fa6.js";function Pt(T){let t,h="Example:",r,d,u;return d=new Ke({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFN3aW52MkNvbmZpZyUyQyUyMFN3aW52Mk1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMFN3aW52MiUyMG1pY3Jvc29mdCUyRnN3aW52Mi10aW55LXBhdGNoNC13aW5kb3c4LTI1NiUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBTd2ludjJDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBtb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwbWljcm9zb2Z0JTJGc3dpbnYyLXRpbnktcGF0Y2g0LXdpbmRvdzgtMjU2JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBTd2ludjJNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Swinv2Config, Swinv2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Swinv2 microsoft/swinv2-tiny-patch4-window8-256 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = Swinv2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the microsoft/swinv2-tiny-patch4-window8-256 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Swinv2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=c("p"),t.textContent=h,r=i(),f(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=h),r=l(o),g(d.$$.fragment,o)},m(o,y){a(o,t,y),a(o,r,y),w(d,o,y),u=!0},p:q,i(o){u||(_(d.$$.fragment,o),u=!0)},o(o){v(d.$$.fragment,o),u=!1},d(o){o&&(s(t),s(r)),b(d,o)}}}function qt(T){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=h},l(r){t=m(r,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(r,d){a(r,t,d)},p:q,d(r){r&&s(t)}}}function Et(T){let t,h="Example:",r,d,u;return d=new Ke({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN3aW52Mk1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmh1Z2dpbmdmYWNlJTJGY2F0cy1pbWFnZSUyMiklMEFpbWFnZSUyMCUzRCUyMGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTVCJTIyaW1hZ2UlMjIlNUQlNUIwJTVEJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZzd2ludjItdGlueS1wYXRjaDQtd2luZG93OC0yNTYlMjIpJTBBbW9kZWwlMjAlM0QlMjBTd2ludjJNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGc3dpbnYyLXRpbnktcGF0Y2g0LXdpbmRvdzgtMjU2JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, Swinv2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Swinv2Model.from_pretrained(<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">768</span>]`,wrap:!1}}),{c(){t=c("p"),t.textContent=h,r=i(),f(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=h),r=l(o),g(d.$$.fragment,o)},m(o,y){a(o,t,y),a(o,r,y),w(d,o,y),u=!0},p:q,i(o){u||(_(d.$$.fragment,o),u=!0)},o(o){v(d.$$.fragment,o),u=!1},d(o){o&&(s(t),s(r)),b(d,o)}}}function At(T){let t,h=`Note that we provide a script to pre-train this model on custom data in our <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining" rel="nofollow">examples
directory</a>.`;return{c(){t=c("p"),t.innerHTML=h},l(r){t=m(r,"P",{"data-svelte-h":!0}),M(t)!=="svelte-7i3y9o"&&(t.innerHTML=h)},m(r,d){a(r,t,d)},p:q,d(r){r&&s(t)}}}function Ot(T){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=h},l(r){t=m(r,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(r,d){a(r,t,d)},p:q,d(r){r&&s(t)}}}function Dt(T){let t,h="Examples:",r,d,u;return d=new Ke({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN3aW52MkZvck1hc2tlZEltYWdlTW9kZWxpbmclMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGc3dpbnYyLXRpbnktcGF0Y2g0LXdpbmRvdzgtMjU2JTIyKSUwQW1vZGVsJTIwJTNEJTIwU3dpbnYyRm9yTWFza2VkSW1hZ2VNb2RlbGluZy5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGc3dpbnYyLXRpbnktcGF0Y2g0LXdpbmRvdzgtMjU2JTIyKSUwQSUwQW51bV9wYXRjaGVzJTIwJTNEJTIwKG1vZGVsLmNvbmZpZy5pbWFnZV9zaXplJTIwJTJGJTJGJTIwbW9kZWwuY29uZmlnLnBhdGNoX3NpemUpJTIwKiolMjAyJTBBcGl4ZWxfdmFsdWVzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikucGl4ZWxfdmFsdWVzJTBBJTIzJTIwY3JlYXRlJTIwcmFuZG9tJTIwYm9vbGVhbiUyMG1hc2slMjBvZiUyMHNoYXBlJTIwKGJhdGNoX3NpemUlMkMlMjBudW1fcGF0Y2hlcyklMEFib29sX21hc2tlZF9wb3MlMjAlM0QlMjB0b3JjaC5yYW5kaW50KGxvdyUzRDAlMkMlMjBoaWdoJTNEMiUyQyUyMHNpemUlM0QoMSUyQyUyMG51bV9wYXRjaGVzKSkuYm9vbCgpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKHBpeGVsX3ZhbHVlcyUyQyUyMGJvb2xfbWFza2VkX3BvcyUzRGJvb2xfbWFza2VkX3BvcyklMEFsb3NzJTJDJTIwcmVjb25zdHJ1Y3RlZF9waXhlbF92YWx1ZXMlMjAlM0QlMjBvdXRwdXRzLmxvc3MlMkMlMjBvdXRwdXRzLnJlY29uc3RydWN0aW9uJTBBbGlzdChyZWNvbnN0cnVjdGVkX3BpeGVsX3ZhbHVlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, Swinv2ForMaskedImageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Swinv2ForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>num_patches = (model.config.image_size // model.config.patch_size) ** <span class="hljs-number">2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create random boolean mask of shape (batch_size, num_patches)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>bool_masked_pos = torch.randint(low=<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=(<span class="hljs-number">1</span>, num_patches)).<span class="hljs-built_in">bool</span>()

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, reconstructed_pixel_values = outputs.loss, outputs.reconstruction
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(reconstructed_pixel_values.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>]`,wrap:!1}}),{c(){t=c("p"),t.textContent=h,r=i(),f(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-kvfsh7"&&(t.textContent=h),r=l(o),g(d.$$.fragment,o)},m(o,y){a(o,t,y),a(o,r,y),w(d,o,y),u=!0},p:q,i(o){u||(_(d.$$.fragment,o),u=!0)},o(o){v(d.$$.fragment,o),u=!1},d(o){o&&(s(t),s(r)),b(d,o)}}}function Kt(T){let t,h=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=h},l(r){t=m(r,"P",{"data-svelte-h":!0}),M(t)!=="svelte-fincs2"&&(t.innerHTML=h)},m(r,d){a(r,t,d)},p:q,d(r){r&&s(t)}}}function en(T){let t,h="Example:",r,d,u;return d=new Ke({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFN3aW52MkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRnN3aW52Mi10aW55LXBhdGNoNC13aW5kb3c4LTI1NiUyMiklMEFtb2RlbCUyMCUzRCUyMFN3aW52MkZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRnN3aW52Mi10aW55LXBhdGNoNC13aW5kb3c4LTI1NiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, Swinv2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = Swinv2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;microsoft/swinv2-tiny-patch4-window8-256&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
Egyptian cat`,wrap:!1}}),{c(){t=c("p"),t.textContent=h,r=i(),f(d.$$.fragment)},l(o){t=m(o,"P",{"data-svelte-h":!0}),M(t)!=="svelte-11lpom8"&&(t.textContent=h),r=l(o),g(d.$$.fragment,o)},m(o,y){a(o,t,y),a(o,r,y),w(d,o,y),u=!0},p:q,i(o){u||(_(d.$$.fragment,o),u=!0)},o(o){v(d.$$.fragment,o),u=!1},d(o){o&&(s(t),s(r)),b(d,o)}}}function tn(T){let t,h,r,d,u,o,y,Ie,E,Mt='The Swin Transformer V2 model was proposed in <a href="https://arxiv.org/abs/2111.09883" rel="nofollow">Swin Transformer V2: Scaling Up Capacity and Resolution</a> by Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, Baining Guo.',Fe,A,yt="The abstract from the paper is the following:",Je,O,Tt="<em>Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google’s billion-level visual models, which consumes 40 times less labelled data and 40 times less training time.</em>",Ue,D,$t=`This model was contributed by <a href="https://huggingface.co/nandwalritik" rel="nofollow">nandwalritik</a>.
The original code can be found <a href="https://github.com/microsoft/Swin-Transformer" rel="nofollow">here</a>.`,We,K,Ze,ee,jt="A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with Swin Transformer v2.",ze,te,Ne,ne,Ct='<li><a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForImageClassification">Swinv2ForImageClassification</a> is supported by this <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">example script</a> and <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">notebook</a>.</li> <li>See also: <a href="../tasks/image_classification">Image classification task guide</a></li>',Be,oe,kt="Besides that:",Ge,se,St='<li><a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling">Swinv2ForMaskedImageModeling</a> is supported by this <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining" rel="nofollow">example script</a>.</li>',Xe,ae,xt="If you’re interested in submitting a resource to be included here, please feel free to open a Pull Request and we’ll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.",Re,re,Le,j,ie,et,ve,It=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Model">Swinv2Model</a>. It is used to instantiate a Swin
Transformer v2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the Swin Transformer v2
<a href="https://huggingface.co/microsoft/swinv2-tiny-patch4-window8-256" rel="nofollow">microsoft/swinv2-tiny-patch4-window8-256</a>
architecture.`,tt,be,Ft=`Configuration objects inherit from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/en/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,nt,X,Ve,le,Ye,F,de,ot,Me,Jt=`The bare Swinv2 Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,st,S,ce,at,ye,Ut='The <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Model">Swinv2Model</a> forward method, overrides the <code>__call__</code> special method.',rt,R,it,L,Qe,me,He,$,pe,lt,Te,Wt=`Swinv2 Model with a decoder on top for masked image modeling, as proposed in
<a href="https://arxiv.org/abs/2111.09886" rel="nofollow">SimMIM</a>.`,dt,V,ct,$e,Zt=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,mt,x,he,pt,je,zt='The <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForMaskedImageModeling">Swinv2ForMaskedImageModeling</a> forward method, overrides the <code>__call__</code> special method.',ht,Y,ut,Q,Pe,ue,qe,C,fe,ft,Ce,Nt=`Swinv2 Model transformer with an image classification head on top (a linear layer on top of the final hidden state
of the [CLS] token) e.g. for ImageNet.`,gt,ke,Bt=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,wt,I,ge,_t,Se,Gt='The <a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2ForImageClassification">Swinv2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',vt,H,bt,P,Ee,xe,Ae;return u=new _e({props:{title:"Swin Transformer V2",local:"swin-transformer-v2",headingTag:"h1"}}),y=new _e({props:{title:"Overview",local:"overview",headingTag:"h2"}}),K=new _e({props:{title:"Resources",local:"resources",headingTag:"h2"}}),te=new Ht({props:{pipeline:"image-classification"}}),re=new _e({props:{title:"Swinv2Config",local:"transformers.Swinv2Config",headingTag:"h2"}}),ie=new we({props:{name:"class transformers.Swinv2Config",anchor:"transformers.Swinv2Config",parameters:[{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 4"},{name:"num_channels",val:" = 3"},{name:"embed_dim",val:" = 96"},{name:"depths",val:" = [2, 2, 6, 2]"},{name:"num_heads",val:" = [3, 6, 12, 24]"},{name:"window_size",val:" = 7"},{name:"pretrained_window_sizes",val:" = [0, 0, 0, 0]"},{name:"mlp_ratio",val:" = 4.0"},{name:"qkv_bias",val:" = True"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"drop_path_rate",val:" = 0.1"},{name:"hidden_act",val:" = 'gelu'"},{name:"use_absolute_embeddings",val:" = False"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"encoder_stride",val:" = 32"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.Swinv2Config.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.Swinv2Config.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.Swinv2Config.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.Swinv2Config.embed_dim",description:`<strong>embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 96) &#x2014;
Dimensionality of patch embedding.`,name:"embed_dim"},{anchor:"transformers.Swinv2Config.depths",description:`<strong>depths</strong> (<code>list(int)</code>, <em>optional</em>, defaults to <code>[2, 2, 6, 2]</code>) &#x2014;
Depth of each layer in the Transformer encoder.`,name:"depths"},{anchor:"transformers.Swinv2Config.num_heads",description:`<strong>num_heads</strong> (<code>list(int)</code>, <em>optional</em>, defaults to <code>[3, 6, 12, 24]</code>) &#x2014;
Number of attention heads in each layer of the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.Swinv2Config.window_size",description:`<strong>window_size</strong> (<code>int</code>, <em>optional</em>, defaults to 7) &#x2014;
Size of windows.`,name:"window_size"},{anchor:"transformers.Swinv2Config.pretrained_window_sizes",description:`<strong>pretrained_window_sizes</strong> (<code>list(int)</code>, <em>optional</em>, defaults to <code>[0, 0, 0, 0]</code>) &#x2014;
Size of windows during pretraining.`,name:"pretrained_window_sizes"},{anchor:"transformers.Swinv2Config.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 4.0) &#x2014;
Ratio of MLP hidden dimensionality to embedding dimensionality.`,name:"mlp_ratio"},{anchor:"transformers.Swinv2Config.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not a learnable bias should be added to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.Swinv2Config.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings and encoder.`,name:"hidden_dropout_prob"},{anchor:"transformers.Swinv2Config.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.Swinv2Config.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Stochastic depth rate.`,name:"drop_path_rate"},{anchor:"transformers.Swinv2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.Swinv2Config.use_absolute_embeddings",description:`<strong>use_absolute_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to add absolute position embeddings to the patch embeddings.`,name:"use_absolute_embeddings"},{anchor:"transformers.Swinv2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.Swinv2Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.Swinv2Config.encoder_stride",description:`<strong>encoder_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
Factor to increase the spatial resolution by in the decoder head for masked image modeling.`,name:"encoder_stride"},{anchor:"transformers.Swinv2Config.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage.`,name:"out_features"},{anchor:"transformers.Swinv2Config.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage.`,name:"out_indices"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swinv2/configuration_swinv2.py#L31"}}),X=new De({props:{anchor:"transformers.Swinv2Config.example",$$slots:{default:[Pt]},$$scope:{ctx:T}}}),le=new _e({props:{title:"Swinv2Model",local:"transformers.Swinv2Model",headingTag:"h2"}}),de=new we({props:{name:"class transformers.Swinv2Model",anchor:"transformers.Swinv2Model",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"},{name:"use_mask_token",val:" = False"}],parametersDescription:[{anchor:"transformers.Swinv2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swinv2/modeling_swinv2.py#L992"}}),ce=new we({props:{name:"forward",anchor:"transformers.Swinv2Model.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Swinv2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.Swinv2Model.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Swinv2Model.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Swinv2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Swinv2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Swinv2Model.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>, <em>optional</em>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swinv2/modeling_swinv2.py#L1024",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.swinv2.modeling_swinv2.Swinv2ModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config"
>Swinv2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>, <em>optional</em>, returned when <code>add_pooling_layer=True</code> is passed) — Average pooling of the last layer hidden-state.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each stage) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.swinv2.modeling_swinv2.Swinv2ModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new Oe({props:{$$slots:{default:[qt]},$$scope:{ctx:T}}}),L=new De({props:{anchor:"transformers.Swinv2Model.forward.example",$$slots:{default:[Et]},$$scope:{ctx:T}}}),me=new _e({props:{title:"Swinv2ForMaskedImageModeling",local:"transformers.Swinv2ForMaskedImageModeling",headingTag:"h2"}}),pe=new we({props:{name:"class transformers.Swinv2ForMaskedImageModeling",anchor:"transformers.Swinv2ForMaskedImageModeling",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.Swinv2ForMaskedImageModeling.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swinv2/modeling_swinv2.py#L1094"}}),V=new Oe({props:{$$slots:{default:[At]},$$scope:{ctx:T}}}),he=new we({props:{name:"forward",anchor:"transformers.Swinv2ForMaskedImageModeling.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swinv2/modeling_swinv2.py#L1125",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.swinv2.modeling_swinv2.Swinv2MaskedImageModelingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config"
>Swinv2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>bool_masked_pos</code> is provided) — Masked image modeling (MLM) loss.</p>
</li>
<li>
<p><strong>reconstruction</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Reconstructed pixel values.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each stage) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.swinv2.modeling_swinv2.Swinv2MaskedImageModelingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Y=new Oe({props:{$$slots:{default:[Ot]},$$scope:{ctx:T}}}),Q=new De({props:{anchor:"transformers.Swinv2ForMaskedImageModeling.forward.example",$$slots:{default:[Dt]},$$scope:{ctx:T}}}),ue=new _e({props:{title:"Swinv2ForImageClassification",local:"transformers.Swinv2ForImageClassification",headingTag:"h2"}}),fe=new we({props:{name:"class transformers.Swinv2ForImageClassification",anchor:"transformers.Swinv2ForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.Swinv2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config">Swinv2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swinv2/modeling_swinv2.py#L1212"}}),ge=new we({props:{name:"forward",anchor:"transformers.Swinv2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.Swinv2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/en/model_doc/layoutlmv2#transformers.LayoutLMv2FeatureExtractor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.Swinv2ForImageClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.Swinv2ForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.Swinv2ForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.Swinv2ForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/en/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.Swinv2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/swinv2/modeling_swinv2.py#L1235",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.swinv2.modeling_swinv2.Swinv2ImageClassifierOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/en/model_doc/swinv2#transformers.Swinv2Config"
>Swinv2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each stage) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.swinv2.modeling_swinv2.Swinv2ImageClassifierOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),H=new Oe({props:{$$slots:{default:[Kt]},$$scope:{ctx:T}}}),P=new De({props:{anchor:"transformers.Swinv2ForImageClassification.forward.example",$$slots:{default:[en]},$$scope:{ctx:T}}}),{c(){t=c("meta"),h=i(),r=c("p"),d=i(),f(u.$$.fragment),o=i(),f(y.$$.fragment),Ie=i(),E=c("p"),E.innerHTML=Mt,Fe=i(),A=c("p"),A.textContent=yt,Je=i(),O=c("p"),O.innerHTML=Tt,Ue=i(),D=c("p"),D.innerHTML=$t,We=i(),f(K.$$.fragment),Ze=i(),ee=c("p"),ee.textContent=jt,ze=i(),f(te.$$.fragment),Ne=i(),ne=c("ul"),ne.innerHTML=Ct,Be=i(),oe=c("p"),oe.textContent=kt,Ge=i(),se=c("ul"),se.innerHTML=St,Xe=i(),ae=c("p"),ae.textContent=xt,Re=i(),f(re.$$.fragment),Le=i(),j=c("div"),f(ie.$$.fragment),et=i(),ve=c("p"),ve.innerHTML=It,tt=i(),be=c("p"),be.innerHTML=Ft,nt=i(),f(X.$$.fragment),Ve=i(),f(le.$$.fragment),Ye=i(),F=c("div"),f(de.$$.fragment),ot=i(),Me=c("p"),Me.innerHTML=Jt,st=i(),S=c("div"),f(ce.$$.fragment),at=i(),ye=c("p"),ye.innerHTML=Ut,rt=i(),f(R.$$.fragment),it=i(),f(L.$$.fragment),Qe=i(),f(me.$$.fragment),He=i(),$=c("div"),f(pe.$$.fragment),lt=i(),Te=c("p"),Te.innerHTML=Wt,dt=i(),f(V.$$.fragment),ct=i(),$e=c("p"),$e.innerHTML=Zt,mt=i(),x=c("div"),f(he.$$.fragment),pt=i(),je=c("p"),je.innerHTML=zt,ht=i(),f(Y.$$.fragment),ut=i(),f(Q.$$.fragment),Pe=i(),f(ue.$$.fragment),qe=i(),C=c("div"),f(fe.$$.fragment),ft=i(),Ce=c("p"),Ce.textContent=Nt,gt=i(),ke=c("p"),ke.innerHTML=Bt,wt=i(),I=c("div"),f(ge.$$.fragment),_t=i(),Se=c("p"),Se.innerHTML=Gt,vt=i(),f(H.$$.fragment),bt=i(),f(P.$$.fragment),Ee=i(),xe=c("p"),this.h()},l(e){const n=Qt("svelte-u9bgzb",document.head);t=m(n,"META",{name:!0,content:!0}),n.forEach(s),h=l(e),r=m(e,"P",{}),B(r).forEach(s),d=l(e),g(u.$$.fragment,e),o=l(e),g(y.$$.fragment,e),Ie=l(e),E=m(e,"P",{"data-svelte-h":!0}),M(E)!=="svelte-vxlvb8"&&(E.innerHTML=Mt),Fe=l(e),A=m(e,"P",{"data-svelte-h":!0}),M(A)!=="svelte-vfdo9a"&&(A.textContent=yt),Je=l(e),O=m(e,"P",{"data-svelte-h":!0}),M(O)!=="svelte-11y9zge"&&(O.innerHTML=Tt),Ue=l(e),D=m(e,"P",{"data-svelte-h":!0}),M(D)!=="svelte-1ifw3nq"&&(D.innerHTML=$t),We=l(e),g(K.$$.fragment,e),Ze=l(e),ee=m(e,"P",{"data-svelte-h":!0}),M(ee)!=="svelte-1p7rh05"&&(ee.textContent=jt),ze=l(e),g(te.$$.fragment,e),Ne=l(e),ne=m(e,"UL",{"data-svelte-h":!0}),M(ne)!=="svelte-1jjmjpo"&&(ne.innerHTML=Ct),Be=l(e),oe=m(e,"P",{"data-svelte-h":!0}),M(oe)!=="svelte-1k821ua"&&(oe.textContent=kt),Ge=l(e),se=m(e,"UL",{"data-svelte-h":!0}),M(se)!=="svelte-1cxkcry"&&(se.innerHTML=St),Xe=l(e),ae=m(e,"P",{"data-svelte-h":!0}),M(ae)!=="svelte-1xesile"&&(ae.textContent=xt),Re=l(e),g(re.$$.fragment,e),Le=l(e),j=m(e,"DIV",{class:!0});var J=B(j);g(ie.$$.fragment,J),et=l(J),ve=m(J,"P",{"data-svelte-h":!0}),M(ve)!=="svelte-hnd3qk"&&(ve.innerHTML=It),tt=l(J),be=m(J,"P",{"data-svelte-h":!0}),M(be)!=="svelte-o55m63"&&(be.innerHTML=Ft),nt=l(J),g(X.$$.fragment,J),J.forEach(s),Ve=l(e),g(le.$$.fragment,e),Ye=l(e),F=m(e,"DIV",{class:!0});var N=B(F);g(de.$$.fragment,N),ot=l(N),Me=m(N,"P",{"data-svelte-h":!0}),M(Me)!=="svelte-24f4e1"&&(Me.innerHTML=Jt),st=l(N),S=m(N,"DIV",{class:!0});var U=B(S);g(ce.$$.fragment,U),at=l(U),ye=m(U,"P",{"data-svelte-h":!0}),M(ye)!=="svelte-png4t4"&&(ye.innerHTML=Ut),rt=l(U),g(R.$$.fragment,U),it=l(U),g(L.$$.fragment,U),U.forEach(s),N.forEach(s),Qe=l(e),g(me.$$.fragment,e),He=l(e),$=m(e,"DIV",{class:!0});var k=B($);g(pe.$$.fragment,k),lt=l(k),Te=m(k,"P",{"data-svelte-h":!0}),M(Te)!=="svelte-npirve"&&(Te.innerHTML=Wt),dt=l(k),g(V.$$.fragment,k),ct=l(k),$e=m(k,"P",{"data-svelte-h":!0}),M($e)!=="svelte-68lg8f"&&($e.innerHTML=Zt),mt=l(k),x=m(k,"DIV",{class:!0});var W=B(x);g(he.$$.fragment,W),pt=l(W),je=m(W,"P",{"data-svelte-h":!0}),M(je)!=="svelte-1uczwda"&&(je.innerHTML=zt),ht=l(W),g(Y.$$.fragment,W),ut=l(W),g(Q.$$.fragment,W),W.forEach(s),k.forEach(s),Pe=l(e),g(ue.$$.fragment,e),qe=l(e),C=m(e,"DIV",{class:!0});var Z=B(C);g(fe.$$.fragment,Z),ft=l(Z),Ce=m(Z,"P",{"data-svelte-h":!0}),M(Ce)!=="svelte-13r27gg"&&(Ce.textContent=Nt),gt=l(Z),ke=m(Z,"P",{"data-svelte-h":!0}),M(ke)!=="svelte-68lg8f"&&(ke.innerHTML=Bt),wt=l(Z),I=m(Z,"DIV",{class:!0});var z=B(I);g(ge.$$.fragment,z),_t=l(z),Se=m(z,"P",{"data-svelte-h":!0}),M(Se)!=="svelte-e8d70y"&&(Se.innerHTML=Gt),vt=l(z),g(H.$$.fragment,z),bt=l(z),g(P.$$.fragment,z),z.forEach(s),Z.forEach(s),Ee=l(e),xe=m(e,"P",{}),B(xe).forEach(s),this.h()},h(){G(t,"name","hf:doc:metadata"),G(t,"content",nn),G(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){p(document.head,t),a(e,h,n),a(e,r,n),a(e,d,n),w(u,e,n),a(e,o,n),w(y,e,n),a(e,Ie,n),a(e,E,n),a(e,Fe,n),a(e,A,n),a(e,Je,n),a(e,O,n),a(e,Ue,n),a(e,D,n),a(e,We,n),w(K,e,n),a(e,Ze,n),a(e,ee,n),a(e,ze,n),w(te,e,n),a(e,Ne,n),a(e,ne,n),a(e,Be,n),a(e,oe,n),a(e,Ge,n),a(e,se,n),a(e,Xe,n),a(e,ae,n),a(e,Re,n),w(re,e,n),a(e,Le,n),a(e,j,n),w(ie,j,null),p(j,et),p(j,ve),p(j,tt),p(j,be),p(j,nt),w(X,j,null),a(e,Ve,n),w(le,e,n),a(e,Ye,n),a(e,F,n),w(de,F,null),p(F,ot),p(F,Me),p(F,st),p(F,S),w(ce,S,null),p(S,at),p(S,ye),p(S,rt),w(R,S,null),p(S,it),w(L,S,null),a(e,Qe,n),w(me,e,n),a(e,He,n),a(e,$,n),w(pe,$,null),p($,lt),p($,Te),p($,dt),w(V,$,null),p($,ct),p($,$e),p($,mt),p($,x),w(he,x,null),p(x,pt),p(x,je),p(x,ht),w(Y,x,null),p(x,ut),w(Q,x,null),a(e,Pe,n),w(ue,e,n),a(e,qe,n),a(e,C,n),w(fe,C,null),p(C,ft),p(C,Ce),p(C,gt),p(C,ke),p(C,wt),p(C,I),w(ge,I,null),p(I,_t),p(I,Se),p(I,vt),w(H,I,null),p(I,bt),w(P,I,null),a(e,Ee,n),a(e,xe,n),Ae=!0},p(e,[n]){const J={};n&2&&(J.$$scope={dirty:n,ctx:e}),X.$set(J);const N={};n&2&&(N.$$scope={dirty:n,ctx:e}),R.$set(N);const U={};n&2&&(U.$$scope={dirty:n,ctx:e}),L.$set(U);const k={};n&2&&(k.$$scope={dirty:n,ctx:e}),V.$set(k);const W={};n&2&&(W.$$scope={dirty:n,ctx:e}),Y.$set(W);const Z={};n&2&&(Z.$$scope={dirty:n,ctx:e}),Q.$set(Z);const z={};n&2&&(z.$$scope={dirty:n,ctx:e}),H.$set(z);const Xt={};n&2&&(Xt.$$scope={dirty:n,ctx:e}),P.$set(Xt)},i(e){Ae||(_(u.$$.fragment,e),_(y.$$.fragment,e),_(K.$$.fragment,e),_(te.$$.fragment,e),_(re.$$.fragment,e),_(ie.$$.fragment,e),_(X.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(ce.$$.fragment,e),_(R.$$.fragment,e),_(L.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),_(V.$$.fragment,e),_(he.$$.fragment,e),_(Y.$$.fragment,e),_(Q.$$.fragment,e),_(ue.$$.fragment,e),_(fe.$$.fragment,e),_(ge.$$.fragment,e),_(H.$$.fragment,e),_(P.$$.fragment,e),Ae=!0)},o(e){v(u.$$.fragment,e),v(y.$$.fragment,e),v(K.$$.fragment,e),v(te.$$.fragment,e),v(re.$$.fragment,e),v(ie.$$.fragment,e),v(X.$$.fragment,e),v(le.$$.fragment,e),v(de.$$.fragment,e),v(ce.$$.fragment,e),v(R.$$.fragment,e),v(L.$$.fragment,e),v(me.$$.fragment,e),v(pe.$$.fragment,e),v(V.$$.fragment,e),v(he.$$.fragment,e),v(Y.$$.fragment,e),v(Q.$$.fragment,e),v(ue.$$.fragment,e),v(fe.$$.fragment,e),v(ge.$$.fragment,e),v(H.$$.fragment,e),v(P.$$.fragment,e),Ae=!1},d(e){e&&(s(h),s(r),s(d),s(o),s(Ie),s(E),s(Fe),s(A),s(Je),s(O),s(Ue),s(D),s(We),s(Ze),s(ee),s(ze),s(Ne),s(ne),s(Be),s(oe),s(Ge),s(se),s(Xe),s(ae),s(Re),s(Le),s(j),s(Ve),s(Ye),s(F),s(Qe),s(He),s($),s(Pe),s(qe),s(C),s(Ee),s(xe)),s(t),b(u,e),b(y,e),b(K,e),b(te,e),b(re,e),b(ie),b(X),b(le,e),b(de),b(ce),b(R),b(L),b(me,e),b(pe),b(V),b(he),b(Y),b(Q),b(ue,e),b(fe),b(ge),b(H),b(P)}}}const nn='{"title":"Swin Transformer V2","local":"swin-transformer-v2","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"Swinv2Config","local":"transformers.Swinv2Config","sections":[],"depth":2},{"title":"Swinv2Model","local":"transformers.Swinv2Model","sections":[],"depth":2},{"title":"Swinv2ForMaskedImageModeling","local":"transformers.Swinv2ForMaskedImageModeling","sections":[],"depth":2},{"title":"Swinv2ForImageClassification","local":"transformers.Swinv2ForImageClassification","sections":[],"depth":2}],"depth":1}';function on(T){return Lt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class hn extends Vt{constructor(t){super(),Yt(this,t,on,tn,Rt,{})}}export{hn as component};
