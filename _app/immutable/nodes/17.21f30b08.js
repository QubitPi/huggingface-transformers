import{s as cn,o as mn,n as ms}from"../chunks/scheduler.9bc65507.js";import{S as Mn,i as hn,g as i,s as n,r as c,A as wn,h as r,f as l,c as a,j as rn,u as m,x as u,k as un,y as dn,a as t,v as M,d as h,t as w,w as d}from"../chunks/index.707bf1b6.js";import{T as hs}from"../chunks/Tip.c2ecdbf4.js";import{C as J}from"../chunks/CodeBlock.54a9f38d.js";import{H as Z}from"../chunks/Heading.342b1fa6.js";import{H as jn,a as on}from"../chunks/HfOption.6d864328.js";function Tn(U){let p,T='For any other installation issues, please <a href="https://github.com/microsoft/DeepSpeed/issues" rel="nofollow">open an issue</a> with the DeepSpeed team.';return{c(){p=i("p"),p.innerHTML=T},l(o){p=r(o,"P",{"data-svelte-h":!0}),u(p)!=="svelte-qjzsty"&&(p.innerHTML=T)},m(o,b){t(o,p,b)},p:ms,d(o){o&&l(p)}}}function bn(U){let p,T="To use NVMe offload, add the <code>DS_BUILD_AIO=1</code> parameter to the build command and make sure you install the libaio-dev package system-wide.";return{c(){p=i("p"),p.innerHTML=T},l(o){p=r(o,"P",{"data-svelte-h":!0}),u(p)!=="svelte-14a3v4i"&&(p.innerHTML=T)},m(o,b){t(o,p,b)},p:ms,d(o){o&&l(p)}}}function Jn(U){let p,T;return p=new J({props:{code:"Q1VEQV9WSVNJQkxFX0RFVklDRVMlM0QwJTIwcHl0aG9uJTIwLWMlMjAlMjJpbXBvcnQlMjB0b3JjaCUzQiUyMHByaW50KHRvcmNoLmN1ZGEuZ2V0X2RldmljZV9jYXBhYmlsaXR5KCkpJTIy",highlighted:'CUDA_VISIBLE_DEVICES=0 python -c <span class="hljs-string">&quot;import torch; print(torch.cuda.get_device_capability())&quot;</span>',wrap:!1}}),{c(){c(p.$$.fragment)},l(o){m(p.$$.fragment,o)},m(o,b){M(p,o,b),T=!0},p:ms,i(o){T||(h(p.$$.fragment,o),T=!0)},o(o){w(p.$$.fragment,o),T=!1},d(o){d(p,o)}}}function fn(U){let p,T="To find the architecture for GPU <code>0</code>:",o,b,j,y,I="This means your GPU architecture is <code>8.6</code>.",$;return b=new J({props:{code:"Q1VEQV9WSVNJQkxFX0RFVklDRVMlM0QwJTIwcHl0aG9uJTIwLWMlMjAlMjJpbXBvcnQlMjB0b3JjaCUzQiUyMCU1QyUwQXByaW50KHRvcmNoLmN1ZGEuZ2V0X2RldmljZV9wcm9wZXJ0aWVzKHRvcmNoLmRldmljZSgnY3VkYScpKSklMEElMjJfQ3VkYURldmljZVByb3BlcnRpZXMobmFtZSUzRCdHZUZvcmNlJTIwUlRYJTIwMzA5MCclMkMlMjBtYWpvciUzRDglMkMlMjBtaW5vciUzRDYlMkMlMjB0b3RhbF9tZW1vcnklM0QyNDI2OE1CJTJDJTIwbXVsdGlfcHJvY2Vzc29yX2NvdW50JTNEODIpJTIy",highlighted:`CUDA_VISIBLE_DEVICES=0 python -c <span class="hljs-string">&quot;import torch; \\
print(torch.cuda.get_device_properties(torch.device(&#x27;cuda&#x27;)))
&quot;</span>_CudaDeviceProperties(name=<span class="hljs-string">&#x27;GeForce RTX 3090&#x27;</span>, major=8, minor=6, total_memory=24268MB, multi_processor_count=82)<span class="hljs-string">&quot;</span>`,wrap:!1}}),{c(){p=i("p"),p.innerHTML=T,o=n(),c(b.$$.fragment),j=n(),y=i("p"),y.innerHTML=I},l(f){p=r(f,"P",{"data-svelte-h":!0}),u(p)!=="svelte-1w9cj4k"&&(p.innerHTML=T),o=a(f),m(b.$$.fragment,f),j=a(f),y=r(f,"P",{"data-svelte-h":!0}),u(y)!=="svelte-c7uhux"&&(y.innerHTML=I)},m(f,C){t(f,p,C),t(f,o,C),M(b,f,C),t(f,j,C),t(f,y,C),$=!0},p:ms,i(f){$||(h(b.$$.fragment,f),$=!0)},o(f){w(b.$$.fragment,f),$=!1},d(f){f&&(l(p),l(o),l(j),l(y)),d(b,f)}}}function yn(U){let p,T,o,b;return p=new on({props:{id:"arch",option:"same GPUs",$$slots:{default:[Jn]},$$scope:{ctx:U}}}),o=new on({props:{id:"arch",option:"specific GPU",$$slots:{default:[fn]},$$scope:{ctx:U}}}),{c(){c(p.$$.fragment),T=n(),c(o.$$.fragment)},l(j){m(p.$$.fragment,j),T=a(j),m(o.$$.fragment,j)},m(j,y){M(p,j,y),t(j,T,y),M(o,j,y),b=!0},p(j,y){const I={};y&2&&(I.$$scope={dirty:y,ctx:j}),p.$set(I);const $={};y&2&&($.$$scope={dirty:y,ctx:j}),o.$set($)},i(j){b||(h(p.$$.fragment,j),h(o.$$.fragment,j),b=!0)},o(j){w(p.$$.fragment,j),w(o.$$.fragment,j),b=!1},d(j){j&&l(T),d(p,j),d(o,j)}}}function Un(U){let p,T="This feature is currently available for PyTorch-only.";return{c(){p=i("p"),p.textContent=T},l(o){p=r(o,"P",{"data-svelte-h":!0}),u(p)!=="svelte-1659hm6"&&(p.textContent=T)},m(o,b){t(o,p,b)},p:ms,d(o){o&&l(p)}}}function In(U){let p,T="For multi-GPU training it requires DDP (<code>torch.distributed.launch</code>).";return{c(){p=i("p"),p.innerHTML=T},l(o){p=r(o,"P",{"data-svelte-h":!0}),u(p)!=="svelte-b2srlm"&&(p.innerHTML=T)},m(o,b){t(o,p,b)},p:ms,d(o){o&&l(p)}}}function Cn(U){let p,T="This feature can be used with any <code>nn.Module</code>-based model.";return{c(){p=i("p"),p.innerHTML=T},l(o){p=r(o,"P",{"data-svelte-h":!0}),u(p)!=="svelte-tkb0f4"&&(p.innerHTML=T)},m(o,b){t(o,p,b)},p:ms,d(o){o&&l(p)}}}function $n(U){let p,T,o,b,j,y,I,$="Training on multiple GPUs can be a tricky endeavor whether you’re running into installation issues or communication problems between your GPUs. This debugging guide covers some issues you may run into and how to resolve them.",f,C,ws,V,st="If you’re using DeepSpeed, you’ve probably already installed it with the following command.",ds,R,js,B,lt="DeepSpeed compiles CUDA C++ code and it can be a potential source of errors when building PyTorch extensions that require CUDA. These errors depend on how CUDA is installed on your system, and this section focuses on PyTorch built with <em>CUDA 10.2</em>.",Ts,A,bs,L,Js,x,tt="PyTorch comes with its own CUDA toolkit, but to use DeepSpeed with PyTorch, you need to have an identical version of CUDA installed system-wide. For example, if you installed PyTorch with <code>cudatoolkit==10.2</code> in your Python environment, then you’ll also need to have CUDA 10.2 installed system-wide. If you don’t have CUDA installed system-wide, you should install it first.",fs,N,nt="The exact location may vary from system to system, but <code>usr/local/cuda-10.2</code> is the most common location on many Unix systems. When CUDA is correctly setup and added to your <code>PATH</code> environment variable, you can find the installation location with the following command:",ys,W,Us,D,Is,E,at="You may also have more than one CUDA toolkit installed system-wide.",Cs,H,$s,X,pt="Typically, package installers set the paths to whatever the last version was installed. If the package build fails because it can’t find the right CUDA version (despite it being installed system-wide already), then you need to configure the <code>PATH</code> and <code>LD_LIBRARY_PATH</code> environment variables to point to the correct path.",Zs,Q,it="Take a look at the contents of these environment variables first:",As,S,vs,Y,rt="<code>PATH</code> lists the locations of the executables and <code>LD_LIBRARY_PATH</code> lists where to look for shared libraries. Earlier entries are prioritized over later ones, and <code>:</code> is used to separate multiple entries. To tell the build program where to find the specific CUDA toolkit you want, insert the correct path to list first. This command prepends rather than overwrites the existing values.",Gs,z,gs,F,ut="In addition, you should also check the directories you assign actually exist. The <code>lib64</code> sub-directory contains various CUDA <code>.so</code> objects (like <code>libcudart.so</code>) and while it is unlikely your system names them differently, you should check the actual names and change them accordingly.",_s,P,ks,q,ot="Sometimes, older CUDA versions may refuse to build with newer compilers. For example, if you have <code>gcc-9</code> but CUDA wants <code>gcc-7</code>. Usually, installing the latest CUDA toolkit enables support for the newer compiler.",Vs,K,ct="You could also install an older version of the compiler in addition to the one you’re currently using (or it may already be installed but it’s not used by default and the build system can’t see it). To resolve this, you can create a symlink to give the build system visibility to the older compiler.",Rs,O,Bs,ee,Ls,se,mt="If you’re still having issues with installing DeepSpeed or if you’re building DeepSpeed at run time, you can try to prebuild the DeepSpeed modules before installing them. To make a local build for DeepSpeed:",xs,le,Ns,v,Ws,te,Mt='Next, you’ll have to specify your GPU’s architecture by editing the <code>TORCH_CUDA_ARCH_LIST</code> variable (find a complete list of NVIDIA GPUs and their corresponding architectures on this <a href="https://developer.nvidia.com/cuda-gpus" rel="nofollow">page</a>). To check the PyTorch version that corresponds to your architecture, run the following command:',Ds,ne,Es,ae,ht="Find the architecture for a GPU with the following command:",Hs,G,Xs,pe,wt="If you get <code>8, 6</code>, then you can set <code>TORCH_CUDA_ARCH_LIST=&quot;8.6&quot;</code>. For multiple GPUs with different architectures, list them like <code>TORCH_CUDA_ARCH_LIST=&quot;6.1;8.6&quot;</code>.",Qs,ie,dt="It is also possible to not specify <code>TORCH_CUDA_ARCH_LIST</code> and the build program automatically queries the GPU architecture of the build. However, it may or may not match the actual GPU on the target machine which is why it is better to explicitly specify the correct architecture.",Ss,re,jt="For training on multiple machines with the same setup, you’ll need to make a binary wheel:",Ys,ue,zs,oe,Tt="This command generates a binary wheel that’ll look something like <code>dist/deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl</code>. Now you can install this wheel locally or on another machine.",Fs,ce,Ps,me,qs,Me,bt="When training or inferencing with <code>DistributedDataParallel</code> and multiple GPU, if you run into issue of inter-communication between processes and/or nodes, you can use the following script to diagnose network issues.",Ks,he,Os,we,Jt="For example to test how 2 GPUs interact do:",el,de,sl,je,ft="If both processes can talk to each and allocate GPU memory each will print an OK status.",ll,Te,yt="For more GPUs or nodes adjust the arguments in the script.",tl,be,Ut="You will find a lot more details inside the diagnostics script and even a recipe to how you could run it in a SLURM environment.",nl,Je,It="An additional level of debug is to add <code>NCCL_DEBUG=INFO</code> environment variable as follows:",al,fe,pl,ye,Ct="This will dump a lot of NCCL-related debug information, which you can then search online if you find that some problems are reported. Or if you’re not sure how to interpret the output you can share the log file in an Issue.",il,Ue,rl,g,ul,_,ol,k,cl,Ie,$t=`If you start getting <code>loss=NaN</code> or the model inhibits some other abnormal behavior due to <code>inf</code> or <code>nan</code> in
activations or weights one needs to discover where the first underflow or overflow happens and what led to it. Luckily
you can accomplish that easily by activating a special module that will do the detection automatically.`,ml,Ce,Zt='If you’re using <a href="/docs/transformers/main/en/main_classes/trainer#transformers.Trainer">Trainer</a>, you just need to add:',Ml,$e,hl,Ze,At=`to the normal command line arguments, or pass <code>debug=&quot;underflow_overflow&quot;</code> when creating the
<a href="/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> object.`,wl,Ae,vt="If you’re using your own training loop or another Trainer you can accomplish the same with:",dl,ve,jl,Ge,Gt=`<a href="/docs/transformers/main/en/internal/trainer_utils#transformers.debug_utils.DebugUnderflowOverflow">DebugUnderflowOverflow</a> inserts hooks into the model that immediately after each
forward call will test input and output variables and also the corresponding module’s weights. As soon as <code>inf</code> or
<code>nan</code> is detected in at least one element of the activations or weights, the program will assert and print a report
like this (this was caught with <code>google/mt5-small</code> under fp16 mixed precision):`,Tl,ge,bl,_e,gt="The example output has been trimmed in the middle for brevity.",Jl,ke,_t=`The second column shows the value of the absolute largest element, so if you have a closer look at the last few frames,
the inputs and outputs were in the range of <code>1e4</code>. So when this training was done under fp16 mixed precision the very
last step overflowed (since under <code>fp16</code> the largest number before <code>inf</code> is <code>64e3</code>). To avoid overflows under
<code>fp16</code> the activations must remain way below <code>1e4</code>, because <code>1e4 * 1e4 = 1e8</code> so any matrix multiplication with
large activations is going to lead to a numerical overflow condition.`,fl,Ve,kt="At the very start of the trace you can discover at which batch number the problem occurred (here <code>Detected inf/nan during batch_number=0</code> means the problem occurred on the first batch).",yl,Re,Vt=`Each reported frame starts by declaring the fully qualified entry for the corresponding module this frame is reporting
for. If we look just at this frame:`,Ul,Be,Il,Le,Rt=`Here, <code>encoder.block.2.layer.1.layer_norm</code> indicates that it was a layer norm for the first layer, of the second
block of the encoder. And the specific calls of the <code>forward</code> is <code>T5LayerNorm</code>.`,Cl,xe,Bt="Let’s look at the last few frames of that report:",$l,Ne,Zl,We,Lt=`The last frame reports for <code>Dropout.forward</code> function with the first entry for the only input and the second for the
only output. You can see that it was called from an attribute <code>dropout</code> inside <code>DenseReluDense</code> class. We can see
that it happened during the first layer, of the 2nd block, during the very first batch. Finally, the absolute largest
input elements was <code>6.27e+04</code> and same for the output was <code>inf</code>.`,Al,De,xt=`You can see here, that <code>T5DenseGatedGeluDense.forward</code> resulted in output activations, whose absolute max value was
around 62.7K, which is very close to fp16’s top limit of 64K. In the next frame we have <code>Dropout</code> which renormalizes
the weights, after it zeroed some of the elements, which pushes the absolute max value to more than 64K, and we get an
overflow (<code>inf</code>).`,vl,Ee,Nt=`As you can see it’s the previous frames that we need to look into when the numbers start going into very large for fp16
numbers.`,Gl,He,Wt="Let’s match the report to the code from <code>models/t5/modeling_t5.py</code>:",gl,Xe,_l,Qe,Dt="Now it’s easy to see the <code>dropout</code> call, and all the previous calls as well.",kl,Se,Et=`Since the detection is happening in a forward hook, these reports are printed immediately after each <code>forward</code>
returns.`,Vl,Ye,Ht=`Going back to the full report, to act on it and to fix the problem, we need to go a few frames up where the numbers
started to go up and most likely switch to the <code>fp32</code> mode here, so that the numbers don’t overflow when multiplied
or summed up. Of course, there might be other solutions. For example, we could turn off <code>amp</code> temporarily if it’s
enabled, after moving the original <code>forward</code> into a helper wrapper, like so:`,Rl,ze,Bl,Fe,Xt=`Since the automatic detector only reports on inputs and outputs of full frames, once you know where to look, you may
want to analyse the intermediary stages of any specific <code>forward</code> function as well. In such a case you can use the
<code>detect_overflow</code> helper function to inject the detector where you want it, for example:`,Ll,Pe,xl,qe,Qt=`You can see that we added 2 of these and now we track if <code>inf</code> or <code>nan</code> for <code>forwarded_states</code> was detected
somewhere in between.`,Nl,Ke,St=`Actually, the detector already reports these because each of the calls in the example above is a <code>nn.Module</code>, but
let’s say if you had some local direct calculations this is how you’d do that.`,Wl,Oe,Yt=`Additionally, if you’re instantiating the debugger in your own code, you can adjust the number of frames printed from
its default, e.g.:`,Dl,es,El,ss,Hl,ls,zt="The same debugging class can be used for per-batch tracing with the underflow/overflow detection feature turned off.",Xl,ts,Ft=`Let’s say you want to watch the absolute min and max values for all the ingredients of each <code>forward</code> call of a given
batch, and only do that for batches 1 and 3. Then you instantiate this class as:`,Ql,ns,Sl,as,Pt="And now full batches 1 and 3 will be traced using the same format as the underflow/overflow detector does.",Yl,ps,qt="Batches are 0-indexed.",zl,is,Kt=`This is helpful if you know that the program starts misbehaving after a certain batch number, so you can fast-forward
right to that area. Here is a sample truncated output for such configuration:`,Fl,rs,Pl,us,Ot=`Here you will get a huge number of frames dumped - as many as there were forward calls in your model, so it may or may
not what you want, but sometimes it can be easier to use for debugging purposes than a normal debugger. For example, if
a problem starts happening at batch number 150. So you can dump traces for batches 149 and 150 and compare where
numbers started to diverge.`,ql,os,en="You can also specify the batch number after which to stop the training, with:",Kl,cs,Ol,Ms,et;return j=new Z({props:{title:"Debugging",local:"debugging",headingTag:"h1"}}),C=new Z({props:{title:"DeepSpeed CUDA installation",local:"deepspeed-cuda-installation",headingTag:"h2"}}),R=new J({props:{code:"cGlwJTIwaW5zdGFsbCUyMGRlZXBzcGVlZA==",highlighted:"pip install deepspeed",wrap:!1}}),A=new hs({props:{$$slots:{default:[Tn]},$$scope:{ctx:U}}}),L=new Z({props:{title:"Non-identical CUDA toolkits",local:"non-identical-cuda-toolkits",headingTag:"h3"}}),W=new J({props:{code:"d2hpY2glMjBudmNj",highlighted:'<span class="hljs-built_in">which</span> nvcc',wrap:!1}}),D=new Z({props:{title:"Multiple CUDA toolkits",local:"multiple-cuda-toolkits",headingTag:"h3"}}),H=new J({props:{code:"JTJGdXNyJTJGbG9jYWwlMkZjdWRhLTEwLjIlMEElMkZ1c3IlMkZsb2NhbCUyRmN1ZGEtMTEuMA==",highlighted:`/usr/local/cuda-10.2
/usr/local/cuda-11.0`,wrap:!1}}),S=new J({props:{code:"ZWNobyUyMCUyNFBBVEglMEFlY2hvJTIwJTI0TERfTElCUkFSWV9QQVRI",highlighted:`<span class="hljs-built_in">echo</span> <span class="hljs-variable">$PATH</span>
<span class="hljs-built_in">echo</span> <span class="hljs-variable">$LD_LIBRARY_PATH</span>`,wrap:!1}}),z=new J({props:{code:"JTIzJTIwYWRqdXN0JTIwdGhlJTIwdmVyc2lvbiUyMGFuZCUyMGZ1bGwlMjBwYXRoJTIwaWYlMjBuZWVkZWQlMEFleHBvcnQlMjBQQVRIJTNEJTJGdXNyJTJGbG9jYWwlMkZjdWRhLTEwLjIlMkZiaW4lM0ElMjRQQVRIJTBBZXhwb3J0JTIwTERfTElCUkFSWV9QQVRIJTNEJTJGdXNyJTJGbG9jYWwlMkZjdWRhLTEwLjIlMkZsaWI2NCUzQSUyNExEX0xJQlJBUllfUEFUSA==",highlighted:`<span class="hljs-comment"># adjust the version and full path if needed</span>
<span class="hljs-built_in">export</span> PATH=/usr/local/cuda-10.2/bin:<span class="hljs-variable">$PATH</span>
<span class="hljs-built_in">export</span> LD_LIBRARY_PATH=/usr/local/cuda-10.2/lib64:<span class="hljs-variable">$LD_LIBRARY_PATH</span>`,wrap:!1}}),P=new Z({props:{title:"Older CUDA versions",local:"older-cuda-versions",headingTag:"h3"}}),O=new J({props:{code:"JTIzJTIwYWRhcHQlMjB0aGUlMjBwYXRoJTIwdG8lMjB5b3VyJTIwc3lzdGVtJTBBc3VkbyUyMGxuJTIwLXMlMjAlMkZ1c3IlMkZiaW4lMkZnY2MtNyUyMCUyMCUyRnVzciUyRmxvY2FsJTJGY3VkYS0xMC4yJTJGYmluJTJGZ2NjJTBBc3VkbyUyMGxuJTIwLXMlMjAlMkZ1c3IlMkZiaW4lMkZnJTJCJTJCLTclMjAlMjAlMkZ1c3IlMkZsb2NhbCUyRmN1ZGEtMTAuMiUyRmJpbiUyRmclMkIlMkI=",highlighted:`<span class="hljs-comment"># adapt the path to your system</span>
sudo <span class="hljs-built_in">ln</span> -s /usr/bin/gcc-7  /usr/local/cuda-10.2/bin/gcc
sudo <span class="hljs-built_in">ln</span> -s /usr/bin/g++-7  /usr/local/cuda-10.2/bin/g++`,wrap:!1}}),ee=new Z({props:{title:"Prebuild",local:"prebuild",headingTag:"h3"}}),le=new J({props:{code:"Z2l0JTIwY2xvbmUlMjBodHRwcyUzQSUyRiUyRmdpdGh1Yi5jb20lMkZtaWNyb3NvZnQlMkZEZWVwU3BlZWQlMkYlMEFjZCUyMERlZXBTcGVlZCUwQXJtJTIwLXJmJTIwYnVpbGQlMEFUT1JDSF9DVURBX0FSQ0hfTElTVCUzRCUyMjguNiUyMiUyMERTX0JVSUxEX0NQVV9BREFNJTNEMSUyMERTX0JVSUxEX1VUSUxTJTNEMSUyMHBpcCUyMGluc3RhbGwlMjAuJTIwJTVDJTBBLS1nbG9iYWwtb3B0aW9uJTNEJTIyYnVpbGRfZXh0JTIyJTIwLS1nbG9iYWwtb3B0aW9uJTNEJTIyLWo4JTIyJTIwLS1uby1jYWNoZSUyMC12JTIwJTVDJTBBLS1kaXNhYmxlLXBpcC12ZXJzaW9uLWNoZWNrJTIwMiUzRSUyNjElMjAlN0MlMjB0ZWUlMjBidWlsZC5sb2c=",highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/microsoft/DeepSpeed/
<span class="hljs-built_in">cd</span> DeepSpeed
<span class="hljs-built_in">rm</span> -rf build
TORCH_CUDA_ARCH_LIST=<span class="hljs-string">&quot;8.6&quot;</span> DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \\
--global-option=<span class="hljs-string">&quot;build_ext&quot;</span> --global-option=<span class="hljs-string">&quot;-j8&quot;</span> --no-cache -v \\
--disable-pip-version-check 2&gt;&amp;1 | <span class="hljs-built_in">tee</span> build.log`,wrap:!1}}),v=new hs({props:{$$slots:{default:[bn]},$$scope:{ctx:U}}}),ne=new J({props:{code:"cHl0aG9uJTIwLWMlMjAlMjJpbXBvcnQlMjB0b3JjaCUzQiUyMHByaW50KHRvcmNoLmN1ZGEuZ2V0X2FyY2hfbGlzdCgpKSUyMg==",highlighted:'python -c <span class="hljs-string">&quot;import torch; print(torch.cuda.get_arch_list())&quot;</span>',wrap:!1}}),G=new jn({props:{id:"arch",options:["same GPUs","specific GPU"],$$slots:{default:[yn]},$$scope:{ctx:U}}}),ue=new J({props:{code:"Z2l0JTIwY2xvbmUlMjBodHRwcyUzQSUyRiUyRmdpdGh1Yi5jb20lMkZtaWNyb3NvZnQlMkZEZWVwU3BlZWQlMkYlMEFjZCUyMERlZXBTcGVlZCUwQXJtJTIwLXJmJTIwYnVpbGQlMEFUT1JDSF9DVURBX0FSQ0hfTElTVCUzRCUyMjguNiUyMiUyMERTX0JVSUxEX0NQVV9BREFNJTNEMSUyMERTX0JVSUxEX1VUSUxTJTNEMSUyMCU1QyUwQXB5dGhvbiUyMHNldHVwLnB5JTIwYnVpbGRfZXh0JTIwLWo4JTIwYmRpc3Rfd2hlZWw=",highlighted:`git <span class="hljs-built_in">clone</span> https://github.com/microsoft/DeepSpeed/
<span class="hljs-built_in">cd</span> DeepSpeed
<span class="hljs-built_in">rm</span> -rf build
TORCH_CUDA_ARCH_LIST=<span class="hljs-string">&quot;8.6&quot;</span> DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 \\
python setup.py build_ext -j8 bdist_wheel`,wrap:!1}}),ce=new J({props:{code:"cGlwJTIwaW5zdGFsbCUyMGRlZXBzcGVlZC0wLjMuMTMlMkI4Y2QwNDZmLWNwMzgtY3AzOC1saW51eF94ODZfNjQud2hs",highlighted:"pip install deepspeed-0.3.13+8cd046f-cp38-cp38-linux_x86_64.whl",wrap:!1}}),me=new Z({props:{title:"Multi-GPU Network Issues Debug",local:"multi-gpu-network-issues-debug",headingTag:"h2"}}),he=new J({props:{code:"d2dldCUyMGh0dHBzJTNBJTJGJTJGcmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSUyRmh1Z2dpbmdmYWNlJTJGdHJhbnNmb3JtZXJzJTJGbWFpbiUyRnNjcmlwdHMlMkZkaXN0cmlidXRlZCUyRnRvcmNoLWRpc3RyaWJ1dGVkLWdwdS10ZXN0LnB5",highlighted:"wget https://raw.githubusercontent.com/huggingface/transformers/main/scripts/distributed/torch-distributed-gpu-test.py",wrap:!1}}),de=new J({props:{code:"cHl0aG9uJTIwLW0lMjB0b3JjaC5kaXN0cmlidXRlZC5ydW4lMjAtLW5wcm9jX3Blcl9ub2RlJTIwMiUyMC0tbm5vZGVzJTIwMSUyMHRvcmNoLWRpc3RyaWJ1dGVkLWdwdS10ZXN0LnB5",highlighted:"python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py",wrap:!1}}),fe=new J({props:{code:"TkNDTF9ERUJVRyUzRElORk8lMjBweXRob24lMjAtbSUyMHRvcmNoLmRpc3RyaWJ1dGVkLnJ1biUyMC0tbnByb2NfcGVyX25vZGUlMjAyJTIwLS1ubm9kZXMlMjAxJTIwdG9yY2gtZGlzdHJpYnV0ZWQtZ3B1LXRlc3QucHk=",highlighted:"NCCL_DEBUG=INFO python -m torch.distributed.run --nproc_per_node 2 --nnodes 1 torch-distributed-gpu-test.py",wrap:!1}}),Ue=new Z({props:{title:"Underflow and Overflow Detection",local:"underflow-and-overflow-detection",headingTag:"h2"}}),g=new hs({props:{$$slots:{default:[Un]},$$scope:{ctx:U}}}),_=new hs({props:{$$slots:{default:[In]},$$scope:{ctx:U}}}),k=new hs({props:{$$slots:{default:[Cn]},$$scope:{ctx:U}}}),$e=new J({props:{code:"LS1kZWJ1ZyUyMHVuZGVyZmxvd19vdmVyZmxvdw==",highlighted:"--debug underflow_overflow",wrap:!1}}),ve=new J({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5kZWJ1Z191dGlscyUyMGltcG9ydCUyMERlYnVnVW5kZXJmbG93T3ZlcmZsb3clMEElMEFkZWJ1Z19vdmVyZmxvdyUyMCUzRCUyMERlYnVnVW5kZXJmbG93T3ZlcmZsb3cobW9kZWwp",highlighted:`<span class="hljs-keyword">from</span> transformers.debug_utils <span class="hljs-keyword">import</span> DebugUnderflowOverflow

debug_overflow = DebugUnderflowOverflow(model)`,wrap:!1}}),ge=new J({props:{code:"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",highlighted:`<span class="hljs-attribute">Detected</span> inf/nan during batch_number=<span class="hljs-number">0</span>
<span class="hljs-attribute">Last</span> <span class="hljs-number">21</span> forward frames:
<span class="hljs-attribute">abs</span> min  abs max  metadata
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">1</span>.layer.<span class="hljs-number">1</span>.DenseReluDense.dropout Dropout
<span class="hljs-attribute">0</span>.<span class="hljs-number">00</span>e+<span class="hljs-number">00</span> <span class="hljs-number">2</span>.<span class="hljs-number">57</span>e+<span class="hljs-number">02</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">0</span>.<span class="hljs-number">00</span>e+<span class="hljs-number">00</span> <span class="hljs-number">2</span>.<span class="hljs-number">85</span>e+<span class="hljs-number">02</span> output<span class="hljs-meta">
[...]</span>
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">0</span> T5LayerSelfAttention
<span class="hljs-attribute">6</span>.<span class="hljs-number">78</span>e-<span class="hljs-number">04</span> <span class="hljs-number">3</span>.<span class="hljs-number">15</span>e+<span class="hljs-number">03</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">2</span>.<span class="hljs-number">65</span>e-<span class="hljs-number">04</span> <span class="hljs-number">3</span>.<span class="hljs-number">42</span>e+<span class="hljs-number">03</span> output[<span class="hljs-number">0</span>]
             <span class="hljs-attribute">None</span> output[<span class="hljs-number">1</span>]
<span class="hljs-attribute">2</span>.<span class="hljs-number">25</span>e-<span class="hljs-number">01</span> <span class="hljs-number">1</span>.<span class="hljs-number">00</span>e+<span class="hljs-number">04</span> output[<span class="hljs-number">2</span>]
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.layer_norm T5LayerNorm
<span class="hljs-attribute">8</span>.<span class="hljs-number">69</span>e-<span class="hljs-number">02</span> <span class="hljs-number">4</span>.<span class="hljs-number">18</span>e-<span class="hljs-number">01</span> weight
<span class="hljs-attribute">2</span>.<span class="hljs-number">65</span>e-<span class="hljs-number">04</span> <span class="hljs-number">3</span>.<span class="hljs-number">42</span>e+<span class="hljs-number">03</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">1</span>.<span class="hljs-number">79</span>e-<span class="hljs-number">06</span> <span class="hljs-number">4</span>.<span class="hljs-number">65</span>e+<span class="hljs-number">00</span> output
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.DenseReluDense.wi_0 Linear
<span class="hljs-attribute">2</span>.<span class="hljs-number">17</span>e-<span class="hljs-number">07</span> <span class="hljs-number">4</span>.<span class="hljs-number">50</span>e+<span class="hljs-number">00</span> weight
<span class="hljs-attribute">1</span>.<span class="hljs-number">79</span>e-<span class="hljs-number">06</span> <span class="hljs-number">4</span>.<span class="hljs-number">65</span>e+<span class="hljs-number">00</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">2</span>.<span class="hljs-number">68</span>e-<span class="hljs-number">06</span> <span class="hljs-number">3</span>.<span class="hljs-number">70</span>e+<span class="hljs-number">01</span> output
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.DenseReluDense.wi_1 Linear
<span class="hljs-attribute">8</span>.<span class="hljs-number">08</span>e-<span class="hljs-number">07</span> <span class="hljs-number">2</span>.<span class="hljs-number">66</span>e+<span class="hljs-number">01</span> weight
<span class="hljs-attribute">1</span>.<span class="hljs-number">79</span>e-<span class="hljs-number">06</span> <span class="hljs-number">4</span>.<span class="hljs-number">65</span>e+<span class="hljs-number">00</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">1</span>.<span class="hljs-number">27</span>e-<span class="hljs-number">04</span> <span class="hljs-number">2</span>.<span class="hljs-number">37</span>e+<span class="hljs-number">02</span> output
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.DenseReluDense.dropout Dropout
<span class="hljs-attribute">0</span>.<span class="hljs-number">00</span>e+<span class="hljs-number">00</span> <span class="hljs-number">8</span>.<span class="hljs-number">76</span>e+<span class="hljs-number">03</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">0</span>.<span class="hljs-number">00</span>e+<span class="hljs-number">00</span> <span class="hljs-number">9</span>.<span class="hljs-number">74</span>e+<span class="hljs-number">03</span> output
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.DenseReluDense.wo Linear
<span class="hljs-attribute">1</span>.<span class="hljs-number">01</span>e-<span class="hljs-number">06</span> <span class="hljs-number">6</span>.<span class="hljs-number">44</span>e+<span class="hljs-number">00</span> weight
<span class="hljs-attribute">0</span>.<span class="hljs-number">00</span>e+<span class="hljs-number">00</span> <span class="hljs-number">9</span>.<span class="hljs-number">74</span>e+<span class="hljs-number">03</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">3</span>.<span class="hljs-number">18</span>e-<span class="hljs-number">04</span> <span class="hljs-number">6</span>.<span class="hljs-number">27</span>e+<span class="hljs-number">04</span> output
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.DenseReluDense T5DenseGatedGeluDense
<span class="hljs-attribute">1</span>.<span class="hljs-number">79</span>e-<span class="hljs-number">06</span> <span class="hljs-number">4</span>.<span class="hljs-number">65</span>e+<span class="hljs-number">00</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">3</span>.<span class="hljs-number">18</span>e-<span class="hljs-number">04</span> <span class="hljs-number">6</span>.<span class="hljs-number">27</span>e+<span class="hljs-number">04</span> output
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.dropout Dropout
<span class="hljs-attribute">3</span>.<span class="hljs-number">18</span>e-<span class="hljs-number">04</span> <span class="hljs-number">6</span>.<span class="hljs-number">27</span>e+<span class="hljs-number">04</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">0</span>.<span class="hljs-number">00</span>e+<span class="hljs-number">00</span>      inf output`,wrap:!1}}),Be=new J({props:{code:"JTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwZW5jb2Rlci5ibG9jay4yLmxheWVyLjEubGF5ZXJfbm9ybSUyMFQ1TGF5ZXJOb3JtJTBBOC42OWUtMDIlMjA0LjE4ZS0wMSUyMHdlaWdodCUwQTIuNjVlLTA0JTIwMy40MmUlMkIwMyUyMGlucHV0JTVCMCU1RCUwQTEuNzllLTA2JTIwNC42NWUlMkIwMCUyMG91dHB1dA==",highlighted:`                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.layer_norm T5LayerNorm
<span class="hljs-attribute">8</span>.<span class="hljs-number">69</span>e-<span class="hljs-number">02</span> <span class="hljs-number">4</span>.<span class="hljs-number">18</span>e-<span class="hljs-number">01</span> weight
<span class="hljs-attribute">2</span>.<span class="hljs-number">65</span>e-<span class="hljs-number">04</span> <span class="hljs-number">3</span>.<span class="hljs-number">42</span>e+<span class="hljs-number">03</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">1</span>.<span class="hljs-number">79</span>e-<span class="hljs-number">06</span> <span class="hljs-number">4</span>.<span class="hljs-number">65</span>e+<span class="hljs-number">00</span> output`,wrap:!1}}),Ne=new J({props:{code:"RGV0ZWN0ZWQlMjBpbmYlMkZuYW4lMjBkdXJpbmclMjBiYXRjaF9udW1iZXIlM0QwJTBBTGFzdCUyMDIxJTIwZm9yd2FyZCUyMGZyYW1lcyUzQSUwQWFicyUyMG1pbiUyMCUyMGFicyUyMG1heCUyMCUyMG1ldGFkYXRhJTBBJTVCLi4uJTVEJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwZW5jb2Rlci5ibG9jay4yLmxheWVyLjEuRGVuc2VSZWx1RGVuc2Uud2lfMCUyMExpbmVhciUwQTIuMTdlLTA3JTIwNC41MGUlMkIwMCUyMHdlaWdodCUwQTEuNzllLTA2JTIwNC42NWUlMkIwMCUyMGlucHV0JTVCMCU1RCUwQTIuNjhlLTA2JTIwMy43MGUlMkIwMSUyMG91dHB1dCUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGVuY29kZXIuYmxvY2suMi5sYXllci4xLkRlbnNlUmVsdURlbnNlLndpXzElMjBMaW5lYXIlMEE4LjA4ZS0wNyUyMDIuNjZlJTJCMDElMjB3ZWlnaHQlMEExLjc5ZS0wNiUyMDQuNjVlJTJCMDAlMjBpbnB1dCU1QjAlNUQlMEExLjI3ZS0wNCUyMDIuMzdlJTJCMDIlMjBvdXRwdXQlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBlbmNvZGVyLmJsb2NrLjIubGF5ZXIuMS5EZW5zZVJlbHVEZW5zZS53byUyMExpbmVhciUwQTEuMDFlLTA2JTIwNi40NGUlMkIwMCUyMHdlaWdodCUwQTAuMDBlJTJCMDAlMjA5Ljc0ZSUyQjAzJTIwaW5wdXQlNUIwJTVEJTBBMy4xOGUtMDQlMjA2LjI3ZSUyQjA0JTIwb3V0cHV0JTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwZW5jb2Rlci5ibG9jay4yLmxheWVyLjEuRGVuc2VSZWx1RGVuc2UlMjBUNURlbnNlR2F0ZWRHZWx1RGVuc2UlMEExLjc5ZS0wNiUyMDQuNjVlJTJCMDAlMjBpbnB1dCU1QjAlNUQlMEEzLjE4ZS0wNCUyMDYuMjdlJTJCMDQlMjBvdXRwdXQlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBlbmNvZGVyLmJsb2NrLjIubGF5ZXIuMS5kcm9wb3V0JTIwRHJvcG91dCUwQTMuMThlLTA0JTIwNi4yN2UlMkIwNCUyMGlucHV0JTVCMCU1RCUwQTAuMDBlJTJCMDAlMjAlMjAlMjAlMjAlMjAlMjBpbmYlMjBvdXRwdXQ=",highlighted:`<span class="hljs-attribute">Detected</span> inf/nan during batch_number=<span class="hljs-number">0</span>
<span class="hljs-attribute">Last</span> <span class="hljs-number">21</span> forward frames:
<span class="hljs-attribute">abs</span> min  abs max  metadata<span class="hljs-meta">
[...]</span>
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.DenseReluDense.wi_0 Linear
<span class="hljs-attribute">2</span>.<span class="hljs-number">17</span>e-<span class="hljs-number">07</span> <span class="hljs-number">4</span>.<span class="hljs-number">50</span>e+<span class="hljs-number">00</span> weight
<span class="hljs-attribute">1</span>.<span class="hljs-number">79</span>e-<span class="hljs-number">06</span> <span class="hljs-number">4</span>.<span class="hljs-number">65</span>e+<span class="hljs-number">00</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">2</span>.<span class="hljs-number">68</span>e-<span class="hljs-number">06</span> <span class="hljs-number">3</span>.<span class="hljs-number">70</span>e+<span class="hljs-number">01</span> output
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.DenseReluDense.wi_1 Linear
<span class="hljs-attribute">8</span>.<span class="hljs-number">08</span>e-<span class="hljs-number">07</span> <span class="hljs-number">2</span>.<span class="hljs-number">66</span>e+<span class="hljs-number">01</span> weight
<span class="hljs-attribute">1</span>.<span class="hljs-number">79</span>e-<span class="hljs-number">06</span> <span class="hljs-number">4</span>.<span class="hljs-number">65</span>e+<span class="hljs-number">00</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">1</span>.<span class="hljs-number">27</span>e-<span class="hljs-number">04</span> <span class="hljs-number">2</span>.<span class="hljs-number">37</span>e+<span class="hljs-number">02</span> output
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.DenseReluDense.wo Linear
<span class="hljs-attribute">1</span>.<span class="hljs-number">01</span>e-<span class="hljs-number">06</span> <span class="hljs-number">6</span>.<span class="hljs-number">44</span>e+<span class="hljs-number">00</span> weight
<span class="hljs-attribute">0</span>.<span class="hljs-number">00</span>e+<span class="hljs-number">00</span> <span class="hljs-number">9</span>.<span class="hljs-number">74</span>e+<span class="hljs-number">03</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">3</span>.<span class="hljs-number">18</span>e-<span class="hljs-number">04</span> <span class="hljs-number">6</span>.<span class="hljs-number">27</span>e+<span class="hljs-number">04</span> output
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.DenseReluDense T5DenseGatedGeluDense
<span class="hljs-attribute">1</span>.<span class="hljs-number">79</span>e-<span class="hljs-number">06</span> <span class="hljs-number">4</span>.<span class="hljs-number">65</span>e+<span class="hljs-number">00</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">3</span>.<span class="hljs-number">18</span>e-<span class="hljs-number">04</span> <span class="hljs-number">6</span>.<span class="hljs-number">27</span>e+<span class="hljs-number">04</span> output
                  <span class="hljs-attribute">encoder</span>.block.<span class="hljs-number">2</span>.layer.<span class="hljs-number">1</span>.dropout Dropout
<span class="hljs-attribute">3</span>.<span class="hljs-number">18</span>e-<span class="hljs-number">04</span> <span class="hljs-number">6</span>.<span class="hljs-number">27</span>e+<span class="hljs-number">04</span> input[<span class="hljs-number">0</span>]
<span class="hljs-attribute">0</span>.<span class="hljs-number">00</span>e+<span class="hljs-number">00</span>      inf output`,wrap:!1}}),Xe=new J({props:{code:"Y2xhc3MlMjBUNURlbnNlR2F0ZWRHZWx1RGVuc2Uobm4uTW9kdWxlKSUzQSUwQSUyMCUyMCUyMCUyMGRlZiUyMF9faW5pdF9fKHNlbGYlMkMlMjBjb25maWcpJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwc3VwZXIoKS5fX2luaXRfXygpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwc2VsZi53aV8wJTIwJTNEJTIwbm4uTGluZWFyKGNvbmZpZy5kX21vZGVsJTJDJTIwY29uZmlnLmRfZmYlMkMlMjBiaWFzJTNERmFsc2UpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwc2VsZi53aV8xJTIwJTNEJTIwbm4uTGluZWFyKGNvbmZpZy5kX21vZGVsJTJDJTIwY29uZmlnLmRfZmYlMkMlMjBiaWFzJTNERmFsc2UpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwc2VsZi53byUyMCUzRCUyMG5uLkxpbmVhcihjb25maWcuZF9mZiUyQyUyMGNvbmZpZy5kX21vZGVsJTJDJTIwYmlhcyUzREZhbHNlKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHNlbGYuZHJvcG91dCUyMCUzRCUyMG5uLkRyb3BvdXQoY29uZmlnLmRyb3BvdXRfcmF0ZSklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBzZWxmLmdlbHVfYWN0JTIwJTNEJTIwQUNUMkZOJTVCJTIyZ2VsdV9uZXclMjIlNUQlMEElMEElMjAlMjAlMjAlMjBkZWYlMjBmb3J3YXJkKHNlbGYlMkMlMjBoaWRkZW5fc3RhdGVzKSUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGhpZGRlbl9nZWx1JTIwJTNEJTIwc2VsZi5nZWx1X2FjdChzZWxmLndpXzAoaGlkZGVuX3N0YXRlcykpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwaGlkZGVuX2xpbmVhciUyMCUzRCUyMHNlbGYud2lfMShoaWRkZW5fc3RhdGVzKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGhpZGRlbl9zdGF0ZXMlMjAlM0QlMjBoaWRkZW5fZ2VsdSUyMColMjBoaWRkZW5fbGluZWFyJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwaGlkZGVuX3N0YXRlcyUyMCUzRCUyMHNlbGYuZHJvcG91dChoaWRkZW5fc3RhdGVzKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGhpZGRlbl9zdGF0ZXMlMjAlM0QlMjBzZWxmLndvKGhpZGRlbl9zdGF0ZXMpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwcmV0dXJuJTIwaGlkZGVuX3N0YXRlcw==",highlighted:`<span class="hljs-keyword">class</span> <span class="hljs-title class_">T5DenseGatedGeluDense</span>(nn.Module):
    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):
        <span class="hljs-built_in">super</span>().__init__()
        self.wi_0 = nn.Linear(config.d_model, config.d_ff, bias=<span class="hljs-literal">False</span>)
        self.wi_1 = nn.Linear(config.d_model, config.d_ff, bias=<span class="hljs-literal">False</span>)
        self.wo = nn.Linear(config.d_ff, config.d_model, bias=<span class="hljs-literal">False</span>)
        self.dropout = nn.Dropout(config.dropout_rate)
        self.gelu_act = ACT2FN[<span class="hljs-string">&quot;gelu_new&quot;</span>]

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states</span>):
        hidden_gelu = self.gelu_act(self.wi_0(hidden_states))
        hidden_linear = self.wi_1(hidden_states)
        hidden_states = hidden_gelu * hidden_linear
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.wo(hidden_states)
        <span class="hljs-keyword">return</span> hidden_states`,wrap:!1}}),ze=new J({props:{code:"ZGVmJTIwX2ZvcndhcmQoc2VsZiUyQyUyMGhpZGRlbl9zdGF0ZXMpJTNBJTBBJTIwJTIwJTIwJTIwaGlkZGVuX2dlbHUlMjAlM0QlMjBzZWxmLmdlbHVfYWN0KHNlbGYud2lfMChoaWRkZW5fc3RhdGVzKSklMEElMjAlMjAlMjAlMjBoaWRkZW5fbGluZWFyJTIwJTNEJTIwc2VsZi53aV8xKGhpZGRlbl9zdGF0ZXMpJTBBJTIwJTIwJTIwJTIwaGlkZGVuX3N0YXRlcyUyMCUzRCUyMGhpZGRlbl9nZWx1JTIwKiUyMGhpZGRlbl9saW5lYXIlMEElMjAlMjAlMjAlMjBoaWRkZW5fc3RhdGVzJTIwJTNEJTIwc2VsZi5kcm9wb3V0KGhpZGRlbl9zdGF0ZXMpJTBBJTIwJTIwJTIwJTIwaGlkZGVuX3N0YXRlcyUyMCUzRCUyMHNlbGYud28oaGlkZGVuX3N0YXRlcyklMEElMjAlMjAlMjAlMjByZXR1cm4lMjBoaWRkZW5fc3RhdGVzJTBBJTBBJTBBaW1wb3J0JTIwdG9yY2glMEElMEElMEFkZWYlMjBmb3J3YXJkKHNlbGYlMkMlMjBoaWRkZW5fc3RhdGVzKSUzQSUwQSUyMCUyMCUyMCUyMGlmJTIwdG9yY2guaXNfYXV0b2Nhc3RfZW5hYmxlZCgpJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwd2l0aCUyMHRvcmNoLmN1ZGEuYW1wLmF1dG9jYXN0KGVuYWJsZWQlM0RGYWxzZSklM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjByZXR1cm4lMjBzZWxmLl9mb3J3YXJkKGhpZGRlbl9zdGF0ZXMpJTBBJTIwJTIwJTIwJTIwZWxzZSUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHJldHVybiUyMHNlbGYuX2ZvcndhcmQoaGlkZGVuX3N0YXRlcyk=",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">_forward</span>(<span class="hljs-params">self, hidden_states</span>):
    hidden_gelu = self.gelu_act(self.wi_0(hidden_states))
    hidden_linear = self.wi_1(hidden_states)
    hidden_states = hidden_gelu * hidden_linear
    hidden_states = self.dropout(hidden_states)
    hidden_states = self.wo(hidden_states)
    <span class="hljs-keyword">return</span> hidden_states


<span class="hljs-keyword">import</span> torch


<span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states</span>):
    <span class="hljs-keyword">if</span> torch.is_autocast_enabled():
        <span class="hljs-keyword">with</span> torch.cuda.amp.autocast(enabled=<span class="hljs-literal">False</span>):
            <span class="hljs-keyword">return</span> self._forward(hidden_states)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> self._forward(hidden_states)`,wrap:!1}}),Pe=new J({props:{code:"ZnJvbSUyMGRlYnVnX3V0aWxzJTIwaW1wb3J0JTIwZGV0ZWN0X292ZXJmbG93JTBBJTBBJTBBY2xhc3MlMjBUNUxheWVyRkYobm4uTW9kdWxlKSUzQSUwQSUyMCUyMCUyMCUyMCU1Qi4uLiU1RCUwQSUwQSUyMCUyMCUyMCUyMGRlZiUyMGZvcndhcmQoc2VsZiUyQyUyMGhpZGRlbl9zdGF0ZXMpJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwZm9yd2FyZGVkX3N0YXRlcyUyMCUzRCUyMHNlbGYubGF5ZXJfbm9ybShoaWRkZW5fc3RhdGVzKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGRldGVjdF9vdmVyZmxvdyhmb3J3YXJkZWRfc3RhdGVzJTJDJTIwJTIyYWZ0ZXIlMjBsYXllcl9ub3JtJTIyKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGZvcndhcmRlZF9zdGF0ZXMlMjAlM0QlMjBzZWxmLkRlbnNlUmVsdURlbnNlKGZvcndhcmRlZF9zdGF0ZXMpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwZGV0ZWN0X292ZXJmbG93KGZvcndhcmRlZF9zdGF0ZXMlMkMlMjAlMjJhZnRlciUyMERlbnNlUmVsdURlbnNlJTIyKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHJldHVybiUyMGhpZGRlbl9zdGF0ZXMlMjAlMkIlMjBzZWxmLmRyb3BvdXQoZm9yd2FyZGVkX3N0YXRlcyk=",highlighted:`<span class="hljs-keyword">from</span> debug_utils <span class="hljs-keyword">import</span> detect_overflow


<span class="hljs-keyword">class</span> <span class="hljs-title class_">T5LayerFF</span>(nn.Module):
    [...]

    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states</span>):
        forwarded_states = self.layer_norm(hidden_states)
        detect_overflow(forwarded_states, <span class="hljs-string">&quot;after layer_norm&quot;</span>)
        forwarded_states = self.DenseReluDense(forwarded_states)
        detect_overflow(forwarded_states, <span class="hljs-string">&quot;after DenseReluDense&quot;</span>)
        <span class="hljs-keyword">return</span> hidden_states + self.dropout(forwarded_states)`,wrap:!1}}),es=new J({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5kZWJ1Z191dGlscyUyMGltcG9ydCUyMERlYnVnVW5kZXJmbG93T3ZlcmZsb3clMEElMEFkZWJ1Z19vdmVyZmxvdyUyMCUzRCUyMERlYnVnVW5kZXJmbG93T3ZlcmZsb3cobW9kZWwlMkMlMjBtYXhfZnJhbWVzX3RvX3NhdmUlM0QxMDAp",highlighted:`<span class="hljs-keyword">from</span> transformers.debug_utils <span class="hljs-keyword">import</span> DebugUnderflowOverflow

debug_overflow = DebugUnderflowOverflow(model, max_frames_to_save=<span class="hljs-number">100</span>)`,wrap:!1}}),ss=new Z({props:{title:"Specific batch absolute min and max value tracing",local:"specific-batch-absolute-min-and-max-value-tracing",headingTag:"h3"}}),ns=new J({props:{code:"ZGVidWdfb3ZlcmZsb3clMjAlM0QlMjBEZWJ1Z1VuZGVyZmxvd092ZXJmbG93KG1vZGVsJTJDJTIwdHJhY2VfYmF0Y2hfbnVtcyUzRCU1QjElMkMlMjAzJTVEKQ==",highlighted:'debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>])',wrap:!1}}),rs=new J({props:{code:"JTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwKioqJTIwU3RhcnRpbmclMjBiYXRjaCUyMG51bWJlciUzRDElMjAqKiolMEFhYnMlMjBtaW4lMjAlMjBhYnMlMjBtYXglMjAlMjBtZXRhZGF0YSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHNoYXJlZCUyMEVtYmVkZGluZyUwQTEuMDFlLTA2JTIwNy45MmUlMkIwMiUyMHdlaWdodCUwQTAuMDBlJTJCMDAlMjAyLjQ3ZSUyQjA0JTIwaW5wdXQlNUIwJTVEJTBBNS4zNmUtMDUlMjA3LjkyZSUyQjAyJTIwb3V0cHV0JTBBJTVCLi4uJTVEJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwZGVjb2Rlci5kcm9wb3V0JTIwRHJvcG91dCUwQTEuNjBlLTA3JTIwMi4yN2UlMkIwMSUyMGlucHV0JTVCMCU1RCUwQTAuMDBlJTJCMDAlMjAyLjUyZSUyQjAxJTIwb3V0cHV0JTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwZGVjb2RlciUyMFQ1U3RhY2slMEElMjAlMjAlMjAlMjAlMjBub3QlMjBhJTIwdGVuc29yJTIwb3V0cHV0JTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbG1faGVhZCUyMExpbmVhciUwQTEuMDFlLTA2JTIwNy45MmUlMkIwMiUyMHdlaWdodCUwQTAuMDBlJTJCMDAlMjAxLjExZSUyQjAwJTIwaW5wdXQlNUIwJTVEJTBBNi4wNmUtMDIlMjA4LjM5ZSUyQjAxJTIwb3V0cHV0JTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwVDVGb3JDb25kaXRpb25hbEdlbmVyYXRpb24lMEElMjAlMjAlMjAlMjAlMjBub3QlMjBhJTIwdGVuc29yJTIwb3V0cHV0JTBBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwKioqJTIwU3RhcnRpbmclMjBiYXRjaCUyMG51bWJlciUzRDMlMjAqKiolMEFhYnMlMjBtaW4lMjAlMjBhYnMlMjBtYXglMjAlMjBtZXRhZGF0YSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHNoYXJlZCUyMEVtYmVkZGluZyUwQTEuMDFlLTA2JTIwNy45MmUlMkIwMiUyMHdlaWdodCUwQTAuMDBlJTJCMDAlMjAyLjc4ZSUyQjA0JTIwaW5wdXQlNUIwJTVEJTBBNS4zNmUtMDUlMjA3LjkyZSUyQjAyJTIwb3V0cHV0JTBBJTVCLi4uJTVE",highlighted:`                  *** Starting batch number=1 ***
abs min  abs max  metadata
                  shared Embedding
1.01e<span class="hljs-string">-06</span> 7.92e<span class="hljs-string">+02</span> weight
0.00e<span class="hljs-string">+00</span> 2.47e<span class="hljs-string">+04</span> input[0]
5.36e<span class="hljs-string">-05</span> 7.92e<span class="hljs-string">+02</span> output
[...]
                  decoder.dropout Dropout
1.60e<span class="hljs-string">-07</span> 2.27e<span class="hljs-string">+01</span> input[0]
0.00e<span class="hljs-string">+00</span> 2.52e<span class="hljs-string">+01</span> output
                  decoder T5Stack
     not a tensor output
                  lm_head Linear
1.01e<span class="hljs-string">-06</span> 7.92e<span class="hljs-string">+02</span> weight
0.00e<span class="hljs-string">+00</span> 1.11e<span class="hljs-string">+00</span> input[0]
6.06e<span class="hljs-string">-02</span> 8.39e<span class="hljs-string">+01</span> output
                   T5ForConditionalGeneration
     not a tensor output

                  *** Starting batch number=3 ***
abs min  abs max  metadata
                  shared Embedding
1.01e<span class="hljs-string">-06</span> 7.92e<span class="hljs-string">+02</span> weight
0.00e<span class="hljs-string">+00</span> 2.78e<span class="hljs-string">+04</span> input[0]
5.36e<span class="hljs-string">-05</span> 7.92e<span class="hljs-string">+02</span> output
[...]`,wrap:!1}}),cs=new J({props:{code:"ZGVidWdfb3ZlcmZsb3clMjAlM0QlMjBEZWJ1Z1VuZGVyZmxvd092ZXJmbG93KG1vZGVsJTJDJTIwdHJhY2VfYmF0Y2hfbnVtcyUzRCU1QjElMkMlMjAzJTVEJTJDJTIwYWJvcnRfYWZ0ZXJfYmF0Y2hfbnVtJTNEMyk=",highlighted:'debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>], abort_after_batch_num=<span class="hljs-number">3</span>)',wrap:!1}}),{c(){p=i("meta"),T=n(),o=i("p"),b=n(),c(j.$$.fragment),y=n(),I=i("p"),I.textContent=$,f=n(),c(C.$$.fragment),ws=n(),V=i("p"),V.textContent=st,ds=n(),c(R.$$.fragment),js=n(),B=i("p"),B.innerHTML=lt,Ts=n(),c(A.$$.fragment),bs=n(),c(L.$$.fragment),Js=n(),x=i("p"),x.innerHTML=tt,fs=n(),N=i("p"),N.innerHTML=nt,ys=n(),c(W.$$.fragment),Us=n(),c(D.$$.fragment),Is=n(),E=i("p"),E.textContent=at,Cs=n(),c(H.$$.fragment),$s=n(),X=i("p"),X.innerHTML=pt,Zs=n(),Q=i("p"),Q.textContent=it,As=n(),c(S.$$.fragment),vs=n(),Y=i("p"),Y.innerHTML=rt,Gs=n(),c(z.$$.fragment),gs=n(),F=i("p"),F.innerHTML=ut,_s=n(),c(P.$$.fragment),ks=n(),q=i("p"),q.innerHTML=ot,Vs=n(),K=i("p"),K.textContent=ct,Rs=n(),c(O.$$.fragment),Bs=n(),c(ee.$$.fragment),Ls=n(),se=i("p"),se.textContent=mt,xs=n(),c(le.$$.fragment),Ns=n(),c(v.$$.fragment),Ws=n(),te=i("p"),te.innerHTML=Mt,Ds=n(),c(ne.$$.fragment),Es=n(),ae=i("p"),ae.textContent=ht,Hs=n(),c(G.$$.fragment),Xs=n(),pe=i("p"),pe.innerHTML=wt,Qs=n(),ie=i("p"),ie.innerHTML=dt,Ss=n(),re=i("p"),re.textContent=jt,Ys=n(),c(ue.$$.fragment),zs=n(),oe=i("p"),oe.innerHTML=Tt,Fs=n(),c(ce.$$.fragment),Ps=n(),c(me.$$.fragment),qs=n(),Me=i("p"),Me.innerHTML=bt,Ks=n(),c(he.$$.fragment),Os=n(),we=i("p"),we.textContent=Jt,el=n(),c(de.$$.fragment),sl=n(),je=i("p"),je.textContent=ft,ll=n(),Te=i("p"),Te.textContent=yt,tl=n(),be=i("p"),be.textContent=Ut,nl=n(),Je=i("p"),Je.innerHTML=It,al=n(),c(fe.$$.fragment),pl=n(),ye=i("p"),ye.textContent=Ct,il=n(),c(Ue.$$.fragment),rl=n(),c(g.$$.fragment),ul=n(),c(_.$$.fragment),ol=n(),c(k.$$.fragment),cl=n(),Ie=i("p"),Ie.innerHTML=$t,ml=n(),Ce=i("p"),Ce.innerHTML=Zt,Ml=n(),c($e.$$.fragment),hl=n(),Ze=i("p"),Ze.innerHTML=At,wl=n(),Ae=i("p"),Ae.textContent=vt,dl=n(),c(ve.$$.fragment),jl=n(),Ge=i("p"),Ge.innerHTML=Gt,Tl=n(),c(ge.$$.fragment),bl=n(),_e=i("p"),_e.textContent=gt,Jl=n(),ke=i("p"),ke.innerHTML=_t,fl=n(),Ve=i("p"),Ve.innerHTML=kt,yl=n(),Re=i("p"),Re.textContent=Vt,Ul=n(),c(Be.$$.fragment),Il=n(),Le=i("p"),Le.innerHTML=Rt,Cl=n(),xe=i("p"),xe.textContent=Bt,$l=n(),c(Ne.$$.fragment),Zl=n(),We=i("p"),We.innerHTML=Lt,Al=n(),De=i("p"),De.innerHTML=xt,vl=n(),Ee=i("p"),Ee.textContent=Nt,Gl=n(),He=i("p"),He.innerHTML=Wt,gl=n(),c(Xe.$$.fragment),_l=n(),Qe=i("p"),Qe.innerHTML=Dt,kl=n(),Se=i("p"),Se.innerHTML=Et,Vl=n(),Ye=i("p"),Ye.innerHTML=Ht,Rl=n(),c(ze.$$.fragment),Bl=n(),Fe=i("p"),Fe.innerHTML=Xt,Ll=n(),c(Pe.$$.fragment),xl=n(),qe=i("p"),qe.innerHTML=Qt,Nl=n(),Ke=i("p"),Ke.innerHTML=St,Wl=n(),Oe=i("p"),Oe.textContent=Yt,Dl=n(),c(es.$$.fragment),El=n(),c(ss.$$.fragment),Hl=n(),ls=i("p"),ls.textContent=zt,Xl=n(),ts=i("p"),ts.innerHTML=Ft,Ql=n(),c(ns.$$.fragment),Sl=n(),as=i("p"),as.textContent=Pt,Yl=n(),ps=i("p"),ps.textContent=qt,zl=n(),is=i("p"),is.textContent=Kt,Fl=n(),c(rs.$$.fragment),Pl=n(),us=i("p"),us.textContent=Ot,ql=n(),os=i("p"),os.textContent=en,Kl=n(),c(cs.$$.fragment),Ol=n(),Ms=i("p"),this.h()},l(e){const s=wn("svelte-u9bgzb",document.head);p=r(s,"META",{name:!0,content:!0}),s.forEach(l),T=a(e),o=r(e,"P",{}),rn(o).forEach(l),b=a(e),m(j.$$.fragment,e),y=a(e),I=r(e,"P",{"data-svelte-h":!0}),u(I)!=="svelte-1kpvh39"&&(I.textContent=$),f=a(e),m(C.$$.fragment,e),ws=a(e),V=r(e,"P",{"data-svelte-h":!0}),u(V)!=="svelte-nt383r"&&(V.textContent=st),ds=a(e),m(R.$$.fragment,e),js=a(e),B=r(e,"P",{"data-svelte-h":!0}),u(B)!=="svelte-1vwyjx9"&&(B.innerHTML=lt),Ts=a(e),m(A.$$.fragment,e),bs=a(e),m(L.$$.fragment,e),Js=a(e),x=r(e,"P",{"data-svelte-h":!0}),u(x)!=="svelte-15jm6uz"&&(x.innerHTML=tt),fs=a(e),N=r(e,"P",{"data-svelte-h":!0}),u(N)!=="svelte-18fwpg9"&&(N.innerHTML=nt),ys=a(e),m(W.$$.fragment,e),Us=a(e),m(D.$$.fragment,e),Is=a(e),E=r(e,"P",{"data-svelte-h":!0}),u(E)!=="svelte-w2symr"&&(E.textContent=at),Cs=a(e),m(H.$$.fragment,e),$s=a(e),X=r(e,"P",{"data-svelte-h":!0}),u(X)!=="svelte-541uqc"&&(X.innerHTML=pt),Zs=a(e),Q=r(e,"P",{"data-svelte-h":!0}),u(Q)!=="svelte-1at4c41"&&(Q.textContent=it),As=a(e),m(S.$$.fragment,e),vs=a(e),Y=r(e,"P",{"data-svelte-h":!0}),u(Y)!=="svelte-jt61zj"&&(Y.innerHTML=rt),Gs=a(e),m(z.$$.fragment,e),gs=a(e),F=r(e,"P",{"data-svelte-h":!0}),u(F)!=="svelte-1acw487"&&(F.innerHTML=ut),_s=a(e),m(P.$$.fragment,e),ks=a(e),q=r(e,"P",{"data-svelte-h":!0}),u(q)!=="svelte-1alz1p9"&&(q.innerHTML=ot),Vs=a(e),K=r(e,"P",{"data-svelte-h":!0}),u(K)!=="svelte-revmac"&&(K.textContent=ct),Rs=a(e),m(O.$$.fragment,e),Bs=a(e),m(ee.$$.fragment,e),Ls=a(e),se=r(e,"P",{"data-svelte-h":!0}),u(se)!=="svelte-1fb5vyg"&&(se.textContent=mt),xs=a(e),m(le.$$.fragment,e),Ns=a(e),m(v.$$.fragment,e),Ws=a(e),te=r(e,"P",{"data-svelte-h":!0}),u(te)!=="svelte-d0tat4"&&(te.innerHTML=Mt),Ds=a(e),m(ne.$$.fragment,e),Es=a(e),ae=r(e,"P",{"data-svelte-h":!0}),u(ae)!=="svelte-1lnm9wo"&&(ae.textContent=ht),Hs=a(e),m(G.$$.fragment,e),Xs=a(e),pe=r(e,"P",{"data-svelte-h":!0}),u(pe)!=="svelte-4ju8gr"&&(pe.innerHTML=wt),Qs=a(e),ie=r(e,"P",{"data-svelte-h":!0}),u(ie)!=="svelte-14w6u6w"&&(ie.innerHTML=dt),Ss=a(e),re=r(e,"P",{"data-svelte-h":!0}),u(re)!=="svelte-wba9x8"&&(re.textContent=jt),Ys=a(e),m(ue.$$.fragment,e),zs=a(e),oe=r(e,"P",{"data-svelte-h":!0}),u(oe)!=="svelte-jhikc"&&(oe.innerHTML=Tt),Fs=a(e),m(ce.$$.fragment,e),Ps=a(e),m(me.$$.fragment,e),qs=a(e),Me=r(e,"P",{"data-svelte-h":!0}),u(Me)!=="svelte-9omdti"&&(Me.innerHTML=bt),Ks=a(e),m(he.$$.fragment,e),Os=a(e),we=r(e,"P",{"data-svelte-h":!0}),u(we)!=="svelte-1l3iq1o"&&(we.textContent=Jt),el=a(e),m(de.$$.fragment,e),sl=a(e),je=r(e,"P",{"data-svelte-h":!0}),u(je)!=="svelte-1mfzxxn"&&(je.textContent=ft),ll=a(e),Te=r(e,"P",{"data-svelte-h":!0}),u(Te)!=="svelte-p47u8u"&&(Te.textContent=yt),tl=a(e),be=r(e,"P",{"data-svelte-h":!0}),u(be)!=="svelte-r57lzq"&&(be.textContent=Ut),nl=a(e),Je=r(e,"P",{"data-svelte-h":!0}),u(Je)!=="svelte-1px963d"&&(Je.innerHTML=It),al=a(e),m(fe.$$.fragment,e),pl=a(e),ye=r(e,"P",{"data-svelte-h":!0}),u(ye)!=="svelte-179ydg6"&&(ye.textContent=Ct),il=a(e),m(Ue.$$.fragment,e),rl=a(e),m(g.$$.fragment,e),ul=a(e),m(_.$$.fragment,e),ol=a(e),m(k.$$.fragment,e),cl=a(e),Ie=r(e,"P",{"data-svelte-h":!0}),u(Ie)!=="svelte-rc8gwf"&&(Ie.innerHTML=$t),ml=a(e),Ce=r(e,"P",{"data-svelte-h":!0}),u(Ce)!=="svelte-5405il"&&(Ce.innerHTML=Zt),Ml=a(e),m($e.$$.fragment,e),hl=a(e),Ze=r(e,"P",{"data-svelte-h":!0}),u(Ze)!=="svelte-1ej0ejr"&&(Ze.innerHTML=At),wl=a(e),Ae=r(e,"P",{"data-svelte-h":!0}),u(Ae)!=="svelte-1l3b7y5"&&(Ae.textContent=vt),dl=a(e),m(ve.$$.fragment,e),jl=a(e),Ge=r(e,"P",{"data-svelte-h":!0}),u(Ge)!=="svelte-1squex3"&&(Ge.innerHTML=Gt),Tl=a(e),m(ge.$$.fragment,e),bl=a(e),_e=r(e,"P",{"data-svelte-h":!0}),u(_e)!=="svelte-2ii9wn"&&(_e.textContent=gt),Jl=a(e),ke=r(e,"P",{"data-svelte-h":!0}),u(ke)!=="svelte-1si0hkb"&&(ke.innerHTML=_t),fl=a(e),Ve=r(e,"P",{"data-svelte-h":!0}),u(Ve)!=="svelte-yj4cwf"&&(Ve.innerHTML=kt),yl=a(e),Re=r(e,"P",{"data-svelte-h":!0}),u(Re)!=="svelte-1w41g2g"&&(Re.textContent=Vt),Ul=a(e),m(Be.$$.fragment,e),Il=a(e),Le=r(e,"P",{"data-svelte-h":!0}),u(Le)!=="svelte-1tqbqml"&&(Le.innerHTML=Rt),Cl=a(e),xe=r(e,"P",{"data-svelte-h":!0}),u(xe)!=="svelte-wdyrpk"&&(xe.textContent=Bt),$l=a(e),m(Ne.$$.fragment,e),Zl=a(e),We=r(e,"P",{"data-svelte-h":!0}),u(We)!=="svelte-unqbdv"&&(We.innerHTML=Lt),Al=a(e),De=r(e,"P",{"data-svelte-h":!0}),u(De)!=="svelte-yxhf7s"&&(De.innerHTML=xt),vl=a(e),Ee=r(e,"P",{"data-svelte-h":!0}),u(Ee)!=="svelte-qa5qyo"&&(Ee.textContent=Nt),Gl=a(e),He=r(e,"P",{"data-svelte-h":!0}),u(He)!=="svelte-lw0sg0"&&(He.innerHTML=Wt),gl=a(e),m(Xe.$$.fragment,e),_l=a(e),Qe=r(e,"P",{"data-svelte-h":!0}),u(Qe)!=="svelte-19gpk8b"&&(Qe.innerHTML=Dt),kl=a(e),Se=r(e,"P",{"data-svelte-h":!0}),u(Se)!=="svelte-ipsm9a"&&(Se.innerHTML=Et),Vl=a(e),Ye=r(e,"P",{"data-svelte-h":!0}),u(Ye)!=="svelte-1rpvta4"&&(Ye.innerHTML=Ht),Rl=a(e),m(ze.$$.fragment,e),Bl=a(e),Fe=r(e,"P",{"data-svelte-h":!0}),u(Fe)!=="svelte-up3p3a"&&(Fe.innerHTML=Xt),Ll=a(e),m(Pe.$$.fragment,e),xl=a(e),qe=r(e,"P",{"data-svelte-h":!0}),u(qe)!=="svelte-152lw1d"&&(qe.innerHTML=Qt),Nl=a(e),Ke=r(e,"P",{"data-svelte-h":!0}),u(Ke)!=="svelte-iv1rb"&&(Ke.innerHTML=St),Wl=a(e),Oe=r(e,"P",{"data-svelte-h":!0}),u(Oe)!=="svelte-1kb9fq"&&(Oe.textContent=Yt),Dl=a(e),m(es.$$.fragment,e),El=a(e),m(ss.$$.fragment,e),Hl=a(e),ls=r(e,"P",{"data-svelte-h":!0}),u(ls)!=="svelte-mwprcu"&&(ls.textContent=zt),Xl=a(e),ts=r(e,"P",{"data-svelte-h":!0}),u(ts)!=="svelte-medkr5"&&(ts.innerHTML=Ft),Ql=a(e),m(ns.$$.fragment,e),Sl=a(e),as=r(e,"P",{"data-svelte-h":!0}),u(as)!=="svelte-zj2sw0"&&(as.textContent=Pt),Yl=a(e),ps=r(e,"P",{"data-svelte-h":!0}),u(ps)!=="svelte-ux7vsk"&&(ps.textContent=qt),zl=a(e),is=r(e,"P",{"data-svelte-h":!0}),u(is)!=="svelte-v84jpj"&&(is.textContent=Kt),Fl=a(e),m(rs.$$.fragment,e),Pl=a(e),us=r(e,"P",{"data-svelte-h":!0}),u(us)!=="svelte-124ox4j"&&(us.textContent=Ot),ql=a(e),os=r(e,"P",{"data-svelte-h":!0}),u(os)!=="svelte-vocb5m"&&(os.textContent=en),Kl=a(e),m(cs.$$.fragment,e),Ol=a(e),Ms=r(e,"P",{}),rn(Ms).forEach(l),this.h()},h(){un(p,"name","hf:doc:metadata"),un(p,"content",Zn)},m(e,s){dn(document.head,p),t(e,T,s),t(e,o,s),t(e,b,s),M(j,e,s),t(e,y,s),t(e,I,s),t(e,f,s),M(C,e,s),t(e,ws,s),t(e,V,s),t(e,ds,s),M(R,e,s),t(e,js,s),t(e,B,s),t(e,Ts,s),M(A,e,s),t(e,bs,s),M(L,e,s),t(e,Js,s),t(e,x,s),t(e,fs,s),t(e,N,s),t(e,ys,s),M(W,e,s),t(e,Us,s),M(D,e,s),t(e,Is,s),t(e,E,s),t(e,Cs,s),M(H,e,s),t(e,$s,s),t(e,X,s),t(e,Zs,s),t(e,Q,s),t(e,As,s),M(S,e,s),t(e,vs,s),t(e,Y,s),t(e,Gs,s),M(z,e,s),t(e,gs,s),t(e,F,s),t(e,_s,s),M(P,e,s),t(e,ks,s),t(e,q,s),t(e,Vs,s),t(e,K,s),t(e,Rs,s),M(O,e,s),t(e,Bs,s),M(ee,e,s),t(e,Ls,s),t(e,se,s),t(e,xs,s),M(le,e,s),t(e,Ns,s),M(v,e,s),t(e,Ws,s),t(e,te,s),t(e,Ds,s),M(ne,e,s),t(e,Es,s),t(e,ae,s),t(e,Hs,s),M(G,e,s),t(e,Xs,s),t(e,pe,s),t(e,Qs,s),t(e,ie,s),t(e,Ss,s),t(e,re,s),t(e,Ys,s),M(ue,e,s),t(e,zs,s),t(e,oe,s),t(e,Fs,s),M(ce,e,s),t(e,Ps,s),M(me,e,s),t(e,qs,s),t(e,Me,s),t(e,Ks,s),M(he,e,s),t(e,Os,s),t(e,we,s),t(e,el,s),M(de,e,s),t(e,sl,s),t(e,je,s),t(e,ll,s),t(e,Te,s),t(e,tl,s),t(e,be,s),t(e,nl,s),t(e,Je,s),t(e,al,s),M(fe,e,s),t(e,pl,s),t(e,ye,s),t(e,il,s),M(Ue,e,s),t(e,rl,s),M(g,e,s),t(e,ul,s),M(_,e,s),t(e,ol,s),M(k,e,s),t(e,cl,s),t(e,Ie,s),t(e,ml,s),t(e,Ce,s),t(e,Ml,s),M($e,e,s),t(e,hl,s),t(e,Ze,s),t(e,wl,s),t(e,Ae,s),t(e,dl,s),M(ve,e,s),t(e,jl,s),t(e,Ge,s),t(e,Tl,s),M(ge,e,s),t(e,bl,s),t(e,_e,s),t(e,Jl,s),t(e,ke,s),t(e,fl,s),t(e,Ve,s),t(e,yl,s),t(e,Re,s),t(e,Ul,s),M(Be,e,s),t(e,Il,s),t(e,Le,s),t(e,Cl,s),t(e,xe,s),t(e,$l,s),M(Ne,e,s),t(e,Zl,s),t(e,We,s),t(e,Al,s),t(e,De,s),t(e,vl,s),t(e,Ee,s),t(e,Gl,s),t(e,He,s),t(e,gl,s),M(Xe,e,s),t(e,_l,s),t(e,Qe,s),t(e,kl,s),t(e,Se,s),t(e,Vl,s),t(e,Ye,s),t(e,Rl,s),M(ze,e,s),t(e,Bl,s),t(e,Fe,s),t(e,Ll,s),M(Pe,e,s),t(e,xl,s),t(e,qe,s),t(e,Nl,s),t(e,Ke,s),t(e,Wl,s),t(e,Oe,s),t(e,Dl,s),M(es,e,s),t(e,El,s),M(ss,e,s),t(e,Hl,s),t(e,ls,s),t(e,Xl,s),t(e,ts,s),t(e,Ql,s),M(ns,e,s),t(e,Sl,s),t(e,as,s),t(e,Yl,s),t(e,ps,s),t(e,zl,s),t(e,is,s),t(e,Fl,s),M(rs,e,s),t(e,Pl,s),t(e,us,s),t(e,ql,s),t(e,os,s),t(e,Kl,s),M(cs,e,s),t(e,Ol,s),t(e,Ms,s),et=!0},p(e,[s]){const sn={};s&2&&(sn.$$scope={dirty:s,ctx:e}),A.$set(sn);const ln={};s&2&&(ln.$$scope={dirty:s,ctx:e}),v.$set(ln);const tn={};s&2&&(tn.$$scope={dirty:s,ctx:e}),G.$set(tn);const nn={};s&2&&(nn.$$scope={dirty:s,ctx:e}),g.$set(nn);const an={};s&2&&(an.$$scope={dirty:s,ctx:e}),_.$set(an);const pn={};s&2&&(pn.$$scope={dirty:s,ctx:e}),k.$set(pn)},i(e){et||(h(j.$$.fragment,e),h(C.$$.fragment,e),h(R.$$.fragment,e),h(A.$$.fragment,e),h(L.$$.fragment,e),h(W.$$.fragment,e),h(D.$$.fragment,e),h(H.$$.fragment,e),h(S.$$.fragment,e),h(z.$$.fragment,e),h(P.$$.fragment,e),h(O.$$.fragment,e),h(ee.$$.fragment,e),h(le.$$.fragment,e),h(v.$$.fragment,e),h(ne.$$.fragment,e),h(G.$$.fragment,e),h(ue.$$.fragment,e),h(ce.$$.fragment,e),h(me.$$.fragment,e),h(he.$$.fragment,e),h(de.$$.fragment,e),h(fe.$$.fragment,e),h(Ue.$$.fragment,e),h(g.$$.fragment,e),h(_.$$.fragment,e),h(k.$$.fragment,e),h($e.$$.fragment,e),h(ve.$$.fragment,e),h(ge.$$.fragment,e),h(Be.$$.fragment,e),h(Ne.$$.fragment,e),h(Xe.$$.fragment,e),h(ze.$$.fragment,e),h(Pe.$$.fragment,e),h(es.$$.fragment,e),h(ss.$$.fragment,e),h(ns.$$.fragment,e),h(rs.$$.fragment,e),h(cs.$$.fragment,e),et=!0)},o(e){w(j.$$.fragment,e),w(C.$$.fragment,e),w(R.$$.fragment,e),w(A.$$.fragment,e),w(L.$$.fragment,e),w(W.$$.fragment,e),w(D.$$.fragment,e),w(H.$$.fragment,e),w(S.$$.fragment,e),w(z.$$.fragment,e),w(P.$$.fragment,e),w(O.$$.fragment,e),w(ee.$$.fragment,e),w(le.$$.fragment,e),w(v.$$.fragment,e),w(ne.$$.fragment,e),w(G.$$.fragment,e),w(ue.$$.fragment,e),w(ce.$$.fragment,e),w(me.$$.fragment,e),w(he.$$.fragment,e),w(de.$$.fragment,e),w(fe.$$.fragment,e),w(Ue.$$.fragment,e),w(g.$$.fragment,e),w(_.$$.fragment,e),w(k.$$.fragment,e),w($e.$$.fragment,e),w(ve.$$.fragment,e),w(ge.$$.fragment,e),w(Be.$$.fragment,e),w(Ne.$$.fragment,e),w(Xe.$$.fragment,e),w(ze.$$.fragment,e),w(Pe.$$.fragment,e),w(es.$$.fragment,e),w(ss.$$.fragment,e),w(ns.$$.fragment,e),w(rs.$$.fragment,e),w(cs.$$.fragment,e),et=!1},d(e){e&&(l(T),l(o),l(b),l(y),l(I),l(f),l(ws),l(V),l(ds),l(js),l(B),l(Ts),l(bs),l(Js),l(x),l(fs),l(N),l(ys),l(Us),l(Is),l(E),l(Cs),l($s),l(X),l(Zs),l(Q),l(As),l(vs),l(Y),l(Gs),l(gs),l(F),l(_s),l(ks),l(q),l(Vs),l(K),l(Rs),l(Bs),l(Ls),l(se),l(xs),l(Ns),l(Ws),l(te),l(Ds),l(Es),l(ae),l(Hs),l(Xs),l(pe),l(Qs),l(ie),l(Ss),l(re),l(Ys),l(zs),l(oe),l(Fs),l(Ps),l(qs),l(Me),l(Ks),l(Os),l(we),l(el),l(sl),l(je),l(ll),l(Te),l(tl),l(be),l(nl),l(Je),l(al),l(pl),l(ye),l(il),l(rl),l(ul),l(ol),l(cl),l(Ie),l(ml),l(Ce),l(Ml),l(hl),l(Ze),l(wl),l(Ae),l(dl),l(jl),l(Ge),l(Tl),l(bl),l(_e),l(Jl),l(ke),l(fl),l(Ve),l(yl),l(Re),l(Ul),l(Il),l(Le),l(Cl),l(xe),l($l),l(Zl),l(We),l(Al),l(De),l(vl),l(Ee),l(Gl),l(He),l(gl),l(_l),l(Qe),l(kl),l(Se),l(Vl),l(Ye),l(Rl),l(Bl),l(Fe),l(Ll),l(xl),l(qe),l(Nl),l(Ke),l(Wl),l(Oe),l(Dl),l(El),l(Hl),l(ls),l(Xl),l(ts),l(Ql),l(Sl),l(as),l(Yl),l(ps),l(zl),l(is),l(Fl),l(Pl),l(us),l(ql),l(os),l(Kl),l(Ol),l(Ms)),l(p),d(j,e),d(C,e),d(R,e),d(A,e),d(L,e),d(W,e),d(D,e),d(H,e),d(S,e),d(z,e),d(P,e),d(O,e),d(ee,e),d(le,e),d(v,e),d(ne,e),d(G,e),d(ue,e),d(ce,e),d(me,e),d(he,e),d(de,e),d(fe,e),d(Ue,e),d(g,e),d(_,e),d(k,e),d($e,e),d(ve,e),d(ge,e),d(Be,e),d(Ne,e),d(Xe,e),d(ze,e),d(Pe,e),d(es,e),d(ss,e),d(ns,e),d(rs,e),d(cs,e)}}}const Zn='{"title":"Debugging","local":"debugging","sections":[{"title":"DeepSpeed CUDA installation","local":"deepspeed-cuda-installation","sections":[{"title":"Non-identical CUDA toolkits","local":"non-identical-cuda-toolkits","sections":[],"depth":3},{"title":"Multiple CUDA toolkits","local":"multiple-cuda-toolkits","sections":[],"depth":3},{"title":"Older CUDA versions","local":"older-cuda-versions","sections":[],"depth":3},{"title":"Prebuild","local":"prebuild","sections":[],"depth":3}],"depth":2},{"title":"Multi-GPU Network Issues Debug","local":"multi-gpu-network-issues-debug","sections":[],"depth":2},{"title":"Underflow and Overflow Detection","local":"underflow-and-overflow-detection","sections":[{"title":"Specific batch absolute min and max value tracing","local":"specific-batch-absolute-min-and-max-value-tracing","sections":[],"depth":3}],"depth":2}],"depth":1}';function An(U){return mn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Rn extends Mn{constructor(p){super(),hn(this,p,An,$n,cn,{})}}export{Rn as component};
