import{s as Ae,o as Fe,n as Qe}from"../chunks/scheduler.36a0863c.js";import{S as Ne,i as Ye,g as n,s as i,r as m,A as Ee,h as o,f as s,c as l,j as Pe,u as d,x as r,k as Ve,y as Ke,a,v as c,d as u,t as f,w as h}from"../chunks/index.9c13489a.js";import{T as De}from"../chunks/Tip.3b06990e.js";import{C as w}from"../chunks/CodeBlock.05d8ec32.js";import{H as Ce}from"../chunks/Heading.7a254a62.js";function Oe(Y){let p,M="Nota che il modello creato casualmente è inizializzato con tensori “vuoti”, che occupano spazio in memoria ma senza riempirlo (quindi i valori casuali sono quelli che si trovavano in questa porzione di memoria in un determinato momento). L’inizializzazione casuale che segue la distribuzione appropriata per il tipo di modello/parametri istanziato (come la distribuzione normale per le istanze) è eseguito solo dopo il passaggio 3 sui pesi non inizializzati, per essere più rapido possibile!";return{c(){p=n("p"),p.textContent=M},l(y){p=o(y,"P",{"data-svelte-h":!0}),r(p)!=="svelte-exxgkr"&&(p.textContent=M)},m(y,Q){a(y,p,Q)},p:Qe,d(y){y&&s(p)}}}function et(Y){let p,M,y,Q,j,E,$,xe=`Quando vuoi utilizzare un modello preaddestrato (pretrained) molto grande, una sfida è minimizzare l’uso della RAM. Il workflow classico
in PyTorch è:`,K,v,Ue="<li>Crea il tuo modello con pesi casuali (random weights).</li> <li>Carica i tuoi pesi preaddestrati.</li> <li>Inserisci i pesi preaddestrati nel tuo modello casuale.</li>",D,_,ke="I passi 1 e 2 una versione completa del modello in memoria, in molti casi non è un problema, ma se il modello inizia a pesare diversi GigaBytes, queste due copie possono sturare la nostra RAM. Ancora peggio, se stai usando <code>torch.distributed</code> per seguire l’addestramento (training) in distribuito, ogni processo caricherà il modello preaddestrato e memorizzerà queste due copie nella RAM.",O,g,ee,b,ze="In questa guida, esploreremo le soluzioni che Transformers offre per affrontare questo problema. C’è da tenere in conto che questa è un’area in cui si sta attualmente sviluppando, quindi le API spiegate qui possono variare velocemente in futuro.",te,C,se,x,Te="Dalla versione 4.18.0, i checkpoints dei modelli che occupano più di 10GB di spazio vengono automaticamente frammentati in più parti. Per quanto riguarda la possibilità di avere un unico checkpoint quando si utilizza <code>model.save_pretrained(save_dir)</code>, si hanno diversi checkpoint parziali (ognuno con dimensione &lt; 10GB) e un  indice che mappa i nomi dei parametri ai file in cui sono memorizzati.",ae,U,Je="Puoi controllare la dimensione massima dopo la frammentazione con il parametro <code>max_shard_size</code>, nel prossimo esempio, useremo modelli di dimensioni normali con frammenti di piccoli dimensioni: prendiamo un modello BERT classico.",ie,k,le,z,Ze="Se tu salvi usando <code>save_pretrained()</code>, avrai una nuova cartella con due file: il config del modello e i suoi pesi:",ne,T,oe,J,Be="Adesso usiamo una dimensione massima di frammentazione di 200MB:",pe,Z,re,B,Re="In aggiunta alla configurazione del modello, vediamo tre differenti file dei pesi, e un file <code>index.json</code> che è il nostro indice. Un checkpoint può essere ricaricato totalmente usando il metodo <code>from_pretrained()</code>:",me,R,de,G,Ge="Il vantaggio principale di applicare questo metodo per modelli grandi è che durante il passo 2 del workflow illustrato in precedenza, ogni frammento del checkpoint viene caricato dopo il precedente, limitando l’utilizzo della RAM alla dimensione del modello più la dimensione del frammento più grande.",ce,I,Ie="Dietro le quinte, il file indice è utilizzato per determinare quali chiavi sono nel checkpoint, e dove i corrispondenti pesi sono memorizzati. Possiamo caricare l’indice come un qualsiasi json e ottenere un dizionario:",ue,X,fe,H,Xe="I metadati consistono solo nella dimensione totale del modello per ora. Abbiamo in programma di aggiungere altre informazioni in futuro:",he,W,ye,q,He="La mappa dei pesi è la parte principale di questo indice, che mappa ogni nome dei parametri (si trova solitamente nei modelli PyTorch come <code>state_dict</code>) al file in cui è memorizzato:",ge,S,we,L,We="Se vuoi caricare direttamente un checkpoint frammentato in un modello senza usare <code>from_pretrained()</code> (come si farebbe con <code>model.load_state_dict()</code> per un checkpoint completo) devi usare <code>load_sharded_checkpoint()</code>:",Me,P,je,V,$e,A,qe="Frammentare i checkpoint l’utilizzo di memoria al passo 2 del workflow citato in precedenza, ma per utilizzare questo modello in un ambiente con poca memoria, consigliamo di utilizzare i nostri strumenti basati sulla libreria Accelerate.",ve,F,Se='Per ulteriori informazioni, leggere la seguente guida: <a href="./main_classes/model#large-model-loading">Large model loading using Accelerate</a>',_e,N,be;return j=new Ce({props:{title:"Istanziare un big model",local:"istanziare-un-big-model",headingTag:"h1"}}),g=new De({props:{$$slots:{default:[Oe]},$$scope:{ctx:Y}}}),C=new Ce({props:{title:"Checkpoints condivisi",local:"checkpoints-condivisi",headingTag:"h2"}}),k=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

model = AutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)`,wrap:!1}}),T=new w({props:{code:"aW1wb3J0JTIwb3MlMEFpbXBvcnQlMjB0ZW1wZmlsZSUwQSUwQXdpdGglMjB0ZW1wZmlsZS5UZW1wb3JhcnlEaXJlY3RvcnkoKSUyMGFzJTIwdG1wX2RpciUzQSUwQSUyMCUyMCUyMCUyMG1vZGVsLnNhdmVfcHJldHJhaW5lZCh0bXBfZGlyKSUwQSUyMCUyMCUyMCUyMHByaW50KHNvcnRlZChvcy5saXN0ZGlyKHRtcF9kaXIpKSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> os
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tempfile

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tempfile.TemporaryDirectory() <span class="hljs-keyword">as</span> tmp_dir:
<span class="hljs-meta">... </span>    model.save_pretrained(tmp_dir)
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">sorted</span>(os.listdir(tmp_dir)))
[<span class="hljs-string">&#x27;config.json&#x27;</span>, <span class="hljs-string">&#x27;pytorch_model.bin&#x27;</span>]`,wrap:!1}}),Z=new w({props:{code:"d2l0aCUyMHRlbXBmaWxlLlRlbXBvcmFyeURpcmVjdG9yeSgpJTIwYXMlMjB0bXBfZGlyJTNBJTBBJTIwJTIwJTIwJTIwbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKHRtcF9kaXIlMkMlMjBtYXhfc2hhcmRfc2l6ZSUzRCUyMjIwME1CJTIyKSUwQSUyMCUyMCUyMCUyMHByaW50KHNvcnRlZChvcy5saXN0ZGlyKHRtcF9kaXIpKSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tempfile.TemporaryDirectory() <span class="hljs-keyword">as</span> tmp_dir:
<span class="hljs-meta">... </span>    model.save_pretrained(tmp_dir, max_shard_size=<span class="hljs-string">&quot;200MB&quot;</span>)
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-built_in">sorted</span>(os.listdir(tmp_dir)))
[<span class="hljs-string">&#x27;config.json&#x27;</span>, <span class="hljs-string">&#x27;pytorch_model-00001-of-00003.bin&#x27;</span>, <span class="hljs-string">&#x27;pytorch_model-00002-of-00003.bin&#x27;</span>, <span class="hljs-string">&#x27;pytorch_model-00003-of-00003.bin&#x27;</span>, <span class="hljs-string">&#x27;pytorch_model.bin.index.json&#x27;</span>]`,wrap:!1}}),R=new w({props:{code:"d2l0aCUyMHRlbXBmaWxlLlRlbXBvcmFyeURpcmVjdG9yeSgpJTIwYXMlMjB0bXBfZGlyJTNBJTBBJTIwJTIwJTIwJTIwbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKHRtcF9kaXIlMkMlMjBtYXhfc2hhcmRfc2l6ZSUzRCUyMjIwME1CJTIyKSUwQSUyMCUyMCUyMCUyMG5ld19tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQodG1wX2Rpcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tempfile.TemporaryDirectory() <span class="hljs-keyword">as</span> tmp_dir:
<span class="hljs-meta">... </span>    model.save_pretrained(tmp_dir, max_shard_size=<span class="hljs-string">&quot;200MB&quot;</span>)
<span class="hljs-meta">... </span>    new_model = AutoModel.from_pretrained(tmp_dir)`,wrap:!1}}),X=new w({props:{code:"aW1wb3J0JTIwanNvbiUwQSUwQXdpdGglMjB0ZW1wZmlsZS5UZW1wb3JhcnlEaXJlY3RvcnkoKSUyMGFzJTIwdG1wX2RpciUzQSUwQSUyMCUyMCUyMCUyMG1vZGVsLnNhdmVfcHJldHJhaW5lZCh0bXBfZGlyJTJDJTIwbWF4X3NoYXJkX3NpemUlM0QlMjIyMDBNQiUyMiklMEElMjAlMjAlMjAlMjB3aXRoJTIwb3Blbihvcy5wYXRoLmpvaW4odG1wX2RpciUyQyUyMCUyMnB5dG9yY2hfbW9kZWwuYmluLmluZGV4Lmpzb24lMjIpJTJDJTIwJTIyciUyMiklMjBhcyUyMGYlM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBpbmRleCUyMCUzRCUyMGpzb24ubG9hZChmKSUwQSUwQXByaW50KGluZGV4LmtleXMoKSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> json

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tempfile.TemporaryDirectory() <span class="hljs-keyword">as</span> tmp_dir:
<span class="hljs-meta">... </span>    model.save_pretrained(tmp_dir, max_shard_size=<span class="hljs-string">&quot;200MB&quot;</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(os.path.join(tmp_dir, <span class="hljs-string">&quot;pytorch_model.bin.index.json&quot;</span>), <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:
<span class="hljs-meta">... </span>        index = json.load(f)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(index.keys())
dict_keys([<span class="hljs-string">&#x27;metadata&#x27;</span>, <span class="hljs-string">&#x27;weight_map&#x27;</span>])`,wrap:!1}}),W=new w({props:{code:"aW5kZXglNUIlMjJtZXRhZGF0YSUyMiU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>index[<span class="hljs-string">&quot;metadata&quot;</span>]
{<span class="hljs-string">&#x27;total_size&#x27;</span>: <span class="hljs-number">433245184</span>}`,wrap:!1}}),S=new w({props:{code:"aW5kZXglNUIlMjJ3ZWlnaHRfbWFwJTIyJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>index[<span class="hljs-string">&quot;weight_map&quot;</span>]
{<span class="hljs-string">&#x27;embeddings.LayerNorm.bias&#x27;</span>: <span class="hljs-string">&#x27;pytorch_model-00001-of-00003.bin&#x27;</span>,
 <span class="hljs-string">&#x27;embeddings.LayerNorm.weight&#x27;</span>: <span class="hljs-string">&#x27;pytorch_model-00001-of-00003.bin&#x27;</span>,
 ...`,wrap:!1}}),P=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5tb2RlbGluZ191dGlscyUyMGltcG9ydCUyMGxvYWRfc2hhcmRlZF9jaGVja3BvaW50JTBBJTBBd2l0aCUyMHRlbXBmaWxlLlRlbXBvcmFyeURpcmVjdG9yeSgpJTIwYXMlMjB0bXBfZGlyJTNBJTBBJTIwJTIwJTIwJTIwbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKHRtcF9kaXIlMkMlMjBtYXhfc2hhcmRfc2l6ZSUzRCUyMjIwME1CJTIyKSUwQSUyMCUyMCUyMCUyMGxvYWRfc2hhcmRlZF9jaGVja3BvaW50KG1vZGVsJTJDJTIwdG1wX2Rpcik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.modeling_utils <span class="hljs-keyword">import</span> load_sharded_checkpoint

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> tempfile.TemporaryDirectory() <span class="hljs-keyword">as</span> tmp_dir:
<span class="hljs-meta">... </span>    model.save_pretrained(tmp_dir, max_shard_size=<span class="hljs-string">&quot;200MB&quot;</span>)
<span class="hljs-meta">... </span>    load_sharded_checkpoint(model, tmp_dir)`,wrap:!1}}),V=new Ce({props:{title:"Caricamento low memory",local:"caricamento-low-memory",headingTag:"h2"}}),{c(){p=n("meta"),M=i(),y=n("p"),Q=i(),m(j.$$.fragment),E=i(),$=n("p"),$.textContent=xe,K=i(),v=n("ol"),v.innerHTML=Ue,D=i(),_=n("p"),_.innerHTML=ke,O=i(),m(g.$$.fragment),ee=i(),b=n("p"),b.textContent=ze,te=i(),m(C.$$.fragment),se=i(),x=n("p"),x.innerHTML=Te,ae=i(),U=n("p"),U.innerHTML=Je,ie=i(),m(k.$$.fragment),le=i(),z=n("p"),z.innerHTML=Ze,ne=i(),m(T.$$.fragment),oe=i(),J=n("p"),J.textContent=Be,pe=i(),m(Z.$$.fragment),re=i(),B=n("p"),B.innerHTML=Re,me=i(),m(R.$$.fragment),de=i(),G=n("p"),G.textContent=Ge,ce=i(),I=n("p"),I.textContent=Ie,ue=i(),m(X.$$.fragment),fe=i(),H=n("p"),H.textContent=Xe,he=i(),m(W.$$.fragment),ye=i(),q=n("p"),q.innerHTML=He,ge=i(),m(S.$$.fragment),we=i(),L=n("p"),L.innerHTML=We,Me=i(),m(P.$$.fragment),je=i(),m(V.$$.fragment),$e=i(),A=n("p"),A.textContent=qe,ve=i(),F=n("p"),F.innerHTML=Se,_e=i(),N=n("p"),this.h()},l(e){const t=Ee("svelte-u9bgzb",document.head);p=o(t,"META",{name:!0,content:!0}),t.forEach(s),M=l(e),y=o(e,"P",{}),Pe(y).forEach(s),Q=l(e),d(j.$$.fragment,e),E=l(e),$=o(e,"P",{"data-svelte-h":!0}),r($)!=="svelte-hpka0t"&&($.textContent=xe),K=l(e),v=o(e,"OL",{"data-svelte-h":!0}),r(v)!=="svelte-1up99oz"&&(v.innerHTML=Ue),D=l(e),_=o(e,"P",{"data-svelte-h":!0}),r(_)!=="svelte-28ptu8"&&(_.innerHTML=ke),O=l(e),d(g.$$.fragment,e),ee=l(e),b=o(e,"P",{"data-svelte-h":!0}),r(b)!=="svelte-vvoxkl"&&(b.textContent=ze),te=l(e),d(C.$$.fragment,e),se=l(e),x=o(e,"P",{"data-svelte-h":!0}),r(x)!=="svelte-mg2nmg"&&(x.innerHTML=Te),ae=l(e),U=o(e,"P",{"data-svelte-h":!0}),r(U)!=="svelte-13ehqwn"&&(U.innerHTML=Je),ie=l(e),d(k.$$.fragment,e),le=l(e),z=o(e,"P",{"data-svelte-h":!0}),r(z)!=="svelte-pg1prv"&&(z.innerHTML=Ze),ne=l(e),d(T.$$.fragment,e),oe=l(e),J=o(e,"P",{"data-svelte-h":!0}),r(J)!=="svelte-1gd3i04"&&(J.textContent=Be),pe=l(e),d(Z.$$.fragment,e),re=l(e),B=o(e,"P",{"data-svelte-h":!0}),r(B)!=="svelte-6ew1q4"&&(B.innerHTML=Re),me=l(e),d(R.$$.fragment,e),de=l(e),G=o(e,"P",{"data-svelte-h":!0}),r(G)!=="svelte-5uw3pb"&&(G.textContent=Ge),ce=l(e),I=o(e,"P",{"data-svelte-h":!0}),r(I)!=="svelte-7x1jfm"&&(I.textContent=Ie),ue=l(e),d(X.$$.fragment,e),fe=l(e),H=o(e,"P",{"data-svelte-h":!0}),r(H)!=="svelte-yi9squ"&&(H.textContent=Xe),he=l(e),d(W.$$.fragment,e),ye=l(e),q=o(e,"P",{"data-svelte-h":!0}),r(q)!=="svelte-raulhv"&&(q.innerHTML=He),ge=l(e),d(S.$$.fragment,e),we=l(e),L=o(e,"P",{"data-svelte-h":!0}),r(L)!=="svelte-rh7ba6"&&(L.innerHTML=We),Me=l(e),d(P.$$.fragment,e),je=l(e),d(V.$$.fragment,e),$e=l(e),A=o(e,"P",{"data-svelte-h":!0}),r(A)!=="svelte-1skddyr"&&(A.textContent=qe),ve=l(e),F=o(e,"P",{"data-svelte-h":!0}),r(F)!=="svelte-8rcien"&&(F.innerHTML=Se),_e=l(e),N=o(e,"P",{}),Pe(N).forEach(s),this.h()},h(){Ve(p,"name","hf:doc:metadata"),Ve(p,"content",tt)},m(e,t){Ke(document.head,p),a(e,M,t),a(e,y,t),a(e,Q,t),c(j,e,t),a(e,E,t),a(e,$,t),a(e,K,t),a(e,v,t),a(e,D,t),a(e,_,t),a(e,O,t),c(g,e,t),a(e,ee,t),a(e,b,t),a(e,te,t),c(C,e,t),a(e,se,t),a(e,x,t),a(e,ae,t),a(e,U,t),a(e,ie,t),c(k,e,t),a(e,le,t),a(e,z,t),a(e,ne,t),c(T,e,t),a(e,oe,t),a(e,J,t),a(e,pe,t),c(Z,e,t),a(e,re,t),a(e,B,t),a(e,me,t),c(R,e,t),a(e,de,t),a(e,G,t),a(e,ce,t),a(e,I,t),a(e,ue,t),c(X,e,t),a(e,fe,t),a(e,H,t),a(e,he,t),c(W,e,t),a(e,ye,t),a(e,q,t),a(e,ge,t),c(S,e,t),a(e,we,t),a(e,L,t),a(e,Me,t),c(P,e,t),a(e,je,t),c(V,e,t),a(e,$e,t),a(e,A,t),a(e,ve,t),a(e,F,t),a(e,_e,t),a(e,N,t),be=!0},p(e,[t]){const Le={};t&2&&(Le.$$scope={dirty:t,ctx:e}),g.$set(Le)},i(e){be||(u(j.$$.fragment,e),u(g.$$.fragment,e),u(C.$$.fragment,e),u(k.$$.fragment,e),u(T.$$.fragment,e),u(Z.$$.fragment,e),u(R.$$.fragment,e),u(X.$$.fragment,e),u(W.$$.fragment,e),u(S.$$.fragment,e),u(P.$$.fragment,e),u(V.$$.fragment,e),be=!0)},o(e){f(j.$$.fragment,e),f(g.$$.fragment,e),f(C.$$.fragment,e),f(k.$$.fragment,e),f(T.$$.fragment,e),f(Z.$$.fragment,e),f(R.$$.fragment,e),f(X.$$.fragment,e),f(W.$$.fragment,e),f(S.$$.fragment,e),f(P.$$.fragment,e),f(V.$$.fragment,e),be=!1},d(e){e&&(s(M),s(y),s(Q),s(E),s($),s(K),s(v),s(D),s(_),s(O),s(ee),s(b),s(te),s(se),s(x),s(ae),s(U),s(ie),s(le),s(z),s(ne),s(oe),s(J),s(pe),s(re),s(B),s(me),s(de),s(G),s(ce),s(I),s(ue),s(fe),s(H),s(he),s(ye),s(q),s(ge),s(we),s(L),s(Me),s(je),s($e),s(A),s(ve),s(F),s(_e),s(N)),s(p),h(j,e),h(g,e),h(C,e),h(k,e),h(T,e),h(Z,e),h(R,e),h(X,e),h(W,e),h(S,e),h(P,e),h(V,e)}}}const tt='{"title":"Istanziare un big model","local":"istanziare-un-big-model","sections":[{"title":"Checkpoints condivisi","local":"checkpoints-condivisi","sections":[],"depth":2},{"title":"Caricamento low memory","local":"caricamento-low-memory","sections":[],"depth":2}],"depth":1}';function st(Y){return Fe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class pt extends Ne{constructor(p){super(),Ye(this,p,st,et,Ae,{})}}export{pt as component};
