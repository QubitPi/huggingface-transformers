import{s as Ce,o as Ie,n as we}from"../chunks/scheduler.36a0863c.js";import{S as ye,i as Le,g as r,s as o,r as E,A as je,h as l,f as i,c as a,j as Pe,u as S,x as p,k as be,y as Me,a as n,v as U,d as J,t as X,w as A}from"../chunks/index.9c13489a.js";import{T as He}from"../chunks/Tip.3b06990e.js";import{H as k}from"../chunks/Heading.7a254a62.js";function qe(G){let s,_="per PyTorch >= 1.14.0. JIT-mode potrebe giovare a qualsiasi modello di prediction e evaluaion visto che il dict input è supportato in jit.trace",f,d,c=`per PyTorch < 1.14.0. JIT-mode potrebbe giovare ai modelli il cui ordine dei parametri corrisponde all’ordine delle tuple in ingresso in jit.trace, come i modelli per question-answering.
Nel caso in cui l’ordine dei parametri seguenti non corrisponda all’ordine delle tuple in ingresso in jit.trace, come nei modelli di text-classification, jit.trace fallirà e lo cattureremo con una eccezione al fine di renderlo un fallback. Il logging è usato per notificare gli utenti.`;return{c(){s=r("p"),s.textContent=_,f=o(),d=r("p"),d.textContent=c},l(m){s=l(m,"P",{"data-svelte-h":!0}),p(s)!=="svelte-1ukl5we"&&(s.textContent=_),f=a(m),d=l(m,"P",{"data-svelte-h":!0}),p(d)!=="svelte-1ajwizk"&&(d.textContent=c)},m(m,u){n(m,s,u),n(m,f,u),n(m,d,u)},p:we,d(m){m&&(i(s),i(f),i(d))}}}function Ee(G){let s,_,f,d,c,m,u,pe="Questa guida si concentra sull’inferenza di modelli di grandi dimensioni in modo efficiente sulla CPU.",O,T,N,$,me='Abbiamo integrato di recente <code>BetterTransformer</code> per fare inferenza più rapidamente con modelli per testi, immagini e audio. Visualizza la documentazione sull’integrazione <a href="https://huggingface.co/docs/optimum/bettertransformer/overview" rel="nofollow">qui</a> per maggiori dettagli.',R,g,F,v,de=`TorchScript è un modo di creare modelli serializzabili e ottimizzabili da codice PyTorch. Ogni programmma TorchScript può esere salvato da un processo Python  e caricato in un processo dove non ci sono dipendenze Python.
Comparandolo con l’eager mode di default, jit mode in PyTorch normalmente fornisce prestazioni migliori per l’inferenza del modello da parte di metodologie di ottimizzazione come la operator fusion.`,Q,z,ue='Per una prima introduzione a TorchScript, vedi la Introduction to <a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#tracing-modules" rel="nofollow">PyTorch TorchScript tutorial</a>.',V,x,D,P,fe="Intel® Extension per PyTorch fornnisce ulteriori ottimizzazioni in jit mode per i modelli della serie Transformers. Consigliamo vivamente agli utenti di usufruire dei vantaggi di Intel® Extension per PyTorch con jit mode. Alcuni operator patterns usati fequentemente dai modelli Transformers models sono già supportati in Intel® Extension per PyTorch con jit mode fusions. Questi fusion patterns come Multi-head-attention fusion, Concat Linear, Linear+Add, Linear+Gelu, Add+LayerNorm fusion and etc. sono abilitati e hanno buone performance. I benefici della fusion è fornito agli utenti in modo trasparente. In base alle analisi, il ~70% dei problemi più popolari in NLP question-answering, text-classification, and token-classification possono avere benefici sulle performance grazie ai fusion patterns sia per Float32 precision che per BFloat16 Mixed precision.",K,b,ce='Vedi maggiori informazioni per <a href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/graph_optimization.html" rel="nofollow">IPEX Graph Optimization</a>.',W,C,Y,I,he='I rilasci di IPEX seguono PyTorch, verifica i vari approcci per <a href="https://intel.github.io/intel-extension-for-pytorch/" rel="nofollow">IPEX installation</a>.',Z,w,ee,y,_e="Per abilitare JIT-mode in Trainer per evaluation e prediction, devi aggiungere <code>jit_mode_eval</code> negli argomenti di Trainer.",te,h,ie,L,Te='Trovi un esempo con caso d’uso in <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering" rel="nofollow">Transformers question-answering</a>',ne,j,$e="<li>Inference using jit mode on CPU:</li>",oe,M,ge=`python run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
<b>--jit_mode_eval </b>`,ae,H,ve="<li>Inference with IPEX using jit mode on CPU:</li>",re,q,ze=`python run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
<b>--use_ipex \\</b>
<b>--jit_mode_eval</b>`,le,B,se;return c=new k({props:{title:"Inferenza Efficiente su CPU",local:"inferenza-efficiente-su-cpu",headingTag:"h1"}}),T=new k({props:{title:"BetterTransformer per inferenza più rapida",local:"bettertransformer-per-inferenza-più-rapida",headingTag:"h2"}}),g=new k({props:{title:"PyTorch JIT-mode (TorchScript)",local:"pytorch-jit-mode-torchscript",headingTag:"h2"}}),x=new k({props:{title:"IPEX Graph Optimization con JIT-mode",local:"ipex-graph-optimization-con-jit-mode",headingTag:"h3"}}),C=new k({props:{title:"Installazione di IPEX",local:"installazione-di-ipex",headingTag:"h4"}}),w=new k({props:{title:"Utilizzo del JIT-mode",local:"utilizzo-del-jit-mode",headingTag:"h3"}}),h=new He({props:{warning:!0,$$slots:{default:[qe]},$$scope:{ctx:G}}}),{c(){s=r("meta"),_=o(),f=r("p"),d=o(),E(c.$$.fragment),m=o(),u=r("p"),u.textContent=pe,O=o(),E(T.$$.fragment),N=o(),$=r("p"),$.innerHTML=me,R=o(),E(g.$$.fragment),F=o(),v=r("p"),v.textContent=de,Q=o(),z=r("p"),z.innerHTML=ue,V=o(),E(x.$$.fragment),D=o(),P=r("p"),P.textContent=fe,K=o(),b=r("p"),b.innerHTML=ce,W=o(),E(C.$$.fragment),Y=o(),I=r("p"),I.innerHTML=he,Z=o(),E(w.$$.fragment),ee=o(),y=r("p"),y.innerHTML=_e,te=o(),E(h.$$.fragment),ie=o(),L=r("p"),L.innerHTML=Te,ne=o(),j=r("ul"),j.innerHTML=$e,oe=o(),M=r("pre"),M.innerHTML=ge,ae=o(),H=r("ul"),H.innerHTML=ve,re=o(),q=r("pre"),q.innerHTML=ze,le=o(),B=r("p"),this.h()},l(e){const t=je("svelte-u9bgzb",document.head);s=l(t,"META",{name:!0,content:!0}),t.forEach(i),_=a(e),f=l(e,"P",{}),Pe(f).forEach(i),d=a(e),S(c.$$.fragment,e),m=a(e),u=l(e,"P",{"data-svelte-h":!0}),p(u)!=="svelte-qu0j64"&&(u.textContent=pe),O=a(e),S(T.$$.fragment,e),N=a(e),$=l(e,"P",{"data-svelte-h":!0}),p($)!=="svelte-67xobp"&&($.innerHTML=me),R=a(e),S(g.$$.fragment,e),F=a(e),v=l(e,"P",{"data-svelte-h":!0}),p(v)!=="svelte-3muwsh"&&(v.textContent=de),Q=a(e),z=l(e,"P",{"data-svelte-h":!0}),p(z)!=="svelte-6flq5m"&&(z.innerHTML=ue),V=a(e),S(x.$$.fragment,e),D=a(e),P=l(e,"P",{"data-svelte-h":!0}),p(P)!=="svelte-1p2d5pw"&&(P.textContent=fe),K=a(e),b=l(e,"P",{"data-svelte-h":!0}),p(b)!=="svelte-1e9vei1"&&(b.innerHTML=ce),W=a(e),S(C.$$.fragment,e),Y=a(e),I=l(e,"P",{"data-svelte-h":!0}),p(I)!=="svelte-1cbxjdx"&&(I.innerHTML=he),Z=a(e),S(w.$$.fragment,e),ee=a(e),y=l(e,"P",{"data-svelte-h":!0}),p(y)!=="svelte-y6f70"&&(y.innerHTML=_e),te=a(e),S(h.$$.fragment,e),ie=a(e),L=l(e,"P",{"data-svelte-h":!0}),p(L)!=="svelte-ibdywp"&&(L.innerHTML=Te),ne=a(e),j=l(e,"UL",{"data-svelte-h":!0}),p(j)!=="svelte-xwyqaf"&&(j.innerHTML=$e),oe=a(e),M=l(e,"PRE",{"data-svelte-h":!0}),p(M)!=="svelte-gw81i0"&&(M.innerHTML=ge),ae=a(e),H=l(e,"UL",{"data-svelte-h":!0}),p(H)!=="svelte-prijpn"&&(H.innerHTML=ve),re=a(e),q=l(e,"PRE",{"data-svelte-h":!0}),p(q)!=="svelte-18ss6jb"&&(q.innerHTML=ze),le=a(e),B=l(e,"P",{}),Pe(B).forEach(i),this.h()},h(){be(s,"name","hf:doc:metadata"),be(s,"content",Se)},m(e,t){Me(document.head,s),n(e,_,t),n(e,f,t),n(e,d,t),U(c,e,t),n(e,m,t),n(e,u,t),n(e,O,t),U(T,e,t),n(e,N,t),n(e,$,t),n(e,R,t),U(g,e,t),n(e,F,t),n(e,v,t),n(e,Q,t),n(e,z,t),n(e,V,t),U(x,e,t),n(e,D,t),n(e,P,t),n(e,K,t),n(e,b,t),n(e,W,t),U(C,e,t),n(e,Y,t),n(e,I,t),n(e,Z,t),U(w,e,t),n(e,ee,t),n(e,y,t),n(e,te,t),U(h,e,t),n(e,ie,t),n(e,L,t),n(e,ne,t),n(e,j,t),n(e,oe,t),n(e,M,t),n(e,ae,t),n(e,H,t),n(e,re,t),n(e,q,t),n(e,le,t),n(e,B,t),se=!0},p(e,[t]){const xe={};t&2&&(xe.$$scope={dirty:t,ctx:e}),h.$set(xe)},i(e){se||(J(c.$$.fragment,e),J(T.$$.fragment,e),J(g.$$.fragment,e),J(x.$$.fragment,e),J(C.$$.fragment,e),J(w.$$.fragment,e),J(h.$$.fragment,e),se=!0)},o(e){X(c.$$.fragment,e),X(T.$$.fragment,e),X(g.$$.fragment,e),X(x.$$.fragment,e),X(C.$$.fragment,e),X(w.$$.fragment,e),X(h.$$.fragment,e),se=!1},d(e){e&&(i(_),i(f),i(d),i(m),i(u),i(O),i(N),i($),i(R),i(F),i(v),i(Q),i(z),i(V),i(D),i(P),i(K),i(b),i(W),i(Y),i(I),i(Z),i(ee),i(y),i(te),i(ie),i(L),i(ne),i(j),i(oe),i(M),i(ae),i(H),i(re),i(q),i(le),i(B)),i(s),A(c,e),A(T,e),A(g,e),A(x,e),A(C,e),A(w,e),A(h,e)}}}const Se='{"title":"Inferenza Efficiente su CPU","local":"inferenza-efficiente-su-cpu","sections":[{"title":"BetterTransformer per inferenza più rapida","local":"bettertransformer-per-inferenza-più-rapida","sections":[],"depth":2},{"title":"PyTorch JIT-mode (TorchScript)","local":"pytorch-jit-mode-torchscript","sections":[{"title":"IPEX Graph Optimization con JIT-mode","local":"ipex-graph-optimization-con-jit-mode","sections":[{"title":"Installazione di IPEX","local":"installazione-di-ipex","sections":[],"depth":4}],"depth":3},{"title":"Utilizzo del JIT-mode","local":"utilizzo-del-jit-mode","sections":[],"depth":3}],"depth":2}],"depth":1}';function Ue(G){return Ie(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Be extends ye{constructor(s){super(),Le(this,s,Ue,Ee,Ce,{})}}export{Be as component};
