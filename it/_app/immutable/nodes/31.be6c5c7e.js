import{s as Ui,o as vi,n as _}from"../chunks/scheduler.36a0863c.js";import{S as zi,i as Ci,g as r,s,r as m,A as Zi,h as p,f as l,c as i,j as wi,u as d,x as c,k as Ji,y as Ii,a as n,v as u,d as M,t as f,w as g}from"../chunks/index.9c13489a.js";import{T as Gt}from"../chunks/Tip.3b06990e.js";import{C as b}from"../chunks/CodeBlock.05d8ec32.js";import{F as ki,M as $i}from"../chunks/Markdown.88297c0b.js";import{H as $}from"../chunks/Heading.7a254a62.js";function Bi(w){let a,y,o,j=`Una volta salvato il checkpoint, possiamo esportarlo su ONNX puntando l‚Äôargomento <code>--model</code>
del pacchetto <code>transformers.onnx</code> nella directory desiderata:`,T,J,U;return a=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwTG9hZCUyMHRva2VuaXplciUyMGFuZCUyMFB5VG9yY2glMjB3ZWlnaHRzJTIwZm9ybSUyMHRoZSUyMEh1YiUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZCUyMiklMEFwdF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZCUyMiklMEElMjMlMjBTYXZlJTIwdG8lMjBkaXNrJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZCglMjJsb2NhbC1wdC1jaGVja3BvaW50JTIyKSUwQXB0X21vZGVsLnNhdmVfcHJldHJhaW5lZCglMjJsb2NhbC1wdC1jaGVja3BvaW50JTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load tokenizer and PyTorch weights form the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Save to disk</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(<span class="hljs-string">&quot;local-pt-checkpoint&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(<span class="hljs-string">&quot;local-pt-checkpoint&quot;</span>)`,wrap:!1}}),J=new b({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0Rsb2NhbC1wdC1jaGVja3BvaW50JTIwb25ueCUyRg==",highlighted:"python -m transformers.onnx --model=local-pt-checkpoint onnx/",wrap:!1}}),{c(){m(a.$$.fragment),y=s(),o=r("p"),o.innerHTML=j,T=s(),m(J.$$.fragment)},l(h){d(a.$$.fragment,h),y=i(h),o=p(h,"P",{"data-svelte-h":!0}),c(o)!=="svelte-1rjvspr"&&(o.innerHTML=j),T=i(h),d(J.$$.fragment,h)},m(h,v){u(a,h,v),n(h,y,v),n(h,o,v),n(h,T,v),u(J,h,v),U=!0},p:_,i(h){U||(M(a.$$.fragment,h),M(J.$$.fragment,h),U=!0)},o(h){f(a.$$.fragment,h),f(J.$$.fragment,h),U=!1},d(h){h&&(l(y),l(o),l(T)),g(a,h),g(J,h)}}}function xi(w){let a,y;return a=new $i({props:{$$slots:{default:[Bi]},$$scope:{ctx:w}}}),{c(){m(a.$$.fragment)},l(o){d(a.$$.fragment,o)},m(o,j){u(a,o,j),y=!0},p(o,j){const T={};j&2&&(T.$$scope={dirty:j,ctx:o}),a.$set(T)},i(o){y||(M(a.$$.fragment,o),y=!0)},o(o){f(a.$$.fragment,o),y=!1},d(o){g(a,o)}}}function _i(w){let a,y,o,j=`Once the checkpoint is saved, we can export it to ONNX by pointing the <code>--model</code>
argument of the <code>transformers.onnx</code> package to the desired directory:`,T,J,U;return a=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBMb2FkJTIwdG9rZW5pemVyJTIwYW5kJTIwVGVuc29yRmxvdyUyMHdlaWdodHMlMjBmcm9tJTIwdGhlJTIwSHViJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydCUyRmRpc3RpbGJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQXRmX21vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIpJTBBJTIzJTIwU2F2ZSUyMHRvJTIwZGlzayUwQXRva2VuaXplci5zYXZlX3ByZXRyYWluZWQoJTIybG9jYWwtdGYtY2hlY2twb2ludCUyMiklMEF0Zl9tb2RlbC5zYXZlX3ByZXRyYWluZWQoJTIybG9jYWwtdGYtY2hlY2twb2ludCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load tokenizer and TensorFlow weights from the Hub</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Save to disk</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(<span class="hljs-string">&quot;local-tf-checkpoint&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(<span class="hljs-string">&quot;local-tf-checkpoint&quot;</span>)`,wrap:!1}}),J=new b({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0Rsb2NhbC10Zi1jaGVja3BvaW50JTIwb25ueCUyRg==",highlighted:"python -m transformers.onnx --model=local-tf-checkpoint onnx/",wrap:!1}}),{c(){m(a.$$.fragment),y=s(),o=r("p"),o.innerHTML=j,T=s(),m(J.$$.fragment)},l(h){d(a.$$.fragment,h),y=i(h),o=p(h,"P",{"data-svelte-h":!0}),c(o)!=="svelte-128pexq"&&(o.innerHTML=j),T=i(h),d(J.$$.fragment,h)},m(h,v){u(a,h,v),n(h,y,v),n(h,o,v),n(h,T,v),u(J,h,v),U=!0},p:_,i(h){U||(M(a.$$.fragment,h),M(J.$$.fragment,h),U=!0)},o(h){f(a.$$.fragment,h),f(J.$$.fragment,h),U=!1},d(h){h&&(l(y),l(o),l(T)),g(a,h),g(J,h)}}}function Wi(w){let a,y;return a=new $i({props:{$$slots:{default:[_i]},$$scope:{ctx:w}}}),{c(){m(a.$$.fragment)},l(o){d(a.$$.fragment,o)},m(o,j){u(a,o,j),y=!0},p(o,j){const T={};j&2&&(T.$$scope={dirty:j,ctx:o}),a.$set(T)},i(o){y||(M(a.$$.fragment,o),y=!0)},o(o){f(a.$$.fragment,o),y=!1},d(o){g(a,o)}}}function Xi(w){let a,y=`Le caratteristiche che hanno un suffisso <code>wtih-past</code> (ad es. <code>causal-lm-with-past</code>)
corrispondono a topologie di modello con stati nascosti precalcolati (chiave e valori
nei blocchi di attenzione) che possono essere utilizzati per la decodifica autoregressiva veloce.`;return{c(){a=r("p"),a.innerHTML=y},l(o){a=p(o,"P",{"data-svelte-h":!0}),c(a)!=="svelte-1on6rqr"&&(a.innerHTML=y)},m(o,j){n(o,a,j)},p:_,d(o){o&&l(a)}}}function Ai(w){let a,y=`Un buon modo per implementare una configurazione ONNX personalizzata √® guardare l‚Äôimplementazione
esistente nel file <code>configuration_&lt;model_name&gt;.py</code> di un‚Äôarchitettura simile.`;return{c(){a=r("p"),a.innerHTML=y},l(o){a=p(o,"P",{"data-svelte-h":!0}),c(a)!=="svelte-k15enw"&&(a.innerHTML=y)},m(o,j){n(o,a,j)},p:_,d(o){o&&l(a)}}}function Ni(w){let a,y=`Puoi notare che la propriet√† <code>inputs</code> per <code>DistilBertOnnxConfig</code> restituisce un
<code>OrdinatoDict</code>. Ci√≤ garantisce che gli input corrispondano alla loro posizione
relativa all‚Äôinterno del metodo <code>PreTrainedModel.forward()</code> durante il tracciamento del grafico.
Raccomandiamo di usare un <code>OrderedDict</code> per le propriet√† <code>inputs</code> e <code>outputs</code>
quando si implementano configurazioni ONNX personalizzate.`;return{c(){a=r("p"),a.innerHTML=y},l(o){a=p(o,"P",{"data-svelte-h":!0}),c(a)!=="svelte-9pdb1s"&&(a.innerHTML=y)},m(o,j){n(o,a,j)},p:_,d(o){o&&l(a)}}}function Gi(w){let a,y=`Tutte le propriet√† e i metodi di base associati a <code>OnnxConfig</code> e le
altre classi di configurazione possono essere sovrascritte se necessario. Guarda
<code>BartOnnxConfig</code> per un esempio avanzato.`;return{c(){a=r("p"),a.innerHTML=y},l(o){a=p(o,"P",{"data-svelte-h":!0}),c(a)!=="svelte-8je27i"&&(a.innerHTML=y)},m(o,j){n(o,a,j)},p:_,d(o){o&&l(a)}}}function Ri(w){let a,y=`Se il tuo modello √® pi√π largo di 2 GB, vedrai che molti file aggiuntivi sono
creati durante l‚Äôesportazione. Questo √® <em>previsto</em> perch√© ONNX utilizza <a href="https://developers.google.com/protocol-buffers/" rel="nofollow">Protocol
Buffer</a> per memorizzare il modello e
questi hanno un limite di dimensione 2 GB. Vedi la <a href="https://github.com/onnx/onnx/blob/master/docs/ExternalData.md" rel="nofollow">Documentazione
ONNX</a>
per istruzioni su come caricare modelli con dati esterni.`;return{c(){a=r("p"),a.innerHTML=y},l(o){a=p(o,"P",{"data-svelte-h":!0}),c(a)!=="svelte-zgeibt"&&(a.innerHTML=y)},m(o,j){n(o,a,j)},p:_,d(o){o&&l(a)}}}function Vi(w){let a,y=`Questo √® l‚Äôinizio dei nostri esperimenti con TorchScript e stiamo ancora esplorando le sue capacit√† con
modelli con variable-input-size. √à una nostra priorit√† e approfondiremo le nostre analisi nelle prossime versioni,
con pi√π esempi di codici, un‚Äôimplementazione pi√π flessibile e benchmark che confrontano i codici basati su Python con quelli compilati con
TorchScript.`;return{c(){a=r("p"),a.textContent=y},l(o){a=p(o,"P",{"data-svelte-h":!0}),c(a)!=="svelte-1puyypy"&&(a.textContent=y)},m(o,j){n(o,a,j)},p:_,d(o){o&&l(a)}}}function Hi(w){let a,y,o,j,T,J,U,h=`Se devi implementare ü§ó modelli Transformers in ambienti di produzione, noi
consigliamo di esportarli in un formato serializzato che pu√≤ essere caricato ed eseguito
su runtime e hardware specializzati. In questa guida ti mostreremo come farlo
esporta ü§ó Modelli Transformers in due formati ampiamente utilizzati: ONNX e TorchScript.`,v,W,es=`Una volta esportato, un modello pu√≤ essere ottimizato per l‚Äôinferenza tramite tecniche come
la quantizzazione e soppressione. Se sei interessato a ottimizzare i tuoi modelli per l‚Äôesecuzione
con la massima efficienza, dai un‚Äôocchiata a <a href="https://github.com/huggingface/optimum" rel="nofollow">ü§ó Optimum
library</a>.`,Vt,X,Ht,A,ts=`Il progetto <a href="http://onnx.ai" rel="nofollow">ONNX (Open Neural Network eXchange)</a> Il progetto onnx √® un open
standard che definisce un insieme comune di operatori e un formato di file comune a
rappresentano modelli di deep learning in un‚Äôampia variet√† di framework, tra cui
PyTorch e TensorFlow. Quando un modello viene esportato nel formato ONNX, questi
operatori sono usati per costruire un grafico computazionale (often called an
<em>intermediate representation</em>) che rappresenta il flusso di dati attraverso la
rete neurale.`,Et,N,ls=`Esponendo un grafico con operatori e tipi di dati standardizzati, ONNX rende
pi√π facile passare da un framework all‚Äôaltro. Ad esempio, un modello allenato in PyTorch pu√≤
essere esportato in formato ONNX e quindi importato in TensorFlow (e viceversa).`,Qt,G,ns=`ü§ó Transformers fornisce un pacchetto <code>transformers.onnx</code> che ti consente di
convertire i checkpoint del modello in un grafico ONNX sfruttando gli oggetti di configurazione.
Questi oggetti di configurazione sono gi√† pronti per una serie di architetture di modelli,
e sono progettati per essere facilmente estensibili ad altre architetture.`,Lt,R,ss="Le configurazioni pronte includono le seguenti architetture:",St,V,is="<li>ALBERT</li> <li>BART</li> <li>BEiT</li> <li>BERT</li> <li>BigBird</li> <li>BigBird-Pegasus</li> <li>Blenderbot</li> <li>BlenderbotSmall</li> <li>CamemBERT</li> <li>ConvBERT</li> <li>Data2VecText</li> <li>Data2VecVision</li> <li>DeiT</li> <li>DistilBERT</li> <li>ELECTRA</li> <li>FlauBERT</li> <li>GPT Neo</li> <li>GPT-J</li> <li>I-BERT</li> <li>LayoutLM</li> <li>M2M100</li> <li>Marian</li> <li>mBART</li> <li>MobileBERT</li> <li>OpenAI GPT-2</li> <li>Perceiver</li> <li>PLBart</li> <li>RoBERTa</li> <li>RoFormer</li> <li>SqueezeBERT</li> <li>T5</li> <li>ViT</li> <li>XLM</li> <li>XLM-RoBERTa</li> <li>XLM-RoBERTa-XL</li>",Ft,H,as="Nelle prossime due sezioni, ti mostreremo come:",qt,E,os="<li>Esporta un modello supportato usando il pacchetto <code>transformers.onnx</code>.</li> <li>Esporta un modello personalizzato per un‚Äôarchitettura non supportata.</li>",Yt,Q,Pt,L,rs=`Per esportare un modello ü§ó Transformers in ONNX, dovrai prima installarne alcune
dipendenze extra:`,Dt,S,Ot,F,ps="Il pacchetto <code>transformers.onnx</code> pu√≤ essere usato come modulo Python:",Kt,q,el,Y,cs="L‚Äôesportazione di un checkpoint utilizzando una configurazione gi√† pronta pu√≤ essere eseguita come segue:",tl,P,ll,D,ms="che dovrebbe mostrare i seguenti log:",nl,O,sl,K,ds=`Questo esporta un grafico ONNX del checkpoint definito dall‚Äôargomento <code>--model</code>.
In questo esempio √® <code>distilbert/distilbert-base-uncased</code>, ma pu√≤ essere qualsiasi checkpoint
Hugging Face Hub o uno memorizzato localmente.`,il,ee,us=`Il file risultante <code>model.onnx</code> pu√≤ quindi essere eseguito su uno dei <a href="https://onnx.ai/supported-tools.html#deployModel" rel="nofollow">tanti
acceleratori</a> che supportano il
lo standard ONNX. Ad esempio, possiamo caricare ed eseguire il modello con <a href="https://onnxruntime.ai/" rel="nofollow">ONNX
Runtime</a> come segue:`,al,te,ol,le,Ms=`I nomi di output richiesti (cio√® <code>[&quot;last_hidden_state&quot;]</code>) possono essere ottenuti
dando un‚Äôocchiata alla configurazione ONNX di ogni modello. Ad esempio, per
DistilBERT abbiamo:`,rl,ne,pl,se,fs=`Il processo √® identico per i checkpoint TensorFlow sull‚Äôhub. Ad esempio, noi
possiamo esportare un checkpoint TensorFlow puro da <a href="https://huggingface.co/keras-io" rel="nofollow">Keras
organizzazione</a> come segue:`,cl,ie,ml,ae,gs=`Per esportare un modello memorizzato localmente, devi disporre dei pesi del modello
e file tokenizer memorizzati in una directory. Ad esempio, possiamo caricare e salvare un
checkpoint come segue:`,dl,z,ul,oe,Ml,re,hs=`Ogni configurazione gi√† pronta viene fornita con una serie di <em>caratteristiche</em> che ti consentono di
esportare modelli per diversi tipi di topologie o attivit√†. Come mostrato nella tabella
di seguito, ogni caratteristica √® associata a una diversa Auto Class:`,fl,pe,ys="<thead><tr><th>Caratteristica</th> <th>Auto Class</th></tr></thead> <tbody><tr><td><code>causal-lm</code>, <code>causal-lm-with-past</code></td> <td><code>AutoModelForCausalLM</code></td></tr> <tr><td><code>default</code>, <code>default-with-past</code></td> <td><code>AutoModel</code></td></tr> <tr><td><code>masked-lm</code></td> <td><code>AutoModelForMaskedLM</code></td></tr> <tr><td><code>question-answering</code></td> <td><code>AutoModelForQuestionAnswering</code></td></tr> <tr><td><code>seq2seq-lm</code>, <code>seq2seq-lm-with-past</code></td> <td><code>AutoModelForSeq2SeqLM</code></td></tr> <tr><td><code>sequence-classification</code></td> <td><code>AutoModelForSequenceClassification</code></td></tr> <tr><td><code>token-classification</code></td> <td><code>AutoModelForTokenClassification</code></td></tr></tbody>",gl,ce,js=`Per ciascuna configurazione, puoi trovare l‚Äôelenco delle funzionalit√† supportate tramite il
<code>FeaturesManager</code>. Ad esempio, per DistilBERT abbiamo:`,hl,me,yl,de,bs=`Puoi quindi passare una di queste funzionalit√† all‚Äôargomento <code>--feature</code> nel
pacchetto <code>transformers.onnx</code>. Ad esempio, per esportare un modello di classificazione del testo
possiamo scegliere un modello ottimizzato dall‚ÄôHub ed eseguire:`,jl,ue,bl,Me,Ts="che visualizzer√† i seguenti registri:",Tl,fe,wl,ge,ws=`Puoi notare che in questo caso, i nomi di output del modello ottimizzato sono
<code>logits</code> invece di <code>last_hidden_state</code> che abbiamo visto con il
checkpoint <code>distilbert/distilbert-base-uncased</code> precedente. Questo √® previsto dal
modello ottimizato visto che ha una testa di e.`,Jl,C,$l,he,Ul,ye,Js=`Se desideri esportare un modello la cui architettura non √® nativamente supportata dalla
libreria, ci sono tre passaggi principali da seguire:`,vl,je,$s="<li>Implementare una configurazione ONNX personalizzata.</li> <li>Esportare il modello in ONNX.</li> <li>Convalidare gli output di PyTorch e dei modelli esportati.</li>",zl,be,Us=`In questa sezione, vedremo come DistilBERT √® stato implementato per mostrare cosa √®
coinvolto in ogni passaggio.`,Cl,Te,Zl,we,vs=`Iniziamo con l‚Äôoggetto di configurazione ONNX. Forniamo tre classi
astratte da cui ereditare, a seconda del tipo di archittettura
del modello che desideri esportare:`,Il,Je,zs="<li>I modelli basati su encoder ereditano da <code>OnnxConfig</code></li> <li>I modelli basati su decoder ereditano da <code>OnnxConfigWithPast</code></li> <li>I modelli encoder-decoder ereditano da<code>OnnxSeq2SeqConfigWithPast</code></li>",kl,Z,Bl,$e,Cs=`Poich√© DistilBERT √® un modello basato su encoder, la sua configurazione eredita da
<code>OnnxConfig</code>:`,xl,Ue,_l,ve,Zs=`Ogni oggetto di configurazione deve implementare la propriet√† <code>inputs</code> e restituire una
mappatura, dove ogni chiave corrisponde a un input previsto e ogni valore
indica l‚Äôasse di quell‚Äôinput. Per DistilBERT, possiamo vedere che sono richiesti
due input: <code>input_ids</code> e <code>attention_mask</code>. Questi inputs hanno la stessa forma di
<code>(batch_size, sequence_length)</code> per questo motivo vediamo gli stessi assi usati nella
configurazione.`,Wl,I,Xl,ze,Is=`Dopo aver implementato una configurazione ONNX, √® possibile istanziarla
fornendo alla configurazione del modello base come segue:`,Al,Ce,Nl,Ze,ks=`L‚Äôoggetto risultante ha diverse propriet√† utili. Ad esempio √® possibile visualizzare il
Set operatore ONNX che verr√† utilizzato durante l‚Äôesportazione:`,Gl,Ie,Rl,ke,Bs="√à inoltre possibile visualizzare gli output associati al modello come segue:",Vl,Be,Hl,xe,xs=`Puoi notare che la propriet√† degli output segue la stessa struttura degli input; esso
restituisce un <code>OrderedDict</code> di output con nome e le loro forme. La struttura di output
√® legato alla scelta della funzione con cui viene inizializzata la configurazione.
Per impostazione predefinita, la configurazione ONNX viene inizializzata con la funzione ‚Äòpredefinita‚Äô
che corrisponde all‚Äôesportazione di un modello caricato con la classe <code>AutoModel</code>. Se tu
desideri esportare una topologia di modello diversa, √® sufficiente fornire una funzionalit√† diversa a
l‚Äôargomento <code>task</code> quando inizializzi la configurazione ONNX. Ad esempio, se
volevamo esportare DistilBERT con una testa di classificazione per sequenze, potremmo
usare:`,El,_e,Ql,k,Ll,We,Sl,Xe,_s=`Una volta implementata la configurazione ONNX, il passaggio successivo consiste nell‚Äôesportare il
modello. Qui possiamo usare la funzione <code>export()</code> fornita dal
pacchetto <code>transformers.onnx</code>. Questa funzione prevede la configurazione ONNX, insieme
con il modello base e il tokenizer e il percorso per salvare il file esportato:`,Fl,Ae,ql,Ne,Ws=`Gli <code>onnx_inputs</code> e <code>onnx_outputs</code> restituiti dalla funzione <code>export()</code> sono
liste di chiavi definite nelle propriet√† di <code>input</code> e <code>output</code> della
configurazione. Una volta esportato il modello, puoi verificare che il modello sia ben
formato come segue:`,Yl,Ge,Pl,B,Dl,Re,Ol,Ve,Xs=`Il passaggio finale consiste nel convalidare gli output dal modello di base e quello esportato
corrispondere entro una soglia di tolleranza assoluta. Qui possiamo usare la
Funzione <code>validate_model_outputs()</code> fornita dal pacchetto <code>transformers.onnx</code>
come segue:`,Kl,He,en,Ee,As=`Questa funzione usa il metodo <code>OnnxConfig.generate_dummy_inputs()</code> per generare
input per il modello di base e quello esportato e la tolleranza assoluta pu√≤ essere
definita nella configurazione. Generalmente troviamo una corrispondenza numerica nell‚Äôintervallo da 1e-6
a 1e-4, anche se √® probabile che qualsiasi cosa inferiore a 1e-3 vada bene.`,tn,Qe,ln,Le,Ns=`Stiamo cercando di espandere l‚Äôinsieme di configurazioni gi√† pronte e di accettare
contributi della community! Se vuoi contribuire con la tua aggiunta
nella libreria, dovrai:`,nn,Se,Gs="<li>Implementare la configurazione ONNX nella corrispondente <code>configuration file _&lt;model_name&gt;.py</code></li> <li>Includere l‚Äôarchitettura del modello e le funzioni corrispondenti in <code>~onnx.features.FeatureManager</code></li> <li>Aggiungere la tua architettura del modello ai test in <code>test_onnx_v2.py</code></li>",sn,Fe,Rs=`Scopri come stato contribuito la configurazione per <a href="https://github.com/huggingface/transformers/pull/14868/files" rel="nofollow">IBERT</a> per
avere un‚Äôidea di cosa √® coinvolto.`,an,qe,on,x,rn,Ye,Vs=`Secondo la documentazione di Pytorch: ‚ÄúTorchScript √® un modo per creare modelli serializzabili e ottimizzabili da codice
Pytorch‚Äù. I due moduli di Pytorch <a href="https://pytorch.org/docs/stable/jit.html" rel="nofollow">JIT e TRACE</a> consentono allo sviluppatore di esportare
il loro modello da riutilizzare in altri programmi, come i programmi C++ orientati all‚Äôefficienza.`,pn,Pe,Hs=`Abbiamo fornito un‚Äôinterfaccia che consente l‚Äôesportazione di modelli ü§ó Transformers in TorchScript in modo che possano essere riutilizzati
in un ambiente diverso rispetto a un programma Python basato su Pytorch. Qui spieghiamo come esportare e utilizzare i nostri modelli utilizzando
TorchScript.`,cn,De,Es="Esportare un modello richiede due cose:",mn,Oe,Qs="<li>Un passaggio in avanti con input fittizzi.</li> <li>Istanziazione del modello con flag <code>torchscript</code>.</li>",dn,Ke,Ls="Queste necessit√† implicano diverse cose a cui gli sviluppatori dovrebbero prestare attenzione. Questi dettagli mostrati sotto.",un,et,Mn,tt,Ss=`Questo flag √® necessario perch√© la maggior parte dei modelli linguistici in questo repository hanno pesi legati tra il loro
strato ‚ÄúEmbedding‚Äù e lo strato ‚ÄúDecoding‚Äù. TorchScript non consente l‚Äôesportazione di modelli che hanno pesi
legati, quindi √® necessario prima slegare e clonare i pesi.`,fn,lt,Fs=`Ci√≤ implica che i modelli istanziati con il flag <code>torchscript</code> hanno il loro strato <code>Embedding</code> e strato <code>Decoding</code>
separato, il che significa che non dovrebbero essere addestrati in futuro. L‚Äôallenamento de-sincronizza i due
strati, portando a risultati inaspettati.`,gn,nt,qs=`Questo non √® il caso per i modelli che non hanno una testa del modello linguistico, poich√© quelli non hanno pesi legati. Questi modelli
pu√≤ essere esportato in sicurezza senza il flag <code>torchscript</code>.`,hn,st,yn,it,Ys=`Gli input fittizzi sono usati per fare un modello passaggio in avanti . Mentre i valori degli input si propagano attraverso i strati,
Pytorch tiene traccia delle diverse operazioni eseguite su ciascun tensore. Queste operazioni registrate vengono quindi utilizzate per
creare la ‚Äútraccia‚Äù del modello.`,jn,at,Ps=`La traccia viene creata relativamente alle dimensioni degli input. √à quindi vincolato dalle dimensioni dell‚Äôinput
fittizio e non funzioner√† per altre lunghezze di sequenza o dimensioni batch. Quando si prover√† con una dimensione diversa, ci sar√† errore
come:`,bn,ot,Ds="<code>La dimensione espansa del tensore (3) deve corrispondere alla dimensione esistente (7) nella dimensione non singleton 2</code>",Tn,rt,Os=`will be raised. Si consiglia pertanto di tracciare il modello con una dimensione di input fittizia grande almeno quanto il pi√π grande
input che verr√† fornito al modello durante l‚Äôinferenza. √à possibile eseguire il padding per riempire i valori mancanti. Il modello
sar√† tracciato con una grande dimensione di input, tuttavia, anche le dimensioni della diverse matrici saranno grandi,
risultando in pi√π calcoli.`,wn,pt,Ks=`Si raccomanda di prestare attenzione al numero totale di operazioni eseguite su ciascun input e di seguire da vicino le prestazioni
durante l‚Äôesportazione di modelli di sequenza-lunghezza variabili.`,Jn,ct,$n,mt,ei="Di seguito √® riportato un esempio, che mostra come salvare, caricare modelli e come utilizzare la traccia per l‚Äôinferenza.",Un,dt,vn,ut,ti=`Questo frammento di codice mostra come usare TorchScript per esportare un <code>BertModel</code>. Qui il <code>BertModel</code> √® istanziato secondo
una classe <code>BertConfig</code> e quindi salvato su disco con il nome del file <code>traced_bert.pt</code>`,zn,Mt,Cn,ft,Zn,gt,li=`Questo frammento di codice mostra come caricare il <code>BertModel</code> che era stato precedentemente salvato su disco con il nome <code>traced_bert.pt</code>.
Stiamo riutilizzando il <code>dummy_input</code> precedentemente inizializzato.`,In,ht,kn,yt,Bn,jt,ni="Usare il modello tracciato per l‚Äôinferenza √® semplice come usare il suo metodo dunder <code>__call__</code>:",xn,bt,_n,Tt,Wn,wt,si=`AWS ha introdotto <a href="https://aws.amazon.com/ec2/instance-types/inf1/" rel="nofollow">Amazon EC2 Inf1</a>
famiglia di istanze per l‚Äôinferenza di machine learning a basso costo e ad alte prestazioni nel cloud.
Le istanze Inf1 sono alimentate dal chip AWS Inferentia, un acceleratore hardware personalizzato,
specializzato in carichi di lavoro di inferenza di deep learning.
<a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#" rel="nofollow">AWS Neuron</a>
√® l‚ÄôSDK per Inferentia che supporta il tracciamento e l‚Äôottimizzazione dei modelli transformers per
distribuzione su Inf1. L‚ÄôSDK Neuron fornisce:`,Xn,Jt,ii=`<li>API di facile utilizzo con una riga di modifica del codice per tracciare e ottimizzare un modello TorchScript per l‚Äôinferenza nel cloud.</li> <li>Ottimizzazioni delle prestazioni pronte all‚Äôuso per <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/%3E" rel="nofollow">miglioramento dei costi-prestazioni</a></li> <li>Supporto per i modelli di trasformatori HuggingFace costruiti con <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html" rel="nofollow">PyTorch</a>
o <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html" rel="nofollow">TensorFlow</a>.</li>`,An,$t,Nn,Ut,ai=`Modelli Transformers basati su architettura <a href="https://huggingface.co/docs/transformers/main/model_doc/bert" rel="nofollow">BERT (Bidirectional Encoder Representations from Transformers)</a>,
o sue varianti come <a href="https://huggingface.co/docs/transformers/main/model_doc/distilbert" rel="nofollow">distilBERT</a>
e <a href="https://huggingface.co/docs/transformers/main/model_doc/roberta" rel="nofollow">roBERTa</a>
funzioneranno meglio su Inf1 per attivit√† non generative come la question answering estrattive,
Classificazione della sequenza, Classificazione dei token. In alternativa, generazione di testo
le attivit√† possono essere adattate per essere eseguite su Inf1, secondo questo <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html" rel="nofollow">tutorial AWS Neuron MarianMT</a>.
Ulteriori informazioni sui modelli che possono essere convertiti fuori dagli schemi su Inferentia possono essere
trovati nella <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia" rel="nofollow">sezione Model Architecture Fit della documentazione Neuron</a>.`,Gn,vt,Rn,zt,oi="L‚Äôutilizzo di AWS Neuron per convertire i modelli richiede le seguenti dipendenze e l‚Äôambiente:",Vn,Ct,ri=`<li>A <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide" rel="nofollow">Neuron SDK environment</a>,
which comes pre-configured on <a href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html" rel="nofollow">AWS Deep Learning AMI</a>.</li>`,Hn,Zt,En,It,pi=`Usando lo stesso script come in <a href="https://huggingface.co/docs/transformers/main/en/serialization#using-torchscript-in-python" rel="nofollow">Usando TorchScipt in Python</a>
per tracciare un ‚ÄúBertModel‚Äù, importi l‚Äôestensione del framework <code>torch.neuron</code> per accedere
i componenti di Neuron SDK tramite un‚ÄôAPI Python.`,Qn,kt,Ln,Bt,ci="E modificare solo la riga di codice di traccia",Sn,xt,mi="Da:",Fn,_t,qn,Wt,di="A:",Yn,Xt,Pn,At,ui="Questa modifica consente a Neuron SDK di tracciare il modello e ottimizzarlo per l‚Äôesecuzione nelle istanze Inf1.",Dn,Nt,Mi=`Per ulteriori informazioni sulle funzionalit√†, gli strumenti, i tutorial di esempi e gli ultimi aggiornamenti di AWS Neuron SDK,
consultare la <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html" rel="nofollow">documentazione AWS NeuronSDK</a>.`,On,Rt,Kn;return T=new $({props:{title:"Esporta modelli ü§ó Transformers",local:"esporta-modelli--transformers",headingTag:"h1"}}),X=new $({props:{title:"ONNX",local:"onnx",headingTag:"h2"}}),Q=new $({props:{title:"Esportazione di un modello in ONNX",local:"esportazione-di-un-modello-in-onnx",headingTag:"h3"}}),S=new b({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyU1Qm9ubnglNUQ=",highlighted:"pip install transformers[onnx]",wrap:!1}}),q=new b({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0taGVscCUwQSUwQXVzYWdlJTNBJTIwSHVnZ2luZyUyMEZhY2UlMjBUcmFuc2Zvcm1lcnMlMjBPTk5YJTIwZXhwb3J0ZXIlMjAlNUItaCU1RCUyMC1tJTIwTU9ERUwlMjAlNUItLWZlYXR1cmUlMjAlN0JjYXVzYWwtbG0lMkMlMjAuLi4lN0QlNUQlMjAlNUItLW9wc2V0JTIwT1BTRVQlNUQlMjAlNUItLWF0b2wlMjBBVE9MJTVEJTIwb3V0cHV0JTBBJTBBcG9zaXRpb25hbCUyMGFyZ3VtZW50cyUzQSUwQSUyMCUyMG91dHB1dCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMFBhdGglMjBpbmRpY2F0aW5nJTIwd2hlcmUlMjB0byUyMHN0b3JlJTIwZ2VuZXJhdGVkJTIwT05OWCUyMG1vZGVsLiUwQSUwQW9wdGlvbmFsJTIwYXJndW1lbnRzJTNBJTBBJTIwJTIwLWglMkMlMjAtLWhlbHAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBzaG93JTIwdGhpcyUyMGhlbHAlMjBtZXNzYWdlJTIwYW5kJTIwZXhpdCUwQSUyMCUyMC1tJTIwTU9ERUwlMkMlMjAtLW1vZGVsJTIwTU9ERUwlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBNb2RlbCUyMElEJTIwb24lMjBodWdnaW5nZmFjZS5jbyUyMG9yJTIwcGF0aCUyMG9uJTIwZGlzayUyMHRvJTIwbG9hZCUyMG1vZGVsJTIwZnJvbS4lMEElMjAlMjAtLWZlYXR1cmUlMjAlN0JjYXVzYWwtbG0lMkMlMjAuLi4lN0QlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBUaGUlMjB0eXBlJTIwb2YlMjBmZWF0dXJlcyUyMHRvJTIwZXhwb3J0JTIwdGhlJTIwbW9kZWwlMjB3aXRoLiUwQSUyMCUyMC0tb3BzZXQlMjBPUFNFVCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyME9OTlglMjBvcHNldCUyMHZlcnNpb24lMjB0byUyMGV4cG9ydCUyMHRoZSUyMG1vZGVsJTIwd2l0aC4lMEElMjAlMjAtLWF0b2wlMjBBVE9MJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwQWJzb2x1dGUlMjBkaWZmZXJlbmNlJTIwdG9sZXJhbmNlJTIwd2hlbiUyMHZhbGlkYXRpbmclMjB0aGUlMjBtb2RlbC4=",highlighted:`python -m transformers.onnx --<span class="hljs-built_in">help</span>

usage: Hugging Face Transformers ONNX exporter [-h] -m MODEL [--feature {causal-lm, ...}] [--opset OPSET] [--atol ATOL] output

positional arguments:
  output                Path indicating <span class="hljs-built_in">where</span> to store generated ONNX model.

optional arguments:
  -h, --<span class="hljs-built_in">help</span>            show this <span class="hljs-built_in">help</span> message and <span class="hljs-built_in">exit</span>
  -m MODEL, --model MODEL
                        Model ID on huggingface.co or path on disk to load model from.
  --feature {causal-lm, ...}
                        The <span class="hljs-built_in">type</span> of features to <span class="hljs-built_in">export</span> the model with.
  --opset OPSET         ONNX opset version to <span class="hljs-built_in">export</span> the model with.
  --atol ATOL           Absolute difference tolerance when validating the model.`,wrap:!1}}),P=new b({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0RkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjBvbm54JTJG",highlighted:"python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/",wrap:!1}}),O=new b({props:{code:"VmFsaWRhdGluZyUyME9OTlglMjBtb2RlbC4uLiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC0lNUIlRTIlOUMlOTMlNUQlMjBPTk5YJTIwbW9kZWwlMjBvdXRwdXQlMjBuYW1lcyUyMG1hdGNoJTIwcmVmZXJlbmNlJTIwbW9kZWwlMjAoJTdCJ2xhc3RfaGlkZGVuX3N0YXRlJyU3RCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAtJTIwVmFsaWRhdGluZyUyME9OTlglMjBNb2RlbCUyMG91dHB1dCUyMCUyMmxhc3RfaGlkZGVuX3N0YXRlJTIyJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLSU1QiVFMiU5QyU5MyU1RCUyMCgyJTJDJTIwOCUyQyUyMDc2OCklMjBtYXRjaGVzJTIwKDIlMkMlMjA4JTJDJTIwNzY4KSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC0lNUIlRTIlOUMlOTMlNUQlMjBhbGwlMjB2YWx1ZXMlMjBjbG9zZSUyMChhdG9sJTNBJTIwMWUtMDUpJTBBQWxsJTIwZ29vZCUyQyUyMG1vZGVsJTIwc2F2ZWQlMjBhdCUzQSUyMG9ubnglMkZtb2RlbC5vbm54",highlighted:`Validating ONNX model...
        -[‚úì] ONNX model output names match reference model ({<span class="hljs-string">&#x27;last_hidden_state&#x27;</span>})
        - Validating ONNX Model output <span class="hljs-string">&quot;last_hidden_state&quot;</span>:
                -[‚úì] (2, 8, 768) matches (2, 8, 768)
                -[‚úì] all values close (atol: 1e-05)
All good, model saved at: onnx/model.onnx`,wrap:!1}}),te=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEFmcm9tJTIwb25ueHJ1bnRpbWUlMjBpbXBvcnQlMjBJbmZlcmVuY2VTZXNzaW9uJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZGlzdGlsYmVydCUyRmRpc3RpbGJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQXNlc3Npb24lMjAlM0QlMjBJbmZlcmVuY2VTZXNzaW9uKCUyMm9ubnglMkZtb2RlbC5vbm54JTIyKSUwQSUyMyUyME9OTlglMjBSdW50aW1lJTIwZXhwZWN0cyUyME51bVB5JTIwYXJyYXlzJTIwYXMlMjBpbnB1dCUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglMjJVc2luZyUyMERpc3RpbEJFUlQlMjB3aXRoJTIwT05OWCUyMFJ1bnRpbWUhJTIyJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJucCUyMiklMEFvdXRwdXRzJTIwJTNEJTIwc2Vzc2lvbi5ydW4ob3V0cHV0X25hbWVzJTNEJTVCJTIybGFzdF9oaWRkZW5fc3RhdGUlMjIlNUQlMkMlMjBpbnB1dF9mZWVkJTNEZGljdChpbnB1dHMpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> onnxruntime <span class="hljs-keyword">import</span> InferenceSession

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>session = InferenceSession(<span class="hljs-string">&quot;onnx/model.onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># ONNX Runtime expects NumPy arrays as input</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(<span class="hljs-string">&quot;Using DistilBERT with ONNX Runtime!&quot;</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = session.run(output_names=[<span class="hljs-string">&quot;last_hidden_state&quot;</span>], input_feed=<span class="hljs-built_in">dict</span>(inputs))`,wrap:!1}}),ne=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5tb2RlbHMuZGlzdGlsYmVydCUyMGltcG9ydCUyMERpc3RpbEJlcnRDb25maWclMkMlMjBEaXN0aWxCZXJ0T25ueENvbmZpZyUwQSUwQWNvbmZpZyUyMCUzRCUyMERpc3RpbEJlcnRDb25maWcoKSUwQW9ubnhfY29uZmlnJTIwJTNEJTIwRGlzdGlsQmVydE9ubnhDb25maWcoY29uZmlnKSUwQXByaW50KGxpc3Qob25ueF9jb25maWcub3V0cHV0cy5rZXlzKCkpKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.models.distilbert <span class="hljs-keyword">import</span> DistilBertConfig, DistilBertOnnxConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>config = DistilBertConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_config = DistilBertOnnxConfig(config)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-built_in">list</span>(onnx_config.outputs.keys()))
[<span class="hljs-string">&quot;last_hidden_state&quot;</span>]`,wrap:!1}}),ie=new b({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0RrZXJhcy1pbyUyRnRyYW5zZm9ybWVycy1xYSUyMG9ubnglMkY=",highlighted:"python -m transformers.onnx --model=keras-io/transformers-qa onnx/",wrap:!1}}),z=new ki({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Wi],pytorch:[xi]},$$scope:{ctx:w}}}),oe=new $({props:{title:"Selezione delle caratteristiche per diverse topologie di modello",local:"selezione-delle-caratteristiche-per-diverse-topologie-di-modello",headingTag:"h3"}}),me=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5vbm54LmZlYXR1cmVzJTIwaW1wb3J0JTIwRmVhdHVyZXNNYW5hZ2VyJTBBJTBBZGlzdGlsYmVydF9mZWF0dXJlcyUyMCUzRCUyMGxpc3QoRmVhdHVyZXNNYW5hZ2VyLmdldF9zdXBwb3J0ZWRfZmVhdHVyZXNfZm9yX21vZGVsX3R5cGUoJTIyZGlzdGlsYmVydCUyMikua2V5cygpKSUwQXByaW50KGRpc3RpbGJlcnRfZmVhdHVyZXMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.onnx.features <span class="hljs-keyword">import</span> FeaturesManager

<span class="hljs-meta">&gt;&gt;&gt; </span>distilbert_features = <span class="hljs-built_in">list</span>(FeaturesManager.get_supported_features_for_model_type(<span class="hljs-string">&quot;distilbert&quot;</span>).keys())
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(distilbert_features)
[<span class="hljs-string">&quot;default&quot;</span>, <span class="hljs-string">&quot;masked-lm&quot;</span>, <span class="hljs-string">&quot;causal-lm&quot;</span>, <span class="hljs-string">&quot;sequence-classification&quot;</span>, <span class="hljs-string">&quot;token-classification&quot;</span>, <span class="hljs-string">&quot;question-answering&quot;</span>]`,wrap:!1}}),ue=new b({props:{code:"cHl0aG9uJTIwLW0lMjB0cmFuc2Zvcm1lcnMub25ueCUyMC0tbW9kZWwlM0RkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQtZmluZXR1bmVkLXNzdC0yLWVuZ2xpc2glMjAlNUMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAtLWZlYXR1cmUlM0RzZXF1ZW5jZS1jbGFzc2lmaWNhdGlvbiUyMG9ubnglMkY=",highlighted:`python -m transformers.onnx --model=distilbert/distilbert-base-uncased-finetuned-sst-2-english \\
                            --feature=sequence-classification onnx/`,wrap:!1}}),fe=new b({props:{code:"VmFsaWRhdGluZyUyME9OTlglMjBtb2RlbC4uLiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMC0lNUIlRTIlOUMlOTMlNUQlMjBPTk5YJTIwbW9kZWwlMjBvdXRwdXQlMjBuYW1lcyUyMG1hdGNoJTIwcmVmZXJlbmNlJTIwbW9kZWwlMjAoJTdCJ2xvZ2l0cyclN0QpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLSUyMFZhbGlkYXRpbmclMjBPTk5YJTIwTW9kZWwlMjBvdXRwdXQlMjAlMjJsb2dpdHMlMjIlM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAtJTVCJUUyJTlDJTkzJTVEJTIwKDIlMkMlMjAyKSUyMG1hdGNoZXMlMjAoMiUyQyUyMDIpJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwLSU1QiVFMiU5QyU5MyU1RCUyMGFsbCUyMHZhbHVlcyUyMGNsb3NlJTIwKGF0b2wlM0ElMjAxZS0wNSklMEFBbGwlMjBnb29kJTJDJTIwbW9kZWwlMjBzYXZlZCUyMGF0JTNBJTIwb25ueCUyRm1vZGVsLm9ubng=",highlighted:`Validating ONNX model...
        -[‚úì] ONNX model output names match reference model ({<span class="hljs-string">&#x27;logits&#x27;</span>})
        - Validating ONNX Model output <span class="hljs-string">&quot;logits&quot;</span>:
                -[‚úì] (2, 2) matches (2, 2)
                -[‚úì] all values close (atol: 1e-05)
All good, model saved at: onnx/model.onnx`,wrap:!1}}),C=new Gt({props:{$$slots:{default:[Xi]},$$scope:{ctx:w}}}),he=new $({props:{title:"Esportazione di un modello per un‚Äôarchitettura non supportata",local:"esportazione-di-un-modello-per-unarchitettura-non-supportata",headingTag:"h3"}}),Te=new $({props:{title:"Implementazione di una configurazione ONNX personalizzata",local:"implementazione-di-una-configurazione-onnx-personalizzata",headingTag:"h4"}}),Z=new Gt({props:{$$slots:{default:[Ai]},$$scope:{ctx:w}}}),Ue=new b({props:{code:"ZnJvbSUyMHR5cGluZyUyMGltcG9ydCUyME1hcHBpbmclMkMlMjBPcmRlcmVkRGljdCUwQWZyb20lMjB0cmFuc2Zvcm1lcnMub25ueCUyMGltcG9ydCUyME9ubnhDb25maWclMEElMEElMEFjbGFzcyUyMERpc3RpbEJlcnRPbm54Q29uZmlnKE9ubnhDb25maWcpJTNBJTBBJTIwJTIwJTIwJTIwJTQwcHJvcGVydHklMEElMjAlMjAlMjAlMjBkZWYlMjBpbnB1dHMoc2VsZiklMjAtJTNFJTIwTWFwcGluZyU1QnN0ciUyQyUyME1hcHBpbmclNUJpbnQlMkMlMjBzdHIlNUQlNUQlM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjByZXR1cm4lMjBPcmRlcmVkRGljdCglMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlNUIlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAoJTIyaW5wdXRfaWRzJTIyJTJDJTIwJTdCMCUzQSUyMCUyMmJhdGNoJTIyJTJDJTIwMSUzQSUyMCUyMnNlcXVlbmNlJTIyJTdEKSUyQyUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMCglMjJhdHRlbnRpb25fbWFzayUyMiUyQyUyMCU3QjAlM0ElMjAlMjJiYXRjaCUyMiUyQyUyMDElM0ElMjAlMjJzZXF1ZW5jZSUyMiU3RCklMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAlNUQlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjAp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> typing <span class="hljs-keyword">import</span> Mapping, OrderedDict
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.onnx <span class="hljs-keyword">import</span> OnnxConfig


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">class</span> <span class="hljs-title class_">DistilBertOnnxConfig</span>(<span class="hljs-title class_ inherited__">OnnxConfig</span>):
<span class="hljs-meta">... </span>    @<span class="hljs-built_in">property</span>
<span class="hljs-meta">... </span>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">inputs</span>(<span class="hljs-params">self</span>) -&gt; Mapping[<span class="hljs-built_in">str</span>, Mapping[<span class="hljs-built_in">int</span>, <span class="hljs-built_in">str</span>]]:
<span class="hljs-meta">... </span>        <span class="hljs-keyword">return</span> OrderedDict(
<span class="hljs-meta">... </span>            [
<span class="hljs-meta">... </span>                (<span class="hljs-string">&quot;input_ids&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;sequence&quot;</span>}),
<span class="hljs-meta">... </span>                (<span class="hljs-string">&quot;attention_mask&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;sequence&quot;</span>}),
<span class="hljs-meta">... </span>            ]
<span class="hljs-meta">... </span>        )`,wrap:!1}}),I=new Gt({props:{$$slots:{default:[Ni]},$$scope:{ctx:w}}}),Ce=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMEElMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIpJTBBb25ueF9jb25maWclMjAlM0QlMjBEaXN0aWxCZXJ0T25ueENvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_config = DistilBertOnnxConfig(config)`,wrap:!1}}),Ie=new b({props:{code:"cHJpbnQob25ueF9jb25maWcuZGVmYXVsdF9vbm54X29wc2V0KQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(onnx_config.default_onnx_opset)
<span class="hljs-number">11</span>`,wrap:!1}}),Be=new b({props:{code:"cHJpbnQob25ueF9jb25maWcub3V0cHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(onnx_config.outputs)
OrderedDict([(<span class="hljs-string">&quot;last_hidden_state&quot;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;batch&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;sequence&quot;</span>})])`,wrap:!1}}),_e=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMEElMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIpJTBBb25ueF9jb25maWdfZm9yX3NlcV9jbGYlMjAlM0QlMjBEaXN0aWxCZXJ0T25ueENvbmZpZyhjb25maWclMkMlMjB0YXNrJTNEJTIyc2VxdWVuY2UtY2xhc3NpZmljYXRpb24lMjIpJTBBcHJpbnQob25ueF9jb25maWdfZm9yX3NlcV9jbGYub3V0cHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_config_for_seq_clf = DistilBertOnnxConfig(config, task=<span class="hljs-string">&quot;sequence-classification&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(onnx_config_for_seq_clf.outputs)
OrderedDict([(<span class="hljs-string">&#x27;logits&#x27;</span>, {<span class="hljs-number">0</span>: <span class="hljs-string">&#x27;batch&#x27;</span>})])`,wrap:!1}}),k=new Gt({props:{$$slots:{default:[Gi]},$$scope:{ctx:w}}}),We=new $({props:{title:"Esportazione del modello",local:"esportazione-del-modello",headingTag:"h4"}}),Ae=new b({props:{code:"ZnJvbSUyMHBhdGhsaWIlMjBpbXBvcnQlMjBQYXRoJTBBZnJvbSUyMHRyYW5zZm9ybWVycy5vbm54JTIwaW1wb3J0JTIwZXhwb3J0JTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBdXRvTW9kZWwlMEElMEFvbm54X3BhdGglMjAlM0QlMjBQYXRoKCUyMm1vZGVsLm9ubnglMjIpJTBBbW9kZWxfY2twdCUyMCUzRCUyMCUyMmRpc3RpbGJlcnQlMkZkaXN0aWxiZXJ0LWJhc2UtdW5jYXNlZCUyMiUwQWJhc2VfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2NrcHQpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfY2twdCklMEElMEFvbm54X2lucHV0cyUyQyUyMG9ubnhfb3V0cHV0cyUyMCUzRCUyMGV4cG9ydCh0b2tlbml6ZXIlMkMlMjBiYXNlX21vZGVsJTJDJTIwb25ueF9jb25maWclMkMlMjBvbm54X2NvbmZpZy5kZWZhdWx0X29ubnhfb3BzZXQlMkMlMjBvbm54X3BhdGgp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.onnx <span class="hljs-keyword">import</span> export
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_path = Path(<span class="hljs-string">&quot;model.onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_ckpt = <span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>base_model = AutoModel.from_pretrained(model_ckpt)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_ckpt)

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_inputs, onnx_outputs = export(tokenizer, base_model, onnx_config, onnx_config.default_onnx_opset, onnx_path)`,wrap:!1}}),Ge=new b({props:{code:"aW1wb3J0JTIwb25ueCUwQSUwQW9ubnhfbW9kZWwlMjAlM0QlMjBvbm54LmxvYWQoJTIybW9kZWwub25ueCUyMiklMEFvbm54LmNoZWNrZXIuY2hlY2tfbW9kZWwob25ueF9tb2RlbCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> onnx

<span class="hljs-meta">&gt;&gt;&gt; </span>onnx_model = onnx.load(<span class="hljs-string">&quot;model.onnx&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>onnx.checker.check_model(onnx_model)`,wrap:!1}}),B=new Gt({props:{$$slots:{default:[Ri]},$$scope:{ctx:w}}}),Re=new $({props:{title:"Convalida degli output del modello",local:"convalida-degli-output-del-modello",headingTag:"h4"}}),He=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5vbm54JTIwaW1wb3J0JTIwdmFsaWRhdGVfbW9kZWxfb3V0cHV0cyUwQSUwQXZhbGlkYXRlX21vZGVsX291dHB1dHMoJTBBJTIwJTIwJTIwJTIwb25ueF9jb25maWclMkMlMjB0b2tlbml6ZXIlMkMlMjBiYXNlX21vZGVsJTJDJTIwb25ueF9wYXRoJTJDJTIwb25ueF9vdXRwdXRzJTJDJTIwb25ueF9jb25maWcuYXRvbF9mb3JfdmFsaWRhdGlvbiUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.onnx <span class="hljs-keyword">import</span> validate_model_outputs

<span class="hljs-meta">&gt;&gt;&gt; </span>validate_model_outputs(
<span class="hljs-meta">... </span>    onnx_config, tokenizer, base_model, onnx_path, onnx_outputs, onnx_config.atol_for_validation
<span class="hljs-meta">... </span>)`,wrap:!1}}),Qe=new $({props:{title:"Contribuire con una nuova configurazione a ü§ó Transformers",local:"contribuire-con-una-nuova-configurazione-a--transformers",headingTag:"h3"}}),qe=new $({props:{title:"TorchScript",local:"torchscript",headingTag:"h2"}}),x=new Gt({props:{$$slots:{default:[Vi]},$$scope:{ctx:w}}}),et=new $({props:{title:"Flag TorchScript e pesi legati",local:"flag-torchscript-e-pesi-legati",headingTag:"h3"}}),st=new $({props:{title:"Input fittizi e standard lengths",local:"input-fittizi-e-standard-lengths",headingTag:"h3"}}),ct=new $({props:{title:"Usare TorchSscript in Python",local:"usare-torchsscript-in-python",headingTag:"h3"}}),dt=new $({props:{title:"Salvare un modello",local:"salvare-un-modello",headingTag:"h4"}}),Mt=new b({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel, BertTokenizer, BertConfig
<span class="hljs-keyword">import</span> torch

enc = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)

<span class="hljs-comment"># Tokenizing input text</span>
text = <span class="hljs-string">&quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;</span>
tokenized_text = enc.tokenize(text)

<span class="hljs-comment"># Masking one of the input tokens</span>
masked_index = <span class="hljs-number">8</span>
tokenized_text[masked_index] = <span class="hljs-string">&quot;[MASK]&quot;</span>
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]

<span class="hljs-comment"># Creating a dummy input</span>
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

<span class="hljs-comment"># Initializing the model with the torchscript flag</span>
<span class="hljs-comment"># Flag set to True even though it is not necessary as this model does not have an LM Head.</span>
config = BertConfig(
    vocab_size_or_config_json_file=<span class="hljs-number">32000</span>,
    hidden_size=<span class="hljs-number">768</span>,
    num_hidden_layers=<span class="hljs-number">12</span>,
    num_attention_heads=<span class="hljs-number">12</span>,
    intermediate_size=<span class="hljs-number">3072</span>,
    torchscript=<span class="hljs-literal">True</span>,
)

<span class="hljs-comment"># Instantiating the model</span>
model = BertModel(config)

<span class="hljs-comment"># The model needs to be in evaluation mode</span>
model.<span class="hljs-built_in">eval</span>()

<span class="hljs-comment"># If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag</span>
model = BertModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>, torchscript=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Creating the trace</span>
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, <span class="hljs-string">&quot;traced_bert.pt&quot;</span>)`,wrap:!1}}),ft=new $({props:{title:"Caricare un modello",local:"caricare-un-modello",headingTag:"h4"}}),ht=new b({props:{code:"bG9hZGVkX21vZGVsJTIwJTNEJTIwdG9yY2guaml0LmxvYWQoJTIydHJhY2VkX2JlcnQucHQlMjIpJTBBbG9hZGVkX21vZGVsLmV2YWwoKSUwQSUwQWFsbF9lbmNvZGVyX2xheWVycyUyQyUyMHBvb2xlZF9vdXRwdXQlMjAlM0QlMjBsb2FkZWRfbW9kZWwoKmR1bW15X2lucHV0KQ==",highlighted:`loaded_model = torch.jit.load(<span class="hljs-string">&quot;traced_bert.pt&quot;</span>)
loaded_model.<span class="hljs-built_in">eval</span>()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)`,wrap:!1}}),yt=new $({props:{title:"Utilizzare un modello tracciato per l‚Äôinferenza",local:"utilizzare-un-modello-tracciato-per-linferenza",headingTag:"h4"}}),bt=new b({props:{code:"dHJhY2VkX21vZGVsKHRva2Vuc190ZW5zb3IlMkMlMjBzZWdtZW50c190ZW5zb3JzKQ==",highlighted:"traced_model(tokens_tensor, segments_tensors)",wrap:!1}}),Tt=new $({props:{title:"Implementare modelli HuggingFace TorchScript su AWS utilizzando Neuron SDK",local:"implementare-modelli-huggingface-torchscript-su-aws-utilizzando-neuron-sdk",headingTag:"h3"}}),$t=new $({props:{title:"Implicazioni",local:"implicazioni",headingTag:"h4"}}),vt=new $({props:{title:"Dipendenze",local:"dipendenze",headingTag:"h4"}}),Zt=new $({props:{title:"Convertire un modello per AWS Neuron",local:"convertire-un-modello-per-aws-neuron",headingTag:"h4"}}),kt=new b({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlcnRNb2RlbCUyQyUyMEJlcnRUb2tlbml6ZXIlMkMlMjBCZXJ0Q29uZmlnJTBBaW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjB0b3JjaC5uZXVyb24=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel, BertTokenizer, BertConfig
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.neuron`,wrap:!1}}),_t=new b({props:{code:"dG9yY2guaml0LnRyYWNlKG1vZGVsJTJDJTIwJTVCdG9rZW5zX3RlbnNvciUyQyUyMHNlZ21lbnRzX3RlbnNvcnMlNUQp",highlighted:"torch.jit.trace(model, [tokens_tensor, segments_tensors])",wrap:!1}}),Xt=new b({props:{code:"dG9yY2gubmV1cm9uLnRyYWNlKG1vZGVsJTJDJTIwJTVCdG9rZW5fdGVuc29yJTJDJTIwc2VnbWVudHNfdGVuc29ycyU1RCk=",highlighted:"torch.neuron.trace(model, [token_tensor, segments_tensors])",wrap:!1}}),{c(){a=r("meta"),y=s(),o=r("p"),j=s(),m(T.$$.fragment),J=s(),U=r("p"),U.textContent=h,v=s(),W=r("p"),W.innerHTML=es,Vt=s(),m(X.$$.fragment),Ht=s(),A=r("p"),A.innerHTML=ts,Et=s(),N=r("p"),N.textContent=ls,Qt=s(),G=r("p"),G.innerHTML=ns,Lt=s(),R=r("p"),R.textContent=ss,St=s(),V=r("ul"),V.innerHTML=is,Ft=s(),H=r("p"),H.textContent=as,qt=s(),E=r("ul"),E.innerHTML=os,Yt=s(),m(Q.$$.fragment),Pt=s(),L=r("p"),L.textContent=rs,Dt=s(),m(S.$$.fragment),Ot=s(),F=r("p"),F.innerHTML=ps,Kt=s(),m(q.$$.fragment),el=s(),Y=r("p"),Y.textContent=cs,tl=s(),m(P.$$.fragment),ll=s(),D=r("p"),D.textContent=ms,nl=s(),m(O.$$.fragment),sl=s(),K=r("p"),K.innerHTML=ds,il=s(),ee=r("p"),ee.innerHTML=us,al=s(),m(te.$$.fragment),ol=s(),le=r("p"),le.innerHTML=Ms,rl=s(),m(ne.$$.fragment),pl=s(),se=r("p"),se.innerHTML=fs,cl=s(),m(ie.$$.fragment),ml=s(),ae=r("p"),ae.textContent=gs,dl=s(),m(z.$$.fragment),ul=s(),m(oe.$$.fragment),Ml=s(),re=r("p"),re.innerHTML=hs,fl=s(),pe=r("table"),pe.innerHTML=ys,gl=s(),ce=r("p"),ce.innerHTML=js,hl=s(),m(me.$$.fragment),yl=s(),de=r("p"),de.innerHTML=bs,jl=s(),m(ue.$$.fragment),bl=s(),Me=r("p"),Me.textContent=Ts,Tl=s(),m(fe.$$.fragment),wl=s(),ge=r("p"),ge.innerHTML=ws,Jl=s(),m(C.$$.fragment),$l=s(),m(he.$$.fragment),Ul=s(),ye=r("p"),ye.textContent=Js,vl=s(),je=r("ol"),je.innerHTML=$s,zl=s(),be=r("p"),be.textContent=Us,Cl=s(),m(Te.$$.fragment),Zl=s(),we=r("p"),we.textContent=vs,Il=s(),Je=r("ul"),Je.innerHTML=zs,kl=s(),m(Z.$$.fragment),Bl=s(),$e=r("p"),$e.innerHTML=Cs,xl=s(),m(Ue.$$.fragment),_l=s(),ve=r("p"),ve.innerHTML=Zs,Wl=s(),m(I.$$.fragment),Xl=s(),ze=r("p"),ze.textContent=Is,Al=s(),m(Ce.$$.fragment),Nl=s(),Ze=r("p"),Ze.textContent=ks,Gl=s(),m(Ie.$$.fragment),Rl=s(),ke=r("p"),ke.textContent=Bs,Vl=s(),m(Be.$$.fragment),Hl=s(),xe=r("p"),xe.innerHTML=xs,El=s(),m(_e.$$.fragment),Ql=s(),m(k.$$.fragment),Ll=s(),m(We.$$.fragment),Sl=s(),Xe=r("p"),Xe.innerHTML=_s,Fl=s(),m(Ae.$$.fragment),ql=s(),Ne=r("p"),Ne.innerHTML=Ws,Yl=s(),m(Ge.$$.fragment),Pl=s(),m(B.$$.fragment),Dl=s(),m(Re.$$.fragment),Ol=s(),Ve=r("p"),Ve.innerHTML=Xs,Kl=s(),m(He.$$.fragment),en=s(),Ee=r("p"),Ee.innerHTML=As,tn=s(),m(Qe.$$.fragment),ln=s(),Le=r("p"),Le.textContent=Ns,nn=s(),Se=r("ul"),Se.innerHTML=Gs,sn=s(),Fe=r("p"),Fe.innerHTML=Rs,an=s(),m(qe.$$.fragment),on=s(),m(x.$$.fragment),rn=s(),Ye=r("p"),Ye.innerHTML=Vs,pn=s(),Pe=r("p"),Pe.textContent=Hs,cn=s(),De=r("p"),De.textContent=Es,mn=s(),Oe=r("ul"),Oe.innerHTML=Qs,dn=s(),Ke=r("p"),Ke.textContent=Ls,un=s(),m(et.$$.fragment),Mn=s(),tt=r("p"),tt.textContent=Ss,fn=s(),lt=r("p"),lt.innerHTML=Fs,gn=s(),nt=r("p"),nt.innerHTML=qs,hn=s(),m(st.$$.fragment),yn=s(),it=r("p"),it.textContent=Ys,jn=s(),at=r("p"),at.textContent=Ps,bn=s(),ot=r("p"),ot.innerHTML=Ds,Tn=s(),rt=r("p"),rt.textContent=Os,wn=s(),pt=r("p"),pt.textContent=Ks,Jn=s(),m(ct.$$.fragment),$n=s(),mt=r("p"),mt.textContent=ei,Un=s(),m(dt.$$.fragment),vn=s(),ut=r("p"),ut.innerHTML=ti,zn=s(),m(Mt.$$.fragment),Cn=s(),m(ft.$$.fragment),Zn=s(),gt=r("p"),gt.innerHTML=li,In=s(),m(ht.$$.fragment),kn=s(),m(yt.$$.fragment),Bn=s(),jt=r("p"),jt.innerHTML=ni,xn=s(),m(bt.$$.fragment),_n=s(),m(Tt.$$.fragment),Wn=s(),wt=r("p"),wt.innerHTML=si,Xn=s(),Jt=r("ol"),Jt.innerHTML=ii,An=s(),m($t.$$.fragment),Nn=s(),Ut=r("p"),Ut.innerHTML=ai,Gn=s(),m(vt.$$.fragment),Rn=s(),zt=r("p"),zt.textContent=oi,Vn=s(),Ct=r("ul"),Ct.innerHTML=ri,Hn=s(),m(Zt.$$.fragment),En=s(),It=r("p"),It.innerHTML=pi,Qn=s(),m(kt.$$.fragment),Ln=s(),Bt=r("p"),Bt.textContent=ci,Sn=s(),xt=r("p"),xt.textContent=mi,Fn=s(),m(_t.$$.fragment),qn=s(),Wt=r("p"),Wt.textContent=di,Yn=s(),m(Xt.$$.fragment),Pn=s(),At=r("p"),At.textContent=ui,Dn=s(),Nt=r("p"),Nt.innerHTML=Mi,On=s(),Rt=r("p"),this.h()},l(e){const t=Zi("svelte-u9bgzb",document.head);a=p(t,"META",{name:!0,content:!0}),t.forEach(l),y=i(e),o=p(e,"P",{}),wi(o).forEach(l),j=i(e),d(T.$$.fragment,e),J=i(e),U=p(e,"P",{"data-svelte-h":!0}),c(U)!=="svelte-do19d6"&&(U.textContent=h),v=i(e),W=p(e,"P",{"data-svelte-h":!0}),c(W)!=="svelte-1f9txfv"&&(W.innerHTML=es),Vt=i(e),d(X.$$.fragment,e),Ht=i(e),A=p(e,"P",{"data-svelte-h":!0}),c(A)!=="svelte-1gdh4oo"&&(A.innerHTML=ts),Et=i(e),N=p(e,"P",{"data-svelte-h":!0}),c(N)!=="svelte-z64a5j"&&(N.textContent=ls),Qt=i(e),G=p(e,"P",{"data-svelte-h":!0}),c(G)!=="svelte-7rl71e"&&(G.innerHTML=ns),Lt=i(e),R=p(e,"P",{"data-svelte-h":!0}),c(R)!=="svelte-sr712u"&&(R.textContent=ss),St=i(e),V=p(e,"UL",{"data-svelte-h":!0}),c(V)!=="svelte-vozkex"&&(V.innerHTML=is),Ft=i(e),H=p(e,"P",{"data-svelte-h":!0}),c(H)!=="svelte-1sn7n41"&&(H.textContent=as),qt=i(e),E=p(e,"UL",{"data-svelte-h":!0}),c(E)!=="svelte-1g8hsur"&&(E.innerHTML=os),Yt=i(e),d(Q.$$.fragment,e),Pt=i(e),L=p(e,"P",{"data-svelte-h":!0}),c(L)!=="svelte-4psczp"&&(L.textContent=rs),Dt=i(e),d(S.$$.fragment,e),Ot=i(e),F=p(e,"P",{"data-svelte-h":!0}),c(F)!=="svelte-1j72zvu"&&(F.innerHTML=ps),Kt=i(e),d(q.$$.fragment,e),el=i(e),Y=p(e,"P",{"data-svelte-h":!0}),c(Y)!=="svelte-13jbmoq"&&(Y.textContent=cs),tl=i(e),d(P.$$.fragment,e),ll=i(e),D=p(e,"P",{"data-svelte-h":!0}),c(D)!=="svelte-wxqvf3"&&(D.textContent=ms),nl=i(e),d(O.$$.fragment,e),sl=i(e),K=p(e,"P",{"data-svelte-h":!0}),c(K)!=="svelte-16irxk2"&&(K.innerHTML=ds),il=i(e),ee=p(e,"P",{"data-svelte-h":!0}),c(ee)!=="svelte-rdoavc"&&(ee.innerHTML=us),al=i(e),d(te.$$.fragment,e),ol=i(e),le=p(e,"P",{"data-svelte-h":!0}),c(le)!=="svelte-1t39lef"&&(le.innerHTML=Ms),rl=i(e),d(ne.$$.fragment,e),pl=i(e),se=p(e,"P",{"data-svelte-h":!0}),c(se)!=="svelte-ej8fgq"&&(se.innerHTML=fs),cl=i(e),d(ie.$$.fragment,e),ml=i(e),ae=p(e,"P",{"data-svelte-h":!0}),c(ae)!=="svelte-h5zd9u"&&(ae.textContent=gs),dl=i(e),d(z.$$.fragment,e),ul=i(e),d(oe.$$.fragment,e),Ml=i(e),re=p(e,"P",{"data-svelte-h":!0}),c(re)!=="svelte-ne5myz"&&(re.innerHTML=hs),fl=i(e),pe=p(e,"TABLE",{"data-svelte-h":!0}),c(pe)!=="svelte-1n68aa"&&(pe.innerHTML=ys),gl=i(e),ce=p(e,"P",{"data-svelte-h":!0}),c(ce)!=="svelte-6wtiyv"&&(ce.innerHTML=js),hl=i(e),d(me.$$.fragment,e),yl=i(e),de=p(e,"P",{"data-svelte-h":!0}),c(de)!=="svelte-3qfl60"&&(de.innerHTML=bs),jl=i(e),d(ue.$$.fragment,e),bl=i(e),Me=p(e,"P",{"data-svelte-h":!0}),c(Me)!=="svelte-lgu95w"&&(Me.textContent=Ts),Tl=i(e),d(fe.$$.fragment,e),wl=i(e),ge=p(e,"P",{"data-svelte-h":!0}),c(ge)!=="svelte-13p6mbz"&&(ge.innerHTML=ws),Jl=i(e),d(C.$$.fragment,e),$l=i(e),d(he.$$.fragment,e),Ul=i(e),ye=p(e,"P",{"data-svelte-h":!0}),c(ye)!=="svelte-17r0w1i"&&(ye.textContent=Js),vl=i(e),je=p(e,"OL",{"data-svelte-h":!0}),c(je)!=="svelte-z67jz4"&&(je.innerHTML=$s),zl=i(e),be=p(e,"P",{"data-svelte-h":!0}),c(be)!=="svelte-10yqqq8"&&(be.textContent=Us),Cl=i(e),d(Te.$$.fragment,e),Zl=i(e),we=p(e,"P",{"data-svelte-h":!0}),c(we)!=="svelte-trahak"&&(we.textContent=vs),Il=i(e),Je=p(e,"UL",{"data-svelte-h":!0}),c(Je)!=="svelte-j9of43"&&(Je.innerHTML=zs),kl=i(e),d(Z.$$.fragment,e),Bl=i(e),$e=p(e,"P",{"data-svelte-h":!0}),c($e)!=="svelte-1jb3jgn"&&($e.innerHTML=Cs),xl=i(e),d(Ue.$$.fragment,e),_l=i(e),ve=p(e,"P",{"data-svelte-h":!0}),c(ve)!=="svelte-13xj4dt"&&(ve.innerHTML=Zs),Wl=i(e),d(I.$$.fragment,e),Xl=i(e),ze=p(e,"P",{"data-svelte-h":!0}),c(ze)!=="svelte-w0rck2"&&(ze.textContent=Is),Al=i(e),d(Ce.$$.fragment,e),Nl=i(e),Ze=p(e,"P",{"data-svelte-h":!0}),c(Ze)!=="svelte-6wsqkw"&&(Ze.textContent=ks),Gl=i(e),d(Ie.$$.fragment,e),Rl=i(e),ke=p(e,"P",{"data-svelte-h":!0}),c(ke)!=="svelte-1ggtlqz"&&(ke.textContent=Bs),Vl=i(e),d(Be.$$.fragment,e),Hl=i(e),xe=p(e,"P",{"data-svelte-h":!0}),c(xe)!=="svelte-1kn83vw"&&(xe.innerHTML=xs),El=i(e),d(_e.$$.fragment,e),Ql=i(e),d(k.$$.fragment,e),Ll=i(e),d(We.$$.fragment,e),Sl=i(e),Xe=p(e,"P",{"data-svelte-h":!0}),c(Xe)!=="svelte-1ocrgze"&&(Xe.innerHTML=_s),Fl=i(e),d(Ae.$$.fragment,e),ql=i(e),Ne=p(e,"P",{"data-svelte-h":!0}),c(Ne)!=="svelte-kxw9bj"&&(Ne.innerHTML=Ws),Yl=i(e),d(Ge.$$.fragment,e),Pl=i(e),d(B.$$.fragment,e),Dl=i(e),d(Re.$$.fragment,e),Ol=i(e),Ve=p(e,"P",{"data-svelte-h":!0}),c(Ve)!=="svelte-zllt9o"&&(Ve.innerHTML=Xs),Kl=i(e),d(He.$$.fragment,e),en=i(e),Ee=p(e,"P",{"data-svelte-h":!0}),c(Ee)!=="svelte-1c44gg3"&&(Ee.innerHTML=As),tn=i(e),d(Qe.$$.fragment,e),ln=i(e),Le=p(e,"P",{"data-svelte-h":!0}),c(Le)!=="svelte-lrvm1j"&&(Le.textContent=Ns),nn=i(e),Se=p(e,"UL",{"data-svelte-h":!0}),c(Se)!=="svelte-a8pd1a"&&(Se.innerHTML=Gs),sn=i(e),Fe=p(e,"P",{"data-svelte-h":!0}),c(Fe)!=="svelte-6wlbs5"&&(Fe.innerHTML=Rs),an=i(e),d(qe.$$.fragment,e),on=i(e),d(x.$$.fragment,e),rn=i(e),Ye=p(e,"P",{"data-svelte-h":!0}),c(Ye)!=="svelte-wgebc9"&&(Ye.innerHTML=Vs),pn=i(e),Pe=p(e,"P",{"data-svelte-h":!0}),c(Pe)!=="svelte-9p2ixq"&&(Pe.textContent=Hs),cn=i(e),De=p(e,"P",{"data-svelte-h":!0}),c(De)!=="svelte-6xn7nf"&&(De.textContent=Es),mn=i(e),Oe=p(e,"UL",{"data-svelte-h":!0}),c(Oe)!=="svelte-oy2125"&&(Oe.innerHTML=Qs),dn=i(e),Ke=p(e,"P",{"data-svelte-h":!0}),c(Ke)!=="svelte-m48voh"&&(Ke.textContent=Ls),un=i(e),d(et.$$.fragment,e),Mn=i(e),tt=p(e,"P",{"data-svelte-h":!0}),c(tt)!=="svelte-tz9w6u"&&(tt.textContent=Ss),fn=i(e),lt=p(e,"P",{"data-svelte-h":!0}),c(lt)!=="svelte-1mq7t8t"&&(lt.innerHTML=Fs),gn=i(e),nt=p(e,"P",{"data-svelte-h":!0}),c(nt)!=="svelte-1jmydvh"&&(nt.innerHTML=qs),hn=i(e),d(st.$$.fragment,e),yn=i(e),it=p(e,"P",{"data-svelte-h":!0}),c(it)!=="svelte-c132kv"&&(it.textContent=Ys),jn=i(e),at=p(e,"P",{"data-svelte-h":!0}),c(at)!=="svelte-1bggopo"&&(at.textContent=Ps),bn=i(e),ot=p(e,"P",{"data-svelte-h":!0}),c(ot)!=="svelte-vyea1g"&&(ot.innerHTML=Ds),Tn=i(e),rt=p(e,"P",{"data-svelte-h":!0}),c(rt)!=="svelte-1yce0u1"&&(rt.textContent=Os),wn=i(e),pt=p(e,"P",{"data-svelte-h":!0}),c(pt)!=="svelte-1sr04ao"&&(pt.textContent=Ks),Jn=i(e),d(ct.$$.fragment,e),$n=i(e),mt=p(e,"P",{"data-svelte-h":!0}),c(mt)!=="svelte-q23m3c"&&(mt.textContent=ei),Un=i(e),d(dt.$$.fragment,e),vn=i(e),ut=p(e,"P",{"data-svelte-h":!0}),c(ut)!=="svelte-k16qri"&&(ut.innerHTML=ti),zn=i(e),d(Mt.$$.fragment,e),Cn=i(e),d(ft.$$.fragment,e),Zn=i(e),gt=p(e,"P",{"data-svelte-h":!0}),c(gt)!=="svelte-wh65i"&&(gt.innerHTML=li),In=i(e),d(ht.$$.fragment,e),kn=i(e),d(yt.$$.fragment,e),Bn=i(e),jt=p(e,"P",{"data-svelte-h":!0}),c(jt)!=="svelte-1gnslzu"&&(jt.innerHTML=ni),xn=i(e),d(bt.$$.fragment,e),_n=i(e),d(Tt.$$.fragment,e),Wn=i(e),wt=p(e,"P",{"data-svelte-h":!0}),c(wt)!=="svelte-fjq776"&&(wt.innerHTML=si),Xn=i(e),Jt=p(e,"OL",{"data-svelte-h":!0}),c(Jt)!=="svelte-121gcdu"&&(Jt.innerHTML=ii),An=i(e),d($t.$$.fragment,e),Nn=i(e),Ut=p(e,"P",{"data-svelte-h":!0}),c(Ut)!=="svelte-nfv9dr"&&(Ut.innerHTML=ai),Gn=i(e),d(vt.$$.fragment,e),Rn=i(e),zt=p(e,"P",{"data-svelte-h":!0}),c(zt)!=="svelte-clfp9m"&&(zt.textContent=oi),Vn=i(e),Ct=p(e,"UL",{"data-svelte-h":!0}),c(Ct)!=="svelte-1yjzy16"&&(Ct.innerHTML=ri),Hn=i(e),d(Zt.$$.fragment,e),En=i(e),It=p(e,"P",{"data-svelte-h":!0}),c(It)!=="svelte-wberyk"&&(It.innerHTML=pi),Qn=i(e),d(kt.$$.fragment,e),Ln=i(e),Bt=p(e,"P",{"data-svelte-h":!0}),c(Bt)!=="svelte-e7n7t3"&&(Bt.textContent=ci),Sn=i(e),xt=p(e,"P",{"data-svelte-h":!0}),c(xt)!=="svelte-yedjtf"&&(xt.textContent=mi),Fn=i(e),d(_t.$$.fragment,e),qn=i(e),Wt=p(e,"P",{"data-svelte-h":!0}),c(Wt)!=="svelte-a9ap49"&&(Wt.textContent=di),Yn=i(e),d(Xt.$$.fragment,e),Pn=i(e),At=p(e,"P",{"data-svelte-h":!0}),c(At)!=="svelte-1pjct15"&&(At.textContent=ui),Dn=i(e),Nt=p(e,"P",{"data-svelte-h":!0}),c(Nt)!=="svelte-1do2s1n"&&(Nt.innerHTML=Mi),On=i(e),Rt=p(e,"P",{}),wi(Rt).forEach(l),this.h()},h(){Ji(a,"name","hf:doc:metadata"),Ji(a,"content",Ei)},m(e,t){Ii(document.head,a),n(e,y,t),n(e,o,t),n(e,j,t),u(T,e,t),n(e,J,t),n(e,U,t),n(e,v,t),n(e,W,t),n(e,Vt,t),u(X,e,t),n(e,Ht,t),n(e,A,t),n(e,Et,t),n(e,N,t),n(e,Qt,t),n(e,G,t),n(e,Lt,t),n(e,R,t),n(e,St,t),n(e,V,t),n(e,Ft,t),n(e,H,t),n(e,qt,t),n(e,E,t),n(e,Yt,t),u(Q,e,t),n(e,Pt,t),n(e,L,t),n(e,Dt,t),u(S,e,t),n(e,Ot,t),n(e,F,t),n(e,Kt,t),u(q,e,t),n(e,el,t),n(e,Y,t),n(e,tl,t),u(P,e,t),n(e,ll,t),n(e,D,t),n(e,nl,t),u(O,e,t),n(e,sl,t),n(e,K,t),n(e,il,t),n(e,ee,t),n(e,al,t),u(te,e,t),n(e,ol,t),n(e,le,t),n(e,rl,t),u(ne,e,t),n(e,pl,t),n(e,se,t),n(e,cl,t),u(ie,e,t),n(e,ml,t),n(e,ae,t),n(e,dl,t),u(z,e,t),n(e,ul,t),u(oe,e,t),n(e,Ml,t),n(e,re,t),n(e,fl,t),n(e,pe,t),n(e,gl,t),n(e,ce,t),n(e,hl,t),u(me,e,t),n(e,yl,t),n(e,de,t),n(e,jl,t),u(ue,e,t),n(e,bl,t),n(e,Me,t),n(e,Tl,t),u(fe,e,t),n(e,wl,t),n(e,ge,t),n(e,Jl,t),u(C,e,t),n(e,$l,t),u(he,e,t),n(e,Ul,t),n(e,ye,t),n(e,vl,t),n(e,je,t),n(e,zl,t),n(e,be,t),n(e,Cl,t),u(Te,e,t),n(e,Zl,t),n(e,we,t),n(e,Il,t),n(e,Je,t),n(e,kl,t),u(Z,e,t),n(e,Bl,t),n(e,$e,t),n(e,xl,t),u(Ue,e,t),n(e,_l,t),n(e,ve,t),n(e,Wl,t),u(I,e,t),n(e,Xl,t),n(e,ze,t),n(e,Al,t),u(Ce,e,t),n(e,Nl,t),n(e,Ze,t),n(e,Gl,t),u(Ie,e,t),n(e,Rl,t),n(e,ke,t),n(e,Vl,t),u(Be,e,t),n(e,Hl,t),n(e,xe,t),n(e,El,t),u(_e,e,t),n(e,Ql,t),u(k,e,t),n(e,Ll,t),u(We,e,t),n(e,Sl,t),n(e,Xe,t),n(e,Fl,t),u(Ae,e,t),n(e,ql,t),n(e,Ne,t),n(e,Yl,t),u(Ge,e,t),n(e,Pl,t),u(B,e,t),n(e,Dl,t),u(Re,e,t),n(e,Ol,t),n(e,Ve,t),n(e,Kl,t),u(He,e,t),n(e,en,t),n(e,Ee,t),n(e,tn,t),u(Qe,e,t),n(e,ln,t),n(e,Le,t),n(e,nn,t),n(e,Se,t),n(e,sn,t),n(e,Fe,t),n(e,an,t),u(qe,e,t),n(e,on,t),u(x,e,t),n(e,rn,t),n(e,Ye,t),n(e,pn,t),n(e,Pe,t),n(e,cn,t),n(e,De,t),n(e,mn,t),n(e,Oe,t),n(e,dn,t),n(e,Ke,t),n(e,un,t),u(et,e,t),n(e,Mn,t),n(e,tt,t),n(e,fn,t),n(e,lt,t),n(e,gn,t),n(e,nt,t),n(e,hn,t),u(st,e,t),n(e,yn,t),n(e,it,t),n(e,jn,t),n(e,at,t),n(e,bn,t),n(e,ot,t),n(e,Tn,t),n(e,rt,t),n(e,wn,t),n(e,pt,t),n(e,Jn,t),u(ct,e,t),n(e,$n,t),n(e,mt,t),n(e,Un,t),u(dt,e,t),n(e,vn,t),n(e,ut,t),n(e,zn,t),u(Mt,e,t),n(e,Cn,t),u(ft,e,t),n(e,Zn,t),n(e,gt,t),n(e,In,t),u(ht,e,t),n(e,kn,t),u(yt,e,t),n(e,Bn,t),n(e,jt,t),n(e,xn,t),u(bt,e,t),n(e,_n,t),u(Tt,e,t),n(e,Wn,t),n(e,wt,t),n(e,Xn,t),n(e,Jt,t),n(e,An,t),u($t,e,t),n(e,Nn,t),n(e,Ut,t),n(e,Gn,t),u(vt,e,t),n(e,Rn,t),n(e,zt,t),n(e,Vn,t),n(e,Ct,t),n(e,Hn,t),u(Zt,e,t),n(e,En,t),n(e,It,t),n(e,Qn,t),u(kt,e,t),n(e,Ln,t),n(e,Bt,t),n(e,Sn,t),n(e,xt,t),n(e,Fn,t),u(_t,e,t),n(e,qn,t),n(e,Wt,t),n(e,Yn,t),u(Xt,e,t),n(e,Pn,t),n(e,At,t),n(e,Dn,t),n(e,Nt,t),n(e,On,t),n(e,Rt,t),Kn=!0},p(e,[t]){const fi={};t&2&&(fi.$$scope={dirty:t,ctx:e}),z.$set(fi);const gi={};t&2&&(gi.$$scope={dirty:t,ctx:e}),C.$set(gi);const hi={};t&2&&(hi.$$scope={dirty:t,ctx:e}),Z.$set(hi);const yi={};t&2&&(yi.$$scope={dirty:t,ctx:e}),I.$set(yi);const ji={};t&2&&(ji.$$scope={dirty:t,ctx:e}),k.$set(ji);const bi={};t&2&&(bi.$$scope={dirty:t,ctx:e}),B.$set(bi);const Ti={};t&2&&(Ti.$$scope={dirty:t,ctx:e}),x.$set(Ti)},i(e){Kn||(M(T.$$.fragment,e),M(X.$$.fragment,e),M(Q.$$.fragment,e),M(S.$$.fragment,e),M(q.$$.fragment,e),M(P.$$.fragment,e),M(O.$$.fragment,e),M(te.$$.fragment,e),M(ne.$$.fragment,e),M(ie.$$.fragment,e),M(z.$$.fragment,e),M(oe.$$.fragment,e),M(me.$$.fragment,e),M(ue.$$.fragment,e),M(fe.$$.fragment,e),M(C.$$.fragment,e),M(he.$$.fragment,e),M(Te.$$.fragment,e),M(Z.$$.fragment,e),M(Ue.$$.fragment,e),M(I.$$.fragment,e),M(Ce.$$.fragment,e),M(Ie.$$.fragment,e),M(Be.$$.fragment,e),M(_e.$$.fragment,e),M(k.$$.fragment,e),M(We.$$.fragment,e),M(Ae.$$.fragment,e),M(Ge.$$.fragment,e),M(B.$$.fragment,e),M(Re.$$.fragment,e),M(He.$$.fragment,e),M(Qe.$$.fragment,e),M(qe.$$.fragment,e),M(x.$$.fragment,e),M(et.$$.fragment,e),M(st.$$.fragment,e),M(ct.$$.fragment,e),M(dt.$$.fragment,e),M(Mt.$$.fragment,e),M(ft.$$.fragment,e),M(ht.$$.fragment,e),M(yt.$$.fragment,e),M(bt.$$.fragment,e),M(Tt.$$.fragment,e),M($t.$$.fragment,e),M(vt.$$.fragment,e),M(Zt.$$.fragment,e),M(kt.$$.fragment,e),M(_t.$$.fragment,e),M(Xt.$$.fragment,e),Kn=!0)},o(e){f(T.$$.fragment,e),f(X.$$.fragment,e),f(Q.$$.fragment,e),f(S.$$.fragment,e),f(q.$$.fragment,e),f(P.$$.fragment,e),f(O.$$.fragment,e),f(te.$$.fragment,e),f(ne.$$.fragment,e),f(ie.$$.fragment,e),f(z.$$.fragment,e),f(oe.$$.fragment,e),f(me.$$.fragment,e),f(ue.$$.fragment,e),f(fe.$$.fragment,e),f(C.$$.fragment,e),f(he.$$.fragment,e),f(Te.$$.fragment,e),f(Z.$$.fragment,e),f(Ue.$$.fragment,e),f(I.$$.fragment,e),f(Ce.$$.fragment,e),f(Ie.$$.fragment,e),f(Be.$$.fragment,e),f(_e.$$.fragment,e),f(k.$$.fragment,e),f(We.$$.fragment,e),f(Ae.$$.fragment,e),f(Ge.$$.fragment,e),f(B.$$.fragment,e),f(Re.$$.fragment,e),f(He.$$.fragment,e),f(Qe.$$.fragment,e),f(qe.$$.fragment,e),f(x.$$.fragment,e),f(et.$$.fragment,e),f(st.$$.fragment,e),f(ct.$$.fragment,e),f(dt.$$.fragment,e),f(Mt.$$.fragment,e),f(ft.$$.fragment,e),f(ht.$$.fragment,e),f(yt.$$.fragment,e),f(bt.$$.fragment,e),f(Tt.$$.fragment,e),f($t.$$.fragment,e),f(vt.$$.fragment,e),f(Zt.$$.fragment,e),f(kt.$$.fragment,e),f(_t.$$.fragment,e),f(Xt.$$.fragment,e),Kn=!1},d(e){e&&(l(y),l(o),l(j),l(J),l(U),l(v),l(W),l(Vt),l(Ht),l(A),l(Et),l(N),l(Qt),l(G),l(Lt),l(R),l(St),l(V),l(Ft),l(H),l(qt),l(E),l(Yt),l(Pt),l(L),l(Dt),l(Ot),l(F),l(Kt),l(el),l(Y),l(tl),l(ll),l(D),l(nl),l(sl),l(K),l(il),l(ee),l(al),l(ol),l(le),l(rl),l(pl),l(se),l(cl),l(ml),l(ae),l(dl),l(ul),l(Ml),l(re),l(fl),l(pe),l(gl),l(ce),l(hl),l(yl),l(de),l(jl),l(bl),l(Me),l(Tl),l(wl),l(ge),l(Jl),l($l),l(Ul),l(ye),l(vl),l(je),l(zl),l(be),l(Cl),l(Zl),l(we),l(Il),l(Je),l(kl),l(Bl),l($e),l(xl),l(_l),l(ve),l(Wl),l(Xl),l(ze),l(Al),l(Nl),l(Ze),l(Gl),l(Rl),l(ke),l(Vl),l(Hl),l(xe),l(El),l(Ql),l(Ll),l(Sl),l(Xe),l(Fl),l(ql),l(Ne),l(Yl),l(Pl),l(Dl),l(Ol),l(Ve),l(Kl),l(en),l(Ee),l(tn),l(ln),l(Le),l(nn),l(Se),l(sn),l(Fe),l(an),l(on),l(rn),l(Ye),l(pn),l(Pe),l(cn),l(De),l(mn),l(Oe),l(dn),l(Ke),l(un),l(Mn),l(tt),l(fn),l(lt),l(gn),l(nt),l(hn),l(yn),l(it),l(jn),l(at),l(bn),l(ot),l(Tn),l(rt),l(wn),l(pt),l(Jn),l($n),l(mt),l(Un),l(vn),l(ut),l(zn),l(Cn),l(Zn),l(gt),l(In),l(kn),l(Bn),l(jt),l(xn),l(_n),l(Wn),l(wt),l(Xn),l(Jt),l(An),l(Nn),l(Ut),l(Gn),l(Rn),l(zt),l(Vn),l(Ct),l(Hn),l(En),l(It),l(Qn),l(Ln),l(Bt),l(Sn),l(xt),l(Fn),l(qn),l(Wt),l(Yn),l(Pn),l(At),l(Dn),l(Nt),l(On),l(Rt)),l(a),g(T,e),g(X,e),g(Q,e),g(S,e),g(q,e),g(P,e),g(O,e),g(te,e),g(ne,e),g(ie,e),g(z,e),g(oe,e),g(me,e),g(ue,e),g(fe,e),g(C,e),g(he,e),g(Te,e),g(Z,e),g(Ue,e),g(I,e),g(Ce,e),g(Ie,e),g(Be,e),g(_e,e),g(k,e),g(We,e),g(Ae,e),g(Ge,e),g(B,e),g(Re,e),g(He,e),g(Qe,e),g(qe,e),g(x,e),g(et,e),g(st,e),g(ct,e),g(dt,e),g(Mt,e),g(ft,e),g(ht,e),g(yt,e),g(bt,e),g(Tt,e),g($t,e),g(vt,e),g(Zt,e),g(kt,e),g(_t,e),g(Xt,e)}}}const Ei='{"title":"Esporta modelli ü§ó Transformers","local":"esporta-modelli--transformers","sections":[{"title":"ONNX","local":"onnx","sections":[{"title":"Esportazione di un modello in ONNX","local":"esportazione-di-un-modello-in-onnx","sections":[],"depth":3},{"title":"Selezione delle caratteristiche per diverse topologie di modello","local":"selezione-delle-caratteristiche-per-diverse-topologie-di-modello","sections":[],"depth":3},{"title":"Esportazione di un modello per un‚Äôarchitettura non supportata","local":"esportazione-di-un-modello-per-unarchitettura-non-supportata","sections":[{"title":"Implementazione di una configurazione ONNX personalizzata","local":"implementazione-di-una-configurazione-onnx-personalizzata","sections":[],"depth":4},{"title":"Esportazione del modello","local":"esportazione-del-modello","sections":[],"depth":4},{"title":"Convalida degli output del modello","local":"convalida-degli-output-del-modello","sections":[],"depth":4}],"depth":3},{"title":"Contribuire con una nuova configurazione a ü§ó Transformers","local":"contribuire-con-una-nuova-configurazione-a--transformers","sections":[],"depth":3}],"depth":2},{"title":"TorchScript","local":"torchscript","sections":[{"title":"Flag TorchScript e pesi legati","local":"flag-torchscript-e-pesi-legati","sections":[],"depth":3},{"title":"Input fittizi e standard lengths","local":"input-fittizi-e-standard-lengths","sections":[],"depth":3},{"title":"Usare TorchSscript in Python","local":"usare-torchsscript-in-python","sections":[{"title":"Salvare un modello","local":"salvare-un-modello","sections":[],"depth":4},{"title":"Caricare un modello","local":"caricare-un-modello","sections":[],"depth":4},{"title":"Utilizzare un modello tracciato per l‚Äôinferenza","local":"utilizzare-un-modello-tracciato-per-linferenza","sections":[],"depth":4}],"depth":3},{"title":"Implementare modelli HuggingFace TorchScript su AWS utilizzando Neuron SDK","local":"implementare-modelli-huggingface-torchscript-su-aws-utilizzando-neuron-sdk","sections":[{"title":"Implicazioni","local":"implicazioni","sections":[],"depth":4},{"title":"Dipendenze","local":"dipendenze","sections":[],"depth":4},{"title":"Convertire un modello per AWS Neuron","local":"convertire-un-modello-per-aws-neuron","sections":[],"depth":4}],"depth":3}],"depth":2}],"depth":1}';function Qi(w){return vi(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Di extends zi{constructor(a){super(),Ci(this,a,Qi,Hi,Ui,{})}}export{Di as component};
