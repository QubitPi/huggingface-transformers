import{s as V,n as G,o as Q}from"../chunks/scheduler.36a0863c.js";import{S as q,i as $,g as b,s as i,r as z,A as j,h as m,f as o,c as r,j as x,u as R,x as F,k as O,y as U,a,v as P,d as D,t as E,w as L}from"../chunks/index.9c13489a.js";import{H as B}from"../chunks/Heading.7a254a62.js";function H(N){let l,p,f,_,n,w,s,S="Questa pagina raggruppa le risorse sviluppate dalla comunit√† riguardo ü§ó Transformers.",T,g,v,d,I='<thead><tr><th align="left">Risorsa</th> <th align="left">Descrizione</th> <th align="right">Autore</th></tr></thead> <tbody><tr><td align="left"><a href="https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards" rel="nofollow">Glossario delle Flashcards di Transformers</a></td> <td align="left">Un insieme di flashcards basate sul <a href="glossary">glossario della documentazione di Transformers</a>, creato in un formato tale da permettere un facile apprendimento e revisione usando <a href="https://apps.ankiweb.net/" rel="nofollow">Anki</a>, un‚Äôapplicazione open-source e multi-piattaforma, specificatamente progettata per ricordare informazioni nel lungo termine. Guarda questo <a href="https://www.youtube.com/watch?v=Dji_h7PILrw" rel="nofollow">video introduttivo su come usare le flashcards</a>.</td> <td align="right"><a href="https://www.darigovresearch.com/" rel="nofollow">Darigov Research</a></td></tr></tbody>',y,c,C,h,M='<thead><tr><th align="left">Notebook</th> <th align="left">Descrizione</th> <th align="left">Autore</th> <th align="right"></th></tr></thead> <tbody><tr><td align="left"><a href="https://github.com/AlekseyKorshuk/huggingartists" rel="nofollow">Fine-tuning di un Transformer pre-addestrato, al fine di generare testi di canzoni</a></td> <td align="left">Come generare testi di canzoni nello stile del vostro artista preferito attraverso il fine-tuning di un modello GPT-2.</td> <td align="left"><a href="https://github.com/AlekseyKorshuk" rel="nofollow">Aleksey Korshuk</a></td> <td align="right"><a href="https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/snapthat/TF-T5-text-to-text" rel="nofollow">Addestramento di T5 in Tensorflow 2</a></td> <td align="left">Come addestrare T5 per qualsiasi attivit√† usando Tensorflow 2. Questo notebook mostra come risolvere l‚Äôattivit√† di ‚ÄúQuestion Answering‚Äù usando Tensorflow 2 e SQUAD.</td> <td align="left"><a href="https://github.com/HarrisDePerceptron" rel="nofollow">Muhammad Harris</a></td> <td align="right"><a href="https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb" rel="nofollow">Addestramento di T5 con TPU</a></td> <td align="left">Come addestrare T5 su SQUAD con Transformers e NLP.</td> <td align="left"><a href="https://github.com/patil-suraj" rel="nofollow">Suraj Patil</a></td> <td align="right"><a href="https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb" rel="nofollow">Fine-tuning di T5 per la classificazione e scelta multipla</a></td> <td align="left">Come effettuare il fine-tuning di T5 per le attivit√† di classificazione a scelta multipla - usando un formato testo-a-testo - con PyTorch Lightning.</td> <td align="left"><a href="https://github.com/patil-suraj" rel="nofollow">Suraj Patil</a></td> <td align="right"><a href="https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb" rel="nofollow">Fine-tuning di DialoGPT su nuovi dataset e lingue</a></td> <td align="left">Come effettuare il fine-tuning di un modello DialoGPT su un nuovo dataset per chatbots conversazionali open-dialog.</td> <td align="left"><a href="https://github.com/ncoop57" rel="nofollow">Nathan Cooper</a></td> <td align="right"><a href="https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb" rel="nofollow">Modellamento di una lunga sequenza con Reformer</a></td> <td align="left">Come addestrare su sequenze di lunghezza fino a 500 mila token con Reformer.</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb" rel="nofollow">Fine-tuning di BART per riassumere testi</a></td> <td align="left">Come effettuare il fine-tuning di BART per riassumere testi con fastai usando blurr.</td> <td align="left"><a href="https://ohmeow.com/" rel="nofollow">Wayde Gilliam</a></td> <td align="right"><a href="https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/_notebooks/2020-05-23-text-generation-with-blurr.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb" rel="nofollow">Fine-tuning di un Transformer pre-addestrato su tweet</a></td> <td align="left">Come generare tweet nello stile del tuo account Twitter preferito attraverso il fine-tuning di un modello GPT-2.</td> <td align="left"><a href="https://github.com/borisdayma" rel="nofollow">Boris Dayma</a></td> <td align="right"><a href="https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb" rel="nofollow">Ottimizzazione di modelli ü§ó Hugging Face con Weights &amp; Biases</a></td> <td align="left">Un tutorial completo che mostra l‚Äôintegrazione di W&amp;B con Hugging Face.</td> <td align="left"><a href="https://github.com/borisdayma" rel="nofollow">Boris Dayma</a></td> <td align="right"><a href="https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb" rel="nofollow">Longformer pre-addestrato</a></td> <td align="left">Come costruire una versione ‚Äúlong‚Äù degli esistenti modelli pre-addestrati.</td> <td align="left"><a href="https://beltagy.net" rel="nofollow">Iz Beltagy</a></td> <td align="right"><a href="https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb" rel="nofollow">Fine-tuning di Longformer per QA</a></td> <td align="left">Come effettuare il fine-tuning di un modello longformer per un task di QA.</td> <td align="left"><a href="https://github.com/patil-suraj" rel="nofollow">Suraj Patil</a></td> <td align="right"><a href="https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb" rel="nofollow">Valutazione di modelli con ü§óNLP</a></td> <td align="left">Come valutare longformer su TriviaQA con <code>NLP</code>.</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb" rel="nofollow">Fine-tuning di T5 per Sentiment Span Extraction</a></td> <td align="left">Come effettuare il fine-tuning di T5 per la sentiment span extraction - usando un formato testo-a-testo - con PyTorch Lightning.</td> <td align="left"><a href="https://github.com/enzoampil" rel="nofollow">Lorenzo Ampil</a></td> <td align="right"><a href="https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb" rel="nofollow">Fine-tuning di DistilBert per la classificazione multi-classe</a></td> <td align="left">Come effettuare il fine-tuning di DistilBert per la classificazione multi-classe con PyTorch.</td> <td align="left"><a href="https://github.com/abhimishra91" rel="nofollow">Abhishek Kumar Mishra</a></td> <td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb" rel="nofollow">Fine-tuning di BERT per la classificazione multi-etichetta</a></td> <td align="left">Come effettuare il fine-tuning di BERT per la classificazione multi-etichetta con PyTorch.</td> <td align="left"><a href="https://github.com/abhimishra91" rel="nofollow">Abhishek Kumar Mishra</a></td> <td align="right"><a href="https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb" rel="nofollow">Accelerazione del fine-tuning con il Dynamic Padding / Bucketing</a></td> <td align="left">Come velocizzare il fine-tuning di un fattore 2X usando il dynamic padding / bucketing.</td> <td align="left"><a href="https://github.com/pommedeterresautee" rel="nofollow">Michael Benesty</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb" rel="nofollow">Pre-addestramento di Reformer per Masked Language Modeling</a></td> <td align="left">Come addestrare un modello Reformer usando livelli di self-attention bi-direzionali.</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb" rel="nofollow">Espansione e fine-tuning di Sci-BERT</a></td> <td align="left">Come incrementare il vocabolario di un modello SciBERT - pre-addestrato da AllenAI sul dataset CORD - e crearne una pipeline.</td> <td align="left"><a href="https://github.com/lordtt13" rel="nofollow">Tanmay Thakur</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb" rel="nofollow">Fine-tuning di BlenderBotSmall per riassumere testi usando Trainer API</a></td> <td align="left">Come effettuare il fine-tuning di BlenderBotSmall per riassumere testi su un dataset personalizzato, usando Trainer API.</td> <td align="left"><a href="https://github.com/lordtt13" rel="nofollow">Tanmay Thakur</a></td> <td align="right"><a href="https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb" rel="nofollow">Fine-tuning di Electra e interpretazione con Integrated Gradients</a></td> <td align="left">Come effettuare il fine-tuning di Electra per l‚Äôanalisi dei sentimenti e intepretare le predizioni con Captum Integrated Gradients.</td> <td align="left"><a href="https://elsanns.github.io" rel="nofollow">Eliza Szczechla</a></td> <td align="right"><a href="https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb" rel="nofollow">Fine-tuning di un modello GPT-2 non inglese con la classe Trainer</a></td> <td align="left">Come effettuare il fine-tuning di un modello GPT-2 non inglese con la classe Trainer.</td> <td align="left"><a href="https://www.philschmid.de" rel="nofollow">Philipp Schmid</a></td> <td align="right"><a href="https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb" rel="nofollow">Fine-tuning di un modello DistilBERT per la classficazione multi-etichetta</a></td> <td align="left">Come effettuare il fine-tuning di un modello DistilBERT per l‚Äôattivit√† di classificazione multi-etichetta.</td> <td align="left"><a href="https://github.com/DhavalTaunk08" rel="nofollow">Dhaval Taunk</a></td> <td align="right"><a href="https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb" rel="nofollow">Fine-tuning di ALBERT per la classifcazione di coppie di frasi</a></td> <td align="left">Come effettuare il fine-tuning di un modello ALBERT - o un altro modello BERT-based - per l‚Äôattivit√† di classificazione di coppie di frasi.</td> <td align="left"><a href="https://github.com/NadirEM" rel="nofollow">Nadir El Manouzi</a></td> <td align="right"><a href="https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb" rel="nofollow">Fine-tuning di Roberta per l‚Äôanalisi di sentimenti</a></td> <td align="left">Come effettuare il fine-tuning di un modello Roberta per l‚Äôanalisi di sentimenti.</td> <td align="left"><a href="https://github.com/DhavalTaunk08" rel="nofollow">Dhaval Taunk</a></td> <td align="right"><a href="https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/flexudy-pipe/qugeev" rel="nofollow">Valutazione di modelli che generano domande</a></td> <td align="left">Quanto sono accurante le risposte alle domande generate dal tuo modello transformer seq2seq?</td> <td align="left"><a href="https://github.com/zolekode" rel="nofollow">Pascal Zoleko</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb" rel="nofollow">Classificazione di testo con DistilBERT e Tensorflow</a></td> <td align="left">Come effettuare il fine-tuning di DistilBERT per la classificazione di testo in TensorFlow.</td> <td align="left"><a href="https://github.com/peterbayerle" rel="nofollow">Peter Bayerle</a></td> <td align="right"><a href="https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb" rel="nofollow">Utilizzo di BERT per riassumere testi con un modello Encoder-Decoder su CNN/Dailymail</a></td> <td align="left">Come avviare ‚Äúa caldo‚Äù un <em>EncoderDecoderModel</em> attraverso l‚Äôutilizzo di un checkpoint <em>google-bert/bert-base-uncased</em> per riassumere testi su CNN/Dailymail.</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Aprilo in Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb" rel="nofollow">Utilizzo di RoBERTa per riassumere testi con un modello Encoder-Decoder su BBC XSum</a></td> <td align="left">Come avviare ‚Äúa caldo‚Äù un <em>EncoderDecoderModel</em> (condiviso) attraverso l‚Äôutilizzo di un checkpoint <em>FacebookAI/roberta-base</em> per riassumere testi su BBC/XSum.</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb" rel="nofollow">Fine-tuning di TAPAS su Sequential Question Answering (SQA)</a></td> <td align="left">Come effettuare il fine-tuning di un modello <em>TapasForQuestionAnswering</em> attraverso l‚Äôutilizzo di un checkpoint <em>tapas-base</em> sul dataset Sequential Question Answering (SQA).</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb" rel="nofollow">Valutazione di TAPAS su Table Fact Checking (TabFact)</a></td> <td align="left">Come valutare un modello <em>TapasForSequenceClassification</em> - fine-tuned con un checkpoint <em>tapas-base-finetuned-tabfact</em> - usando una combinazione delle librerie ü§ó datasets e ü§ó transformers.</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb" rel="nofollow">Fine-tuning di mBART per la traduzione</a></td> <td align="left">Come effettuare il fine-tuning di mBART usando Seq2SeqTrainer per la traduzione da hindi a inglese.</td> <td align="left"><a href="https://github.com/vasudevgupta7" rel="nofollow">Vasudev Gupta</a></td> <td align="right"><a href="https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb" rel="nofollow">Fine-tuning di LayoutLM su FUNSD (un dataset per la comprensione della forma)</a></td> <td align="left">Come effettuare il fine-tuning di un modello <em>LayoutLMForTokenClassification</em> sul dataset FUNSD per l‚Äôestrazione di informazioni da documenti scannerizzati.</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb" rel="nofollow">Fine-tuning di DistilGPT2 e generazione di testo</a></td> <td align="left">Come effettuare il fine-tuning di DistilGPT2 e generare testo.</td> <td align="left"><a href="https://github.com/tripathiaakash" rel="nofollow">Aakash Tripathi</a></td> <td align="right"><a href="https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb" rel="nofollow">Fine-tuning di LED fino a 8 mila token</a></td> <td align="left">Come effettuare il fine-tuning di LED su PubMed per riassumere ‚Äúlunghi‚Äù testi.</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb" rel="nofollow">Valutazione di LED su Arxiv</a></td> <td align="left">Come valutare efficacemente LED sull‚Äôattivit√† di riassumere ‚Äúlunghi‚Äù testi.</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb" rel="nofollow">Fine-tuning di LayoutLM su RVL-CDIP, un dataset per la classificazione di documenti (immagini)</a></td> <td align="left">Come effettuare il fine-tuning di un modello <em>LayoutLMForSequenceClassification</em> sul dataset RVL-CDIP per la classificazione di documenti scannerizzati.</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb" rel="nofollow">Decodifica Wav2Vec2 CTC con variazioni di GPT2</a></td> <td align="left">Come decodificare sequenze CTC, variate da modelli di linguaggio.</td> <td align="left"><a href="https://github.com/voidful" rel="nofollow">Eric Lam</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb" rel="nofollow">Fine-tuning di BART per riassumere testi in due lingue con la classe Trainer</a></td> <td align="left">Come effettuare il fine-tuning di BART per riassumere testi in due lingue usando la classe Trainer.</td> <td align="left"><a href="https://github.com/elsanns" rel="nofollow">Eliza Szczechla</a></td> <td align="right"><a href="https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb" rel="nofollow">Valutazione di Big Bird su Trivia QA</a></td> <td align="left">Come valutare BigBird su question answering di ‚Äúlunghi‚Äù documenti attraverso Trivia QA.</td> <td align="left"><a href="https://github.com/patrickvonplaten" rel="nofollow">Patrick von Platen</a></td> <td align="right"><a href="https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb" rel="nofollow">Creazione di sottotitoli per video usando Wav2Vec2</a></td> <td align="left">Come creare sottotitoli per qualsiasi video di YouTube trascrivendo l‚Äôaudio con Wav2Vec.</td> <td align="left"><a href="https://github.com/Muennighoff" rel="nofollow">Niklas Muennighoff</a></td> <td align="right"><a href="https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb" rel="nofollow">Fine-tuning di Vision Transformer su CIFAR-10 usando PyTorch Lightning</a></td> <td align="left">Come effettuare il fine-tuning di Vision Transformer (ViT) su CIFAR-10 usando HuggingFace Transformers, Datasets e PyTorch Lightning.</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb" rel="nofollow">Fine-tuning di Vision Transformer su CIFAR-10 usando ü§ó Trainer</a></td> <td align="left">Come effettuare il fine-tuning di Vision Transformer (ViT) su CIFAR-10 usando HuggingFace Transformers, Datasets e ü§ó Trainer.</td> <td align="left"><a href="https://github.com/nielsrogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb" rel="nofollow">Valutazione di LUKE su Open Entity, un dataset di entity typing</a></td> <td align="left">Come valutare un modello <em>LukeForEntityClassification</em> sul dataset Open Entity.</td> <td align="left"><a href="https://github.com/ikuyamada" rel="nofollow">Ikuya Yamada</a></td> <td align="right"><a href="https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb" rel="nofollow">Valutazione di LUKE su TACRED, un dataset per l‚Äôestrazione di relazioni</a></td> <td align="left">Come valutare un modello <em>LukeForEntityPairClassification</em> sul dataset TACRED.</td> <td align="left"><a href="https://github.com/ikuyamada" rel="nofollow">Ikuya Yamada</a></td> <td align="right"><a href="https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb" rel="nofollow">Valutazione di LUKE su CoNLL-2003, un importante benchmark NER</a></td> <td align="left">Come valutare un modello <em>LukeForEntitySpanClassification</em> sul dataset CoNLL-2003.</td> <td align="left"><a href="https://github.com/ikuyamada" rel="nofollow">Ikuya Yamada</a></td> <td align="right"><a href="https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb" rel="nofollow">Valutazione di BigBird-Pegasus su dataset PubMed</a></td> <td align="left">Come valutare un modello <em>BigBirdPegasusForConditionalGeneration</em> su dataset PubMed.</td> <td align="left"><a href="https://github.com/vasudevgupta7" rel="nofollow">Vasudev Gupta</a></td> <td align="right"><a href="https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb" rel="nofollow">Classificazione di emozioni dal discorso con Wav2Vec2</a></td> <td align="left">Come utilizzare un modello pre-addestrato Wav2Vec2 per la classificazione di emozioni sul dataset MEGA.</td> <td align="left"><a href="https://github.com/m3hrdadfi" rel="nofollow">Mehrdad Farahani</a></td> <td align="right"><a href="https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb" rel="nofollow">Rilevamento oggetti in un‚Äôimmagine con DETR</a></td> <td align="left">Come usare un modello addestrato <em>DetrForObjectDetection</em> per rilevare oggetti in un‚Äôimmagine e visualizzare l‚Äôattention.</td> <td align="left"><a href="https://github.com/NielsRogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_(with_DetrFeatureExtractor).ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb" rel="nofollow">Fine-tuning di DETR su un dataset personalizzato per rilevare oggetti</a></td> <td align="left">Come effettuare fine-tuning di un modello <em>DetrForObjectDetection</em> su un dataset personalizzato per rilevare oggetti.</td> <td align="left"><a href="https://github.com/NielsRogge" rel="nofollow">Niels Rogge</a></td> <td align="right"><a href="https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_(balloon).ipynb" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr> <tr><td align="left"><a href="https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb" rel="nofollow">Fine-tuning di T5 per Named Entity Recognition</a></td> <td align="left">Come effettuare fine-tunining di <em>T5</em> per un‚Äôattivit√† di Named Entity Recognition.</td> <td align="left"><a href="https://github.com/ToluClassics" rel="nofollow">Ogundepo Odunayo</a></td> <td align="right"><a href="https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></td></tr></tbody>',k,u,A;return n=new B({props:{title:"Comunit√†",local:"comunit√†",headingTag:"h1"}}),g=new B({props:{title:"Risorse della comunit√†:",local:"risorse-della-comunit√†",headingTag:"h2"}}),c=new B({props:{title:"Notebook della comunit√†:",local:"notebook-della-comunit√†",headingTag:"h2"}}),{c(){l=b("meta"),p=i(),f=b("p"),_=i(),z(n.$$.fragment),w=i(),s=b("p"),s.textContent=S,T=i(),z(g.$$.fragment),v=i(),d=b("table"),d.innerHTML=I,y=i(),z(c.$$.fragment),C=i(),h=b("table"),h.innerHTML=M,k=i(),u=b("p"),this.h()},l(t){const e=j("svelte-u9bgzb",document.head);l=m(e,"META",{name:!0,content:!0}),e.forEach(o),p=r(t),f=m(t,"P",{}),x(f).forEach(o),_=r(t),R(n.$$.fragment,t),w=r(t),s=m(t,"P",{"data-svelte-h":!0}),F(s)!=="svelte-ybodo8"&&(s.textContent=S),T=r(t),R(g.$$.fragment,t),v=r(t),d=m(t,"TABLE",{"data-svelte-h":!0}),F(d)!=="svelte-p50ua7"&&(d.innerHTML=I),y=r(t),R(c.$$.fragment,t),C=r(t),h=m(t,"TABLE",{"data-svelte-h":!0}),F(h)!=="svelte-1jntrgb"&&(h.innerHTML=M),k=r(t),u=m(t,"P",{}),x(u).forEach(o),this.h()},h(){O(l,"name","hf:doc:metadata"),O(l,"content",W)},m(t,e){U(document.head,l),a(t,p,e),a(t,f,e),a(t,_,e),P(n,t,e),a(t,w,e),a(t,s,e),a(t,T,e),P(g,t,e),a(t,v,e),a(t,d,e),a(t,y,e),P(c,t,e),a(t,C,e),a(t,h,e),a(t,k,e),a(t,u,e),A=!0},p:G,i(t){A||(D(n.$$.fragment,t),D(g.$$.fragment,t),D(c.$$.fragment,t),A=!0)},o(t){E(n.$$.fragment,t),E(g.$$.fragment,t),E(c.$$.fragment,t),A=!1},d(t){t&&(o(p),o(f),o(_),o(w),o(s),o(T),o(v),o(d),o(y),o(C),o(h),o(k),o(u)),o(l),L(n,t),L(g,t),L(c,t)}}}const W='{"title":"Comunit√†","local":"comunit√†","sections":[{"title":"Risorse della comunit√†:","local":"risorse-della-comunit√†","sections":[],"depth":2},{"title":"Notebook della comunit√†:","local":"notebook-della-comunit√†","sections":[],"depth":2}],"depth":1}';function K(N){return Q(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Z extends q{constructor(l){super(),$(this,l,K,H,V,{})}}export{Z as component};
