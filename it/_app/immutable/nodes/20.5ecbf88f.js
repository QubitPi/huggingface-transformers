import{s as et,o as tt,n as it}from"../chunks/scheduler.36a0863c.js";import{S as lt,i as nt,g as a,s as n,r as m,A as ot,h as s,f as i,c as o,j as De,u as d,x as r,k as Ke,y as at,a as l,v as u,d as c,t as f,w as b}from"../chunks/index.9c13489a.js";import{T as st}from"../chunks/Tip.3b06990e.js";import{C as We}from"../chunks/CodeBlock.05d8ec32.js";import{H as S}from"../chunks/Heading.7a254a62.js";function rt(D){let p,y="Nota che questa funzione può essere utilizzata anche nelle configurazioni multi GPU.";return{c(){p=a("p"),p.textContent=y},l(g){p=s(g,"P",{"data-svelte-h":!0}),r(p)!=="svelte-vzcdbv"&&(p.textContent=y)},m(g,A){l(g,p,A)},p:it,d(g){g&&i(p)}}}function pt(D){let p,y,g,A,z,K,T,je='Questo documento sarà presto completato con informazioni su come effetture l’inferenza su una singola GPU. Nel frattempo è possibile consultare <a href="perf_train_gpu_one">la guida per l’addestramento su una singola GPU</a> e <a href="perf_infer_cpu">la guida per l’inferenza su CPU</a>.',ee,$,te,v,Ie='Abbiamo recentemente integrato <code>BetterTransformer</code> per velocizzare l’inferenza su GPU per modelli di testo, immagini e audio. Per maggiori dettagli, consultare la documentazione su questa integrazione <a href="https://huggingface.co/docs/optimum/bettertransformer/overview" rel="nofollow">qui</a>.',ie,h,le,M,ne,U,Re=`Dal paper <a href="https://arxiv.org/abs/2208.07339" rel="nofollow"><code>LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale</code></a>, noi supportiamo l’integrazione di Hugging Face per tutti i modelli dell’Hub con poche righe di codice.
Il metodo <code>nn.Linear</code> riduce la dimensione di 2 per i pesi <code>float16</code> e <code>bfloat16</code> e di 4 per i pesi <code>float32</code>, con un impatto quasi nullo sulla qualità, operando sugli outlier in half-precision.`,oe,x,Ze='<img src="https://cdn-uploads.huggingface.co/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png" alt="HFxbitsandbytes.png"/>',ae,_,Fe=`Il metodo Int8 mixed-precision matrix decomposition funziona separando la moltiplicazione tra matrici in due flussi: (1) una matrice di flusso di outlier di caratteristiche sistematiche moltiplicata in fp16, (2) in flusso regolare di moltiplicazione di matrici int8 (99,9%). Con questo metodo, è possibile effettutare inferenza int8 per modelli molto grandi senza degrado predittivo.
Per maggiori dettagli sul metodo, consultare il <a href="https://arxiv.org/abs/2208.07339" rel="nofollow">paper</a> o il nostro <a href="https://huggingface.co/blog/hf-bitsandbytes-integration" rel="nofollow">blogpost sull’integrazione</a>.`,se,C,Pe='<img src="https://cdn-uploads.huggingface.co/production/uploads/1660567469965-62441d1d9fdefb55a0b7d12c.gif" alt="MixedInt8.gif"/>',re,J,qe=`Nota che è necessaria una GPU per eseguire modelli di tipo mixed-8bit, poiché i kernel sono stati compilati solo per le GPU. Prima di utilizzare questa funzione, assicurarsi di disporre di memoria sufficiente sulla GPU per memorizzare un quarto del modello (o la metà se i pesi del modello sono in mezza precisione).
Di seguito sono riportate alcune note per aiutarvi a utilizzare questo modulo, oppure seguite le dimostrazioni su <a href="#colab-demos">Google colab</a>.`,pe,w,me,G,ke=`<li>Se si dispone di <code>bitsandbytes&lt;0.37.0</code>, assicurarsi di eseguire su GPU NVIDIA che supportano tensor cores a 8 bit (Turing, Ampere o architetture più recenti - ad esempio T4, RTX20s RTX30s, A40-A100). Per <code>bitsandbytes&gt;=0.37.0</code>, tutte le GPU dovrebbero essere supportate.</li> <li>Installare la versione corretta di <code>bitsandbytes</code> eseguendo:
<code>pip install bitsandbytes&gt;=0.31.5</code>.</li> <li>Installare <code>accelerate</code> <code>pip install accelerate&gt;=0.12.0</code></li>`,de,W,ue,j,Ve="Dopo aver installato le librerie necessarie, per caricare il tuo modello mixed 8-bit è il seguente:",ce,I,fe,R,Le="Per la generazione di testo, si consiglia di:",be,Z,Xe="<li>utilizzare il metodo <code>generate()</code> del modello invece della funzione <code>pipeline()</code>. Sebbene l’inferenza sia possibile con la funzione <code>pipeline()</code>, essa non è ottimizzata per i modelli mixed-8bit e sarà più lenta rispetto all’uso del metodo <code>generate()</code>. Inoltre, alcune strategie di campionamento, come il campionamento nucleaus, non sono supportate dalla funzione <code>pipeline()</code> per i modelli mixed-8bit.</li> <li>collocare tutti gli ingressi sullo stesso dispositivo del modello.</li>",ge,F,He="Ecco un semplice esempio:",Me,P,ye,q,ze,k,Ee="Usare il seguente modo caricare il modello mixed-8bit su più GPU (stesso comando della configurazione a GPU singola):",Te,V,$e,L,Ne="Puoi controllare la RAM della GPU che si vuole allocare su ogni GPU usando <code>accelerate</code>. Utilizzare l’argomento <code>max_memory</code> come segue:",ve,X,he,H,Be="In questo esempio, la prima GPU utilizzerà 1 GB di memoria e la seconda 2 GB.",Ue,E,xe,N,Ye=`Con questo metodo è possibile inferire modelli che prima non era possibile inferire su Google Colab.
Guardate la demo per l’esecuzione di T5-11b (42GB in fp32)! Utilizzo la quantizzazione a 8 bit su Google Colab:`,_e,B,Qe='<a href="https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab: T5-11b demo"/></a>',Ce,Y,Se="Oppure questa demo di BLOOM-3B:",Je,Q,Ae='<a href="https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab: BLOOM-3b demo"/></a>',we,O,Ge;return z=new S({props:{title:"Inferenza efficiente su GPU singola",local:"inferenza-efficiente-su-gpu-singola",headingTag:"h1"}}),$=new S({props:{title:"BetterTransformer per l’inferenza più veloce",local:"bettertransformer-per-linferenza-più-veloce",headingTag:"h2"}}),h=new S({props:{title:"Integrazione di bitsandbytes per Int8 mixed-precision matrix decomposition",local:"integrazione-di-bitsandbytes-per-int8-mixed-precision-matrix-decomposition",headingTag:"h2"}}),M=new st({props:{$$slots:{default:[rt]},$$scope:{ctx:D}}}),w=new S({props:{title:"Requisiti",local:"requisiti",headingTag:"h3"}}),W=new S({props:{title:"Esecuzione di modelli mixed-Int8 - configurazione per singola GPU",local:"esecuzione-di-modelli-mixed-int8---configurazione-per-singola-gpu",headingTag:"h3"}}),I=new We({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfbmFtZSUyMCUzRCUyMCUyMmJpZ3NjaWVuY2UlMkZibG9vbS0yYjUlMjIlMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),P=new We({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsX25hbWUlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMmI1JTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfbmFtZSklMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUpJTBBJTBBdGV4dCUyMCUzRCUyMCUyMkhlbGxvJTJDJTIwbXklMjBsbGFtYSUyMGlzJTIwY3V0ZSUyMiUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplcihwcm9tcHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS50byglMjJjdWRhJTIyKSUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKmlucHV0cyklMEFvdXRwdXRzJTIwJTNEJTIwdG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)

text = <span class="hljs-string">&quot;Hello, my llama is cute&quot;</span>
inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)`,wrap:!1}}),q=new S({props:{title:"Esecuzione di modelli mixed-8bit - configurazione multi GPU",local:"esecuzione-di-modelli-mixed-8bit---configurazione-multi-gpu",headingTag:"h3"}}),V=new We({props:{code:"bW9kZWxfbmFtZSUyMCUzRCUyMCUyMmJpZ3NjaWVuY2UlMkZibG9vbS0yYjUlMjIlMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUp",highlighted:`model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),X=new We({props:{code:"bWF4X21lbW9yeV9tYXBwaW5nJTIwJTNEJTIwJTdCMCUzQSUyMCUyMjFHQiUyMiUyQyUyMDElM0ElMjAlMjIyR0IlMjIlN0QlMEFtb2RlbF9uYW1lJTIwJTNEJTIwJTIyYmlnc2NpZW5jZSUyRmJsb29tLTNiJTIyJTBBbW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjBtb2RlbF9uYW1lJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlJTJDJTIwbWF4X21lbW9yeSUzRG1heF9tZW1vcnlfbWFwcGluZyUwQSk=",highlighted:`max_memory_mapping = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;1GB&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;2GB&quot;</span>}
model_name = <span class="hljs-string">&quot;bigscience/bloom-3b&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, max_memory=max_memory_mapping
)`,wrap:!1}}),E=new S({props:{title:"Colab demos",local:"colab-demos",headingTag:"h3"}}),{c(){p=a("meta"),y=n(),g=a("p"),A=n(),m(z.$$.fragment),K=n(),T=a("p"),T.innerHTML=je,ee=n(),m($.$$.fragment),te=n(),v=a("p"),v.innerHTML=Ie,ie=n(),m(h.$$.fragment),le=n(),m(M.$$.fragment),ne=n(),U=a("p"),U.innerHTML=Re,oe=n(),x=a("p"),x.innerHTML=Ze,ae=n(),_=a("p"),_.innerHTML=Fe,se=n(),C=a("p"),C.innerHTML=Pe,re=n(),J=a("p"),J.innerHTML=qe,pe=n(),m(w.$$.fragment),me=n(),G=a("ul"),G.innerHTML=ke,de=n(),m(W.$$.fragment),ue=n(),j=a("p"),j.textContent=Ve,ce=n(),m(I.$$.fragment),fe=n(),R=a("p"),R.textContent=Le,be=n(),Z=a("ul"),Z.innerHTML=Xe,ge=n(),F=a("p"),F.textContent=He,Me=n(),m(P.$$.fragment),ye=n(),m(q.$$.fragment),ze=n(),k=a("p"),k.textContent=Ee,Te=n(),m(V.$$.fragment),$e=n(),L=a("p"),L.innerHTML=Ne,ve=n(),m(X.$$.fragment),he=n(),H=a("p"),H.textContent=Be,Ue=n(),m(E.$$.fragment),xe=n(),N=a("p"),N.textContent=Ye,_e=n(),B=a("p"),B.innerHTML=Qe,Ce=n(),Y=a("p"),Y.textContent=Se,Je=n(),Q=a("p"),Q.innerHTML=Ae,we=n(),O=a("p"),this.h()},l(e){const t=ot("svelte-u9bgzb",document.head);p=s(t,"META",{name:!0,content:!0}),t.forEach(i),y=o(e),g=s(e,"P",{}),De(g).forEach(i),A=o(e),d(z.$$.fragment,e),K=o(e),T=s(e,"P",{"data-svelte-h":!0}),r(T)!=="svelte-nhozkm"&&(T.innerHTML=je),ee=o(e),d($.$$.fragment,e),te=o(e),v=s(e,"P",{"data-svelte-h":!0}),r(v)!=="svelte-1z110ap"&&(v.innerHTML=Ie),ie=o(e),d(h.$$.fragment,e),le=o(e),d(M.$$.fragment,e),ne=o(e),U=s(e,"P",{"data-svelte-h":!0}),r(U)!=="svelte-om1tv8"&&(U.innerHTML=Re),oe=o(e),x=s(e,"P",{"data-svelte-h":!0}),r(x)!=="svelte-1tsrdqi"&&(x.innerHTML=Ze),ae=o(e),_=s(e,"P",{"data-svelte-h":!0}),r(_)!=="svelte-1hgrk11"&&(_.innerHTML=Fe),se=o(e),C=s(e,"P",{"data-svelte-h":!0}),r(C)!=="svelte-y2mgdg"&&(C.innerHTML=Pe),re=o(e),J=s(e,"P",{"data-svelte-h":!0}),r(J)!=="svelte-1b4hp8z"&&(J.innerHTML=qe),pe=o(e),d(w.$$.fragment,e),me=o(e),G=s(e,"UL",{"data-svelte-h":!0}),r(G)!=="svelte-50hzsq"&&(G.innerHTML=ke),de=o(e),d(W.$$.fragment,e),ue=o(e),j=s(e,"P",{"data-svelte-h":!0}),r(j)!=="svelte-14t427w"&&(j.textContent=Ve),ce=o(e),d(I.$$.fragment,e),fe=o(e),R=s(e,"P",{"data-svelte-h":!0}),r(R)!=="svelte-7gz38r"&&(R.textContent=Le),be=o(e),Z=s(e,"UL",{"data-svelte-h":!0}),r(Z)!=="svelte-drw5lu"&&(Z.innerHTML=Xe),ge=o(e),F=s(e,"P",{"data-svelte-h":!0}),r(F)!=="svelte-1s7g1s7"&&(F.textContent=He),Me=o(e),d(P.$$.fragment,e),ye=o(e),d(q.$$.fragment,e),ze=o(e),k=s(e,"P",{"data-svelte-h":!0}),r(k)!=="svelte-df9yha"&&(k.textContent=Ee),Te=o(e),d(V.$$.fragment,e),$e=o(e),L=s(e,"P",{"data-svelte-h":!0}),r(L)!=="svelte-1qhnznq"&&(L.innerHTML=Ne),ve=o(e),d(X.$$.fragment,e),he=o(e),H=s(e,"P",{"data-svelte-h":!0}),r(H)!=="svelte-80anhf"&&(H.textContent=Be),Ue=o(e),d(E.$$.fragment,e),xe=o(e),N=s(e,"P",{"data-svelte-h":!0}),r(N)!=="svelte-e4lo0z"&&(N.textContent=Ye),_e=o(e),B=s(e,"P",{"data-svelte-h":!0}),r(B)!=="svelte-1yb5ek4"&&(B.innerHTML=Qe),Ce=o(e),Y=s(e,"P",{"data-svelte-h":!0}),r(Y)!=="svelte-1u8p947"&&(Y.textContent=Se),Je=o(e),Q=s(e,"P",{"data-svelte-h":!0}),r(Q)!=="svelte-6z7881"&&(Q.innerHTML=Ae),we=o(e),O=s(e,"P",{}),De(O).forEach(i),this.h()},h(){Ke(p,"name","hf:doc:metadata"),Ke(p,"content",mt)},m(e,t){at(document.head,p),l(e,y,t),l(e,g,t),l(e,A,t),u(z,e,t),l(e,K,t),l(e,T,t),l(e,ee,t),u($,e,t),l(e,te,t),l(e,v,t),l(e,ie,t),u(h,e,t),l(e,le,t),u(M,e,t),l(e,ne,t),l(e,U,t),l(e,oe,t),l(e,x,t),l(e,ae,t),l(e,_,t),l(e,se,t),l(e,C,t),l(e,re,t),l(e,J,t),l(e,pe,t),u(w,e,t),l(e,me,t),l(e,G,t),l(e,de,t),u(W,e,t),l(e,ue,t),l(e,j,t),l(e,ce,t),u(I,e,t),l(e,fe,t),l(e,R,t),l(e,be,t),l(e,Z,t),l(e,ge,t),l(e,F,t),l(e,Me,t),u(P,e,t),l(e,ye,t),u(q,e,t),l(e,ze,t),l(e,k,t),l(e,Te,t),u(V,e,t),l(e,$e,t),l(e,L,t),l(e,ve,t),u(X,e,t),l(e,he,t),l(e,H,t),l(e,Ue,t),u(E,e,t),l(e,xe,t),l(e,N,t),l(e,_e,t),l(e,B,t),l(e,Ce,t),l(e,Y,t),l(e,Je,t),l(e,Q,t),l(e,we,t),l(e,O,t),Ge=!0},p(e,[t]){const Oe={};t&2&&(Oe.$$scope={dirty:t,ctx:e}),M.$set(Oe)},i(e){Ge||(c(z.$$.fragment,e),c($.$$.fragment,e),c(h.$$.fragment,e),c(M.$$.fragment,e),c(w.$$.fragment,e),c(W.$$.fragment,e),c(I.$$.fragment,e),c(P.$$.fragment,e),c(q.$$.fragment,e),c(V.$$.fragment,e),c(X.$$.fragment,e),c(E.$$.fragment,e),Ge=!0)},o(e){f(z.$$.fragment,e),f($.$$.fragment,e),f(h.$$.fragment,e),f(M.$$.fragment,e),f(w.$$.fragment,e),f(W.$$.fragment,e),f(I.$$.fragment,e),f(P.$$.fragment,e),f(q.$$.fragment,e),f(V.$$.fragment,e),f(X.$$.fragment,e),f(E.$$.fragment,e),Ge=!1},d(e){e&&(i(y),i(g),i(A),i(K),i(T),i(ee),i(te),i(v),i(ie),i(le),i(ne),i(U),i(oe),i(x),i(ae),i(_),i(se),i(C),i(re),i(J),i(pe),i(me),i(G),i(de),i(ue),i(j),i(ce),i(fe),i(R),i(be),i(Z),i(ge),i(F),i(Me),i(ye),i(ze),i(k),i(Te),i($e),i(L),i(ve),i(he),i(H),i(Ue),i(xe),i(N),i(_e),i(B),i(Ce),i(Y),i(Je),i(Q),i(we),i(O)),i(p),b(z,e),b($,e),b(h,e),b(M,e),b(w,e),b(W,e),b(I,e),b(P,e),b(q,e),b(V,e),b(X,e),b(E,e)}}}const mt='{"title":"Inferenza efficiente su GPU singola","local":"inferenza-efficiente-su-gpu-singola","sections":[{"title":"BetterTransformer per l’inferenza più veloce","local":"bettertransformer-per-linferenza-più-veloce","sections":[],"depth":2},{"title":"Integrazione di bitsandbytes per Int8 mixed-precision matrix decomposition","local":"integrazione-di-bitsandbytes-per-int8-mixed-precision-matrix-decomposition","sections":[{"title":"Requisiti","local":"requisiti","sections":[],"depth":3},{"title":"Esecuzione di modelli mixed-Int8 - configurazione per singola GPU","local":"esecuzione-di-modelli-mixed-int8---configurazione-per-singola-gpu","sections":[],"depth":3},{"title":"Esecuzione di modelli mixed-8bit - configurazione multi GPU","local":"esecuzione-di-modelli-mixed-8bit---configurazione-multi-gpu","sections":[],"depth":3},{"title":"Colab demos","local":"colab-demos","sections":[],"depth":3}],"depth":2}],"depth":1}';function dt(D){return tt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Mt extends lt{constructor(p){super(),nt(this,p,dt,pt,et,{})}}export{Mt as component};
