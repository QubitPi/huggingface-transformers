import{s as _e,n as $e,o as Ce}from"../chunks/scheduler.36a0863c.js";import{S as be,i as ve,g as o,s as l,r as I,A as ze,h as r,f as i,c as a,j as Pe,u as X,x as s,k as xe,y as Te,a as n,v as E,d as y,t as L,w as U}from"../chunks/index.9c13489a.js";import{C as Me}from"../chunks/CodeBlock.05d8ec32.js";import{H as k}from"../chunks/Heading.7a254a62.js";function we(le){let p,V,H,B,d,G,c,ae="Questa guida si concentra su come addestrare in maniera efficiente grandi modelli su CPU.",q,m,F,u,oe="IPEX è ottimizzato per CPU con AVX-512 o superiore, e funziona per le CPU con solo AVX2. Pertanto, si prevede che le prestazioni saranno più vantaggiose per le le CPU Intel con AVX-512 o superiori, mentre le CPU con solo AVX2 (ad esempio, le CPU AMD o le CPU Intel più vecchie) potrebbero ottenere prestazioni migliori con IPEX, ma non sono garantite. IPEX offre ottimizzazioni delle prestazioni per l’addestramento della CPU sia con Float32 che con BFloat16. L’uso di BFloat16 è l’argomento principale delle seguenti sezioni.",R,f,re="Il tipo di dati a bassa precisione BFloat16 è stato supportato in modo nativo su 3rd Generation Xeon® Scalable Processors (aka Cooper Lake) con AVX512 e sarà supportata dalla prossima generazione di Intel® Xeon® Scalable Processors con Intel® Advanced Matrix Extensions (Intel® AMX) instruction set con prestazioni ulteriormente migliorate. L’Auto Mixed Precision per il backende della CPU è stato abilitato da PyTorch-1.10. allo stesso tempo, il supporto di Auto Mixed Precision con BFloat16 per CPU e l’ottimizzazione degli operatori BFloat16 è stata abilitata in modo massiccio in Intel® Extension per PyTorch, and parzialmente aggiornato al branch master di PyTorch. Gli utenti possono ottenere prestazioni migliori ed users experience con IPEX Auto Mixed Precision..",S,g,se='Vedi informazioni più dettagliate su <a href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/amp.html" rel="nofollow">Auto Mixed Precision</a>.',Z,h,J,P,pe="Il rilascio di IPEX segue quello di PyTorch, da installare via pip:",j,x,de='<thead><tr><th align="center">PyTorch Version</th> <th align="center">IPEX version</th></tr></thead> <tbody><tr><td align="center">1.13</td> <td align="center">1.13.0+cpu</td></tr> <tr><td align="center">1.12</td> <td align="center">1.12.300+cpu</td></tr> <tr><td align="center">1.11</td> <td align="center">1.11.200+cpu</td></tr> <tr><td align="center">1.10</td> <td align="center">1.10.100+cpu</td></tr></tbody>',W,_,Q,$,ce='Vedi altri approcci per <a href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/installation.html" rel="nofollow">IPEX installation</a>.',Y,C,N,b,me="Per abilitare la auto mixed precision con IPEX in Trainer, l’utende dovrebbe aggiungere <code>use_ipex</code>, <code>bf16</code> e <code>no_cuda</code> negli argomenti del comando di addestramento.",D,v,ue='Vedi un sempio di un caso d’uso <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering" rel="nofollow">Transformers question-answering</a>',K,z,fe="<li>Training with IPEX using BF16 auto mixed precision on CPU:</li>",O,T,ge=` python run_qa.py \\
--model_name_or_path google-bert/bert-base-uncased \\
--dataset_name squad \\
--do_train \\
--do_eval \\
--per_device_train_batch_size 12 \\
--learning_rate 3e-5 \\
--num_train_epochs 2 \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/debug_squad/ \\
<b>--use_ipex \\</b>
<b>--bf16 --no_cuda</b>`,ee,M,te,w,he='Blog: <a href="https://huggingface.co/blog/intel-sapphire-rapids" rel="nofollow">Accelerating PyTorch Transformers with Intel Sapphire Rapids</a>',ie,A,ne;return d=new k({props:{title:"Addestramento efficiente su CPU",local:"addestramento-efficiente-su-cpu",headingTag:"h1"}}),m=new k({props:{title:"Mixed precision con IPEX",local:"mixed-precision-con-ipex",headingTag:"h2"}}),h=new k({props:{title:"Installazione di IPEX:",local:"installazione-di-ipex",headingTag:"h3"}}),_=new Me({props:{code:"cGlwJTIwaW5zdGFsbCUyMGludGVsX2V4dGVuc2lvbl9mb3JfcHl0b3JjaCUzRCUzRCUzQ3ZlcnNpb25fbmFtZSUzRSUyMC1mJTIwaHR0cHMlM0ElMkYlMkZkZXZlbG9wZXIuaW50ZWwuY29tJTJGaXBleC13aGwtc3RhYmxlLWNwdQ==",highlighted:"pip install intel_extension_for_pytorch==&lt;version_name&gt; -f https://developer.intel.com/ipex-whl-stable-cpu",wrap:!1}}),C=new k({props:{title:"Utilizzo nel Trainer",local:"utilizzo-nel-trainer",headingTag:"h3"}}),M=new k({props:{title:"Esempi pratici",local:"esempi-pratici",headingTag:"h3"}}),{c(){p=o("meta"),V=l(),H=o("p"),B=l(),I(d.$$.fragment),G=l(),c=o("p"),c.textContent=ae,q=l(),I(m.$$.fragment),F=l(),u=o("p"),u.textContent=oe,R=l(),f=o("p"),f.textContent=re,S=l(),g=o("p"),g.innerHTML=se,Z=l(),I(h.$$.fragment),J=l(),P=o("p"),P.textContent=pe,j=l(),x=o("table"),x.innerHTML=de,W=l(),I(_.$$.fragment),Q=l(),$=o("p"),$.innerHTML=ce,Y=l(),I(C.$$.fragment),N=l(),b=o("p"),b.innerHTML=me,D=l(),v=o("p"),v.innerHTML=ue,K=l(),z=o("ul"),z.innerHTML=fe,O=l(),T=o("pre"),T.innerHTML=ge,ee=l(),I(M.$$.fragment),te=l(),w=o("p"),w.innerHTML=he,ie=l(),A=o("p"),this.h()},l(e){const t=ze("svelte-u9bgzb",document.head);p=r(t,"META",{name:!0,content:!0}),t.forEach(i),V=a(e),H=r(e,"P",{}),Pe(H).forEach(i),B=a(e),X(d.$$.fragment,e),G=a(e),c=r(e,"P",{"data-svelte-h":!0}),s(c)!=="svelte-uyppuu"&&(c.textContent=ae),q=a(e),X(m.$$.fragment,e),F=a(e),u=r(e,"P",{"data-svelte-h":!0}),s(u)!=="svelte-1kpslxw"&&(u.textContent=oe),R=a(e),f=r(e,"P",{"data-svelte-h":!0}),s(f)!=="svelte-uzxkpk"&&(f.textContent=re),S=a(e),g=r(e,"P",{"data-svelte-h":!0}),s(g)!=="svelte-n2t196"&&(g.innerHTML=se),Z=a(e),X(h.$$.fragment,e),J=a(e),P=r(e,"P",{"data-svelte-h":!0}),s(P)!=="svelte-hyexng"&&(P.textContent=pe),j=a(e),x=r(e,"TABLE",{"data-svelte-h":!0}),s(x)!=="svelte-okkx16"&&(x.innerHTML=de),W=a(e),X(_.$$.fragment,e),Q=a(e),$=r(e,"P",{"data-svelte-h":!0}),s($)!=="svelte-yqjm2f"&&($.innerHTML=ce),Y=a(e),X(C.$$.fragment,e),N=a(e),b=r(e,"P",{"data-svelte-h":!0}),s(b)!=="svelte-1ldgg7n"&&(b.innerHTML=me),D=a(e),v=r(e,"P",{"data-svelte-h":!0}),s(v)!=="svelte-pr9u3e"&&(v.innerHTML=ue),K=a(e),z=r(e,"UL",{"data-svelte-h":!0}),s(z)!=="svelte-gvsghn"&&(z.innerHTML=fe),O=a(e),T=r(e,"PRE",{"data-svelte-h":!0}),s(T)!=="svelte-nhfrun"&&(T.innerHTML=ge),ee=a(e),X(M.$$.fragment,e),te=a(e),w=r(e,"P",{"data-svelte-h":!0}),s(w)!=="svelte-w7zs5j"&&(w.innerHTML=he),ie=a(e),A=r(e,"P",{}),Pe(A).forEach(i),this.h()},h(){xe(p,"name","hf:doc:metadata"),xe(p,"content",Ie)},m(e,t){Te(document.head,p),n(e,V,t),n(e,H,t),n(e,B,t),E(d,e,t),n(e,G,t),n(e,c,t),n(e,q,t),E(m,e,t),n(e,F,t),n(e,u,t),n(e,R,t),n(e,f,t),n(e,S,t),n(e,g,t),n(e,Z,t),E(h,e,t),n(e,J,t),n(e,P,t),n(e,j,t),n(e,x,t),n(e,W,t),E(_,e,t),n(e,Q,t),n(e,$,t),n(e,Y,t),E(C,e,t),n(e,N,t),n(e,b,t),n(e,D,t),n(e,v,t),n(e,K,t),n(e,z,t),n(e,O,t),n(e,T,t),n(e,ee,t),E(M,e,t),n(e,te,t),n(e,w,t),n(e,ie,t),n(e,A,t),ne=!0},p:$e,i(e){ne||(y(d.$$.fragment,e),y(m.$$.fragment,e),y(h.$$.fragment,e),y(_.$$.fragment,e),y(C.$$.fragment,e),y(M.$$.fragment,e),ne=!0)},o(e){L(d.$$.fragment,e),L(m.$$.fragment,e),L(h.$$.fragment,e),L(_.$$.fragment,e),L(C.$$.fragment,e),L(M.$$.fragment,e),ne=!1},d(e){e&&(i(V),i(H),i(B),i(G),i(c),i(q),i(F),i(u),i(R),i(f),i(S),i(g),i(Z),i(J),i(P),i(j),i(x),i(W),i(Q),i($),i(Y),i(N),i(b),i(D),i(v),i(K),i(z),i(O),i(T),i(ee),i(te),i(w),i(ie),i(A)),i(p),U(d,e),U(m,e),U(h,e),U(_,e),U(C,e),U(M,e)}}}const Ie='{"title":"Addestramento efficiente su CPU","local":"addestramento-efficiente-su-cpu","sections":[{"title":"Mixed precision con IPEX","local":"mixed-precision-con-ipex","sections":[{"title":"Installazione di IPEX:","local":"installazione-di-ipex","sections":[],"depth":3},{"title":"Utilizzo nel Trainer","local":"utilizzo-nel-trainer","sections":[],"depth":3},{"title":"Esempi pratici","local":"esempi-pratici","sections":[],"depth":3}],"depth":2}],"depth":1}';function Xe(le){return Ce(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class He extends be{constructor(p){super(),ve(this,p,Xe,we,_e,{})}}export{He as component};
