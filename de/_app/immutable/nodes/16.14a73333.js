import{s as $s,o as gs,n as H}from"../chunks/scheduler.987d3921.js";import{S as hs,i as bs,g as T,s as o,r as d,A as Ms,h as y,f as l,c as p,j as fs,u as c,x as j,k as ds,y as ws,a as r,v as $,d as g,t as h,w as b}from"../chunks/index.c8b1fed4.js";import{T as tt}from"../chunks/Tip.6bc1e794.js";import{Y as cs}from"../chunks/Youtube.67f0a1fc.js";import{C as U}from"../chunks/CodeBlock.18094d58.js";import{D as Ts}from"../chunks/DocNotebookDropdown.8be6c56e.js";import{F as et,M as I}from"../chunks/Markdown.10b8ab04.js";import{H as S}from"../chunks/Heading.3fa3b67f.js";function ys(w){let n,i=`Alle in der Dokumentation vorgestellten Codebeispiele haben oben links einen Umschalter f√ºr PyTorch und TensorFlow. Wenn
nicht, wird erwartet, dass der Code f√ºr beide Backends ohne √Ñnderungen funktioniert.`;return{c(){n=T("p"),n.textContent=i},l(t){n=y(t,"P",{"data-svelte-h":!0}),j(n)!=="svelte-10pwefj"&&(n.textContent=i)},m(t,a){r(t,n,a)},p:H,d(t){t&&l(n)}}}function js(w){let n,i='F√ºr mehr Details √ºber die <code>pipeline()</code> und assoziierte Aufgaben, schauen Sie in die Dokumentation <a href="./main_classes/pipelines">hier</a>.';return{c(){n=T("p"),n.innerHTML=i},l(t){n=y(t,"P",{"data-svelte-h":!0}),j(n)!=="svelte-scxafs"&&(n.innerHTML=i)},m(t,a){r(t,n,a)},p:H,d(t){t&&l(n)}}}function ks(w){let n,i;return n=new U({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRvcmNo",highlighted:"pip install torch",wrap:!1}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p:H,i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function vs(w){let n,i;return n=new I({props:{$$slots:{default:[ks]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function _s(w){let n,i;return n=new U({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRlbnNvcmZsb3c=",highlighted:"pip install tensorflow",wrap:!1}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p:H,i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Us(w){let n,i;return n=new I({props:{$$slots:{default:[_s]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Js(w){let n,i="Use the <code>AutoModelForSequenceClassification</code> and <code>AutoTokenizer</code> to load the pretrained model and it‚Äôs associated tokenizer (more on an <code>AutoClass</code> below):",t,a,f;return a=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`,wrap:!1}}),{c(){n=T("p"),n.innerHTML=i,t=o(),d(a.$$.fragment)},l(m){n=y(m,"P",{"data-svelte-h":!0}),j(n)!=="svelte-4l15zc"&&(n.innerHTML=i),t=p(m),c(a.$$.fragment,m)},m(m,v){r(m,n,v),r(m,t,v),$(a,m,v),f=!0},p:H,i(m){f||(g(a.$$.fragment,m),f=!0)},o(m){h(a.$$.fragment,m),f=!1},d(m){m&&(l(n),l(t)),b(a,m)}}}function Zs(w){let n,i;return n=new I({props:{$$slots:{default:[Js]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Cs(w){let n,i="Use the <code>TFAutoModelForSequenceClassification</code> and <code>AutoTokenizer</code> to load the pretrained model and it‚Äôs associated tokenizer (more on an <code>TFAutoClass</code> below):",t,a,f;return a=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQobW9kZWxfbmFtZSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(model_name)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`,wrap:!1}}),{c(){n=T("p"),n.innerHTML=i,t=o(),d(a.$$.fragment)},l(m){n=y(m,"P",{"data-svelte-h":!0}),j(n)!=="svelte-11eq110"&&(n.innerHTML=i),t=p(m),c(a.$$.fragment,m)},m(m,v){r(m,n,v),r(m,t,v),$(a,m,v),f=!0},p:H,i(m){f||(g(a.$$.fragment,m),f=!0)},o(m){h(a.$$.fragment,m),f=!1},d(m){m&&(l(n),l(t)),b(a,m)}}}function zs(w){let n,i;return n=new I({props:{$$slots:{default:[Cs]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Ws(w){let n,i;return n=new U({props:{code:"cHRfYmF0Y2glMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwJTVCJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiUyQyUyMCUyMldlJTIwaG9wZSUyMHlvdSUyMGRvbid0JTIwaGF0ZSUyMGl0LiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHBhZGRpbmclM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwdHJ1bmNhdGlvbiUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBtYXhfbGVuZ3RoJTNENTEyJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;pt&quot;</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p:H,i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Hs(w){let n,i;return n=new I({props:{$$slots:{default:[Ws]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function xs(w){let n,i;return n=new U({props:{code:"dGZfYmF0Y2glMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwJTVCJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiUyQyUyMCUyMldlJTIwaG9wZSUyMHlvdSUyMGRvbid0JTIwaGF0ZSUyMGl0LiUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHBhZGRpbmclM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwdHJ1bmNhdGlvbiUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBtYXhfbGVuZ3RoJTNENTEyJTJDJTBBJTIwJTIwJTIwJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMiUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_batch = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>],
<span class="hljs-meta">... </span>    padding=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    truncation=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    max_length=<span class="hljs-number">512</span>,
<span class="hljs-meta">... </span>    return_tensors=<span class="hljs-string">&quot;tf&quot;</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p:H,i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Is(w){let n,i;return n=new I({props:{$$slots:{default:[xs]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Rs(w){let n,i='In der <a href="./task_summary">Aufgabenzusammenfassung</a> steht, welche [AutoModel]-Klasse f√ºr welche Aufgabe zu verwenden ist.';return{c(){n=T("p"),n.innerHTML=i},l(t){n=y(t,"P",{"data-svelte-h":!0}),j(n)!=="svelte-1nw2wtu"&&(n.innerHTML=i)},m(t,a){r(t,n,a)},p:H,d(t){t&&l(n)}}}function Gs(w){let n,i="ü§ó Transformers bietet eine einfache und einheitliche M√∂glichkeit, vortrainierte Instanzen zu laden. Das bedeutet, dass Sie ein <code>AutoModel</code> laden k√∂nnen, wie Sie einen <code>AutoTokenizer</code> laden w√ºrden. Der einzige Unterschied ist die Auswahl des richtigen <code>AutoModel</code> f√ºr die Aufgabe. Da Sie eine Text- oder Sequenzklassifizierung vornehmen, laden Sie <code>AutoModelForSequenceClassification</code>:",t,a,f,m,v,J,Z="Jetzt k√∂nnen Sie Ihren vorverarbeiteten Stapel von Eingaben direkt an das Modell √ºbergeben. Sie m√ºssen nur das W√∂rterbuch entpacken, indem Sie <code>**</code> hinzuf√ºgen:",z,M,_,W,R="Das Modell gibt die endg√ºltigen Aktivierungen in dem Attribut ‚Äúlogits‚Äù aus. Wenden Sie die Softmax-Funktion auf die ‚Äúlogits‚Äù an, um die Wahrscheinlichkeiten zu erhalten:",x,C,G;return a=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbF9uYW1lJTIwJTNEJTIwJTIybmxwdG93biUyRmJlcnQtYmFzZS1tdWx0aWxpbmd1YWwtdW5jYXNlZC1zZW50aW1lbnQlMjIlMEFwdF9tb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)`,wrap:!1}}),m=new tt({props:{$$slots:{default:[Rs]},$$scope:{ctx:w}}}),M=new U({props:{code:"cHRfb3V0cHV0cyUyMCUzRCUyMHB0X21vZGVsKCoqcHRfYmF0Y2gp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_outputs = pt_model(**pt_batch)',wrap:!1}}),C=new U({props:{code:"ZnJvbSUyMHRvcmNoJTIwaW1wb3J0JTIwbm4lMEElMEFwdF9wcmVkaWN0aW9ucyUyMCUzRCUyMG5uLmZ1bmN0aW9uYWwuc29mdG1heChwdF9vdXRwdXRzLmxvZ2l0cyUyQyUyMGRpbSUzRC0xKSUwQXByaW50KHB0X3ByZWRpY3Rpb25zKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn

<span class="hljs-meta">&gt;&gt;&gt; </span>pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(pt_predictions)
tensor([[<span class="hljs-number">0.0021</span>, <span class="hljs-number">0.0018</span>, <span class="hljs-number">0.0115</span>, <span class="hljs-number">0.2121</span>, <span class="hljs-number">0.7725</span>],
        [<span class="hljs-number">0.2084</span>, <span class="hljs-number">0.1826</span>, <span class="hljs-number">0.1969</span>, <span class="hljs-number">0.1755</span>, <span class="hljs-number">0.2365</span>]], grad_fn=&lt;SoftmaxBackward0&gt;)`,wrap:!1}}),{c(){n=T("p"),n.innerHTML=i,t=o(),d(a.$$.fragment),f=o(),d(m.$$.fragment),v=o(),J=T("p"),J.innerHTML=Z,z=o(),d(M.$$.fragment),_=o(),W=T("p"),W.textContent=R,x=o(),d(C.$$.fragment)},l(u){n=y(u,"P",{"data-svelte-h":!0}),j(n)!=="svelte-e1edih"&&(n.innerHTML=i),t=p(u),c(a.$$.fragment,u),f=p(u),c(m.$$.fragment,u),v=p(u),J=y(u,"P",{"data-svelte-h":!0}),j(J)!=="svelte-1x2uldv"&&(J.innerHTML=Z),z=p(u),c(M.$$.fragment,u),_=p(u),W=y(u,"P",{"data-svelte-h":!0}),j(W)!=="svelte-1v08vwl"&&(W.textContent=R),x=p(u),c(C.$$.fragment,u)},m(u,k){r(u,n,k),r(u,t,k),$(a,u,k),r(u,f,k),$(m,u,k),r(u,v,k),r(u,J,k),r(u,z,k),$(M,u,k),r(u,_,k),r(u,W,k),r(u,x,k),$(C,u,k),G=!0},p(u,k){const F={};k&2&&(F.$$scope={dirty:k,ctx:u}),m.$set(F)},i(u){G||(g(a.$$.fragment,u),g(m.$$.fragment,u),g(M.$$.fragment,u),g(C.$$.fragment,u),G=!0)},o(u){h(a.$$.fragment,u),h(m.$$.fragment,u),h(M.$$.fragment,u),h(C.$$.fragment,u),G=!1},d(u){u&&(l(n),l(t),l(f),l(v),l(J),l(z),l(_),l(W),l(x)),b(a,u),b(m,u),b(M,u),b(C,u)}}}function Fs(w){let n,i;return n=new I({props:{$$slots:{default:[Gs]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Ss(w){let n,i='In der <a href="./task_summary">Aufgabenzusammenfassung</a> steht, welche [AutoModel]-Klasse f√ºr welche Aufgabe zu verwenden ist.';return{c(){n=T("p"),n.innerHTML=i},l(t){n=y(t,"P",{"data-svelte-h":!0}),j(n)!=="svelte-1nw2wtu"&&(n.innerHTML=i)},m(t,a){r(t,n,a)},p:H,d(t){t&&l(n)}}}function Vs(w){let n,i="ü§ó Transformers bietet eine einfache und einheitliche Methode zum Laden von vortrainierten Instanzen. Das bedeutet, dass Sie ein <code>TFAutoModel</code> genauso laden k√∂nnen, wie Sie einen <code>AutoTokenizer</code> laden w√ºrden. Der einzige Unterschied ist die Auswahl des richtigen <code>TFAutoModel</code> f√ºr die Aufgabe. Da Sie Text - oder Sequenz - Klassifizierung machen, laden Sie <code>TFAutoModelForSequenceClassification</code>:",t,a,f,m,v,J,Z="Jetzt k√∂nnen Sie Ihren vorverarbeiteten Stapel von Eingaben direkt an das Modell √ºbergeben, indem Sie die W√∂rterbuchschl√ºssel direkt an die Tensoren √ºbergeben:",z,M,_,W,R="Das Modell gibt die endg√ºltigen Aktivierungen in dem Attribut ‚Äúlogits‚Äù aus. Wenden Sie die Softmax-Funktion auf die ‚Äúlogits‚Äù an, um die Wahrscheinlichkeiten zu erhalten:",x,C,G;return a=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsX25hbWUlMjAlM0QlMjAlMjJubHB0b3duJTJGYmVydC1iYXNlLW11bHRpbGluZ3VhbC11bmNhc2VkLXNlbnRpbWVudCUyMiUwQXRmX21vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)`,wrap:!1}}),m=new tt({props:{$$slots:{default:[Ss]},$$scope:{ctx:w}}}),M=new U({props:{code:"dGZfb3V0cHV0cyUyMCUzRCUyMHRmX21vZGVsKHRmX2JhdGNoKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_outputs = tf_model(tf_batch)',wrap:!1}}),C=new U({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEF0Zl9wcmVkaWN0aW9ucyUyMCUzRCUyMHRmLm5uLnNvZnRtYXgodGZfb3V0cHV0cy5sb2dpdHMlMkMlMjBheGlzJTNELTEpJTBBdGZfcHJlZGljdGlvbnM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_predictions`,wrap:!1}}),{c(){n=T("p"),n.innerHTML=i,t=o(),d(a.$$.fragment),f=o(),d(m.$$.fragment),v=o(),J=T("p"),J.textContent=Z,z=o(),d(M.$$.fragment),_=o(),W=T("p"),W.textContent=R,x=o(),d(C.$$.fragment)},l(u){n=y(u,"P",{"data-svelte-h":!0}),j(n)!=="svelte-ombd0e"&&(n.innerHTML=i),t=p(u),c(a.$$.fragment,u),f=p(u),c(m.$$.fragment,u),v=p(u),J=y(u,"P",{"data-svelte-h":!0}),j(J)!=="svelte-l6pbq"&&(J.textContent=Z),z=p(u),c(M.$$.fragment,u),_=p(u),W=y(u,"P",{"data-svelte-h":!0}),j(W)!=="svelte-1v08vwl"&&(W.textContent=R),x=p(u),c(C.$$.fragment,u)},m(u,k){r(u,n,k),r(u,t,k),$(a,u,k),r(u,f,k),$(m,u,k),r(u,v,k),r(u,J,k),r(u,z,k),$(M,u,k),r(u,_,k),r(u,W,k),r(u,x,k),$(C,u,k),G=!0},p(u,k){const F={};k&2&&(F.$$scope={dirty:k,ctx:u}),m.$set(F)},i(u){G||(g(a.$$.fragment,u),g(m.$$.fragment,u),g(M.$$.fragment,u),g(C.$$.fragment,u),G=!0)},o(u){h(a.$$.fragment,u),h(m.$$.fragment,u),h(M.$$.fragment,u),h(C.$$.fragment,u),G=!1},d(u){u&&(l(n),l(t),l(f),l(v),l(J),l(z),l(_),l(W),l(x)),b(a,u),b(m,u),b(M,u),b(C,u)}}}function As(w){let n,i;return n=new I({props:{$$slots:{default:[Vs]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Bs(w){let n,i=`Alle ü§ó Transformers-Modelle (PyTorch oder TensorFlow) geben die Tensoren <em>vor</em> der endg√ºltigen Aktivierungsfunktion
Funktion (wie Softmax) aus, da die endg√ºltige Aktivierungsfunktion oft mit dem Verlusten verschmolzen ist.`;return{c(){n=T("p"),n.innerHTML=i},l(t){n=y(t,"P",{"data-svelte-h":!0}),j(n)!=="svelte-1j0s89w"&&(n.innerHTML=i)},m(t,a){r(t,n,a)},p:H,d(t){t&&l(n)}}}function Es(w){let n,i=`Transformers-Modellausgaben sind spezielle Datenklassen, so dass ihre Attribute in einer IDE automatisch vervollst√§ndigt werden.
Die Modellausg√§nge verhalten sich auch wie ein Tupel oder ein W√∂rterbuch (z.B. k√∂nnen Sie mit einem Integer, einem Slice oder einem String indexieren), wobei die Attribute, die ‚ÄúNone‚Äù sind, ignoriert werden.`;return{c(){n=T("p"),n.textContent=i},l(t){n=y(t,"P",{"data-svelte-h":!0}),j(n)!=="svelte-yxbdov"&&(n.textContent=i)},m(t,a){r(t,n,a)},p:H,d(t){t&&l(n)}}}function Ns(w){let n,i="Sobald Ihr Modell feinabgestimmt ist, k√∂nnen Sie es mit seinem Tokenizer speichern, indem Sie <code>PreTrainedModel.save_pretrained()</code> verwenden:",t,a,f,m,v="Wenn Sie bereit sind, das Modell erneut zu verwenden, laden Sie es mit <code>PreTrainedModel.from_pretrained()</code>:",J,Z,z;return a=new U({props:{code:"cHRfc2F2ZV9kaXJlY3RvcnklMjAlM0QlMjAlMjIuJTJGcHRfc2F2ZV9wcmV0cmFpbmVkJTIyJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZChwdF9zYXZlX2RpcmVjdG9yeSklMEFwdF9tb2RlbC5zYXZlX3ByZXRyYWluZWQocHRfc2F2ZV9kaXJlY3Rvcnkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>pt_save_directory = <span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model.save_pretrained(pt_save_directory)`,wrap:!1}}),Z=new U({props:{code:"cHRfbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfc2F2ZV9wcmV0cmFpbmVkJTIyKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./pt_save_pretrained&quot;</span>)',wrap:!1}}),{c(){n=T("p"),n.innerHTML=i,t=o(),d(a.$$.fragment),f=o(),m=T("p"),m.innerHTML=v,J=o(),d(Z.$$.fragment)},l(M){n=y(M,"P",{"data-svelte-h":!0}),j(n)!=="svelte-1uysolt"&&(n.innerHTML=i),t=p(M),c(a.$$.fragment,M),f=p(M),m=y(M,"P",{"data-svelte-h":!0}),j(m)!=="svelte-ie9uvm"&&(m.innerHTML=v),J=p(M),c(Z.$$.fragment,M)},m(M,_){r(M,n,_),r(M,t,_),$(a,M,_),r(M,f,_),r(M,m,_),r(M,J,_),$(Z,M,_),z=!0},p:H,i(M){z||(g(a.$$.fragment,M),g(Z.$$.fragment,M),z=!0)},o(M){h(a.$$.fragment,M),h(Z.$$.fragment,M),z=!1},d(M){M&&(l(n),l(t),l(f),l(m),l(J)),b(a,M),b(Z,M)}}}function Xs(w){let n,i;return n=new I({props:{$$slots:{default:[Ns]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Ls(w){let n,i="Sobald Ihr Modell feinabgestimmt ist, k√∂nnen Sie es mit seinem Tokenizer unter Verwendung von <code>TFPreTrainedModel.save_pretrained()</code> speichern:",t,a,f,m,v="Wenn Sie bereit sind, das Modell wieder zu verwenden, laden Sie es mit <code>TFPreTrainedModel.from_pretrained()</code>:",J,Z,z;return a=new U({props:{code:"dGZfc2F2ZV9kaXJlY3RvcnklMjAlM0QlMjAlMjIuJTJGdGZfc2F2ZV9wcmV0cmFpbmVkJTIyJTBBdG9rZW5pemVyLnNhdmVfcHJldHJhaW5lZCh0Zl9zYXZlX2RpcmVjdG9yeSklMEF0Zl9tb2RlbC5zYXZlX3ByZXRyYWluZWQodGZfc2F2ZV9kaXJlY3Rvcnkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_save_directory = <span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model.save_pretrained(tf_save_directory)`,wrap:!1}}),Z=new U({props:{code:"dGZfbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0Zl9zYXZlX3ByZXRyYWluZWQlMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;./tf_save_pretrained&quot;</span>)',wrap:!1}}),{c(){n=T("p"),n.innerHTML=i,t=o(),d(a.$$.fragment),f=o(),m=T("p"),m.innerHTML=v,J=o(),d(Z.$$.fragment)},l(M){n=y(M,"P",{"data-svelte-h":!0}),j(n)!=="svelte-1cia17v"&&(n.innerHTML=i),t=p(M),c(a.$$.fragment,M),f=p(M),m=y(M,"P",{"data-svelte-h":!0}),j(m)!=="svelte-1o1dzw5"&&(m.innerHTML=v),J=p(M),c(Z.$$.fragment,M)},m(M,_){r(M,n,_),r(M,t,_),$(a,M,_),r(M,f,_),r(M,m,_),r(M,J,_),$(Z,M,_),z=!0},p:H,i(M){z||(g(a.$$.fragment,M),g(Z.$$.fragment,M),z=!0)},o(M){h(a.$$.fragment,M),h(Z.$$.fragment,M),z=!1},d(M){M&&(l(n),l(t),l(f),l(m),l(J)),b(a,M),b(Z,M)}}}function Ys(w){let n,i;return n=new I({props:{$$slots:{default:[Ls]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Ps(w){let n,i;return n=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKHRmX3NhdmVfZGlyZWN0b3J5KSUwQXB0X21vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQodGZfc2F2ZV9kaXJlY3RvcnklMkMlMjBmcm9tX3RmJTNEVHJ1ZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=<span class="hljs-literal">True</span>)`,wrap:!1}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p:H,i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function qs(w){let n,i;return n=new I({props:{$$slots:{default:[Ps]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Qs(w){let n,i;return n=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQocHRfc2F2ZV9kaXJlY3RvcnkpJTBBdGZfbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKHB0X3NhdmVfZGlyZWN0b3J5JTJDJTIwZnJvbV9wdCUzRFRydWUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=<span class="hljs-literal">True</span>)`,wrap:!1}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p:H,i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Ds(w){let n,i;return n=new I({props:{$$slots:{default:[Qs]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function Os(w){let n,i="Create a model from your custom configuration with <code>AutoModel.from_config()</code>:",t,a,f;return a=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbCUwQSUwQW15X21vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fY29uZmlnKG15X2NvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = AutoModel.from_config(my_config)`,wrap:!1}}),{c(){n=T("p"),n.innerHTML=i,t=o(),d(a.$$.fragment)},l(m){n=y(m,"P",{"data-svelte-h":!0}),j(n)!=="svelte-1ibmd4l"&&(n.innerHTML=i),t=p(m),c(a.$$.fragment,m)},m(m,v){r(m,n,v),r(m,t,v),$(a,m,v),f=!0},p:H,i(m){f||(g(a.$$.fragment,m),f=!0)},o(m){h(a.$$.fragment,m),f=!1},d(m){m&&(l(n),l(t)),b(a,m)}}}function Ks(w){let n,i;return n=new I({props:{$$slots:{default:[Os]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function el(w){let n,i="Create a model from your custom configuration with <code>TFAutoModel.from_config()</code>:",t,a,f;return a=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsJTBBJTBBbXlfbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbC5mcm9tX2NvbmZpZyhteV9jb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span>my_model = TFAutoModel.from_config(my_config)`,wrap:!1}}),{c(){n=T("p"),n.innerHTML=i,t=o(),d(a.$$.fragment)},l(m){n=y(m,"P",{"data-svelte-h":!0}),j(n)!=="svelte-1g5jr9f"&&(n.innerHTML=i),t=p(m),c(a.$$.fragment,m)},m(m,v){r(m,n,v),r(m,t,v),$(a,m,v),f=!0},p:H,i(m){f||(g(a.$$.fragment,m),f=!0)},o(m){h(a.$$.fragment,m),f=!1},d(m){m&&(l(n),l(t)),b(a,m)}}}function tl(w){let n,i;return n=new I({props:{$$slots:{default:[el]},$$scope:{ctx:w}}}),{c(){d(n.$$.fragment)},l(t){c(n.$$.fragment,t)},m(t,a){$(n,t,a),i=!0},p(t,a){const f={};a&2&&(f.$$scope={dirty:a,ctx:t}),n.$set(f)},i(t){i||(g(n.$$.fragment,t),i=!0)},o(t){h(n.$$.fragment,t),i=!1},d(t){b(n,t)}}}function nl(w){let n,i,t,a,f,m,v,J,Z,z='Mit ü§ó Transformers k√∂nnen Sie sofort loslegen! Verwenden Sie die <code>pipeline()</code> f√ºr schnelle Inferenz und laden Sie schnell ein vortrainiertes Modell und einen Tokenizer mit einer <a href="./model_doc/auto">AutoClass</a>, um Ihre Text-, Bild- oder Audioaufgabe zu l√∂sen.',M,_,W,R,x,C,G="<code>pipeline()</code> ist der einfachste Weg, ein vortrainiertes Modell f√ºr eine bestimmte Aufgabe zu verwenden.",u,k,F,Q,Mn="Die <code>pipeline()</code> unterst√ºtzt viele g√§ngige Aufgaben:",st,D,wn="<strong>Text</strong>:",lt,O,Tn="<li>Stimmungsanalyse: Klassifizierung der Polarit√§t eines gegebenen Textes.</li> <li>Textgenerierung (auf Englisch): Generierung von Text aus einer gegebenen Eingabe.</li> <li>Name-Entity-Recognition (NER): Kennzeichnung jedes Worts mit der Entit√§t, die es repr√§sentiert (Person, Datum, Ort usw.).</li> <li>Beantwortung von Fragen: Extrahieren der Antwort aus dem Kontext, wenn ein gewisser Kontext und eine Frage gegeben sind.</li> <li>Fill-mask: Ausf√ºllen von L√ºcken in einem Text mit maskierten W√∂rtern.</li> <li>Zusammenfassung: Erstellung einer Zusammenfassung einer langen Text- oder Dokumentensequenz.</li> <li>√úbersetzung: √úbersetzen eines Textes in eine andere Sprache.</li> <li>Merkmalsextraktion: Erstellen einer Tensordarstellung des Textes.</li>",at,K,yn="<strong>Bild</strong>:",rt,ee,jn="<li>Bildklassifizierung: Klassifizierung eines Bildes.</li> <li>Bildsegmentierung: Klassifizierung jedes Pixels in einem Bild.</li> <li>Objekterkennung: Erkennen von Objekten innerhalb eines Bildes.</li>",it,te,kn="<strong>Audio</strong>:",ot,ne,vn="<li>Audioklassifizierung: Zuweisung eines Labels zu einem bestimmten Audiosegment.</li> <li>Automatische Spracherkennung (ASR): Transkription von Audiodaten in Text.</li>",pt,V,mt,se,ut,le,_n="Im folgenden Beispiel werden Sie die <code>pipeline()</code> f√ºr die Stimmungsanalyse verwenden.",ft,ae,Un="Installieren Sie die folgenden Abh√§ngigkeiten, falls Sie dies nicht bereits getan haben:",dt,A,ct,re,Jn="Importieren sie die <code>pipeline()</code> und spezifizieren sie die Aufgabe, welche sie l√∂sen m√∂chten:",$t,ie,gt,oe,Zn='Die Pipeline l√§dt ein standardm√§√üiges <a href="https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english" rel="nofollow">vortrainiertes Modell</a> und einen Tokenizer f√ºr die Stimmungs-Analyse herunter und speichert sie. Jetzt k√∂nnen Sie den ‚ÄúKlassifikator‚Äù auf Ihren Zieltext anwenden:',ht,pe,bt,me,Cn="For more than one sentence, pass a list of sentences to the <code>pipeline()</code> which returns a list of dictionaries:",Mt,ue,wt,fe,zn='Die <code>pipeline()</code> kann auch √ºber einen ganzen Datensatz iterieren. Starten wir mit der Installation der <a href="https://huggingface.co/docs/datasets/" rel="nofollow">ü§ó Datasets</a> Bibliothek:',Tt,de,yt,ce,Wn="Erstellen wir eine <code>pipeline()</code> mit der Aufgabe die wir l√∂sen und dem Modell welches wir nutzen m√∂chten.",jt,$e,kt,ge,Hn='Als n√§chstes laden wir den Datensatz (siehe ü§ó Datasets <a href="https://huggingface.co/docs/datasets/quickstart" rel="nofollow">Quick Start</a> f√ºr mehr Details) welches wir nutzen m√∂chten. Zum Beispiel laden wir den <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">MInDS-14</a> Datensatz:',vt,he,_t,be,xn="Wir m√ºssen sicherstellen, dass die Abtastrate des Datensatzes der Abtastrate entspricht, mit der <code>facebook/wav2vec2-base-960h</code> trainiert wurde.",Ut,Me,Jt,we,In=`Audiodateien werden automatisch geladen und neu abgetastet, wenn die Spalte ‚Äúaudio‚Äù aufgerufen wird.
Extrahieren wir die rohen Wellenform-Arrays der ersten 4 Beispiele und √ºbergeben wir sie als Liste an die Pipeline:`,Zt,Te,Ct,ye,Rn='Bei einem gr√∂√üeren Datensatz mit vielen Eingaben (wie bei Sprache oder Bildverarbeitung) sollten Sie einen Generator anstelle einer Liste √ºbergeben, der alle Eingaben in den Speicher l√§dt. Weitere Informationen finden Sie in der <a href="./main_classes/pipelines">Pipeline-Dokumentation</a>.',zt,je,Wt,ke,Gn='Die <code>pipeline()</code> kann jedes Modell aus dem <a href="https://huggingface.co/models" rel="nofollow">Model Hub</a> verwenden, wodurch es einfach ist, die <code>pipeline()</code> f√ºr andere Anwendungsf√§lle anzupassen. Wenn Sie beispielsweise ein Modell w√ºnschen, das franz√∂sischen Text verarbeiten kann, verwenden Sie die Tags im Model Hub, um nach einem geeigneten Modell zu filtern. Das oberste gefilterte Ergebnis liefert ein mehrsprachiges <a href="https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment" rel="nofollow">BERT-Modell</a>, das auf die Stimmungsanalyse abgestimmt ist. Gro√üartig, verwenden wir dieses Modell!',Ht,ve,xt,B,It,_e,Fn="Dann k√∂nnen Sie das Modell und den Tokenizer in der <code>pipeline()</code> angeben und den <code>Klassifikator</code> auf Ihren Zieltext anwenden:",Rt,Ue,Gt,Je,Sn='Wenn Sie kein Modell f√ºr Ihren Anwendungsfall finden k√∂nnen, m√ºssen Sie ein vortrainiertes Modell auf Ihren Daten feinabstimmen. Schauen Sie sich unser <a href="./training">Feinabstimmungs-Tutorial</a> an, um zu erfahren, wie das geht. Und schlie√ülich, nachdem Sie Ihr trainiertes Modell verfeinert haben, sollten Sie es mit der Community im Model Hub teilen (siehe Tutorial <a href="./model_sharing">hier</a>), um NLP f√ºr alle zu demokratisieren! ü§ó',Ft,Ze,St,Ce,Vt,ze,Vn='Unter der Haube arbeiten die Klassen <code>AutoModelForSequenceClassification</code> und <code>AutoTokenizer</code> zusammen, um die <code>pipeline()</code> zu betreiben. Eine <a href="./model_doc/auto"><code>AutoClass</code></a> ist eine Abk√ºrzung, die automatisch die Architektur eines trainierten Modells aus dessen Namen oder Pfad abruft. Sie m√ºssen nur die passende <code>AutoClass</code> f√ºr Ihre Aufgabe und den zugeh√∂rigen Tokenizer mit <code>AutoTokenizer</code> ausw√§hlen.',At,We,An="Kehren wir zu unserem Beispiel zur√ºck und sehen wir uns an, wie Sie die <code>AutoClass</code> verwenden k√∂nnen, um die Ergebnisse der <code>pipeline()</code> zu replizieren.",Bt,He,Et,xe,Bn=`Ein Tokenizer ist f√ºr die Vorverarbeitung von Text in ein f√ºr das Modell verst√§ndliches Format zust√§ndig. Zun√§chst zerlegt der Tokenisierer den Text in W√∂rter, die <em>Token</em> genannt werden. Es gibt mehrere Regeln f√ºr den Tokenisierungsprozess, z. B. wie und auf welcher Ebene ein Wort aufgespalten wird (weitere Informationen √ºber Tokenisierung <a href="./tokenizer_summary">hier</a>). Das Wichtigste ist jedoch, dass Sie den Tokenizer mit demselben Modellnamen instanziieren m√ºssen, um sicherzustellen, dass Sie dieselben Tokenisierungsregeln verwenden, mit denen ein Modell zuvor trainiert wurde.
Laden sie einen Tokenizer mit <code>AutoTokenizer</code>:`,Nt,Ie,Xt,Re,En="Anschlie√üend wandelt der Tokenizer die Token in Zahlen um, um einen Tensor als Eingabe f√ºr das Modell zu konstruieren. Dieser wird als <em>Vokabular</em> des Modells bezeichnet.",Lt,Ge,Nn="√úbergeben Sie Ihren Text an den Tokenizer:",Yt,Fe,Pt,Se,Xn="Der Tokenizer gibt ein W√∂rterbuch zur√ºck, das Folgendes enth√§lt:",qt,Ve,Ln='<li><a href="./glossary#input-ids">input_ids</a>: numerische Repr√§sentationen Ihrer Token.</li> <li><a href=".glossary#attention-mask">atttention_mask</a>: gibt an, welche Token beachtet werden sollen.</li>',Qt,Ae,Yn="Genau wie die <code>pipeline()</code> akzeptiert der Tokenizer eine Liste von Eingaben. Dar√ºber hinaus kann der Tokenizer den Text auch auff√ºllen und k√ºrzen, um einen Stapel mit einheitlicher L√§nge zur√ºckzugeben:",Dt,E,Ot,Be,Pn='Lesen Sie das Tutorial <a href="./preprocessing">preprocessing</a> f√ºr weitere Details zur Tokenisierung.',Kt,Ee,en,N,tn,X,nn,Ne,qn='Modelle sind ein standardm√§√üiges <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow"><code>torch.nn.Module</code></a> oder ein <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow"><code>tf.keras.Model</code></a>, sodass Sie sie in Ihrer √ºblichen Trainingsschleife verwenden k√∂nnen. Um jedoch die Dinge einfacher zu machen, bietet ü§ó Transformers eine <code>Trainer</code>-Klasse f√ºr PyTorch, die Funktionalit√§t f√ºr verteiltes Training, gemischte Pr√§zision und mehr bietet. F√ºr TensorFlow k√∂nnen Sie die Methode <code>fit</code> aus <a href="https://keras.io/" rel="nofollow">Keras</a> verwenden. Siehe das <a href="./training">training tutorial</a> f√ºr weitere Details.',sn,L,ln,Xe,an,Y,rn,Le,Qn="Ein besonders cooles ü§ó Transformers-Feature ist die M√∂glichkeit, ein Modell zu speichern und es entweder als PyTorch- oder TensorFlow-Modell wieder zu laden. Der Parameter ‚Äúfrom_pt‚Äù oder ‚Äúfrom_tf‚Äù kann das Modell von einem Framework in das andere konvertieren:",on,P,pn,Ye,mn,Pe,Dn="Sie k√∂nnen die Konfigurationsklasse des Modells √§ndern, um zu bestimmen, wie ein Modell aufgebaut ist. Die Konfiguration legt die Attribute eines Modells fest, z. B. die Anzahl der verborgenen Schichten oder der Aufmerksamkeitsk√∂pfe. Wenn Sie ein Modell aus einer benutzerdefinierten Konfigurationsklasse initialisieren, beginnen Sie bei Null. Die Modellattribute werden zuf√§llig initialisiert, und Sie m√ºssen das Modell trainieren, bevor Sie es verwenden k√∂nnen, um aussagekr√§ftige Ergebnisse zu erhalten.",un,qe,On="Beginnen Sie mit dem Import von <code>AutoConfig</code> und laden Sie dann das trainierte Modell, das Sie √§ndern m√∂chten. Innerhalb von <code>AutoConfig.from_pretrained()</code> k√∂nnen Sie das Attribut angeben, das Sie √§ndern m√∂chten, z. B. die Anzahl der Aufmerksamkeitsk√∂pfe:",fn,Qe,dn,q,cn,De,Kn='Weitere Informationen zur Erstellung von benutzerdefinierten Konfigurationen finden Sie in der Anleitung <a href="./create_a_model">Erstellen einer benutzerdefinierten Architektur</a>.',$n,Oe,gn,Ke,es="Nachdem Sie nun die ü§ó Transformers-Kurztour abgeschlossen haben, schauen Sie sich unsere Anleitungen an und erfahren Sie, wie Sie spezifischere Dinge tun k√∂nnen, wie das Schreiben eines benutzerdefinierten Modells, die Feinabstimmung eines Modells f√ºr eine Aufgabe und wie man ein Modell mit einem Skript trainiert. Wenn Sie mehr √ºber die Kernkonzepte von ü§ó Transformers erfahren m√∂chten, nehmen Sie sich eine Tasse Kaffee und werfen Sie einen Blick auf unsere konzeptionellen Leitf√§den!",hn,nt,bn;return f=new S({props:{title:"Schnellstart",local:"schnellstart",headingTag:"h1"}}),v=new Ts({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/de/quicktour.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/de/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/de/tensorflow/quicktour.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/de/quicktour.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/de/pytorch/quicktour.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/de/tensorflow/quicktour.ipynb"}]}}),_=new tt({props:{$$slots:{default:[ys]},$$scope:{ctx:w}}}),R=new S({props:{title:"Pipeline",local:"pipeline",headingTag:"h2"}}),k=new cs({props:{id:"tiZFewofSLM"}}),V=new tt({props:{$$slots:{default:[js]},$$scope:{ctx:w}}}),se=new S({props:{title:"Verwendung der Pipeline",local:"verwendung-der-pipeline",headingTag:"h3"}}),A=new et({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Us],pytorch:[vs]},$$scope:{ctx:w}}}),ie=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMnNlbnRpbWVudC1hbmFseXNpcyUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>)`,wrap:!1}}),pe=new U({props:{code:"Y2xhc3NpZmllciglMjJXZSUyMGFyZSUyMHZlcnklMjBoYXBweSUyMHRvJTIwc2hvdyUyMHlvdSUyMHRoZSUyMCVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycyUyMGxpYnJhcnkuJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;POSITIVE&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.9998</span>}]`,wrap:!1}}),ue=new U({props:{code:"cmVzdWx0cyUyMCUzRCUyMGNsYXNzaWZpZXIoJTVCJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiUyQyUyMCUyMldlJTIwaG9wZSUyMHlvdSUyMGRvbid0JTIwaGF0ZSUyMGl0LiUyMiU1RCklMEFmb3IlMjByZXN1bHQlMjBpbiUyMHJlc3VsdHMlM0ElMEElMjAlMjAlMjAlMjBwcmludChmJTIybGFiZWwlM0ElMjAlN0JyZXN1bHQlNUInbGFiZWwnJTVEJTdEJTJDJTIwd2l0aCUyMHNjb3JlJTNBJTIwJTdCcm91bmQocmVzdWx0JTVCJ3Njb3JlJyU1RCUyQyUyMDQpJTdEJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>results = classifier([<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span>, <span class="hljs-string">&quot;We hope you don&#x27;t hate it.&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> results:
<span class="hljs-meta">... </span>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;label: <span class="hljs-subst">{result[<span class="hljs-string">&#x27;label&#x27;</span>]}</span>, with score: <span class="hljs-subst">{<span class="hljs-built_in">round</span>(result[<span class="hljs-string">&#x27;score&#x27;</span>], <span class="hljs-number">4</span>)}</span>&quot;</span>)
label: POSITIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.9998</span>
label: NEGATIVE, <span class="hljs-keyword">with</span> score: <span class="hljs-number">0.5309</span>`,wrap:!1}}),de=new U({props:{code:"cGlwJTIwaW5zdGFsbCUyMGRhdGFzZXRzJTIw",highlighted:"pip install datasets ",wrap:!1}}),$e=new U({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwcGlwZWxpbmUlMEElMEFzcGVlY2hfcmVjb2duaXplciUyMCUzRCUyMHBpcGVsaW5lKCUyMmF1dG9tYXRpYy1zcGVlY2gtcmVjb2duaXRpb24lMjIlMkMlMjBtb2RlbCUzRCUyMmZhY2Vib29rJTJGd2F2MnZlYzItYmFzZS05NjBoJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>speech_recognizer = pipeline(<span class="hljs-string">&quot;automatic-speech-recognition&quot;</span>, model=<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)`,wrap:!1}}),he=new U({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMlBvbHlBSSUyRm1pbmRzMTQlMjIlMkMlMjBuYW1lJTNEJTIyZW4tVVMlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`,wrap:!1}}),Me=new U({props:{code:"ZGF0YXNldCUyMCUzRCUyMGRhdGFzZXQuY2FzdF9jb2x1bW4oJTIyYXVkaW8lMjIlMkMlMjBBdWRpbyhzYW1wbGluZ19yYXRlJTNEc3BlZWNoX3JlY29nbml6ZXIuZmVhdHVyZV9leHRyYWN0b3Iuc2FtcGxpbmdfcmF0ZSkp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))',wrap:!1}}),Te=new U({props:{code:"cmVzdWx0JTIwJTNEJTIwc3BlZWNoX3JlY29nbml6ZXIoZGF0YXNldCU1QiUzQTQlNUQlNUIlMjJhdWRpbyUyMiU1RCklMEFwcmludCglNUJkJTVCJTIydGV4dCUyMiU1RCUyMGZvciUyMGQlMjBpbiUyMHJlc3VsdCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>result = speech_recognizer(dataset[:<span class="hljs-number">4</span>][<span class="hljs-string">&quot;audio&quot;</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>([d[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> d <span class="hljs-keyword">in</span> result])
[<span class="hljs-string">&#x27;I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT&#x27;</span>, <span class="hljs-string">&quot;FODING HOW I&#x27;D SET UP A JOIN TO HET WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;</span>, <span class="hljs-string">&quot;I I&#x27;D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I&#x27;M NOT SEEING THE OPTION TO DO IT ON THE AP SO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AND I&#x27;M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;</span>, <span class="hljs-string">&#x27;HOW DO I THURN A JOIN A COUNT&#x27;</span>]`,wrap:!1}}),je=new S({props:{title:"Ein anderes Modell und einen anderen Tokenizer in der Pipeline verwenden",local:"ein-anderes-modell-und-einen-anderen-tokenizer-in-der-pipeline-verwenden",headingTag:"h3"}}),ve=new U({props:{code:"bW9kZWxfbmFtZSUyMCUzRCUyMCUyMm5scHRvd24lMkZiZXJ0LWJhc2UtbXVsdGlsaW5ndWFsLXVuY2FzZWQtc2VudGltZW50JTIy",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>',wrap:!1}}),B=new et({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[zs],pytorch:[Zs]},$$scope:{ctx:w}}}),Ue=new U({props:{code:"Y2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMnNlbnRpbWVudC1hbmFseXNpcyUyMiUyQyUyMG1vZGVsJTNEbW9kZWwlMkMlMjB0b2tlbml6ZXIlM0R0b2tlbml6ZXIpJTBBY2xhc3NpZmllciglMjJOb3VzJTIwc29tbWVzJTIwdHIlQzMlQThzJTIwaGV1cmV1eCUyMGRlJTIwdm91cyUyMHByJUMzJUE5c2VudGVyJTIwbGElMjBiaWJsaW90aCVDMyVBOHF1ZSUyMCVGMCU5RiVBNCU5NyUyMFRyYW5zZm9ybWVycy4lMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;sentiment-analysis&quot;</span>, model=model, tokenizer=tokenizer)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(<span class="hljs-string">&quot;Nous sommes tr√®s heureux de vous pr√©senter la biblioth√®que ü§ó Transformers.&quot;</span>)
[{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;5 stars&#x27;</span>, <span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.7273</span>}]`,wrap:!1}}),Ze=new S({props:{title:"AutoClass",local:"autoclass",headingTag:"h2"}}),Ce=new cs({props:{id:"AhChOFRegn4"}}),He=new S({props:{title:"AutoTokenizer",local:"autotokenizer",headingTag:"h3"}}),Ie=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEFtb2RlbF9uYW1lJTIwJTNEJTIwJTIybmxwdG93biUyRmJlcnQtYmFzZS1tdWx0aWxpbmd1YWwtdW5jYXNlZC1zZW50aW1lbnQlMjIlMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9uYW1lKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>model_name = <span class="hljs-string">&quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(model_name)`,wrap:!1}}),Fe=new U({props:{code:"ZW5jb2RpbmclMjAlM0QlMjB0b2tlbml6ZXIoJTIyV2UlMjBhcmUlMjB2ZXJ5JTIwaGFwcHklMjB0byUyMHNob3clMjB5b3UlMjB0aGUlMjAlRjAlOUYlQTQlOTclMjBUcmFuc2Zvcm1lcnMlMjBsaWJyYXJ5LiUyMiklMEFwcmludChlbmNvZGluZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = tokenizer(<span class="hljs-string">&quot;We are very happy to show you the ü§ó Transformers library.&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(encoding)
{<span class="hljs-string">&#x27;input_ids&#x27;</span>: [<span class="hljs-number">101</span>, <span class="hljs-number">11312</span>, <span class="hljs-number">10320</span>, <span class="hljs-number">12495</span>, <span class="hljs-number">19308</span>, <span class="hljs-number">10114</span>, <span class="hljs-number">11391</span>, <span class="hljs-number">10855</span>, <span class="hljs-number">10103</span>, <span class="hljs-number">100</span>, <span class="hljs-number">58263</span>, <span class="hljs-number">13299</span>, <span class="hljs-number">119</span>, <span class="hljs-number">102</span>],
 <span class="hljs-string">&#x27;token_type_ids&#x27;</span>: [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
 <span class="hljs-string">&#x27;attention_mask&#x27;</span>: [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]}`,wrap:!1}}),E=new et({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Is],pytorch:[Hs]},$$scope:{ctx:w}}}),Ee=new S({props:{title:"AutoModel",local:"automodel",headingTag:"h3"}}),N=new et({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[As],pytorch:[Fs]},$$scope:{ctx:w}}}),X=new tt({props:{$$slots:{default:[Bs]},$$scope:{ctx:w}}}),L=new tt({props:{$$slots:{default:[Es]},$$scope:{ctx:w}}}),Xe=new S({props:{title:"Modell speichern",local:"modell-speichern",headingTag:"h3"}}),Y=new et({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ys],pytorch:[Xs]},$$scope:{ctx:w}}}),P=new et({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Ds],pytorch:[qs]},$$scope:{ctx:w}}}),Ye=new S({props:{title:"Custom model builds",local:"custom-model-builds",headingTag:"h2"}}),Qe=new U({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMEElMEFteV9jb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJkaXN0aWxiZXJ0JTJGZGlzdGlsYmVydC1iYXNlLXVuY2FzZWQlMjIlMkMlMjBuX2hlYWRzJTNEMTIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span>my_config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;distilbert/distilbert-base-uncased&quot;</span>, n_heads=<span class="hljs-number">12</span>)`,wrap:!1}}),q=new et({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[tl],pytorch:[Ks]},$$scope:{ctx:w}}}),Oe=new S({props:{title:"Wie geht es weiter?",local:"wie-geht-es-weiter",headingTag:"h2"}}),{c(){n=T("meta"),i=o(),t=T("p"),a=o(),d(f.$$.fragment),m=o(),d(v.$$.fragment),J=o(),Z=T("p"),Z.innerHTML=z,M=o(),d(_.$$.fragment),W=o(),d(R.$$.fragment),x=o(),C=T("p"),C.innerHTML=G,u=o(),d(k.$$.fragment),F=o(),Q=T("p"),Q.innerHTML=Mn,st=o(),D=T("p"),D.innerHTML=wn,lt=o(),O=T("ul"),O.innerHTML=Tn,at=o(),K=T("p"),K.innerHTML=yn,rt=o(),ee=T("ul"),ee.innerHTML=jn,it=o(),te=T("p"),te.innerHTML=kn,ot=o(),ne=T("ul"),ne.innerHTML=vn,pt=o(),d(V.$$.fragment),mt=o(),d(se.$$.fragment),ut=o(),le=T("p"),le.innerHTML=_n,ft=o(),ae=T("p"),ae.textContent=Un,dt=o(),d(A.$$.fragment),ct=o(),re=T("p"),re.innerHTML=Jn,$t=o(),d(ie.$$.fragment),gt=o(),oe=T("p"),oe.innerHTML=Zn,ht=o(),d(pe.$$.fragment),bt=o(),me=T("p"),me.innerHTML=Cn,Mt=o(),d(ue.$$.fragment),wt=o(),fe=T("p"),fe.innerHTML=zn,Tt=o(),d(de.$$.fragment),yt=o(),ce=T("p"),ce.innerHTML=Wn,jt=o(),d($e.$$.fragment),kt=o(),ge=T("p"),ge.innerHTML=Hn,vt=o(),d(he.$$.fragment),_t=o(),be=T("p"),be.innerHTML=xn,Ut=o(),d(Me.$$.fragment),Jt=o(),we=T("p"),we.textContent=In,Zt=o(),d(Te.$$.fragment),Ct=o(),ye=T("p"),ye.innerHTML=Rn,zt=o(),d(je.$$.fragment),Wt=o(),ke=T("p"),ke.innerHTML=Gn,Ht=o(),d(ve.$$.fragment),xt=o(),d(B.$$.fragment),It=o(),_e=T("p"),_e.innerHTML=Fn,Rt=o(),d(Ue.$$.fragment),Gt=o(),Je=T("p"),Je.innerHTML=Sn,Ft=o(),d(Ze.$$.fragment),St=o(),d(Ce.$$.fragment),Vt=o(),ze=T("p"),ze.innerHTML=Vn,At=o(),We=T("p"),We.innerHTML=An,Bt=o(),d(He.$$.fragment),Et=o(),xe=T("p"),xe.innerHTML=Bn,Nt=o(),d(Ie.$$.fragment),Xt=o(),Re=T("p"),Re.innerHTML=En,Lt=o(),Ge=T("p"),Ge.textContent=Nn,Yt=o(),d(Fe.$$.fragment),Pt=o(),Se=T("p"),Se.textContent=Xn,qt=o(),Ve=T("ul"),Ve.innerHTML=Ln,Qt=o(),Ae=T("p"),Ae.innerHTML=Yn,Dt=o(),d(E.$$.fragment),Ot=o(),Be=T("p"),Be.innerHTML=Pn,Kt=o(),d(Ee.$$.fragment),en=o(),d(N.$$.fragment),tn=o(),d(X.$$.fragment),nn=o(),Ne=T("p"),Ne.innerHTML=qn,sn=o(),d(L.$$.fragment),ln=o(),d(Xe.$$.fragment),an=o(),d(Y.$$.fragment),rn=o(),Le=T("p"),Le.textContent=Qn,on=o(),d(P.$$.fragment),pn=o(),d(Ye.$$.fragment),mn=o(),Pe=T("p"),Pe.textContent=Dn,un=o(),qe=T("p"),qe.innerHTML=On,fn=o(),d(Qe.$$.fragment),dn=o(),d(q.$$.fragment),cn=o(),De=T("p"),De.innerHTML=Kn,$n=o(),d(Oe.$$.fragment),gn=o(),Ke=T("p"),Ke.textContent=es,hn=o(),nt=T("p"),this.h()},l(e){const s=Ms("svelte-u9bgzb",document.head);n=y(s,"META",{name:!0,content:!0}),s.forEach(l),i=p(e),t=y(e,"P",{}),fs(t).forEach(l),a=p(e),c(f.$$.fragment,e),m=p(e),c(v.$$.fragment,e),J=p(e),Z=y(e,"P",{"data-svelte-h":!0}),j(Z)!=="svelte-17aqmc5"&&(Z.innerHTML=z),M=p(e),c(_.$$.fragment,e),W=p(e),c(R.$$.fragment,e),x=p(e),C=y(e,"P",{"data-svelte-h":!0}),j(C)!=="svelte-1n211b1"&&(C.innerHTML=G),u=p(e),c(k.$$.fragment,e),F=p(e),Q=y(e,"P",{"data-svelte-h":!0}),j(Q)!=="svelte-1in5mry"&&(Q.innerHTML=Mn),st=p(e),D=y(e,"P",{"data-svelte-h":!0}),j(D)!=="svelte-1xs7k56"&&(D.innerHTML=wn),lt=p(e),O=y(e,"UL",{"data-svelte-h":!0}),j(O)!=="svelte-ligh71"&&(O.innerHTML=Tn),at=p(e),K=y(e,"P",{"data-svelte-h":!0}),j(K)!=="svelte-a2b524"&&(K.innerHTML=yn),rt=p(e),ee=y(e,"UL",{"data-svelte-h":!0}),j(ee)!=="svelte-p9z2u4"&&(ee.innerHTML=jn),it=p(e),te=y(e,"P",{"data-svelte-h":!0}),j(te)!=="svelte-hdckv1"&&(te.innerHTML=kn),ot=p(e),ne=y(e,"UL",{"data-svelte-h":!0}),j(ne)!=="svelte-1a76nqx"&&(ne.innerHTML=vn),pt=p(e),c(V.$$.fragment,e),mt=p(e),c(se.$$.fragment,e),ut=p(e),le=y(e,"P",{"data-svelte-h":!0}),j(le)!=="svelte-wiinot"&&(le.innerHTML=_n),ft=p(e),ae=y(e,"P",{"data-svelte-h":!0}),j(ae)!=="svelte-1dwmh3a"&&(ae.textContent=Un),dt=p(e),c(A.$$.fragment,e),ct=p(e),re=y(e,"P",{"data-svelte-h":!0}),j(re)!=="svelte-9izc39"&&(re.innerHTML=Jn),$t=p(e),c(ie.$$.fragment,e),gt=p(e),oe=y(e,"P",{"data-svelte-h":!0}),j(oe)!=="svelte-11vi8zf"&&(oe.innerHTML=Zn),ht=p(e),c(pe.$$.fragment,e),bt=p(e),me=y(e,"P",{"data-svelte-h":!0}),j(me)!=="svelte-3exaep"&&(me.innerHTML=Cn),Mt=p(e),c(ue.$$.fragment,e),wt=p(e),fe=y(e,"P",{"data-svelte-h":!0}),j(fe)!=="svelte-14esals"&&(fe.innerHTML=zn),Tt=p(e),c(de.$$.fragment,e),yt=p(e),ce=y(e,"P",{"data-svelte-h":!0}),j(ce)!=="svelte-n2uv7l"&&(ce.innerHTML=Wn),jt=p(e),c($e.$$.fragment,e),kt=p(e),ge=y(e,"P",{"data-svelte-h":!0}),j(ge)!=="svelte-1nj4h1c"&&(ge.innerHTML=Hn),vt=p(e),c(he.$$.fragment,e),_t=p(e),be=y(e,"P",{"data-svelte-h":!0}),j(be)!=="svelte-1e0gv19"&&(be.innerHTML=xn),Ut=p(e),c(Me.$$.fragment,e),Jt=p(e),we=y(e,"P",{"data-svelte-h":!0}),j(we)!=="svelte-hz1odv"&&(we.textContent=In),Zt=p(e),c(Te.$$.fragment,e),Ct=p(e),ye=y(e,"P",{"data-svelte-h":!0}),j(ye)!=="svelte-1h0ynny"&&(ye.innerHTML=Rn),zt=p(e),c(je.$$.fragment,e),Wt=p(e),ke=y(e,"P",{"data-svelte-h":!0}),j(ke)!=="svelte-b6cavk"&&(ke.innerHTML=Gn),Ht=p(e),c(ve.$$.fragment,e),xt=p(e),c(B.$$.fragment,e),It=p(e),_e=y(e,"P",{"data-svelte-h":!0}),j(_e)!=="svelte-1e4dzxj"&&(_e.innerHTML=Fn),Rt=p(e),c(Ue.$$.fragment,e),Gt=p(e),Je=y(e,"P",{"data-svelte-h":!0}),j(Je)!=="svelte-1yeo5fa"&&(Je.innerHTML=Sn),Ft=p(e),c(Ze.$$.fragment,e),St=p(e),c(Ce.$$.fragment,e),Vt=p(e),ze=y(e,"P",{"data-svelte-h":!0}),j(ze)!=="svelte-1dcmhdw"&&(ze.innerHTML=Vn),At=p(e),We=y(e,"P",{"data-svelte-h":!0}),j(We)!=="svelte-ynpqb8"&&(We.innerHTML=An),Bt=p(e),c(He.$$.fragment,e),Et=p(e),xe=y(e,"P",{"data-svelte-h":!0}),j(xe)!=="svelte-1hntns8"&&(xe.innerHTML=Bn),Nt=p(e),c(Ie.$$.fragment,e),Xt=p(e),Re=y(e,"P",{"data-svelte-h":!0}),j(Re)!=="svelte-bsho7f"&&(Re.innerHTML=En),Lt=p(e),Ge=y(e,"P",{"data-svelte-h":!0}),j(Ge)!=="svelte-13aqyyl"&&(Ge.textContent=Nn),Yt=p(e),c(Fe.$$.fragment,e),Pt=p(e),Se=y(e,"P",{"data-svelte-h":!0}),j(Se)!=="svelte-1hdgm35"&&(Se.textContent=Xn),qt=p(e),Ve=y(e,"UL",{"data-svelte-h":!0}),j(Ve)!=="svelte-1uywgiq"&&(Ve.innerHTML=Ln),Qt=p(e),Ae=y(e,"P",{"data-svelte-h":!0}),j(Ae)!=="svelte-1t8f72n"&&(Ae.innerHTML=Yn),Dt=p(e),c(E.$$.fragment,e),Ot=p(e),Be=y(e,"P",{"data-svelte-h":!0}),j(Be)!=="svelte-ht6hdn"&&(Be.innerHTML=Pn),Kt=p(e),c(Ee.$$.fragment,e),en=p(e),c(N.$$.fragment,e),tn=p(e),c(X.$$.fragment,e),nn=p(e),Ne=y(e,"P",{"data-svelte-h":!0}),j(Ne)!=="svelte-lthixs"&&(Ne.innerHTML=qn),sn=p(e),c(L.$$.fragment,e),ln=p(e),c(Xe.$$.fragment,e),an=p(e),c(Y.$$.fragment,e),rn=p(e),Le=y(e,"P",{"data-svelte-h":!0}),j(Le)!=="svelte-je327v"&&(Le.textContent=Qn),on=p(e),c(P.$$.fragment,e),pn=p(e),c(Ye.$$.fragment,e),mn=p(e),Pe=y(e,"P",{"data-svelte-h":!0}),j(Pe)!=="svelte-1bbu1z2"&&(Pe.textContent=Dn),un=p(e),qe=y(e,"P",{"data-svelte-h":!0}),j(qe)!=="svelte-19f7yf9"&&(qe.innerHTML=On),fn=p(e),c(Qe.$$.fragment,e),dn=p(e),c(q.$$.fragment,e),cn=p(e),De=y(e,"P",{"data-svelte-h":!0}),j(De)!=="svelte-133dhu9"&&(De.innerHTML=Kn),$n=p(e),c(Oe.$$.fragment,e),gn=p(e),Ke=y(e,"P",{"data-svelte-h":!0}),j(Ke)!=="svelte-7q9nr"&&(Ke.textContent=es),hn=p(e),nt=y(e,"P",{}),fs(nt).forEach(l),this.h()},h(){ds(n,"name","hf:doc:metadata"),ds(n,"content",sl)},m(e,s){ws(document.head,n),r(e,i,s),r(e,t,s),r(e,a,s),$(f,e,s),r(e,m,s),$(v,e,s),r(e,J,s),r(e,Z,s),r(e,M,s),$(_,e,s),r(e,W,s),$(R,e,s),r(e,x,s),r(e,C,s),r(e,u,s),$(k,e,s),r(e,F,s),r(e,Q,s),r(e,st,s),r(e,D,s),r(e,lt,s),r(e,O,s),r(e,at,s),r(e,K,s),r(e,rt,s),r(e,ee,s),r(e,it,s),r(e,te,s),r(e,ot,s),r(e,ne,s),r(e,pt,s),$(V,e,s),r(e,mt,s),$(se,e,s),r(e,ut,s),r(e,le,s),r(e,ft,s),r(e,ae,s),r(e,dt,s),$(A,e,s),r(e,ct,s),r(e,re,s),r(e,$t,s),$(ie,e,s),r(e,gt,s),r(e,oe,s),r(e,ht,s),$(pe,e,s),r(e,bt,s),r(e,me,s),r(e,Mt,s),$(ue,e,s),r(e,wt,s),r(e,fe,s),r(e,Tt,s),$(de,e,s),r(e,yt,s),r(e,ce,s),r(e,jt,s),$($e,e,s),r(e,kt,s),r(e,ge,s),r(e,vt,s),$(he,e,s),r(e,_t,s),r(e,be,s),r(e,Ut,s),$(Me,e,s),r(e,Jt,s),r(e,we,s),r(e,Zt,s),$(Te,e,s),r(e,Ct,s),r(e,ye,s),r(e,zt,s),$(je,e,s),r(e,Wt,s),r(e,ke,s),r(e,Ht,s),$(ve,e,s),r(e,xt,s),$(B,e,s),r(e,It,s),r(e,_e,s),r(e,Rt,s),$(Ue,e,s),r(e,Gt,s),r(e,Je,s),r(e,Ft,s),$(Ze,e,s),r(e,St,s),$(Ce,e,s),r(e,Vt,s),r(e,ze,s),r(e,At,s),r(e,We,s),r(e,Bt,s),$(He,e,s),r(e,Et,s),r(e,xe,s),r(e,Nt,s),$(Ie,e,s),r(e,Xt,s),r(e,Re,s),r(e,Lt,s),r(e,Ge,s),r(e,Yt,s),$(Fe,e,s),r(e,Pt,s),r(e,Se,s),r(e,qt,s),r(e,Ve,s),r(e,Qt,s),r(e,Ae,s),r(e,Dt,s),$(E,e,s),r(e,Ot,s),r(e,Be,s),r(e,Kt,s),$(Ee,e,s),r(e,en,s),$(N,e,s),r(e,tn,s),$(X,e,s),r(e,nn,s),r(e,Ne,s),r(e,sn,s),$(L,e,s),r(e,ln,s),$(Xe,e,s),r(e,an,s),$(Y,e,s),r(e,rn,s),r(e,Le,s),r(e,on,s),$(P,e,s),r(e,pn,s),$(Ye,e,s),r(e,mn,s),r(e,Pe,s),r(e,un,s),r(e,qe,s),r(e,fn,s),$(Qe,e,s),r(e,dn,s),$(q,e,s),r(e,cn,s),r(e,De,s),r(e,$n,s),$(Oe,e,s),r(e,gn,s),r(e,Ke,s),r(e,hn,s),r(e,nt,s),bn=!0},p(e,[s]){const ts={};s&2&&(ts.$$scope={dirty:s,ctx:e}),_.$set(ts);const ns={};s&2&&(ns.$$scope={dirty:s,ctx:e}),V.$set(ns);const ss={};s&2&&(ss.$$scope={dirty:s,ctx:e}),A.$set(ss);const ls={};s&2&&(ls.$$scope={dirty:s,ctx:e}),B.$set(ls);const as={};s&2&&(as.$$scope={dirty:s,ctx:e}),E.$set(as);const rs={};s&2&&(rs.$$scope={dirty:s,ctx:e}),N.$set(rs);const is={};s&2&&(is.$$scope={dirty:s,ctx:e}),X.$set(is);const os={};s&2&&(os.$$scope={dirty:s,ctx:e}),L.$set(os);const ps={};s&2&&(ps.$$scope={dirty:s,ctx:e}),Y.$set(ps);const ms={};s&2&&(ms.$$scope={dirty:s,ctx:e}),P.$set(ms);const us={};s&2&&(us.$$scope={dirty:s,ctx:e}),q.$set(us)},i(e){bn||(g(f.$$.fragment,e),g(v.$$.fragment,e),g(_.$$.fragment,e),g(R.$$.fragment,e),g(k.$$.fragment,e),g(V.$$.fragment,e),g(se.$$.fragment,e),g(A.$$.fragment,e),g(ie.$$.fragment,e),g(pe.$$.fragment,e),g(ue.$$.fragment,e),g(de.$$.fragment,e),g($e.$$.fragment,e),g(he.$$.fragment,e),g(Me.$$.fragment,e),g(Te.$$.fragment,e),g(je.$$.fragment,e),g(ve.$$.fragment,e),g(B.$$.fragment,e),g(Ue.$$.fragment,e),g(Ze.$$.fragment,e),g(Ce.$$.fragment,e),g(He.$$.fragment,e),g(Ie.$$.fragment,e),g(Fe.$$.fragment,e),g(E.$$.fragment,e),g(Ee.$$.fragment,e),g(N.$$.fragment,e),g(X.$$.fragment,e),g(L.$$.fragment,e),g(Xe.$$.fragment,e),g(Y.$$.fragment,e),g(P.$$.fragment,e),g(Ye.$$.fragment,e),g(Qe.$$.fragment,e),g(q.$$.fragment,e),g(Oe.$$.fragment,e),bn=!0)},o(e){h(f.$$.fragment,e),h(v.$$.fragment,e),h(_.$$.fragment,e),h(R.$$.fragment,e),h(k.$$.fragment,e),h(V.$$.fragment,e),h(se.$$.fragment,e),h(A.$$.fragment,e),h(ie.$$.fragment,e),h(pe.$$.fragment,e),h(ue.$$.fragment,e),h(de.$$.fragment,e),h($e.$$.fragment,e),h(he.$$.fragment,e),h(Me.$$.fragment,e),h(Te.$$.fragment,e),h(je.$$.fragment,e),h(ve.$$.fragment,e),h(B.$$.fragment,e),h(Ue.$$.fragment,e),h(Ze.$$.fragment,e),h(Ce.$$.fragment,e),h(He.$$.fragment,e),h(Ie.$$.fragment,e),h(Fe.$$.fragment,e),h(E.$$.fragment,e),h(Ee.$$.fragment,e),h(N.$$.fragment,e),h(X.$$.fragment,e),h(L.$$.fragment,e),h(Xe.$$.fragment,e),h(Y.$$.fragment,e),h(P.$$.fragment,e),h(Ye.$$.fragment,e),h(Qe.$$.fragment,e),h(q.$$.fragment,e),h(Oe.$$.fragment,e),bn=!1},d(e){e&&(l(i),l(t),l(a),l(m),l(J),l(Z),l(M),l(W),l(x),l(C),l(u),l(F),l(Q),l(st),l(D),l(lt),l(O),l(at),l(K),l(rt),l(ee),l(it),l(te),l(ot),l(ne),l(pt),l(mt),l(ut),l(le),l(ft),l(ae),l(dt),l(ct),l(re),l($t),l(gt),l(oe),l(ht),l(bt),l(me),l(Mt),l(wt),l(fe),l(Tt),l(yt),l(ce),l(jt),l(kt),l(ge),l(vt),l(_t),l(be),l(Ut),l(Jt),l(we),l(Zt),l(Ct),l(ye),l(zt),l(Wt),l(ke),l(Ht),l(xt),l(It),l(_e),l(Rt),l(Gt),l(Je),l(Ft),l(St),l(Vt),l(ze),l(At),l(We),l(Bt),l(Et),l(xe),l(Nt),l(Xt),l(Re),l(Lt),l(Ge),l(Yt),l(Pt),l(Se),l(qt),l(Ve),l(Qt),l(Ae),l(Dt),l(Ot),l(Be),l(Kt),l(en),l(tn),l(nn),l(Ne),l(sn),l(ln),l(an),l(rn),l(Le),l(on),l(pn),l(mn),l(Pe),l(un),l(qe),l(fn),l(dn),l(cn),l(De),l($n),l(gn),l(Ke),l(hn),l(nt)),l(n),b(f,e),b(v,e),b(_,e),b(R,e),b(k,e),b(V,e),b(se,e),b(A,e),b(ie,e),b(pe,e),b(ue,e),b(de,e),b($e,e),b(he,e),b(Me,e),b(Te,e),b(je,e),b(ve,e),b(B,e),b(Ue,e),b(Ze,e),b(Ce,e),b(He,e),b(Ie,e),b(Fe,e),b(E,e),b(Ee,e),b(N,e),b(X,e),b(L,e),b(Xe,e),b(Y,e),b(P,e),b(Ye,e),b(Qe,e),b(q,e),b(Oe,e)}}}const sl='{"title":"Schnellstart","local":"schnellstart","sections":[{"title":"Pipeline","local":"pipeline","sections":[{"title":"Verwendung der Pipeline","local":"verwendung-der-pipeline","sections":[],"depth":3},{"title":"Ein anderes Modell und einen anderen Tokenizer in der Pipeline verwenden","local":"ein-anderes-modell-und-einen-anderen-tokenizer-in-der-pipeline-verwenden","sections":[],"depth":3}],"depth":2},{"title":"AutoClass","local":"autoclass","sections":[{"title":"AutoTokenizer","local":"autotokenizer","sections":[],"depth":3},{"title":"AutoModel","local":"automodel","sections":[],"depth":3},{"title":"Modell speichern","local":"modell-speichern","sections":[],"depth":3}],"depth":2},{"title":"Custom model builds","local":"custom-model-builds","sections":[],"depth":2},{"title":"Wie geht es weiter?","local":"wie-geht-es-weiter","sections":[],"depth":2}],"depth":1}';function ll(w){return gs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class dl extends hs{constructor(n){super(),bs(this,n,ll,nl,$s,{})}}export{dl as component};
