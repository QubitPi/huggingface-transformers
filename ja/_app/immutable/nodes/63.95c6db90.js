import{s as st,o as at,n as nt}from"../chunks/scheduler.9bc65507.js";import{S as ot,i as rt,g as l,s,r as h,A as lt,h as i,f as n,c as a,j as W,u as g,x as d,k as G,y as r,a as o,v as k,d as b,t as T,w as M}from"../chunks/index.707bf1b6.js";import{T as it}from"../chunks/Tip.c2ecdbf4.js";import{D as le}from"../chunks/Docstring.17db21ae.js";import{C as Re}from"../chunks/CodeBlock.54a9f38d.js";import{E as pt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as Qe}from"../chunks/Heading.342b1fa6.js";function ct(F){let p,j='<li>この実装はトークン化方法を除いて BERT と同じです。その他の使用例については、<a href="bert">BERT のドキュメント</a> を参照してください。</li>';return{c(){p=l("ul"),p.innerHTML=j},l(_){p=i(_,"UL",{"data-svelte-h":!0}),d(p)!=="svelte-1wmwtfo"&&(p.innerHTML=j)},m(_,f){o(_,p,f)},p:nt,d(_){_&&n(p)}}}function mt(F){let p,j="pair mask has the following format:",_,f,w;return f=new Re({props:{code:"MCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAwJTIwMCUyMDAlMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMjAxJTIwMSUyMDElMEElN0MlMjBmaXJzdCUyMHNlcXVlbmNlJTIwJTIwJTIwJTIwJTdDJTIwc2Vjb25kJTIwc2VxdWVuY2UlMjAlN0M=",highlighted:`0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 0 </span>0<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1<span class="hljs-number"> 1 </span>1 1
| first sequence    | second sequence |`,wrap:!1}}),{c(){p=l("p"),p.textContent=j,_=s(),h(f.$$.fragment)},l(c){p=i(c,"P",{"data-svelte-h":!0}),d(p)!=="svelte-qjgeij"&&(p.textContent=j),_=a(c),g(f.$$.fragment,c)},m(c,v){o(c,p,v),o(c,_,v),k(f,c,v),w=!0},p:nt,i(c){w||(b(f.$$.fragment,c),w=!0)},o(c){T(f.$$.fragment,c),w=!1},d(c){c&&(n(p),n(_)),M(f,c)}}}function dt(F){let p,j,_,f,w,c,v,ie,B,He="BERT モデルは日本語テキストでトレーニングされました。",pe,q,Pe="2 つの異なるトークン化方法を備えたモデルがあります。",ce,U,Ze='<li>MeCab と WordPiece を使用してトークン化します。これには、<a href="https://taku910.github.io/mecab/" rel="nofollow">MeCab</a> のラッパーである <a href="https://github.com/polm/fugashi" rel="nofollow">fugashi</a> という追加の依存関係が必要です。</li> <li>文字にトークン化します。</li>',me,L,Ve=`<em>MecabTokenizer</em> を使用するには、<code>pip installTransformers[&quot;ja&quot;]</code> (または、インストールする場合は <code>pip install -e .[&quot;ja&quot;]</code>) する必要があります。
ソースから）依存関係をインストールします。`,de,D,Ae='<a href="https://github.com/cl-tohaku/bert-japanese" rel="nofollow">cl-tohakuリポジトリの詳細</a>を参照してください。',ue,Q,Ne="MeCab および WordPiece トークン化でモデルを使用する例:",_e,R,fe,H,Xe="文字トークン化を使用したモデルの使用例:",he,P,ge,y,ke,Z,Se='このモデルは<a href="https://huggingface.co/cl-tohaku" rel="nofollow">cl-tohaku</a>から提供されました。',be,V,Te,m,A,je,Y,Oe="Construct a BERT tokenizer for Japanese text.",Je,K,We=`This tokenizer inherits from <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer
to: this superclass for more information regarding those methods.`,xe,J,N,ye,ee,Ge=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERT sequence has the following format:`,ze,te,Fe="<li>single sequence: <code>[CLS] X [SEP]</code></li> <li>pair of sequences: <code>[CLS] A [SEP] B [SEP]</code></li>",Ce,z,X,Ee,ne,Ye="Converts a sequence of tokens (string) in a single string.",Ie,$,S,Be,se,Ke="Create a mask from the two sequences passed to be used in a sequence-pair classification task. A BERT sequence",qe,C,Ue,ae,et="If <code>token_ids_1</code> is <code>None</code>, this method only returns the first portion of the mask (0s).",Le,E,O,De,oe,tt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,Me,re,we;return w=new Qe({props:{title:"BertJapanese",local:"bertjapanese",headingTag:"h1"}}),v=new Qe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),R=new Re({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQWJlcnRqYXBhbmVzZSUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyY2wtdG9ob2t1JTJGYmVydC1iYXNlLWphcGFuZXNlJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmNsLXRvaG9rdSUyRmJlcnQtYmFzZS1qYXBhbmVzZSUyMiklMEElMEElMjMlMjMlMjBJbnB1dCUyMEphcGFuZXNlJTIwVGV4dCUwQWxpbmUlMjAlM0QlMjAlMjIlRTUlOTAlQkUlRTglQkMlQTklRTMlODElQUYlRTclOEMlQUIlRTMlODElQTclRTMlODElODIlRTMlODIlOEIlRTMlODAlODIlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIobGluZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShpbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQlNUIwJTVEKSklMEElMEFvdXRwdXRzJTIwJTNEJTIwYmVydGphcGFuZXNlKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>bertjapanese = AutoModel.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">## Input Japanese Text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;吾輩は猫である。&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(line, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]))
[CLS] 吾輩 は 猫 で ある 。 [SEP]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = bertjapanese(**inputs)`,wrap:!1}}),P=new Re({props:{code:"YmVydGphcGFuZXNlJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJjbC10b2hva3UlMkZiZXJ0LWJhc2UtamFwYW5lc2UtY2hhciUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJjbC10b2hva3UlMkZiZXJ0LWJhc2UtamFwYW5lc2UtY2hhciUyMiklMEElMEElMjMlMjMlMjBJbnB1dCUyMEphcGFuZXNlJTIwVGV4dCUwQWxpbmUlMjAlM0QlMjAlMjIlRTUlOTAlQkUlRTglQkMlQTklRTMlODElQUYlRTclOEMlQUIlRTMlODElQTclRTMlODElODIlRTMlODIlOEIlRTMlODAlODIlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIobGluZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBcHJpbnQodG9rZW5pemVyLmRlY29kZShpbnB1dHMlNUIlMjJpbnB1dF9pZHMlMjIlNUQlNUIwJTVEKSklMEElMEFvdXRwdXRzJTIwJTNEJTIwYmVydGphcGFuZXNlKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>bertjapanese = AutoModel.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese-char&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;cl-tohoku/bert-base-japanese-char&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment">## Input Japanese Text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;吾輩は猫である。&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(line, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(tokenizer.decode(inputs[<span class="hljs-string">&quot;input_ids&quot;</span>][<span class="hljs-number">0</span>]))
[CLS] 吾 輩 は 猫 で あ る 。 [SEP]

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = bertjapanese(**inputs)`,wrap:!1}}),y=new it({props:{$$slots:{default:[ct]},$$scope:{ctx:F}}}),V=new Qe({props:{title:"BertJapaneseTokenizer",local:"transformers.BertJapaneseTokenizer",headingTag:"h2"}}),A=new le({props:{name:"class transformers.BertJapaneseTokenizer",anchor:"transformers.BertJapaneseTokenizer",parameters:[{name:"vocab_file",val:""},{name:"spm_file",val:" = None"},{name:"do_lower_case",val:" = False"},{name:"do_word_tokenize",val:" = True"},{name:"do_subword_tokenize",val:" = True"},{name:"word_tokenizer_type",val:" = 'basic'"},{name:"subword_tokenizer_type",val:" = 'wordpiece'"},{name:"never_split",val:" = None"},{name:"unk_token",val:" = '[UNK]'"},{name:"sep_token",val:" = '[SEP]'"},{name:"pad_token",val:" = '[PAD]'"},{name:"cls_token",val:" = '[CLS]'"},{name:"mask_token",val:" = '[MASK]'"},{name:"mecab_kwargs",val:" = None"},{name:"sudachi_kwargs",val:" = None"},{name:"jumanpp_kwargs",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to a one-wordpiece-per-line vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertJapaneseTokenizer.spm_file",description:`<strong>spm_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to <a href="https://github.com/google/sentencepiece" rel="nofollow">SentencePiece</a> file (generally has a .spm or .model
extension) that contains the vocabulary.`,name:"spm_file"},{anchor:"transformers.BertJapaneseTokenizer.do_lower_case",description:`<strong>do_lower_case</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to lower case the input. Only has an effect when do_basic_tokenize=True.`,name:"do_lower_case"},{anchor:"transformers.BertJapaneseTokenizer.do_word_tokenize",description:`<strong>do_word_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to do word tokenization.`,name:"do_word_tokenize"},{anchor:"transformers.BertJapaneseTokenizer.do_subword_tokenize",description:`<strong>do_subword_tokenize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to do subword tokenization.`,name:"do_subword_tokenize"},{anchor:"transformers.BertJapaneseTokenizer.word_tokenizer_type",description:`<strong>word_tokenizer_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;basic&quot;</code>) &#x2014;
Type of word tokenizer. Choose from [&#x201C;basic&#x201D;, &#x201C;mecab&#x201D;, &#x201C;sudachi&#x201D;, &#x201C;jumanpp&#x201D;].`,name:"word_tokenizer_type"},{anchor:"transformers.BertJapaneseTokenizer.subword_tokenizer_type",description:`<strong>subword_tokenizer_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;wordpiece&quot;</code>) &#x2014;
Type of subword tokenizer. Choose from [&#x201C;wordpiece&#x201D;, &#x201C;character&#x201D;, &#x201C;sentencepiece&#x201D;,].`,name:"subword_tokenizer_type"},{anchor:"transformers.BertJapaneseTokenizer.mecab_kwargs",description:`<strong>mecab_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary passed to the <code>MecabTokenizer</code> constructor.`,name:"mecab_kwargs"},{anchor:"transformers.BertJapaneseTokenizer.sudachi_kwargs",description:`<strong>sudachi_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary passed to the <code>SudachiTokenizer</code> constructor.`,name:"sudachi_kwargs"},{anchor:"transformers.BertJapaneseTokenizer.jumanpp_kwargs",description:`<strong>jumanpp_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary passed to the <code>JumanppTokenizer</code> constructor.`,name:"jumanpp_kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L107"}}),N=new le({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertJapaneseTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertJapaneseTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L307",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),X=new le({props:{name:"convert_tokens_to_string",anchor:"transformers.BertJapaneseTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L299"}}),S=new le({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BertJapaneseTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertJapaneseTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L362",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#token-type-ids">token type IDs</a> according to the given sequence(s).</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),C=new pt({props:{anchor:"transformers.BertJapaneseTokenizer.create_token_type_ids_from_sequences.example",$$slots:{default:[mt]},$$scope:{ctx:F}}}),O=new le({props:{name:"get_special_tokens_mask",anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertJapaneseTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bert_japanese/tokenization_bert_japanese.py#L333",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),{c(){p=l("meta"),j=s(),_=l("p"),f=s(),h(w.$$.fragment),c=s(),h(v.$$.fragment),ie=s(),B=l("p"),B.textContent=He,pe=s(),q=l("p"),q.textContent=Pe,ce=s(),U=l("ul"),U.innerHTML=Ze,me=s(),L=l("p"),L.innerHTML=Ve,de=s(),D=l("p"),D.innerHTML=Ae,ue=s(),Q=l("p"),Q.textContent=Ne,_e=s(),h(R.$$.fragment),fe=s(),H=l("p"),H.textContent=Xe,he=s(),h(P.$$.fragment),ge=s(),h(y.$$.fragment),ke=s(),Z=l("p"),Z.innerHTML=Se,be=s(),h(V.$$.fragment),Te=s(),m=l("div"),h(A.$$.fragment),je=s(),Y=l("p"),Y.textContent=Oe,Je=s(),K=l("p"),K.innerHTML=We,xe=s(),J=l("div"),h(N.$$.fragment),ye=s(),ee=l("p"),ee.textContent=Ge,ze=s(),te=l("ul"),te.innerHTML=Fe,Ce=s(),z=l("div"),h(X.$$.fragment),Ee=s(),ne=l("p"),ne.textContent=Ye,Ie=s(),$=l("div"),h(S.$$.fragment),Be=s(),se=l("p"),se.textContent=Ke,qe=s(),h(C.$$.fragment),Ue=s(),ae=l("p"),ae.innerHTML=et,Le=s(),E=l("div"),h(O.$$.fragment),De=s(),oe=l("p"),oe.innerHTML=tt,Me=s(),re=l("p"),this.h()},l(e){const t=lt("svelte-u9bgzb",document.head);p=i(t,"META",{name:!0,content:!0}),t.forEach(n),j=a(e),_=i(e,"P",{}),W(_).forEach(n),f=a(e),g(w.$$.fragment,e),c=a(e),g(v.$$.fragment,e),ie=a(e),B=i(e,"P",{"data-svelte-h":!0}),d(B)!=="svelte-1h66ss7"&&(B.textContent=He),pe=a(e),q=i(e,"P",{"data-svelte-h":!0}),d(q)!=="svelte-1dntg98"&&(q.textContent=Pe),ce=a(e),U=i(e,"UL",{"data-svelte-h":!0}),d(U)!=="svelte-1absglq"&&(U.innerHTML=Ze),me=a(e),L=i(e,"P",{"data-svelte-h":!0}),d(L)!=="svelte-1ih4eq4"&&(L.innerHTML=Ve),de=a(e),D=i(e,"P",{"data-svelte-h":!0}),d(D)!=="svelte-o8cd2v"&&(D.innerHTML=Ae),ue=a(e),Q=i(e,"P",{"data-svelte-h":!0}),d(Q)!=="svelte-e6qqvy"&&(Q.textContent=Ne),_e=a(e),g(R.$$.fragment,e),fe=a(e),H=i(e,"P",{"data-svelte-h":!0}),d(H)!=="svelte-1rz0bcr"&&(H.textContent=Xe),he=a(e),g(P.$$.fragment,e),ge=a(e),g(y.$$.fragment,e),ke=a(e),Z=i(e,"P",{"data-svelte-h":!0}),d(Z)!=="svelte-rqfe8h"&&(Z.innerHTML=Se),be=a(e),g(V.$$.fragment,e),Te=a(e),m=i(e,"DIV",{class:!0});var u=W(m);g(A.$$.fragment,u),je=a(u),Y=i(u,"P",{"data-svelte-h":!0}),d(Y)!=="svelte-1tm7ou1"&&(Y.textContent=Oe),Je=a(u),K=i(u,"P",{"data-svelte-h":!0}),d(K)!=="svelte-1yon92w"&&(K.innerHTML=We),xe=a(u),J=i(u,"DIV",{class:!0});var x=W(J);g(N.$$.fragment,x),ye=a(x),ee=i(x,"P",{"data-svelte-h":!0}),d(ee)!=="svelte-t7qurq"&&(ee.textContent=Ge),ze=a(x),te=i(x,"UL",{"data-svelte-h":!0}),d(te)!=="svelte-xi6653"&&(te.innerHTML=Fe),x.forEach(n),Ce=a(u),z=i(u,"DIV",{class:!0});var $e=W(z);g(X.$$.fragment,$e),Ee=a($e),ne=i($e,"P",{"data-svelte-h":!0}),d(ne)!=="svelte-b3k2yi"&&(ne.textContent=Ye),$e.forEach(n),Ie=a(u),$=i(u,"DIV",{class:!0});var I=W($);g(S.$$.fragment,I),Be=a(I),se=i(I,"P",{"data-svelte-h":!0}),d(se)!=="svelte-gn6wi7"&&(se.textContent=Ke),qe=a(I),g(C.$$.fragment,I),Ue=a(I),ae=i(I,"P",{"data-svelte-h":!0}),d(ae)!=="svelte-owoxgn"&&(ae.innerHTML=et),I.forEach(n),Le=a(u),E=i(u,"DIV",{class:!0});var ve=W(E);g(O.$$.fragment,ve),De=a(ve),oe=i(ve,"P",{"data-svelte-h":!0}),d(oe)!=="svelte-1f4f5kp"&&(oe.innerHTML=tt),ve.forEach(n),u.forEach(n),Me=a(e),re=i(e,"P",{}),W(re).forEach(n),this.h()},h(){G(p,"name","hf:doc:metadata"),G(p,"content",ut),G(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),G(m,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){r(document.head,p),o(e,j,t),o(e,_,t),o(e,f,t),k(w,e,t),o(e,c,t),k(v,e,t),o(e,ie,t),o(e,B,t),o(e,pe,t),o(e,q,t),o(e,ce,t),o(e,U,t),o(e,me,t),o(e,L,t),o(e,de,t),o(e,D,t),o(e,ue,t),o(e,Q,t),o(e,_e,t),k(R,e,t),o(e,fe,t),o(e,H,t),o(e,he,t),k(P,e,t),o(e,ge,t),k(y,e,t),o(e,ke,t),o(e,Z,t),o(e,be,t),k(V,e,t),o(e,Te,t),o(e,m,t),k(A,m,null),r(m,je),r(m,Y),r(m,Je),r(m,K),r(m,xe),r(m,J),k(N,J,null),r(J,ye),r(J,ee),r(J,ze),r(J,te),r(m,Ce),r(m,z),k(X,z,null),r(z,Ee),r(z,ne),r(m,Ie),r(m,$),k(S,$,null),r($,Be),r($,se),r($,qe),k(C,$,null),r($,Ue),r($,ae),r(m,Le),r(m,E),k(O,E,null),r(E,De),r(E,oe),o(e,Me,t),o(e,re,t),we=!0},p(e,[t]){const u={};t&2&&(u.$$scope={dirty:t,ctx:e}),y.$set(u);const x={};t&2&&(x.$$scope={dirty:t,ctx:e}),C.$set(x)},i(e){we||(b(w.$$.fragment,e),b(v.$$.fragment,e),b(R.$$.fragment,e),b(P.$$.fragment,e),b(y.$$.fragment,e),b(V.$$.fragment,e),b(A.$$.fragment,e),b(N.$$.fragment,e),b(X.$$.fragment,e),b(S.$$.fragment,e),b(C.$$.fragment,e),b(O.$$.fragment,e),we=!0)},o(e){T(w.$$.fragment,e),T(v.$$.fragment,e),T(R.$$.fragment,e),T(P.$$.fragment,e),T(y.$$.fragment,e),T(V.$$.fragment,e),T(A.$$.fragment,e),T(N.$$.fragment,e),T(X.$$.fragment,e),T(S.$$.fragment,e),T(C.$$.fragment,e),T(O.$$.fragment,e),we=!1},d(e){e&&(n(j),n(_),n(f),n(c),n(ie),n(B),n(pe),n(q),n(ce),n(U),n(me),n(L),n(de),n(D),n(ue),n(Q),n(_e),n(fe),n(H),n(he),n(ge),n(ke),n(Z),n(be),n(Te),n(m),n(Me),n(re)),n(p),M(w,e),M(v,e),M(R,e),M(P,e),M(y,e),M(V,e),M(A),M(N),M(X),M(S),M(C),M(O)}}}const ut='{"title":"BertJapanese","local":"bertjapanese","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"BertJapaneseTokenizer","local":"transformers.BertJapaneseTokenizer","sections":[],"depth":2}],"depth":1}';function _t(F){return at(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class wt extends ot{constructor(p){super(),rt(this,p,_t,dt,st,{})}}export{wt as component};
