import{s as rt,o as at,n as it}from"../chunks/scheduler.9bc65507.js";import{S as lt,i as dt,g as a,s as o,r as f,A as mt,h as i,f as n,c as r,j as T,u as g,x as p,k as v,y as t,a as m,v as u,d as h,t as k,w as _}from"../chunks/index.707bf1b6.js";import{T as ct}from"../chunks/Tip.c2ecdbf4.js";import{D as I}from"../chunks/Docstring.17db21ae.js";import{C as pt}from"../chunks/CodeBlock.54a9f38d.js";import{H as ze}from"../chunks/Heading.342b1fa6.js";function ft(le){let c,q=`この実装は、トークン化方法を除いて BERT と同じです。詳細については、<a href="bert">BERT ドキュメント</a> を参照してください。
API リファレンス情報。`;return{c(){c=a("p"),c.innerHTML=q},l(b){c=i(b,"P",{"data-svelte-h":!0}),p(c)!=="svelte-19gvqi2"&&(c.innerHTML=q)},m(b,N){m(b,c,N)},p:it,d(b){b&&n(c)}}}function gt(le){let c,q,b,N,C,de,U,me,j,De='BERTweet モデルは、Dat Quoc Nguyen、Thanh Vu によって <a href="https://www.aclweb.org/anthology/2020.emnlp-demos.2.pdf" rel="nofollow">BERTweet: A pre-trained language model for English Tweets</a> で提案されました。アン・トゥアン・グエンさん。',ce,E,We="論文の要約は次のとおりです。",pe,R,Se=`<em>私たちは、英語ツイート用に初めて公開された大規模な事前トレーニング済み言語モデルである BERTweet を紹介します。私たちのBERTweetは、
BERT ベースと同じアーキテクチャ (Devlin et al., 2019) は、RoBERTa 事前トレーニング手順 (Liu et al.) を使用してトレーニングされます。
al.、2019）。実験では、BERTweet が強力なベースラインである RoBERTa ベースおよび XLM-R ベースを上回るパフォーマンスを示すことが示されています (Conneau et al.,
2020)、3 つのツイート NLP タスクにおいて、以前の最先端モデルよりも優れたパフォーマンス結果が得られました。
品詞タグ付け、固有表現認識およびテキスト分類。</em>`,fe,V,ge,L,ue,$,he,Z,Ae='このモデルは <a href="https://huggingface.co/dqnguyen" rel="nofollow">dqnguyen</a> によって提供されました。元のコードは <a href="https://github.com/VinAIResearch/BERTweet" rel="nofollow">ここ</a> にあります。',ke,F,_e,l,H,xe,G,Ne="Constructs a BERTweet tokenizer, using Byte-Pair-Encoding.",Be,Y,Ge=`This tokenizer inherits from <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,Ie,y,P,qe,O,Ye="Loads a pre-existing dictionary from a text file and adds its symbols to this instance.",Ce,w,Q,Ue,K,Oe=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A BERTweet sequence has the following format:`,je,ee,Ke="<li>single sequence: <code>&lt;s&gt; X &lt;/s&gt;</code></li> <li>pair of sequences: <code>&lt;s&gt; A &lt;/s&gt;&lt;/s&gt; B &lt;/s&gt;</code></li>",Ee,J,X,Re,te,et="Converts a sequence of tokens (string) in a single string.",Ve,M,D,Le,ne,tt=`Create a mask from the two sequences passed to be used in a sequence-pair classification task. BERTweet does
not make use of token type ids, therefore a list of zeros is returned.`,Ze,z,W,Fe,se,nt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,He,x,S,Pe,oe,st="Normalize tokens in a Tweet",Qe,B,A,Xe,re,ot="Normalize a raw Tweet",be,ie,we;return C=new ze({props:{title:"BERTweet",local:"bertweet",headingTag:"h1"}}),U=new ze({props:{title:"Overview",local:"overview",headingTag:"h2"}}),V=new ze({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),L=new pt({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQWJlcnR3ZWV0JTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJ2aW5haSUyRmJlcnR3ZWV0LWJhc2UlMjIpJTBBJTBBJTIzJTIwRm9yJTIwdHJhbnNmb3JtZXJzJTIwdjQueCUyQiUzQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMnZpbmFpJTJGYmVydHdlZXQtYmFzZSUyMiUyQyUyMHVzZV9mYXN0JTNERmFsc2UpJTBBJTBBJTIzJTIwRm9yJTIwdHJhbnNmb3JtZXJzJTIwdjMueCUzQSUwQSUyMyUyMHRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMnZpbmFpJTJGYmVydHdlZXQtYmFzZSUyMiklMEElMEElMjMlMjBJTlBVVCUyMFRXRUVUJTIwSVMlMjBBTFJFQURZJTIwTk9STUFMSVpFRCElMEFsaW5lJTIwJTNEJTIwJTIyU0MlMjBoYXMlMjBmaXJzdCUyMHR3byUyMHByZXN1bXB0aXZlJTIwY2FzZXMlMjBvZiUyMGNvcm9uYXZpcnVzJTIwJTJDJTIwREhFQyUyMGNvbmZpcm1zJTIwSFRUUFVSTCUyMHZpYSUyMCU0MFVTRVIlMjAlM0FjcnklM0ElMjIlMEElMEFpbnB1dF9pZHMlMjAlM0QlMjB0b3JjaC50ZW5zb3IoJTVCdG9rZW5pemVyLmVuY29kZShsaW5lKSU1RCklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwZmVhdHVyZXMlMjAlM0QlMjBiZXJ0d2VldChpbnB1dF9pZHMpJTIwJTIwJTIzJTIwTW9kZWxzJTIwb3V0cHV0cyUyMGFyZSUyMG5vdyUyMHR1cGxlcyUwQSUwQSUyMyUyMFdpdGglMjBUZW5zb3JGbG93JTIwMi4wJTJCJTNBJTBBJTIzJTIwZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsJTBBJTIzJTIwYmVydHdlZXQlMjAlM0QlMjBURkF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIydmluYWklMkZiZXJ0d2VldC1iYXNlJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModel, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>bertweet = AutoModel.from_pretrained(<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For transformers v4.x+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;vinai/bertweet-base&quot;</span>, use_fast=<span class="hljs-literal">False</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># For transformers v3.x:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># tokenizer = AutoTokenizer.from_pretrained(&quot;vinai/bertweet-base&quot;)</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># INPUT TWEET IS ALREADY NORMALIZED!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>line = <span class="hljs-string">&quot;SC has first two presumptive cases of coronavirus , DHEC confirms HTTPURL via @USER :cry:&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>input_ids = torch.tensor([tokenizer.encode(line)])

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    features = bertweet(input_ids)  <span class="hljs-comment"># Models outputs are now tuples</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With TensorFlow 2.0+:</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># from transformers import TFAutoModel</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># bertweet = TFAutoModel.from_pretrained(&quot;vinai/bertweet-base&quot;)</span>`,wrap:!1}}),$=new ct({props:{$$slots:{default:[ft]},$$scope:{ctx:le}}}),F=new ze({props:{title:"BertweetTokenizer",local:"transformers.BertweetTokenizer",headingTag:"h2"}}),H=new I({props:{name:"class transformers.BertweetTokenizer",anchor:"transformers.BertweetTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"normalization",val:" = False"},{name:"bos_token",val:" = '<s>'"},{name:"eos_token",val:" = '</s>'"},{name:"sep_token",val:" = '</s>'"},{name:"cls_token",val:" = '<s>'"},{name:"unk_token",val:" = '<unk>'"},{name:"pad_token",val:" = '<pad>'"},{name:"mask_token",val:" = '<mask>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.BertweetTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.BertweetTokenizer.normalization",description:`<strong>normalization</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to apply a normalization preprocess.`,name:"normalization"},{anchor:"transformers.BertweetTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the beginning of
sequence. The token used is the <code>cls_token</code>.</p>

					</div>`,name:"bos_token"},{anchor:"transformers.BertweetTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The end of sequence token.</p>
<div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400">
						
<p>When building a sequence using special tokens, this is not the token that is used for the end of sequence.
The token used is the <code>sep_token</code>.</p>

					</div>`,name:"eos_token"},{anchor:"transformers.BertweetTokenizer.sep_token",description:`<strong>sep_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;/s&gt;&quot;</code>) &#x2014;
The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for
sequence classification or for a text and a question for question answering. It is also used as the last
token of a sequence built with special tokens.`,name:"sep_token"},{anchor:"transformers.BertweetTokenizer.cls_token",description:`<strong>cls_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;s&gt;&quot;</code>) &#x2014;
The classifier token which is used when doing sequence classification (classification of the whole sequence
instead of per-token classification). It is the first token of the sequence when built with special tokens.`,name:"cls_token"},{anchor:"transformers.BertweetTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;unk&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.BertweetTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;pad&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"},{anchor:"transformers.BertweetTokenizer.mask_token",description:`<strong>mask_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;mask&gt;&quot;</code>) &#x2014;
The token used for masking values. This is the token used when training this model with masked language
modeling. This is the token which the model will try to predict.`,name:"mask_token"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L68"}}),P=new I({props:{name:"add_from_file",anchor:"transformers.BertweetTokenizer.add_from_file",parameters:[{name:"f",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L418"}}),Q=new I({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L183",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),X=new I({props:{name:"convert_tokens_to_string",anchor:"transformers.BertweetTokenizer.convert_tokens_to_string",parameters:[{name:"tokens",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L384"}}),D=new I({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L237",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),W=new I({props:{name:"get_special_tokens_mask",anchor:"transformers.BertweetTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.BertweetTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L209",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),S=new I({props:{name:"normalizeToken",anchor:"transformers.BertweetTokenizer.normalizeToken",parameters:[{name:"token",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L357"}}),A=new I({props:{name:"normalizeTweet",anchor:"transformers.BertweetTokenizer.normalizeTweet",parameters:[{name:"tweet",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bertweet/tokenization_bertweet.py#L323"}}),{c(){c=a("meta"),q=o(),b=a("p"),N=o(),f(C.$$.fragment),de=o(),f(U.$$.fragment),me=o(),j=a("p"),j.innerHTML=De,ce=o(),E=a("p"),E.textContent=We,pe=o(),R=a("p"),R.innerHTML=Se,fe=o(),f(V.$$.fragment),ge=o(),f(L.$$.fragment),ue=o(),f($.$$.fragment),he=o(),Z=a("p"),Z.innerHTML=Ae,ke=o(),f(F.$$.fragment),_e=o(),l=a("div"),f(H.$$.fragment),xe=o(),G=a("p"),G.textContent=Ne,Be=o(),Y=a("p"),Y.innerHTML=Ge,Ie=o(),y=a("div"),f(P.$$.fragment),qe=o(),O=a("p"),O.textContent=Ye,Ce=o(),w=a("div"),f(Q.$$.fragment),Ue=o(),K=a("p"),K.textContent=Oe,je=o(),ee=a("ul"),ee.innerHTML=Ke,Ee=o(),J=a("div"),f(X.$$.fragment),Re=o(),te=a("p"),te.textContent=et,Ve=o(),M=a("div"),f(D.$$.fragment),Le=o(),ne=a("p"),ne.textContent=tt,Ze=o(),z=a("div"),f(W.$$.fragment),Fe=o(),se=a("p"),se.innerHTML=nt,He=o(),x=a("div"),f(S.$$.fragment),Pe=o(),oe=a("p"),oe.textContent=st,Qe=o(),B=a("div"),f(A.$$.fragment),Xe=o(),re=a("p"),re.textContent=ot,be=o(),ie=a("p"),this.h()},l(e){const s=mt("svelte-u9bgzb",document.head);c=i(s,"META",{name:!0,content:!0}),s.forEach(n),q=r(e),b=i(e,"P",{}),T(b).forEach(n),N=r(e),g(C.$$.fragment,e),de=r(e),g(U.$$.fragment,e),me=r(e),j=i(e,"P",{"data-svelte-h":!0}),p(j)!=="svelte-87k501"&&(j.innerHTML=De),ce=r(e),E=i(e,"P",{"data-svelte-h":!0}),p(E)!=="svelte-1cv3nri"&&(E.textContent=We),pe=r(e),R=i(e,"P",{"data-svelte-h":!0}),p(R)!=="svelte-11sd9cy"&&(R.innerHTML=Se),fe=r(e),g(V.$$.fragment,e),ge=r(e),g(L.$$.fragment,e),ue=r(e),g($.$$.fragment,e),he=r(e),Z=i(e,"P",{"data-svelte-h":!0}),p(Z)!=="svelte-3zyik5"&&(Z.innerHTML=Ae),ke=r(e),g(F.$$.fragment,e),_e=r(e),l=i(e,"DIV",{class:!0});var d=T(l);g(H.$$.fragment,d),xe=r(d),G=i(d,"P",{"data-svelte-h":!0}),p(G)!=="svelte-b8riyv"&&(G.textContent=Ne),Be=r(d),Y=i(d,"P",{"data-svelte-h":!0}),p(Y)!=="svelte-rs9us"&&(Y.innerHTML=Ge),Ie=r(d),y=i(d,"DIV",{class:!0});var Te=T(y);g(P.$$.fragment,Te),qe=r(Te),O=i(Te,"P",{"data-svelte-h":!0}),p(O)!=="svelte-ooaeix"&&(O.textContent=Ye),Te.forEach(n),Ce=r(d),w=i(d,"DIV",{class:!0});var ae=T(w);g(Q.$$.fragment,ae),Ue=r(ae),K=i(ae,"P",{"data-svelte-h":!0}),p(K)!=="svelte-zjm6uf"&&(K.textContent=Oe),je=r(ae),ee=i(ae,"UL",{"data-svelte-h":!0}),p(ee)!=="svelte-rq8uot"&&(ee.innerHTML=Ke),ae.forEach(n),Ee=r(d),J=i(d,"DIV",{class:!0});var ve=T(J);g(X.$$.fragment,ve),Re=r(ve),te=i(ve,"P",{"data-svelte-h":!0}),p(te)!=="svelte-b3k2yi"&&(te.textContent=et),ve.forEach(n),Ve=r(d),M=i(d,"DIV",{class:!0});var $e=T(M);g(D.$$.fragment,$e),Le=r($e),ne=i($e,"P",{"data-svelte-h":!0}),p(ne)!=="svelte-vpfvn5"&&(ne.textContent=tt),$e.forEach(n),Ze=r(d),z=i(d,"DIV",{class:!0});var ye=T(z);g(W.$$.fragment,ye),Fe=r(ye),se=i(ye,"P",{"data-svelte-h":!0}),p(se)!=="svelte-1f4f5kp"&&(se.innerHTML=nt),ye.forEach(n),He=r(d),x=i(d,"DIV",{class:!0});var Je=T(x);g(S.$$.fragment,Je),Pe=r(Je),oe=i(Je,"P",{"data-svelte-h":!0}),p(oe)!=="svelte-1jdrmaw"&&(oe.textContent=st),Je.forEach(n),Qe=r(d),B=i(d,"DIV",{class:!0});var Me=T(B);g(A.$$.fragment,Me),Xe=r(Me),re=i(Me,"P",{"data-svelte-h":!0}),p(re)!=="svelte-15su17z"&&(re.textContent=ot),Me.forEach(n),d.forEach(n),be=r(e),ie=i(e,"P",{}),T(ie).forEach(n),this.h()},h(){v(c,"name","hf:doc:metadata"),v(c,"content",ut),v(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(l,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){t(document.head,c),m(e,q,s),m(e,b,s),m(e,N,s),u(C,e,s),m(e,de,s),u(U,e,s),m(e,me,s),m(e,j,s),m(e,ce,s),m(e,E,s),m(e,pe,s),m(e,R,s),m(e,fe,s),u(V,e,s),m(e,ge,s),u(L,e,s),m(e,ue,s),u($,e,s),m(e,he,s),m(e,Z,s),m(e,ke,s),u(F,e,s),m(e,_e,s),m(e,l,s),u(H,l,null),t(l,xe),t(l,G),t(l,Be),t(l,Y),t(l,Ie),t(l,y),u(P,y,null),t(y,qe),t(y,O),t(l,Ce),t(l,w),u(Q,w,null),t(w,Ue),t(w,K),t(w,je),t(w,ee),t(l,Ee),t(l,J),u(X,J,null),t(J,Re),t(J,te),t(l,Ve),t(l,M),u(D,M,null),t(M,Le),t(M,ne),t(l,Ze),t(l,z),u(W,z,null),t(z,Fe),t(z,se),t(l,He),t(l,x),u(S,x,null),t(x,Pe),t(x,oe),t(l,Qe),t(l,B),u(A,B,null),t(B,Xe),t(B,re),m(e,be,s),m(e,ie,s),we=!0},p(e,[s]){const d={};s&2&&(d.$$scope={dirty:s,ctx:e}),$.$set(d)},i(e){we||(h(C.$$.fragment,e),h(U.$$.fragment,e),h(V.$$.fragment,e),h(L.$$.fragment,e),h($.$$.fragment,e),h(F.$$.fragment,e),h(H.$$.fragment,e),h(P.$$.fragment,e),h(Q.$$.fragment,e),h(X.$$.fragment,e),h(D.$$.fragment,e),h(W.$$.fragment,e),h(S.$$.fragment,e),h(A.$$.fragment,e),we=!0)},o(e){k(C.$$.fragment,e),k(U.$$.fragment,e),k(V.$$.fragment,e),k(L.$$.fragment,e),k($.$$.fragment,e),k(F.$$.fragment,e),k(H.$$.fragment,e),k(P.$$.fragment,e),k(Q.$$.fragment,e),k(X.$$.fragment,e),k(D.$$.fragment,e),k(W.$$.fragment,e),k(S.$$.fragment,e),k(A.$$.fragment,e),we=!1},d(e){e&&(n(q),n(b),n(N),n(de),n(me),n(j),n(ce),n(E),n(pe),n(R),n(fe),n(ge),n(ue),n(he),n(Z),n(ke),n(_e),n(l),n(be),n(ie)),n(c),_(C,e),_(U,e),_(V,e),_(L,e),_($,e),_(F,e),_(H),_(P),_(Q),_(X),_(D),_(W),_(S),_(A)}}}const ut='{"title":"BERTweet","local":"bertweet","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"BertweetTokenizer","local":"transformers.BertweetTokenizer","sections":[],"depth":2}],"depth":1}';function ht(le){return at(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class $t extends lt{constructor(c){super(),dt(this,c,ht,gt,rt,{})}}export{$t as component};
