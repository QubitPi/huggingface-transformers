import{s as is,f as ls,o as ds,n as J}from"../chunks/scheduler.9bc65507.js";import{S as cs,i as ms,g as d,s,r as h,A as ps,h as c,f as a,c as r,j as $,u as f,x as y,k as x,y as i,a as l,v as g,d as u,t as _,w as b}from"../chunks/index.707bf1b6.js";import{T as Jt}from"../chunks/Tip.c2ecdbf4.js";import{D as B}from"../chunks/Docstring.17db21ae.js";import{C as Je}from"../chunks/CodeBlock.54a9f38d.js";import{E as Ie}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as hs}from"../chunks/PipelineTag.44585822.js";import{H as I}from"../chunks/Heading.342b1fa6.js";function fs(v){let t,M="Example:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlaXRDb25maWclMkMlMjBCZWl0TW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQkVpVCUyMGJlaXQtYmFzZS1wYXRjaDE2LTIyNC1wdDIyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCZWl0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGJlaXQtYmFzZS1wYXRjaDE2LTIyNC1wdDIyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmVpdE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BeitConfig, BeitModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BEiT beit-base-patch16-224-pt22k style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BeitConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the beit-base-patch16-224-pt22k style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-11lpom8"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function gs(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function us(v){let t,M="Example:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJlaXRNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLXBhdGNoMTYtMjI0LXB0MjJrJTIyKSUwQW1vZGVsJTIwJTNEJTIwQmVpdE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZiZWl0LWJhc2UtcGF0Y2gxNi0yMjQtcHQyMmslMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BeitModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitModel.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>]`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-11lpom8"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function _s(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function bs(v){let t,M="Examples:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJlaXRGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmJlaXQtYmFzZS1wYXRjaDE2LTIyNC1wdDIyayUyMiklMEFtb2RlbCUyMCUzRCUyMEJlaXRGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZiZWl0LWJhc2UtcGF0Y2gxNi0yMjQtcHQyMmslMjIpJTBBJTBBbnVtX3BhdGNoZXMlMjAlM0QlMjAobW9kZWwuY29uZmlnLmltYWdlX3NpemUlMjAlMkYlMkYlMjBtb2RlbC5jb25maWcucGF0Y2hfc2l6ZSklMjAqKiUyMDIlMEFwaXhlbF92YWx1ZXMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5waXhlbF92YWx1ZXMlMEElMjMlMjBjcmVhdGUlMjByYW5kb20lMjBib29sZWFuJTIwbWFzayUyMG9mJTIwc2hhcGUlMjAoYmF0Y2hfc2l6ZSUyQyUyMG51bV9wYXRjaGVzKSUwQWJvb2xfbWFza2VkX3BvcyUyMCUzRCUyMHRvcmNoLnJhbmRpbnQobG93JTNEMCUyQyUyMGhpZ2glM0QyJTJDJTIwc2l6ZSUzRCgxJTJDJTIwbnVtX3BhdGNoZXMpKS5ib29sKCklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwocGl4ZWxfdmFsdWVzJTJDJTIwYm9vbF9tYXNrZWRfcG9zJTNEYm9vbF9tYXNrZWRfcG9zKSUwQWxvc3MlMkMlMjBsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvc3MlMkMlMjBvdXRwdXRzLmxvZ2l0cyUwQWxpc3QobG9naXRzLnNoYXBlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BeitForMaskedImageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>num_patches = (model.config.image_size // model.config.patch_size) ** <span class="hljs-number">2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create random boolean mask of shape (batch_size, num_patches)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>bool_masked_pos = torch.randint(low=<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=(<span class="hljs-number">1</span>, num_patches)).<span class="hljs-built_in">bool</span>()

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, logits = outputs.loss, outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">196</span>, <span class="hljs-number">8192</span>]`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function ys(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function Ms(v){let t,M="Example:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJlaXRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmh1Z2dpbmdmYWNlJTJGY2F0cy1pbWFnZSUyMiklMEFpbWFnZSUyMCUzRCUyMGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTVCJTIyaW1hZ2UlMjIlNUQlNUIwJTVEJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZiZWl0LWJhc2UtcGF0Y2gxNi0yMjQlMjIpJTBBbW9kZWwlMjAlM0QlMjBCZWl0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLXBhdGNoMTYtMjI0JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BeitForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitForImageClassification.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-11lpom8"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function Ts(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function ws(v){let t,M="Examples:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJlaXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLWZpbmV0dW5lZC1hZGUtNjQwLTY0MCUyMiklMEFtb2RlbCUyMCUzRCUyMEJlaXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLWZpbmV0dW5lZC1hZGUtNjQwLTY0MCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMjMlMjBsb2dpdHMlMjBhcmUlMjBvZiUyMHNoYXBlJTIwKGJhdGNoX3NpemUlMkMlMjBudW1fbGFiZWxzJTJDJTIwaGVpZ2h0JTJDJTIwd2lkdGgpJTBBbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BeitForSemanticSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-finetuned-ade-640-640&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-finetuned-ade-640-640&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># logits are of shape (batch_size, num_labels, height, width)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function vs(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function xs(v){let t,M="Examples:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEZsYXhCZWl0TW9kZWwlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmJlaXQtYmFzZS1wYXRjaDE2LTIyNC1wdDIyay1mdDIyayUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhCZWl0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmJlaXQtYmFzZS1wYXRjaDE2LTIyNC1wdDIyay1mdDIyayUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMm5wJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, FlaxBeitModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k-ft22k&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxBeitModel.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k-ft22k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function $s(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function Bs(v){let t,M="Examples:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJlaXRGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZiZWl0LWJhc2UtcGF0Y2gxNi0yMjQtcHQyMmslMjIpJTBBbW9kZWwlMjAlM0QlMjBCZWl0Rm9yTWFza2VkSW1hZ2VNb2RlbGluZy5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLXBhdGNoMTYtMjI0LXB0MjJrJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIybnAlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0cyUyMCUzRCUyMG91dHB1dHMubG9naXRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BeitForMaskedImageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function js(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function Cs(v){let t,M="Example:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEZsYXhCZWl0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLXBhdGNoMTYtMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEJlaXRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZiZWl0LWJhc2UtcGF0Y2gxNi0yMjQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJucCUyMiklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2NsYXNzX2lkeCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KCUyMlByZWRpY3RlZCUyMGNsYXNzJTNBJTIyJTJDJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2NsYXNzX2lkeCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, FlaxBeitForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxBeitForImageClassification.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_idx = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-11lpom8"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function Fs(v){let t,M,m,p,T,o,w,bo,ke,ha=`BEiT ãƒ¢ãƒ‡ãƒ«ã¯ã€<a href="https://arxiv.org/abs/2106.08254" rel="nofollow">BEiT: BERT Pre-Training of Image Transformers</a> ã§ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚
ãƒãƒ³ãƒœãƒ»ãƒã‚ªã€ãƒªãƒ¼ãƒ»ãƒ‰ãƒ³ã€ãƒ•ãƒ«ãƒ»ã‚¦ã‚§ã‚¤ã€‚ BERT ã«è§¦ç™ºã•ã‚ŒãŸ BEiT ã¯ã€è‡ªå·±æ•™å¸«ã‚ã‚Šã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½œæˆã—ãŸæœ€åˆã®è«–æ–‡ã§ã™ã€‚
ãƒ“ã‚¸ãƒ§ãƒ³ ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ (ViT) ã¯ã€æ•™å¸«ä»˜ãäº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªã
(<a href="https://arxiv.org/abs/2010.11929" rel="nofollow">ã‚ªãƒªã‚¸ãƒŠãƒ«ã® ViT è«–æ–‡</a> ã§è¡Œã‚ã‚ŒãŸã‚ˆã†ã«) ç”»åƒã® BEiT ãƒ¢ãƒ‡ãƒ«ã¯ã€æ¬¡ã®ã‚ˆã†ã«äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚
ãƒã‚¹ã‚¯ã•ã‚ŒãŸ OpenAI ã® <a href="https://arxiv.org/abs/2102.12092" rel="nofollow">DALL-E ãƒ¢ãƒ‡ãƒ«</a> ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‹ã‚‰ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ« ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã—ã¾ã™
ãƒ‘ãƒƒãƒã€‚`,yo,ze,fa="è«–æ–‡ã®è¦ç´„ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚",Mo,We,ga=`<em>è‡ªå·±æ•™å¸«ã‚ã‚Šè¦–è¦šè¡¨ç¾ãƒ¢ãƒ‡ãƒ« BEiT (Bidirectional Encoderpresentation) ã‚’å°å…¥ã—ã¾ã™ã€‚
ã‚¤ãƒ¡ãƒ¼ã‚¸ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚ˆã‚Šã€‚è‡ªç„¶è¨€èªå‡¦ç†åˆ†é‡ã§é–‹ç™ºã•ã‚ŒãŸBERTã«å€£ã„ã€ãƒã‚¹ã‚¯ç”»åƒã‚’ææ¡ˆã—ã¾ã™ã€‚
ãƒ“ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’äº‹å‰ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã€‚å…·ä½“çš„ã«ã¯ã€äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯å„ç”»åƒã« 2 ã¤ã®ãƒ“ãƒ¥ãƒ¼ãŒã‚ã‚Šã¾ã™ã€‚
ãƒ‘ãƒƒãƒ (16x16 ãƒ”ã‚¯ã‚»ãƒ«ãªã©)ã€ãŠã‚ˆã³ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ« ãƒˆãƒ¼ã‚¯ãƒ³ (ã¤ã¾ã‚Šã€å€‹åˆ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³)ã€‚ã¾ãšã€å…ƒã®ç”»åƒã‚’ã€Œãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã€ã—ã¦ã€
ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒˆãƒ¼ã‚¯ãƒ³ã€‚æ¬¡ã«ã€ã„ãã¤ã‹ã®ç”»åƒãƒ‘ãƒƒãƒã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒã‚¹ã‚¯ã—ã€ãã‚Œã‚‰ã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã® Transformer ã«ä¾›çµ¦ã—ã¾ã™ã€‚äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
ç›®çš„ã¯ã€ç ´æã—ãŸã‚¤ãƒ¡ãƒ¼ã‚¸ ãƒ‘ãƒƒãƒã«åŸºã¥ã„ã¦å…ƒã®ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ« ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å›å¾©ã™ã‚‹ã“ã¨ã§ã™ã€‚ BEiTã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã€
äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã«ã‚¿ã‚¹ã‚¯ ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ  ã‚¿ã‚¹ã‚¯ã®ãƒ¢ãƒ‡ãƒ« ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ç›´æ¥å¾®èª¿æ•´ã—ã¾ã™ã€‚
ç”»åƒåˆ†é¡ã¨ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«é–¢ã™ã‚‹å®Ÿé¨“çµæœã¯ã€ç§ãŸã¡ã®ãƒ¢ãƒ‡ãƒ«ãŒç«¶äº‰åŠ›ã®ã‚ã‚‹çµæœã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™
ä»¥å‰ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã‚’ä½¿ç”¨ã—ã¦ã€‚ãŸã¨ãˆã°ã€åŸºæœ¬ã‚µã‚¤ã‚ºã® BEiT ã¯ã€ImageNet-1K ã§ 83.2% ã®ãƒˆãƒƒãƒ— 1 ç²¾åº¦ã‚’é”æˆã—ã¾ã™ã€‚
åŒã˜è¨­å®šã§ã‚¼ãƒ­ã‹ã‚‰ã® DeiT ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° (81.8%) ã‚’å¤§å¹…ã«ä¸Šå›ã‚Šã¾ã—ãŸã€‚ã¾ãŸã€å¤§å‹BEiTã¯
86.3% ã¯ ImageNet-1K ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€ImageNet-22K ã§ã®æ•™å¸«ä»˜ãäº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ãŸ ViT-L (85.2%) ã‚’ä¸Šå›ã£ã¦ã„ã¾ã™ã€‚</em>`,To,Ue,wo,Ze,ua=`<li>BEiT ãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã®ãƒ“ã‚¸ãƒ§ãƒ³ ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã™ãŒã€æ•™å¸«ã‚ã‚Šã§ã¯ãªãè‡ªå·±æ•™å¸«ã‚ã‚Šã®æ–¹æ³•ã§äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚å½¼ã‚‰ã¯
ImageNet-1K ãŠã‚ˆã³ CIFAR-100 ã§å¾®èª¿æ•´ã™ã‚‹ã¨ã€<a href="vit">ã‚ªãƒªã‚¸ãƒŠãƒ« ãƒ¢ãƒ‡ãƒ« (ViT)</a> ã¨ <a href="deit">ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã®é«˜ã„ã‚¤ãƒ¡ãƒ¼ã‚¸ ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ (DeiT)</a> ã®ä¸¡æ–¹ã‚’ä¸Šå›ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚æ¨è«–ã«é–¢ã™ã‚‹ãƒ‡ãƒ¢ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚‚ãƒã‚§ãƒƒã‚¯ã§ãã¾ã™ã€‚
ã‚«ã‚¹ã‚¿ãƒ  ãƒ‡ãƒ¼ã‚¿ã®å¾®èª¿æ•´ã¯ <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer" rel="nofollow">ã“ã¡ã‚‰</a> (ç½®ãæ›ãˆã‚‹ã ã‘ã§æ¸ˆã¿ã¾ã™)
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitImageProcessor">BeitImageProcessor</a> ã«ã‚ˆã‚‹ <code>ViTFeatureExtractor</code> ã¨
<code>ViTForImageClassification</code> by <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a>)ã€‚</li> <li>DALL-E ã®ç”»åƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ BEiT ã‚’çµ„ã¿åˆã‚ã›ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã™ã‚‹ãƒ‡ãƒ¢ ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚‚åˆ©ç”¨å¯èƒ½ã§ã™ã€‚
ãƒã‚¹ã‚¯ã•ã‚ŒãŸç”»åƒãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT" rel="nofollow">ã“ã“</a> ã§è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚</li> <li>BEiT ãƒ¢ãƒ‡ãƒ«ã¯å„ç”»åƒãŒåŒã˜ã‚µã‚¤ã‚º (è§£åƒåº¦) ã§ã‚ã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ã„ã‚‹ãŸã‚ã€æ¬¡ã®ã‚ˆã†ã«ä½¿ç”¨ã§ãã¾ã™ã€‚
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitImageProcessor">BeitImageProcessor</a> ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã®ã‚µã‚¤ã‚ºã‚’å¤‰æ›´ (ã¾ãŸã¯å†ã‚¹ã‚±ãƒ¼ãƒ«) ã—ã€æ­£è¦åŒ–ã—ã¾ã™ã€‚</li> <li>äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¾ãŸã¯å¾®èª¿æ•´ä¸­ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‘ãƒƒãƒè§£åƒåº¦ã¨ç”»åƒè§£åƒåº¦ã®ä¸¡æ–¹ãŒåå‰ã«åæ˜ ã•ã‚Œã¾ã™ã€‚
å„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€‚ãŸã¨ãˆã°ã€<code>microsoft/beit-base-patch16-224</code>ã¯ã€ãƒ‘ãƒƒãƒä»˜ãã®åŸºæœ¬ã‚µã‚¤ã‚ºã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æŒ‡ã—ã¾ã™ã€‚
è§£åƒåº¦ã¯ 16x16ã€å¾®èª¿æ•´è§£åƒåº¦ã¯ 224x224 ã§ã™ã€‚ã™ã¹ã¦ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ <a href="https://huggingface.co/models?search=microsoft/beit" rel="nofollow">ãƒãƒ–</a> ã§è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚</li> <li>åˆ©ç”¨å¯èƒ½ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã€(1) <a href="http://www.image-net.org/" rel="nofollow">ImageNet-22k</a> ã§äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ (
1,400 ä¸‡ã®ç”»åƒã¨ 22,000 ã®ã‚¯ãƒ©ã‚¹) ã®ã¿ã€(2) ImageNet-22k ã§ã‚‚å¾®èª¿æ•´ã€ã¾ãŸã¯ (3) <a href="http://www.image-net.org/challenges/LSVRC" rel="nofollow">ImageNet-1k</a>ã§ã‚‚å¾®èª¿æ•´/2012/) (ILSVRC 2012 ã¨ã‚‚å‘¼ã°ã‚Œã€130 ä¸‡ä»¶ã®ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³)
ç”»åƒã¨ 1,000 ã‚¯ãƒ©ã‚¹)ã€‚</li> <li>BEiT ã¯ã€T5 ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒ”ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¾—ãŸç›¸å¯¾ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã€è‘—è€…ã¯æ¬¡ã®ã“ã¨ã‚’å…±æœ‰ã—ã¾ã—ãŸã€‚
ã„ãã¤ã‹ã®è‡ªå·±æ³¨æ„å±¤é–“ã®ç›¸å¯¾çš„ãªä½ç½®ã®åã‚Šã€‚å¾®èª¿æ•´ä¸­ã€å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ç›¸å¯¾ä½ç½®
ãƒã‚¤ã‚¢ã‚¹ã¯ã€äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã«å–å¾—ã•ã‚ŒãŸå…±æœ‰ç›¸å¯¾ä½ç½®ãƒã‚¤ã‚¢ã‚¹ã§åˆæœŸåŒ–ã•ã‚Œã¾ã™ã€‚ã”å¸Œæœ›ã®å ´åˆã¯ã€
ãƒ¢ãƒ‡ãƒ«ã‚’æœ€åˆã‹ã‚‰äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã«ã¯ã€<code>use_relative_position_bias</code> ã¾ãŸã¯
è¿½åŠ ã™ã‚‹ã«ã¯ã€<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a> ã® <code>use_relative_position_bias</code> å±æ€§ã‚’ <code>True</code> ã«è¨­å®šã—ã¾ã™ã€‚
ä½ç½®ã®åŸ‹ã‚è¾¼ã¿ã€‚</li>`,vo,ie,_a,xo,Ne,ba='BEiT ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚ <a href="https://arxiv.org/abs/2106.08254">å…ƒã®è«–æ–‡ã‹ã‚‰æŠœç²‹ã€‚</a>',$o,Pe,ya=`ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€<a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a> ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã¾ã—ãŸã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã® JAX/FLAX ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ã€
<a href="https://huggingface.co/kamalkraj" rel="nofollow">kamalkraj</a> ã«ã‚ˆã‚‹æŠ•ç¨¿ã€‚å…ƒã®ã‚³ãƒ¼ãƒ‰ã¯ <a href="https://github.com/microsoft/unilm/tree/master/beit" rel="nofollow">ã“ã“</a> ã«ã‚ã‚Šã¾ã™ã€‚`,Bo,Ee,jo,Le,Ma="BEiT ã®ä½¿ç”¨ã‚’é–‹å§‹ã™ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ (ğŸŒ ã§ç¤ºã•ã‚Œã¦ã„ã‚‹) ãƒªã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆã€‚",Co,Re,Fo,Se,Ta='<li><a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> ã¯ã€ã“ã® <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ</a> ãŠã‚ˆã³ <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯</a>ã€‚</li> <li>å‚ç…§: <a href="../tasks/image_classification">ç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰</a></li>',Io,Ve,wa="<strong>ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³</strong>",Jo,Xe,va='<li><a href="../tasks/semantic_segmentation">ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰</a></li>',ko,Ge,xa="ã“ã“ã«å«ã‚ã‚‹ãƒªã‚½ãƒ¼ã‚¹ã®é€ä¿¡ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€ãŠæ°—è»½ã«ãƒ—ãƒ« ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é–‹ã„ã¦ãã ã•ã„ã€‚å¯©æŸ»ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ãƒªã‚½ãƒ¼ã‚¹ã¯ã€æ—¢å­˜ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’è¤‡è£½ã™ã‚‹ã®ã§ã¯ãªãã€ä½•ã‹æ–°ã—ã„ã‚‚ã®ã‚’ç¤ºã™ã“ã¨ãŒç†æƒ³çš„ã§ã™ã€‚",zo,He,Wo,te,qe,an,kt,$a='Class for outputs of <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitModel">BeitModel</a>.',Uo,oe,Ye,sn,zt,Ba='Class for outputs of <a href="/docs/transformers/main/ja/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a>.',Zo,Qe,No,G,De,rn,Wt,ja=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitModel">BeitModel</a>. It is used to instantiate an BEiT
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the BEiT
<a href="https://huggingface.co/microsoft/beit-base-patch16-224-pt22k" rel="nofollow">microsoft/beit-base-patch16-224-pt22k</a> architecture.`,ln,le,Po,Ae,Eo,H,Oe,dn,Ut,Ke,cn,de,et,mn,Zt,Ca='Converts the output of <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> into semantic segmentation maps. Only supports PyTorch.',Lo,tt,Ro,Z,ot,pn,Nt,Fa="Constructs a BEiT image processor.",hn,ce,nt,fn,Pt,Ia="Preprocess an image or batch of images.",gn,me,at,un,Et,Ja='Converts the output of <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> into semantic segmentation maps. Only supports PyTorch.',So,st,Vo,q,rt,_n,Lt,ka=`The bare Beit Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,bn,E,it,yn,Rt,za='The <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitModel">BeitModel</a> forward method, overrides the <code>__call__</code> special method.',Mn,pe,Tn,he,Xo,lt,Go,Y,dt,wn,St,Wa=`Beit Model transformer with a â€˜languageâ€™ modeling head on top. BEiT does masked image modeling by predicting
visual tokens of a Vector-Quantize Variational Autoencoder (VQ-VAE), whereas other vision models like ViT and DeiT
predict RGB pixel values. As a result, this class is incompatible with <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoModelForMaskedImageModeling">AutoModelForMaskedImageModeling</a>, so you
will need to use <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForMaskedImageModeling">BeitForMaskedImageModeling</a> directly if you wish to do masked image modeling with BEiT.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,vn,L,ct,xn,Vt,Ua='The <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForMaskedImageModeling">BeitForMaskedImageModeling</a> forward method, overrides the <code>__call__</code> special method.',$n,fe,Bn,ge,Ho,mt,qo,N,pt,jn,Xt,Za=`Beit Model transformer with an image classification head on top (a linear layer on top of the average of the final
hidden states of the patch tokens) e.g. for ImageNet.`,Cn,Gt,Na=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Fn,R,ht,In,Ht,Pa='The <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',Jn,ue,kn,_e,Yo,ft,Qo,P,gt,zn,qt,Ea="Beit Model transformer with a semantic segmentation head on top e.g. for ADE20k, CityScapes.",Wn,Yt,La=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Un,S,ut,Zn,Qt,Ra='The <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> forward method, overrides the <code>__call__</code> special method.',Nn,be,Pn,ye,Do,_t,Ao,j,bt,En,Dt,Sa="The bare Beit Model transformer outputting raw hidden-states without any specific head on top.",Ln,At,Va=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`,Rn,Ot,Xa=`This model is also a
<a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html" rel="nofollow">flax.linen.Module</a> subclass. Use it as
a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
behavior.`,Sn,Kt,Ga="Finally, this model supports inherent JAX features such as:",Vn,eo,Ha='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',Xn,V,yt,Gn,to,qa="The <code>FlaxBeitPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",Hn,Me,qn,Te,Oo,Mt,Ko,C,Tt,Yn,oo,Ya="Beit Model transformer with a â€˜languageâ€™ modeling head on top (to predict visual tokens).",Qn,no,Qa=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`,Dn,ao,Da=`This model is also a
<a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html" rel="nofollow">flax.linen.Module</a> subclass. Use it as
a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
behavior.`,An,so,Aa="Finally, this model supports inherent JAX features such as:",On,ro,Oa='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',Kn,k,wt,ea,io,Ka="The <code>FlaxBeitPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",ta,we,oa,lo,es=`bool_masked_pos (<code>numpy.ndarray</code> of shape <code>(batch_size, num_patches)</code>):
Boolean masked positions. Indicates which patches are masked (1) and which arenâ€™t (0).`,na,ve,en,vt,tn,F,xt,aa,co,ts=`Beit Model transformer with an image classification head on top (a linear layer on top of the average of the final
hidden states of the patch tokens) e.g. for ImageNet.`,sa,mo,os=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`,ra,po,ns=`This model is also a
<a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html" rel="nofollow">flax.linen.Module</a> subclass. Use it as
a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
behavior.`,ia,ho,as="Finally, this model supports inherent JAX features such as:",la,fo,ss='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',da,X,$t,ca,go,rs="The <code>FlaxBeitPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",ma,xe,pa,$e,on,uo,nn;return T=new I({props:{title:"BEiT",local:"beit",headingTag:"h1"}}),w=new I({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Ue=new I({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),Ee=new I({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Re=new hs({props:{pipeline:"image-classification"}}),He=new I({props:{title:"BEiT specific outputs",local:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling",headingTag:"h2"}}),qe=new B({props:{name:"class transformers.models.beit.modeling_beit.BeitModelOutputWithPooling",anchor:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"pooler_output",val:": FloatTensor = None"},{name:"hidden_states",val:": Optional = None"},{name:"attentions",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Average of the last layer hidden states of the patch tokens (excluding the <em>[CLS]</em> token) if
<em>config.use_mean_pooling</em> is set to True. If set to False, then the final hidden state of the <em>[CLS]</em> token
will be returned.`,name:"pooler_output"},{anchor:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L69"}}),Ye=new B({props:{name:"class transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling",anchor:"transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": Array = None"},{name:"pooler_output",val:": Array = None"},{name:"hidden_states",val:": Optional = None"},{name:"attentions",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Average of the last layer hidden states of the patch tokens (excluding the <em>[CLS]</em> token) if
<em>config.use_mean_pooling</em> is set to True. If set to False, then the final hidden state of the <em>[CLS]</em> token
will be returned.`,name:"pooler_output"},{anchor:"transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of each layer plus
the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the weighted average in
the self-attention heads.`,name:"attentions"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L44"}}),Qe=new I({props:{title:"BeitConfig",local:"transformers.BeitConfig",headingTag:"h2"}}),De=new B({props:{name:"class transformers.BeitConfig",anchor:"transformers.BeitConfig",parameters:[{name:"vocab_size",val:" = 8192"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"use_mask_token",val:" = False"},{name:"use_absolute_position_embeddings",val:" = False"},{name:"use_relative_position_bias",val:" = False"},{name:"use_shared_relative_position_bias",val:" = False"},{name:"layer_scale_init_value",val:" = 0.1"},{name:"drop_path_rate",val:" = 0.1"},{name:"use_mean_pooling",val:" = True"},{name:"pool_scales",val:" = [1, 2, 3, 6]"},{name:"use_auxiliary_head",val:" = True"},{name:"auxiliary_loss_weight",val:" = 0.4"},{name:"auxiliary_channels",val:" = 256"},{name:"auxiliary_num_convs",val:" = 1"},{name:"auxiliary_concat_input",val:" = False"},{name:"semantic_loss_ignore_index",val:" = 255"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"add_fpn",val:" = False"},{name:"reshape_hidden_states",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BeitConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the BEiT model. Defines the number of different image tokens that can be used during
pre-training.`,name:"vocab_size"},{anchor:"transformers.BeitConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.BeitConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.BeitConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.BeitConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.BeitConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.BeitConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.BeitConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.BeitConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BeitConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.BeitConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.BeitConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.BeitConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.BeitConfig.use_mask_token",description:`<strong>use_mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a mask token for masked image modeling.`,name:"use_mask_token"},{anchor:"transformers.BeitConfig.use_absolute_position_embeddings",description:`<strong>use_absolute_position_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use BERT-style absolute position embeddings.`,name:"use_absolute_position_embeddings"},{anchor:"transformers.BeitConfig.use_relative_position_bias",description:`<strong>use_relative_position_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use T5-style relative position embeddings in the self-attention layers.`,name:"use_relative_position_bias"},{anchor:"transformers.BeitConfig.use_shared_relative_position_bias",description:`<strong>use_shared_relative_position_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use the same relative position embeddings across all self-attention layers of the Transformer.`,name:"use_shared_relative_position_bias"},{anchor:"transformers.BeitConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Scale to use in the self-attention layers. 0.1 for base, 1e-5 for large. Set 0 to disable layer scale.`,name:"layer_scale_init_value"},{anchor:"transformers.BeitConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Stochastic depth rate per sample (when applied in the main path of residual layers).`,name:"drop_path_rate"},{anchor:"transformers.BeitConfig.use_mean_pooling",description:`<strong>use_mean_pooling</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to mean pool the final hidden states of the patches instead of using the final hidden state of the
CLS token, before applying the classification head.`,name:"use_mean_pooling"},{anchor:"transformers.BeitConfig.pool_scales",description:`<strong>pool_scales</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 3, 6]</code>) &#x2014;
Pooling scales used in Pooling Pyramid Module applied on the last feature map.`,name:"pool_scales"},{anchor:"transformers.BeitConfig.use_auxiliary_head",description:`<strong>use_auxiliary_head</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an auxiliary head during training.`,name:"use_auxiliary_head"},{anchor:"transformers.BeitConfig.auxiliary_loss_weight",description:`<strong>auxiliary_loss_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 0.4) &#x2014;
Weight of the cross-entropy loss of the auxiliary head.`,name:"auxiliary_loss_weight"},{anchor:"transformers.BeitConfig.auxiliary_channels",description:`<strong>auxiliary_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Number of channels to use in the auxiliary head.`,name:"auxiliary_channels"},{anchor:"transformers.BeitConfig.auxiliary_num_convs",description:`<strong>auxiliary_num_convs</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of convolutional layers to use in the auxiliary head.`,name:"auxiliary_num_convs"},{anchor:"transformers.BeitConfig.auxiliary_concat_input",description:`<strong>auxiliary_concat_input</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to concatenate the output of the auxiliary head with the input before the classification layer.`,name:"auxiliary_concat_input"},{anchor:"transformers.BeitConfig.semantic_loss_ignore_index",description:`<strong>semantic_loss_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to 255) &#x2014;
The index that is ignored by the loss function of the semantic segmentation model.`,name:"semantic_loss_ignore_index"},{anchor:"transformers.BeitConfig.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.BeitConfig.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"},{anchor:"transformers.BeitConfig.add_fpn",description:`<strong>add_fpn</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to add a FPN as part of the backbone. Only relevant for <code>BeitBackbone</code>.`,name:"add_fpn"},{anchor:"transformers.BeitConfig.reshape_hidden_states",description:`<strong>reshape_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to reshape the feature maps to 4D tensors of shape <code>(batch_size, hidden_size, height, width)</code> in
case the model is used as backbone. If <code>False</code>, the feature maps will be 3D tensors of shape <code>(batch_size, seq_len, hidden_size)</code>. Only relevant for <code>BeitBackbone</code>.`,name:"reshape_hidden_states"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/configuration_beit.py#L37"}}),le=new Ie({props:{anchor:"transformers.BeitConfig.example",$$slots:{default:[fs]},$$scope:{ctx:v}}}),Ae=new I({props:{title:"BeitFeatureExtractor",local:"transformers.BeitFeatureExtractor",headingTag:"h2"}}),Oe=new B({props:{name:"class transformers.BeitFeatureExtractor",anchor:"transformers.BeitFeatureExtractor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/feature_extraction_beit.py#L26"}}),Ke=new B({props:{name:"__call__",anchor:"transformers.BeitFeatureExtractor.__call__",parameters:[{name:"images",val:""},{name:"segmentation_maps",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/image_processing_beit.py#L307"}}),et=new B({props:{name:"post_process_semantic_segmentation",anchor:"transformers.BeitFeatureExtractor.post_process_semantic_segmentation",parameters:[{name:"outputs",val:""},{name:"target_sizes",val:": List = None"}],parametersDescription:[{anchor:"transformers.BeitFeatureExtractor.post_process_semantic_segmentation.outputs",description:`<strong>outputs</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.BeitFeatureExtractor.post_process_semantic_segmentation.target_sizes",description:`<strong>target_sizes</strong> (<code>List[Tuple]</code> of length <code>batch_size</code>, <em>optional</em>) &#x2014;
List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,
predictions will not be resized.`,name:"target_sizes"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/image_processing_beit.py#L464",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[torch.Tensor]</code> of length <code>batch_size</code>, where each item is a semantic
segmentation map of shape (height, width) corresponding to the target_sizes entry (if <code>target_sizes</code> is
specified). Each entry of each <code>torch.Tensor</code> correspond to a semantic class id.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>semantic_segmentation</p>
`}}),tt=new I({props:{title:"BeitImageProcessor",local:"transformers.BeitImageProcessor",headingTag:"h2"}}),ot=new B({props:{name:"class transformers.BeitImageProcessor",anchor:"transformers.BeitImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": Dict = None"},{name:"rescale_factor",val:": Union = 0.00392156862745098"},{name:"do_rescale",val:": bool = True"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"do_reduce_labels",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BeitImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.BeitImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 256, &quot;width&quot;: 256}</code>):
Size of the output image after resizing. Can be overridden by the <code>size</code> parameter in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.BeitImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in the
<code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.BeitImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the image. If the input size is smaller than <code>crop_size</code> along any edge, the image
is padded with 0&#x2019;s and then center cropped. Can be overridden by the <code>do_center_crop</code> parameter in the
<code>preprocess</code> method.`,name:"do_center_crop"},{anchor:"transformers.BeitImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Desired output size when applying center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.
Can be overridden by the <code>crop_size</code> parameter in the <code>preprocess</code> method.`,name:"crop_size"},{anchor:"transformers.BeitImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in the
<code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.BeitImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.BeitImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method.`,name:"do_normalize"},{anchor:"transformers.BeitImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
The mean to use if normalizing the image. This is a float or list of floats of length of the number of
channels of the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.BeitImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
The standard deviation to use if normalizing the image. This is a float or list of floats of length of the
number of channels of the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.BeitImageProcessor.do_reduce_labels",description:`<strong>do_reduce_labels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0 is
used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k). The
background label will be replaced by 255. Can be overridden by the <code>do_reduce_labels</code> parameter in the
<code>preprocess</code> method.`,name:"do_reduce_labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/image_processing_beit.py#L49"}}),nt=new B({props:{name:"preprocess",anchor:"transformers.BeitImageProcessor.preprocess",parameters:[{name:"images",val:": Union"},{name:"segmentation_maps",val:": Union = None"},{name:"do_resize",val:": bool = None"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = None"},{name:"do_center_crop",val:": bool = None"},{name:"crop_size",val:": Dict = None"},{name:"do_rescale",val:": bool = None"},{name:"rescale_factor",val:": float = None"},{name:"do_normalize",val:": bool = None"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"do_reduce_labels",val:": Optional = None"},{name:"return_tensors",val:": Union = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BeitImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.BeitImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.BeitImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image after resizing.`,name:"size"},{anchor:"transformers.BeitImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>, Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.BeitImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.BeitImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the image after center crop. If one edge the image is smaller than <code>crop_size</code>, it will be
padded with zeros and then cropped`,name:"crop_size"},{anchor:"transformers.BeitImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.BeitImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.BeitImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.BeitImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.BeitImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.BeitImageProcessor.preprocess.do_reduce_labels",description:`<strong>do_reduce_labels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_reduce_labels</code>) &#x2014;
Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0
is used for background, and background itself is not included in all classes of a dataset (e.g.
ADE20k). The background label will be replaced by 255.`,name:"do_reduce_labels"},{anchor:"transformers.BeitImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.BeitImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.BeitImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/image_processing_beit.py#L312"}}),at=new B({props:{name:"post_process_semantic_segmentation",anchor:"transformers.BeitImageProcessor.post_process_semantic_segmentation",parameters:[{name:"outputs",val:""},{name:"target_sizes",val:": List = None"}],parametersDescription:[{anchor:"transformers.BeitImageProcessor.post_process_semantic_segmentation.outputs",description:`<strong>outputs</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.BeitImageProcessor.post_process_semantic_segmentation.target_sizes",description:`<strong>target_sizes</strong> (<code>List[Tuple]</code> of length <code>batch_size</code>, <em>optional</em>) &#x2014;
List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,
predictions will not be resized.`,name:"target_sizes"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/image_processing_beit.py#L464",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[torch.Tensor]</code> of length <code>batch_size</code>, where each item is a semantic
segmentation map of shape (height, width) corresponding to the target_sizes entry (if <code>target_sizes</code> is
specified). Each entry of each <code>torch.Tensor</code> correspond to a semantic class id.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>semantic_segmentation</p>
`}}),st=new I({props:{title:"BeitModel",local:"transformers.BeitModel",headingTag:"h2"}}),rt=new B({props:{name:"class transformers.BeitModel",anchor:"transformers.BeitModel",parameters:[{name:"config",val:": BeitConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.BeitModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L620"}}),it=new B({props:{name:"forward",anchor:"transformers.BeitModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BeitModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitFeatureExtractor.__call__">BeitImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BeitModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BeitModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BeitModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BeitModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BeitModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>, <em>optional</em>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L651",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling"
>transformers.models.beit.modeling_beit.BeitModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig"
>BeitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Average of the last layer hidden states of the patch tokens (excluding the <em>[CLS]</em> token) if
<em>config.use_mean_pooling</em> is set to True. If set to False, then the final hidden state of the <em>[CLS]</em> token
will be returned.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling"
>transformers.models.beit.modeling_beit.BeitModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pe=new Jt({props:{$$slots:{default:[gs]},$$scope:{ctx:v}}}),he=new Ie({props:{anchor:"transformers.BeitModel.forward.example",$$slots:{default:[us]},$$scope:{ctx:v}}}),lt=new I({props:{title:"BeitForMaskedImageModeling",local:"transformers.BeitForMaskedImageModeling",headingTag:"h2"}}),dt=new B({props:{name:"class transformers.BeitForMaskedImageModeling",anchor:"transformers.BeitForMaskedImageModeling",parameters:[{name:"config",val:": BeitConfig"}],parametersDescription:[{anchor:"transformers.BeitForMaskedImageModeling.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L732"}}),ct=new B({props:{name:"forward",anchor:"transformers.BeitForMaskedImageModeling.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BeitForMaskedImageModeling.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitFeatureExtractor.__call__">BeitImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BeitForMaskedImageModeling.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BeitForMaskedImageModeling.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BeitForMaskedImageModeling.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BeitForMaskedImageModeling.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BeitForMaskedImageModeling.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.BeitForMaskedImageModeling.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L753",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig"
>BeitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) â€” Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new Jt({props:{$$slots:{default:[_s]},$$scope:{ctx:v}}}),ge=new Ie({props:{anchor:"transformers.BeitForMaskedImageModeling.forward.example",$$slots:{default:[bs]},$$scope:{ctx:v}}}),mt=new I({props:{title:"BeitForImageClassification",local:"transformers.BeitForImageClassification",headingTag:"h2"}}),pt=new B({props:{name:"class transformers.BeitForImageClassification",anchor:"transformers.BeitForImageClassification",parameters:[{name:"config",val:": BeitConfig"}],parametersDescription:[{anchor:"transformers.BeitForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L832"}}),ht=new B({props:{name:"forward",anchor:"transformers.BeitForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BeitForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitFeatureExtractor.__call__">BeitImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BeitForImageClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BeitForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BeitForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BeitForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BeitForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L852",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig"
>BeitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ue=new Jt({props:{$$slots:{default:[ys]},$$scope:{ctx:v}}}),_e=new Ie({props:{anchor:"transformers.BeitForImageClassification.forward.example",$$slots:{default:[Ms]},$$scope:{ctx:v}}}),ft=new I({props:{title:"BeitForSemanticSegmentation",local:"transformers.BeitForSemanticSegmentation",headingTag:"h2"}}),gt=new B({props:{name:"class transformers.BeitForSemanticSegmentation",anchor:"transformers.BeitForSemanticSegmentation",parameters:[{name:"config",val:": BeitConfig"}],parametersDescription:[{anchor:"transformers.BeitForSemanticSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L1156"}}),ut=new B({props:{name:"forward",anchor:"transformers.BeitForSemanticSegmentation.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BeitForSemanticSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitFeatureExtractor.__call__">BeitImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BeitForSemanticSegmentation.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BeitForSemanticSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BeitForSemanticSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BeitForSemanticSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BeitForSemanticSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Ground truth semantic segmentation maps for computing the loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels &gt; 1</code>, a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L1214",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig"
>BeitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels, logits_height, logits_width)</code>) â€” Classification scores for each pixel.</p>
<Tip warning={true}>
<p>The logits returned do not necessarily have the same size as the <code>pixel_values</code> passed as inputs. This is
to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the
original image size as post-processing. You should always check your logits shape and resize as needed.</p>
</Tip>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, patch_size, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),be=new Jt({props:{$$slots:{default:[Ts]},$$scope:{ctx:v}}}),ye=new Ie({props:{anchor:"transformers.BeitForSemanticSegmentation.forward.example",$$slots:{default:[ws]},$$scope:{ctx:v}}}),_t=new I({props:{title:"FlaxBeitModel",local:"transformers.FlaxBeitModel",headingTag:"h2"}}),bt=new B({props:{name:"class transformers.FlaxBeitModel",anchor:"transformers.FlaxBeitModel",parameters:[{name:"config",val:": BeitConfig"},{name:"input_shape",val:" = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxBeitModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxBeitModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L741"}}),yt=new B({props:{name:"__call__",anchor:"transformers.FlaxBeitModel.__call__",parameters:[{name:"pixel_values",val:""},{name:"bool_masked_pos",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L632",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling"
>transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.beit.configuration_beit.BeitConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</li>
<li><strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) â€” Average of the last layer hidden states of the patch tokens (excluding the <em>[CLS]</em> token) if
<em>config.use_mean_pooling</em> is set to True. If set to False, then the final hidden state of the <em>[CLS]</em> token
will be returned.</li>
<li><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of each layer plus
the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the weighted average in
the self-attention heads.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling"
>transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Me=new Jt({props:{$$slots:{default:[vs]},$$scope:{ctx:v}}}),Te=new Ie({props:{anchor:"transformers.FlaxBeitModel.__call__.example",$$slots:{default:[xs]},$$scope:{ctx:v}}}),Mt=new I({props:{title:"FlaxBeitForMaskedImageModeling",local:"transformers.FlaxBeitForMaskedImageModeling",headingTag:"h2"}}),Tt=new B({props:{name:"class transformers.FlaxBeitForMaskedImageModeling",anchor:"transformers.FlaxBeitForMaskedImageModeling",parameters:[{name:"config",val:": BeitConfig"},{name:"input_shape",val:" = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxBeitForMaskedImageModeling.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxBeitForMaskedImageModeling.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L825"}}),wt=new B({props:{name:"__call__",anchor:"transformers.FlaxBeitForMaskedImageModeling.__call__",parameters:[{name:"pixel_values",val:""},{name:"bool_masked_pos",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L632",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.beit.configuration_beit.BeitConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) â€” Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),we=new Jt({props:{$$slots:{default:[$s]},$$scope:{ctx:v}}}),ve=new Ie({props:{anchor:"transformers.FlaxBeitForMaskedImageModeling.__call__.example",$$slots:{default:[Bs]},$$scope:{ctx:v}}}),vt=new I({props:{title:"FlaxBeitForImageClassification",local:"transformers.FlaxBeitForImageClassification",headingTag:"h2"}}),xt=new B({props:{name:"class transformers.FlaxBeitForImageClassification",anchor:"transformers.FlaxBeitForImageClassification",parameters:[{name:"config",val:": BeitConfig"},{name:"input_shape",val:" = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxBeitForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxBeitForImageClassification.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L909"}}),$t=new B({props:{name:"__call__",anchor:"transformers.FlaxBeitForImageClassification.__call__",parameters:[{name:"pixel_values",val:""},{name:"bool_masked_pos",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L632",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.beit.configuration_beit.BeitConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),xe=new Jt({props:{$$slots:{default:[js]},$$scope:{ctx:v}}}),$e=new Ie({props:{anchor:"transformers.FlaxBeitForImageClassification.__call__.example",$$slots:{default:[Cs]},$$scope:{ctx:v}}}),{c(){t=d("meta"),M=s(),m=d("p"),p=s(),h(T.$$.fragment),o=s(),h(w.$$.fragment),bo=s(),ke=d("p"),ke.innerHTML=ha,yo=s(),ze=d("p"),ze.textContent=fa,Mo=s(),We=d("p"),We.innerHTML=ga,To=s(),h(Ue.$$.fragment),wo=s(),Ze=d("ul"),Ze.innerHTML=ua,vo=s(),ie=d("img"),xo=s(),Ne=d("small"),Ne.innerHTML=ba,$o=s(),Pe=d("p"),Pe.innerHTML=ya,Bo=s(),h(Ee.$$.fragment),jo=s(),Le=d("p"),Le.textContent=Ma,Co=s(),h(Re.$$.fragment),Fo=s(),Se=d("ul"),Se.innerHTML=Ta,Io=s(),Ve=d("p"),Ve.innerHTML=wa,Jo=s(),Xe=d("ul"),Xe.innerHTML=va,ko=s(),Ge=d("p"),Ge.textContent=xa,zo=s(),h(He.$$.fragment),Wo=s(),te=d("div"),h(qe.$$.fragment),an=s(),kt=d("p"),kt.innerHTML=$a,Uo=s(),oe=d("div"),h(Ye.$$.fragment),sn=s(),zt=d("p"),zt.innerHTML=Ba,Zo=s(),h(Qe.$$.fragment),No=s(),G=d("div"),h(De.$$.fragment),rn=s(),Wt=d("p"),Wt.innerHTML=ja,ln=s(),h(le.$$.fragment),Po=s(),h(Ae.$$.fragment),Eo=s(),H=d("div"),h(Oe.$$.fragment),dn=s(),Ut=d("div"),h(Ke.$$.fragment),cn=s(),de=d("div"),h(et.$$.fragment),mn=s(),Zt=d("p"),Zt.innerHTML=Ca,Lo=s(),h(tt.$$.fragment),Ro=s(),Z=d("div"),h(ot.$$.fragment),pn=s(),Nt=d("p"),Nt.textContent=Fa,hn=s(),ce=d("div"),h(nt.$$.fragment),fn=s(),Pt=d("p"),Pt.textContent=Ia,gn=s(),me=d("div"),h(at.$$.fragment),un=s(),Et=d("p"),Et.innerHTML=Ja,So=s(),h(st.$$.fragment),Vo=s(),q=d("div"),h(rt.$$.fragment),_n=s(),Lt=d("p"),Lt.innerHTML=ka,bn=s(),E=d("div"),h(it.$$.fragment),yn=s(),Rt=d("p"),Rt.innerHTML=za,Mn=s(),h(pe.$$.fragment),Tn=s(),h(he.$$.fragment),Xo=s(),h(lt.$$.fragment),Go=s(),Y=d("div"),h(dt.$$.fragment),wn=s(),St=d("p"),St.innerHTML=Wa,vn=s(),L=d("div"),h(ct.$$.fragment),xn=s(),Vt=d("p"),Vt.innerHTML=Ua,$n=s(),h(fe.$$.fragment),Bn=s(),h(ge.$$.fragment),Ho=s(),h(mt.$$.fragment),qo=s(),N=d("div"),h(pt.$$.fragment),jn=s(),Xt=d("p"),Xt.textContent=Za,Cn=s(),Gt=d("p"),Gt.innerHTML=Na,Fn=s(),R=d("div"),h(ht.$$.fragment),In=s(),Ht=d("p"),Ht.innerHTML=Pa,Jn=s(),h(ue.$$.fragment),kn=s(),h(_e.$$.fragment),Yo=s(),h(ft.$$.fragment),Qo=s(),P=d("div"),h(gt.$$.fragment),zn=s(),qt=d("p"),qt.textContent=Ea,Wn=s(),Yt=d("p"),Yt.innerHTML=La,Un=s(),S=d("div"),h(ut.$$.fragment),Zn=s(),Qt=d("p"),Qt.innerHTML=Ra,Nn=s(),h(be.$$.fragment),Pn=s(),h(ye.$$.fragment),Do=s(),h(_t.$$.fragment),Ao=s(),j=d("div"),h(bt.$$.fragment),En=s(),Dt=d("p"),Dt.textContent=Sa,Ln=s(),At=d("p"),At.innerHTML=Va,Rn=s(),Ot=d("p"),Ot.innerHTML=Xa,Sn=s(),Kt=d("p"),Kt.textContent=Ga,Vn=s(),eo=d("ul"),eo.innerHTML=Ha,Xn=s(),V=d("div"),h(yt.$$.fragment),Gn=s(),to=d("p"),to.innerHTML=qa,Hn=s(),h(Me.$$.fragment),qn=s(),h(Te.$$.fragment),Oo=s(),h(Mt.$$.fragment),Ko=s(),C=d("div"),h(Tt.$$.fragment),Yn=s(),oo=d("p"),oo.textContent=Ya,Qn=s(),no=d("p"),no.innerHTML=Qa,Dn=s(),ao=d("p"),ao.innerHTML=Da,An=s(),so=d("p"),so.textContent=Aa,On=s(),ro=d("ul"),ro.innerHTML=Oa,Kn=s(),k=d("div"),h(wt.$$.fragment),ea=s(),io=d("p"),io.innerHTML=Ka,ta=s(),h(we.$$.fragment),oa=s(),lo=d("p"),lo.innerHTML=es,na=s(),h(ve.$$.fragment),en=s(),h(vt.$$.fragment),tn=s(),F=d("div"),h(xt.$$.fragment),aa=s(),co=d("p"),co.textContent=ts,sa=s(),mo=d("p"),mo.innerHTML=os,ra=s(),po=d("p"),po.innerHTML=ns,ia=s(),ho=d("p"),ho.textContent=as,la=s(),fo=d("ul"),fo.innerHTML=ss,da=s(),X=d("div"),h($t.$$.fragment),ca=s(),go=d("p"),go.innerHTML=rs,ma=s(),h(xe.$$.fragment),pa=s(),h($e.$$.fragment),on=s(),uo=d("p"),this.h()},l(e){const n=ps("svelte-u9bgzb",document.head);t=c(n,"META",{name:!0,content:!0}),n.forEach(a),M=r(e),m=c(e,"P",{}),$(m).forEach(a),p=r(e),f(T.$$.fragment,e),o=r(e),f(w.$$.fragment,e),bo=r(e),ke=c(e,"P",{"data-svelte-h":!0}),y(ke)!=="svelte-1hvidxz"&&(ke.innerHTML=ha),yo=r(e),ze=c(e,"P",{"data-svelte-h":!0}),y(ze)!=="svelte-1cv3nri"&&(ze.textContent=fa),Mo=r(e),We=c(e,"P",{"data-svelte-h":!0}),y(We)!=="svelte-sr2vy6"&&(We.innerHTML=ga),To=r(e),f(Ue.$$.fragment,e),wo=r(e),Ze=c(e,"UL",{"data-svelte-h":!0}),y(Ze)!=="svelte-y3jyj7"&&(Ze.innerHTML=ua),vo=r(e),ie=c(e,"IMG",{src:!0,alt:!0,width:!0}),xo=r(e),Ne=c(e,"SMALL",{"data-svelte-h":!0}),y(Ne)!=="svelte-1swelvj"&&(Ne.innerHTML=ba),$o=r(e),Pe=c(e,"P",{"data-svelte-h":!0}),y(Pe)!=="svelte-16tj0i7"&&(Pe.innerHTML=ya),Bo=r(e),f(Ee.$$.fragment,e),jo=r(e),Le=c(e,"P",{"data-svelte-h":!0}),y(Le)!=="svelte-12jbpwc"&&(Le.textContent=Ma),Co=r(e),f(Re.$$.fragment,e),Fo=r(e),Se=c(e,"UL",{"data-svelte-h":!0}),y(Se)!=="svelte-1x9g5qd"&&(Se.innerHTML=Ta),Io=r(e),Ve=c(e,"P",{"data-svelte-h":!0}),y(Ve)!=="svelte-oevypi"&&(Ve.innerHTML=wa),Jo=r(e),Xe=c(e,"UL",{"data-svelte-h":!0}),y(Xe)!=="svelte-jacek8"&&(Xe.innerHTML=va),ko=r(e),Ge=c(e,"P",{"data-svelte-h":!0}),y(Ge)!=="svelte-17ytafw"&&(Ge.textContent=xa),zo=r(e),f(He.$$.fragment,e),Wo=r(e),te=c(e,"DIV",{class:!0});var Bt=$(te);f(qe.$$.fragment,Bt),an=r(Bt),kt=c(Bt,"P",{"data-svelte-h":!0}),y(kt)!=="svelte-1mesiz"&&(kt.innerHTML=$a),Bt.forEach(a),Uo=r(e),oe=c(e,"DIV",{class:!0});var jt=$(oe);f(Ye.$$.fragment,jt),sn=r(jt),zt=c(jt,"P",{"data-svelte-h":!0}),y(zt)!=="svelte-zxi4gf"&&(zt.innerHTML=Ba),jt.forEach(a),Zo=r(e),f(Qe.$$.fragment,e),No=r(e),G=c(e,"DIV",{class:!0});var ne=$(G);f(De.$$.fragment,ne),rn=r(ne),Wt=c(ne,"P",{"data-svelte-h":!0}),y(Wt)!=="svelte-h4bfyh"&&(Wt.innerHTML=ja),ln=r(ne),f(le.$$.fragment,ne),ne.forEach(a),Po=r(e),f(Ae.$$.fragment,e),Eo=r(e),H=c(e,"DIV",{class:!0});var ae=$(H);f(Oe.$$.fragment,ae),dn=r(ae),Ut=c(ae,"DIV",{class:!0});var _o=$(Ut);f(Ke.$$.fragment,_o),_o.forEach(a),cn=r(ae),de=c(ae,"DIV",{class:!0});var Ct=$(de);f(et.$$.fragment,Ct),mn=r(Ct),Zt=c(Ct,"P",{"data-svelte-h":!0}),y(Zt)!=="svelte-8uar9g"&&(Zt.innerHTML=Ca),Ct.forEach(a),ae.forEach(a),Lo=r(e),f(tt.$$.fragment,e),Ro=r(e),Z=c(e,"DIV",{class:!0});var Q=$(Z);f(ot.$$.fragment,Q),pn=r(Q),Nt=c(Q,"P",{"data-svelte-h":!0}),y(Nt)!=="svelte-ky7k50"&&(Nt.textContent=Fa),hn=r(Q),ce=c(Q,"DIV",{class:!0});var Ft=$(ce);f(nt.$$.fragment,Ft),fn=r(Ft),Pt=c(Ft,"P",{"data-svelte-h":!0}),y(Pt)!=="svelte-1x3yxsa"&&(Pt.textContent=Ia),Ft.forEach(a),gn=r(Q),me=c(Q,"DIV",{class:!0});var It=$(me);f(at.$$.fragment,It),un=r(It),Et=c(It,"P",{"data-svelte-h":!0}),y(Et)!=="svelte-8uar9g"&&(Et.innerHTML=Ja),It.forEach(a),Q.forEach(a),So=r(e),f(st.$$.fragment,e),Vo=r(e),q=c(e,"DIV",{class:!0});var se=$(q);f(rt.$$.fragment,se),_n=r(se),Lt=c(se,"P",{"data-svelte-h":!0}),y(Lt)!=="svelte-1wah8ut"&&(Lt.innerHTML=ka),bn=r(se),E=c(se,"DIV",{class:!0});var D=$(E);f(it.$$.fragment,D),yn=r(D),Rt=c(D,"P",{"data-svelte-h":!0}),y(Rt)!=="svelte-1l2zdux"&&(Rt.innerHTML=za),Mn=r(D),f(pe.$$.fragment,D),Tn=r(D),f(he.$$.fragment,D),D.forEach(a),se.forEach(a),Xo=r(e),f(lt.$$.fragment,e),Go=r(e),Y=c(e,"DIV",{class:!0});var re=$(Y);f(dt.$$.fragment,re),wn=r(re),St=c(re,"P",{"data-svelte-h":!0}),y(St)!=="svelte-1z0z0yr"&&(St.innerHTML=Wa),vn=r(re),L=c(re,"DIV",{class:!0});var A=$(L);f(ct.$$.fragment,A),xn=r(A),Vt=c(A,"P",{"data-svelte-h":!0}),y(Vt)!=="svelte-1sm3wk1"&&(Vt.innerHTML=Ua),$n=r(A),f(fe.$$.fragment,A),Bn=r(A),f(ge.$$.fragment,A),A.forEach(a),re.forEach(a),Ho=r(e),f(mt.$$.fragment,e),qo=r(e),N=c(e,"DIV",{class:!0});var O=$(N);f(pt.$$.fragment,O),jn=r(O),Xt=c(O,"P",{"data-svelte-h":!0}),y(Xt)!=="svelte-83lcss"&&(Xt.textContent=Za),Cn=r(O),Gt=c(O,"P",{"data-svelte-h":!0}),y(Gt)!=="svelte-1gjh92c"&&(Gt.innerHTML=Na),Fn=r(O),R=c(O,"DIV",{class:!0});var K=$(R);f(ht.$$.fragment,K),In=r(K),Ht=c(K,"P",{"data-svelte-h":!0}),y(Ht)!=="svelte-9xqli1"&&(Ht.innerHTML=Pa),Jn=r(K),f(ue.$$.fragment,K),kn=r(K),f(_e.$$.fragment,K),K.forEach(a),O.forEach(a),Yo=r(e),f(ft.$$.fragment,e),Qo=r(e),P=c(e,"DIV",{class:!0});var Be=$(P);f(gt.$$.fragment,Be),zn=r(Be),qt=c(Be,"P",{"data-svelte-h":!0}),y(qt)!=="svelte-olwnwh"&&(qt.textContent=Ea),Wn=r(Be),Yt=c(Be,"P",{"data-svelte-h":!0}),y(Yt)!=="svelte-1gjh92c"&&(Yt.innerHTML=La),Un=r(Be),S=c(Be,"DIV",{class:!0});var je=$(S);f(ut.$$.fragment,je),Zn=r(je),Qt=c(je,"P",{"data-svelte-h":!0}),y(Qt)!=="svelte-hxuxnh"&&(Qt.innerHTML=Ra),Nn=r(je),f(be.$$.fragment,je),Pn=r(je),f(ye.$$.fragment,je),je.forEach(a),Be.forEach(a),Do=r(e),f(_t.$$.fragment,e),Ao=r(e),j=c(e,"DIV",{class:!0});var z=$(j);f(bt.$$.fragment,z),En=r(z),Dt=c(z,"P",{"data-svelte-h":!0}),y(Dt)!=="svelte-yakhoj"&&(Dt.textContent=Sa),Ln=r(z),At=c(z,"P",{"data-svelte-h":!0}),y(At)!=="svelte-99cpmj"&&(At.innerHTML=Va),Rn=r(z),Ot=c(z,"P",{"data-svelte-h":!0}),y(Ot)!=="svelte-10nfsf3"&&(Ot.innerHTML=Xa),Sn=r(z),Kt=c(z,"P",{"data-svelte-h":!0}),y(Kt)!=="svelte-1pplc4a"&&(Kt.textContent=Ga),Vn=r(z),eo=c(z,"UL",{"data-svelte-h":!0}),y(eo)!=="svelte-1w7z84m"&&(eo.innerHTML=Ha),Xn=r(z),V=c(z,"DIV",{class:!0});var Ce=$(V);f(yt.$$.fragment,Ce),Gn=r(Ce),to=c(Ce,"P",{"data-svelte-h":!0}),y(to)!=="svelte-15yd1qb"&&(to.innerHTML=qa),Hn=r(Ce),f(Me.$$.fragment,Ce),qn=r(Ce),f(Te.$$.fragment,Ce),Ce.forEach(a),z.forEach(a),Oo=r(e),f(Mt.$$.fragment,e),Ko=r(e),C=c(e,"DIV",{class:!0});var W=$(C);f(Tt.$$.fragment,W),Yn=r(W),oo=c(W,"P",{"data-svelte-h":!0}),y(oo)!=="svelte-1xpbgj8"&&(oo.textContent=Ya),Qn=r(W),no=c(W,"P",{"data-svelte-h":!0}),y(no)!=="svelte-99cpmj"&&(no.innerHTML=Qa),Dn=r(W),ao=c(W,"P",{"data-svelte-h":!0}),y(ao)!=="svelte-10nfsf3"&&(ao.innerHTML=Da),An=r(W),so=c(W,"P",{"data-svelte-h":!0}),y(so)!=="svelte-1pplc4a"&&(so.textContent=Aa),On=r(W),ro=c(W,"UL",{"data-svelte-h":!0}),y(ro)!=="svelte-1w7z84m"&&(ro.innerHTML=Oa),Kn=r(W),k=c(W,"DIV",{class:!0});var ee=$(k);f(wt.$$.fragment,ee),ea=r(ee),io=c(ee,"P",{"data-svelte-h":!0}),y(io)!=="svelte-15yd1qb"&&(io.innerHTML=Ka),ta=r(ee),f(we.$$.fragment,ee),oa=r(ee),lo=c(ee,"P",{"data-svelte-h":!0}),y(lo)!=="svelte-1rn5vq7"&&(lo.innerHTML=es),na=r(ee),f(ve.$$.fragment,ee),ee.forEach(a),W.forEach(a),en=r(e),f(vt.$$.fragment,e),tn=r(e),F=c(e,"DIV",{class:!0});var U=$(F);f(xt.$$.fragment,U),aa=r(U),co=c(U,"P",{"data-svelte-h":!0}),y(co)!=="svelte-83lcss"&&(co.textContent=ts),sa=r(U),mo=c(U,"P",{"data-svelte-h":!0}),y(mo)!=="svelte-99cpmj"&&(mo.innerHTML=os),ra=r(U),po=c(U,"P",{"data-svelte-h":!0}),y(po)!=="svelte-10nfsf3"&&(po.innerHTML=ns),ia=r(U),ho=c(U,"P",{"data-svelte-h":!0}),y(ho)!=="svelte-1pplc4a"&&(ho.textContent=as),la=r(U),fo=c(U,"UL",{"data-svelte-h":!0}),y(fo)!=="svelte-1w7z84m"&&(fo.innerHTML=ss),da=r(U),X=c(U,"DIV",{class:!0});var Fe=$(X);f($t.$$.fragment,Fe),ca=r(Fe),go=c(Fe,"P",{"data-svelte-h":!0}),y(go)!=="svelte-15yd1qb"&&(go.innerHTML=rs),ma=r(Fe),f(xe.$$.fragment,Fe),pa=r(Fe),f($e.$$.fragment,Fe),Fe.forEach(a),U.forEach(a),on=r(e),uo=c(e,"P",{}),$(uo).forEach(a),this.h()},h(){x(t,"name","hf:doc:metadata"),x(t,"content",Is),ls(ie.src,_a="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/beit_architecture.jpg")||x(ie,"src",_a),x(ie,"alt","drawing"),x(ie,"width","600"),x(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){i(document.head,t),l(e,M,n),l(e,m,n),l(e,p,n),g(T,e,n),l(e,o,n),g(w,e,n),l(e,bo,n),l(e,ke,n),l(e,yo,n),l(e,ze,n),l(e,Mo,n),l(e,We,n),l(e,To,n),g(Ue,e,n),l(e,wo,n),l(e,Ze,n),l(e,vo,n),l(e,ie,n),l(e,xo,n),l(e,Ne,n),l(e,$o,n),l(e,Pe,n),l(e,Bo,n),g(Ee,e,n),l(e,jo,n),l(e,Le,n),l(e,Co,n),g(Re,e,n),l(e,Fo,n),l(e,Se,n),l(e,Io,n),l(e,Ve,n),l(e,Jo,n),l(e,Xe,n),l(e,ko,n),l(e,Ge,n),l(e,zo,n),g(He,e,n),l(e,Wo,n),l(e,te,n),g(qe,te,null),i(te,an),i(te,kt),l(e,Uo,n),l(e,oe,n),g(Ye,oe,null),i(oe,sn),i(oe,zt),l(e,Zo,n),g(Qe,e,n),l(e,No,n),l(e,G,n),g(De,G,null),i(G,rn),i(G,Wt),i(G,ln),g(le,G,null),l(e,Po,n),g(Ae,e,n),l(e,Eo,n),l(e,H,n),g(Oe,H,null),i(H,dn),i(H,Ut),g(Ke,Ut,null),i(H,cn),i(H,de),g(et,de,null),i(de,mn),i(de,Zt),l(e,Lo,n),g(tt,e,n),l(e,Ro,n),l(e,Z,n),g(ot,Z,null),i(Z,pn),i(Z,Nt),i(Z,hn),i(Z,ce),g(nt,ce,null),i(ce,fn),i(ce,Pt),i(Z,gn),i(Z,me),g(at,me,null),i(me,un),i(me,Et),l(e,So,n),g(st,e,n),l(e,Vo,n),l(e,q,n),g(rt,q,null),i(q,_n),i(q,Lt),i(q,bn),i(q,E),g(it,E,null),i(E,yn),i(E,Rt),i(E,Mn),g(pe,E,null),i(E,Tn),g(he,E,null),l(e,Xo,n),g(lt,e,n),l(e,Go,n),l(e,Y,n),g(dt,Y,null),i(Y,wn),i(Y,St),i(Y,vn),i(Y,L),g(ct,L,null),i(L,xn),i(L,Vt),i(L,$n),g(fe,L,null),i(L,Bn),g(ge,L,null),l(e,Ho,n),g(mt,e,n),l(e,qo,n),l(e,N,n),g(pt,N,null),i(N,jn),i(N,Xt),i(N,Cn),i(N,Gt),i(N,Fn),i(N,R),g(ht,R,null),i(R,In),i(R,Ht),i(R,Jn),g(ue,R,null),i(R,kn),g(_e,R,null),l(e,Yo,n),g(ft,e,n),l(e,Qo,n),l(e,P,n),g(gt,P,null),i(P,zn),i(P,qt),i(P,Wn),i(P,Yt),i(P,Un),i(P,S),g(ut,S,null),i(S,Zn),i(S,Qt),i(S,Nn),g(be,S,null),i(S,Pn),g(ye,S,null),l(e,Do,n),g(_t,e,n),l(e,Ao,n),l(e,j,n),g(bt,j,null),i(j,En),i(j,Dt),i(j,Ln),i(j,At),i(j,Rn),i(j,Ot),i(j,Sn),i(j,Kt),i(j,Vn),i(j,eo),i(j,Xn),i(j,V),g(yt,V,null),i(V,Gn),i(V,to),i(V,Hn),g(Me,V,null),i(V,qn),g(Te,V,null),l(e,Oo,n),g(Mt,e,n),l(e,Ko,n),l(e,C,n),g(Tt,C,null),i(C,Yn),i(C,oo),i(C,Qn),i(C,no),i(C,Dn),i(C,ao),i(C,An),i(C,so),i(C,On),i(C,ro),i(C,Kn),i(C,k),g(wt,k,null),i(k,ea),i(k,io),i(k,ta),g(we,k,null),i(k,oa),i(k,lo),i(k,na),g(ve,k,null),l(e,en,n),g(vt,e,n),l(e,tn,n),l(e,F,n),g(xt,F,null),i(F,aa),i(F,co),i(F,sa),i(F,mo),i(F,ra),i(F,po),i(F,ia),i(F,ho),i(F,la),i(F,fo),i(F,da),i(F,X),g($t,X,null),i(X,ca),i(X,go),i(X,ma),g(xe,X,null),i(X,pa),g($e,X,null),l(e,on,n),l(e,uo,n),nn=!0},p(e,[n]){const Bt={};n&2&&(Bt.$$scope={dirty:n,ctx:e}),le.$set(Bt);const jt={};n&2&&(jt.$$scope={dirty:n,ctx:e}),pe.$set(jt);const ne={};n&2&&(ne.$$scope={dirty:n,ctx:e}),he.$set(ne);const ae={};n&2&&(ae.$$scope={dirty:n,ctx:e}),fe.$set(ae);const _o={};n&2&&(_o.$$scope={dirty:n,ctx:e}),ge.$set(_o);const Ct={};n&2&&(Ct.$$scope={dirty:n,ctx:e}),ue.$set(Ct);const Q={};n&2&&(Q.$$scope={dirty:n,ctx:e}),_e.$set(Q);const Ft={};n&2&&(Ft.$$scope={dirty:n,ctx:e}),be.$set(Ft);const It={};n&2&&(It.$$scope={dirty:n,ctx:e}),ye.$set(It);const se={};n&2&&(se.$$scope={dirty:n,ctx:e}),Me.$set(se);const D={};n&2&&(D.$$scope={dirty:n,ctx:e}),Te.$set(D);const re={};n&2&&(re.$$scope={dirty:n,ctx:e}),we.$set(re);const A={};n&2&&(A.$$scope={dirty:n,ctx:e}),ve.$set(A);const O={};n&2&&(O.$$scope={dirty:n,ctx:e}),xe.$set(O);const K={};n&2&&(K.$$scope={dirty:n,ctx:e}),$e.$set(K)},i(e){nn||(u(T.$$.fragment,e),u(w.$$.fragment,e),u(Ue.$$.fragment,e),u(Ee.$$.fragment,e),u(Re.$$.fragment,e),u(He.$$.fragment,e),u(qe.$$.fragment,e),u(Ye.$$.fragment,e),u(Qe.$$.fragment,e),u(De.$$.fragment,e),u(le.$$.fragment,e),u(Ae.$$.fragment,e),u(Oe.$$.fragment,e),u(Ke.$$.fragment,e),u(et.$$.fragment,e),u(tt.$$.fragment,e),u(ot.$$.fragment,e),u(nt.$$.fragment,e),u(at.$$.fragment,e),u(st.$$.fragment,e),u(rt.$$.fragment,e),u(it.$$.fragment,e),u(pe.$$.fragment,e),u(he.$$.fragment,e),u(lt.$$.fragment,e),u(dt.$$.fragment,e),u(ct.$$.fragment,e),u(fe.$$.fragment,e),u(ge.$$.fragment,e),u(mt.$$.fragment,e),u(pt.$$.fragment,e),u(ht.$$.fragment,e),u(ue.$$.fragment,e),u(_e.$$.fragment,e),u(ft.$$.fragment,e),u(gt.$$.fragment,e),u(ut.$$.fragment,e),u(be.$$.fragment,e),u(ye.$$.fragment,e),u(_t.$$.fragment,e),u(bt.$$.fragment,e),u(yt.$$.fragment,e),u(Me.$$.fragment,e),u(Te.$$.fragment,e),u(Mt.$$.fragment,e),u(Tt.$$.fragment,e),u(wt.$$.fragment,e),u(we.$$.fragment,e),u(ve.$$.fragment,e),u(vt.$$.fragment,e),u(xt.$$.fragment,e),u($t.$$.fragment,e),u(xe.$$.fragment,e),u($e.$$.fragment,e),nn=!0)},o(e){_(T.$$.fragment,e),_(w.$$.fragment,e),_(Ue.$$.fragment,e),_(Ee.$$.fragment,e),_(Re.$$.fragment,e),_(He.$$.fragment,e),_(qe.$$.fragment,e),_(Ye.$$.fragment,e),_(Qe.$$.fragment,e),_(De.$$.fragment,e),_(le.$$.fragment,e),_(Ae.$$.fragment,e),_(Oe.$$.fragment,e),_(Ke.$$.fragment,e),_(et.$$.fragment,e),_(tt.$$.fragment,e),_(ot.$$.fragment,e),_(nt.$$.fragment,e),_(at.$$.fragment,e),_(st.$$.fragment,e),_(rt.$$.fragment,e),_(it.$$.fragment,e),_(pe.$$.fragment,e),_(he.$$.fragment,e),_(lt.$$.fragment,e),_(dt.$$.fragment,e),_(ct.$$.fragment,e),_(fe.$$.fragment,e),_(ge.$$.fragment,e),_(mt.$$.fragment,e),_(pt.$$.fragment,e),_(ht.$$.fragment,e),_(ue.$$.fragment,e),_(_e.$$.fragment,e),_(ft.$$.fragment,e),_(gt.$$.fragment,e),_(ut.$$.fragment,e),_(be.$$.fragment,e),_(ye.$$.fragment,e),_(_t.$$.fragment,e),_(bt.$$.fragment,e),_(yt.$$.fragment,e),_(Me.$$.fragment,e),_(Te.$$.fragment,e),_(Mt.$$.fragment,e),_(Tt.$$.fragment,e),_(wt.$$.fragment,e),_(we.$$.fragment,e),_(ve.$$.fragment,e),_(vt.$$.fragment,e),_(xt.$$.fragment,e),_($t.$$.fragment,e),_(xe.$$.fragment,e),_($e.$$.fragment,e),nn=!1},d(e){e&&(a(M),a(m),a(p),a(o),a(bo),a(ke),a(yo),a(ze),a(Mo),a(We),a(To),a(wo),a(Ze),a(vo),a(ie),a(xo),a(Ne),a($o),a(Pe),a(Bo),a(jo),a(Le),a(Co),a(Fo),a(Se),a(Io),a(Ve),a(Jo),a(Xe),a(ko),a(Ge),a(zo),a(Wo),a(te),a(Uo),a(oe),a(Zo),a(No),a(G),a(Po),a(Eo),a(H),a(Lo),a(Ro),a(Z),a(So),a(Vo),a(q),a(Xo),a(Go),a(Y),a(Ho),a(qo),a(N),a(Yo),a(Qo),a(P),a(Do),a(Ao),a(j),a(Oo),a(Ko),a(C),a(en),a(tn),a(F),a(on),a(uo)),a(t),b(T,e),b(w,e),b(Ue,e),b(Ee,e),b(Re,e),b(He,e),b(qe),b(Ye),b(Qe,e),b(De),b(le),b(Ae,e),b(Oe),b(Ke),b(et),b(tt,e),b(ot),b(nt),b(at),b(st,e),b(rt),b(it),b(pe),b(he),b(lt,e),b(dt),b(ct),b(fe),b(ge),b(mt,e),b(pt),b(ht),b(ue),b(_e),b(ft,e),b(gt),b(ut),b(be),b(ye),b(_t,e),b(bt),b(yt),b(Me),b(Te),b(Mt,e),b(Tt),b(wt),b(we),b(ve),b(vt,e),b(xt),b($t),b(xe),b($e)}}}const Is='{"title":"BEiT","local":"beit","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"BEiT specific outputs","local":"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling","sections":[],"depth":2},{"title":"BeitConfig","local":"transformers.BeitConfig","sections":[],"depth":2},{"title":"BeitFeatureExtractor","local":"transformers.BeitFeatureExtractor","sections":[],"depth":2},{"title":"BeitImageProcessor","local":"transformers.BeitImageProcessor","sections":[],"depth":2},{"title":"BeitModel","local":"transformers.BeitModel","sections":[],"depth":2},{"title":"BeitForMaskedImageModeling","local":"transformers.BeitForMaskedImageModeling","sections":[],"depth":2},{"title":"BeitForImageClassification","local":"transformers.BeitForImageClassification","sections":[],"depth":2},{"title":"BeitForSemanticSegmentation","local":"transformers.BeitForSemanticSegmentation","sections":[],"depth":2},{"title":"FlaxBeitModel","local":"transformers.FlaxBeitModel","sections":[],"depth":2},{"title":"FlaxBeitForMaskedImageModeling","local":"transformers.FlaxBeitForMaskedImageModeling","sections":[],"depth":2},{"title":"FlaxBeitForImageClassification","local":"transformers.FlaxBeitForImageClassification","sections":[],"depth":2}],"depth":1}';function Js(v){return ds(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ls extends cs{constructor(t){super(),ms(this,t,Js,Fs,is,{})}}export{Ls as component};
