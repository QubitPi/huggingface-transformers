import{s as is,f as ls,o as ds,n as J}from"../chunks/scheduler.9bc65507.js";import{S as cs,i as ms,g as d,s,r as h,A as ps,h as c,f as a,c as r,j as $,u as f,x as y,k as x,y as i,a as l,v as g,d as u,t as _,w as b}from"../chunks/index.707bf1b6.js";import{T as Jt}from"../chunks/Tip.c2ecdbf4.js";import{D as B}from"../chunks/Docstring.17db21ae.js";import{C as Je}from"../chunks/CodeBlock.54a9f38d.js";import{E as Ie}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as hs}from"../chunks/PipelineTag.44585822.js";import{H as I}from"../chunks/Heading.342b1fa6.js";function fs(v){let t,M="Example:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlaXRDb25maWclMkMlMjBCZWl0TW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQkVpVCUyMGJlaXQtYmFzZS1wYXRjaDE2LTIyNC1wdDIyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCZWl0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGJlaXQtYmFzZS1wYXRjaDE2LTIyNC1wdDIyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmVpdE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BeitConfig, BeitModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BEiT beit-base-patch16-224-pt22k style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BeitConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the beit-base-patch16-224-pt22k style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-11lpom8"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function gs(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function us(v){let t,M="Example:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJlaXRNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLXBhdGNoMTYtMjI0LXB0MjJrJTIyKSUwQW1vZGVsJTIwJTNEJTIwQmVpdE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZiZWl0LWJhc2UtcGF0Y2gxNi0yMjQtcHQyMmslMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BeitModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitModel.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">197</span>, <span class="hljs-number">768</span>]`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-11lpom8"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function _s(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function bs(v){let t,M="Examples:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJlaXRGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmJlaXQtYmFzZS1wYXRjaDE2LTIyNC1wdDIyayUyMiklMEFtb2RlbCUyMCUzRCUyMEJlaXRGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZiZWl0LWJhc2UtcGF0Y2gxNi0yMjQtcHQyMmslMjIpJTBBJTBBbnVtX3BhdGNoZXMlMjAlM0QlMjAobW9kZWwuY29uZmlnLmltYWdlX3NpemUlMjAlMkYlMkYlMjBtb2RlbC5jb25maWcucGF0Y2hfc2l6ZSklMjAqKiUyMDIlMEFwaXhlbF92YWx1ZXMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5waXhlbF92YWx1ZXMlMEElMjMlMjBjcmVhdGUlMjByYW5kb20lMjBib29sZWFuJTIwbWFzayUyMG9mJTIwc2hhcGUlMjAoYmF0Y2hfc2l6ZSUyQyUyMG51bV9wYXRjaGVzKSUwQWJvb2xfbWFza2VkX3BvcyUyMCUzRCUyMHRvcmNoLnJhbmRpbnQobG93JTNEMCUyQyUyMGhpZ2glM0QyJTJDJTIwc2l6ZSUzRCgxJTJDJTIwbnVtX3BhdGNoZXMpKS5ib29sKCklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwocGl4ZWxfdmFsdWVzJTJDJTIwYm9vbF9tYXNrZWRfcG9zJTNEYm9vbF9tYXNrZWRfcG9zKSUwQWxvc3MlMkMlMjBsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvc3MlMkMlMjBvdXRwdXRzLmxvZ2l0cyUwQWxpc3QobG9naXRzLnNoYXBlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BeitForMaskedImageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>num_patches = (model.config.image_size // model.config.patch_size) ** <span class="hljs-number">2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create random boolean mask of shape (batch_size, num_patches)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>bool_masked_pos = torch.randint(low=<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=(<span class="hljs-number">1</span>, num_patches)).<span class="hljs-built_in">bool</span>()

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, logits = outputs.loss, outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(logits.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">196</span>, <span class="hljs-number">8192</span>]`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function ys(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function Ms(v){let t,M="Example:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJlaXRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmh1Z2dpbmdmYWNlJTJGY2F0cy1pbWFnZSUyMiklMEFpbWFnZSUyMCUzRCUyMGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTVCJTIyaW1hZ2UlMjIlNUQlNUIwJTVEJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZiZWl0LWJhc2UtcGF0Y2gxNi0yMjQlMjIpJTBBbW9kZWwlMjAlM0QlMjBCZWl0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLXBhdGNoMTYtMjI0JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BeitForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitForImageClassification.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-11lpom8"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function Ts(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function ws(v){let t,M="Examples:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJlaXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLWZpbmV0dW5lZC1hZGUtNjQwLTY0MCUyMiklMEFtb2RlbCUyMCUzRCUyMEJlaXRGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLWZpbmV0dW5lZC1hZGUtNjQwLTY0MCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMjMlMjBsb2dpdHMlMjBhcmUlMjBvZiUyMHNoYXBlJTIwKGJhdGNoX3NpemUlMkMlMjBudW1fbGFiZWxzJTJDJTIwaGVpZ2h0JTJDJTIwd2lkdGgpJTBBbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BeitForSemanticSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-finetuned-ade-640-640&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-finetuned-ade-640-640&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># logits are of shape (batch_size, num_labels, height, width)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function vs(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function xs(v){let t,M="Examples:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEZsYXhCZWl0TW9kZWwlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmJlaXQtYmFzZS1wYXRjaDE2LTIyNC1wdDIyay1mdDIyayUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhCZWl0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmJlaXQtYmFzZS1wYXRjaDE2LTIyNC1wdDIyay1mdDIyayUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMm5wJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, FlaxBeitModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k-ft22k&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxBeitModel.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k-ft22k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function $s(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function Bs(v){let t,M="Examples:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEJlaXRGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZiZWl0LWJhc2UtcGF0Y2gxNi0yMjQtcHQyMmslMjIpJTBBbW9kZWwlMjAlM0QlMjBCZWl0Rm9yTWFza2VkSW1hZ2VNb2RlbGluZy5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLXBhdGNoMTYtMjI0LXB0MjJrJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIybnAlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0cyUyMCUzRCUyMG91dHB1dHMubG9naXRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, BeitForMaskedImageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BeitForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-kvfsh7"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function js(v){let t,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=d("p"),t.innerHTML=M},l(m){t=c(m,"P",{"data-svelte-h":!0}),y(t)!=="svelte-fincs2"&&(t.innerHTML=M)},m(m,p){l(m,t,p)},p:J,d(m){m&&a(t)}}}function Cs(v){let t,M="Example:",m,p,T;return p=new Je({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEZsYXhCZWl0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGYmVpdC1iYXNlLXBhdGNoMTYtMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEJlaXRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZiZWl0LWJhc2UtcGF0Y2gxNi0yMjQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJucCUyMiklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2NsYXNzX2lkeCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KCUyMlByZWRpY3RlZCUyMGNsYXNzJTNBJTIyJTJDJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2NsYXNzX2lkeCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, FlaxBeitForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxBeitForImageClassification.from_pretrained(<span class="hljs-string">&quot;microsoft/beit-base-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_idx = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])`,wrap:!1}}),{c(){t=d("p"),t.textContent=M,m=s(),h(p.$$.fragment)},l(o){t=c(o,"P",{"data-svelte-h":!0}),y(t)!=="svelte-11lpom8"&&(t.textContent=M),m=r(o),f(p.$$.fragment,o)},m(o,w){l(o,t,w),l(o,m,w),g(p,o,w),T=!0},p:J,i(o){T||(u(p.$$.fragment,o),T=!0)},o(o){_(p.$$.fragment,o),T=!1},d(o){o&&(a(t),a(m)),b(p,o)}}}function Fs(v){let t,M,m,p,T,o,w,bo,ke,ha=`BEiT モデルは、<a href="https://arxiv.org/abs/2106.08254" rel="nofollow">BEiT: BERT Pre-Training of Image Transformers</a> で提案されました。
ハンボ・バオ、リー・ドン、フル・ウェイ。 BERT に触発された BEiT は、自己教師ありの事前トレーニングを作成した最初の論文です。
ビジョン トランスフォーマー (ViT) は、教師付き事前トレーニングよりも優れたパフォーマンスを発揮します。クラスを予測するためにモデルを事前トレーニングするのではなく
(<a href="https://arxiv.org/abs/2010.11929" rel="nofollow">オリジナルの ViT 論文</a> で行われたように) 画像の BEiT モデルは、次のように事前トレーニングされています。
マスクされた OpenAI の <a href="https://arxiv.org/abs/2102.12092" rel="nofollow">DALL-E モデル</a> のコードブックからビジュアル トークンを予測します
パッチ。`,yo,ze,fa="論文の要約は次のとおりです。",Mo,We,ga=`<em>自己教師あり視覚表現モデル BEiT (Bidirectional Encoderpresentation) を導入します。
イメージトランスフォーマーより。自然言語処理分野で開発されたBERTに倣い、マスク画像を提案します。
ビジョントランスフォーマーを事前にトレーニングするためのモデリングタスク。具体的には、事前トレーニングでは各画像に 2 つのビューがあります。
パッチ (16x16 ピクセルなど)、およびビジュアル トークン (つまり、個別のトークン)。まず、元の画像を「トークン化」して、
ビジュアルトークン。次に、いくつかの画像パッチをランダムにマスクし、それらをバックボーンの Transformer に供給します。事前トレーニング
目的は、破損したイメージ パッチに基づいて元のビジュアル トークンを回復することです。 BEiTの事前トレーニング後、
事前トレーニングされたエンコーダーにタスク レイヤーを追加することで、ダウンストリーム タスクのモデル パラメーターを直接微調整します。
画像分類とセマンティックセグメンテーションに関する実験結果は、私たちのモデルが競争力のある結果を達成することを示しています
以前の事前トレーニング方法を使用して。たとえば、基本サイズの BEiT は、ImageNet-1K で 83.2% のトップ 1 精度を達成します。
同じ設定でゼロからの DeiT トレーニング (81.8%) を大幅に上回りました。また、大型BEiTは
86.3% は ImageNet-1K のみを使用しており、ImageNet-22K での教師付き事前トレーニングを使用した ViT-L (85.2%) を上回っています。</em>`,To,Ue,wo,Ze,ua=`<li>BEiT モデルは通常のビジョン トランスフォーマーですが、教師ありではなく自己教師ありの方法で事前トレーニングされています。彼らは
ImageNet-1K および CIFAR-100 で微調整すると、<a href="vit">オリジナル モデル (ViT)</a> と <a href="deit">データ効率の高いイメージ トランスフォーマー (DeiT)</a> の両方を上回るパフォーマンスを発揮します。推論に関するデモノートブックもチェックできます。
カスタム データの微調整は <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer" rel="nofollow">こちら</a> (置き換えるだけで済みます)
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitImageProcessor">BeitImageProcessor</a> による <code>ViTFeatureExtractor</code> と
<code>ViTForImageClassification</code> by <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a>)。</li> <li>DALL-E の画像トークナイザーと BEiT を組み合わせる方法を紹介するデモ ノートブックも利用可能です。
マスクされた画像モデリングを実行します。 <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/BEiT" rel="nofollow">ここ</a> で見つけることができます。</li> <li>BEiT モデルは各画像が同じサイズ (解像度) であることを期待しているため、次のように使用できます。
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitImageProcessor">BeitImageProcessor</a> を使用して、モデルの画像のサイズを変更 (または再スケール) し、正規化します。</li> <li>事前トレーニングまたは微調整中に使用されるパッチ解像度と画像解像度の両方が名前に反映されます。
各チェックポイント。たとえば、<code>microsoft/beit-base-patch16-224</code>は、パッチ付きの基本サイズのアーキテクチャを指します。
解像度は 16x16、微調整解像度は 224x224 です。すべてのチェックポイントは <a href="https://huggingface.co/models?search=microsoft/beit" rel="nofollow">ハブ</a> で見つけることができます。</li> <li>利用可能なチェックポイントは、(1) <a href="http://www.image-net.org/" rel="nofollow">ImageNet-22k</a> で事前トレーニングされています (
1,400 万の画像と 22,000 のクラス) のみ、(2) ImageNet-22k でも微調整、または (3) <a href="http://www.image-net.org/challenges/LSVRC" rel="nofollow">ImageNet-1k</a>でも微調整/2012/) (ILSVRC 2012 とも呼ばれ、130 万件のコレクション)
画像と 1,000 クラス)。</li> <li>BEiT は、T5 モデルからインスピレーションを得た相対位置埋め込みを使用します。事前トレーニング中に、著者は次のことを共有しました。
いくつかの自己注意層間の相対的な位置の偏り。微調整中、各レイヤーの相対位置
バイアスは、事前トレーニング後に取得された共有相対位置バイアスで初期化されます。ご希望の場合は、
モデルを最初から事前トレーニングするには、<code>use_relative_position_bias</code> または
追加するには、<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a> の <code>use_relative_position_bias</code> 属性を <code>True</code> に設定します。
位置の埋め込み。</li>`,vo,ie,_a,xo,Ne,ba='BEiT の事前トレーニング。 <a href="https://arxiv.org/abs/2106.08254">元の論文から抜粋。</a>',$o,Pe,ya=`このモデルは、<a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a> によって提供されました。このモデルの JAX/FLAX バージョンは、
<a href="https://huggingface.co/kamalkraj" rel="nofollow">kamalkraj</a> による投稿。元のコードは <a href="https://github.com/microsoft/unilm/tree/master/beit" rel="nofollow">ここ</a> にあります。`,Bo,Ee,jo,Le,Ma="BEiT の使用を開始するのに役立つ公式 Hugging Face およびコミュニティ (🌎 で示されている) リソースのリスト。",Co,Re,Fo,Se,Ta='<li><a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> は、この <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">サンプル スクリプト</a> および <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">ノートブック</a>。</li> <li>参照: <a href="../tasks/image_classification">画像分類タスク ガイド</a></li>',Io,Ve,wa="<strong>セマンティック セグメンテーション</strong>",Jo,Xe,va='<li><a href="../tasks/semantic_segmentation">セマンティック セグメンテーション タスク ガイド</a></li>',ko,Ge,xa="ここに含めるリソースの送信に興味がある場合は、お気軽にプル リクエストを開いてください。審査させていただきます。リソースは、既存のリソースを複製するのではなく、何か新しいものを示すことが理想的です。",zo,He,Wo,te,qe,an,kt,$a='Class for outputs of <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitModel">BeitModel</a>.',Uo,oe,Ye,sn,zt,Ba='Class for outputs of <a href="/docs/transformers/main/ja/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a>.',Zo,Qe,No,G,De,rn,Wt,ja=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitModel">BeitModel</a>. It is used to instantiate an BEiT
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the BEiT
<a href="https://huggingface.co/microsoft/beit-base-patch16-224-pt22k" rel="nofollow">microsoft/beit-base-patch16-224-pt22k</a> architecture.`,ln,le,Po,Ae,Eo,H,Oe,dn,Ut,Ke,cn,de,et,mn,Zt,Ca='Converts the output of <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> into semantic segmentation maps. Only supports PyTorch.',Lo,tt,Ro,Z,ot,pn,Nt,Fa="Constructs a BEiT image processor.",hn,ce,nt,fn,Pt,Ia="Preprocess an image or batch of images.",gn,me,at,un,Et,Ja='Converts the output of <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> into semantic segmentation maps. Only supports PyTorch.',So,st,Vo,q,rt,_n,Lt,ka=`The bare Beit Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,bn,E,it,yn,Rt,za='The <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitModel">BeitModel</a> forward method, overrides the <code>__call__</code> special method.',Mn,pe,Tn,he,Xo,lt,Go,Y,dt,wn,St,Wa=`Beit Model transformer with a ‘language’ modeling head on top. BEiT does masked image modeling by predicting
visual tokens of a Vector-Quantize Variational Autoencoder (VQ-VAE), whereas other vision models like ViT and DeiT
predict RGB pixel values. As a result, this class is incompatible with <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoModelForMaskedImageModeling">AutoModelForMaskedImageModeling</a>, so you
will need to use <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForMaskedImageModeling">BeitForMaskedImageModeling</a> directly if you wish to do masked image modeling with BEiT.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,vn,L,ct,xn,Vt,Ua='The <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForMaskedImageModeling">BeitForMaskedImageModeling</a> forward method, overrides the <code>__call__</code> special method.',$n,fe,Bn,ge,Ho,mt,qo,N,pt,jn,Xt,Za=`Beit Model transformer with an image classification head on top (a linear layer on top of the average of the final
hidden states of the patch tokens) e.g. for ImageNet.`,Cn,Gt,Na=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Fn,R,ht,In,Ht,Pa='The <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',Jn,ue,kn,_e,Yo,ft,Qo,P,gt,zn,qt,Ea="Beit Model transformer with a semantic segmentation head on top e.g. for ADE20k, CityScapes.",Wn,Yt,La=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Un,S,ut,Zn,Qt,Ra='The <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> forward method, overrides the <code>__call__</code> special method.',Nn,be,Pn,ye,Do,_t,Ao,j,bt,En,Dt,Sa="The bare Beit Model transformer outputting raw hidden-states without any specific head on top.",Ln,At,Va=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`,Rn,Ot,Xa=`This model is also a
<a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html" rel="nofollow">flax.linen.Module</a> subclass. Use it as
a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
behavior.`,Sn,Kt,Ga="Finally, this model supports inherent JAX features such as:",Vn,eo,Ha='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',Xn,V,yt,Gn,to,qa="The <code>FlaxBeitPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",Hn,Me,qn,Te,Oo,Mt,Ko,C,Tt,Yn,oo,Ya="Beit Model transformer with a ‘language’ modeling head on top (to predict visual tokens).",Qn,no,Qa=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`,Dn,ao,Da=`This model is also a
<a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html" rel="nofollow">flax.linen.Module</a> subclass. Use it as
a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
behavior.`,An,so,Aa="Finally, this model supports inherent JAX features such as:",On,ro,Oa='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',Kn,k,wt,ea,io,Ka="The <code>FlaxBeitPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",ta,we,oa,lo,es=`bool_masked_pos (<code>numpy.ndarray</code> of shape <code>(batch_size, num_patches)</code>):
Boolean masked positions. Indicates which patches are masked (1) and which aren’t (0).`,na,ve,en,vt,tn,F,xt,aa,co,ts=`Beit Model transformer with an image classification head on top (a linear layer on top of the average of the final
hidden states of the patch tokens) e.g. for ImageNet.`,sa,mo,os=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`,ra,po,ns=`This model is also a
<a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html" rel="nofollow">flax.linen.Module</a> subclass. Use it as
a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
behavior.`,ia,ho,as="Finally, this model supports inherent JAX features such as:",la,fo,ss='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',da,X,$t,ca,go,rs="The <code>FlaxBeitPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",ma,xe,pa,$e,on,uo,nn;return T=new I({props:{title:"BEiT",local:"beit",headingTag:"h1"}}),w=new I({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Ue=new I({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),Ee=new I({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Re=new hs({props:{pipeline:"image-classification"}}),He=new I({props:{title:"BEiT specific outputs",local:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling",headingTag:"h2"}}),qe=new B({props:{name:"class transformers.models.beit.modeling_beit.BeitModelOutputWithPooling",anchor:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": FloatTensor = None"},{name:"pooler_output",val:": FloatTensor = None"},{name:"hidden_states",val:": Optional = None"},{name:"attentions",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Average of the last layer hidden states of the patch tokens (excluding the <em>[CLS]</em> token) if
<em>config.use_mean_pooling</em> is set to True. If set to False, then the final hidden state of the <em>[CLS]</em> token
will be returned.`,name:"pooler_output"},{anchor:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.`,name:"attentions"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L69"}}),Ye=new B({props:{name:"class transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling",anchor:"transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling",parameters:[{name:"last_hidden_state",val:": Array = None"},{name:"pooler_output",val:": Array = None"},{name:"hidden_states",val:": Optional = None"},{name:"attentions",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling.last_hidden_state",description:`<strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) &#x2014;
Sequence of hidden-states at the output of the last layer of the model.`,name:"last_hidden_state"},{anchor:"transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling.pooler_output",description:`<strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) &#x2014;
Average of the last layer hidden states of the patch tokens (excluding the <em>[CLS]</em> token) if
<em>config.use_mean_pooling</em> is set to True. If set to False, then the final hidden state of the <em>[CLS]</em> token
will be returned.`,name:"pooler_output"},{anchor:"transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling.hidden_states",description:`<strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of each layer plus
the initial embedding outputs.`,name:"hidden_states"},{anchor:"transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling.attentions",description:`<strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) &#x2014;
Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the weighted average in
the self-attention heads.`,name:"attentions"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L44"}}),Qe=new I({props:{title:"BeitConfig",local:"transformers.BeitConfig",headingTag:"h2"}}),De=new B({props:{name:"class transformers.BeitConfig",anchor:"transformers.BeitConfig",parameters:[{name:"vocab_size",val:" = 8192"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"use_mask_token",val:" = False"},{name:"use_absolute_position_embeddings",val:" = False"},{name:"use_relative_position_bias",val:" = False"},{name:"use_shared_relative_position_bias",val:" = False"},{name:"layer_scale_init_value",val:" = 0.1"},{name:"drop_path_rate",val:" = 0.1"},{name:"use_mean_pooling",val:" = True"},{name:"pool_scales",val:" = [1, 2, 3, 6]"},{name:"use_auxiliary_head",val:" = True"},{name:"auxiliary_loss_weight",val:" = 0.4"},{name:"auxiliary_channels",val:" = 256"},{name:"auxiliary_num_convs",val:" = 1"},{name:"auxiliary_concat_input",val:" = False"},{name:"semantic_loss_ignore_index",val:" = 255"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"add_fpn",val:" = False"},{name:"reshape_hidden_states",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BeitConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 8192) &#x2014;
Vocabulary size of the BEiT model. Defines the number of different image tokens that can be used during
pre-training.`,name:"vocab_size"},{anchor:"transformers.BeitConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.BeitConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.BeitConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.BeitConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.BeitConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.BeitConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.BeitConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.BeitConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BeitConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.BeitConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.BeitConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.BeitConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.BeitConfig.use_mask_token",description:`<strong>use_mask_token</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a mask token for masked image modeling.`,name:"use_mask_token"},{anchor:"transformers.BeitConfig.use_absolute_position_embeddings",description:`<strong>use_absolute_position_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use BERT-style absolute position embeddings.`,name:"use_absolute_position_embeddings"},{anchor:"transformers.BeitConfig.use_relative_position_bias",description:`<strong>use_relative_position_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use T5-style relative position embeddings in the self-attention layers.`,name:"use_relative_position_bias"},{anchor:"transformers.BeitConfig.use_shared_relative_position_bias",description:`<strong>use_shared_relative_position_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use the same relative position embeddings across all self-attention layers of the Transformer.`,name:"use_shared_relative_position_bias"},{anchor:"transformers.BeitConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Scale to use in the self-attention layers. 0.1 for base, 1e-5 for large. Set 0 to disable layer scale.`,name:"layer_scale_init_value"},{anchor:"transformers.BeitConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Stochastic depth rate per sample (when applied in the main path of residual layers).`,name:"drop_path_rate"},{anchor:"transformers.BeitConfig.use_mean_pooling",description:`<strong>use_mean_pooling</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to mean pool the final hidden states of the patches instead of using the final hidden state of the
CLS token, before applying the classification head.`,name:"use_mean_pooling"},{anchor:"transformers.BeitConfig.pool_scales",description:`<strong>pool_scales</strong> (<code>Tuple[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 3, 6]</code>) &#x2014;
Pooling scales used in Pooling Pyramid Module applied on the last feature map.`,name:"pool_scales"},{anchor:"transformers.BeitConfig.use_auxiliary_head",description:`<strong>use_auxiliary_head</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use an auxiliary head during training.`,name:"use_auxiliary_head"},{anchor:"transformers.BeitConfig.auxiliary_loss_weight",description:`<strong>auxiliary_loss_weight</strong> (<code>float</code>, <em>optional</em>, defaults to 0.4) &#x2014;
Weight of the cross-entropy loss of the auxiliary head.`,name:"auxiliary_loss_weight"},{anchor:"transformers.BeitConfig.auxiliary_channels",description:`<strong>auxiliary_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 256) &#x2014;
Number of channels to use in the auxiliary head.`,name:"auxiliary_channels"},{anchor:"transformers.BeitConfig.auxiliary_num_convs",description:`<strong>auxiliary_num_convs</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Number of convolutional layers to use in the auxiliary head.`,name:"auxiliary_num_convs"},{anchor:"transformers.BeitConfig.auxiliary_concat_input",description:`<strong>auxiliary_concat_input</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to concatenate the output of the auxiliary head with the input before the classification layer.`,name:"auxiliary_concat_input"},{anchor:"transformers.BeitConfig.semantic_loss_ignore_index",description:`<strong>semantic_loss_ignore_index</strong> (<code>int</code>, <em>optional</em>, defaults to 255) &#x2014;
The index that is ignored by the loss function of the semantic segmentation model.`,name:"semantic_loss_ignore_index"},{anchor:"transformers.BeitConfig.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.BeitConfig.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"},{anchor:"transformers.BeitConfig.add_fpn",description:`<strong>add_fpn</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to add a FPN as part of the backbone. Only relevant for <code>BeitBackbone</code>.`,name:"add_fpn"},{anchor:"transformers.BeitConfig.reshape_hidden_states",description:`<strong>reshape_hidden_states</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to reshape the feature maps to 4D tensors of shape <code>(batch_size, hidden_size, height, width)</code> in
case the model is used as backbone. If <code>False</code>, the feature maps will be 3D tensors of shape <code>(batch_size, seq_len, hidden_size)</code>. Only relevant for <code>BeitBackbone</code>.`,name:"reshape_hidden_states"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/configuration_beit.py#L37"}}),le=new Ie({props:{anchor:"transformers.BeitConfig.example",$$slots:{default:[fs]},$$scope:{ctx:v}}}),Ae=new I({props:{title:"BeitFeatureExtractor",local:"transformers.BeitFeatureExtractor",headingTag:"h2"}}),Oe=new B({props:{name:"class transformers.BeitFeatureExtractor",anchor:"transformers.BeitFeatureExtractor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/feature_extraction_beit.py#L26"}}),Ke=new B({props:{name:"__call__",anchor:"transformers.BeitFeatureExtractor.__call__",parameters:[{name:"images",val:""},{name:"segmentation_maps",val:" = None"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/image_processing_beit.py#L307"}}),et=new B({props:{name:"post_process_semantic_segmentation",anchor:"transformers.BeitFeatureExtractor.post_process_semantic_segmentation",parameters:[{name:"outputs",val:""},{name:"target_sizes",val:": List = None"}],parametersDescription:[{anchor:"transformers.BeitFeatureExtractor.post_process_semantic_segmentation.outputs",description:`<strong>outputs</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.BeitFeatureExtractor.post_process_semantic_segmentation.target_sizes",description:`<strong>target_sizes</strong> (<code>List[Tuple]</code> of length <code>batch_size</code>, <em>optional</em>) &#x2014;
List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,
predictions will not be resized.`,name:"target_sizes"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/image_processing_beit.py#L464",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[torch.Tensor]</code> of length <code>batch_size</code>, where each item is a semantic
segmentation map of shape (height, width) corresponding to the target_sizes entry (if <code>target_sizes</code> is
specified). Each entry of each <code>torch.Tensor</code> correspond to a semantic class id.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>semantic_segmentation</p>
`}}),tt=new I({props:{title:"BeitImageProcessor",local:"transformers.BeitImageProcessor",headingTag:"h2"}}),ot=new B({props:{name:"class transformers.BeitImageProcessor",anchor:"transformers.BeitImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": Dict = None"},{name:"rescale_factor",val:": Union = 0.00392156862745098"},{name:"do_rescale",val:": bool = True"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"do_reduce_labels",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BeitImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.BeitImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 256, &quot;width&quot;: 256}</code>):
Size of the output image after resizing. Can be overridden by the <code>size</code> parameter in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.BeitImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by the <code>resample</code> parameter in the
<code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.BeitImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the image. If the input size is smaller than <code>crop_size</code> along any edge, the image
is padded with 0&#x2019;s and then center cropped. Can be overridden by the <code>do_center_crop</code> parameter in the
<code>preprocess</code> method.`,name:"do_center_crop"},{anchor:"transformers.BeitImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Desired output size when applying center-cropping. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.
Can be overridden by the <code>crop_size</code> parameter in the <code>preprocess</code> method.`,name:"crop_size"},{anchor:"transformers.BeitImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in the
<code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.BeitImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.BeitImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method.`,name:"do_normalize"},{anchor:"transformers.BeitImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
The mean to use if normalizing the image. This is a float or list of floats of length of the number of
channels of the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.BeitImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
The standard deviation to use if normalizing the image. This is a float or list of floats of length of the
number of channels of the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.BeitImageProcessor.do_reduce_labels",description:`<strong>do_reduce_labels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0 is
used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k). The
background label will be replaced by 255. Can be overridden by the <code>do_reduce_labels</code> parameter in the
<code>preprocess</code> method.`,name:"do_reduce_labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/image_processing_beit.py#L49"}}),nt=new B({props:{name:"preprocess",anchor:"transformers.BeitImageProcessor.preprocess",parameters:[{name:"images",val:": Union"},{name:"segmentation_maps",val:": Union = None"},{name:"do_resize",val:": bool = None"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = None"},{name:"do_center_crop",val:": bool = None"},{name:"crop_size",val:": Dict = None"},{name:"do_rescale",val:": bool = None"},{name:"rescale_factor",val:": float = None"},{name:"do_normalize",val:": bool = None"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"do_reduce_labels",val:": Optional = None"},{name:"return_tensors",val:": Union = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BeitImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.BeitImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.BeitImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image after resizing.`,name:"size"},{anchor:"transformers.BeitImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>, Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.BeitImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.BeitImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the image after center crop. If one edge the image is smaller than <code>crop_size</code>, it will be
padded with zeros and then cropped`,name:"crop_size"},{anchor:"transformers.BeitImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.BeitImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.BeitImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.BeitImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.BeitImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.BeitImageProcessor.preprocess.do_reduce_labels",description:`<strong>do_reduce_labels</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_reduce_labels</code>) &#x2014;
Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0
is used for background, and background itself is not included in all classes of a dataset (e.g.
ADE20k). The background label will be replaced by 255.`,name:"do_reduce_labels"},{anchor:"transformers.BeitImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.BeitImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.BeitImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/image_processing_beit.py#L312"}}),at=new B({props:{name:"post_process_semantic_segmentation",anchor:"transformers.BeitImageProcessor.post_process_semantic_segmentation",parameters:[{name:"outputs",val:""},{name:"target_sizes",val:": List = None"}],parametersDescription:[{anchor:"transformers.BeitImageProcessor.post_process_semantic_segmentation.outputs",description:`<strong>outputs</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a>) &#x2014;
Raw outputs of the model.`,name:"outputs"},{anchor:"transformers.BeitImageProcessor.post_process_semantic_segmentation.target_sizes",description:`<strong>target_sizes</strong> (<code>List[Tuple]</code> of length <code>batch_size</code>, <em>optional</em>) &#x2014;
List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,
predictions will not be resized.`,name:"target_sizes"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/image_processing_beit.py#L464",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[torch.Tensor]</code> of length <code>batch_size</code>, where each item is a semantic
segmentation map of shape (height, width) corresponding to the target_sizes entry (if <code>target_sizes</code> is
specified). Each entry of each <code>torch.Tensor</code> correspond to a semantic class id.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>semantic_segmentation</p>
`}}),st=new I({props:{title:"BeitModel",local:"transformers.BeitModel",headingTag:"h2"}}),rt=new B({props:{name:"class transformers.BeitModel",anchor:"transformers.BeitModel",parameters:[{name:"config",val:": BeitConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.BeitModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L620"}}),it=new B({props:{name:"forward",anchor:"transformers.BeitModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BeitModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitFeatureExtractor.__call__">BeitImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BeitModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BeitModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BeitModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BeitModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BeitModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>, <em>optional</em>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L651",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling"
>transformers.models.beit.modeling_beit.BeitModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig"
>BeitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Average of the last layer hidden states of the patch tokens (excluding the <em>[CLS]</em> token) if
<em>config.use_mean_pooling</em> is set to True. If set to False, then the final hidden state of the <em>[CLS]</em> token
will be returned.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.models.beit.modeling_beit.BeitModelOutputWithPooling"
>transformers.models.beit.modeling_beit.BeitModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pe=new Jt({props:{$$slots:{default:[gs]},$$scope:{ctx:v}}}),he=new Ie({props:{anchor:"transformers.BeitModel.forward.example",$$slots:{default:[us]},$$scope:{ctx:v}}}),lt=new I({props:{title:"BeitForMaskedImageModeling",local:"transformers.BeitForMaskedImageModeling",headingTag:"h2"}}),dt=new B({props:{name:"class transformers.BeitForMaskedImageModeling",anchor:"transformers.BeitForMaskedImageModeling",parameters:[{name:"config",val:": BeitConfig"}],parametersDescription:[{anchor:"transformers.BeitForMaskedImageModeling.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L732"}}),ct=new B({props:{name:"forward",anchor:"transformers.BeitForMaskedImageModeling.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BeitForMaskedImageModeling.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitFeatureExtractor.__call__">BeitImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BeitForMaskedImageModeling.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BeitForMaskedImageModeling.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BeitForMaskedImageModeling.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BeitForMaskedImageModeling.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BeitForMaskedImageModeling.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"},{anchor:"transformers.BeitForMaskedImageModeling.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L753",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig"
>BeitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new Jt({props:{$$slots:{default:[_s]},$$scope:{ctx:v}}}),ge=new Ie({props:{anchor:"transformers.BeitForMaskedImageModeling.forward.example",$$slots:{default:[bs]},$$scope:{ctx:v}}}),mt=new I({props:{title:"BeitForImageClassification",local:"transformers.BeitForImageClassification",headingTag:"h2"}}),pt=new B({props:{name:"class transformers.BeitForImageClassification",anchor:"transformers.BeitForImageClassification",parameters:[{name:"config",val:": BeitConfig"}],parametersDescription:[{anchor:"transformers.BeitForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L832"}}),ht=new B({props:{name:"forward",anchor:"transformers.BeitForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BeitForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitFeatureExtractor.__call__">BeitImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BeitForImageClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BeitForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BeitForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BeitForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BeitForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L852",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig"
>BeitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ue=new Jt({props:{$$slots:{default:[ys]},$$scope:{ctx:v}}}),_e=new Ie({props:{anchor:"transformers.BeitForImageClassification.forward.example",$$slots:{default:[Ms]},$$scope:{ctx:v}}}),ft=new I({props:{title:"BeitForSemanticSegmentation",local:"transformers.BeitForSemanticSegmentation",headingTag:"h2"}}),gt=new B({props:{name:"class transformers.BeitForSemanticSegmentation",anchor:"transformers.BeitForSemanticSegmentation",parameters:[{name:"config",val:": BeitConfig"}],parametersDescription:[{anchor:"transformers.BeitForSemanticSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L1156"}}),ut=new B({props:{name:"forward",anchor:"transformers.BeitForSemanticSegmentation.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BeitForSemanticSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitFeatureExtractor.__call__">BeitImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BeitForSemanticSegmentation.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BeitForSemanticSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BeitForSemanticSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BeitForSemanticSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BeitForSemanticSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Ground truth semantic segmentation maps for computing the loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels &gt; 1</code>, a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_beit.py#L1214",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig"
>BeitConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels, logits_height, logits_width)</code>) — Classification scores for each pixel.</p>
<Tip warning={true}>
<p>The logits returned do not necessarily have the same size as the <code>pixel_values</code> passed as inputs. This is
to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the
original image size as post-processing. You should always check your logits shape and resize as needed.</p>
</Tip>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, patch_size, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.SemanticSegmenterOutput"
>transformers.modeling_outputs.SemanticSegmenterOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),be=new Jt({props:{$$slots:{default:[Ts]},$$scope:{ctx:v}}}),ye=new Ie({props:{anchor:"transformers.BeitForSemanticSegmentation.forward.example",$$slots:{default:[ws]},$$scope:{ctx:v}}}),_t=new I({props:{title:"FlaxBeitModel",local:"transformers.FlaxBeitModel",headingTag:"h2"}}),bt=new B({props:{name:"class transformers.FlaxBeitModel",anchor:"transformers.FlaxBeitModel",parameters:[{name:"config",val:": BeitConfig"},{name:"input_shape",val:" = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxBeitModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxBeitModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L741"}}),yt=new B({props:{name:"__call__",anchor:"transformers.FlaxBeitModel.__call__",parameters:[{name:"pixel_values",val:""},{name:"bool_masked_pos",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L632",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling"
>transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.beit.configuration_beit.BeitConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</li>
<li><strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) — Average of the last layer hidden states of the patch tokens (excluding the <em>[CLS]</em> token) if
<em>config.use_mean_pooling</em> is set to True. If set to False, then the final hidden state of the <em>[CLS]</em> token
will be returned.</li>
<li><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of each layer plus
the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the weighted average in
the self-attention heads.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/beit#transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling"
>transformers.models.beit.modeling_flax_beit.FlaxBeitModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Me=new Jt({props:{$$slots:{default:[vs]},$$scope:{ctx:v}}}),Te=new Ie({props:{anchor:"transformers.FlaxBeitModel.__call__.example",$$slots:{default:[xs]},$$scope:{ctx:v}}}),Mt=new I({props:{title:"FlaxBeitForMaskedImageModeling",local:"transformers.FlaxBeitForMaskedImageModeling",headingTag:"h2"}}),Tt=new B({props:{name:"class transformers.FlaxBeitForMaskedImageModeling",anchor:"transformers.FlaxBeitForMaskedImageModeling",parameters:[{name:"config",val:": BeitConfig"},{name:"input_shape",val:" = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxBeitForMaskedImageModeling.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxBeitForMaskedImageModeling.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L825"}}),wt=new B({props:{name:"__call__",anchor:"transformers.FlaxBeitForMaskedImageModeling.__call__",parameters:[{name:"pixel_values",val:""},{name:"bool_masked_pos",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L632",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.beit.configuration_beit.BeitConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxMaskedLMOutput"
>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),we=new Jt({props:{$$slots:{default:[$s]},$$scope:{ctx:v}}}),ve=new Ie({props:{anchor:"transformers.FlaxBeitForMaskedImageModeling.__call__.example",$$slots:{default:[Bs]},$$scope:{ctx:v}}}),vt=new I({props:{title:"FlaxBeitForImageClassification",local:"transformers.FlaxBeitForImageClassification",headingTag:"h2"}}),xt=new B({props:{name:"class transformers.FlaxBeitForImageClassification",anchor:"transformers.FlaxBeitForImageClassification",parameters:[{name:"config",val:": BeitConfig"},{name:"input_shape",val:" = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxBeitForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxBeitForImageClassification.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L909"}}),$t=new B({props:{name:"__call__",anchor:"transformers.FlaxBeitForImageClassification.__call__",parameters:[{name:"pixel_values",val:""},{name:"bool_masked_pos",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/beit/modeling_flax_beit.py#L632",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.beit.configuration_beit.BeitConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>logits</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput"
>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),xe=new Jt({props:{$$slots:{default:[js]},$$scope:{ctx:v}}}),$e=new Ie({props:{anchor:"transformers.FlaxBeitForImageClassification.__call__.example",$$slots:{default:[Cs]},$$scope:{ctx:v}}}),{c(){t=d("meta"),M=s(),m=d("p"),p=s(),h(T.$$.fragment),o=s(),h(w.$$.fragment),bo=s(),ke=d("p"),ke.innerHTML=ha,yo=s(),ze=d("p"),ze.textContent=fa,Mo=s(),We=d("p"),We.innerHTML=ga,To=s(),h(Ue.$$.fragment),wo=s(),Ze=d("ul"),Ze.innerHTML=ua,vo=s(),ie=d("img"),xo=s(),Ne=d("small"),Ne.innerHTML=ba,$o=s(),Pe=d("p"),Pe.innerHTML=ya,Bo=s(),h(Ee.$$.fragment),jo=s(),Le=d("p"),Le.textContent=Ma,Co=s(),h(Re.$$.fragment),Fo=s(),Se=d("ul"),Se.innerHTML=Ta,Io=s(),Ve=d("p"),Ve.innerHTML=wa,Jo=s(),Xe=d("ul"),Xe.innerHTML=va,ko=s(),Ge=d("p"),Ge.textContent=xa,zo=s(),h(He.$$.fragment),Wo=s(),te=d("div"),h(qe.$$.fragment),an=s(),kt=d("p"),kt.innerHTML=$a,Uo=s(),oe=d("div"),h(Ye.$$.fragment),sn=s(),zt=d("p"),zt.innerHTML=Ba,Zo=s(),h(Qe.$$.fragment),No=s(),G=d("div"),h(De.$$.fragment),rn=s(),Wt=d("p"),Wt.innerHTML=ja,ln=s(),h(le.$$.fragment),Po=s(),h(Ae.$$.fragment),Eo=s(),H=d("div"),h(Oe.$$.fragment),dn=s(),Ut=d("div"),h(Ke.$$.fragment),cn=s(),de=d("div"),h(et.$$.fragment),mn=s(),Zt=d("p"),Zt.innerHTML=Ca,Lo=s(),h(tt.$$.fragment),Ro=s(),Z=d("div"),h(ot.$$.fragment),pn=s(),Nt=d("p"),Nt.textContent=Fa,hn=s(),ce=d("div"),h(nt.$$.fragment),fn=s(),Pt=d("p"),Pt.textContent=Ia,gn=s(),me=d("div"),h(at.$$.fragment),un=s(),Et=d("p"),Et.innerHTML=Ja,So=s(),h(st.$$.fragment),Vo=s(),q=d("div"),h(rt.$$.fragment),_n=s(),Lt=d("p"),Lt.innerHTML=ka,bn=s(),E=d("div"),h(it.$$.fragment),yn=s(),Rt=d("p"),Rt.innerHTML=za,Mn=s(),h(pe.$$.fragment),Tn=s(),h(he.$$.fragment),Xo=s(),h(lt.$$.fragment),Go=s(),Y=d("div"),h(dt.$$.fragment),wn=s(),St=d("p"),St.innerHTML=Wa,vn=s(),L=d("div"),h(ct.$$.fragment),xn=s(),Vt=d("p"),Vt.innerHTML=Ua,$n=s(),h(fe.$$.fragment),Bn=s(),h(ge.$$.fragment),Ho=s(),h(mt.$$.fragment),qo=s(),N=d("div"),h(pt.$$.fragment),jn=s(),Xt=d("p"),Xt.textContent=Za,Cn=s(),Gt=d("p"),Gt.innerHTML=Na,Fn=s(),R=d("div"),h(ht.$$.fragment),In=s(),Ht=d("p"),Ht.innerHTML=Pa,Jn=s(),h(ue.$$.fragment),kn=s(),h(_e.$$.fragment),Yo=s(),h(ft.$$.fragment),Qo=s(),P=d("div"),h(gt.$$.fragment),zn=s(),qt=d("p"),qt.textContent=Ea,Wn=s(),Yt=d("p"),Yt.innerHTML=La,Un=s(),S=d("div"),h(ut.$$.fragment),Zn=s(),Qt=d("p"),Qt.innerHTML=Ra,Nn=s(),h(be.$$.fragment),Pn=s(),h(ye.$$.fragment),Do=s(),h(_t.$$.fragment),Ao=s(),j=d("div"),h(bt.$$.fragment),En=s(),Dt=d("p"),Dt.textContent=Sa,Ln=s(),At=d("p"),At.innerHTML=Va,Rn=s(),Ot=d("p"),Ot.innerHTML=Xa,Sn=s(),Kt=d("p"),Kt.textContent=Ga,Vn=s(),eo=d("ul"),eo.innerHTML=Ha,Xn=s(),V=d("div"),h(yt.$$.fragment),Gn=s(),to=d("p"),to.innerHTML=qa,Hn=s(),h(Me.$$.fragment),qn=s(),h(Te.$$.fragment),Oo=s(),h(Mt.$$.fragment),Ko=s(),C=d("div"),h(Tt.$$.fragment),Yn=s(),oo=d("p"),oo.textContent=Ya,Qn=s(),no=d("p"),no.innerHTML=Qa,Dn=s(),ao=d("p"),ao.innerHTML=Da,An=s(),so=d("p"),so.textContent=Aa,On=s(),ro=d("ul"),ro.innerHTML=Oa,Kn=s(),k=d("div"),h(wt.$$.fragment),ea=s(),io=d("p"),io.innerHTML=Ka,ta=s(),h(we.$$.fragment),oa=s(),lo=d("p"),lo.innerHTML=es,na=s(),h(ve.$$.fragment),en=s(),h(vt.$$.fragment),tn=s(),F=d("div"),h(xt.$$.fragment),aa=s(),co=d("p"),co.textContent=ts,sa=s(),mo=d("p"),mo.innerHTML=os,ra=s(),po=d("p"),po.innerHTML=ns,ia=s(),ho=d("p"),ho.textContent=as,la=s(),fo=d("ul"),fo.innerHTML=ss,da=s(),X=d("div"),h($t.$$.fragment),ca=s(),go=d("p"),go.innerHTML=rs,ma=s(),h(xe.$$.fragment),pa=s(),h($e.$$.fragment),on=s(),uo=d("p"),this.h()},l(e){const n=ps("svelte-u9bgzb",document.head);t=c(n,"META",{name:!0,content:!0}),n.forEach(a),M=r(e),m=c(e,"P",{}),$(m).forEach(a),p=r(e),f(T.$$.fragment,e),o=r(e),f(w.$$.fragment,e),bo=r(e),ke=c(e,"P",{"data-svelte-h":!0}),y(ke)!=="svelte-1hvidxz"&&(ke.innerHTML=ha),yo=r(e),ze=c(e,"P",{"data-svelte-h":!0}),y(ze)!=="svelte-1cv3nri"&&(ze.textContent=fa),Mo=r(e),We=c(e,"P",{"data-svelte-h":!0}),y(We)!=="svelte-sr2vy6"&&(We.innerHTML=ga),To=r(e),f(Ue.$$.fragment,e),wo=r(e),Ze=c(e,"UL",{"data-svelte-h":!0}),y(Ze)!=="svelte-y3jyj7"&&(Ze.innerHTML=ua),vo=r(e),ie=c(e,"IMG",{src:!0,alt:!0,width:!0}),xo=r(e),Ne=c(e,"SMALL",{"data-svelte-h":!0}),y(Ne)!=="svelte-1swelvj"&&(Ne.innerHTML=ba),$o=r(e),Pe=c(e,"P",{"data-svelte-h":!0}),y(Pe)!=="svelte-16tj0i7"&&(Pe.innerHTML=ya),Bo=r(e),f(Ee.$$.fragment,e),jo=r(e),Le=c(e,"P",{"data-svelte-h":!0}),y(Le)!=="svelte-12jbpwc"&&(Le.textContent=Ma),Co=r(e),f(Re.$$.fragment,e),Fo=r(e),Se=c(e,"UL",{"data-svelte-h":!0}),y(Se)!=="svelte-1x9g5qd"&&(Se.innerHTML=Ta),Io=r(e),Ve=c(e,"P",{"data-svelte-h":!0}),y(Ve)!=="svelte-oevypi"&&(Ve.innerHTML=wa),Jo=r(e),Xe=c(e,"UL",{"data-svelte-h":!0}),y(Xe)!=="svelte-jacek8"&&(Xe.innerHTML=va),ko=r(e),Ge=c(e,"P",{"data-svelte-h":!0}),y(Ge)!=="svelte-17ytafw"&&(Ge.textContent=xa),zo=r(e),f(He.$$.fragment,e),Wo=r(e),te=c(e,"DIV",{class:!0});var Bt=$(te);f(qe.$$.fragment,Bt),an=r(Bt),kt=c(Bt,"P",{"data-svelte-h":!0}),y(kt)!=="svelte-1mesiz"&&(kt.innerHTML=$a),Bt.forEach(a),Uo=r(e),oe=c(e,"DIV",{class:!0});var jt=$(oe);f(Ye.$$.fragment,jt),sn=r(jt),zt=c(jt,"P",{"data-svelte-h":!0}),y(zt)!=="svelte-zxi4gf"&&(zt.innerHTML=Ba),jt.forEach(a),Zo=r(e),f(Qe.$$.fragment,e),No=r(e),G=c(e,"DIV",{class:!0});var ne=$(G);f(De.$$.fragment,ne),rn=r(ne),Wt=c(ne,"P",{"data-svelte-h":!0}),y(Wt)!=="svelte-h4bfyh"&&(Wt.innerHTML=ja),ln=r(ne),f(le.$$.fragment,ne),ne.forEach(a),Po=r(e),f(Ae.$$.fragment,e),Eo=r(e),H=c(e,"DIV",{class:!0});var ae=$(H);f(Oe.$$.fragment,ae),dn=r(ae),Ut=c(ae,"DIV",{class:!0});var _o=$(Ut);f(Ke.$$.fragment,_o),_o.forEach(a),cn=r(ae),de=c(ae,"DIV",{class:!0});var Ct=$(de);f(et.$$.fragment,Ct),mn=r(Ct),Zt=c(Ct,"P",{"data-svelte-h":!0}),y(Zt)!=="svelte-8uar9g"&&(Zt.innerHTML=Ca),Ct.forEach(a),ae.forEach(a),Lo=r(e),f(tt.$$.fragment,e),Ro=r(e),Z=c(e,"DIV",{class:!0});var Q=$(Z);f(ot.$$.fragment,Q),pn=r(Q),Nt=c(Q,"P",{"data-svelte-h":!0}),y(Nt)!=="svelte-ky7k50"&&(Nt.textContent=Fa),hn=r(Q),ce=c(Q,"DIV",{class:!0});var Ft=$(ce);f(nt.$$.fragment,Ft),fn=r(Ft),Pt=c(Ft,"P",{"data-svelte-h":!0}),y(Pt)!=="svelte-1x3yxsa"&&(Pt.textContent=Ia),Ft.forEach(a),gn=r(Q),me=c(Q,"DIV",{class:!0});var It=$(me);f(at.$$.fragment,It),un=r(It),Et=c(It,"P",{"data-svelte-h":!0}),y(Et)!=="svelte-8uar9g"&&(Et.innerHTML=Ja),It.forEach(a),Q.forEach(a),So=r(e),f(st.$$.fragment,e),Vo=r(e),q=c(e,"DIV",{class:!0});var se=$(q);f(rt.$$.fragment,se),_n=r(se),Lt=c(se,"P",{"data-svelte-h":!0}),y(Lt)!=="svelte-1wah8ut"&&(Lt.innerHTML=ka),bn=r(se),E=c(se,"DIV",{class:!0});var D=$(E);f(it.$$.fragment,D),yn=r(D),Rt=c(D,"P",{"data-svelte-h":!0}),y(Rt)!=="svelte-1l2zdux"&&(Rt.innerHTML=za),Mn=r(D),f(pe.$$.fragment,D),Tn=r(D),f(he.$$.fragment,D),D.forEach(a),se.forEach(a),Xo=r(e),f(lt.$$.fragment,e),Go=r(e),Y=c(e,"DIV",{class:!0});var re=$(Y);f(dt.$$.fragment,re),wn=r(re),St=c(re,"P",{"data-svelte-h":!0}),y(St)!=="svelte-1z0z0yr"&&(St.innerHTML=Wa),vn=r(re),L=c(re,"DIV",{class:!0});var A=$(L);f(ct.$$.fragment,A),xn=r(A),Vt=c(A,"P",{"data-svelte-h":!0}),y(Vt)!=="svelte-1sm3wk1"&&(Vt.innerHTML=Ua),$n=r(A),f(fe.$$.fragment,A),Bn=r(A),f(ge.$$.fragment,A),A.forEach(a),re.forEach(a),Ho=r(e),f(mt.$$.fragment,e),qo=r(e),N=c(e,"DIV",{class:!0});var O=$(N);f(pt.$$.fragment,O),jn=r(O),Xt=c(O,"P",{"data-svelte-h":!0}),y(Xt)!=="svelte-83lcss"&&(Xt.textContent=Za),Cn=r(O),Gt=c(O,"P",{"data-svelte-h":!0}),y(Gt)!=="svelte-1gjh92c"&&(Gt.innerHTML=Na),Fn=r(O),R=c(O,"DIV",{class:!0});var K=$(R);f(ht.$$.fragment,K),In=r(K),Ht=c(K,"P",{"data-svelte-h":!0}),y(Ht)!=="svelte-9xqli1"&&(Ht.innerHTML=Pa),Jn=r(K),f(ue.$$.fragment,K),kn=r(K),f(_e.$$.fragment,K),K.forEach(a),O.forEach(a),Yo=r(e),f(ft.$$.fragment,e),Qo=r(e),P=c(e,"DIV",{class:!0});var Be=$(P);f(gt.$$.fragment,Be),zn=r(Be),qt=c(Be,"P",{"data-svelte-h":!0}),y(qt)!=="svelte-olwnwh"&&(qt.textContent=Ea),Wn=r(Be),Yt=c(Be,"P",{"data-svelte-h":!0}),y(Yt)!=="svelte-1gjh92c"&&(Yt.innerHTML=La),Un=r(Be),S=c(Be,"DIV",{class:!0});var je=$(S);f(ut.$$.fragment,je),Zn=r(je),Qt=c(je,"P",{"data-svelte-h":!0}),y(Qt)!=="svelte-hxuxnh"&&(Qt.innerHTML=Ra),Nn=r(je),f(be.$$.fragment,je),Pn=r(je),f(ye.$$.fragment,je),je.forEach(a),Be.forEach(a),Do=r(e),f(_t.$$.fragment,e),Ao=r(e),j=c(e,"DIV",{class:!0});var z=$(j);f(bt.$$.fragment,z),En=r(z),Dt=c(z,"P",{"data-svelte-h":!0}),y(Dt)!=="svelte-yakhoj"&&(Dt.textContent=Sa),Ln=r(z),At=c(z,"P",{"data-svelte-h":!0}),y(At)!=="svelte-99cpmj"&&(At.innerHTML=Va),Rn=r(z),Ot=c(z,"P",{"data-svelte-h":!0}),y(Ot)!=="svelte-10nfsf3"&&(Ot.innerHTML=Xa),Sn=r(z),Kt=c(z,"P",{"data-svelte-h":!0}),y(Kt)!=="svelte-1pplc4a"&&(Kt.textContent=Ga),Vn=r(z),eo=c(z,"UL",{"data-svelte-h":!0}),y(eo)!=="svelte-1w7z84m"&&(eo.innerHTML=Ha),Xn=r(z),V=c(z,"DIV",{class:!0});var Ce=$(V);f(yt.$$.fragment,Ce),Gn=r(Ce),to=c(Ce,"P",{"data-svelte-h":!0}),y(to)!=="svelte-15yd1qb"&&(to.innerHTML=qa),Hn=r(Ce),f(Me.$$.fragment,Ce),qn=r(Ce),f(Te.$$.fragment,Ce),Ce.forEach(a),z.forEach(a),Oo=r(e),f(Mt.$$.fragment,e),Ko=r(e),C=c(e,"DIV",{class:!0});var W=$(C);f(Tt.$$.fragment,W),Yn=r(W),oo=c(W,"P",{"data-svelte-h":!0}),y(oo)!=="svelte-1xpbgj8"&&(oo.textContent=Ya),Qn=r(W),no=c(W,"P",{"data-svelte-h":!0}),y(no)!=="svelte-99cpmj"&&(no.innerHTML=Qa),Dn=r(W),ao=c(W,"P",{"data-svelte-h":!0}),y(ao)!=="svelte-10nfsf3"&&(ao.innerHTML=Da),An=r(W),so=c(W,"P",{"data-svelte-h":!0}),y(so)!=="svelte-1pplc4a"&&(so.textContent=Aa),On=r(W),ro=c(W,"UL",{"data-svelte-h":!0}),y(ro)!=="svelte-1w7z84m"&&(ro.innerHTML=Oa),Kn=r(W),k=c(W,"DIV",{class:!0});var ee=$(k);f(wt.$$.fragment,ee),ea=r(ee),io=c(ee,"P",{"data-svelte-h":!0}),y(io)!=="svelte-15yd1qb"&&(io.innerHTML=Ka),ta=r(ee),f(we.$$.fragment,ee),oa=r(ee),lo=c(ee,"P",{"data-svelte-h":!0}),y(lo)!=="svelte-1rn5vq7"&&(lo.innerHTML=es),na=r(ee),f(ve.$$.fragment,ee),ee.forEach(a),W.forEach(a),en=r(e),f(vt.$$.fragment,e),tn=r(e),F=c(e,"DIV",{class:!0});var U=$(F);f(xt.$$.fragment,U),aa=r(U),co=c(U,"P",{"data-svelte-h":!0}),y(co)!=="svelte-83lcss"&&(co.textContent=ts),sa=r(U),mo=c(U,"P",{"data-svelte-h":!0}),y(mo)!=="svelte-99cpmj"&&(mo.innerHTML=os),ra=r(U),po=c(U,"P",{"data-svelte-h":!0}),y(po)!=="svelte-10nfsf3"&&(po.innerHTML=ns),ia=r(U),ho=c(U,"P",{"data-svelte-h":!0}),y(ho)!=="svelte-1pplc4a"&&(ho.textContent=as),la=r(U),fo=c(U,"UL",{"data-svelte-h":!0}),y(fo)!=="svelte-1w7z84m"&&(fo.innerHTML=ss),da=r(U),X=c(U,"DIV",{class:!0});var Fe=$(X);f($t.$$.fragment,Fe),ca=r(Fe),go=c(Fe,"P",{"data-svelte-h":!0}),y(go)!=="svelte-15yd1qb"&&(go.innerHTML=rs),ma=r(Fe),f(xe.$$.fragment,Fe),pa=r(Fe),f($e.$$.fragment,Fe),Fe.forEach(a),U.forEach(a),on=r(e),uo=c(e,"P",{}),$(uo).forEach(a),this.h()},h(){x(t,"name","hf:doc:metadata"),x(t,"content",Is),ls(ie.src,_a="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/beit_architecture.jpg")||x(ie,"src",_a),x(ie,"alt","drawing"),x(ie,"width","600"),x(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){i(document.head,t),l(e,M,n),l(e,m,n),l(e,p,n),g(T,e,n),l(e,o,n),g(w,e,n),l(e,bo,n),l(e,ke,n),l(e,yo,n),l(e,ze,n),l(e,Mo,n),l(e,We,n),l(e,To,n),g(Ue,e,n),l(e,wo,n),l(e,Ze,n),l(e,vo,n),l(e,ie,n),l(e,xo,n),l(e,Ne,n),l(e,$o,n),l(e,Pe,n),l(e,Bo,n),g(Ee,e,n),l(e,jo,n),l(e,Le,n),l(e,Co,n),g(Re,e,n),l(e,Fo,n),l(e,Se,n),l(e,Io,n),l(e,Ve,n),l(e,Jo,n),l(e,Xe,n),l(e,ko,n),l(e,Ge,n),l(e,zo,n),g(He,e,n),l(e,Wo,n),l(e,te,n),g(qe,te,null),i(te,an),i(te,kt),l(e,Uo,n),l(e,oe,n),g(Ye,oe,null),i(oe,sn),i(oe,zt),l(e,Zo,n),g(Qe,e,n),l(e,No,n),l(e,G,n),g(De,G,null),i(G,rn),i(G,Wt),i(G,ln),g(le,G,null),l(e,Po,n),g(Ae,e,n),l(e,Eo,n),l(e,H,n),g(Oe,H,null),i(H,dn),i(H,Ut),g(Ke,Ut,null),i(H,cn),i(H,de),g(et,de,null),i(de,mn),i(de,Zt),l(e,Lo,n),g(tt,e,n),l(e,Ro,n),l(e,Z,n),g(ot,Z,null),i(Z,pn),i(Z,Nt),i(Z,hn),i(Z,ce),g(nt,ce,null),i(ce,fn),i(ce,Pt),i(Z,gn),i(Z,me),g(at,me,null),i(me,un),i(me,Et),l(e,So,n),g(st,e,n),l(e,Vo,n),l(e,q,n),g(rt,q,null),i(q,_n),i(q,Lt),i(q,bn),i(q,E),g(it,E,null),i(E,yn),i(E,Rt),i(E,Mn),g(pe,E,null),i(E,Tn),g(he,E,null),l(e,Xo,n),g(lt,e,n),l(e,Go,n),l(e,Y,n),g(dt,Y,null),i(Y,wn),i(Y,St),i(Y,vn),i(Y,L),g(ct,L,null),i(L,xn),i(L,Vt),i(L,$n),g(fe,L,null),i(L,Bn),g(ge,L,null),l(e,Ho,n),g(mt,e,n),l(e,qo,n),l(e,N,n),g(pt,N,null),i(N,jn),i(N,Xt),i(N,Cn),i(N,Gt),i(N,Fn),i(N,R),g(ht,R,null),i(R,In),i(R,Ht),i(R,Jn),g(ue,R,null),i(R,kn),g(_e,R,null),l(e,Yo,n),g(ft,e,n),l(e,Qo,n),l(e,P,n),g(gt,P,null),i(P,zn),i(P,qt),i(P,Wn),i(P,Yt),i(P,Un),i(P,S),g(ut,S,null),i(S,Zn),i(S,Qt),i(S,Nn),g(be,S,null),i(S,Pn),g(ye,S,null),l(e,Do,n),g(_t,e,n),l(e,Ao,n),l(e,j,n),g(bt,j,null),i(j,En),i(j,Dt),i(j,Ln),i(j,At),i(j,Rn),i(j,Ot),i(j,Sn),i(j,Kt),i(j,Vn),i(j,eo),i(j,Xn),i(j,V),g(yt,V,null),i(V,Gn),i(V,to),i(V,Hn),g(Me,V,null),i(V,qn),g(Te,V,null),l(e,Oo,n),g(Mt,e,n),l(e,Ko,n),l(e,C,n),g(Tt,C,null),i(C,Yn),i(C,oo),i(C,Qn),i(C,no),i(C,Dn),i(C,ao),i(C,An),i(C,so),i(C,On),i(C,ro),i(C,Kn),i(C,k),g(wt,k,null),i(k,ea),i(k,io),i(k,ta),g(we,k,null),i(k,oa),i(k,lo),i(k,na),g(ve,k,null),l(e,en,n),g(vt,e,n),l(e,tn,n),l(e,F,n),g(xt,F,null),i(F,aa),i(F,co),i(F,sa),i(F,mo),i(F,ra),i(F,po),i(F,ia),i(F,ho),i(F,la),i(F,fo),i(F,da),i(F,X),g($t,X,null),i(X,ca),i(X,go),i(X,ma),g(xe,X,null),i(X,pa),g($e,X,null),l(e,on,n),l(e,uo,n),nn=!0},p(e,[n]){const Bt={};n&2&&(Bt.$$scope={dirty:n,ctx:e}),le.$set(Bt);const jt={};n&2&&(jt.$$scope={dirty:n,ctx:e}),pe.$set(jt);const ne={};n&2&&(ne.$$scope={dirty:n,ctx:e}),he.$set(ne);const ae={};n&2&&(ae.$$scope={dirty:n,ctx:e}),fe.$set(ae);const _o={};n&2&&(_o.$$scope={dirty:n,ctx:e}),ge.$set(_o);const Ct={};n&2&&(Ct.$$scope={dirty:n,ctx:e}),ue.$set(Ct);const Q={};n&2&&(Q.$$scope={dirty:n,ctx:e}),_e.$set(Q);const Ft={};n&2&&(Ft.$$scope={dirty:n,ctx:e}),be.$set(Ft);const It={};n&2&&(It.$$scope={dirty:n,ctx:e}),ye.$set(It);const se={};n&2&&(se.$$scope={dirty:n,ctx:e}),Me.$set(se);const D={};n&2&&(D.$$scope={dirty:n,ctx:e}),Te.$set(D);const re={};n&2&&(re.$$scope={dirty:n,ctx:e}),we.$set(re);const A={};n&2&&(A.$$scope={dirty:n,ctx:e}),ve.$set(A);const O={};n&2&&(O.$$scope={dirty:n,ctx:e}),xe.$set(O);const K={};n&2&&(K.$$scope={dirty:n,ctx:e}),$e.$set(K)},i(e){nn||(u(T.$$.fragment,e),u(w.$$.fragment,e),u(Ue.$$.fragment,e),u(Ee.$$.fragment,e),u(Re.$$.fragment,e),u(He.$$.fragment,e),u(qe.$$.fragment,e),u(Ye.$$.fragment,e),u(Qe.$$.fragment,e),u(De.$$.fragment,e),u(le.$$.fragment,e),u(Ae.$$.fragment,e),u(Oe.$$.fragment,e),u(Ke.$$.fragment,e),u(et.$$.fragment,e),u(tt.$$.fragment,e),u(ot.$$.fragment,e),u(nt.$$.fragment,e),u(at.$$.fragment,e),u(st.$$.fragment,e),u(rt.$$.fragment,e),u(it.$$.fragment,e),u(pe.$$.fragment,e),u(he.$$.fragment,e),u(lt.$$.fragment,e),u(dt.$$.fragment,e),u(ct.$$.fragment,e),u(fe.$$.fragment,e),u(ge.$$.fragment,e),u(mt.$$.fragment,e),u(pt.$$.fragment,e),u(ht.$$.fragment,e),u(ue.$$.fragment,e),u(_e.$$.fragment,e),u(ft.$$.fragment,e),u(gt.$$.fragment,e),u(ut.$$.fragment,e),u(be.$$.fragment,e),u(ye.$$.fragment,e),u(_t.$$.fragment,e),u(bt.$$.fragment,e),u(yt.$$.fragment,e),u(Me.$$.fragment,e),u(Te.$$.fragment,e),u(Mt.$$.fragment,e),u(Tt.$$.fragment,e),u(wt.$$.fragment,e),u(we.$$.fragment,e),u(ve.$$.fragment,e),u(vt.$$.fragment,e),u(xt.$$.fragment,e),u($t.$$.fragment,e),u(xe.$$.fragment,e),u($e.$$.fragment,e),nn=!0)},o(e){_(T.$$.fragment,e),_(w.$$.fragment,e),_(Ue.$$.fragment,e),_(Ee.$$.fragment,e),_(Re.$$.fragment,e),_(He.$$.fragment,e),_(qe.$$.fragment,e),_(Ye.$$.fragment,e),_(Qe.$$.fragment,e),_(De.$$.fragment,e),_(le.$$.fragment,e),_(Ae.$$.fragment,e),_(Oe.$$.fragment,e),_(Ke.$$.fragment,e),_(et.$$.fragment,e),_(tt.$$.fragment,e),_(ot.$$.fragment,e),_(nt.$$.fragment,e),_(at.$$.fragment,e),_(st.$$.fragment,e),_(rt.$$.fragment,e),_(it.$$.fragment,e),_(pe.$$.fragment,e),_(he.$$.fragment,e),_(lt.$$.fragment,e),_(dt.$$.fragment,e),_(ct.$$.fragment,e),_(fe.$$.fragment,e),_(ge.$$.fragment,e),_(mt.$$.fragment,e),_(pt.$$.fragment,e),_(ht.$$.fragment,e),_(ue.$$.fragment,e),_(_e.$$.fragment,e),_(ft.$$.fragment,e),_(gt.$$.fragment,e),_(ut.$$.fragment,e),_(be.$$.fragment,e),_(ye.$$.fragment,e),_(_t.$$.fragment,e),_(bt.$$.fragment,e),_(yt.$$.fragment,e),_(Me.$$.fragment,e),_(Te.$$.fragment,e),_(Mt.$$.fragment,e),_(Tt.$$.fragment,e),_(wt.$$.fragment,e),_(we.$$.fragment,e),_(ve.$$.fragment,e),_(vt.$$.fragment,e),_(xt.$$.fragment,e),_($t.$$.fragment,e),_(xe.$$.fragment,e),_($e.$$.fragment,e),nn=!1},d(e){e&&(a(M),a(m),a(p),a(o),a(bo),a(ke),a(yo),a(ze),a(Mo),a(We),a(To),a(wo),a(Ze),a(vo),a(ie),a(xo),a(Ne),a($o),a(Pe),a(Bo),a(jo),a(Le),a(Co),a(Fo),a(Se),a(Io),a(Ve),a(Jo),a(Xe),a(ko),a(Ge),a(zo),a(Wo),a(te),a(Uo),a(oe),a(Zo),a(No),a(G),a(Po),a(Eo),a(H),a(Lo),a(Ro),a(Z),a(So),a(Vo),a(q),a(Xo),a(Go),a(Y),a(Ho),a(qo),a(N),a(Yo),a(Qo),a(P),a(Do),a(Ao),a(j),a(Oo),a(Ko),a(C),a(en),a(tn),a(F),a(on),a(uo)),a(t),b(T,e),b(w,e),b(Ue,e),b(Ee,e),b(Re,e),b(He,e),b(qe),b(Ye),b(Qe,e),b(De),b(le),b(Ae,e),b(Oe),b(Ke),b(et),b(tt,e),b(ot),b(nt),b(at),b(st,e),b(rt),b(it),b(pe),b(he),b(lt,e),b(dt),b(ct),b(fe),b(ge),b(mt,e),b(pt),b(ht),b(ue),b(_e),b(ft,e),b(gt),b(ut),b(be),b(ye),b(_t,e),b(bt),b(yt),b(Me),b(Te),b(Mt,e),b(Tt),b(wt),b(we),b(ve),b(vt,e),b(xt),b($t),b(xe),b($e)}}}const Is='{"title":"BEiT","local":"beit","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"BEiT specific outputs","local":"transformers.models.beit.modeling_beit.BeitModelOutputWithPooling","sections":[],"depth":2},{"title":"BeitConfig","local":"transformers.BeitConfig","sections":[],"depth":2},{"title":"BeitFeatureExtractor","local":"transformers.BeitFeatureExtractor","sections":[],"depth":2},{"title":"BeitImageProcessor","local":"transformers.BeitImageProcessor","sections":[],"depth":2},{"title":"BeitModel","local":"transformers.BeitModel","sections":[],"depth":2},{"title":"BeitForMaskedImageModeling","local":"transformers.BeitForMaskedImageModeling","sections":[],"depth":2},{"title":"BeitForImageClassification","local":"transformers.BeitForImageClassification","sections":[],"depth":2},{"title":"BeitForSemanticSegmentation","local":"transformers.BeitForSemanticSegmentation","sections":[],"depth":2},{"title":"FlaxBeitModel","local":"transformers.FlaxBeitModel","sections":[],"depth":2},{"title":"FlaxBeitForMaskedImageModeling","local":"transformers.FlaxBeitForMaskedImageModeling","sections":[],"depth":2},{"title":"FlaxBeitForImageClassification","local":"transformers.FlaxBeitForImageClassification","sections":[],"depth":2}],"depth":1}';function Js(v){return ds(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ls extends cs{constructor(t){super(),ms(this,t,Js,Fs,is,{})}}export{Ls as component};
