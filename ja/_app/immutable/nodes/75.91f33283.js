import{s as Xr,f as Sr,o as Qr,n as S}from"../chunks/scheduler.9bc65507.js";import{S as Ar,i as Yr,g as c,s,r as g,A as Dr,h as m,f as r,c as a,j as B,u as h,x as T,k as j,y as d,a as i,v as f,d as u,t as _,w}from"../chunks/index.707bf1b6.js";import{T as Tt}from"../chunks/Tip.c2ecdbf4.js";import{D as J}from"../chunks/Docstring.17db21ae.js";import{C as A}from"../chunks/CodeBlock.54a9f38d.js";import{E as lo}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as N}from"../chunks/Heading.342b1fa6.js";function Or(v){let n,y="Example:",p,l,b;return l=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJyaWRnZVRvd2VyTW9kZWwlMkMlMjBCcmlkZ2VUb3dlckNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCcmlkZ2VUb3dlciUyMEJyaWRnZVRvd2VyJTJGYnJpZGdldG93ZXItYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCcmlkZ2VUb3dlckNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwZnJvbSUyMHRoZSUyMEJyaWRnZVRvd2VyJTJGYnJpZGdldG93ZXItYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQnJpZGdlVG93ZXJNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BridgeTowerModel, BridgeTowerConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BridgeTower BridgeTower/bridgetower-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BridgeTowerConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model from the BridgeTower/bridgetower-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BridgeTowerModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=c("p"),n.textContent=y,p=s(),g(l.$$.fragment)},l(o){n=m(o,"P",{"data-svelte-h":!0}),T(n)!=="svelte-11lpom8"&&(n.textContent=y),p=a(o),h(l.$$.fragment,o)},m(o,M){i(o,n,M),i(o,p,M),f(l,o,M),b=!0},p:S,i(o){b||(u(l.$$.fragment,o),b=!0)},o(o){_(l.$$.fragment,o),b=!1},d(o){o&&(r(n),r(p)),w(l,o)}}}function Kr(v){let n,y="Example:",p,l,b;return l=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJyaWRnZVRvd2VyVGV4dENvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCcmlkZ2VUb3dlciUyMEJyaWRnZVRvd2VyJTJGYnJpZGdldG93ZXItYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUyMGZvciUyMHRoZSUyMHRleHQlMjBtb2RlbCUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCcmlkZ2VUb3dlclRleHRDb25maWcoKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9u",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BridgeTowerTextConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BridgeTower BridgeTower/bridgetower-base style configuration for the text model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BridgeTowerTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration`,wrap:!1}}),{c(){n=c("p"),n.textContent=y,p=s(),g(l.$$.fragment)},l(o){n=m(o,"P",{"data-svelte-h":!0}),T(n)!=="svelte-11lpom8"&&(n.textContent=y),p=a(o),h(l.$$.fragment,o)},m(o,M){i(o,n,M),i(o,p,M),f(l,o,M),b=!0},p:S,i(o){b||(u(l.$$.fragment,o),b=!0)},o(o){_(l.$$.fragment,o),b=!1},d(o){o&&(r(n),r(p)),w(l,o)}}}function en(v){let n,y="Example:",p,l,b;return l=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJyaWRnZVRvd2VyVmlzaW9uQ29uZmlnJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJyaWRnZVRvd2VyJTIwQnJpZGdlVG93ZXIlMkZicmlkZ2V0b3dlci1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTIwZm9yJTIwdGhlJTIwdmlzaW9uJTIwbW9kZWwlMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQnJpZGdlVG93ZXJWaXNpb25Db25maWcoKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9u",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BridgeTowerVisionConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a BridgeTower BridgeTower/bridgetower-base style configuration for the vision model</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BridgeTowerVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration`,wrap:!1}}),{c(){n=c("p"),n.textContent=y,p=s(),g(l.$$.fragment)},l(o){n=m(o,"P",{"data-svelte-h":!0}),T(n)!=="svelte-11lpom8"&&(n.textContent=y),p=a(o),h(l.$$.fragment,o)},m(o,M){i(o,n,M),i(o,p,M),f(l,o,M),b=!0},p:S,i(o){b||(u(l.$$.fragment,o),b=!0)},o(o){_(l.$$.fragment,o),b=!1},d(o){o&&(r(n),r(p)),w(l,o)}}}function on(v){let n,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=c("p"),n.innerHTML=y},l(p){n=m(p,"P",{"data-svelte-h":!0}),T(n)!=="svelte-fincs2"&&(n.innerHTML=y)},m(p,l){i(p,n,l)},p:S,d(p){p&&r(n)}}}function tn(v){let n,y="Examples:",p,l,b;return l=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJyaWRnZVRvd2VyUHJvY2Vzc29yJTJDJTIwQnJpZGdlVG93ZXJNb2RlbCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBJTIzJTIwcHJlcGFyZSUyMGltYWdlJTIwYW5kJTIwdGV4dCUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEF0ZXh0JTIwJTNEJTIwJTIyaGVsbG8lMjB3b3JsZCUyMiUwQXByb2Nlc3NvciUyMCUzRCUyMEJyaWRnZVRvd2VyUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCcmlkZ2VUb3dlciUyRmJyaWRnZXRvd2VyLWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBCcmlkZ2VUb3dlck1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJCcmlkZ2VUb3dlciUyRmJyaWRnZXRvd2VyLWJhc2UlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlJTJDJTIwdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQW91dHB1dHMua2V5cygp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BridgeTowerProcessor, BridgeTowerModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare image and text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;hello world&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = BridgeTowerProcessor.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BridgeTowerModel.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(image, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs.keys()
odict_keys([<span class="hljs-string">&#x27;text_features&#x27;</span>, <span class="hljs-string">&#x27;image_features&#x27;</span>, <span class="hljs-string">&#x27;pooler_output&#x27;</span>])`,wrap:!1}}),{c(){n=c("p"),n.textContent=y,p=s(),g(l.$$.fragment)},l(o){n=m(o,"P",{"data-svelte-h":!0}),T(n)!=="svelte-kvfsh7"&&(n.textContent=y),p=a(o),h(l.$$.fragment,o)},m(o,M){i(o,n,M),i(o,p,M),f(l,o,M),b=!0},p:S,i(o){b||(u(l.$$.fragment,o),b=!0)},o(o){_(l.$$.fragment,o),b=!1},d(o){o&&(r(n),r(p)),w(l,o)}}}function rn(v){let n,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=c("p"),n.innerHTML=y},l(p){n=m(p,"P",{"data-svelte-h":!0}),T(n)!=="svelte-fincs2"&&(n.innerHTML=y)},m(p,l){i(p,n,l)},p:S,d(p){p&&r(n)}}}function nn(v){let n,y="Examples:",p,l,b;return l=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJyaWRnZVRvd2VyUHJvY2Vzc29yJTJDJTIwQnJpZGdlVG93ZXJGb3JDb250cmFzdGl2ZUxlYXJuaW5nJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjB0b3JjaCUwQSUwQWltYWdlX3VybHMlMjAlM0QlMjAlNUIlMEElMjAlMjAlMjAlMjAlMjJodHRwcyUzQSUyRiUyRmZhcm00LnN0YXRpY2ZsaWNrci5jb20lMkYzMzk1JTJGMzQyODI3ODQxNV84MWMzZTI3ZjE1X3ouanBnJTIyJTJDJTBBJTIwJTIwJTIwJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUyQyUwQSU1RCUwQXRleHRzJTIwJTNEJTIwJTVCJTIydHdvJTIwZG9ncyUyMGluJTIwYSUyMGNhciUyMiUyQyUyMCUyMnR3byUyMGNhdHMlMjBzbGVlcGluZyUyMG9uJTIwYSUyMGNvdWNoJTIyJTVEJTBBaW1hZ2VzJTIwJTNEJTIwJTVCSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUyMGZvciUyMHVybCUyMGluJTIwaW1hZ2VfdXJscyU1RCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEJyaWRnZVRvd2VyUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCcmlkZ2VUb3dlciUyRmJyaWRnZXRvd2VyLWxhcmdlLWl0bS1tbG0taXRjJTIyKSUwQW1vZGVsJTIwJTNEJTIwQnJpZGdlVG93ZXJGb3JDb250cmFzdGl2ZUxlYXJuaW5nLmZyb21fcHJldHJhaW5lZCglMjJCcmlkZ2VUb3dlciUyRmJyaWRnZXRvd2VyLWxhcmdlLWl0bS1tbG0taXRjJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlMkMlMjB0ZXh0cyUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFsb3NzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMlMkMlMjByZXR1cm5fbG9zcyUzRFRydWUpLmxvc3MlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTJDJTIwdGV4dHMlNUIlM0ElM0EtMSU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFsb3NzX3N3YXBwZWQlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyUyQyUyMHJldHVybl9sb3NzJTNEVHJ1ZSkubG9zcyUwQSUwQXByaW50KCUyMkxvc3MlMjIlMkMlMjByb3VuZChsb3NzLml0ZW0oKSUyQyUyMDQpKSUwQSUwQXByaW50KCUyMkxvc3MlMjB3aXRoJTIwc3dhcHBlZCUyMGltYWdlcyUyMiUyQyUyMHJvdW5kKGxvc3Nfc3dhcHBlZC5pdGVtKCklMkMlMjA0KSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BridgeTowerProcessor, BridgeTowerForContrastiveLearning
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>image_urls = [
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;https://farm4.staticflickr.com/3395/3428278415_81c3e27f15_z.jpg&quot;</span>,
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>,
<span class="hljs-meta">... </span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;two dogs in a car&quot;</span>, <span class="hljs-string">&quot;two cats sleeping on a couch&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>images = [Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw) <span class="hljs-keyword">for</span> url <span class="hljs-keyword">in</span> image_urls]

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = BridgeTowerProcessor.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-large-itm-mlm-itc&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BridgeTowerForContrastiveLearning.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-large-itm-mlm-itc&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images, texts, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs, return_loss=<span class="hljs-literal">True</span>).loss

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images, texts[::-<span class="hljs-number">1</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss_swapped = model(**inputs, return_loss=<span class="hljs-literal">True</span>).loss

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Loss&quot;</span>, <span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">4</span>))
Loss <span class="hljs-number">0.0019</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Loss with swapped images&quot;</span>, <span class="hljs-built_in">round</span>(loss_swapped.item(), <span class="hljs-number">4</span>))
Loss <span class="hljs-keyword">with</span> swapped images <span class="hljs-number">2.126</span>`,wrap:!1}}),{c(){n=c("p"),n.textContent=y,p=s(),g(l.$$.fragment)},l(o){n=m(o,"P",{"data-svelte-h":!0}),T(n)!=="svelte-kvfsh7"&&(n.textContent=y),p=a(o),h(l.$$.fragment,o)},m(o,M){i(o,n,M),i(o,p,M),f(l,o,M),b=!0},p:S,i(o){b||(u(l.$$.fragment,o),b=!0)},o(o){_(l.$$.fragment,o),b=!1},d(o){o&&(r(n),r(p)),w(l,o)}}}function sn(v){let n,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=c("p"),n.innerHTML=y},l(p){n=m(p,"P",{"data-svelte-h":!0}),T(n)!=="svelte-fincs2"&&(n.innerHTML=y)},m(p,l){i(p,n,l)},p:S,d(p){p&&r(n)}}}function an(v){let n,y="Examples:",p,l,b;return l=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJyaWRnZVRvd2VyUHJvY2Vzc29yJTJDJTIwQnJpZGdlVG93ZXJGb3JNYXNrZWRMTSUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMzYwOTQzLmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KS5jb252ZXJ0KCUyMlJHQiUyMiklMEF0ZXh0JTIwJTNEJTIwJTIyYSUyMCUzQ21hc2slM0UlMjBsb29raW5nJTIwb3V0JTIwb2YlMjB0aGUlMjB3aW5kb3clMjIlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBCcmlkZ2VUb3dlclByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyQnJpZGdlVG93ZXIlMkZicmlkZ2V0b3dlci1iYXNlLWl0bS1tbG0lMjIpJTBBbW9kZWwlMjAlM0QlMjBCcmlkZ2VUb3dlckZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJCcmlkZ2VUb3dlciUyRmJyaWRnZXRvd2VyLWJhc2UtaXRtLW1sbSUyMiklMEElMEElMjMlMjBwcmVwYXJlJTIwaW5wdXRzJTBBZW5jb2RpbmclMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2UlMkMlMjB0ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEElMjMlMjBmb3J3YXJkJTIwcGFzcyUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmVuY29kaW5nKSUwQSUwQXJlc3VsdHMlMjAlM0QlMjBwcm9jZXNzb3IuZGVjb2RlKG91dHB1dHMubG9naXRzLmFyZ21heChkaW0lM0QtMSkuc3F1ZWV6ZSgwKS50b2xpc3QoKSklMEElMEFwcmludChyZXN1bHRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BridgeTowerProcessor, BridgeTowerForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000360943.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;a &lt;mask&gt; looking out of the window&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = BridgeTowerProcessor.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-base-itm-mlm&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BridgeTowerForMaskedLM.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-base-itm-mlm&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare inputs</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = processor(image, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoding)

<span class="hljs-meta">&gt;&gt;&gt; </span>results = processor.decode(outputs.logits.argmax(dim=-<span class="hljs-number">1</span>).squeeze(<span class="hljs-number">0</span>).tolist())

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(results)
.a cat looking out of the window.`,wrap:!1}}),{c(){n=c("p"),n.textContent=y,p=s(),g(l.$$.fragment)},l(o){n=m(o,"P",{"data-svelte-h":!0}),T(n)!=="svelte-kvfsh7"&&(n.textContent=y),p=a(o),h(l.$$.fragment,o)},m(o,M){i(o,n,M),i(o,p,M),f(l,o,M),b=!0},p:S,i(o){b||(u(l.$$.fragment,o),b=!0)},o(o){_(l.$$.fragment,o),b=!1},d(o){o&&(r(n),r(p)),w(l,o)}}}function dn(v){let n,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=c("p"),n.innerHTML=y},l(p){n=m(p,"P",{"data-svelte-h":!0}),T(n)!=="svelte-fincs2"&&(n.innerHTML=y)},m(p,l){i(p,n,l)},p:S,d(p){p&&r(n)}}}function ln(v){let n,y="Examples:",p,l,b;return l=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJyaWRnZVRvd2VyUHJvY2Vzc29yJTJDJTIwQnJpZGdlVG93ZXJGb3JJbWFnZUFuZFRleHRSZXRyaWV2YWwlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEF0ZXh0cyUyMCUzRCUyMCU1QiUyMkFuJTIwaW1hZ2UlMjBvZiUyMHR3byUyMGNhdHMlMjBjaGlsbGluZyUyMG9uJTIwYSUyMGNvdWNoJTIyJTJDJTIwJTIyQSUyMGZvb3RiYWxsJTIwcGxheWVyJTIwc2NvcmluZyUyMGElMjBnb2FsJTIyJTVEJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQnJpZGdlVG93ZXJQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkJyaWRnZVRvd2VyJTJGYnJpZGdldG93ZXItYmFzZS1pdG0tbWxtJTIyKSUwQW1vZGVsJTIwJTNEJTIwQnJpZGdlVG93ZXJGb3JJbWFnZUFuZFRleHRSZXRyaWV2YWwuZnJvbV9wcmV0cmFpbmVkKCUyMkJyaWRnZVRvd2VyJTJGYnJpZGdldG93ZXItYmFzZS1pdG0tbWxtJTIyKSUwQSUwQSUyMyUyMGZvcndhcmQlMjBwYXNzJTBBc2NvcmVzJTIwJTNEJTIwZGljdCgpJTBBZm9yJTIwdGV4dCUyMGluJTIwdGV4dHMlM0ElMEElMjAlMjAlMjAlMjAlMjMlMjBwcmVwYXJlJTIwaW5wdXRzJTBBJTIwJTIwJTIwJTIwZW5jb2RpbmclMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2UlMkMlMjB0ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKiplbmNvZGluZyklMEElMjAlMjAlMjAlMjBzY29yZXMlNUJ0ZXh0JTVEJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlNUIwJTJDJTIwMSU1RC5pdGVtKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;An image of two cats chilling on a couch&quot;</span>, <span class="hljs-string">&quot;A football player scoring a goal&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = BridgeTowerProcessor.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-base-itm-mlm&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BridgeTowerForImageAndTextRetrieval.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-base-itm-mlm&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>scores = <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:
<span class="hljs-meta">... </span>    <span class="hljs-comment"># prepare inputs</span>
<span class="hljs-meta">... </span>    encoding = processor(image, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">... </span>    outputs = model(**encoding)
<span class="hljs-meta">... </span>    scores[text] = outputs.logits[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>].item()`,wrap:!1}}),{c(){n=c("p"),n.textContent=y,p=s(),g(l.$$.fragment)},l(o){n=m(o,"P",{"data-svelte-h":!0}),T(n)!=="svelte-kvfsh7"&&(n.textContent=y),p=a(o),h(l.$$.fragment,o)},m(o,M){i(o,n,M),i(o,p,M),f(l,o,M),b=!0},p:S,i(o){b||(u(l.$$.fragment,o),b=!0)},o(o){_(l.$$.fragment,o),b=!1},d(o){o&&(r(n),r(p)),w(l,o)}}}function cn(v){let n,y,p,l,b,o,M,Wo,we,dr=`BridgeTower モデルは、Xiao Xu、Chenfei Wu、Shachar Rosenman、Vasudev Lal、Wanxiang Che、Nan Duan <a href="https://arxiv.org/abs/2206.08657" rel="nofollow">BridgeTower: Building Bridges Between Encoders in Vision-Language Representative Learning</a> で提案されました。ドゥアン。このモデルの目標は、
各ユニモーダル エンコーダとクロスモーダル エンコーダの間のブリッジにより、クロスモーダル エンコーダの各層での包括的かつ詳細な対話が可能になり、追加のパフォーマンスと計算コストがほとんど無視できる程度で、さまざまな下流タスクで優れたパフォーマンスを実現します。`,Ro,Te,lr='この論文は <a href="https://aaai.org/Conferences/AAAI-23/" rel="nofollow">AAAI’23</a> 会議に採択されました。',Fo,be,cr="論文の要約は次のとおりです。",Vo,ye,mr=`<em>TWO-TOWER アーキテクチャを備えたビジョン言語 (VL) モデルは、近年の視覚言語表現学習の主流となっています。
現在の VL モデルは、軽量のユニモーダル エンコーダーを使用して、ディープ クロスモーダル エンコーダーで両方のモダリティを同時に抽出、位置合わせ、融合することを学習するか、事前にトレーニングされたディープ ユニモーダル エンコーダーから最終層のユニモーダル表現を上部のクロスモーダルエンコーダー。
どちらのアプローチも、視覚言語表現の学習を制限し、モデルのパフォーマンスを制限する可能性があります。この論文では、ユニモーダル エンコーダの最上位層とクロスモーダル エンコーダの各層の間の接続を構築する複数のブリッジ層を導入する BRIDGETOWER を提案します。
これにより、効果的なボトムアップのクロスモーダル調整と、クロスモーダル エンコーダー内の事前トレーニング済みユニモーダル エンコーダーのさまざまなセマンティック レベルの視覚表現とテキスト表現の間の融合が可能になります。 BRIDGETOWER は 4M 画像のみで事前トレーニングされており、さまざまな下流の視覚言語タスクで最先端のパフォーマンスを実現します。
特に、VQAv2 テスト標準セットでは、BRIDGETOWER は 78.73% の精度を達成し、同じ事前トレーニング データとほぼ無視できる追加パラメータと計算コストで以前の最先端モデル METER を 1.09% 上回りました。
特に、モデルをさらにスケーリングすると、BRIDGETOWER は 81.15% の精度を達成し、桁違いに大きなデータセットで事前トレーニングされたモデルを上回りました。</em>`,No,K,pr,Lo,Me,gr='ブリッジタワー アーキテクチャ。 <a href="https://arxiv.org/abs/2206.08657">元の論文から抜粋。</a>',Go,ve,hr='このモデルは、<a href="https://huggingface.co/anahita-b" rel="nofollow">Anahita Bhiwandiwalla</a>、<a href="https://huggingface.co/Tile" rel="nofollow">Tiep Le</a>、<a href="https://huggingface.co/shaoyent" rel="nofollow">Shaoyen Tseng</a> 。元のコードは <a href="https://github.com/microsoft/BridgeTower" rel="nofollow">ここ</a> にあります。',Po,je,Eo,Be,fr=`BridgeTower は、ビジュアル エンコーダー、テキスト エンコーダー、および複数の軽量ブリッジ レイヤーを備えたクロスモーダル エンコーダーで構成されます。
このアプローチの目標は、各ユニモーダル エンコーダーとクロスモーダル エンコーダーの間にブリッジを構築し、クロスモーダル エンコーダーの各層で包括的かつ詳細な対話を可能にすることでした。
原則として、提案されたアーキテクチャでは、任意のビジュアル、テキスト、またはクロスモーダル エンコーダを適用できます。`,qo,Je,ur=`<a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerProcessor">BridgeTowerProcessor</a> は、<code>RobertaTokenizer</code> と <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerImageProcessor">BridgeTowerImageProcessor</a> を単一のインスタンスにラップし、両方の機能を実現します。
テキストをエンコードし、画像をそれぞれ用意します。`,Ho,xe,_r='次の例は、<a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerProcessor">BridgeTowerProcessor</a> と <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerForContrastiveLearning">BridgeTowerForContrastiveLearning</a> を使用して対照学習を実行する方法を示しています。',Xo,ke,So,$e,wr='次の例は、<a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerProcessor">BridgeTowerProcessor</a> と <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerForImageAndTextRetrieval">BridgeTowerForImageAndTextRetrieval</a> を使用して画像テキストの取得を実行する方法を示しています。',Qo,Ce,Ao,Ie,Tr='次の例は、<a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerProcessor">BridgeTowerProcessor</a> と <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerForMaskedLM">BridgeTowerForMaskedLM</a> を使用してマスクされた言語モデリングを実行する方法を示しています。',Yo,Ue,Do,ze,br="チップ：",Oo,Ze,yr='<li>BridgeTower のこの実装では、<code>RobertaTokenizer</code> を使用してテキスト埋め込みを生成し、OpenAI の CLIP/ViT モデルを使用して視覚的埋め込みを計算します。</li> <li>事前トレーニングされた <a href="https://huggingface.co/BridgeTower/bridgetower-base" rel="nofollow">bridgeTower-base</a> および <a href="https://huggingface.co/BridgeTower/bridgetower--base-itm-mlm" rel="nofollow">bridgetower マスクされた言語モデリングと画像テキスト マッチング</a> のチェックポイント がリリースされました。</li> <li>画像検索およびその他の下流タスクにおける BridgeTower のパフォーマンスについては、<a href="https://arxiv.org/pdf/2206.08657.pdf" rel="nofollow">表 5</a> を参照してください。</li> <li>このモデルの PyTorch バージョンは、torch 1.10 以降でのみ使用できます。</li>',Ko,We,et,x,Re,bt,co,Mr=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a>. It is used to instantiate a
BridgeTower model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the bridgetower-base
<a href="https://huggingface.co/BridgeTower/bridgetower-base/" rel="nofollow">BridgeTower/bridgetower-base</a> architecture.`,yt,mo,vr=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Mt,ee,vt,oe,Fe,jt,po,jr=`Instantiate a <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a> (or a derived class) from BridgeTower text model configuration. Returns:
<a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a>: An instance of a configuration object`,ot,Ve,tt,k,Ne,Bt,go,Br=`This is the configuration class to store the text configuration of a <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a>. The default values here
are copied from RoBERTa. Instantiating a configuration with the defaults will yield a similar configuration to that
of the bridgetower-base <a href="https://huggingface.co/BridgeTower/bridgetower-base/" rel="nofollow">BridegTower/bridgetower-base</a>
architecture.`,Jt,ho,Jr=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,xt,te,rt,Le,nt,$,Ge,kt,fo,xr=`This is the configuration class to store the vision configuration of a <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a>. Instantiating a
configuration with the defaults will yield a similar configuration to that of the bridgetower-base
<a href="https://huggingface.co/BridgeTower/bridgetower-base/" rel="nofollow">BridgeTower/bridgetower-base</a> architecture.`,$t,uo,kr=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ct,re,st,Pe,at,L,Ee,It,_o,$r="Constructs a BridgeTower image processor.",Ut,ne,qe,zt,wo,Cr="Preprocess an image or batch of images.",it,He,dt,C,Xe,Zt,To,Ir=`Constructs a BridgeTower processor which wraps a Roberta tokenizer and BridgeTower image processor into a single
processor.`,Wt,bo,Ur=`<a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerProcessor">BridgeTowerProcessor</a> offers all the functionalities of <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerImageProcessor">BridgeTowerImageProcessor</a> and
<code>RobertaTokenizerFast</code>. See the docstring of <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerProcessor.__call__"><strong>call</strong>()</a> and
<code>decode()</code> for more information.`,Rt,Q,Se,Ft,yo,zr=`This method uses <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">BridgeTowerImageProcessor.<strong>call</strong>()</a> method to prepare image(s) for the model, and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">RobertaTokenizerFast.<strong>call</strong>()</a> to prepare text for the model.`,Vt,Mo,Zr="Please refer to the docstring of the above two methods for more information.",lt,Qe,ct,G,Ae,Nt,vo,Wr=`The bare BridgeTower Model transformer outputting BridgeTowerModelOutput object without any specific head on top.
This model is a PyTorch <code>torch.nn.Module &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&gt;</code>_ subclass. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Lt,W,Ye,Gt,jo,Rr='The <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a> forward method, overrides the <code>__call__</code> special method.',Pt,se,Et,ae,mt,De,pt,I,Oe,qt,Bo,Fr="BridgeTower Model with a image-text contrastive head on top computing image-text contrastive loss.",Ht,Jo,Vr=`This model is a PyTorch <code>torch.nn.Module &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&gt;</code>_ subclass. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Xt,R,Ke,St,xo,Nr='The <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerForContrastiveLearning">BridgeTowerForContrastiveLearning</a> forward method, overrides the <code>__call__</code> special method.',Qt,ie,At,de,gt,eo,ht,U,oo,Yt,ko,Lr="BridgeTower Model with a language modeling head on top as done during pretraining.",Dt,$o,Gr=`This model is a PyTorch <code>torch.nn.Module &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&gt;</code>_ subclass. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Ot,F,to,Kt,Co,Pr='The <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerForMaskedLM">BridgeTowerForMaskedLM</a> forward method, overrides the <code>__call__</code> special method.',er,le,or,ce,ft,ro,ut,z,no,tr,Io,Er=`BridgeTower Model transformer with a classifier head on top (a linear layer on top of the final hidden state of the
[CLS] token) for image-to-text matching.`,rr,Uo,qr=`This model is a PyTorch <code>torch.nn.Module &lt;https://pytorch.org/docs/stable/nn.html#torch.nn.Module&gt;</code>_ subclass. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,nr,V,so,sr,zo,Hr='The <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerForImageAndTextRetrieval">BridgeTowerForImageAndTextRetrieval</a> forward method, overrides the <code>__call__</code> special method.',ar,me,ir,pe,_t,Zo,wt;return b=new N({props:{title:"BridgeTower",local:"bridgetower",headingTag:"h1"}}),M=new N({props:{title:"Overview",local:"overview",headingTag:"h2"}}),je=new N({props:{title:"Usage tips and examples",local:"usage-tips-and-examples",headingTag:"h2"}}),ke=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJyaWRnZVRvd2VyUHJvY2Vzc29yJTJDJTIwQnJpZGdlVG93ZXJGb3JDb250cmFzdGl2ZUxlYXJuaW5nJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBdGV4dHMlMjAlM0QlMjAlNUIlMjJBbiUyMGltYWdlJTIwb2YlMjB0d28lMjBjYXRzJTIwY2hpbGxpbmclMjBvbiUyMGElMjBjb3VjaCUyMiUyQyUyMCUyMkElMjBmb290YmFsbCUyMHBsYXllciUyMHNjb3JpbmclMjBhJTIwZ29hbCUyMiU1RCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEJyaWRnZVRvd2VyUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCcmlkZ2VUb3dlciUyRmJyaWRnZXRvd2VyLWxhcmdlLWl0bS1tbG0taXRjJTIyKSUwQW1vZGVsJTIwJTNEJTIwQnJpZGdlVG93ZXJGb3JDb250cmFzdGl2ZUxlYXJuaW5nLmZyb21fcHJldHJhaW5lZCglMjJCcmlkZ2VUb3dlciUyRmJyaWRnZXRvd2VyLWxhcmdlLWl0bS1tbG0taXRjJTIyKSUwQSUwQSUyMyUyMGZvcndhcmQlMjBwYXNzJTBBc2NvcmVzJTIwJTNEJTIwZGljdCgpJTBBZm9yJTIwdGV4dCUyMGluJTIwdGV4dHMlM0ElMEElMjAlMjAlMjAlMjAlMjMlMjBwcmVwYXJlJTIwaW5wdXRzJTBBJTIwJTIwJTIwJTIwZW5jb2RpbmclMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2UlMkMlMjB0ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKiplbmNvZGluZyklMEElMjAlMjAlMjAlMjBzY29yZXMlNUJ0ZXh0JTVEJTIwJTNEJTIwb3V0cHV0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BridgeTowerProcessor, BridgeTowerForContrastiveLearning
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;An image of two cats chilling on a couch&quot;</span>, <span class="hljs-string">&quot;A football player scoring a goal&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = BridgeTowerProcessor.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-large-itm-mlm-itc&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BridgeTowerForContrastiveLearning.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-large-itm-mlm-itc&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>scores = <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:
<span class="hljs-meta">... </span>    <span class="hljs-comment"># prepare inputs</span>
<span class="hljs-meta">... </span>    encoding = processor(image, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">... </span>    outputs = model(**encoding)
<span class="hljs-meta">... </span>    scores[text] = outputs`,wrap:!1}}),Ce=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJyaWRnZVRvd2VyUHJvY2Vzc29yJTJDJTIwQnJpZGdlVG93ZXJGb3JJbWFnZUFuZFRleHRSZXRyaWV2YWwlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEF0ZXh0cyUyMCUzRCUyMCU1QiUyMkFuJTIwaW1hZ2UlMjBvZiUyMHR3byUyMGNhdHMlMjBjaGlsbGluZyUyMG9uJTIwYSUyMGNvdWNoJTIyJTJDJTIwJTIyQSUyMGZvb3RiYWxsJTIwcGxheWVyJTIwc2NvcmluZyUyMGElMjBnb2FsJTIyJTVEJTBBJTBBcHJvY2Vzc29yJTIwJTNEJTIwQnJpZGdlVG93ZXJQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkJyaWRnZVRvd2VyJTJGYnJpZGdldG93ZXItYmFzZS1pdG0tbWxtJTIyKSUwQW1vZGVsJTIwJTNEJTIwQnJpZGdlVG93ZXJGb3JJbWFnZUFuZFRleHRSZXRyaWV2YWwuZnJvbV9wcmV0cmFpbmVkKCUyMkJyaWRnZVRvd2VyJTJGYnJpZGdldG93ZXItYmFzZS1pdG0tbWxtJTIyKSUwQSUwQSUyMyUyMGZvcndhcmQlMjBwYXNzJTBBc2NvcmVzJTIwJTNEJTIwZGljdCgpJTBBZm9yJTIwdGV4dCUyMGluJTIwdGV4dHMlM0ElMEElMjAlMjAlMjAlMjAlMjMlMjBwcmVwYXJlJTIwaW5wdXRzJTBBJTIwJTIwJTIwJTIwZW5jb2RpbmclMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2UlMkMlMjB0ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKiplbmNvZGluZyklMEElMjAlMjAlMjAlMjBzY29yZXMlNUJ0ZXh0JTVEJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlNUIwJTJDJTIwMSU1RC5pdGVtKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BridgeTowerProcessor, BridgeTowerForImageAndTextRetrieval
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;An image of two cats chilling on a couch&quot;</span>, <span class="hljs-string">&quot;A football player scoring a goal&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = BridgeTowerProcessor.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-base-itm-mlm&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BridgeTowerForImageAndTextRetrieval.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-base-itm-mlm&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>scores = <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> text <span class="hljs-keyword">in</span> texts:
<span class="hljs-meta">... </span>    <span class="hljs-comment"># prepare inputs</span>
<span class="hljs-meta">... </span>    encoding = processor(image, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">... </span>    outputs = model(**encoding)
<span class="hljs-meta">... </span>    scores[text] = outputs.logits[<span class="hljs-number">0</span>, <span class="hljs-number">1</span>].item()`,wrap:!1}}),Ue=new A({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJyaWRnZVRvd2VyUHJvY2Vzc29yJTJDJTIwQnJpZGdlVG93ZXJGb3JNYXNrZWRMTSUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMzYwOTQzLmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KS5jb252ZXJ0KCUyMlJHQiUyMiklMEF0ZXh0JTIwJTNEJTIwJTIyYSUyMCUzQ21hc2slM0UlMjBsb29raW5nJTIwb3V0JTIwb2YlMjB0aGUlMjB3aW5kb3clMjIlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBCcmlkZ2VUb3dlclByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyQnJpZGdlVG93ZXIlMkZicmlkZ2V0b3dlci1iYXNlLWl0bS1tbG0lMjIpJTBBbW9kZWwlMjAlM0QlMjBCcmlkZ2VUb3dlckZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJCcmlkZ2VUb3dlciUyRmJyaWRnZXRvd2VyLWJhc2UtaXRtLW1sbSUyMiklMEElMEElMjMlMjBwcmVwYXJlJTIwaW5wdXRzJTBBZW5jb2RpbmclMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2UlMkMlMjB0ZXh0JTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEElMjMlMjBmb3J3YXJkJTIwcGFzcyUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmVuY29kaW5nKSUwQSUwQXJlc3VsdHMlMjAlM0QlMjBwcm9jZXNzb3IuZGVjb2RlKG91dHB1dHMubG9naXRzLmFyZ21heChkaW0lM0QtMSkuc3F1ZWV6ZSgwKS50b2xpc3QoKSklMEElMEFwcmludChyZXN1bHRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BridgeTowerProcessor, BridgeTowerForMaskedLM
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000360943.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw).convert(<span class="hljs-string">&quot;RGB&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;a &lt;mask&gt; looking out of the window&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = BridgeTowerProcessor.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-base-itm-mlm&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BridgeTowerForMaskedLM.from_pretrained(<span class="hljs-string">&quot;BridgeTower/bridgetower-base-itm-mlm&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># prepare inputs</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>encoding = processor(image, text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># forward pass</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**encoding)

<span class="hljs-meta">&gt;&gt;&gt; </span>results = processor.decode(outputs.logits.argmax(dim=-<span class="hljs-number">1</span>).squeeze(<span class="hljs-number">0</span>).tolist())

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(results)
.a cat looking out of the window.`,wrap:!1}}),We=new N({props:{title:"BridgeTowerConfig",local:"transformers.BridgeTowerConfig",headingTag:"h2"}}),Re=new J({props:{name:"class transformers.BridgeTowerConfig",anchor:"transformers.BridgeTowerConfig",parameters:[{name:"share_cross_modal_transformer_layers",val:" = True"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_size",val:" = 768"},{name:"initializer_factor",val:" = 1"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"share_link_tower_layers",val:" = False"},{name:"link_tower_type",val:" = 'add'"},{name:"num_attention_heads",val:" = 12"},{name:"num_hidden_layers",val:" = 6"},{name:"tie_word_embeddings",val:" = False"},{name:"init_layernorm_from_vision_encoder",val:" = False"},{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BridgeTowerConfig.share_cross_modal_transformer_layers",description:`<strong>share_cross_modal_transformer_layers</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether cross modal transformer layers are shared.`,name:"share_cross_modal_transformer_layers"},{anchor:"transformers.BridgeTowerConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler.`,name:"hidden_act"},{anchor:"transformers.BridgeTowerConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.BridgeTowerConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.BridgeTowerConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.BridgeTowerConfig.share_link_tower_layers",description:`<strong>share_link_tower_layers</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the bride/link tower layers are shared.`,name:"share_link_tower_layers"},{anchor:"transformers.BridgeTowerConfig.link_tower_type",description:`<strong>link_tower_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;add&quot;</code>) &#x2014;
Type of the bridge/link layer.`,name:"link_tower_type"},{anchor:"transformers.BridgeTowerConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.BridgeTowerConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 6) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.BridgeTowerConfig.tie_word_embeddings",description:`<strong>tie_word_embeddings</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to tie input and output embeddings.`,name:"tie_word_embeddings"},{anchor:"transformers.BridgeTowerConfig.init_layernorm_from_vision_encoder",description:`<strong>init_layernorm_from_vision_encoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to init LayerNorm from the vision encoder.`,name:"init_layernorm_from_vision_encoder"},{anchor:"transformers.BridgeTowerConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerTextConfig">BridgeTowerTextConfig</a>.`,name:"text_config"},{anchor:"transformers.BridgeTowerConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerVisionConfig">BridgeTowerVisionConfig</a>.`,name:"vision_config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/configuration_bridgetower.py#L243"}}),ee=new lo({props:{anchor:"transformers.BridgeTowerConfig.example",$$slots:{default:[Or]},$$scope:{ctx:v}}}),Fe=new J({props:{name:"from_text_vision_configs",anchor:"transformers.BridgeTowerConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": BridgeTowerTextConfig"},{name:"vision_config",val:": BridgeTowerVisionConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/configuration_bridgetower.py#L344"}}),Ve=new N({props:{title:"BridgeTowerTextConfig",local:"transformers.BridgeTowerTextConfig",headingTag:"h2"}}),Ne=new J({props:{name:"class transformers.BridgeTowerTextConfig",anchor:"transformers.BridgeTowerTextConfig",parameters:[{name:"vocab_size",val:" = 50265"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"initializer_factor",val:" = 1"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 514"},{name:"type_vocab_size",val:" = 1"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BridgeTowerTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 50265) &#x2014;
Vocabulary size of the text part of the model. Defines the number of different tokens that can be
represented by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a>.`,name:"vocab_size"},{anchor:"transformers.BridgeTowerTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.BridgeTowerTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.BridgeTowerTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.BridgeTowerTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.BridgeTowerTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.BridgeTowerTextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.BridgeTowerTextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.BridgeTowerTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 514) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.BridgeTowerTextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code>.`,name:"type_vocab_size"},{anchor:"transformers.BridgeTowerTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.BridgeTowerTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.BridgeTowerTextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.BridgeTowerTextConfig.is_decoder",description:`<strong>is_decoder</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether the model is used as a decoder or not. If <code>False</code>, the model is used as an encoder.`,name:"is_decoder"},{anchor:"transformers.BridgeTowerTextConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/configuration_bridgetower.py#L121"}}),te=new lo({props:{anchor:"transformers.BridgeTowerTextConfig.example",$$slots:{default:[Kr]},$$scope:{ctx:v}}}),Le=new N({props:{title:"BridgeTowerVisionConfig",local:"transformers.BridgeTowerVisionConfig",headingTag:"h2"}}),Ge=new J({props:{name:"class transformers.BridgeTowerVisionConfig",anchor:"transformers.BridgeTowerVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"patch_size",val:" = 16"},{name:"image_size",val:" = 288"},{name:"initializer_factor",val:" = 1"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"stop_gradient",val:" = False"},{name:"share_layernorm",val:" = True"},{name:"remove_last_layer",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BridgeTowerVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.BridgeTowerVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in visual encoder model.`,name:"num_hidden_layers"},{anchor:"transformers.BridgeTowerVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.BridgeTowerVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 288) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.BridgeTowerVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.BridgeTowerVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.BridgeTowerVisionConfig.stop_gradient",description:`<strong>stop_gradient</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to stop gradient for training.`,name:"stop_gradient"},{anchor:"transformers.BridgeTowerVisionConfig.share_layernorm",description:`<strong>share_layernorm</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether LayerNorm layers are shared.`,name:"share_layernorm"},{anchor:"transformers.BridgeTowerVisionConfig.remove_last_layer",description:`<strong>remove_last_layer</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to remove the last layer from the vision encoder.`,name:"remove_last_layer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/configuration_bridgetower.py#L34"}}),re=new lo({props:{anchor:"transformers.BridgeTowerVisionConfig.example",$$slots:{default:[en]},$$scope:{ctx:v}}}),Pe=new N({props:{title:"BridgeTowerImageProcessor",local:"transformers.BridgeTowerImageProcessor",headingTag:"h2"}}),Ee=new J({props:{name:"class transformers.BridgeTowerImageProcessor",anchor:"transformers.BridgeTowerImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": Dict = 288"},{name:"size_divisor",val:": int = 32"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": Union = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"do_center_crop",val:": bool = True"},{name:"do_pad",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BridgeTowerImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by the
<code>do_resize</code> parameter in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.BridgeTowerImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to 288) &#x2014;
Resize the shorter side of the input to <code>size[&quot;shortest_edge&quot;]</code>. The longer side will be limited to under
<code>int((1333 / 800) * size[&quot;shortest_edge&quot;])</code> while preserving the aspect ratio. Only has an effect if
<code>do_resize</code> is set to <code>True</code>. Can be overridden by the <code>size</code> parameter in the <code>preprocess</code> method.`,name:"size"},{anchor:"transformers.BridgeTowerImageProcessor.size_divisor",description:`<strong>size_divisor</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size by which to make sure both the height and width can be divided. Only has an effect if <code>do_resize</code>
is set to <code>True</code>. Can be overridden by the <code>size_divisor</code> parameter in the <code>preprocess</code> method.`,name:"size_divisor"},{anchor:"transformers.BridgeTowerImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Only has an effect if <code>do_resize</code> is set to <code>True</code>. Can be
overridden by the <code>resample</code> parameter in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.BridgeTowerImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.BridgeTowerImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Only has an effect if <code>do_rescale</code> is set to <code>True</code>. Can be
overridden by the <code>rescale_factor</code> parameter in the <code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.BridgeTowerImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code> method.`,name:"do_normalize"},{anchor:"transformers.BridgeTowerImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method. Can be
overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.BridgeTowerImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.
Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.BridgeTowerImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the image. Can be overridden by the <code>do_center_crop</code> parameter in the <code>preprocess</code>
method.`,name:"do_center_crop"},{anchor:"transformers.BridgeTowerImageProcessor.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to pad the image to the <code>(max_height, max_width)</code> of the images in the batch. Can be overridden by
the <code>do_pad</code> parameter in the <code>preprocess</code> method.`,name:"do_pad"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/image_processing_bridgetower.py#L123"}}),qe=new J({props:{name:"preprocess",anchor:"transformers.BridgeTowerImageProcessor.preprocess",parameters:[{name:"images",val:": Union"},{name:"do_resize",val:": Optional = None"},{name:"size",val:": Optional = None"},{name:"size_divisor",val:": Optional = None"},{name:"resample",val:": Resampling = None"},{name:"do_rescale",val:": Optional = None"},{name:"rescale_factor",val:": Optional = None"},{name:"do_normalize",val:": Optional = None"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"do_pad",val:": Optional = None"},{name:"do_center_crop",val:": Optional = None"},{name:"return_tensors",val:": Union = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BridgeTowerImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Controls the size of the image after <code>resize</code>. The shortest edge of the image is resized to
<code>size[&quot;shortest_edge&quot;]</code> whilst preserving the aspect ratio. If the longest edge of this resized image
is &gt; <code>int(size[&quot;shortest_edge&quot;] * (1333 / 800))</code>, then the image is resized again to make the longest
edge equal to <code>int(size[&quot;shortest_edge&quot;] * (1333 / 800))</code>.`,name:"size"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.size_divisor",description:`<strong>size_divisor</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.size_divisor</code>) &#x2014;
The image is resized to a size that is a multiple of this value.`,name:"size_divisor"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean to normalize the image by if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation to normalize the image by if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_std"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.do_pad",description:`<strong>do_pad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_pad</code>) &#x2014;
Whether to pad the image to the (max_height, max_width) in the batch. If <code>True</code>, a pixel mask is also
created and returned.`,name:"do_pad"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image. If the input size is smaller than <code>crop_size</code> along any edge, the
image is padded with 0&#x2019;s and then center cropped.`,name:"do_center_crop"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.BridgeTowerImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/image_processing_bridgetower.py#L367"}}),He=new N({props:{title:"BridgeTowerProcessor",local:"transformers.BridgeTowerProcessor",headingTag:"h2"}}),Xe=new J({props:{name:"class transformers.BridgeTowerProcessor",anchor:"transformers.BridgeTowerProcessor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.BridgeTowerProcessor.image_processor",description:`<strong>image_processor</strong> (<code>BridgeTowerImageProcessor</code>) &#x2014;
An instance of <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerImageProcessor">BridgeTowerImageProcessor</a>. The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.BridgeTowerProcessor.tokenizer",description:"<strong>tokenizer</strong> (<code>RobertaTokenizerFast</code>) &#x2014;\nAn instance of [&#x2018;RobertaTokenizerFast`]. The tokenizer is a required input.",name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/processing_bridgetower.py#L26"}}),Se=new J({props:{name:"__call__",anchor:"transformers.BridgeTowerProcessor.__call__",parameters:[{name:"images",val:""},{name:"text",val:": Union = None"},{name:"add_special_tokens",val:": bool = True"},{name:"padding",val:": Union = False"},{name:"truncation",val:": Union = None"},{name:"max_length",val:": Optional = None"},{name:"stride",val:": int = 0"},{name:"pad_to_multiple_of",val:": Optional = None"},{name:"return_token_type_ids",val:": Optional = None"},{name:"return_attention_mask",val:": Optional = None"},{name:"return_overflowing_tokens",val:": bool = False"},{name:"return_special_tokens_mask",val:": bool = False"},{name:"return_offsets_mapping",val:": bool = False"},{name:"return_length",val:": bool = False"},{name:"verbose",val:": bool = True"},{name:"return_tensors",val:": Union = None"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/processing_bridgetower.py#L49"}}),Qe=new N({props:{title:"BridgeTowerModel",local:"transformers.BridgeTowerModel",headingTag:"h2"}}),Ae=new J({props:{name:"class transformers.BridgeTowerModel",anchor:"transformers.BridgeTowerModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BridgeTowerModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/modeling_bridgetower.py#L1197"}}),Ye=new J({props:{name:"forward",anchor:"transformers.BridgeTowerModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"pixel_mask",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"image_embeds",val:": Optional = None"},{name:"image_token_type_idx",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"},{name:"labels",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BridgeTowerModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.BridgeTowerModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.BridgeTowerModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.BridgeTowerModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerImageProcessor">BridgeTowerImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">BridgeTowerImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BridgeTowerModel.forward.pixel_mask",description:`<strong>pixel_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding pixel values. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for pixels that are real (i.e. <strong>not masked</strong>),</li>
<li>0 for pixels that are padding (i.e. <strong>masked</strong>).
<code>What are attention masks? &lt;../glossary.html#attention-mask&gt;</code>__</li>
</ul>`,name:"pixel_mask"},{anchor:"transformers.BridgeTowerModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BridgeTowerModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.BridgeTowerModel.forward.image_embeds",description:`<strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>pixel_values</code>, you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>pixel_values</code> into patch embeddings.`,name:"image_embeds"},{anchor:"transformers.BridgeTowerModel.forward.image_token_type_idx",description:`<strong>image_token_type_idx</strong> (<code>int</code>, <em>optional</em>) &#x2014;</p>
<ul>
<li>The token type ids for images.</li>
</ul>`,name:"image_token_type_idx"},{anchor:"transformers.BridgeTowerModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BridgeTowerModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BridgeTowerModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BridgeTowerModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, hidden states are returned as a list containing the hidden states of text, image, and
cross-modal components respectively. i.e. <code>(hidden_states_text, hidden_states_image, hidden_states_cross_modal)</code> where each element is a list of the hidden states of the corresponding
modality. <code>hidden_states_txt/img</code> are a list of tensors corresponding to unimodal hidden states and
<code>hidden_states_cross_modal</code> is a list of tuples containing <code>cross_modal_text_hidden_states</code> and
<code>cross_modal_image_hidden_states</code> of each brdige layer.`,name:"output_hidden_states"},{anchor:"transformers.BridgeTowerModel.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels are currently not supported.`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/modeling_bridgetower.py#L1265",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.bridgetower.modeling_bridgetower.BridgeTowerModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig"
>BridgeTowerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>text_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, text_sequence_length, hidden_size)</code>) — Sequence of hidden-states at the text output of the last layer of the model.</p>
</li>
<li>
<p><strong>image_features</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, image_sequence_length, hidden_size)</code>) — Sequence of hidden-states at the image output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size x 2)</code>) — Concatenation of last layer hidden-state of the first token of the text and image sequence (classification
token), respectively, after further processing through layers used for auxiliary pretraining tasks.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of
the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.bridgetower.modeling_bridgetower.BridgeTowerModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),se=new Tt({props:{$$slots:{default:[on]},$$scope:{ctx:v}}}),ae=new lo({props:{anchor:"transformers.BridgeTowerModel.forward.example",$$slots:{default:[tn]},$$scope:{ctx:v}}}),De=new N({props:{title:"BridgeTowerForContrastiveLearning",local:"transformers.BridgeTowerForContrastiveLearning",headingTag:"h2"}}),Oe=new J({props:{name:"class transformers.BridgeTowerForContrastiveLearning",anchor:"transformers.BridgeTowerForContrastiveLearning",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BridgeTowerForContrastiveLearning.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/modeling_bridgetower.py#L1761"}}),Ke=new J({props:{name:"forward",anchor:"transformers.BridgeTowerForContrastiveLearning.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"pixel_mask",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"image_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = True"},{name:"return_dict",val:": Optional = None"},{name:"return_loss",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerImageProcessor">BridgeTowerImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">BridgeTowerImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.pixel_mask",description:`<strong>pixel_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding pixel values. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for pixels that are real (i.e. <strong>not masked</strong>),</li>
<li>0 for pixels that are padding (i.e. <strong>masked</strong>).
<code>What are attention masks? &lt;../glossary.html#attention-mask&gt;</code>__</li>
</ul>`,name:"pixel_mask"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.image_embeds",description:`<strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>pixel_values</code>, you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>pixel_values</code> into patch embeddings.`,name:"image_embeds"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.image_token_type_idx",description:`<strong>image_token_type_idx</strong> (<code>int</code>, <em>optional</em>) &#x2014;</p>
<ul>
<li>The token type ids for images.</li>
</ul>`,name:"image_token_type_idx"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/modeling_bridgetower.py#L1781",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.bridgetower.modeling_bridgetower.BridgeTowerContrastiveOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig"
>BridgeTowerConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code> — Image-text contrastive loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</li>
<li><strong>text_embeds</strong> (<code>torch.FloatTensor)</code>, <em>optional</em>, returned when model is initialized with <code>with_projection=True</code>) — The text embeddings obtained by applying the projection layer to the pooler_output.</li>
<li><strong>image_embeds</strong> (<code>torch.FloatTensor)</code>, <em>optional</em>, returned when model is initialized with <code>with_projection=True</code>) — The image embeddings obtained by applying the projection layer to the pooler_output.</li>
<li><strong>cross_embeds</strong>  (<code>torch.FloatTensor)</code>, <em>optional</em>, returned when model is initialized with <code>with_projection=True</code>) — The text-image cross-modal embeddings obtained by applying the projection layer to the pooler_output.</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of
the model at the output of each layer plus the optional initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.bridgetower.modeling_bridgetower.BridgeTowerContrastiveOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ie=new Tt({props:{$$slots:{default:[rn]},$$scope:{ctx:v}}}),de=new lo({props:{anchor:"transformers.BridgeTowerForContrastiveLearning.forward.example",$$slots:{default:[nn]},$$scope:{ctx:v}}}),eo=new N({props:{title:"BridgeTowerForMaskedLM",local:"transformers.BridgeTowerForMaskedLM",headingTag:"h2"}}),oo=new J({props:{name:"class transformers.BridgeTowerForMaskedLM",anchor:"transformers.BridgeTowerForMaskedLM",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BridgeTowerForMaskedLM.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/modeling_bridgetower.py#L1541"}}),to=new J({props:{name:"forward",anchor:"transformers.BridgeTowerForMaskedLM.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"pixel_mask",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"image_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"},{name:"labels",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BridgeTowerForMaskedLM.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerImageProcessor">BridgeTowerImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">BridgeTowerImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.pixel_mask",description:`<strong>pixel_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding pixel values. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for pixels that are real (i.e. <strong>not masked</strong>),</li>
<li>0 for pixels that are padding (i.e. <strong>masked</strong>).
<code>What are attention masks? &lt;../glossary.html#attention-mask&gt;</code>__</li>
</ul>`,name:"pixel_mask"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.image_embeds",description:`<strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>pixel_values</code>, you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>pixel_values</code> into patch embeddings.`,name:"image_embeds"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.image_token_type_idx",description:`<strong>image_token_type_idx</strong> (<code>int</code>, <em>optional</em>) &#x2014;</p>
<ul>
<li>The token type ids for images.</li>
</ul>`,name:"image_token_type_idx"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BridgeTowerForMaskedLM.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Labels for computing the masked language modeling loss. Indices should be in <code>[-100, 0, ..., config.vocab_size]</code> (see <code>input_ids</code> docstring) Tokens with indices set to <code>-100</code> are ignored (masked), the
loss is only computed for the tokens with labels in <code>[0, ..., config.vocab_size]</code>`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/modeling_bridgetower.py#L1565",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig"
>BridgeTowerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Masked language modeling (MLM) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, config.vocab_size)</code>) — Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.MaskedLMOutput"
>transformers.modeling_outputs.MaskedLMOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),le=new Tt({props:{$$slots:{default:[sn]},$$scope:{ctx:v}}}),ce=new lo({props:{anchor:"transformers.BridgeTowerForMaskedLM.forward.example",$$slots:{default:[an]},$$scope:{ctx:v}}}),ro=new N({props:{title:"BridgeTowerForImageAndTextRetrieval",local:"transformers.BridgeTowerForImageAndTextRetrieval",headingTag:"h2"}}),no=new J({props:{name:"class transformers.BridgeTowerForImageAndTextRetrieval",anchor:"transformers.BridgeTowerForImageAndTextRetrieval",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/modeling_bridgetower.py#L1649"}}),so=new J({props:{name:"forward",anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"pixel_mask",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"image_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"},{name:"labels",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input
IDs?</a>`,name:"input_ids"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.FloatTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.
<a href="../glossary#attention-mask">What are attention masks?</a></li>
</ul>`,name:"attention_mask"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.
<a href="../glossary#token-type-ids">What are token type IDs?</a></li>
</ul>`,name:"token_type_ids"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerImageProcessor">BridgeTowerImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">BridgeTowerImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.pixel_mask",description:`<strong>pixel_mask</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, height, width)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding pixel values. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for pixels that are real (i.e. <strong>not masked</strong>),</li>
<li>0 for pixels that are padding (i.e. <strong>masked</strong>).
<code>What are attention masks? &lt;../glossary.html#attention-mask&gt;</code>__</li>
</ul>`,name:"pixel_mask"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.image_embeds",description:`<strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_patches, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>pixel_values</code>, you can choose to directly pass an embedded representation.
This is useful if you want more control over how to convert <code>pixel_values</code> into patch embeddings.`,name:"image_embeds"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.image_token_type_idx",description:`<strong>image_token_type_idx</strong> (<code>int</code>, <em>optional</em>) &#x2014;</p>
<ul>
<li>The token type ids for images.</li>
</ul>`,name:"image_token_type_idx"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, 1)</code>, <em>optional</em>) &#x2014;
Labels for computing the image-text matching loss. 0 means the pairs don&#x2019;t match and 1 means they match.
The pairs with 0 will be skipped for calculation.`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bridgetower/modeling_bridgetower.py#L1667",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig"
>BridgeTowerConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),me=new Tt({props:{$$slots:{default:[dn]},$$scope:{ctx:v}}}),pe=new lo({props:{anchor:"transformers.BridgeTowerForImageAndTextRetrieval.forward.example",$$slots:{default:[ln]},$$scope:{ctx:v}}}),{c(){n=c("meta"),y=s(),p=c("p"),l=s(),g(b.$$.fragment),o=s(),g(M.$$.fragment),Wo=s(),we=c("p"),we.innerHTML=dr,Ro=s(),Te=c("p"),Te.innerHTML=lr,Fo=s(),be=c("p"),be.textContent=cr,Vo=s(),ye=c("p"),ye.innerHTML=mr,No=s(),K=c("img"),Lo=s(),Me=c("small"),Me.innerHTML=gr,Go=s(),ve=c("p"),ve.innerHTML=hr,Po=s(),g(je.$$.fragment),Eo=s(),Be=c("p"),Be.textContent=fr,qo=s(),Je=c("p"),Je.innerHTML=ur,Ho=s(),xe=c("p"),xe.innerHTML=_r,Xo=s(),g(ke.$$.fragment),So=s(),$e=c("p"),$e.innerHTML=wr,Qo=s(),g(Ce.$$.fragment),Ao=s(),Ie=c("p"),Ie.innerHTML=Tr,Yo=s(),g(Ue.$$.fragment),Do=s(),ze=c("p"),ze.textContent=br,Oo=s(),Ze=c("ul"),Ze.innerHTML=yr,Ko=s(),g(We.$$.fragment),et=s(),x=c("div"),g(Re.$$.fragment),bt=s(),co=c("p"),co.innerHTML=Mr,yt=s(),mo=c("p"),mo.innerHTML=vr,Mt=s(),g(ee.$$.fragment),vt=s(),oe=c("div"),g(Fe.$$.fragment),jt=s(),po=c("p"),po.innerHTML=jr,ot=s(),g(Ve.$$.fragment),tt=s(),k=c("div"),g(Ne.$$.fragment),Bt=s(),go=c("p"),go.innerHTML=Br,Jt=s(),ho=c("p"),ho.innerHTML=Jr,xt=s(),g(te.$$.fragment),rt=s(),g(Le.$$.fragment),nt=s(),$=c("div"),g(Ge.$$.fragment),kt=s(),fo=c("p"),fo.innerHTML=xr,$t=s(),uo=c("p"),uo.innerHTML=kr,Ct=s(),g(re.$$.fragment),st=s(),g(Pe.$$.fragment),at=s(),L=c("div"),g(Ee.$$.fragment),It=s(),_o=c("p"),_o.textContent=$r,Ut=s(),ne=c("div"),g(qe.$$.fragment),zt=s(),wo=c("p"),wo.textContent=Cr,it=s(),g(He.$$.fragment),dt=s(),C=c("div"),g(Xe.$$.fragment),Zt=s(),To=c("p"),To.textContent=Ir,Wt=s(),bo=c("p"),bo.innerHTML=Ur,Rt=s(),Q=c("div"),g(Se.$$.fragment),Ft=s(),yo=c("p"),yo.innerHTML=zr,Vt=s(),Mo=c("p"),Mo.textContent=Zr,lt=s(),g(Qe.$$.fragment),ct=s(),G=c("div"),g(Ae.$$.fragment),Nt=s(),vo=c("p"),vo.innerHTML=Wr,Lt=s(),W=c("div"),g(Ye.$$.fragment),Gt=s(),jo=c("p"),jo.innerHTML=Rr,Pt=s(),g(se.$$.fragment),Et=s(),g(ae.$$.fragment),mt=s(),g(De.$$.fragment),pt=s(),I=c("div"),g(Oe.$$.fragment),qt=s(),Bo=c("p"),Bo.textContent=Fr,Ht=s(),Jo=c("p"),Jo.innerHTML=Vr,Xt=s(),R=c("div"),g(Ke.$$.fragment),St=s(),xo=c("p"),xo.innerHTML=Nr,Qt=s(),g(ie.$$.fragment),At=s(),g(de.$$.fragment),gt=s(),g(eo.$$.fragment),ht=s(),U=c("div"),g(oo.$$.fragment),Yt=s(),ko=c("p"),ko.textContent=Lr,Dt=s(),$o=c("p"),$o.innerHTML=Gr,Ot=s(),F=c("div"),g(to.$$.fragment),Kt=s(),Co=c("p"),Co.innerHTML=Pr,er=s(),g(le.$$.fragment),or=s(),g(ce.$$.fragment),ft=s(),g(ro.$$.fragment),ut=s(),z=c("div"),g(no.$$.fragment),tr=s(),Io=c("p"),Io.textContent=Er,rr=s(),Uo=c("p"),Uo.innerHTML=qr,nr=s(),V=c("div"),g(so.$$.fragment),sr=s(),zo=c("p"),zo.innerHTML=Hr,ar=s(),g(me.$$.fragment),ir=s(),g(pe.$$.fragment),_t=s(),Zo=c("p"),this.h()},l(e){const t=Dr("svelte-u9bgzb",document.head);n=m(t,"META",{name:!0,content:!0}),t.forEach(r),y=a(e),p=m(e,"P",{}),B(p).forEach(r),l=a(e),h(b.$$.fragment,e),o=a(e),h(M.$$.fragment,e),Wo=a(e),we=m(e,"P",{"data-svelte-h":!0}),T(we)!=="svelte-1y5q9qu"&&(we.innerHTML=dr),Ro=a(e),Te=m(e,"P",{"data-svelte-h":!0}),T(Te)!=="svelte-1gc4c4w"&&(Te.innerHTML=lr),Fo=a(e),be=m(e,"P",{"data-svelte-h":!0}),T(be)!=="svelte-1cv3nri"&&(be.textContent=cr),Vo=a(e),ye=m(e,"P",{"data-svelte-h":!0}),T(ye)!=="svelte-1k1npc1"&&(ye.innerHTML=mr),No=a(e),K=m(e,"IMG",{src:!0,alt:!0,width:!0}),Lo=a(e),Me=m(e,"SMALL",{"data-svelte-h":!0}),T(Me)!=="svelte-zwxe54"&&(Me.innerHTML=gr),Go=a(e),ve=m(e,"P",{"data-svelte-h":!0}),T(ve)!=="svelte-lhhz16"&&(ve.innerHTML=hr),Po=a(e),h(je.$$.fragment,e),Eo=a(e),Be=m(e,"P",{"data-svelte-h":!0}),T(Be)!=="svelte-2e29iy"&&(Be.textContent=fr),qo=a(e),Je=m(e,"P",{"data-svelte-h":!0}),T(Je)!=="svelte-istyay"&&(Je.innerHTML=ur),Ho=a(e),xe=m(e,"P",{"data-svelte-h":!0}),T(xe)!=="svelte-1q1k5z4"&&(xe.innerHTML=_r),Xo=a(e),h(ke.$$.fragment,e),So=a(e),$e=m(e,"P",{"data-svelte-h":!0}),T($e)!=="svelte-a9a60"&&($e.innerHTML=wr),Qo=a(e),h(Ce.$$.fragment,e),Ao=a(e),Ie=m(e,"P",{"data-svelte-h":!0}),T(Ie)!=="svelte-zeathb"&&(Ie.innerHTML=Tr),Yo=a(e),h(Ue.$$.fragment,e),Do=a(e),ze=m(e,"P",{"data-svelte-h":!0}),T(ze)!=="svelte-fhhf9b"&&(ze.textContent=br),Oo=a(e),Ze=m(e,"UL",{"data-svelte-h":!0}),T(Ze)!=="svelte-a7g7wf"&&(Ze.innerHTML=yr),Ko=a(e),h(We.$$.fragment,e),et=a(e),x=m(e,"DIV",{class:!0});var Z=B(x);h(Re.$$.fragment,Z),bt=a(Z),co=m(Z,"P",{"data-svelte-h":!0}),T(co)!=="svelte-lfy63m"&&(co.innerHTML=Mr),yt=a(Z),mo=m(Z,"P",{"data-svelte-h":!0}),T(mo)!=="svelte-1s6wgpv"&&(mo.innerHTML=vr),Mt=a(Z),h(ee.$$.fragment,Z),vt=a(Z),oe=m(Z,"DIV",{class:!0});var ao=B(oe);h(Fe.$$.fragment,ao),jt=a(ao),po=m(ao,"P",{"data-svelte-h":!0}),T(po)!=="svelte-1frlhnf"&&(po.innerHTML=jr),ao.forEach(r),Z.forEach(r),ot=a(e),h(Ve.$$.fragment,e),tt=a(e),k=m(e,"DIV",{class:!0});var P=B(k);h(Ne.$$.fragment,P),Bt=a(P),go=m(P,"P",{"data-svelte-h":!0}),T(go)!=="svelte-1634d5o"&&(go.innerHTML=Br),Jt=a(P),ho=m(P,"P",{"data-svelte-h":!0}),T(ho)!=="svelte-1s6wgpv"&&(ho.innerHTML=Jr),xt=a(P),h(te.$$.fragment,P),P.forEach(r),rt=a(e),h(Le.$$.fragment,e),nt=a(e),$=m(e,"DIV",{class:!0});var E=B($);h(Ge.$$.fragment,E),kt=a(E),fo=m(E,"P",{"data-svelte-h":!0}),T(fo)!=="svelte-1wrzz88"&&(fo.innerHTML=xr),$t=a(E),uo=m(E,"P",{"data-svelte-h":!0}),T(uo)!=="svelte-1s6wgpv"&&(uo.innerHTML=kr),Ct=a(E),h(re.$$.fragment,E),E.forEach(r),st=a(e),h(Pe.$$.fragment,e),at=a(e),L=m(e,"DIV",{class:!0});var Y=B(L);h(Ee.$$.fragment,Y),It=a(Y),_o=m(Y,"P",{"data-svelte-h":!0}),T(_o)!=="svelte-6xw7x4"&&(_o.textContent=$r),Ut=a(Y),ne=m(Y,"DIV",{class:!0});var io=B(ne);h(qe.$$.fragment,io),zt=a(io),wo=m(io,"P",{"data-svelte-h":!0}),T(wo)!=="svelte-1x3yxsa"&&(wo.textContent=Cr),io.forEach(r),Y.forEach(r),it=a(e),h(He.$$.fragment,e),dt=a(e),C=m(e,"DIV",{class:!0});var q=B(C);h(Xe.$$.fragment,q),Zt=a(q),To=m(q,"P",{"data-svelte-h":!0}),T(To)!=="svelte-wdq1yt"&&(To.textContent=Ir),Wt=a(q),bo=m(q,"P",{"data-svelte-h":!0}),T(bo)!=="svelte-shahmv"&&(bo.innerHTML=Ur),Rt=a(q),Q=m(q,"DIV",{class:!0});var D=B(Q);h(Se.$$.fragment,D),Ft=a(D),yo=m(D,"P",{"data-svelte-h":!0}),T(yo)!=="svelte-14yzepc"&&(yo.innerHTML=zr),Vt=a(D),Mo=m(D,"P",{"data-svelte-h":!0}),T(Mo)!=="svelte-ws0hzs"&&(Mo.textContent=Zr),D.forEach(r),q.forEach(r),lt=a(e),h(Qe.$$.fragment,e),ct=a(e),G=m(e,"DIV",{class:!0});var O=B(G);h(Ae.$$.fragment,O),Nt=a(O),vo=m(O,"P",{"data-svelte-h":!0}),T(vo)!=="svelte-1xuccxr"&&(vo.innerHTML=Wr),Lt=a(O),W=m(O,"DIV",{class:!0});var H=B(W);h(Ye.$$.fragment,H),Gt=a(H),jo=m(H,"P",{"data-svelte-h":!0}),T(jo)!=="svelte-1qa8gip"&&(jo.innerHTML=Rr),Pt=a(H),h(se.$$.fragment,H),Et=a(H),h(ae.$$.fragment,H),H.forEach(r),O.forEach(r),mt=a(e),h(De.$$.fragment,e),pt=a(e),I=m(e,"DIV",{class:!0});var X=B(I);h(Oe.$$.fragment,X),qt=a(X),Bo=m(X,"P",{"data-svelte-h":!0}),T(Bo)!=="svelte-7iw5gf"&&(Bo.textContent=Fr),Ht=a(X),Jo=m(X,"P",{"data-svelte-h":!0}),T(Jo)!=="svelte-1alo1i2"&&(Jo.innerHTML=Vr),Xt=a(X),R=m(X,"DIV",{class:!0});var ge=B(R);h(Ke.$$.fragment,ge),St=a(ge),xo=m(ge,"P",{"data-svelte-h":!0}),T(xo)!=="svelte-pqmhn7"&&(xo.innerHTML=Nr),Qt=a(ge),h(ie.$$.fragment,ge),At=a(ge),h(de.$$.fragment,ge),ge.forEach(r),X.forEach(r),gt=a(e),h(eo.$$.fragment,e),ht=a(e),U=m(e,"DIV",{class:!0});var he=B(U);h(oo.$$.fragment,he),Yt=a(he),ko=m(he,"P",{"data-svelte-h":!0}),T(ko)!=="svelte-1mczdox"&&(ko.textContent=Lr),Dt=a(he),$o=m(he,"P",{"data-svelte-h":!0}),T($o)!=="svelte-1alo1i2"&&($o.innerHTML=Gr),Ot=a(he),F=m(he,"DIV",{class:!0});var fe=B(F);h(to.$$.fragment,fe),Kt=a(fe),Co=m(fe,"P",{"data-svelte-h":!0}),T(Co)!=="svelte-c4woel"&&(Co.innerHTML=Pr),er=a(fe),h(le.$$.fragment,fe),or=a(fe),h(ce.$$.fragment,fe),fe.forEach(r),he.forEach(r),ft=a(e),h(ro.$$.fragment,e),ut=a(e),z=m(e,"DIV",{class:!0});var ue=B(z);h(no.$$.fragment,ue),tr=a(ue),Io=m(ue,"P",{"data-svelte-h":!0}),T(Io)!=="svelte-7q7qrd"&&(Io.textContent=Er),rr=a(ue),Uo=m(ue,"P",{"data-svelte-h":!0}),T(Uo)!=="svelte-1alo1i2"&&(Uo.innerHTML=qr),nr=a(ue),V=m(ue,"DIV",{class:!0});var _e=B(V);h(so.$$.fragment,_e),sr=a(_e),zo=m(_e,"P",{"data-svelte-h":!0}),T(zo)!=="svelte-bw0usz"&&(zo.innerHTML=Hr),ar=a(_e),h(me.$$.fragment,_e),ir=a(_e),h(pe.$$.fragment,_e),_e.forEach(r),ue.forEach(r),_t=a(e),Zo=m(e,"P",{}),B(Zo).forEach(r),this.h()},h(){j(n,"name","hf:doc:metadata"),j(n,"content",mn),Sr(K.src,pr="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/bridgetower_architecture%20.jpg")||j(K,"src",pr),j(K,"alt","drawing"),j(K,"width","600"),j(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),j(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){d(document.head,n),i(e,y,t),i(e,p,t),i(e,l,t),f(b,e,t),i(e,o,t),f(M,e,t),i(e,Wo,t),i(e,we,t),i(e,Ro,t),i(e,Te,t),i(e,Fo,t),i(e,be,t),i(e,Vo,t),i(e,ye,t),i(e,No,t),i(e,K,t),i(e,Lo,t),i(e,Me,t),i(e,Go,t),i(e,ve,t),i(e,Po,t),f(je,e,t),i(e,Eo,t),i(e,Be,t),i(e,qo,t),i(e,Je,t),i(e,Ho,t),i(e,xe,t),i(e,Xo,t),f(ke,e,t),i(e,So,t),i(e,$e,t),i(e,Qo,t),f(Ce,e,t),i(e,Ao,t),i(e,Ie,t),i(e,Yo,t),f(Ue,e,t),i(e,Do,t),i(e,ze,t),i(e,Oo,t),i(e,Ze,t),i(e,Ko,t),f(We,e,t),i(e,et,t),i(e,x,t),f(Re,x,null),d(x,bt),d(x,co),d(x,yt),d(x,mo),d(x,Mt),f(ee,x,null),d(x,vt),d(x,oe),f(Fe,oe,null),d(oe,jt),d(oe,po),i(e,ot,t),f(Ve,e,t),i(e,tt,t),i(e,k,t),f(Ne,k,null),d(k,Bt),d(k,go),d(k,Jt),d(k,ho),d(k,xt),f(te,k,null),i(e,rt,t),f(Le,e,t),i(e,nt,t),i(e,$,t),f(Ge,$,null),d($,kt),d($,fo),d($,$t),d($,uo),d($,Ct),f(re,$,null),i(e,st,t),f(Pe,e,t),i(e,at,t),i(e,L,t),f(Ee,L,null),d(L,It),d(L,_o),d(L,Ut),d(L,ne),f(qe,ne,null),d(ne,zt),d(ne,wo),i(e,it,t),f(He,e,t),i(e,dt,t),i(e,C,t),f(Xe,C,null),d(C,Zt),d(C,To),d(C,Wt),d(C,bo),d(C,Rt),d(C,Q),f(Se,Q,null),d(Q,Ft),d(Q,yo),d(Q,Vt),d(Q,Mo),i(e,lt,t),f(Qe,e,t),i(e,ct,t),i(e,G,t),f(Ae,G,null),d(G,Nt),d(G,vo),d(G,Lt),d(G,W),f(Ye,W,null),d(W,Gt),d(W,jo),d(W,Pt),f(se,W,null),d(W,Et),f(ae,W,null),i(e,mt,t),f(De,e,t),i(e,pt,t),i(e,I,t),f(Oe,I,null),d(I,qt),d(I,Bo),d(I,Ht),d(I,Jo),d(I,Xt),d(I,R),f(Ke,R,null),d(R,St),d(R,xo),d(R,Qt),f(ie,R,null),d(R,At),f(de,R,null),i(e,gt,t),f(eo,e,t),i(e,ht,t),i(e,U,t),f(oo,U,null),d(U,Yt),d(U,ko),d(U,Dt),d(U,$o),d(U,Ot),d(U,F),f(to,F,null),d(F,Kt),d(F,Co),d(F,er),f(le,F,null),d(F,or),f(ce,F,null),i(e,ft,t),f(ro,e,t),i(e,ut,t),i(e,z,t),f(no,z,null),d(z,tr),d(z,Io),d(z,rr),d(z,Uo),d(z,nr),d(z,V),f(so,V,null),d(V,sr),d(V,zo),d(V,ar),f(me,V,null),d(V,ir),f(pe,V,null),i(e,_t,t),i(e,Zo,t),wt=!0},p(e,[t]){const Z={};t&2&&(Z.$$scope={dirty:t,ctx:e}),ee.$set(Z);const ao={};t&2&&(ao.$$scope={dirty:t,ctx:e}),te.$set(ao);const P={};t&2&&(P.$$scope={dirty:t,ctx:e}),re.$set(P);const E={};t&2&&(E.$$scope={dirty:t,ctx:e}),se.$set(E);const Y={};t&2&&(Y.$$scope={dirty:t,ctx:e}),ae.$set(Y);const io={};t&2&&(io.$$scope={dirty:t,ctx:e}),ie.$set(io);const q={};t&2&&(q.$$scope={dirty:t,ctx:e}),de.$set(q);const D={};t&2&&(D.$$scope={dirty:t,ctx:e}),le.$set(D);const O={};t&2&&(O.$$scope={dirty:t,ctx:e}),ce.$set(O);const H={};t&2&&(H.$$scope={dirty:t,ctx:e}),me.$set(H);const X={};t&2&&(X.$$scope={dirty:t,ctx:e}),pe.$set(X)},i(e){wt||(u(b.$$.fragment,e),u(M.$$.fragment,e),u(je.$$.fragment,e),u(ke.$$.fragment,e),u(Ce.$$.fragment,e),u(Ue.$$.fragment,e),u(We.$$.fragment,e),u(Re.$$.fragment,e),u(ee.$$.fragment,e),u(Fe.$$.fragment,e),u(Ve.$$.fragment,e),u(Ne.$$.fragment,e),u(te.$$.fragment,e),u(Le.$$.fragment,e),u(Ge.$$.fragment,e),u(re.$$.fragment,e),u(Pe.$$.fragment,e),u(Ee.$$.fragment,e),u(qe.$$.fragment,e),u(He.$$.fragment,e),u(Xe.$$.fragment,e),u(Se.$$.fragment,e),u(Qe.$$.fragment,e),u(Ae.$$.fragment,e),u(Ye.$$.fragment,e),u(se.$$.fragment,e),u(ae.$$.fragment,e),u(De.$$.fragment,e),u(Oe.$$.fragment,e),u(Ke.$$.fragment,e),u(ie.$$.fragment,e),u(de.$$.fragment,e),u(eo.$$.fragment,e),u(oo.$$.fragment,e),u(to.$$.fragment,e),u(le.$$.fragment,e),u(ce.$$.fragment,e),u(ro.$$.fragment,e),u(no.$$.fragment,e),u(so.$$.fragment,e),u(me.$$.fragment,e),u(pe.$$.fragment,e),wt=!0)},o(e){_(b.$$.fragment,e),_(M.$$.fragment,e),_(je.$$.fragment,e),_(ke.$$.fragment,e),_(Ce.$$.fragment,e),_(Ue.$$.fragment,e),_(We.$$.fragment,e),_(Re.$$.fragment,e),_(ee.$$.fragment,e),_(Fe.$$.fragment,e),_(Ve.$$.fragment,e),_(Ne.$$.fragment,e),_(te.$$.fragment,e),_(Le.$$.fragment,e),_(Ge.$$.fragment,e),_(re.$$.fragment,e),_(Pe.$$.fragment,e),_(Ee.$$.fragment,e),_(qe.$$.fragment,e),_(He.$$.fragment,e),_(Xe.$$.fragment,e),_(Se.$$.fragment,e),_(Qe.$$.fragment,e),_(Ae.$$.fragment,e),_(Ye.$$.fragment,e),_(se.$$.fragment,e),_(ae.$$.fragment,e),_(De.$$.fragment,e),_(Oe.$$.fragment,e),_(Ke.$$.fragment,e),_(ie.$$.fragment,e),_(de.$$.fragment,e),_(eo.$$.fragment,e),_(oo.$$.fragment,e),_(to.$$.fragment,e),_(le.$$.fragment,e),_(ce.$$.fragment,e),_(ro.$$.fragment,e),_(no.$$.fragment,e),_(so.$$.fragment,e),_(me.$$.fragment,e),_(pe.$$.fragment,e),wt=!1},d(e){e&&(r(y),r(p),r(l),r(o),r(Wo),r(we),r(Ro),r(Te),r(Fo),r(be),r(Vo),r(ye),r(No),r(K),r(Lo),r(Me),r(Go),r(ve),r(Po),r(Eo),r(Be),r(qo),r(Je),r(Ho),r(xe),r(Xo),r(So),r($e),r(Qo),r(Ao),r(Ie),r(Yo),r(Do),r(ze),r(Oo),r(Ze),r(Ko),r(et),r(x),r(ot),r(tt),r(k),r(rt),r(nt),r($),r(st),r(at),r(L),r(it),r(dt),r(C),r(lt),r(ct),r(G),r(mt),r(pt),r(I),r(gt),r(ht),r(U),r(ft),r(ut),r(z),r(_t),r(Zo)),r(n),w(b,e),w(M,e),w(je,e),w(ke,e),w(Ce,e),w(Ue,e),w(We,e),w(Re),w(ee),w(Fe),w(Ve,e),w(Ne),w(te),w(Le,e),w(Ge),w(re),w(Pe,e),w(Ee),w(qe),w(He,e),w(Xe),w(Se),w(Qe,e),w(Ae),w(Ye),w(se),w(ae),w(De,e),w(Oe),w(Ke),w(ie),w(de),w(eo,e),w(oo),w(to),w(le),w(ce),w(ro,e),w(no),w(so),w(me),w(pe)}}}const mn='{"title":"BridgeTower","local":"bridgetower","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips and examples","local":"usage-tips-and-examples","sections":[],"depth":2},{"title":"BridgeTowerConfig","local":"transformers.BridgeTowerConfig","sections":[],"depth":2},{"title":"BridgeTowerTextConfig","local":"transformers.BridgeTowerTextConfig","sections":[],"depth":2},{"title":"BridgeTowerVisionConfig","local":"transformers.BridgeTowerVisionConfig","sections":[],"depth":2},{"title":"BridgeTowerImageProcessor","local":"transformers.BridgeTowerImageProcessor","sections":[],"depth":2},{"title":"BridgeTowerProcessor","local":"transformers.BridgeTowerProcessor","sections":[],"depth":2},{"title":"BridgeTowerModel","local":"transformers.BridgeTowerModel","sections":[],"depth":2},{"title":"BridgeTowerForContrastiveLearning","local":"transformers.BridgeTowerForContrastiveLearning","sections":[],"depth":2},{"title":"BridgeTowerForMaskedLM","local":"transformers.BridgeTowerForMaskedLM","sections":[],"depth":2},{"title":"BridgeTowerForImageAndTextRetrieval","local":"transformers.BridgeTowerForImageAndTextRetrieval","sections":[],"depth":2}],"depth":1}';function pn(v){return Qr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class bn extends Ar{constructor(n){super(),Yr(this,n,pn,cn,Xr,{})}}export{bn as component};
