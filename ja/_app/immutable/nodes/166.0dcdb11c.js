import{s as St,o as kt,n as Ht}from"../chunks/scheduler.9bc65507.js";import{S as Gt,i as Wt,g as o,s,r as p,A as Rt,h as i,f as l,c as a,j as It,u as m,x as r,k as xt,y as Vt,a as n,v as c,d,t as h,w as M}from"../chunks/index.707bf1b6.js";import{T as Nt}from"../chunks/Tip.c2ecdbf4.js";import{C as me}from"../chunks/CodeBlock.54a9f38d.js";import{H as y}from"../chunks/Heading.342b1fa6.js";function Qt(de){let u,J="これはTorchScriptを使用した実験の最初であり、可変入力サイズのモデルに対するその能力をまだ探求中です。これは私たちの関心の焦点であり、今後のリリースでは、より柔軟な実装や、PythonベースのコードとコンパイルされたTorchScriptを比較するベンチマークを含む、より多くのコード例で詳細な分析を行います。";return{c(){u=o("p"),u.textContent=J},l(f){u=i(f,"P",{"data-svelte-h":!0}),r(u)!=="svelte-ptvqxn"&&(u.textContent=J)},m(f,pe){n(f,u,pe)},p:Ht,d(f){f&&l(u)}}}function Xt(de){let u,J,f,pe,w,he,T,Me,b,st='<a href="https://pytorch.org/docs/stable/jit.html" rel="nofollow">TorchScriptのドキュメント</a>によれば：',ue,$,at="<p>TorchScriptは、PyTorchコードから直列化および最適化可能なモデルを作成する方法です。</p>",fe,U,ot="TorchScriptを使用すると、効率志向のC++プログラムなど、他のプログラムでモデルを再利用できるようになります。PyTorchベースのPythonプログラム以外の環境で🤗 Transformersモデルをエクスポートして使用するためのインターフェースを提供しています。ここでは、TorchScriptを使用してモデルをエクスポートし、使用する方法を説明します。",ye,j,it="モデルをエクスポートするには、次の2つの要件があります：",Te,g,rt="<li><code>torchscript</code>フラグを使用したモデルのインスタンス化</li> <li>ダミーの入力を使用したフォワードパス</li>",Je,v,pt="これらの必要条件は、以下で詳細に説明されているように、開発者が注意する必要があるいくつかのことを意味します。",we,C,be,Z,mt=`<code>torchscript</code>フラグは、ほとんどの🤗 Transformers言語モデルにおいて、<code>Embedding</code>レイヤーと<code>Decoding</code>レイヤー間で重みが連結されているため必要です。
TorchScriptでは、重みが連結されているモデルをエクスポートすることはできませんので、事前に重みを切り離して複製する必要があります。`,$e,_,ct=`<code>torchscript</code>フラグを使用してインスタンス化されたモデルは、<code>Embedding</code>レイヤーと<code>Decoding</code>レイヤーが分離されており、そのため後でトレーニングしてはいけません。
トレーニングは、これらの2つのレイヤーを非同期にする可能性があり、予期しない結果をもたらす可能性があります。`,Ue,B,dt="言語モデルヘッドを持たないモデルには言及しませんが、これらのモデルには連結された重みが存在しないため、<code>torchscript</code>フラグなしで安全にエクスポートできます。",je,I,ge,x,ht="ダミー入力はモデルのフォワードパスに使用されます。入力の値はレイヤーを通じて伝播される間、PyTorchは各テンソルに実行された異なる操作を追跡します。これらの記録された操作は、モデルの<em>トレース</em>を作成するために使用されます。",ve,S,Mt="トレースは入力の寸法に対して作成されます。そのため、ダミー入力の寸法に制約され、他のシーケンス長やバッチサイズでは動作しません。異なるサイズで試すと、以下のエラーが発生します：",Ce,k,Ze,H,ut="お勧めしますのは、モデルの推論中に供給される最大の入力と同じ大きさのダミー入力サイズでモデルをトレースすることです。パディングを使用して不足値を補完することもできます。ただし、モデルがより大きな入力サイズでトレースされるため、行列の寸法も大きくなり、より多くの計算が発生します。",_e,G,ft="異なるシーケンス長のモデルをエクスポートする際に、各入力に対して実行される演算の総数に注意して、パフォーマンスを密接にフォローすることをお勧めします。",Be,W,Ie,R,yt="このセクションでは、モデルの保存と読み込み、および推論にトレースを使用する方法を示します。",xe,V,Se,N,Tt="TorchScriptで<code>BertModel</code>をエクスポートするには、<code>BertConfig</code>クラスから<code>BertModel</code>をインスタンス化し、それをファイル名<code>traced_bert.pt</code>でディスクに保存します：",ke,Q,He,X,Ge,E,Jt="以前に保存した <code>BertModel</code>、<code>traced_bert.pt</code> をディスクから読み込んで、以前に初期化した <code>dummy_input</code> で使用できます。",We,z,Re,L,Ve,A,wt="トレースモデルを使用して推論を行うには、その <code>__call__</code> ダンダーメソッドを使用します。",Ne,P,Qe,F,Xe,Y,bt='AWSはクラウドでの低コストで高性能な機械学習推論向けに <a href="https://aws.amazon.com/ec2/instance-types/inf1/" rel="nofollow">Amazon EC2 Inf1</a> インスタンスファミリーを導入しました。Inf1インスタンスはAWS Inferentiaチップによって駆動され、ディープラーニング推論ワークロードに特化したカスタムビルドのハードウェアアクセラレータです。<a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#" rel="nofollow">AWS Neuron</a> はInferentia用のSDKで、トランスフォーマーモデルをトレースして最適化し、Inf1に展開するためのサポートを提供します。',Ee,D,$t="Neuron SDK が提供するもの:",ze,K,Ut='<li>クラウドでの推論のためにTorchScriptモデルをトレースして最適化するための、1行のコード変更で使用できる簡単なAPI。</li> <li><a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/" rel="nofollow">改善されたコストパフォーマンス</a> のためのボックス外のパフォーマンス最適化。</li> <li><a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html" rel="nofollow">PyTorch</a> または <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html" rel="nofollow">TensorFlow</a> で構築されたHugging Faceトランスフォーマーモデルへのサポート。</li>',Le,q,Ae,O,jt='BERT（Bidirectional Encoder Representations from Transformers）アーキテクチャやその変種（<a href="https://huggingface.co/docs/transformers/main/model_doc/distilbert" rel="nofollow">distilBERT</a> や <a href="https://huggingface.co/docs/transformers/main/model_doc/roberta" rel="nofollow">roBERTa</a> など）に基づくトランスフォーマーモデルは、非生成タスク（抽出型質問応答、シーケンス分類、トークン分類など）において、Inf1上で最適に動作します。ただし、テキスト生成タスクも <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html" rel="nofollow">AWS Neuron MarianMT チュートリアル</a> に従ってInf1上で実行できます。Inferentiaでボックス外で変換できるモデルに関する詳細情報は、Neuronドキュメンテーションの <a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia" rel="nofollow">Model Architecture Fit</a> セクションにあります。',Pe,ee,Fe,te,gt='モデルをAWS Neuronに変換するには、<a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide" rel="nofollow">Neuron SDK 環境</a> が必要で、<a href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html" rel="nofollow">AWS Deep Learning AMI</a> に事前に構成されています。',Ye,le,De,ne,vt='モデルをAWS NEURON用に変換するには、<a href="torchscript#using-torchscript-in-python">PythonでTorchScriptを使用する</a> と同じコードを使用して <code>BertModel</code> をトレースします。Python APIを介してNeuron SDKのコンポーネントにアクセスするために、<code>torch.neuron</code> フレームワーク拡張をインポートします。',Ke,se,qe,ae,Ct="次の行を変更するだけで済みます。",Oe,oe,et,ie,Zt="これにより、Neuron SDKはモデルをトレースし、Inf1インスタンス向けに最適化します。",tt,re,_t='AWS Neuron SDKの機能、ツール、サンプルチュートリアル、最新のアップデートについて詳しく知りたい場合は、<a href="https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html" rel="nofollow">AWS NeuronSDK ドキュメンテーション</a> をご覧ください。',lt,ce,nt;return w=new y({props:{title:"Export to TorchScript",local:"export-to-torchscript",headingTag:"h1"}}),T=new Nt({props:{$$slots:{default:[Qt]},$$scope:{ctx:de}}}),C=new y({props:{title:"TorchScript flag and tied weights",local:"torchscript-flag-and-tied-weights",headingTag:"h2"}}),I=new y({props:{title:"Dummy inputs and standard lengths",local:"dummy-inputs-and-standard-lengths",headingTag:"h2"}}),k=new me({props:{code:"JTYwVGhlJTIwZXhwYW5kZWQlMjBzaXplJTIwb2YlMjB0aGUlMjB0ZW5zb3IlMjAoMyklMjBtdXN0JTIwbWF0Y2glMjB0aGUlMjBleGlzdGluZyUyMHNpemUlMjAoNyklMjBhdCUyMG5vbi1zaW5nbGV0b24lMjBkaW1lbnNpb24lMjAyJTYw",highlighted:'`The expanded <span class="hljs-built_in">size</span> of the tensor (<span class="hljs-number">3</span>) must match the existing <span class="hljs-built_in">size</span> (<span class="hljs-number">7</span>) at non-singleton <span class="hljs-keyword">dimension</span> <span class="hljs-number">2</span>`',wrap:!1}}),W=new y({props:{title:"Using TorchScript in Python",local:"using-torchscript-in-python",headingTag:"h2"}}),V=new y({props:{title:"Saving a model",local:"saving-a-model",headingTag:"h3"}}),Q=new me({props:{code:"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",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel, BertTokenizer, BertConfig
<span class="hljs-keyword">import</span> torch

enc = BertTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)

<span class="hljs-comment"># Tokenizing input text</span>
text = <span class="hljs-string">&quot;[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]&quot;</span>
tokenized_text = enc.tokenize(text)

<span class="hljs-comment"># Masking one of the input tokens</span>
masked_index = <span class="hljs-number">8</span>
tokenized_text[masked_index] = <span class="hljs-string">&quot;[MASK]&quot;</span>
indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)
segments_ids = [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]

<span class="hljs-comment"># Creating a dummy input</span>
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])
dummy_input = [tokens_tensor, segments_tensors]

<span class="hljs-comment"># Initializing the model with the torchscript flag</span>
<span class="hljs-comment"># Flag set to True even though it is not necessary as this model does not have an LM Head.</span>
config = BertConfig(
    vocab_size_or_config_json_file=<span class="hljs-number">32000</span>,
    hidden_size=<span class="hljs-number">768</span>,
    num_hidden_layers=<span class="hljs-number">12</span>,
    num_attention_heads=<span class="hljs-number">12</span>,
    intermediate_size=<span class="hljs-number">3072</span>,
    torchscript=<span class="hljs-literal">True</span>,
)

<span class="hljs-comment"># Instantiating the model</span>
model = BertModel(config)

<span class="hljs-comment"># The model needs to be in evaluation mode</span>
model.<span class="hljs-built_in">eval</span>()

<span class="hljs-comment"># If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag</span>
model = BertModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>, torchscript=<span class="hljs-literal">True</span>)

<span class="hljs-comment"># Creating the trace</span>
traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])
torch.jit.save(traced_model, <span class="hljs-string">&quot;traced_bert.pt&quot;</span>)`,wrap:!1}}),X=new y({props:{title:"Loading a model",local:"loading-a-model",headingTag:"h3"}}),z=new me({props:{code:"bG9hZGVkX21vZGVsJTIwJTNEJTIwdG9yY2guaml0LmxvYWQoJTIydHJhY2VkX2JlcnQucHQlMjIpJTBBbG9hZGVkX21vZGVsLmV2YWwoKSUwQSUwQWFsbF9lbmNvZGVyX2xheWVycyUyQyUyMHBvb2xlZF9vdXRwdXQlMjAlM0QlMjBsb2FkZWRfbW9kZWwoKmR1bW15X2lucHV0KQ==",highlighted:`loaded_model = torch.jit.load(<span class="hljs-string">&quot;traced_bert.pt&quot;</span>)
loaded_model.<span class="hljs-built_in">eval</span>()

all_encoder_layers, pooled_output = loaded_model(*dummy_input)`,wrap:!1}}),L=new y({props:{title:"Using a traced model for inference",local:"using-a-traced-model-for-inference",headingTag:"h3"}}),P=new me({props:{code:"dHJhY2VkX21vZGVsKHRva2Vuc190ZW5zb3IlMkMlMjBzZWdtZW50c190ZW5zb3JzKQ==",highlighted:"traced_model(tokens_tensor, segments_tensors)",wrap:!1}}),F=new y({props:{title:"Deploy Hugging Face TorchScript models to AWS with the Neuron SDK",local:"deploy-hugging-face-torchscript-models-to-aws-with-the-neuron-sdk",headingTag:"h2"}}),q=new y({props:{title:"Implications",local:"implications",headingTag:"h3"}}),ee=new y({props:{title:"Dependencies",local:"dependencies",headingTag:"h3"}}),le=new y({props:{title:"Converting a model for AWS Neuron",local:"converting-a-model-for-aws-neuron",headingTag:"h3"}}),se=new me({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJlcnRNb2RlbCUyQyUyMEJlcnRUb2tlbml6ZXIlMkMlMjBCZXJ0Q29uZmlnJTBBaW1wb3J0JTIwdG9yY2glMEFpbXBvcnQlMjB0b3JjaC5uZXVyb24=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertModel, BertTokenizer, BertConfig
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.neuron`,wrap:!1}}),oe=new me({props:{code:"LSUyMHRvcmNoLmppdC50cmFjZShtb2RlbCUyQyUyMCU1QnRva2Vuc190ZW5zb3IlMkMlMjBzZWdtZW50c190ZW5zb3JzJTVEKSUwQSUyQiUyMHRvcmNoLm5ldXJvbi50cmFjZShtb2RlbCUyQyUyMCU1QnRva2VuX3RlbnNvciUyQyUyMHNlZ21lbnRzX3RlbnNvcnMlNUQp",highlighted:`<span class="hljs-deletion">- torch.jit.trace(model, [tokens_tensor, segments_tensors])</span>
<span class="hljs-addition">+ torch.neuron.trace(model, [token_tensor, segments_tensors])</span>`,wrap:!1}}),{c(){u=o("meta"),J=s(),f=o("p"),pe=s(),p(w.$$.fragment),he=s(),p(T.$$.fragment),Me=s(),b=o("p"),b.innerHTML=st,ue=s(),$=o("blockquote"),$.innerHTML=at,fe=s(),U=o("p"),U.textContent=ot,ye=s(),j=o("p"),j.textContent=it,Te=s(),g=o("ul"),g.innerHTML=rt,Je=s(),v=o("p"),v.textContent=pt,we=s(),p(C.$$.fragment),be=s(),Z=o("p"),Z.innerHTML=mt,$e=s(),_=o("p"),_.innerHTML=ct,Ue=s(),B=o("p"),B.innerHTML=dt,je=s(),p(I.$$.fragment),ge=s(),x=o("p"),x.innerHTML=ht,ve=s(),S=o("p"),S.textContent=Mt,Ce=s(),p(k.$$.fragment),Ze=s(),H=o("p"),H.textContent=ut,_e=s(),G=o("p"),G.textContent=ft,Be=s(),p(W.$$.fragment),Ie=s(),R=o("p"),R.textContent=yt,xe=s(),p(V.$$.fragment),Se=s(),N=o("p"),N.innerHTML=Tt,ke=s(),p(Q.$$.fragment),He=s(),p(X.$$.fragment),Ge=s(),E=o("p"),E.innerHTML=Jt,We=s(),p(z.$$.fragment),Re=s(),p(L.$$.fragment),Ve=s(),A=o("p"),A.innerHTML=wt,Ne=s(),p(P.$$.fragment),Qe=s(),p(F.$$.fragment),Xe=s(),Y=o("p"),Y.innerHTML=bt,Ee=s(),D=o("p"),D.textContent=$t,ze=s(),K=o("ol"),K.innerHTML=Ut,Le=s(),p(q.$$.fragment),Ae=s(),O=o("p"),O.innerHTML=jt,Pe=s(),p(ee.$$.fragment),Fe=s(),te=o("p"),te.innerHTML=gt,Ye=s(),p(le.$$.fragment),De=s(),ne=o("p"),ne.innerHTML=vt,Ke=s(),p(se.$$.fragment),qe=s(),ae=o("p"),ae.textContent=Ct,Oe=s(),p(oe.$$.fragment),et=s(),ie=o("p"),ie.textContent=Zt,tt=s(),re=o("p"),re.innerHTML=_t,lt=s(),ce=o("p"),this.h()},l(e){const t=Rt("svelte-u9bgzb",document.head);u=i(t,"META",{name:!0,content:!0}),t.forEach(l),J=a(e),f=i(e,"P",{}),It(f).forEach(l),pe=a(e),m(w.$$.fragment,e),he=a(e),m(T.$$.fragment,e),Me=a(e),b=i(e,"P",{"data-svelte-h":!0}),r(b)!=="svelte-1y8u98h"&&(b.innerHTML=st),ue=a(e),$=i(e,"BLOCKQUOTE",{"data-svelte-h":!0}),r($)!=="svelte-bbe93u"&&($.innerHTML=at),fe=a(e),U=i(e,"P",{"data-svelte-h":!0}),r(U)!=="svelte-q151zz"&&(U.textContent=ot),ye=a(e),j=i(e,"P",{"data-svelte-h":!0}),r(j)!=="svelte-rp9w2r"&&(j.textContent=it),Te=a(e),g=i(e,"UL",{"data-svelte-h":!0}),r(g)!=="svelte-llh74c"&&(g.innerHTML=rt),Je=a(e),v=i(e,"P",{"data-svelte-h":!0}),r(v)!=="svelte-wcmnvf"&&(v.textContent=pt),we=a(e),m(C.$$.fragment,e),be=a(e),Z=i(e,"P",{"data-svelte-h":!0}),r(Z)!=="svelte-1kh9vu9"&&(Z.innerHTML=mt),$e=a(e),_=i(e,"P",{"data-svelte-h":!0}),r(_)!=="svelte-ixvxgj"&&(_.innerHTML=ct),Ue=a(e),B=i(e,"P",{"data-svelte-h":!0}),r(B)!=="svelte-itcrsj"&&(B.innerHTML=dt),je=a(e),m(I.$$.fragment,e),ge=a(e),x=i(e,"P",{"data-svelte-h":!0}),r(x)!=="svelte-44fh6t"&&(x.innerHTML=ht),ve=a(e),S=i(e,"P",{"data-svelte-h":!0}),r(S)!=="svelte-7rv39"&&(S.textContent=Mt),Ce=a(e),m(k.$$.fragment,e),Ze=a(e),H=i(e,"P",{"data-svelte-h":!0}),r(H)!=="svelte-pderge"&&(H.textContent=ut),_e=a(e),G=i(e,"P",{"data-svelte-h":!0}),r(G)!=="svelte-114ekvc"&&(G.textContent=ft),Be=a(e),m(W.$$.fragment,e),Ie=a(e),R=i(e,"P",{"data-svelte-h":!0}),r(R)!=="svelte-1rrj7ee"&&(R.textContent=yt),xe=a(e),m(V.$$.fragment,e),Se=a(e),N=i(e,"P",{"data-svelte-h":!0}),r(N)!=="svelte-7qqugy"&&(N.innerHTML=Tt),ke=a(e),m(Q.$$.fragment,e),He=a(e),m(X.$$.fragment,e),Ge=a(e),E=i(e,"P",{"data-svelte-h":!0}),r(E)!=="svelte-1ncupfh"&&(E.innerHTML=Jt),We=a(e),m(z.$$.fragment,e),Re=a(e),m(L.$$.fragment,e),Ve=a(e),A=i(e,"P",{"data-svelte-h":!0}),r(A)!=="svelte-an02is"&&(A.innerHTML=wt),Ne=a(e),m(P.$$.fragment,e),Qe=a(e),m(F.$$.fragment,e),Xe=a(e),Y=i(e,"P",{"data-svelte-h":!0}),r(Y)!=="svelte-1tpyqua"&&(Y.innerHTML=bt),Ee=a(e),D=i(e,"P",{"data-svelte-h":!0}),r(D)!=="svelte-1rx34yk"&&(D.textContent=$t),ze=a(e),K=i(e,"OL",{"data-svelte-h":!0}),r(K)!=="svelte-wvdsqe"&&(K.innerHTML=Ut),Le=a(e),m(q.$$.fragment,e),Ae=a(e),O=i(e,"P",{"data-svelte-h":!0}),r(O)!=="svelte-1tfqdw1"&&(O.innerHTML=jt),Pe=a(e),m(ee.$$.fragment,e),Fe=a(e),te=i(e,"P",{"data-svelte-h":!0}),r(te)!=="svelte-1tj3zis"&&(te.innerHTML=gt),Ye=a(e),m(le.$$.fragment,e),De=a(e),ne=i(e,"P",{"data-svelte-h":!0}),r(ne)!=="svelte-4o4276"&&(ne.innerHTML=vt),Ke=a(e),m(se.$$.fragment,e),qe=a(e),ae=i(e,"P",{"data-svelte-h":!0}),r(ae)!=="svelte-1bu3neu"&&(ae.textContent=Ct),Oe=a(e),m(oe.$$.fragment,e),et=a(e),ie=i(e,"P",{"data-svelte-h":!0}),r(ie)!=="svelte-jz1ko9"&&(ie.textContent=Zt),tt=a(e),re=i(e,"P",{"data-svelte-h":!0}),r(re)!=="svelte-14npq7y"&&(re.innerHTML=_t),lt=a(e),ce=i(e,"P",{}),It(ce).forEach(l),this.h()},h(){xt(u,"name","hf:doc:metadata"),xt(u,"content",Et)},m(e,t){Vt(document.head,u),n(e,J,t),n(e,f,t),n(e,pe,t),c(w,e,t),n(e,he,t),c(T,e,t),n(e,Me,t),n(e,b,t),n(e,ue,t),n(e,$,t),n(e,fe,t),n(e,U,t),n(e,ye,t),n(e,j,t),n(e,Te,t),n(e,g,t),n(e,Je,t),n(e,v,t),n(e,we,t),c(C,e,t),n(e,be,t),n(e,Z,t),n(e,$e,t),n(e,_,t),n(e,Ue,t),n(e,B,t),n(e,je,t),c(I,e,t),n(e,ge,t),n(e,x,t),n(e,ve,t),n(e,S,t),n(e,Ce,t),c(k,e,t),n(e,Ze,t),n(e,H,t),n(e,_e,t),n(e,G,t),n(e,Be,t),c(W,e,t),n(e,Ie,t),n(e,R,t),n(e,xe,t),c(V,e,t),n(e,Se,t),n(e,N,t),n(e,ke,t),c(Q,e,t),n(e,He,t),c(X,e,t),n(e,Ge,t),n(e,E,t),n(e,We,t),c(z,e,t),n(e,Re,t),c(L,e,t),n(e,Ve,t),n(e,A,t),n(e,Ne,t),c(P,e,t),n(e,Qe,t),c(F,e,t),n(e,Xe,t),n(e,Y,t),n(e,Ee,t),n(e,D,t),n(e,ze,t),n(e,K,t),n(e,Le,t),c(q,e,t),n(e,Ae,t),n(e,O,t),n(e,Pe,t),c(ee,e,t),n(e,Fe,t),n(e,te,t),n(e,Ye,t),c(le,e,t),n(e,De,t),n(e,ne,t),n(e,Ke,t),c(se,e,t),n(e,qe,t),n(e,ae,t),n(e,Oe,t),c(oe,e,t),n(e,et,t),n(e,ie,t),n(e,tt,t),n(e,re,t),n(e,lt,t),n(e,ce,t),nt=!0},p(e,[t]){const Bt={};t&2&&(Bt.$$scope={dirty:t,ctx:e}),T.$set(Bt)},i(e){nt||(d(w.$$.fragment,e),d(T.$$.fragment,e),d(C.$$.fragment,e),d(I.$$.fragment,e),d(k.$$.fragment,e),d(W.$$.fragment,e),d(V.$$.fragment,e),d(Q.$$.fragment,e),d(X.$$.fragment,e),d(z.$$.fragment,e),d(L.$$.fragment,e),d(P.$$.fragment,e),d(F.$$.fragment,e),d(q.$$.fragment,e),d(ee.$$.fragment,e),d(le.$$.fragment,e),d(se.$$.fragment,e),d(oe.$$.fragment,e),nt=!0)},o(e){h(w.$$.fragment,e),h(T.$$.fragment,e),h(C.$$.fragment,e),h(I.$$.fragment,e),h(k.$$.fragment,e),h(W.$$.fragment,e),h(V.$$.fragment,e),h(Q.$$.fragment,e),h(X.$$.fragment,e),h(z.$$.fragment,e),h(L.$$.fragment,e),h(P.$$.fragment,e),h(F.$$.fragment,e),h(q.$$.fragment,e),h(ee.$$.fragment,e),h(le.$$.fragment,e),h(se.$$.fragment,e),h(oe.$$.fragment,e),nt=!1},d(e){e&&(l(J),l(f),l(pe),l(he),l(Me),l(b),l(ue),l($),l(fe),l(U),l(ye),l(j),l(Te),l(g),l(Je),l(v),l(we),l(be),l(Z),l($e),l(_),l(Ue),l(B),l(je),l(ge),l(x),l(ve),l(S),l(Ce),l(Ze),l(H),l(_e),l(G),l(Be),l(Ie),l(R),l(xe),l(Se),l(N),l(ke),l(He),l(Ge),l(E),l(We),l(Re),l(Ve),l(A),l(Ne),l(Qe),l(Xe),l(Y),l(Ee),l(D),l(ze),l(K),l(Le),l(Ae),l(O),l(Pe),l(Fe),l(te),l(Ye),l(De),l(ne),l(Ke),l(qe),l(ae),l(Oe),l(et),l(ie),l(tt),l(re),l(lt),l(ce)),l(u),M(w,e),M(T,e),M(C,e),M(I,e),M(k,e),M(W,e),M(V,e),M(Q,e),M(X,e),M(z,e),M(L,e),M(P,e),M(F,e),M(q,e),M(ee,e),M(le,e),M(se,e),M(oe,e)}}}const Et='{"title":"Export to TorchScript","local":"export-to-torchscript","sections":[{"title":"TorchScript flag and tied weights","local":"torchscript-flag-and-tied-weights","sections":[],"depth":2},{"title":"Dummy inputs and standard lengths","local":"dummy-inputs-and-standard-lengths","sections":[],"depth":2},{"title":"Using TorchScript in Python","local":"using-torchscript-in-python","sections":[{"title":"Saving a model","local":"saving-a-model","sections":[],"depth":3},{"title":"Loading a model","local":"loading-a-model","sections":[],"depth":3},{"title":"Using a traced model for inference","local":"using-a-traced-model-for-inference","sections":[],"depth":3}],"depth":2},{"title":"Deploy Hugging Face TorchScript models to AWS with the Neuron SDK","local":"deploy-hugging-face-torchscript-models-to-aws-with-the-neuron-sdk","sections":[{"title":"Implications","local":"implications","sections":[],"depth":3},{"title":"Dependencies","local":"dependencies","sections":[],"depth":3},{"title":"Converting a model for AWS Neuron","local":"converting-a-model-for-aws-neuron","sections":[],"depth":3}],"depth":2}],"depth":1}';function zt(de){return kt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Dt extends Gt{constructor(u){super(),Wt(this,u,zt,Xt,St,{})}}export{Dt as component};
