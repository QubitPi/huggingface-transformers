import{s as Ye,f as Oe,o as Ke,n as be}from"../chunks/scheduler.9bc65507.js";import{S as et,i as tt,g as m,s as l,r as C,A as ot,h as p,f as a,c as i,j as ce,u as x,x as b,k as O,y as h,a as r,v as y,d as $,t as T,w as M}from"../chunks/index.707bf1b6.js";import{T as We}from"../chunks/Tip.c2ecdbf4.js";import{D as ue}from"../chunks/Docstring.17db21ae.js";import{C as qe}from"../chunks/CodeBlock.54a9f38d.js";import{F as nt,M as De}from"../chunks/Markdown.8ab98a13.js";import{E as Je}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as st}from"../chunks/PipelineTag.44585822.js";import{H as $e}from"../chunks/Heading.342b1fa6.js";function at(F){let e,f="Example:",o,s,g;return s=new qe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENvbnZOZXh0Q29uZmlnJTJDJTIwQ29udk5leHRNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDb252TmV4dCUyMGNvbnZuZXh0LXRpbnktMjI0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMENvbnZOZXh0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGNvbnZuZXh0LXRpbnktMjI0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBDb252TmV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ConvNextConfig, ConvNextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ConvNext convnext-tiny-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ConvNextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the convnext-tiny-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ConvNextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){e=m("p"),e.textContent=f,o=l(),C(s.$$.fragment)},l(n){e=p(n,"P",{"data-svelte-h":!0}),b(e)!=="svelte-11lpom8"&&(e.textContent=f),o=i(n),x(s.$$.fragment,n)},m(n,v){r(n,e,v),r(n,o,v),y(s,n,v),g=!0},p:be,i(n){g||($(s.$$.fragment,n),g=!0)},o(n){T(s.$$.fragment,n),g=!1},d(n){n&&(a(e),a(o)),M(s,n)}}}function rt(F){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=m("p"),e.innerHTML=f},l(o){e=p(o,"P",{"data-svelte-h":!0}),b(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(o,s){r(o,e,s)},p:be,d(o){o&&a(e)}}}function lt(F){let e,f="Example:",o,s,g;return s=new qe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMENvbnZOZXh0TW9kZWwlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGY29udm5leHQtdGlueS0yMjQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDb252TmV4dE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmNvbnZuZXh0LXRpbnktMjI0JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ConvNextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnext-tiny-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ConvNextModel.from_pretrained(<span class="hljs-string">&quot;facebook/convnext-tiny-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">768</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`,wrap:!1}}),{c(){e=m("p"),e.textContent=f,o=l(),C(s.$$.fragment)},l(n){e=p(n,"P",{"data-svelte-h":!0}),b(e)!=="svelte-11lpom8"&&(e.textContent=f),o=i(n),x(s.$$.fragment,n)},m(n,v){r(n,e,v),r(n,o,v),y(s,n,v),g=!0},p:be,i(n){g||($(s.$$.fragment,n),g=!0)},o(n){T(s.$$.fragment,n),g=!1},d(n){n&&(a(e),a(o)),M(s,n)}}}function it(F){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=m("p"),e.innerHTML=f},l(o){e=p(o,"P",{"data-svelte-h":!0}),b(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(o,s){r(o,e,s)},p:be,d(o){o&&a(e)}}}function ct(F){let e,f="Example:",o,s,g;return s=new qe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMENvbnZOZXh0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dC10aW55LTIyNCUyMiklMEFtb2RlbCUyMCUzRCUyMENvbnZOZXh0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dC10aW55LTIyNCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ConvNextForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnext-tiny-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ConvNextForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/convnext-tiny-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){e=m("p"),e.textContent=f,o=l(),C(s.$$.fragment)},l(n){e=p(n,"P",{"data-svelte-h":!0}),b(e)!=="svelte-11lpom8"&&(e.textContent=f),o=i(n),x(s.$$.fragment,n)},m(n,v){r(n,e,v),r(n,o,v),y(s,n,v),g=!0},p:be,i(n){g||($(s.$$.fragment,n),g=!0)},o(n){T(s.$$.fragment,n),g=!1},d(n){n&&(a(e),a(o)),M(s,n)}}}function dt(F){let e,f,o,s,g,n,v=`The bare ConvNext model outputting raw features without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,G,N,Z,Q,I,R='The <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> forward method, overrides the <code>__call__</code> special method.',k,c,U,B,K,re,V,J,X,de,L,Ce=`ConvNext Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,W,H,he=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,S,A,ee,D,le,ie='The <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',me,te,se,E,pe;return e=new $e({props:{title:"ConvNextModel",local:"transformers.ConvNextModel",headingTag:"h2"}}),s=new ue({props:{name:"class transformers.ConvNextModel",anchor:"transformers.ConvNextModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ConvNextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/modeling_convnext.py#L324"}}),Z=new ue({props:{name:"forward",anchor:"transformers.ConvNextModel.forward",parameters:[{name:"pixel_values",val:": FloatTensor = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ConvNextModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.ConvNextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ConvNextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/modeling_convnext.py#L342",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig"
>ConvNextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),c=new We({props:{$$slots:{default:[rt]},$$scope:{ctx:F}}}),B=new Je({props:{anchor:"transformers.ConvNextModel.forward.example",$$slots:{default:[lt]},$$scope:{ctx:F}}}),re=new $e({props:{title:"ConvNextForImageClassification",local:"transformers.ConvNextForImageClassification",headingTag:"h2"}}),X=new ue({props:{name:"class transformers.ConvNextForImageClassification",anchor:"transformers.ConvNextForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ConvNextForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/modeling_convnext.py#L387"}}),ee=new ue({props:{name:"forward",anchor:"transformers.ConvNextForImageClassification.forward",parameters:[{name:"pixel_values",val:": FloatTensor = None"},{name:"labels",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ConvNextForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.ConvNextForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ConvNextForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ConvNextForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/modeling_convnext.py#L409",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig"
>ConvNextConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),te=new We({props:{$$slots:{default:[it]},$$scope:{ctx:F}}}),E=new Je({props:{anchor:"transformers.ConvNextForImageClassification.forward.example",$$slots:{default:[ct]},$$scope:{ctx:F}}}),{c(){C(e.$$.fragment),f=l(),o=m("div"),C(s.$$.fragment),g=l(),n=m("p"),n.innerHTML=v,G=l(),N=m("div"),C(Z.$$.fragment),Q=l(),I=m("p"),I.innerHTML=R,k=l(),C(c.$$.fragment),U=l(),C(B.$$.fragment),K=l(),C(re.$$.fragment),V=l(),J=m("div"),C(X.$$.fragment),de=l(),L=m("p"),L.textContent=Ce,W=l(),H=m("p"),H.innerHTML=he,S=l(),A=m("div"),C(ee.$$.fragment),D=l(),le=m("p"),le.innerHTML=ie,me=l(),C(te.$$.fragment),se=l(),C(E.$$.fragment),this.h()},l(d){x(e.$$.fragment,d),f=i(d),o=p(d,"DIV",{class:!0});var j=ce(o);x(s.$$.fragment,j),g=i(j),n=p(j,"P",{"data-svelte-h":!0}),b(n)!=="svelte-22827l"&&(n.innerHTML=v),G=i(j),N=p(j,"DIV",{class:!0});var z=ce(N);x(Z.$$.fragment,z),Q=i(z),I=p(z,"P",{"data-svelte-h":!0}),b(I)!=="svelte-15742xo"&&(I.innerHTML=R),k=i(z),x(c.$$.fragment,z),U=i(z),x(B.$$.fragment,z),z.forEach(a),j.forEach(a),K=i(d),x(re.$$.fragment,d),V=i(d),J=p(d,"DIV",{class:!0});var P=ce(J);x(X.$$.fragment,P),de=i(P),L=p(P,"P",{"data-svelte-h":!0}),b(L)!=="svelte-xy24s5"&&(L.textContent=Ce),W=i(P),H=p(P,"P",{"data-svelte-h":!0}),b(H)!=="svelte-1gjh92c"&&(H.innerHTML=he),S=i(P),A=p(P,"DIV",{class:!0});var Y=ce(A);x(ee.$$.fragment,Y),D=i(Y),le=p(Y,"P",{"data-svelte-h":!0}),b(le)!=="svelte-1w05j62"&&(le.innerHTML=ie),me=i(Y),x(te.$$.fragment,Y),se=i(Y),x(E.$$.fragment,Y),Y.forEach(a),P.forEach(a),this.h()},h(){O(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(o,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(d,j){y(e,d,j),r(d,f,j),r(d,o,j),y(s,o,null),h(o,g),h(o,n),h(o,G),h(o,N),y(Z,N,null),h(N,Q),h(N,I),h(N,k),y(c,N,null),h(N,U),y(B,N,null),r(d,K,j),y(re,d,j),r(d,V,j),r(d,J,j),y(X,J,null),h(J,de),h(J,L),h(J,W),h(J,H),h(J,S),h(J,A),y(ee,A,null),h(A,D),h(A,le),h(A,me),y(te,A,null),h(A,se),y(E,A,null),pe=!0},p(d,j){const z={};j&2&&(z.$$scope={dirty:j,ctx:d}),c.$set(z);const P={};j&2&&(P.$$scope={dirty:j,ctx:d}),B.$set(P);const Y={};j&2&&(Y.$$scope={dirty:j,ctx:d}),te.$set(Y);const fe={};j&2&&(fe.$$scope={dirty:j,ctx:d}),E.$set(fe)},i(d){pe||($(e.$$.fragment,d),$(s.$$.fragment,d),$(Z.$$.fragment,d),$(c.$$.fragment,d),$(B.$$.fragment,d),$(re.$$.fragment,d),$(X.$$.fragment,d),$(ee.$$.fragment,d),$(te.$$.fragment,d),$(E.$$.fragment,d),pe=!0)},o(d){T(e.$$.fragment,d),T(s.$$.fragment,d),T(Z.$$.fragment,d),T(c.$$.fragment,d),T(B.$$.fragment,d),T(re.$$.fragment,d),T(X.$$.fragment,d),T(ee.$$.fragment,d),T(te.$$.fragment,d),T(E.$$.fragment,d),pe=!1},d(d){d&&(a(f),a(o),a(K),a(V),a(J)),M(e,d),M(s),M(Z),M(c),M(B),M(re,d),M(X),M(ee),M(te),M(E)}}}function mt(F){let e,f;return e=new De({props:{$$slots:{default:[dt]},$$scope:{ctx:F}}}),{c(){C(e.$$.fragment)},l(o){x(e.$$.fragment,o)},m(o,s){y(e,o,s),f=!0},p(o,s){const g={};s&2&&(g.$$scope={dirty:s,ctx:o}),e.$set(g)},i(o){f||($(e.$$.fragment,o),f=!0)},o(o){T(e.$$.fragment,o),f=!1},d(o){M(e,o)}}}function pt(F){let e,f="TensorFlow models and layers in <code>transformers</code> accept two formats as input:",o,s,g="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional argument.</li>",n,v,G=`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like <code>model.fit()</code> things should “just work” for you - just
pass your inputs and labels in any format that <code>model.fit()</code> supports! If, however, you want to use the second
format outside of Keras methods like <code>fit()</code> and <code>predict()</code>, such as when creating your own layers or models with
the Keras <code>Functional</code> API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`,N,Z,Q=`<li>a single Tensor with <code>pixel_values</code> only and nothing else: <code>model(pixel_values)</code></li> <li>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code>model([pixel_values, attention_mask])</code> or <code>model([pixel_values, attention_mask, token_type_ids])</code></li> <li>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code>model({&quot;pixel_values&quot;: pixel_values, &quot;token_type_ids&quot;: token_type_ids})</code></li>`,I,R,k=`Note that when creating models and layers with
<a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="nofollow">subclassing</a> then you don’t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`;return{c(){e=m("p"),e.innerHTML=f,o=l(),s=m("ul"),s.innerHTML=g,n=l(),v=m("p"),v.innerHTML=G,N=l(),Z=m("ul"),Z.innerHTML=Q,I=l(),R=m("p"),R.innerHTML=k},l(c){e=p(c,"P",{"data-svelte-h":!0}),b(e)!=="svelte-1ajbfxg"&&(e.innerHTML=f),o=i(c),s=p(c,"UL",{"data-svelte-h":!0}),b(s)!=="svelte-qm1t26"&&(s.innerHTML=g),n=i(c),v=p(c,"P",{"data-svelte-h":!0}),b(v)!=="svelte-1v9qsc5"&&(v.innerHTML=G),N=i(c),Z=p(c,"UL",{"data-svelte-h":!0}),b(Z)!=="svelte-99h8aq"&&(Z.innerHTML=Q),I=i(c),R=p(c,"P",{"data-svelte-h":!0}),b(R)!=="svelte-1an3odd"&&(R.innerHTML=k)},m(c,U){r(c,e,U),r(c,o,U),r(c,s,U),r(c,n,U),r(c,v,U),r(c,N,U),r(c,Z,U),r(c,I,U),r(c,R,U)},p:be,d(c){c&&(a(e),a(o),a(s),a(n),a(v),a(N),a(Z),a(I),a(R))}}}function ft(F){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=m("p"),e.innerHTML=f},l(o){e=p(o,"P",{"data-svelte-h":!0}),b(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(o,s){r(o,e,s)},p:be,d(o){o&&a(e)}}}function ut(F){let e,f="Examples:",o,s,g;return s=new qe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGQ29udk5leHRNb2RlbCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dC10aW55LTIyNCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQ29udk5leHRNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dC10aW55LTIyNCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFConvNextModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnext-tiny-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFConvNextModel.from_pretrained(<span class="hljs-string">&quot;facebook/convnext-tiny-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){e=m("p"),e.textContent=f,o=l(),C(s.$$.fragment)},l(n){e=p(n,"P",{"data-svelte-h":!0}),b(e)!=="svelte-kvfsh7"&&(e.textContent=f),o=i(n),x(s.$$.fragment,n)},m(n,v){r(n,e,v),r(n,o,v),y(s,n,v),g=!0},p:be,i(n){g||($(s.$$.fragment,n),g=!0)},o(n){T(s.$$.fragment,n),g=!1},d(n){n&&(a(e),a(o)),M(s,n)}}}function ht(F){let e,f="TensorFlow models and layers in <code>transformers</code> accept two formats as input:",o,s,g="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional argument.</li>",n,v,G=`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like <code>model.fit()</code> things should “just work” for you - just
pass your inputs and labels in any format that <code>model.fit()</code> supports! If, however, you want to use the second
format outside of Keras methods like <code>fit()</code> and <code>predict()</code>, such as when creating your own layers or models with
the Keras <code>Functional</code> API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`,N,Z,Q=`<li>a single Tensor with <code>pixel_values</code> only and nothing else: <code>model(pixel_values)</code></li> <li>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code>model([pixel_values, attention_mask])</code> or <code>model([pixel_values, attention_mask, token_type_ids])</code></li> <li>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code>model({&quot;pixel_values&quot;: pixel_values, &quot;token_type_ids&quot;: token_type_ids})</code></li>`,I,R,k=`Note that when creating models and layers with
<a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="nofollow">subclassing</a> then you don’t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`;return{c(){e=m("p"),e.innerHTML=f,o=l(),s=m("ul"),s.innerHTML=g,n=l(),v=m("p"),v.innerHTML=G,N=l(),Z=m("ul"),Z.innerHTML=Q,I=l(),R=m("p"),R.innerHTML=k},l(c){e=p(c,"P",{"data-svelte-h":!0}),b(e)!=="svelte-1ajbfxg"&&(e.innerHTML=f),o=i(c),s=p(c,"UL",{"data-svelte-h":!0}),b(s)!=="svelte-qm1t26"&&(s.innerHTML=g),n=i(c),v=p(c,"P",{"data-svelte-h":!0}),b(v)!=="svelte-1v9qsc5"&&(v.innerHTML=G),N=i(c),Z=p(c,"UL",{"data-svelte-h":!0}),b(Z)!=="svelte-99h8aq"&&(Z.innerHTML=Q),I=i(c),R=p(c,"P",{"data-svelte-h":!0}),b(R)!=="svelte-1an3odd"&&(R.innerHTML=k)},m(c,U){r(c,e,U),r(c,o,U),r(c,s,U),r(c,n,U),r(c,v,U),r(c,N,U),r(c,Z,U),r(c,I,U),r(c,R,U)},p:be,d(c){c&&(a(e),a(o),a(s),a(n),a(v),a(N),a(Z),a(I),a(R))}}}function gt(F){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=m("p"),e.innerHTML=f},l(o){e=p(o,"P",{"data-svelte-h":!0}),b(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(o,s){r(o,e,s)},p:be,d(o){o&&a(e)}}}function _t(F){let e,f="Examples:",o,s,g;return s=new qe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGQ29udk5leHRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBaW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGY29udm5leHQtdGlueS0yMjQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkNvbnZOZXh0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dC10aW55LTIyNCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cyUwQSUyMyUyMG1vZGVsJTIwcHJlZGljdHMlMjBvbmUlMjBvZiUyMHRoZSUyMDEwMDAlMjBJbWFnZU5ldCUyMGNsYXNzZXMlMEFwcmVkaWN0ZWRfY2xhc3NfaWR4JTIwJTNEJTIwdGYubWF0aC5hcmdtYXgobG9naXRzJTJDJTIwYXhpcyUzRC0xKSU1QjAlNUQlMEFwcmludCglMjJQcmVkaWN0ZWQlMjBjbGFzcyUzQSUyMiUyQyUyMG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QmludChwcmVkaWN0ZWRfY2xhc3NfaWR4KSU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFConvNextForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnext-tiny-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFConvNextForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/convnext-tiny-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_idx = tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[<span class="hljs-built_in">int</span>(predicted_class_idx)])`,wrap:!1}}),{c(){e=m("p"),e.textContent=f,o=l(),C(s.$$.fragment)},l(n){e=p(n,"P",{"data-svelte-h":!0}),b(e)!=="svelte-kvfsh7"&&(e.textContent=f),o=i(n),x(s.$$.fragment,n)},m(n,v){r(n,e,v),r(n,o,v),y(s,n,v),g=!0},p:be,i(n){g||($(s.$$.fragment,n),g=!0)},o(n){T(s.$$.fragment,n),g=!1},d(n){n&&(a(e),a(o)),M(s,n)}}}function vt(F){let e,f,o,s,g,n,v=`The bare ConvNext model outputting raw features without any specific head on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,G,N,Z=`This model is also a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a> subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`,Q,I,R,k,c,U,B,K='The <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> forward method, overrides the <code>__call__</code> special method.',re,V,J,X,de,L,Ce,W,H,he,S,A=`ConvNext Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,ee,D,le=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ie,me,te=`This model is also a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a> subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`,se,E,pe,d,j,z,P,Y='The <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',fe,ge,we,ae,Ne;return e=new $e({props:{title:"TFConvNextModel",local:"transformers.TFConvNextModel",headingTag:"h2"}}),s=new ue({props:{name:"class transformers.TFConvNextModel",anchor:"transformers.TFConvNextModel",parameters:[{name:"config",val:""},{name:"*inputs",val:""},{name:"add_pooling_layer",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFConvNextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/modeling_tf_convnext.py#L492"}}),I=new We({props:{$$slots:{default:[pt]},$$scope:{ctx:F}}}),c=new ue({props:{name:"call",anchor:"transformers.TFConvNextModel.call",parameters:[{name:"pixel_values",val:": TFModelInputType | None = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFConvNextModel.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFConvNextModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFConvNextModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/modeling_tf_convnext.py#L501",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"
>transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig"
>ConvNextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you’re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"
>transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling</a> or <code>tuple(tf.Tensor)</code></p>
`}}),V=new We({props:{$$slots:{default:[ft]},$$scope:{ctx:F}}}),X=new Je({props:{anchor:"transformers.TFConvNextModel.call.example",$$slots:{default:[ut]},$$scope:{ctx:F}}}),L=new $e({props:{title:"TFConvNextForImageClassification",local:"transformers.TFConvNextForImageClassification",headingTag:"h2"}}),H=new ue({props:{name:"class transformers.TFConvNextForImageClassification",anchor:"transformers.TFConvNextForImageClassification",parameters:[{name:"config",val:": ConvNextConfig"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFConvNextForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/modeling_tf_convnext.py#L564"}}),E=new We({props:{$$slots:{default:[ht]},$$scope:{ctx:F}}}),j=new ue({props:{name:"call",anchor:"transformers.TFConvNextForImageClassification.call",parameters:[{name:"pixel_values",val:": TFModelInputType | None = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"labels",val:": np.ndarray | tf.Tensor | None = None"},{name:"training",val:": Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFConvNextForImageClassification.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFConvNextForImageClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFConvNextForImageClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFConvNextForImageClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> or <code>np.ndarray</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/modeling_tf_convnext.py#L587",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig"
>ConvNextConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, )</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_tf_outputs.TFSequenceClassifierOutput"
>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ge=new We({props:{$$slots:{default:[gt]},$$scope:{ctx:F}}}),ae=new Je({props:{anchor:"transformers.TFConvNextForImageClassification.call.example",$$slots:{default:[_t]},$$scope:{ctx:F}}}),{c(){C(e.$$.fragment),f=l(),o=m("div"),C(s.$$.fragment),g=l(),n=m("p"),n.innerHTML=v,G=l(),N=m("p"),N.innerHTML=Z,Q=l(),C(I.$$.fragment),R=l(),k=m("div"),C(c.$$.fragment),U=l(),B=m("p"),B.innerHTML=K,re=l(),C(V.$$.fragment),J=l(),C(X.$$.fragment),de=l(),C(L.$$.fragment),Ce=l(),W=m("div"),C(H.$$.fragment),he=l(),S=m("p"),S.textContent=A,ee=l(),D=m("p"),D.innerHTML=le,ie=l(),me=m("p"),me.innerHTML=te,se=l(),C(E.$$.fragment),pe=l(),d=m("div"),C(j.$$.fragment),z=l(),P=m("p"),P.innerHTML=Y,fe=l(),C(ge.$$.fragment),we=l(),C(ae.$$.fragment),this.h()},l(u){x(e.$$.fragment,u),f=i(u),o=p(u,"DIV",{class:!0});var w=ce(o);x(s.$$.fragment,w),g=i(w),n=p(w,"P",{"data-svelte-h":!0}),b(n)!=="svelte-sgqamf"&&(n.innerHTML=v),G=i(w),N=p(w,"P",{"data-svelte-h":!0}),b(N)!=="svelte-1be7e3c"&&(N.innerHTML=Z),Q=i(w),x(I.$$.fragment,w),R=i(w),k=p(w,"DIV",{class:!0});var oe=ce(k);x(c.$$.fragment,oe),U=i(oe),B=p(oe,"P",{"data-svelte-h":!0}),b(B)!=="svelte-1buimc"&&(B.innerHTML=K),re=i(oe),x(V.$$.fragment,oe),J=i(oe),x(X.$$.fragment,oe),oe.forEach(a),w.forEach(a),de=i(u),x(L.$$.fragment,u),Ce=i(u),W=p(u,"DIV",{class:!0});var q=ce(W);x(H.$$.fragment,q),he=i(q),S=p(q,"P",{"data-svelte-h":!0}),b(S)!=="svelte-xy24s5"&&(S.textContent=A),ee=i(q),D=p(q,"P",{"data-svelte-h":!0}),b(D)!=="svelte-x53t1u"&&(D.innerHTML=le),ie=i(q),me=p(q,"P",{"data-svelte-h":!0}),b(me)!=="svelte-1be7e3c"&&(me.innerHTML=te),se=i(q),x(E.$$.fragment,q),pe=i(q),d=p(q,"DIV",{class:!0});var ne=ce(d);x(j.$$.fragment,ne),z=i(ne),P=p(ne,"P",{"data-svelte-h":!0}),b(P)!=="svelte-h88lg6"&&(P.innerHTML=Y),fe=i(ne),x(ge.$$.fragment,ne),we=i(ne),x(ae.$$.fragment,ne),ne.forEach(a),q.forEach(a),this.h()},h(){O(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(o,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(d,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(u,w){y(e,u,w),r(u,f,w),r(u,o,w),y(s,o,null),h(o,g),h(o,n),h(o,G),h(o,N),h(o,Q),y(I,o,null),h(o,R),h(o,k),y(c,k,null),h(k,U),h(k,B),h(k,re),y(V,k,null),h(k,J),y(X,k,null),r(u,de,w),y(L,u,w),r(u,Ce,w),r(u,W,w),y(H,W,null),h(W,he),h(W,S),h(W,ee),h(W,D),h(W,ie),h(W,me),h(W,se),y(E,W,null),h(W,pe),h(W,d),y(j,d,null),h(d,z),h(d,P),h(d,fe),y(ge,d,null),h(d,we),y(ae,d,null),Ne=!0},p(u,w){const oe={};w&2&&(oe.$$scope={dirty:w,ctx:u}),I.$set(oe);const q={};w&2&&(q.$$scope={dirty:w,ctx:u}),V.$set(q);const ne={};w&2&&(ne.$$scope={dirty:w,ctx:u}),X.$set(ne);const xe={};w&2&&(xe.$$scope={dirty:w,ctx:u}),E.$set(xe);const ye={};w&2&&(ye.$$scope={dirty:w,ctx:u}),ge.$set(ye);const je={};w&2&&(je.$$scope={dirty:w,ctx:u}),ae.$set(je)},i(u){Ne||($(e.$$.fragment,u),$(s.$$.fragment,u),$(I.$$.fragment,u),$(c.$$.fragment,u),$(V.$$.fragment,u),$(X.$$.fragment,u),$(L.$$.fragment,u),$(H.$$.fragment,u),$(E.$$.fragment,u),$(j.$$.fragment,u),$(ge.$$.fragment,u),$(ae.$$.fragment,u),Ne=!0)},o(u){T(e.$$.fragment,u),T(s.$$.fragment,u),T(I.$$.fragment,u),T(c.$$.fragment,u),T(V.$$.fragment,u),T(X.$$.fragment,u),T(L.$$.fragment,u),T(H.$$.fragment,u),T(E.$$.fragment,u),T(j.$$.fragment,u),T(ge.$$.fragment,u),T(ae.$$.fragment,u),Ne=!1},d(u){u&&(a(f),a(o),a(de),a(Ce),a(W)),M(e,u),M(s),M(I),M(c),M(V),M(X),M(L,u),M(H),M(E),M(j),M(ge),M(ae)}}}function bt(F){let e,f;return e=new De({props:{$$slots:{default:[vt]},$$scope:{ctx:F}}}),{c(){C(e.$$.fragment)},l(o){x(e.$$.fragment,o)},m(o,s){y(e,o,s),f=!0},p(o,s){const g={};s&2&&(g.$$scope={dirty:s,ctx:o}),e.$set(g)},i(o){f||($(e.$$.fragment,o),f=!0)},o(o){T(e.$$.fragment,o),f=!1},d(o){M(e,o)}}}function Ct(F){let e,f,o,s,g,n,v,G,N,Z=`ConvNeXT モデルは、<a href="https://arxiv.org/abs/2201.03545" rel="nofollow">A ConvNet for the 2020s</a> で Zhuang Liu、Hanzi Mao、Chao-Yuan Wu、Christoph Feichtenhofer、Trevor Darrell、Saining Xie によって提案されました。
ConvNeXT は、ビジョン トランスフォーマーの設計からインスピレーションを得た純粋な畳み込みモデル (ConvNet) であり、ビジョン トランスフォーマーよりも優れたパフォーマンスを発揮すると主張しています。`,Q,I,R="論文の要約は次のとおりです。",k,c,U=`<em>視覚認識の「狂騒の 20 年代」は、最先端の画像分類モデルとして ConvNet にすぐに取って代わられた Vision Transformers (ViT) の導入から始まりました。
一方、バニラ ViT は、オブジェクト検出やセマンティック セグメンテーションなどの一般的なコンピューター ビジョン タスクに適用すると困難に直面します。階層型トランスフォーマーです
(Swin Transformers など) は、いくつかの ConvNet の以前の機能を再導入し、Transformers を汎用ビジョン バックボーンとして実用的に可能にし、幅広い環境で顕著なパフォーマンスを実証しました。
さまざまな視覚タスク。ただし、このようなハイブリッド アプローチの有効性は、依然として、固有の誘導性ではなく、トランスフォーマーの本質的な優位性によるところが大きいと考えられています。
畳み込みのバイアス。この作業では、設計空間を再検討し、純粋な ConvNet が達成できる限界をテストします。標準 ResNet を設計に向けて徐々に「最新化」します。
ビジョン Transformer の概要を確認し、途中でパフォーマンスの違いに寄与するいくつかの重要なコンポーネントを発見します。この調査の結果は、純粋な ConvNet モデルのファミリーです。
ConvNextと呼ばれます。 ConvNeXts は完全に標準の ConvNet モジュールから構築されており、精度と拡張性の点で Transformers と有利に競合し、87.8% の ImageNet トップ 1 精度を達成しています。
標準 ConvNet のシンプルさと効率を維持しながら、COCO 検出と ADE20K セグメンテーションでは Swin Transformers よりも優れたパフォーマンスを発揮します。</em>`,B,K,re,V,J,X='ConvNeXT アーキテクチャ。 <a href="https://arxiv.org/abs/2201.03545">元の論文</a>から抜粋。',de,L,Ce=`このモデルは、<a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a> によって提供されました。 TensorFlow バージョンのモデルは <a href="https://github.com/ariG23498" rel="nofollow">ariG23498</a> によって提供されました。
<a href="https://github.com/gante" rel="nofollow">gante</a>、および <a href="https://github.com/sayakpaul" rel="nofollow">sayakpaul</a> (同等の貢献)。元のコードは <a href="https://github.com/facebookresearch/ConvNeXt" rel="nofollow">こちら</a> にあります。`,W,H,he,S,A="ConvNeXT の使用を開始するのに役立つ公式 Hugging Face およびコミュニティ (🌎 で示される) リソースのリスト。",ee,D,le,ie,me='<li><a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> は、この <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">サンプル スクリプト</a> および <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">ノートブック</a>。</li> <li>参照: <a href="../tasks/image_classification">画像分類タスク ガイド</a></li>',te,se,E="ここに含めるリソースの送信に興味がある場合は、お気軽にプル リクエストを開いてください。審査させていただきます。リソースは、既存のリソースを複製するのではなく、何か新しいものを示すことが理想的です。",pe,d,j,z,P,Y,fe,ge=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a>. It is used to instantiate an
ConvNeXT model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the ConvNeXT
<a href="https://huggingface.co/facebook/convnext-tiny-224" rel="nofollow">facebook/convnext-tiny-224</a> architecture.`,we,ae,Ne=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,u,w,oe,q,ne,xe,ye,je,Ie,Le,_e,Fe,Ge,Ze,Se="Constructs a ConvNeXT image processor.",Qe,Te,Ue,Ve,ze,Ae="Preprocess an image or batch of images.",He,Me,Be,Pe,Ee;return g=new $e({props:{title:"ConvNeXT",local:"convnext",headingTag:"h1"}}),v=new $e({props:{title:"Overview",local:"overview",headingTag:"h2"}}),H=new $e({props:{title:"Resources",local:"resources",headingTag:"h2"}}),D=new st({props:{pipeline:"image-classification"}}),d=new $e({props:{title:"ConvNextConfig",local:"transformers.ConvNextConfig",headingTag:"h2"}}),P=new ue({props:{name:"class transformers.ConvNextConfig",anchor:"transformers.ConvNextConfig",parameters:[{name:"num_channels",val:" = 3"},{name:"patch_size",val:" = 4"},{name:"num_stages",val:" = 4"},{name:"hidden_sizes",val:" = None"},{name:"depths",val:" = None"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"layer_scale_init_value",val:" = 1e-06"},{name:"drop_path_rate",val:" = 0.0"},{name:"image_size",val:" = 224"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ConvNextConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.ConvNextConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, optional, defaults to 4) &#x2014;
Patch size to use in the patch embedding layer.`,name:"patch_size"},{anchor:"transformers.ConvNextConfig.num_stages",description:`<strong>num_stages</strong> (<code>int</code>, optional, defaults to 4) &#x2014;
The number of stages in the model.`,name:"num_stages"},{anchor:"transformers.ConvNextConfig.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [96, 192, 384, 768]) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.ConvNextConfig.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to [3, 3, 9, 3]) &#x2014;
Depth (number of blocks) for each stage.`,name:"depths"},{anchor:"transformers.ConvNextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each block. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.ConvNextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ConvNextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.ConvNextConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The initial value for the layer scale.`,name:"layer_scale_init_value"},{anchor:"transformers.ConvNextConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The drop rate for stochastic depth.`,name:"drop_path_rate"},{anchor:"transformers.ConvNextConfig.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.ConvNextConfig.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/configuration_convnext.py#L36"}}),w=new Je({props:{anchor:"transformers.ConvNextConfig.example",$$slots:{default:[at]},$$scope:{ctx:F}}}),q=new $e({props:{title:"ConvNextFeatureExtractor",local:"transformers.ConvNextFeatureExtractor",headingTag:"h2"}}),ye=new ue({props:{name:"class transformers.ConvNextFeatureExtractor",anchor:"transformers.ConvNextFeatureExtractor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/feature_extraction_convnext.py#L26"}}),Ie=new $e({props:{title:"ConvNextImageProcessor",local:"transformers.ConvNextImageProcessor",headingTag:"h2"}}),Fe=new ue({props:{name:"class transformers.ConvNextImageProcessor",anchor:"transformers.ConvNextImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": Dict = None"},{name:"crop_pct",val:": float = None"},{name:"resample",val:": Resampling = <Resampling.BILINEAR: 2>"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": Union = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ConvNextImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Controls whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overriden
by <code>do_resize</code> in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.ConvNextImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;shortest_edge&quot; -- 384}</code>):
Resolution of the output image after <code>resize</code> is applied. If <code>size[&quot;shortest_edge&quot;]</code> &gt;= 384, the image is
resized to <code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Otherwise, the smaller edge of the image will
be matched to <code>int(size[&quot;shortest_edge&quot;]/crop_pct)</code>, after which the image is cropped to
<code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Only has an effect if <code>do_resize</code> is set to <code>True</code>. Can
be overriden by <code>size</code> in the <code>preprocess</code> method.`,name:"size"},{anchor:"transformers.ConvNextImageProcessor.crop_pct",description:`<strong>crop_pct</strong> (<code>float</code> <em>optional</em>, defaults to 224 / 256) &#x2014;
Percentage of the image to crop. Only has an effect if <code>do_resize</code> is <code>True</code> and size &lt; 384. Can be
overriden by <code>crop_pct</code> in the <code>preprocess</code> method.`,name:"crop_pct"},{anchor:"transformers.ConvNextImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BILINEAR</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overriden by <code>resample</code> in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.ConvNextImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overriden by <code>do_rescale</code> in
the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.ConvNextImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overriden by <code>rescale_factor</code> in the <code>preprocess</code>
method.`,name:"rescale_factor"},{anchor:"transformers.ConvNextImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method.`,name:"do_normalize"},{anchor:"transformers.ConvNextImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.ConvNextImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/image_processing_convnext.py#L50"}}),Ue=new ue({props:{name:"preprocess",anchor:"transformers.ConvNextImageProcessor.preprocess",parameters:[{name:"images",val:": Union"},{name:"do_resize",val:": bool = None"},{name:"size",val:": Dict = None"},{name:"crop_pct",val:": float = None"},{name:"resample",val:": Resampling = None"},{name:"do_rescale",val:": bool = None"},{name:"rescale_factor",val:": float = None"},{name:"do_normalize",val:": bool = None"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"return_tensors",val:": Union = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ConvNextImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.ConvNextImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.ConvNextImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the output image after <code>resize</code> has been applied. If <code>size[&quot;shortest_edge&quot;]</code> &gt;= 384, the image
is resized to <code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Otherwise, the smaller edge of the
image will be matched to <code>int(size[&quot;shortest_edge&quot;]/ crop_pct)</code>, after which the image is cropped to
<code>(size[&quot;shortest_edge&quot;], size[&quot;shortest_edge&quot;])</code>. Only has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"size"},{anchor:"transformers.ConvNextImageProcessor.preprocess.crop_pct",description:`<strong>crop_pct</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.crop_pct</code>) &#x2014;
Percentage of the image to crop if size &lt; 384.`,name:"crop_pct"},{anchor:"transformers.ConvNextImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of <code>PILImageResampling</code>, filters. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.ConvNextImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.ConvNextImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.ConvNextImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.ConvNextImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.ConvNextImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.ConvNextImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.ConvNextImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.ConvNextImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnext/image_processing_convnext.py#L185"}}),Me=new nt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[bt],pytorch:[mt]},$$scope:{ctx:F}}}),{c(){e=m("meta"),f=l(),o=m("p"),s=l(),C(g.$$.fragment),n=l(),C(v.$$.fragment),G=l(),N=m("p"),N.innerHTML=Z,Q=l(),I=m("p"),I.textContent=R,k=l(),c=m("p"),c.innerHTML=U,B=l(),K=m("img"),V=l(),J=m("small"),J.innerHTML=X,de=l(),L=m("p"),L.innerHTML=Ce,W=l(),C(H.$$.fragment),he=l(),S=m("p"),S.textContent=A,ee=l(),C(D.$$.fragment),le=l(),ie=m("ul"),ie.innerHTML=me,te=l(),se=m("p"),se.textContent=E,pe=l(),C(d.$$.fragment),j=l(),z=m("div"),C(P.$$.fragment),Y=l(),fe=m("p"),fe.innerHTML=ge,we=l(),ae=m("p"),ae.innerHTML=Ne,u=l(),C(w.$$.fragment),oe=l(),C(q.$$.fragment),ne=l(),xe=m("div"),C(ye.$$.fragment),je=l(),C(Ie.$$.fragment),Le=l(),_e=m("div"),C(Fe.$$.fragment),Ge=l(),Ze=m("p"),Ze.textContent=Se,Qe=l(),Te=m("div"),C(Ue.$$.fragment),Ve=l(),ze=m("p"),ze.textContent=Ae,He=l(),C(Me.$$.fragment),Be=l(),Pe=m("p"),this.h()},l(t){const _=ot("svelte-u9bgzb",document.head);e=p(_,"META",{name:!0,content:!0}),_.forEach(a),f=i(t),o=p(t,"P",{}),ce(o).forEach(a),s=i(t),x(g.$$.fragment,t),n=i(t),x(v.$$.fragment,t),G=i(t),N=p(t,"P",{"data-svelte-h":!0}),b(N)!=="svelte-1luwy58"&&(N.innerHTML=Z),Q=i(t),I=p(t,"P",{"data-svelte-h":!0}),b(I)!=="svelte-1cv3nri"&&(I.textContent=R),k=i(t),c=p(t,"P",{"data-svelte-h":!0}),b(c)!=="svelte-1ecpmdi"&&(c.innerHTML=U),B=i(t),K=p(t,"IMG",{src:!0,alt:!0,width:!0}),V=i(t),J=p(t,"SMALL",{"data-svelte-h":!0}),b(J)!=="svelte-1nlm37k"&&(J.innerHTML=X),de=i(t),L=p(t,"P",{"data-svelte-h":!0}),b(L)!=="svelte-1lpe6b6"&&(L.innerHTML=Ce),W=i(t),x(H.$$.fragment,t),he=i(t),S=p(t,"P",{"data-svelte-h":!0}),b(S)!=="svelte-13uoa0b"&&(S.textContent=A),ee=i(t),x(D.$$.fragment,t),le=i(t),ie=p(t,"UL",{"data-svelte-h":!0}),b(ie)!=="svelte-hqg4o2"&&(ie.innerHTML=me),te=i(t),se=p(t,"P",{"data-svelte-h":!0}),b(se)!=="svelte-17ytafw"&&(se.textContent=E),pe=i(t),x(d.$$.fragment,t),j=i(t),z=p(t,"DIV",{class:!0});var ve=ce(z);x(P.$$.fragment,ve),Y=i(ve),fe=p(ve,"P",{"data-svelte-h":!0}),b(fe)!=="svelte-ln2z0c"&&(fe.innerHTML=ge),we=i(ve),ae=p(ve,"P",{"data-svelte-h":!0}),b(ae)!=="svelte-1s6wgpv"&&(ae.innerHTML=Ne),u=i(ve),x(w.$$.fragment,ve),ve.forEach(a),oe=i(t),x(q.$$.fragment,t),ne=i(t),xe=p(t,"DIV",{class:!0});var Re=ce(xe);x(ye.$$.fragment,Re),Re.forEach(a),je=i(t),x(Ie.$$.fragment,t),Le=i(t),_e=p(t,"DIV",{class:!0});var ke=ce(_e);x(Fe.$$.fragment,ke),Ge=i(ke),Ze=p(ke,"P",{"data-svelte-h":!0}),b(Ze)!=="svelte-12gun77"&&(Ze.textContent=Se),Qe=i(ke),Te=p(ke,"DIV",{class:!0});var Xe=ce(Te);x(Ue.$$.fragment,Xe),Ve=i(Xe),ze=p(Xe,"P",{"data-svelte-h":!0}),b(ze)!=="svelte-1x3yxsa"&&(ze.textContent=Ae),Xe.forEach(a),ke.forEach(a),He=i(t),x(Me.$$.fragment,t),Be=i(t),Pe=p(t,"P",{}),ce(Pe).forEach(a),this.h()},h(){O(e,"name","hf:doc:metadata"),O(e,"content",xt),Oe(K.src,re="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnext_architecture.jpg")||O(K,"src",re),O(K,"alt","描画"),O(K,"width","600"),O(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),O(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,_){h(document.head,e),r(t,f,_),r(t,o,_),r(t,s,_),y(g,t,_),r(t,n,_),y(v,t,_),r(t,G,_),r(t,N,_),r(t,Q,_),r(t,I,_),r(t,k,_),r(t,c,_),r(t,B,_),r(t,K,_),r(t,V,_),r(t,J,_),r(t,de,_),r(t,L,_),r(t,W,_),y(H,t,_),r(t,he,_),r(t,S,_),r(t,ee,_),y(D,t,_),r(t,le,_),r(t,ie,_),r(t,te,_),r(t,se,_),r(t,pe,_),y(d,t,_),r(t,j,_),r(t,z,_),y(P,z,null),h(z,Y),h(z,fe),h(z,we),h(z,ae),h(z,u),y(w,z,null),r(t,oe,_),y(q,t,_),r(t,ne,_),r(t,xe,_),y(ye,xe,null),r(t,je,_),y(Ie,t,_),r(t,Le,_),r(t,_e,_),y(Fe,_e,null),h(_e,Ge),h(_e,Ze),h(_e,Qe),h(_e,Te),y(Ue,Te,null),h(Te,Ve),h(Te,ze),r(t,He,_),y(Me,t,_),r(t,Be,_),r(t,Pe,_),Ee=!0},p(t,[_]){const ve={};_&2&&(ve.$$scope={dirty:_,ctx:t}),w.$set(ve);const Re={};_&2&&(Re.$$scope={dirty:_,ctx:t}),Me.$set(Re)},i(t){Ee||($(g.$$.fragment,t),$(v.$$.fragment,t),$(H.$$.fragment,t),$(D.$$.fragment,t),$(d.$$.fragment,t),$(P.$$.fragment,t),$(w.$$.fragment,t),$(q.$$.fragment,t),$(ye.$$.fragment,t),$(Ie.$$.fragment,t),$(Fe.$$.fragment,t),$(Ue.$$.fragment,t),$(Me.$$.fragment,t),Ee=!0)},o(t){T(g.$$.fragment,t),T(v.$$.fragment,t),T(H.$$.fragment,t),T(D.$$.fragment,t),T(d.$$.fragment,t),T(P.$$.fragment,t),T(w.$$.fragment,t),T(q.$$.fragment,t),T(ye.$$.fragment,t),T(Ie.$$.fragment,t),T(Fe.$$.fragment,t),T(Ue.$$.fragment,t),T(Me.$$.fragment,t),Ee=!1},d(t){t&&(a(f),a(o),a(s),a(n),a(G),a(N),a(Q),a(I),a(k),a(c),a(B),a(K),a(V),a(J),a(de),a(L),a(W),a(he),a(S),a(ee),a(le),a(ie),a(te),a(se),a(pe),a(j),a(z),a(oe),a(ne),a(xe),a(je),a(Le),a(_e),a(He),a(Be),a(Pe)),a(e),M(g,t),M(v,t),M(H,t),M(D,t),M(d,t),M(P),M(w),M(q,t),M(ye),M(Ie,t),M(Fe),M(Ue),M(Me,t)}}}const xt='{"title":"ConvNeXT","local":"convnext","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"ConvNextConfig","local":"transformers.ConvNextConfig","sections":[],"depth":2},{"title":"ConvNextFeatureExtractor","local":"transformers.ConvNextFeatureExtractor","sections":[],"depth":2},{"title":"ConvNextImageProcessor","local":"transformers.ConvNextImageProcessor","sections":[],"depth":2},{"title":"ConvNextModel","local":"transformers.ConvNextModel","sections":[],"depth":2},{"title":"ConvNextForImageClassification","local":"transformers.ConvNextForImageClassification","sections":[],"depth":2},{"title":"TFConvNextModel","local":"transformers.TFConvNextModel","sections":[],"depth":2},{"title":"TFConvNextForImageClassification","local":"transformers.TFConvNextForImageClassification","sections":[],"depth":2}],"depth":1}';function yt(F){return Ke(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Zt extends et{constructor(e){super(),tt(this,e,yt,Ct,Ye,{})}}export{Zt as component};
