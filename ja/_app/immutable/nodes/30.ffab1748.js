import{s as St,o as At,n as Pt}from"../chunks/scheduler.9bc65507.js";import{S as Kt,i as Dt,g as i,s as a,r as o,A as Ot,h as p,f as l,c as n,j as Yt,u as m,x as r,k as ft,y as el,a as s,v as g,d as c,t as u,w as d}from"../chunks/index.707bf1b6.js";import{T as tl}from"../chunks/Tip.c2ecdbf4.js";import{C as T}from"../chunks/CodeBlock.54a9f38d.js";import{D as ll}from"../chunks/DocNotebookDropdown.41f65cb5.js";import{H as h}from"../chunks/Heading.342b1fa6.js";function sl(ye){let M,J='基本的なLLMの使用に興味がある場合、高レベルの <a href="pipeline_tutorial"><code>Pipeline</code></a> インターフェースが良い出発点です。ただし、LLMはしばしば量子化やトークン選択ステップの細かい制御などの高度な機能が必要であり、これは <a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> を介して最良に行われます。LLMとの自己回帰生成はリソースが多く必要であり、適切なスループットのためにGPUで実行する必要があります。';return{c(){M=i("p"),M.innerHTML=J},l(f){M=p(f,"P",{"data-svelte-h":!0}),r(M)!=="svelte-1sday44"&&(M.innerHTML=J)},m(f,fe){s(f,M,fe)},p:Pt,d(f){f&&l(M)}}}function al(ye){let M,J,f,fe,w,be,U,je,_,ht="LLM、またはLarge Language Models（大規模言語モデル）は、テキスト生成の鍵となる要素です。要するに、これらは大規模な事前訓練済みトランスフォーマーモデルで、与えられた入力テキストに基づいて次の単語（または、より正確にはトークン）を予測するように訓練されています。トークンを1つずつ予測するため、モデルを呼び出すだけでは新しい文を生成するために何かより精巧なことをする必要があります。自己回帰生成を行う必要があります。",Te,Z,yt='自己回帰生成は、推論時の手続きで、いくつかの初期入力を与えた状態で、モデルを反復的に呼び出す手法です。🤗 Transformersでは、これは<a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>メソッドによって処理され、これは生成能力を持つすべてのモデルで利用可能です。',Je,$,bt="このチュートリアルでは、以下のことを示します：",we,k,jt="<li>LLMを使用してテキストを生成する方法</li> <li>一般的な落とし穴を回避する方法</li> <li>LLMを最大限に活用するための次のステップ</li>",Ue,G,Tt="始める前に、必要なライブラリがすべてインストールされていることを確認してください：",_e,v,Ze,C,$e,I,Jt='<a href="tasks/language_modeling">因果言語モデリング</a>のためにトレーニングされた言語モデルは、テキストトークンのシーケンスを入力として受け取り、次のトークンの確率分布を返します。',ke,y,wt='<video style="max-width: 90%; margin: auto;" autoplay="" loop="" muted="" playsinline="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov"></video> <figcaption>&quot;Forward pass of an LLM&quot;</figcaption>',Ge,W,Ut="LLM（Language Model）による自己回帰生成の重要な側面の1つは、この確率分布から次のトークンを選択する方法です。このステップでは、次のイテレーションのためのトークンが得られる限り、何でも可能です。これは、確率分布から最も可能性の高いトークンを選択するだけのシンプルな方法から、結果の分布からサンプリングする前に数々の変換を適用するほど複雑な方法まで、あらゆる方法が考えられます。",ve,b,_t='<video style="max-width: 90%; margin: auto;" autoplay="" loop="" muted="" playsinline="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov"></video> <figcaption>&quot;Autoregressive generation iteratively selects the next token from a probability distribution to generate text&quot;</figcaption>',Ce,x,Zt="上記のプロセスは、ある停止条件が満たされるまで反復的に繰り返されます。理想的には、停止条件はモデルによって指示され、モデルは終了シーケンス（<code>EOS</code>）トークンを出力するタイミングを学習すべきです。これがそうでない場合、生成はあらかじめ定義された最大長に達したときに停止します。",Ie,L,$t='トークン選択ステップと停止条件を適切に設定することは、モデルがタスクで期待どおりに振る舞うために重要です。それが、各モデルに関連付けられた <a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a> ファイルがある理由であり、これには優れたデフォルトの生成パラメータ化が含まれ、モデルと一緒に読み込まれます。',We,V,kt="コードについて話しましょう！",xe,j,Le,X,Gt="まず、モデルを読み込む必要があります。",Ve,R,Xe,z,vt="<code>from_pretrained</code> 呼び出しで2つのフラグがあることに注意してください：",Re,B,Ct='<li><code>device_map</code> はモデルをあなたのGPUに移動させます</li> <li><code>load_in_4bit</code> は<a href="main_classes/quantization">4ビットの動的量子化</a>を適用してリソース要件を大幅に削減します</li>',ze,H,It="モデルを初期化する他の方法もありますが、これはLLMを始めるための良い基準です。",Be,F,Wt='次に、<a href="tokenizer_summary">トークナイザ</a>を使用してテキスト入力を前処理する必要があります。',He,q,Fe,Q,xt='<code>model_inputs</code> 変数は、トークン化されたテキスト入力とアテンションマスクを保持しています。 <a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> は、アテンションマスクが渡されていない場合でも、最善の努力をしてそれを推測しようとしますが、できる限り渡すことをお勧めします。最適な結果を得るためです。',qe,E,Lt='最後に、<a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> メソッドを呼び出して生成されたトークンを取得し、それを表示する前にテキストに変換する必要があります。',Qe,N,Ee,Y,Vt="これで完了です！わずかなコード行数で、LLM（Large Language Model）のパワーを活用できます。",Ne,S,Ye,A,Xt='<a href="generation_strategies">生成戦略</a>はたくさんあり、デフォルトの値があなたのユースケースに適していないことがあります。出力が期待通りでない場合、最も一般的な落とし穴とその回避方法のリストを作成しました。',Se,P,Ae,K,Pe,D,Rt='<a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a> ファイルで指定されていない場合、<code>generate</code> はデフォルトで最大で 20 トークンまで返します。我々は <code>generate</code> コールで <code>max_new_tokens</code> を手動で設定することを強くお勧めします。これにより、返される新しいトークンの最大数を制御できます。LLM（正確には、<a href="https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt" rel="nofollow">デコーダー専用モデル</a>）も出力の一部として入力プロンプトを返すことに注意してください。',Ke,O,De,ee,Oe,te,zt='デフォルトでは、 <a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a> ファイルで指定されていない限り、<code>generate</code> は各イテレーションで最も可能性の高いトークンを選択します（貪欲デコーディング）。タスクに応じて、これは望ましくないことがあります。チャットボットやエッセイのような創造的なタスクでは、サンプリングが有益です。一方、音声の転写や翻訳のような入力に基づくタスクでは、貪欲デコーディングが有益です。<code>do_sample=True</code> でサンプリングを有効にできます。このトピックについての詳細は、この<a href="https://huggingface.co/blog/how-to-generate" rel="nofollow">ブログポスト</a>で学ぶことができます。',et,le,tt,se,lt,ae,Bt='LLM（Large Language Models）は<a href="https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt" rel="nofollow">デコーダー専用</a>のアーキテクチャであり、入力プロンプトを繰り返し処理することを意味します。入力が同じ長さでない場合、それらをパディングする必要があります。LLMはパッドトークンからの続きを学習していないため、入力は左パディングする必要があります。また、生成に対して注目マスクを渡し忘れないようにしてください！',st,ne,at,ie,nt,pe,Ht="オートリグレッシブ生成プロセスは比較的簡単ですが、LLMを最大限に活用することは多くの要素が絡むため、挑戦的な試みとなります。LLMの使用と理解をさらに深めるための次のステップについては以下のリソースをご覧ください。",it,re,pt,oe,Ft='<li><a href="generation_strategies">ガイド</a>：異なる生成方法を制御する方法、生成構成ファイルの設定方法、出力のストリーミング方法についてのガイド;</li> <li><a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationConfig">GenerationConfig</a>、<a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a>、および<a href="internal/generation_utils">生成関連クラス</a>に関するAPIリファレンス。</li>',rt,me,ot,ge,qt='<li><a href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="nofollow">Open LLM リーダーボード</a>：オープンソースモデルの品質に焦点を当てたリーダーボード;</li> <li><a href="https://huggingface.co/spaces/optimum/llm-perf-leaderboard" rel="nofollow">Open LLM-Perf リーダーボード</a>：LLMのスループットに焦点を当てたリーダーボード。</li>',mt,ce,gt,ue,Qt='<li><a href="main_classes/quantization">ガイド</a>：ダイナミッククオンタイズに関するガイド。これによりメモリ要件を劇的に削減する方法が示されています。</li>',ct,de,ut,Me,Et='<li><a href="https://github.com/huggingface/text-generation-inference" rel="nofollow"><code>text-generation-inference</code></a>：LLM用の本番向けサーバー;</li> <li><a href="https://github.com/huggingface/optimum" rel="nofollow"><code>optimum</code></a>：特定のハードウェアデバイス向けに最適化された🤗 Transformersの拡張。</li>',dt,he,Mt;return w=new h({props:{title:"Generation with LLMs",local:"generation-with-llms",headingTag:"h1"}}),U=new ll({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/llm_tutorial.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/pytorch/llm_tutorial.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/tensorflow/llm_tutorial.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/llm_tutorial.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/pytorch/llm_tutorial.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/tensorflow/llm_tutorial.ipynb"}]}}),v=new T({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGJpdHNhbmRieXRlcyUzRSUzRDAuMzkuMCUyMC1x",highlighted:"pip install transformers bitsandbytes&gt;=0.39.0 -q",wrap:!1}}),C=new h({props:{title:"Generate text",local:"generate-text",headingTag:"h2"}}),j=new tl({props:{$$slots:{default:[sl]},$$scope:{ctx:ye}}}),R=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyb3BlbmxtLXJlc2VhcmNoJTJGb3Blbl9sbGFtYV83YiUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTIwbG9hZF9pbl80Yml0JTNEVHJ1ZSUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;openlm-research/open_llama_7b&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)`,wrap:!1}}),q=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJvcGVubG0tcmVzZWFyY2glMkZvcGVuX2xsYW1hXzdiJTIyKSUwQW1vZGVsX2lucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJBJTIwbGlzdCUyMG9mJTIwY29sb3JzJTNBJTIwcmVkJTJDJTIwYmx1ZSUyMiU1RCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKCUyMmN1ZGElMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openlm-research/open_llama_7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer([<span class="hljs-string">&quot;A list of colors: red, blue&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)`,wrap:!1}}),N=new T({props:{code:"Z2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;A list of colors: red, blue, green, yellow, black, white, and brown&#x27;</span>`,wrap:!1}}),S=new h({props:{title:"Common pitfalls",local:"common-pitfalls",headingTag:"h2"}}),P=new T({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5sbS1yZXNlYXJjaCUyRm9wZW5fbGxhbWFfN2IlMjIpJTBBdG9rZW5pemVyLnBhZF90b2tlbiUyMCUzRCUyMHRva2VuaXplci5lb3NfdG9rZW4lMjAlMjAlMjMlMjBMbGFtYSUyMGhhcyUyMG5vJTIwcGFkJTIwdG9rZW4lMjBieSUyMGRlZmF1bHQlMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVubG0tcmVzZWFyY2glMkZvcGVuX2xsYW1hXzdiJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBsb2FkX2luXzRiaXQlM0RUcnVlJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openlm-research/open_llama_7b&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pad_token = tokenizer.eos_token  <span class="hljs-comment"># Llama has no pad token by default</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;openlm-research/open_llama_7b&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)`,wrap:!1}}),K=new h({props:{title:"Generated output is too short/long",local:"generated-output-is-too-shortlong",headingTag:"h3"}}),O=new T({props:{code:"bW9kZWxfaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMkElMjBzZXF1ZW5jZSUyMG9mJTIwbnVtYmVycyUzQSUyMDElMkMlMjAyJTIyJTVEJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikudG8oJTIyY3VkYSUyMiklMEElMEElMjMlMjBCeSUyMGRlZmF1bHQlMkMlMjB0aGUlMjBvdXRwdXQlMjB3aWxsJTIwY29udGFpbiUyMHVwJTIwdG8lMjAyMCUyMHRva2VucyUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKm1vZGVsX2lucHV0cyklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVEJTBBJTBBJTIzJTIwU2V0dGluZyUyMCU2MG1heF9uZXdfdG9rZW5zJTYwJTIwYWxsb3dzJTIweW91JTIwdG8lMjBjb250cm9sJTIwdGhlJTIwbWF4aW11bSUyMGxlbmd0aCUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKm1vZGVsX2lucHV0cyUyQyUyMG1heF9uZXdfdG9rZW5zJTNENTApJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer([<span class="hljs-string">&quot;A sequence of numbers: 1, 2&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># By default, the output will contain up to 20 tokens</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;A sequence of numbers: 1, 2, 3, 4, 5&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Setting \`max_new_tokens\` allows you to control the maximum length</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs, max_new_tokens=<span class="hljs-number">50</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;A sequence of numbers: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,&#x27;</span>`,wrap:!1}}),ee=new h({props:{title:"Incorrect generation mode",local:"incorrect-generation-mode",headingTag:"h3"}}),le=new T({props:{code:"JTIzJTIwU2V0JTIwc2VlZCUyMG9yJTIwcmVwcm9kdWNpYmlsaXR5JTIwLS0lMjB5b3UlMjBkb24ndCUyMG5lZWQlMjB0aGlzJTIwdW5sZXNzJTIweW91JTIwd2FudCUyMGZ1bGwlMjByZXByb2R1Y2liaWxpdHklMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwc2V0X3NlZWQlMEFzZXRfc2VlZCgwKSUwQSUwQW1vZGVsX2lucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJJJTIwYW0lMjBhJTIwY2F0LiUyMiU1RCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKCUyMmN1ZGElMjIpJTBBJTBBJTIzJTIwTExNJTIwJTJCJTIwZ3JlZWR5JTIwZGVjb2RpbmclMjAlM0QlMjByZXBldGl0aXZlJTJDJTIwYm9yaW5nJTIwb3V0cHV0JTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSU1QjAlNUQlMEElMEElMjMlMjBXaXRoJTIwc2FtcGxpbmclMkMlMjB0aGUlMjBvdXRwdXQlMjBiZWNvbWVzJTIwbW9yZSUyMGNyZWF0aXZlISUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKm1vZGVsX2lucHV0cyUyQyUyMGRvX3NhbXBsZSUzRFRydWUpJTBBdG9rZW5pemVyLmJhdGNoX2RlY29kZShnZW5lcmF0ZWRfaWRzJTJDJTIwc2tpcF9zcGVjaWFsX3Rva2VucyUzRFRydWUpJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Set seed or reproducibility -- you don&#x27;t need this unless you want full reproducibility</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> set_seed
<span class="hljs-meta">&gt;&gt;&gt; </span>set_seed(<span class="hljs-number">0</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer([<span class="hljs-string">&quot;I am a cat.&quot;</span>], return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># LLM + greedy decoding = repetitive, boring output</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;I am a cat. I am a cat. I am a cat. I am a cat&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With sampling, the output becomes more creative!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs, do_sample=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;I am a cat.\\nI just need to be. I am always.\\nEvery time&#x27;</span>`,wrap:!1}}),se=new h({props:{title:"Wrong padding side",local:"wrong-padding-side",headingTag:"h3"}}),ne=new T({props:{code:"JTIzJTIwVGhlJTIwdG9rZW5pemVyJTIwaW5pdGlhbGl6ZWQlMjBhYm92ZSUyMGhhcyUyMHJpZ2h0LXBhZGRpbmclMjBhY3RpdmUlMjBieSUyMGRlZmF1bHQlM0ElMjB0aGUlMjAxc3QlMjBzZXF1ZW5jZSUyQyUwQSUyMyUyMHdoaWNoJTIwaXMlMjBzaG9ydGVyJTJDJTIwaGFzJTIwcGFkZGluZyUyMG9uJTIwdGhlJTIwcmlnaHQlMjBzaWRlLiUyMEdlbmVyYXRpb24lMjBmYWlscy4lMEFtb2RlbF9pbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTBBJTIwJTIwJTIwJTIwJTVCJTIyMSUyQyUyMDIlMkMlMjAzJTIyJTJDJTIwJTIyQSUyQyUyMEIlMkMlMjBDJTJDJTIwRCUyQyUyMEUlMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMEEpLnRvKCUyMmN1ZGElMjIpJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqbW9kZWxfaW5wdXRzKSUwQXRva2VuaXplci5iYXRjaF9kZWNvZGUoZ2VuZXJhdGVkX2lkcyU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVEJTBBJTBBJTIzJTIwV2l0aCUyMGxlZnQtcGFkZGluZyUyQyUyMGl0JTIwd29ya3MlMjBhcyUyMGV4cGVjdGVkISUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5sbS1yZXNlYXJjaCUyRm9wZW5fbGxhbWFfN2IlMjIlMkMlMjBwYWRkaW5nX3NpZGUlM0QlMjJsZWZ0JTIyKSUwQXRva2VuaXplci5wYWRfdG9rZW4lMjAlM0QlMjB0b2tlbml6ZXIuZW9zX3Rva2VuJTIwJTIwJTIzJTIwTGxhbWElMjBoYXMlMjBubyUyMHBhZCUyMHRva2VuJTIwYnklMjBkZWZhdWx0JTBBbW9kZWxfaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCUwQSUyMCUyMCUyMCUyMCU1QiUyMjElMkMlMjAyJTJDJTIwMyUyMiUyQyUyMCUyMkElMkMlMjBCJTJDJTIwQyUyQyUyMEQlMkMlMjBFJTIyJTVEJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTBBKS50byglMjJjdWRhJTIyKSUwQWdlbmVyYXRlZF9pZHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZSgqKm1vZGVsX2lucHV0cyklMEF0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># The tokenizer initialized above has right-padding active by default: the 1st sequence,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># which is shorter, has padding on the right side. Generation fails.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;1, 2, 3&quot;</span>, <span class="hljs-string">&quot;A, B, C, D, E&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># With left-padding, it works as expected!</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openlm-research/open_llama_7b&quot;</span>, padding_side=<span class="hljs-string">&quot;left&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pad_token = tokenizer.eos_token  <span class="hljs-comment"># Llama has no pad token by default</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model_inputs = tokenizer(
<span class="hljs-meta">... </span>    [<span class="hljs-string">&quot;1, 2, 3&quot;</span>, <span class="hljs-string">&quot;A, B, C, D, E&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>
<span class="hljs-meta">... </span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>generated_ids = model.generate(**model_inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)[<span class="hljs-number">0</span>]
<span class="hljs-string">&#x27;1, 2, 3, 4, 5, 6,&#x27;</span>`,wrap:!1}}),ie=new h({props:{title:"Further resources",local:"further-resources",headingTag:"h2"}}),re=new h({props:{title:"Advanced generate usage",local:"advanced-generate-usage",headingTag:"h3"}}),me=new h({props:{title:"LLM leaderboards",local:"llm-leaderboards",headingTag:"h3"}}),ce=new h({props:{title:"Latency and throughput",local:"latency-and-throughput",headingTag:"h3"}}),de=new h({props:{title:"Related libraries",local:"related-libraries",headingTag:"h3"}}),{c(){M=i("meta"),J=a(),f=i("p"),fe=a(),o(w.$$.fragment),be=a(),o(U.$$.fragment),je=a(),_=i("p"),_.textContent=ht,Te=a(),Z=i("p"),Z.innerHTML=yt,Je=a(),$=i("p"),$.textContent=bt,we=a(),k=i("ul"),k.innerHTML=jt,Ue=a(),G=i("p"),G.textContent=Tt,_e=a(),o(v.$$.fragment),Ze=a(),o(C.$$.fragment),$e=a(),I=i("p"),I.innerHTML=Jt,ke=a(),y=i("figure"),y.innerHTML=wt,Ge=a(),W=i("p"),W.textContent=Ut,ve=a(),b=i("figure"),b.innerHTML=_t,Ce=a(),x=i("p"),x.innerHTML=Zt,Ie=a(),L=i("p"),L.innerHTML=$t,We=a(),V=i("p"),V.textContent=kt,xe=a(),o(j.$$.fragment),Le=a(),X=i("p"),X.textContent=Gt,Ve=a(),o(R.$$.fragment),Xe=a(),z=i("p"),z.innerHTML=vt,Re=a(),B=i("ul"),B.innerHTML=Ct,ze=a(),H=i("p"),H.textContent=It,Be=a(),F=i("p"),F.innerHTML=Wt,He=a(),o(q.$$.fragment),Fe=a(),Q=i("p"),Q.innerHTML=xt,qe=a(),E=i("p"),E.innerHTML=Lt,Qe=a(),o(N.$$.fragment),Ee=a(),Y=i("p"),Y.textContent=Vt,Ne=a(),o(S.$$.fragment),Ye=a(),A=i("p"),A.innerHTML=Xt,Se=a(),o(P.$$.fragment),Ae=a(),o(K.$$.fragment),Pe=a(),D=i("p"),D.innerHTML=Rt,Ke=a(),o(O.$$.fragment),De=a(),o(ee.$$.fragment),Oe=a(),te=i("p"),te.innerHTML=zt,et=a(),o(le.$$.fragment),tt=a(),o(se.$$.fragment),lt=a(),ae=i("p"),ae.innerHTML=Bt,st=a(),o(ne.$$.fragment),at=a(),o(ie.$$.fragment),nt=a(),pe=i("p"),pe.textContent=Ht,it=a(),o(re.$$.fragment),pt=a(),oe=i("ol"),oe.innerHTML=Ft,rt=a(),o(me.$$.fragment),ot=a(),ge=i("ol"),ge.innerHTML=qt,mt=a(),o(ce.$$.fragment),gt=a(),ue=i("ol"),ue.innerHTML=Qt,ct=a(),o(de.$$.fragment),ut=a(),Me=i("ol"),Me.innerHTML=Et,dt=a(),he=i("p"),this.h()},l(e){const t=Ot("svelte-u9bgzb",document.head);M=p(t,"META",{name:!0,content:!0}),t.forEach(l),J=n(e),f=p(e,"P",{}),Yt(f).forEach(l),fe=n(e),m(w.$$.fragment,e),be=n(e),m(U.$$.fragment,e),je=n(e),_=p(e,"P",{"data-svelte-h":!0}),r(_)!=="svelte-1p8oyus"&&(_.textContent=ht),Te=n(e),Z=p(e,"P",{"data-svelte-h":!0}),r(Z)!=="svelte-18326uo"&&(Z.innerHTML=yt),Je=n(e),$=p(e,"P",{"data-svelte-h":!0}),r($)!=="svelte-10d5jzs"&&($.textContent=bt),we=n(e),k=p(e,"UL",{"data-svelte-h":!0}),r(k)!=="svelte-114gruu"&&(k.innerHTML=jt),Ue=n(e),G=p(e,"P",{"data-svelte-h":!0}),r(G)!=="svelte-5jpx9c"&&(G.textContent=Tt),_e=n(e),m(v.$$.fragment,e),Ze=n(e),m(C.$$.fragment,e),$e=n(e),I=p(e,"P",{"data-svelte-h":!0}),r(I)!=="svelte-1kvqgos"&&(I.innerHTML=Jt),ke=n(e),y=p(e,"FIGURE",{class:!0,"data-svelte-h":!0}),r(y)!=="svelte-hjgddv"&&(y.innerHTML=wt),Ge=n(e),W=p(e,"P",{"data-svelte-h":!0}),r(W)!=="svelte-1vvn9v3"&&(W.textContent=Ut),ve=n(e),b=p(e,"FIGURE",{class:!0,"data-svelte-h":!0}),r(b)!=="svelte-1uqc9hk"&&(b.innerHTML=_t),Ce=n(e),x=p(e,"P",{"data-svelte-h":!0}),r(x)!=="svelte-1q7bq18"&&(x.innerHTML=Zt),Ie=n(e),L=p(e,"P",{"data-svelte-h":!0}),r(L)!=="svelte-1xlt8wi"&&(L.innerHTML=$t),We=n(e),V=p(e,"P",{"data-svelte-h":!0}),r(V)!=="svelte-1gzvd9k"&&(V.textContent=kt),xe=n(e),m(j.$$.fragment,e),Le=n(e),X=p(e,"P",{"data-svelte-h":!0}),r(X)!=="svelte-1mik2gq"&&(X.textContent=Gt),Ve=n(e),m(R.$$.fragment,e),Xe=n(e),z=p(e,"P",{"data-svelte-h":!0}),r(z)!=="svelte-dj98s8"&&(z.innerHTML=vt),Re=n(e),B=p(e,"UL",{"data-svelte-h":!0}),r(B)!=="svelte-k3ww82"&&(B.innerHTML=Ct),ze=n(e),H=p(e,"P",{"data-svelte-h":!0}),r(H)!=="svelte-1ved9f1"&&(H.textContent=It),Be=n(e),F=p(e,"P",{"data-svelte-h":!0}),r(F)!=="svelte-1vkr8y3"&&(F.innerHTML=Wt),He=n(e),m(q.$$.fragment,e),Fe=n(e),Q=p(e,"P",{"data-svelte-h":!0}),r(Q)!=="svelte-1ri477u"&&(Q.innerHTML=xt),qe=n(e),E=p(e,"P",{"data-svelte-h":!0}),r(E)!=="svelte-voqumo"&&(E.innerHTML=Lt),Qe=n(e),m(N.$$.fragment,e),Ee=n(e),Y=p(e,"P",{"data-svelte-h":!0}),r(Y)!=="svelte-1fgh7wz"&&(Y.textContent=Vt),Ne=n(e),m(S.$$.fragment,e),Ye=n(e),A=p(e,"P",{"data-svelte-h":!0}),r(A)!=="svelte-k0wny5"&&(A.innerHTML=Xt),Se=n(e),m(P.$$.fragment,e),Ae=n(e),m(K.$$.fragment,e),Pe=n(e),D=p(e,"P",{"data-svelte-h":!0}),r(D)!=="svelte-80musy"&&(D.innerHTML=Rt),Ke=n(e),m(O.$$.fragment,e),De=n(e),m(ee.$$.fragment,e),Oe=n(e),te=p(e,"P",{"data-svelte-h":!0}),r(te)!=="svelte-gscn2d"&&(te.innerHTML=zt),et=n(e),m(le.$$.fragment,e),tt=n(e),m(se.$$.fragment,e),lt=n(e),ae=p(e,"P",{"data-svelte-h":!0}),r(ae)!=="svelte-fd0pc8"&&(ae.innerHTML=Bt),st=n(e),m(ne.$$.fragment,e),at=n(e),m(ie.$$.fragment,e),nt=n(e),pe=p(e,"P",{"data-svelte-h":!0}),r(pe)!=="svelte-1d601do"&&(pe.textContent=Ht),it=n(e),m(re.$$.fragment,e),pt=n(e),oe=p(e,"OL",{"data-svelte-h":!0}),r(oe)!=="svelte-142agms"&&(oe.innerHTML=Ft),rt=n(e),m(me.$$.fragment,e),ot=n(e),ge=p(e,"OL",{"data-svelte-h":!0}),r(ge)!=="svelte-1mag1j5"&&(ge.innerHTML=qt),mt=n(e),m(ce.$$.fragment,e),gt=n(e),ue=p(e,"OL",{"data-svelte-h":!0}),r(ue)!=="svelte-1d7xcac"&&(ue.innerHTML=Qt),ct=n(e),m(de.$$.fragment,e),ut=n(e),Me=p(e,"OL",{"data-svelte-h":!0}),r(Me)!=="svelte-1v1y2ns"&&(Me.innerHTML=Et),dt=n(e),he=p(e,"P",{}),Yt(he).forEach(l),this.h()},h(){ft(M,"name","hf:doc:metadata"),ft(M,"content",nl),ft(y,"class","image table text-center m-0 w-full"),ft(b,"class","image table text-center m-0 w-full")},m(e,t){el(document.head,M),s(e,J,t),s(e,f,t),s(e,fe,t),g(w,e,t),s(e,be,t),g(U,e,t),s(e,je,t),s(e,_,t),s(e,Te,t),s(e,Z,t),s(e,Je,t),s(e,$,t),s(e,we,t),s(e,k,t),s(e,Ue,t),s(e,G,t),s(e,_e,t),g(v,e,t),s(e,Ze,t),g(C,e,t),s(e,$e,t),s(e,I,t),s(e,ke,t),s(e,y,t),s(e,Ge,t),s(e,W,t),s(e,ve,t),s(e,b,t),s(e,Ce,t),s(e,x,t),s(e,Ie,t),s(e,L,t),s(e,We,t),s(e,V,t),s(e,xe,t),g(j,e,t),s(e,Le,t),s(e,X,t),s(e,Ve,t),g(R,e,t),s(e,Xe,t),s(e,z,t),s(e,Re,t),s(e,B,t),s(e,ze,t),s(e,H,t),s(e,Be,t),s(e,F,t),s(e,He,t),g(q,e,t),s(e,Fe,t),s(e,Q,t),s(e,qe,t),s(e,E,t),s(e,Qe,t),g(N,e,t),s(e,Ee,t),s(e,Y,t),s(e,Ne,t),g(S,e,t),s(e,Ye,t),s(e,A,t),s(e,Se,t),g(P,e,t),s(e,Ae,t),g(K,e,t),s(e,Pe,t),s(e,D,t),s(e,Ke,t),g(O,e,t),s(e,De,t),g(ee,e,t),s(e,Oe,t),s(e,te,t),s(e,et,t),g(le,e,t),s(e,tt,t),g(se,e,t),s(e,lt,t),s(e,ae,t),s(e,st,t),g(ne,e,t),s(e,at,t),g(ie,e,t),s(e,nt,t),s(e,pe,t),s(e,it,t),g(re,e,t),s(e,pt,t),s(e,oe,t),s(e,rt,t),g(me,e,t),s(e,ot,t),s(e,ge,t),s(e,mt,t),g(ce,e,t),s(e,gt,t),s(e,ue,t),s(e,ct,t),g(de,e,t),s(e,ut,t),s(e,Me,t),s(e,dt,t),s(e,he,t),Mt=!0},p(e,[t]){const Nt={};t&2&&(Nt.$$scope={dirty:t,ctx:e}),j.$set(Nt)},i(e){Mt||(c(w.$$.fragment,e),c(U.$$.fragment,e),c(v.$$.fragment,e),c(C.$$.fragment,e),c(j.$$.fragment,e),c(R.$$.fragment,e),c(q.$$.fragment,e),c(N.$$.fragment,e),c(S.$$.fragment,e),c(P.$$.fragment,e),c(K.$$.fragment,e),c(O.$$.fragment,e),c(ee.$$.fragment,e),c(le.$$.fragment,e),c(se.$$.fragment,e),c(ne.$$.fragment,e),c(ie.$$.fragment,e),c(re.$$.fragment,e),c(me.$$.fragment,e),c(ce.$$.fragment,e),c(de.$$.fragment,e),Mt=!0)},o(e){u(w.$$.fragment,e),u(U.$$.fragment,e),u(v.$$.fragment,e),u(C.$$.fragment,e),u(j.$$.fragment,e),u(R.$$.fragment,e),u(q.$$.fragment,e),u(N.$$.fragment,e),u(S.$$.fragment,e),u(P.$$.fragment,e),u(K.$$.fragment,e),u(O.$$.fragment,e),u(ee.$$.fragment,e),u(le.$$.fragment,e),u(se.$$.fragment,e),u(ne.$$.fragment,e),u(ie.$$.fragment,e),u(re.$$.fragment,e),u(me.$$.fragment,e),u(ce.$$.fragment,e),u(de.$$.fragment,e),Mt=!1},d(e){e&&(l(J),l(f),l(fe),l(be),l(je),l(_),l(Te),l(Z),l(Je),l($),l(we),l(k),l(Ue),l(G),l(_e),l(Ze),l($e),l(I),l(ke),l(y),l(Ge),l(W),l(ve),l(b),l(Ce),l(x),l(Ie),l(L),l(We),l(V),l(xe),l(Le),l(X),l(Ve),l(Xe),l(z),l(Re),l(B),l(ze),l(H),l(Be),l(F),l(He),l(Fe),l(Q),l(qe),l(E),l(Qe),l(Ee),l(Y),l(Ne),l(Ye),l(A),l(Se),l(Ae),l(Pe),l(D),l(Ke),l(De),l(Oe),l(te),l(et),l(tt),l(lt),l(ae),l(st),l(at),l(nt),l(pe),l(it),l(pt),l(oe),l(rt),l(ot),l(ge),l(mt),l(gt),l(ue),l(ct),l(ut),l(Me),l(dt),l(he)),l(M),d(w,e),d(U,e),d(v,e),d(C,e),d(j,e),d(R,e),d(q,e),d(N,e),d(S,e),d(P,e),d(K,e),d(O,e),d(ee,e),d(le,e),d(se,e),d(ne,e),d(ie,e),d(re,e),d(me,e),d(ce,e),d(de,e)}}}const nl='{"title":"Generation with LLMs","local":"generation-with-llms","sections":[{"title":"Generate text","local":"generate-text","sections":[],"depth":2},{"title":"Common pitfalls","local":"common-pitfalls","sections":[{"title":"Generated output is too short/long","local":"generated-output-is-too-shortlong","sections":[],"depth":3},{"title":"Incorrect generation mode","local":"incorrect-generation-mode","sections":[],"depth":3},{"title":"Wrong padding side","local":"wrong-padding-side","sections":[],"depth":3}],"depth":2},{"title":"Further resources","local":"further-resources","sections":[{"title":"Advanced generate usage","local":"advanced-generate-usage","sections":[],"depth":3},{"title":"LLM leaderboards","local":"llm-leaderboards","sections":[],"depth":3},{"title":"Latency and throughput","local":"latency-and-throughput","sections":[],"depth":3},{"title":"Related libraries","local":"related-libraries","sections":[],"depth":3}],"depth":2}],"depth":1}';function il(ye){return At(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ul extends Kt{constructor(M){super(),Dt(this,M,il,al,St,{})}}export{ul as component};
