import{s as cX,o as mX,n as x}from"../chunks/scheduler.9bc65507.js";import{S as fX,i as gX,g as i,s as r,r as p,A as pX,h as l,f as d,c as a,j as C,u,x as f,k as w,y as t,a as y,v as h,d as b,t as _,w as M}from"../chunks/index.707bf1b6.js";import{T as Jj}from"../chunks/Tip.c2ecdbf4.js";import{D as j}from"../chunks/Docstring.17db21ae.js";import{C as $}from"../chunks/CodeBlock.54a9f38d.js";import{E as k}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as Z}from"../chunks/Heading.342b1fa6.js";function uX(v){let n,F='あなたの<code>NewModelConfig</code>が<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>のサブクラスである場合、その<code>model_type</code>属性がコンフィグを登録するときに使用するキー（ここでは<code>&quot;new-model&quot;</code>）と同じに設定されていることを確認してください。',c,s,m='同様に、あなたの<code>NewModel</code>が<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>のサブクラスである場合、その<code>config_class</code>属性がモデルを登録する際に使用するクラス（ここでは<code>NewModelConfig</code>）と同じに設定されていることを確認してください。';return{c(){n=i("p"),n.innerHTML=F,c=r(),s=i("p"),s.innerHTML=m},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-r0m7dc"&&(n.innerHTML=F),c=a(e),s=l(e,"P",{"data-svelte-h":!0}),f(s)!=="svelte-z947bb"&&(s.innerHTML=m)},m(e,T){y(e,n,T),y(e,c,T),y(e,s,T)},p:x,d(e){e&&(d(n),d(c),d(s))}}}function hX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMCh1c2VyLXVwbG9hZGVkKSUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmRibWR6JTJGYmVydC1iYXNlLWdlcm1hbi1jYXNlZCUyMiklMEElMEElMjMlMjBJZiUyMGNvbmZpZ3VyYXRpb24lMjBmaWxlJTIwaXMlMjBpbiUyMGElMjBkaXJlY3RvcnklMjAoZS5nLiUyQyUyMHdhcyUyMHNhdmVkJTIwdXNpbmclMjAqc2F2ZV9wcmV0cmFpbmVkKCcuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJykqKS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGVzdCUyRmJlcnRfc2F2ZWRfbW9kZWwlMkYlMjIpJTBBJTBBJTIzJTIwTG9hZCUyMGElMjBzcGVjaWZpYyUyMGNvbmZpZ3VyYXRpb24lMjBmaWxlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0ZXN0JTJGYmVydF9zYXZlZF9tb2RlbCUyRm15X2NvbmZpZ3VyYXRpb24uanNvbiUyMiklMEElMEElMjMlMjBDaGFuZ2UlMjBzb21lJTIwY29uZmlnJTIwYXR0cmlidXRlcyUyMHdoZW4lMjBsb2FkaW5nJTIwYSUyMHByZXRyYWluZWQlMjBjb25maWcuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtdW5jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSUyQyUyMGZvbyUzREZhbHNlKSUwQWNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQWNvbmZpZyUyQyUyMHVudXNlZF9rd2FyZ3MlMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlJTJDJTIwZm9vJTNERmFsc2UlMkMlMjByZXR1cm5fdW51c2VkX2t3YXJncyUzRFRydWUlMEEpJTBBY29uZmlnLm91dHB1dF9hdHRlbnRpb25zJTBBJTBBdW51c2VkX2t3YXJncw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If configuration file is in a directory (e.g., was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*).</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Load a specific configuration file.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./test/bert_saved_model/my_configuration.json&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Change some config attributes when loading a pretrained config.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>config, unused_kwargs = AutoConfig.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>unused_kwargs
{<span class="hljs-string">&#x27;foo&#x27;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function bX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEElMjMlMjBEb3dubG9hZCUyMHZvY2FidWxhcnklMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS11bmNhc2VkJTIyKSUwQSUwQSUyMyUyMERvd25sb2FkJTIwdm9jYWJ1bGFyeSUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMCh1c2VyLXVwbG9hZGVkKSUyMGFuZCUyMGNhY2hlLiUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmRibWR6JTJGYmVydC1iYXNlLWdlcm1hbi1jYXNlZCUyMiklMEElMEElMjMlMjBJZiUyMHZvY2FidWxhcnklMjBmaWxlcyUyMGFyZSUyMGluJTIwYSUyMGRpcmVjdG9yeSUyMChlLmcuJTIwdG9rZW5pemVyJTIwd2FzJTIwc2F2ZWQlMjB1c2luZyUyMCpzYXZlX3ByZXRyYWluZWQoJy4lMkZ0ZXN0JTJGc2F2ZWRfbW9kZWwlMkYnKSopJTBBJTIzJTIwdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRlc3QlMkZiZXJ0X3NhdmVkX21vZGVsJTJGJTIyKSUwQSUwQSUyMyUyMERvd25sb2FkJTIwdm9jYWJ1bGFyeSUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGRlZmluZSUyMG1vZGVsLXNwZWNpZmljJTIwYXJndW1lbnRzJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyRmFjZWJvb2tBSSUyRnJvYmVydGEtYmFzZSUyMiUyQyUyMGFkZF9wcmVmaXhfc3BhY2UlM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-uncased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co (user-uploaded) and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;dbmdz/bert-base-german-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If vocabulary files are in a directory (e.g. tokenizer was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># tokenizer = AutoTokenizer.from_pretrained(&quot;./test/bert_saved_model/&quot;)</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download vocabulary from huggingface.co and define model-specific arguments</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;FacebookAI/roberta-base&quot;</span>, add_prefix_space=<span class="hljs-literal">True</span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function _X(v){let n,F="Passing <code>token=True</code> is required when you want to use a private model.";return{c(){n=i("p"),n.innerHTML=F},l(c){n=l(c,"P",{"data-svelte-h":!0}),f(n)!=="svelte-15auxyb"&&(n.innerHTML=F)},m(c,s){y(c,n,s)},p:x,d(c){c&&d(n)}}}function MX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBmZWF0dXJlJTIwZXh0cmFjdG9yJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlLTk2MGglMjIpJTBBJTBBJTIzJTIwSWYlMjBmZWF0dXJlJTIwZXh0cmFjdG9yJTIwZmlsZXMlMjBhcmUlMjBpbiUyMGElMjBkaXJlY3RvcnklMjAoZS5nLiUyMGZlYXR1cmUlMjBleHRyYWN0b3IlMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiklMEElMjMlMjBmZWF0dXJlX2V4dHJhY3RvciUyMCUzRCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download feature extractor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If feature extractor files are in a directory (e.g. feature extractor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># feature_extractor = AutoFeatureExtractor.from_pretrained(&quot;./test/saved_model/&quot;)</span>`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function TX(v){let n,F="Passing <code>token=True</code> is required when you want to use a private model.";return{c(){n=i("p"),n.innerHTML=F},l(c){n=l(c,"P",{"data-svelte-h":!0}),f(n)!=="svelte-15auxyb"&&(n.innerHTML=F)},m(c,s){y(c,n,s)},p:x,d(c){c&&d(n)}}}function yX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQSUyMyUyMERvd25sb2FkJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnZpdC1iYXNlLXBhdGNoMTYtMjI0LWluMjFrJTIyKSUwQSUwQSUyMyUyMElmJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjBmaWxlcyUyMGFyZSUyMGluJTIwYSUyMGRpcmVjdG9yeSUyMChlLmcuJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiklMEElMjMlMjBpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0ZXN0JTJGc2F2ZWRfbW9kZWwlMkYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download image processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If image processor files are in a directory (e.g. image processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># image_processor = AutoImageProcessor.from_pretrained(&quot;./test/saved_model/&quot;)</span>`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function FX(v){let n,F="Passing <code>token=True</code> is required when you want to use a private model.";return{c(){n=i("p"),n.innerHTML=F},l(c){n=l(c,"P",{"data-svelte-h":!0}),f(n)!=="svelte-15auxyb"&&(n.innerHTML=F)},m(c,s){y(c,n,s)},p:x,d(c){c&&d(n)}}}function vX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMEElMEElMjMlMjBEb3dubG9hZCUyMHByb2Nlc3NvciUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGd2F2MnZlYzItYmFzZS05NjBoJTIyKSUwQSUwQSUyMyUyMElmJTIwcHJvY2Vzc29yJTIwZmlsZXMlMjBhcmUlMjBpbiUyMGElMjBkaXJlY3RvcnklMjAoZS5nLiUyMHByb2Nlc3NvciUyMHdhcyUyMHNhdmVkJTIwdXNpbmclMjAqc2F2ZV9wcmV0cmFpbmVkKCcuJTJGdGVzdCUyRnNhdmVkX21vZGVsJTJGJykqKSUwQSUyMyUyMHByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0ZXN0JTJGc2F2ZWRfbW9kZWwlMkYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download processor from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base-960h&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># If processor files are in a directory (e.g. processor was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># processor = AutoProcessor.from_pretrained(&quot;./test/saved_model/&quot;)</span>`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function CX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWwlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function wX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWwlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function jX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbCUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWwuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function xX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbCUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function $X(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function kX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModel.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function ZX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JQcmVUcmFpbmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yUHJlVHJhaW5pbmcuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function LX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JQcmVUcmFpbmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JQcmVUcmFpbmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function BX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclByZVRyYWluaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function AX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclByZVRyYWluaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yUHJlVHJhaW5pbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JQcmVUcmFpbmluZy5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function RX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yUHJlVHJhaW5pbmclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JQcmVUcmFpbmluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function WX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yUHJlVHJhaW5pbmclMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclByZVRyYWluaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForPreTraining

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForPreTraining.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function JX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function VX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function GX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function EX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function PX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function SX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForCausalLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function UX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yTWFza2VkTE0uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function IX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function NX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck1hc2tlZExNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function XX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck1hc2tlZExNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yTWFza2VkTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function qX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTWFza2VkTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JNYXNrZWRMTS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function QX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTWFza2VkTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck1hc2tlZExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMaskedLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMaskedLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function HX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtdDUlMkZ0NS1iYXNlJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function DX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtdDUlMkZ0NS1iYXNlJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIlMkMlMjBvdXRwdXRfYXR0ZW50aW9ucyUzRFRydWUpJTBBbW9kZWwuY29uZmlnLm91dHB1dF9hdHRlbnRpb25zJTBBJTBBJTIzJTIwTG9hZGluZyUyMGZyb20lMjBhJTIwVEYlMjBjaGVja3BvaW50JTIwZmlsZSUyMGluc3RlYWQlMjBvZiUyMGElMjBQeVRvcmNoJTIwbW9kZWwlMjAoc2xvd2VyKSUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZ0Zl9tb2RlbCUyRnQ1X3RmX21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZ0NV90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/t5_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/t5_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function YX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function zX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLXQ1JTJGdDUtYmFzZSUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIlMkMlMjBvdXRwdXRfYXR0ZW50aW9ucyUzRFRydWUpJTBBbW9kZWwuY29uZmlnLm91dHB1dF9hdHRlbnRpb25zJTBBJTBBJTIzJTIwTG9hZGluZyUyMGZyb20lMjBhJTIwUHlUb3JjaCUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFRlbnNvckZsb3clMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnB0X21vZGVsJTJGdDVfcHRfbW9kZWxfY29uZmlnLmpzb24lMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGdDVfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function OX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU2VxMlNlcUxNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLXQ1JTJGdDUtYmFzZSUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function KX(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU2VxMlNlcUxNJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS10NSUyRnQ1LWJhc2UlMjIlMkMlMjBvdXRwdXRfYXR0ZW50aW9ucyUzRFRydWUpJTBBbW9kZWwuY29uZmlnLm91dHB1dF9hdHRlbnRpb25zJTBBJTBBJTIzJTIwTG9hZGluZyUyMGZyb20lMjBhJTIwUHlUb3JjaCUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFRlbnNvckZsb3clMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnB0X21vZGVsJTJGdDVfcHRfbW9kZWxfY29uZmlnLmpzb24lMjIpJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZ0NV9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;google-t5/t5-base&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/t5_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSeq2SeqLM.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/t5_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function e3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function o3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function t3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function n3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function r3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function a3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JTZXF1ZW5jZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSequenceClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function s3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yTXVsdGlwbGVDaG9pY2UuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function i3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function l3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function d3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yTXVsdGlwbGVDaG9pY2UuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function c3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTXVsdGlwbGVDaG9pY2UlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JNdWx0aXBsZUNob2ljZS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function m3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTXVsdGlwbGVDaG9pY2UlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck11bHRpcGxlQ2hvaWNlLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForMultipleChoice

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForMultipleChoice.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function f3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function g3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function p3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yTmV4dFNlbnRlbmNlUHJlZGljdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForNextSentencePrediction.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function u3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function h3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTmV4dFNlbnRlbmNlUHJlZGljdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function b3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yTmV4dFNlbnRlbmNlUHJlZGljdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yTmV4dFNlbnRlbmNlUHJlZGljdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JOZXh0U2VudGVuY2VQcmVkaWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvck5leHRTZW50ZW5jZVByZWRpY3Rpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForNextSentencePrediction

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForNextSentencePrediction.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function _3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function M3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function T3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yVG9rZW5DbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function y3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function F3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yVG9rZW5DbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function v3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yVG9rZW5DbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yVG9rZW5DbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JUb2tlbkNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclRva2VuQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForTokenClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForTokenClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function C3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function w3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function j3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function x3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function $3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JRdWVzdGlvbkFuc3dlcmluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function k3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yUXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Z3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEZXB0aEVzdGltYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function L3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEZXB0aEVzdGltYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yRGVwdGhFc3RpbWF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yRGVwdGhFc3RpbWF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function B3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function A3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function R3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function W3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function J3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function V3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function G3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVideoClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function E3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpZGVvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaWRlb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpZGVvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVideoClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVideoClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function P3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function S3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZEltYWdlTW9kZWxpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck1hc2tlZEltYWdlTW9kZWxpbmcuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function U3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck1hc2tlZEltYWdlTW9kZWxpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yTWFza2VkSW1hZ2VNb2RlbGluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedImageModeling.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function I3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvck1hc2tlZEltYWdlTW9kZWxpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForMaskedImageModeling

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForMaskedImageModeling.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function N3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JPYmplY3REZXRlY3Rpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck9iamVjdERldGVjdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function X3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JPYmplY3REZXRlY3Rpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yT2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yT2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvck9iamVjdERldGVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function q3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9ySW1hZ2VTZWdtZW50YXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Q3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbWFnZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbWFnZVNlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForImageSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function H3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VtYW50aWNTZWdtZW50YXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function D3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Y3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSemanticSegmentation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function z3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU2VtYW50aWNTZWdtZW50YXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlbWFudGljU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTZW1hbnRpY1NlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGYmVydF9weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSemanticSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSemanticSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSemanticSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function O3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbnN0YW5jZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9ySW5zdGFuY2VTZWdtZW50YXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForInstanceSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function K3(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JJbnN0YW5jZVNlZ21lbnRhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JJbnN0YW5jZVNlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckluc3RhbmNlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckluc3RhbmNlU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForInstanceSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForInstanceSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function eq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JVbml2ZXJzYWxTZWdtZW50YXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclVuaXZlcnNhbFNlZ21lbnRhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForUniversalSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function oq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JVbml2ZXJzYWxTZWdtZW50YXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVW5pdmVyc2FsU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVW5pdmVyc2FsU2VnbWVudGF0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclVuaXZlcnNhbFNlZ21lbnRhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForUniversalSegmentation

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForUniversalSegmentation.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function tq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdEltYWdlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function nq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdEltYWdlQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yWmVyb1Nob3RJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yWmVyb1Nob3RJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function rq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JaZXJvU2hvdEltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForZeroShotImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForZeroShotImageClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function aq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclplcm9TaG90SW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yWmVyb1Nob3RJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JaZXJvU2hvdEltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForZeroShotImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForZeroShotImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForZeroShotImageClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForZeroShotImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function sq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdE9iamVjdERldGVjdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yWmVyb1Nob3RPYmplY3REZXRlY3Rpb24uZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function iq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdE9iamVjdERldGVjdGlvbiUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JaZXJvU2hvdE9iamVjdERldGVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90T2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclplcm9TaG90T2JqZWN0RGV0ZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZiZXJ0X3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForZeroShotObjectDetection

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForZeroShotObjectDetection.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function lq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function dq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function cq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yQXVkaW9DbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForAudioClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function mq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JBdWRpb0NsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function fq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0ZyYW1lQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvRnJhbWVDbGFzc2lmaWNhdGlvbi5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function gq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb0ZyYW1lQ2xhc3NpZmljYXRpb24lMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9GcmFtZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9GcmFtZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvRnJhbWVDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioFrameClassification

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioFrameClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function pq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDVEMlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNUQy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function uq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JDVEMlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ1RDLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ1RDLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNUQy5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForCTC

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForCTC.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function hq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function bq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXEuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXEuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function _q(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXElMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yU3BlZWNoU2VxMlNlcS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Mq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXElMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Tq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU3BlZWNoU2VxMlNlcSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXEuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSpeechSeq2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function yq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yU3BlZWNoU2VxMlNlcSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yU3BlZWNoU2VxMlNlcS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JTcGVlY2hTZXEyU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclNwZWVjaFNlcTJTZXEuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForSpeechSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Fq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb1hWZWN0b3IlMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvWFZlY3Rvci5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function vq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JBdWRpb1hWZWN0b3IlMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9YVmVjdG9yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9YVmVjdG9yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvWFZlY3Rvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForAudioXVector

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioXVector.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Cq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUYWJsZVF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGdGFwYXMtYmFzZS1maW5ldHVuZWQtd3RxJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVGFibGVRdWVzdGlvbkFuc3dlcmluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function wq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JUYWJsZVF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRnRhcGFzLWJhc2UtZmluZXR1bmVkLXd0cSUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVGFibGVRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGdGFwYXMtYmFzZS1maW5ldHVuZWQtd3RxJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlKSUwQW1vZGVsLmNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQSUyMyUyMExvYWRpbmclMjBmcm9tJTIwYSUyMFRGJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwUHlUb3JjaCUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGZfbW9kZWwlMkZ0YXBhc190Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRnRhcGFzX3RmX2NoZWNrcG9pbnQuY2twdC5pbmRleCUyMiUyQyUyMGZyb21fdGYlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/tapas_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/tapas_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function jq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZ0YXBhcy1iYXNlLWZpbmV0dW5lZC13dHElMjIpJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function xq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclRhYmxlUXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JUYWJsZVF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZ0YXBhcy1iYXNlLWZpbmV0dW5lZC13dHElMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yVGFibGVRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlJTJGdGFwYXMtYmFzZS1maW5ldHVuZWQtd3RxJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlKSUwQW1vZGVsLmNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQSUyMyUyMExvYWRpbmclMjBmcm9tJTIwYSUyMFB5VG9yY2glMjBjaGVja3BvaW50JTIwZmlsZSUyMGluc3RlYWQlMjBvZiUyMGElMjBUZW5zb3JGbG93JTIwbW9kZWwlMjAoc2xvd2VyKSUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMi4lMkZwdF9tb2RlbCUyRnRhcGFzX3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JUYWJsZVF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZ0YXBhc19weXRvcmNoX21vZGVsLmJpbiUyMiUyQyUyMGZyb21fcHQlM0RUcnVlJTJDJTIwY29uZmlnJTNEY29uZmlnJTBBKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForTableQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;google/tapas-base-finetuned-wtq&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/tapas_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForTableQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/tapas_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function $q(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyaW1waXJhJTJGbGF5b3V0bG0tZG9jdW1lbnQtcWElMjIlMkMlMjByZXZpc2lvbiUzRCUyMjUyZTAxYjMlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDocumentQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function kq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRvY3VtZW50UXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUyMmltcGlyYSUyRmxheW91dGxtLWRvY3VtZW50LXFhJTIyJTJDJTIwcmV2aXNpb24lM0QlMjI1MmUwMWIzJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJpbXBpcmElMkZsYXlvdXRsbS1kb2N1bWVudC1xYSUyMiUyQyUyMHJldmlzaW9uJTNEJTIyNTJlMDFiMyUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGbGF5b3V0bG1fdGZfbW9kZWxfY29uZmlnLmpzb24lMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGdGZfbW9kZWwlMkZsYXlvdXRsbV90Zl9jaGVja3BvaW50LmNrcHQuaW5kZXglMjIlMkMlMjBmcm9tX3RmJTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForDocumentQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/layoutlm_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDocumentQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/layoutlm_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Zq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckRvY3VtZW50UXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJpbXBpcmElMkZsYXlvdXRsbS1kb2N1bWVudC1xYSUyMiUyQyUyMHJldmlzaW9uJTNEJTIyNTJlMDFiMyUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yRG9jdW1lbnRRdWVzdGlvbkFuc3dlcmluZy5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForDocumentQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForDocumentQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Lq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvckRvY3VtZW50UXVlc3Rpb25BbnN3ZXJpbmclMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJpbXBpcmElMkZsYXlvdXRsbS1kb2N1bWVudC1xYSUyMiUyQyUyMHJldmlzaW9uJTNEJTIyNTJlMDFiMyUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JEb2N1bWVudFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fcHJldHJhaW5lZCglMjJpbXBpcmElMkZsYXlvdXRsbS1kb2N1bWVudC1xYSUyMiUyQyUyMHJldmlzaW9uJTNEJTIyNTJlMDFiMyUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZsYXlvdXRsbV9wdF9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yRG9jdW1lbnRRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnB0X21vZGVsJTJGbGF5b3V0bG1fcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForDocumentQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForDocumentQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForDocumentQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;impira/layoutlm-document-qa&quot;</span>, revision=<span class="hljs-string">&quot;52e01b3&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/layoutlm_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForDocumentQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/layoutlm_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Bq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmRhbmRlbGluJTJGdmlsdC1iMzItZmluZXR1bmVkLXZxYSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpc3VhbFF1ZXN0aW9uQW5zd2VyaW5nLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVisualQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;dandelin/vilt-b32-finetuned-vqa&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Aq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZyUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGFuZGVsaW4lMkZ2aWx0LWIzMi1maW5ldHVuZWQtdnFhJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaXN1YWxRdWVzdGlvbkFuc3dlcmluZy5mcm9tX3ByZXRyYWluZWQoJTIyZGFuZGVsaW4lMkZ2aWx0LWIzMi1maW5ldHVuZWQtdnFhJTIyJTJDJTIwb3V0cHV0X2F0dGVudGlvbnMlM0RUcnVlKSUwQW1vZGVsLmNvbmZpZy5vdXRwdXRfYXR0ZW50aW9ucyUwQSUwQSUyMyUyMExvYWRpbmclMjBmcm9tJTIwYSUyMFRGJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwUHlUb3JjaCUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGdGZfbW9kZWwlMkZ2aWx0X3RmX21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yVmlzdWFsUXVlc3Rpb25BbnN3ZXJpbmcuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRnZpbHRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVisualQuestionAnswering

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;dandelin/vilt-b32-finetuned-vqa&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_pretrained(<span class="hljs-string">&quot;dandelin/vilt-b32-finetuned-vqa&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/vilt_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVisualQuestionAnswering.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/vilt_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Rq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaXNpb24yU2VxJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fY29uZmlnKGNvbmZpZyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Wq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWxGb3JWaXNpb24yU2VxJTBBJTBBJTIzJTIwRG93bmxvYWQlMjBtb2RlbCUyMGFuZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpc2lvbjJTZXEuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUwQSUyMyUyMFVwZGF0ZSUyMGNvbmZpZ3VyYXRpb24lMjBkdXJpbmclMjBsb2FkaW5nJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBURiUyMGNoZWNrcG9pbnQlMjBmaWxlJTIwaW5zdGVhZCUyMG9mJTIwYSUyMFB5VG9yY2glMjBtb2RlbCUyMChzbG93ZXIpJTBBY29uZmlnJTIwJTNEJTIwQXV0b0NvbmZpZy5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRmX21vZGVsJTJGYmVydF90Zl9tb2RlbF9jb25maWcuanNvbiUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclZpc2lvbjJTZXEuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZ0Zl9tb2RlbCUyRmJlcnRfdGZfY2hlY2twb2ludC5ja3B0LmluZGV4JTIyJTJDJTIwZnJvbV90ZiUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a TF checkpoint file instead of a PyTorch model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./tf_model/bert_tf_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./tf_model/bert_tf_checkpoint.ckpt.index&quot;</span>, from_tf=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Jq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclZpc2lvbjJTZXElMEElMEElMjMlMjBEb3dubG9hZCUyMGNvbmZpZ3VyYXRpb24lMjBmcm9tJTIwaHVnZ2luZ2ZhY2UuY28lMjBhbmQlMjBjYWNoZS4lMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQXV0b01vZGVsRm9yVmlzaW9uMlNlcS5mcm9tX2NvbmZpZyhjb25maWcp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Vq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBURkF1dG9Nb2RlbEZvclZpc2lvbjJTZXElMEElMEElMjMlMjBEb3dubG9hZCUyMG1vZGVsJTIwYW5kJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEElMjMlMjBVcGRhdGUlMjBjb25maWd1cmF0aW9uJTIwZHVyaW5nJTIwbG9hZGluZyUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B5dG9yY2hfbW9kZWwuYmluJTIyJTJDJTIwZnJvbV9wdCUzRFRydWUlMkMlMjBjb25maWclM0Rjb25maWclMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, TFAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Gq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yVmlzaW9uMlNlcSUwQSUwQSUyMyUyMERvd25sb2FkJTIwY29uZmlndXJhdGlvbiUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWNvbmZpZyUyMCUzRCUyMEF1dG9Db25maWcuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclZpc2lvbjJTZXEuZnJvbV9jb25maWcoY29uZmlnKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_config(config)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Eq(v){let n,F="Examples:",c,s,m;return s=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBGbGF4QXV0b01vZGVsRm9yVmlzaW9uMlNlcSUwQSUwQSUyMyUyMERvd25sb2FkJTIwbW9kZWwlMjBhbmQlMjBjb25maWd1cmF0aW9uJTIwZnJvbSUyMGh1Z2dpbmdmYWNlLmNvJTIwYW5kJTIwY2FjaGUuJTBBbW9kZWwlMjAlM0QlMjBGbGF4QXV0b01vZGVsRm9yVmlzaW9uMlNlcS5mcm9tX3ByZXRyYWluZWQoJTIyZ29vZ2xlLWJlcnQlMkZiZXJ0LWJhc2UtY2FzZWQlMjIpJTBBJTBBJTIzJTIwVXBkYXRlJTIwY29uZmlndXJhdGlvbiUyMGR1cmluZyUyMGxvYWRpbmclMEFtb2RlbCUyMCUzRCUyMEZsYXhBdXRvTW9kZWxGb3JWaXNpb24yU2VxLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiUyQyUyMG91dHB1dF9hdHRlbnRpb25zJTNEVHJ1ZSklMEFtb2RlbC5jb25maWcub3V0cHV0X2F0dGVudGlvbnMlMEElMEElMjMlMjBMb2FkaW5nJTIwZnJvbSUyMGElMjBQeVRvcmNoJTIwY2hlY2twb2ludCUyMGZpbGUlMjBpbnN0ZWFkJTIwb2YlMjBhJTIwVGVuc29yRmxvdyUyMG1vZGVsJTIwKHNsb3dlciklMEFjb25maWclMjAlM0QlMjBBdXRvQ29uZmlnLmZyb21fcHJldHJhaW5lZCglMjIuJTJGcHRfbW9kZWwlMkZiZXJ0X3B0X21vZGVsX2NvbmZpZy5qc29uJTIyKSUwQW1vZGVsJTIwJTNEJTIwRmxheEF1dG9Nb2RlbEZvclZpc2lvbjJTZXEuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMi4lMkZwdF9tb2RlbCUyRmJlcnRfcHl0b3JjaF9tb2RlbC5iaW4lMjIlMkMlMjBmcm9tX3B0JTNEVHJ1ZSUyQyUyMGNvbmZpZyUzRGNvbmZpZyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, FlaxAutoModelForVision2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Download model and configuration from huggingface.co and cache.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Update configuration during loading</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, output_attentions=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.output_attentions
<span class="hljs-literal">True</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Loading from a PyTorch checkpoint file instead of a TensorFlow model (slower)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config = AutoConfig.from_pretrained(<span class="hljs-string">&quot;./pt_model/bert_pt_model_config.json&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxAutoModelForVision2Seq.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;./pt_model/bert_pytorch_model.bin&quot;</span>, from_pt=<span class="hljs-literal">True</span>, config=config
<span class="hljs-meta">... </span>)`,wrap:!1}}),{c(){n=i("p"),n.textContent=F,c=r(),p(s.$$.fragment)},l(e){n=l(e,"P",{"data-svelte-h":!0}),f(n)!=="svelte-kvfsh7"&&(n.textContent=F),c=a(e),u(s.$$.fragment,e)},m(e,T){y(e,n,T),y(e,c,T),h(s,e,T),m=!0},p:x,i(e){m||(b(s.$$.fragment,e),m=!0)},o(e){_(s.$$.fragment,e),m=!1},d(e){e&&(d(n),d(c)),M(s,e)}}}function Pq(v){let n,F,c,s,m,e,T,SG="多くの場合、<code>from_pretrained()</code>メソッドに与えられた事前学習済みモデルの名前やパスから、使用したいアーキテクチャを推測することができます。自動クラスはこの仕事をあなたに代わって行うためにここにありますので、事前学習済みの重み/設定/語彙への名前/パスを与えると自動的に関連するモデルを取得できます。",xC,vd,UG='<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoConfig">AutoConfig</a>、<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoModel">AutoModel</a>、<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>のいずれかをインスタンス化すると、関連するアーキテクチャのクラスが直接作成されます。例えば、',$C,Cd,kC,wd,IG='これは<a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertModel">BertModel</a>のインスタンスであるモデルを作成します。',ZC,jd,NG="各タスクごと、そして各バックエンド（PyTorch、TensorFlow、またはFlax）ごとに<code>AutoModel</code>のクラスが存在します。",LC,xd,BC,$d,XG="それぞれの自動クラスには、カスタムクラスで拡張するためのメソッドがあります。例えば、<code>NewModel</code>というモデルのカスタムクラスを定義した場合、<code>NewModelConfig</code>を確保しておけばこのようにして自動クラスに追加することができます：",AC,kd,RC,Zd,qG="その後、通常どおりauto classesを使用することができるようになります！",WC,vs,JC,Ld,VC,ce,Bd,Vj,pu,QG=`This is a generic configuration class that will be instantiated as one of the configuration classes of the library
when created with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoConfig.from_pretrained">from_pretrained()</a> class method.`,Gj,uu,HG="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Ej,Uo,Ad,Pj,hu,DG="Instantiate one of the configuration classes of the library from a pretrained model configuration.",Sj,bu,YG=`The configuration class to instantiate is selected based on the <code>model_type</code> property of the config object that
is loaded, or when it’s missing, by falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Uj,_u,zG='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> (ALBERT model)</li> <li><strong>align</strong> — <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a> (ALIGN model)</li> <li><strong>altclip</strong> — <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPConfig">AltCLIPConfig</a> (AltCLIP model)</li> <li><strong>audio-spectrogram-transformer</strong> — <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a> (Audio Spectrogram Transformer model)</li> <li><strong>autoformer</strong> — <a href="/docs/transformers/main/ja/model_doc/autoformer#transformers.AutoformerConfig">AutoformerConfig</a> (Autoformer model)</li> <li><strong>bark</strong> — <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkConfig">BarkConfig</a> (Bark model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> (BART model)</li> <li><strong>beit</strong> — <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a> (BEiT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> (BERT model)</li> <li><strong>bert-generation</strong> — <a href="/docs/transformers/main/ja/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> (Bert Generation model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong> — <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong> — <a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> (BioGpt model)</li> <li><strong>bit</strong> — <a href="/docs/transformers/main/ja/model_doc/bit#transformers.BitConfig">BitConfig</a> (BiT model)</li> <li><strong>blenderbot</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> (BlenderbotSmall model)</li> <li><strong>blip</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipConfig">BlipConfig</a> (BLIP model)</li> <li><strong>blip-2</strong> — <a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> (BLIP-2 model)</li> <li><strong>bloom</strong> — <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> (BLOOM model)</li> <li><strong>bridgetower</strong> — <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a> (BridgeTower model)</li> <li><strong>bros</strong> — <a href="/docs/transformers/main/ja/model_doc/bros#transformers.BrosConfig">BrosConfig</a> (BROS model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> (CamemBERT model)</li> <li><strong>canine</strong> — <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineConfig">CanineConfig</a> (CANINE model)</li> <li><strong>chinese_clip</strong> — <a href="/docs/transformers/main/ja/model_doc/chinese_clip#transformers.ChineseCLIPConfig">ChineseCLIPConfig</a> (Chinese-CLIP model)</li> <li><strong>clap</strong> — <a href="/docs/transformers/main/ja/model_doc/clap#transformers.ClapConfig">ClapConfig</a> (CLAP model)</li> <li><strong>clip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> (CLIP model)</li> <li><strong>clip_vision_model</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionConfig">CLIPVisionConfig</a> (CLIPVisionModel model)</li> <li><strong>clipseg</strong> — <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> (CLIPSeg model)</li> <li><strong>clvp</strong> — <a href="/docs/transformers/main/ja/model_doc/clvp#transformers.ClvpConfig">ClvpConfig</a> (CLVP model)</li> <li><strong>code_llama</strong> — <code>LlamaConfig</code> (CodeLlama model)</li> <li><strong>codegen</strong> — <a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenConfig">CodeGenConfig</a> (CodeGen model)</li> <li><strong>conditional_detr</strong> — <a href="/docs/transformers/main/ja/model_doc/conditional_detr#transformers.ConditionalDetrConfig">ConditionalDetrConfig</a> (Conditional DETR model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> (ConvBERT model)</li> <li><strong>convnext</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong> — <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> (ConvNeXTV2 model)</li> <li><strong>cpmant</strong> — <a href="/docs/transformers/main/ja/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a> (CPM-Ant model)</li> <li><strong>ctrl</strong> — <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> (CTRL model)</li> <li><strong>cvt</strong> — <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> (CvT model)</li> <li><strong>data2vec-audio</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> (Data2VecAudio model)</li> <li><strong>data2vec-text</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> (Data2VecText model)</li> <li><strong>data2vec-vision</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> (Data2VecVision model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> (DeBERTa-v2 model)</li> <li><strong>decision_transformer</strong> — <a href="/docs/transformers/main/ja/model_doc/decision_transformer#transformers.DecisionTransformerConfig">DecisionTransformerConfig</a> (Decision Transformer model)</li> <li><strong>deformable_detr</strong> — <a href="/docs/transformers/main/ja/model_doc/deformable_detr#transformers.DeformableDetrConfig">DeformableDetrConfig</a> (Deformable DETR model)</li> <li><strong>deit</strong> — <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> (DeiT model)</li> <li><strong>depth_anything</strong> — <code>DepthAnythingConfig</code> (Depth Anything model)</li> <li><strong>deta</strong> — <a href="/docs/transformers/main/ja/model_doc/deta#transformers.DetaConfig">DetaConfig</a> (DETA model)</li> <li><strong>detr</strong> — <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrConfig">DetrConfig</a> (DETR model)</li> <li><strong>dinat</strong> — <a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatConfig">DinatConfig</a> (DiNAT model)</li> <li><strong>dinov2</strong> — <code>Dinov2Config</code> (DINOv2 model)</li> <li><strong>distilbert</strong> — <code>DistilBertConfig</code> (DistilBERT model)</li> <li><strong>donut-swin</strong> — <code>DonutSwinConfig</code> (DonutSwin model)</li> <li><strong>dpr</strong> — <code>DPRConfig</code> (DPR model)</li> <li><strong>dpt</strong> — <code>DPTConfig</code> (DPT model)</li> <li><strong>efficientformer</strong> — <code>EfficientFormerConfig</code> (EfficientFormer model)</li> <li><strong>efficientnet</strong> — <code>EfficientNetConfig</code> (EfficientNet model)</li> <li><strong>electra</strong> — <code>ElectraConfig</code> (ELECTRA model)</li> <li><strong>encodec</strong> — <code>EncodecConfig</code> (EnCodec model)</li> <li><strong>encoder-decoder</strong> — <code>EncoderDecoderConfig</code> (Encoder decoder model)</li> <li><strong>ernie</strong> — <code>ErnieConfig</code> (ERNIE model)</li> <li><strong>ernie_m</strong> — <code>ErnieMConfig</code> (ErnieM model)</li> <li><strong>esm</strong> — <code>EsmConfig</code> (ESM model)</li> <li><strong>falcon</strong> — <code>FalconConfig</code> (Falcon model)</li> <li><strong>fastspeech2_conformer</strong> — <code>FastSpeech2ConformerConfig</code> (FastSpeech2Conformer model)</li> <li><strong>flaubert</strong> — <code>FlaubertConfig</code> (FlauBERT model)</li> <li><strong>flava</strong> — <code>FlavaConfig</code> (FLAVA model)</li> <li><strong>fnet</strong> — <code>FNetConfig</code> (FNet model)</li> <li><strong>focalnet</strong> — <code>FocalNetConfig</code> (FocalNet model)</li> <li><strong>fsmt</strong> — <code>FSMTConfig</code> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong> — <code>FunnelConfig</code> (Funnel Transformer model)</li> <li><strong>fuyu</strong> — <code>FuyuConfig</code> (Fuyu model)</li> <li><strong>git</strong> — <code>GitConfig</code> (GIT model)</li> <li><strong>glpn</strong> — <code>GLPNConfig</code> (GLPN model)</li> <li><strong>gpt-sw3</strong> — <code>GPT2Config</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>GPT2Config</code> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong> — <code>GPTBigCodeConfig</code> (GPTBigCode model)</li> <li><strong>gpt_neo</strong> — <code>GPTNeoConfig</code> (GPT Neo model)</li> <li><strong>gpt_neox</strong> — <code>GPTNeoXConfig</code> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong> — <code>GPTNeoXJapaneseConfig</code> (GPT NeoX Japanese model)</li> <li><strong>gptj</strong> — <code>GPTJConfig</code> (GPT-J model)</li> <li><strong>gptsan-japanese</strong> — <code>GPTSanJapaneseConfig</code> (GPTSAN-japanese model)</li> <li><strong>graphormer</strong> — <code>GraphormerConfig</code> (Graphormer model)</li> <li><strong>groupvit</strong> — <code>GroupViTConfig</code> (GroupViT model)</li> <li><strong>hubert</strong> — <code>HubertConfig</code> (Hubert model)</li> <li><strong>ibert</strong> — <code>IBertConfig</code> (I-BERT model)</li> <li><strong>idefics</strong> — <code>IdeficsConfig</code> (IDEFICS model)</li> <li><strong>imagegpt</strong> — <code>ImageGPTConfig</code> (ImageGPT model)</li> <li><strong>informer</strong> — <code>InformerConfig</code> (Informer model)</li> <li><strong>instructblip</strong> — <code>InstructBlipConfig</code> (InstructBLIP model)</li> <li><strong>jukebox</strong> — <code>JukeboxConfig</code> (Jukebox model)</li> <li><strong>kosmos-2</strong> — <code>Kosmos2Config</code> (KOSMOS-2 model)</li> <li><strong>layoutlm</strong> — <code>LayoutLMConfig</code> (LayoutLM model)</li> <li><strong>layoutlmv2</strong> — <code>LayoutLMv2Config</code> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong> — <code>LayoutLMv3Config</code> (LayoutLMv3 model)</li> <li><strong>led</strong> — <code>LEDConfig</code> (LED model)</li> <li><strong>levit</strong> — <code>LevitConfig</code> (LeViT model)</li> <li><strong>lilt</strong> — <code>LiltConfig</code> (LiLT model)</li> <li><strong>llama</strong> — <code>LlamaConfig</code> (LLaMA model)</li> <li><strong>llava</strong> — <code>LlavaConfig</code> (LLaVa model)</li> <li><strong>longformer</strong> — <code>LongformerConfig</code> (Longformer model)</li> <li><strong>longt5</strong> — <code>LongT5Config</code> (LongT5 model)</li> <li><strong>luke</strong> — <code>LukeConfig</code> (LUKE model)</li> <li><strong>lxmert</strong> — <code>LxmertConfig</code> (LXMERT model)</li> <li><strong>m2m_100</strong> — <code>M2M100Config</code> (M2M100 model)</li> <li><strong>marian</strong> — <code>MarianConfig</code> (Marian model)</li> <li><strong>markuplm</strong> — <code>MarkupLMConfig</code> (MarkupLM model)</li> <li><strong>mask2former</strong> — <code>Mask2FormerConfig</code> (Mask2Former model)</li> <li><strong>maskformer</strong> — <code>MaskFormerConfig</code> (MaskFormer model)</li> <li><strong>maskformer-swin</strong> — <code>MaskFormerSwinConfig</code> (MaskFormerSwin model)</li> <li><strong>mbart</strong> — <code>MBartConfig</code> (mBART model)</li> <li><strong>mctct</strong> — <code>MCTCTConfig</code> (M-CTC-T model)</li> <li><strong>mega</strong> — <code>MegaConfig</code> (MEGA model)</li> <li><strong>megatron-bert</strong> — <code>MegatronBertConfig</code> (Megatron-BERT model)</li> <li><strong>mgp-str</strong> — <code>MgpstrConfig</code> (MGP-STR model)</li> <li><strong>mistral</strong> — <code>MistralConfig</code> (Mistral model)</li> <li><strong>mixtral</strong> — <code>MixtralConfig</code> (Mixtral model)</li> <li><strong>mobilebert</strong> — <code>MobileBertConfig</code> (MobileBERT model)</li> <li><strong>mobilenet_v1</strong> — <code>MobileNetV1Config</code> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong> — <code>MobileNetV2Config</code> (MobileNetV2 model)</li> <li><strong>mobilevit</strong> — <code>MobileViTConfig</code> (MobileViT model)</li> <li><strong>mobilevitv2</strong> — <code>MobileViTV2Config</code> (MobileViTV2 model)</li> <li><strong>mpnet</strong> — <code>MPNetConfig</code> (MPNet model)</li> <li><strong>mpt</strong> — <code>MptConfig</code> (MPT model)</li> <li><strong>mra</strong> — <code>MraConfig</code> (MRA model)</li> <li><strong>mt5</strong> — <code>MT5Config</code> (MT5 model)</li> <li><strong>musicgen</strong> — <code>MusicgenConfig</code> (MusicGen model)</li> <li><strong>mvp</strong> — <code>MvpConfig</code> (MVP model)</li> <li><strong>nat</strong> — <code>NatConfig</code> (NAT model)</li> <li><strong>nezha</strong> — <code>NezhaConfig</code> (Nezha model)</li> <li><strong>nllb-moe</strong> — <code>NllbMoeConfig</code> (NLLB-MOE model)</li> <li><strong>nougat</strong> — <code>VisionEncoderDecoderConfig</code> (Nougat model)</li> <li><strong>nystromformer</strong> — <code>NystromformerConfig</code> (Nyströmformer model)</li> <li><strong>oneformer</strong> — <code>OneFormerConfig</code> (OneFormer model)</li> <li><strong>open-llama</strong> — <code>OpenLlamaConfig</code> (OpenLlama model)</li> <li><strong>openai-gpt</strong> — <code>OpenAIGPTConfig</code> (OpenAI GPT model)</li> <li><strong>opt</strong> — <code>OPTConfig</code> (OPT model)</li> <li><strong>owlv2</strong> — <code>Owlv2Config</code> (OWLv2 model)</li> <li><strong>owlvit</strong> — <code>OwlViTConfig</code> (OWL-ViT model)</li> <li><strong>patchtsmixer</strong> — <code>PatchTSMixerConfig</code> (PatchTSMixer model)</li> <li><strong>patchtst</strong> — <code>PatchTSTConfig</code> (PatchTST model)</li> <li><strong>pegasus</strong> — <code>PegasusConfig</code> (Pegasus model)</li> <li><strong>pegasus_x</strong> — <code>PegasusXConfig</code> (PEGASUS-X model)</li> <li><strong>perceiver</strong> — <code>PerceiverConfig</code> (Perceiver model)</li> <li><strong>persimmon</strong> — <code>PersimmonConfig</code> (Persimmon model)</li> <li><strong>phi</strong> — <code>PhiConfig</code> (Phi model)</li> <li><strong>pix2struct</strong> — <code>Pix2StructConfig</code> (Pix2Struct model)</li> <li><strong>plbart</strong> — <code>PLBartConfig</code> (PLBart model)</li> <li><strong>poolformer</strong> — <code>PoolFormerConfig</code> (PoolFormer model)</li> <li><strong>pop2piano</strong> — <code>Pop2PianoConfig</code> (Pop2Piano model)</li> <li><strong>prophetnet</strong> — <code>ProphetNetConfig</code> (ProphetNet model)</li> <li><strong>pvt</strong> — <code>PvtConfig</code> (PVT model)</li> <li><strong>qdqbert</strong> — <code>QDQBertConfig</code> (QDQBert model)</li> <li><strong>qwen2</strong> — <code>Qwen2Config</code> (Qwen2 model)</li> <li><strong>rag</strong> — <code>RagConfig</code> (RAG model)</li> <li><strong>realm</strong> — <code>RealmConfig</code> (REALM model)</li> <li><strong>reformer</strong> — <code>ReformerConfig</code> (Reformer model)</li> <li><strong>regnet</strong> — <code>RegNetConfig</code> (RegNet model)</li> <li><strong>rembert</strong> — <code>RemBertConfig</code> (RemBERT model)</li> <li><strong>resnet</strong> — <code>ResNetConfig</code> (ResNet model)</li> <li><strong>retribert</strong> — <code>RetriBertConfig</code> (RetriBERT model)</li> <li><strong>roberta</strong> — <code>RobertaConfig</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>RobertaPreLayerNormConfig</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong> — <code>RoCBertConfig</code> (RoCBert model)</li> <li><strong>roformer</strong> — <code>RoFormerConfig</code> (RoFormer model)</li> <li><strong>rwkv</strong> — <code>RwkvConfig</code> (RWKV model)</li> <li><strong>sam</strong> — <code>SamConfig</code> (SAM model)</li> <li><strong>seamless_m4t</strong> — <code>SeamlessM4TConfig</code> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong> — <code>SeamlessM4Tv2Config</code> (SeamlessM4Tv2 model)</li> <li><strong>segformer</strong> — <code>SegformerConfig</code> (SegFormer model)</li> <li><strong>sew</strong> — <code>SEWConfig</code> (SEW model)</li> <li><strong>sew-d</strong> — <code>SEWDConfig</code> (SEW-D model)</li> <li><strong>siglip</strong> — <code>SiglipConfig</code> (SigLIP model)</li> <li><strong>siglip_vision_model</strong> — <code>SiglipVisionConfig</code> (SiglipVisionModel model)</li> <li><strong>speech-encoder-decoder</strong> — <code>SpeechEncoderDecoderConfig</code> (Speech Encoder decoder model)</li> <li><strong>speech_to_text</strong> — <code>Speech2TextConfig</code> (Speech2Text model)</li> <li><strong>speech_to_text_2</strong> — <code>Speech2Text2Config</code> (Speech2Text2 model)</li> <li><strong>speecht5</strong> — <code>SpeechT5Config</code> (SpeechT5 model)</li> <li><strong>splinter</strong> — <code>SplinterConfig</code> (Splinter model)</li> <li><strong>squeezebert</strong> — <code>SqueezeBertConfig</code> (SqueezeBERT model)</li> <li><strong>stablelm</strong> — <code>StableLmConfig</code> (StableLm model)</li> <li><strong>swiftformer</strong> — <code>SwiftFormerConfig</code> (SwiftFormer model)</li> <li><strong>swin</strong> — <code>SwinConfig</code> (Swin Transformer model)</li> <li><strong>swin2sr</strong> — <code>Swin2SRConfig</code> (Swin2SR model)</li> <li><strong>swinv2</strong> — <code>Swinv2Config</code> (Swin Transformer V2 model)</li> <li><strong>switch_transformers</strong> — <code>SwitchTransformersConfig</code> (SwitchTransformers model)</li> <li><strong>t5</strong> — <code>T5Config</code> (T5 model)</li> <li><strong>table-transformer</strong> — <code>TableTransformerConfig</code> (Table Transformer model)</li> <li><strong>tapas</strong> — <code>TapasConfig</code> (TAPAS model)</li> <li><strong>time_series_transformer</strong> — <code>TimeSeriesTransformerConfig</code> (Time Series Transformer model)</li> <li><strong>timesformer</strong> — <code>TimesformerConfig</code> (TimeSformer model)</li> <li><strong>timm_backbone</strong> — <code>TimmBackboneConfig</code> (TimmBackbone model)</li> <li><strong>trajectory_transformer</strong> — <code>TrajectoryTransformerConfig</code> (Trajectory Transformer model)</li> <li><strong>transfo-xl</strong> — <code>TransfoXLConfig</code> (Transformer-XL model)</li> <li><strong>trocr</strong> — <code>TrOCRConfig</code> (TrOCR model)</li> <li><strong>tvlt</strong> — <code>TvltConfig</code> (TVLT model)</li> <li><strong>tvp</strong> — <code>TvpConfig</code> (TVP model)</li> <li><strong>umt5</strong> — <code>UMT5Config</code> (UMT5 model)</li> <li><strong>unispeech</strong> — <code>UniSpeechConfig</code> (UniSpeech model)</li> <li><strong>unispeech-sat</strong> — <code>UniSpeechSatConfig</code> (UniSpeechSat model)</li> <li><strong>univnet</strong> — <code>UnivNetConfig</code> (UnivNet model)</li> <li><strong>upernet</strong> — <code>UperNetConfig</code> (UPerNet model)</li> <li><strong>van</strong> — <code>VanConfig</code> (VAN model)</li> <li><strong>videomae</strong> — <code>VideoMAEConfig</code> (VideoMAE model)</li> <li><strong>vilt</strong> — <code>ViltConfig</code> (ViLT model)</li> <li><strong>vipllava</strong> — <code>VipLlavaConfig</code> (VipLlava model)</li> <li><strong>vision-encoder-decoder</strong> — <code>VisionEncoderDecoderConfig</code> (Vision Encoder decoder model)</li> <li><strong>vision-text-dual-encoder</strong> — <code>VisionTextDualEncoderConfig</code> (VisionTextDualEncoder model)</li> <li><strong>visual_bert</strong> — <code>VisualBertConfig</code> (VisualBERT model)</li> <li><strong>vit</strong> — <code>ViTConfig</code> (ViT model)</li> <li><strong>vit_hybrid</strong> — <code>ViTHybridConfig</code> (ViT Hybrid model)</li> <li><strong>vit_mae</strong> — <code>ViTMAEConfig</code> (ViTMAE model)</li> <li><strong>vit_msn</strong> — <code>ViTMSNConfig</code> (ViTMSN model)</li> <li><strong>vitdet</strong> — <code>VitDetConfig</code> (VitDet model)</li> <li><strong>vitmatte</strong> — <code>VitMatteConfig</code> (ViTMatte model)</li> <li><strong>vits</strong> — <code>VitsConfig</code> (VITS model)</li> <li><strong>vivit</strong> — <code>VivitConfig</code> (ViViT model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2Config</code> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong> — <code>Wav2Vec2BertConfig</code> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong> — <code>Wav2Vec2ConformerConfig</code> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong> — <code>WavLMConfig</code> (WavLM model)</li> <li><strong>whisper</strong> — <code>WhisperConfig</code> (Whisper model)</li> <li><strong>xclip</strong> — <code>XCLIPConfig</code> (X-CLIP model)</li> <li><strong>xglm</strong> — <code>XGLMConfig</code> (XGLM model)</li> <li><strong>xlm</strong> — <code>XLMConfig</code> (XLM model)</li> <li><strong>xlm-prophetnet</strong> — <code>XLMProphetNetConfig</code> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong> — <code>XLMRobertaConfig</code> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong> — <code>XLMRobertaXLConfig</code> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong> — <code>XLNetConfig</code> (XLNet model)</li> <li><strong>xmod</strong> — <code>XmodConfig</code> (X-MOD model)</li> <li><strong>yolos</strong> — <code>YolosConfig</code> (YOLOS model)</li> <li><strong>yoso</strong> — <code>YosoConfig</code> (YOSO model)</li>',Ij,Cs,Nj,ws,Rd,Xj,Mu,OG="Register a new configuration for this class.",GC,Wd,EC,me,Jd,qj,Tu,KG=`This is a generic tokenizer class that will be instantiated as one of the tokenizer classes of the library when
created with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer.from_pretrained">AutoTokenizer.from_pretrained()</a> class method.`,Qj,yu,eE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Hj,Io,Vd,Dj,Fu,oE="Instantiate one of the tokenizer classes of the library from a pretrained model vocabulary.",Yj,vu,tE=`The tokenizer class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,zj,Cu,nE='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertTokenizer">AlbertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertTokenizerFast">AlbertTokenizerFast</a> (ALBERT model)</li> <li><strong>align</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (ALIGN model)</li> <li><strong>bark</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Bark model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartTokenizer">BartTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartTokenizerFast">BartTokenizerFast</a> (BART model)</li> <li><strong>barthez</strong> — <a href="/docs/transformers/main/ja/model_doc/barthez#transformers.BarthezTokenizer">BarthezTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/barthez#transformers.BarthezTokenizerFast">BarthezTokenizerFast</a> (BARThez model)</li> <li><strong>bartpho</strong> — <a href="/docs/transformers/main/ja/model_doc/bartpho#transformers.BartphoTokenizer">BartphoTokenizer</a> (BARTpho model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (BERT model)</li> <li><strong>bert-generation</strong> — <a href="/docs/transformers/main/ja/model_doc/bert-generation#transformers.BertGenerationTokenizer">BertGenerationTokenizer</a> (Bert Generation model)</li> <li><strong>bert-japanese</strong> — <a href="/docs/transformers/main/ja/model_doc/bert-japanese#transformers.BertJapaneseTokenizer">BertJapaneseTokenizer</a> (BertJapanese model)</li> <li><strong>bertweet</strong> — <a href="/docs/transformers/main/ja/model_doc/bertweet#transformers.BertweetTokenizer">BertweetTokenizer</a> (BERTweet model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdTokenizer">BigBirdTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdTokenizerFast">BigBirdTokenizerFast</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong> — <code>PegasusTokenizer</code> or <code>PegasusTokenizerFast</code> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong> — <a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptTokenizer">BioGptTokenizer</a> (BioGpt model)</li> <li><strong>blenderbot</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotTokenizer">BlenderbotTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotTokenizerFast">BlenderbotTokenizerFast</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallTokenizer">BlenderbotSmallTokenizer</a> (BlenderbotSmall model)</li> <li><strong>blip</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (BLIP model)</li> <li><strong>blip-2</strong> — <code>GPT2Tokenizer</code> or <code>GPT2TokenizerFast</code> (BLIP-2 model)</li> <li><strong>bloom</strong> — <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomTokenizerFast">BloomTokenizerFast</a> (BLOOM model)</li> <li><strong>bridgetower</strong> — <code>RobertaTokenizer</code> or <code>RobertaTokenizerFast</code> (BridgeTower model)</li> <li><strong>bros</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (BROS model)</li> <li><strong>byt5</strong> — <a href="/docs/transformers/main/ja/model_doc/byt5#transformers.ByT5Tokenizer">ByT5Tokenizer</a> (ByT5 model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertTokenizer">CamembertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertTokenizerFast">CamembertTokenizerFast</a> (CamemBERT model)</li> <li><strong>canine</strong> — <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineTokenizer">CanineTokenizer</a> (CANINE model)</li> <li><strong>chinese_clip</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Chinese-CLIP model)</li> <li><strong>clap</strong> — <code>RobertaTokenizer</code> or <code>RobertaTokenizerFast</code> (CLAP model)</li> <li><strong>clip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (CLIP model)</li> <li><strong>clipseg</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (CLIPSeg model)</li> <li><strong>clvp</strong> — <a href="/docs/transformers/main/ja/model_doc/clvp#transformers.ClvpTokenizer">ClvpTokenizer</a> (CLVP model)</li> <li><strong>code_llama</strong> — <a href="/docs/transformers/main/ja/model_doc/code_llama#transformers.CodeLlamaTokenizer">CodeLlamaTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/code_llama#transformers.CodeLlamaTokenizerFast">CodeLlamaTokenizerFast</a> (CodeLlama model)</li> <li><strong>codegen</strong> — <a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenTokenizer">CodeGenTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenTokenizerFast">CodeGenTokenizerFast</a> (CodeGen model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertTokenizer">ConvBertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertTokenizerFast">ConvBertTokenizerFast</a> (ConvBERT model)</li> <li><strong>cpm</strong> — <a href="/docs/transformers/main/ja/model_doc/cpm#transformers.CpmTokenizer">CpmTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/cpm#transformers.CpmTokenizerFast">CpmTokenizerFast</a> (CPM model)</li> <li><strong>cpmant</strong> — <a href="/docs/transformers/main/ja/model_doc/cpmant#transformers.CpmAntTokenizer">CpmAntTokenizer</a> (CPM-Ant model)</li> <li><strong>ctrl</strong> — <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLTokenizer">CTRLTokenizer</a> (CTRL model)</li> <li><strong>data2vec-audio</strong> — <code>Wav2Vec2CTCTokenizer</code> (Data2VecAudio model)</li> <li><strong>data2vec-text</strong> — <code>RobertaTokenizer</code> or <code>RobertaTokenizerFast</code> (Data2VecText model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaTokenizer">DebertaTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaTokenizerFast">DebertaTokenizerFast</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Tokenizer">DebertaV2Tokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2TokenizerFast">DebertaV2TokenizerFast</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>DistilBertTokenizer</code> or <code>DistilBertTokenizerFast</code> (DistilBERT model)</li> <li><strong>dpr</strong> — <code>DPRQuestionEncoderTokenizer</code> or <code>DPRQuestionEncoderTokenizerFast</code> (DPR model)</li> <li><strong>electra</strong> — <code>ElectraTokenizer</code> or <code>ElectraTokenizerFast</code> (ELECTRA model)</li> <li><strong>ernie</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (ERNIE model)</li> <li><strong>ernie_m</strong> — <code>ErnieMTokenizer</code> (ErnieM model)</li> <li><strong>esm</strong> — <code>EsmTokenizer</code> (ESM model)</li> <li><strong>falcon</strong> — <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> (Falcon model)</li> <li><strong>fastspeech2_conformer</strong> —  (FastSpeech2Conformer model)</li> <li><strong>flaubert</strong> — <code>FlaubertTokenizer</code> (FlauBERT model)</li> <li><strong>fnet</strong> — <code>FNetTokenizer</code> or <code>FNetTokenizerFast</code> (FNet model)</li> <li><strong>fsmt</strong> — <code>FSMTTokenizer</code> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong> — <code>FunnelTokenizer</code> or <code>FunnelTokenizerFast</code> (Funnel Transformer model)</li> <li><strong>git</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (GIT model)</li> <li><strong>gpt-sw3</strong> — <code>GPTSw3Tokenizer</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>GPT2Tokenizer</code> or <code>GPT2TokenizerFast</code> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong> — <code>GPT2Tokenizer</code> or <code>GPT2TokenizerFast</code> (GPTBigCode model)</li> <li><strong>gpt_neo</strong> — <code>GPT2Tokenizer</code> or <code>GPT2TokenizerFast</code> (GPT Neo model)</li> <li><strong>gpt_neox</strong> — <code>GPTNeoXTokenizerFast</code> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong> — <code>GPTNeoXJapaneseTokenizer</code> (GPT NeoX Japanese model)</li> <li><strong>gptj</strong> — <code>GPT2Tokenizer</code> or <code>GPT2TokenizerFast</code> (GPT-J model)</li> <li><strong>gptsan-japanese</strong> — <code>GPTSanJapaneseTokenizer</code> (GPTSAN-japanese model)</li> <li><strong>groupvit</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (GroupViT model)</li> <li><strong>herbert</strong> — <code>HerbertTokenizer</code> or <code>HerbertTokenizerFast</code> (HerBERT model)</li> <li><strong>hubert</strong> — <code>Wav2Vec2CTCTokenizer</code> (Hubert model)</li> <li><strong>ibert</strong> — <code>RobertaTokenizer</code> or <code>RobertaTokenizerFast</code> (I-BERT model)</li> <li><strong>idefics</strong> — <code>LlamaTokenizerFast</code> (IDEFICS model)</li> <li><strong>instructblip</strong> — <code>GPT2Tokenizer</code> or <code>GPT2TokenizerFast</code> (InstructBLIP model)</li> <li><strong>jukebox</strong> — <code>JukeboxTokenizer</code> (Jukebox model)</li> <li><strong>kosmos-2</strong> — <code>XLMRobertaTokenizer</code> or <code>XLMRobertaTokenizerFast</code> (KOSMOS-2 model)</li> <li><strong>layoutlm</strong> — <code>LayoutLMTokenizer</code> or <code>LayoutLMTokenizerFast</code> (LayoutLM model)</li> <li><strong>layoutlmv2</strong> — <code>LayoutLMv2Tokenizer</code> or <code>LayoutLMv2TokenizerFast</code> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong> — <code>LayoutLMv3Tokenizer</code> or <code>LayoutLMv3TokenizerFast</code> (LayoutLMv3 model)</li> <li><strong>layoutxlm</strong> — <code>LayoutXLMTokenizer</code> or <code>LayoutXLMTokenizerFast</code> (LayoutXLM model)</li> <li><strong>led</strong> — <code>LEDTokenizer</code> or <code>LEDTokenizerFast</code> (LED model)</li> <li><strong>lilt</strong> — <code>LayoutLMv3Tokenizer</code> or <code>LayoutLMv3TokenizerFast</code> (LiLT model)</li> <li><strong>llama</strong> — <code>LlamaTokenizer</code> or <code>LlamaTokenizerFast</code> (LLaMA model)</li> <li><strong>llava</strong> — <code>LlamaTokenizer</code> or <code>LlamaTokenizerFast</code> (LLaVa model)</li> <li><strong>longformer</strong> — <code>LongformerTokenizer</code> or <code>LongformerTokenizerFast</code> (Longformer model)</li> <li><strong>longt5</strong> — <code>T5Tokenizer</code> or <code>T5TokenizerFast</code> (LongT5 model)</li> <li><strong>luke</strong> — <code>LukeTokenizer</code> (LUKE model)</li> <li><strong>lxmert</strong> — <code>LxmertTokenizer</code> or <code>LxmertTokenizerFast</code> (LXMERT model)</li> <li><strong>m2m_100</strong> — <code>M2M100Tokenizer</code> (M2M100 model)</li> <li><strong>marian</strong> — <code>MarianTokenizer</code> (Marian model)</li> <li><strong>mbart</strong> — <code>MBartTokenizer</code> or <code>MBartTokenizerFast</code> (mBART model)</li> <li><strong>mbart50</strong> — <code>MBart50Tokenizer</code> or <code>MBart50TokenizerFast</code> (mBART-50 model)</li> <li><strong>mega</strong> — <code>RobertaTokenizer</code> or <code>RobertaTokenizerFast</code> (MEGA model)</li> <li><strong>megatron-bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Megatron-BERT model)</li> <li><strong>mgp-str</strong> — <code>MgpstrTokenizer</code> (MGP-STR model)</li> <li><strong>mistral</strong> — <code>LlamaTokenizer</code> or <code>LlamaTokenizerFast</code> (Mistral model)</li> <li><strong>mixtral</strong> — <code>LlamaTokenizer</code> or <code>LlamaTokenizerFast</code> (Mixtral model)</li> <li><strong>mluke</strong> — <code>MLukeTokenizer</code> (mLUKE model)</li> <li><strong>mobilebert</strong> — <code>MobileBertTokenizer</code> or <code>MobileBertTokenizerFast</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>MPNetTokenizer</code> or <code>MPNetTokenizerFast</code> (MPNet model)</li> <li><strong>mpt</strong> — <code>GPTNeoXTokenizerFast</code> (MPT model)</li> <li><strong>mra</strong> — <code>RobertaTokenizer</code> or <code>RobertaTokenizerFast</code> (MRA model)</li> <li><strong>mt5</strong> — <code>MT5Tokenizer</code> or <code>MT5TokenizerFast</code> (MT5 model)</li> <li><strong>musicgen</strong> — <code>T5Tokenizer</code> or <code>T5TokenizerFast</code> (MusicGen model)</li> <li><strong>mvp</strong> — <code>MvpTokenizer</code> or <code>MvpTokenizerFast</code> (MVP model)</li> <li><strong>nezha</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (Nezha model)</li> <li><strong>nllb</strong> — <code>NllbTokenizer</code> or <code>NllbTokenizerFast</code> (NLLB model)</li> <li><strong>nllb-moe</strong> — <code>NllbTokenizer</code> or <code>NllbTokenizerFast</code> (NLLB-MOE model)</li> <li><strong>nystromformer</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertTokenizer">AlbertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertTokenizerFast">AlbertTokenizerFast</a> (Nyströmformer model)</li> <li><strong>oneformer</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (OneFormer model)</li> <li><strong>openai-gpt</strong> — <code>OpenAIGPTTokenizer</code> or <code>OpenAIGPTTokenizerFast</code> (OpenAI GPT model)</li> <li><strong>opt</strong> — <code>GPT2Tokenizer</code> or <code>GPT2TokenizerFast</code> (OPT model)</li> <li><strong>owlv2</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (OWLv2 model)</li> <li><strong>owlvit</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (OWL-ViT model)</li> <li><strong>pegasus</strong> — <code>PegasusTokenizer</code> or <code>PegasusTokenizerFast</code> (Pegasus model)</li> <li><strong>pegasus_x</strong> — <code>PegasusTokenizer</code> or <code>PegasusTokenizerFast</code> (PEGASUS-X model)</li> <li><strong>perceiver</strong> — <code>PerceiverTokenizer</code> (Perceiver model)</li> <li><strong>persimmon</strong> — <code>LlamaTokenizer</code> or <code>LlamaTokenizerFast</code> (Persimmon model)</li> <li><strong>phi</strong> — <a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenTokenizer">CodeGenTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenTokenizerFast">CodeGenTokenizerFast</a> (Phi model)</li> <li><strong>phobert</strong> — <code>PhobertTokenizer</code> (PhoBERT model)</li> <li><strong>pix2struct</strong> — <code>T5Tokenizer</code> or <code>T5TokenizerFast</code> (Pix2Struct model)</li> <li><strong>plbart</strong> — <code>PLBartTokenizer</code> (PLBart model)</li> <li><strong>prophetnet</strong> — <code>ProphetNetTokenizer</code> (ProphetNet model)</li> <li><strong>qdqbert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (QDQBert model)</li> <li><strong>qwen2</strong> — <code>Qwen2Tokenizer</code> or <code>Qwen2TokenizerFast</code> (Qwen2 model)</li> <li><strong>rag</strong> — <code>RagTokenizer</code> (RAG model)</li> <li><strong>realm</strong> — <code>RealmTokenizer</code> or <code>RealmTokenizerFast</code> (REALM model)</li> <li><strong>reformer</strong> — <code>ReformerTokenizer</code> or <code>ReformerTokenizerFast</code> (Reformer model)</li> <li><strong>rembert</strong> — <code>RemBertTokenizer</code> or <code>RemBertTokenizerFast</code> (RemBERT model)</li> <li><strong>retribert</strong> — <code>RetriBertTokenizer</code> or <code>RetriBertTokenizerFast</code> (RetriBERT model)</li> <li><strong>roberta</strong> — <code>RobertaTokenizer</code> or <code>RobertaTokenizerFast</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>RobertaTokenizer</code> or <code>RobertaTokenizerFast</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong> — <code>RoCBertTokenizer</code> (RoCBert model)</li> <li><strong>roformer</strong> — <code>RoFormerTokenizer</code> or <code>RoFormerTokenizerFast</code> (RoFormer model)</li> <li><strong>rwkv</strong> — <code>GPTNeoXTokenizerFast</code> (RWKV model)</li> <li><strong>seamless_m4t</strong> — <code>SeamlessM4TTokenizer</code> or <code>SeamlessM4TTokenizerFast</code> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong> — <code>SeamlessM4TTokenizer</code> or <code>SeamlessM4TTokenizerFast</code> (SeamlessM4Tv2 model)</li> <li><strong>siglip</strong> — <code>SiglipTokenizer</code> (SigLIP model)</li> <li><strong>speech_to_text</strong> — <code>Speech2TextTokenizer</code> (Speech2Text model)</li> <li><strong>speech_to_text_2</strong> — <code>Speech2Text2Tokenizer</code> (Speech2Text2 model)</li> <li><strong>speecht5</strong> — <code>SpeechT5Tokenizer</code> (SpeechT5 model)</li> <li><strong>splinter</strong> — <code>SplinterTokenizer</code> or <code>SplinterTokenizerFast</code> (Splinter model)</li> <li><strong>squeezebert</strong> — <code>SqueezeBertTokenizer</code> or <code>SqueezeBertTokenizerFast</code> (SqueezeBERT model)</li> <li><strong>stablelm</strong> — <code>GPTNeoXTokenizerFast</code> (StableLm model)</li> <li><strong>switch_transformers</strong> — <code>T5Tokenizer</code> or <code>T5TokenizerFast</code> (SwitchTransformers model)</li> <li><strong>t5</strong> — <code>T5Tokenizer</code> or <code>T5TokenizerFast</code> (T5 model)</li> <li><strong>tapas</strong> — <code>TapasTokenizer</code> (TAPAS model)</li> <li><strong>tapex</strong> — <code>TapexTokenizer</code> (TAPEX model)</li> <li><strong>transfo-xl</strong> — <code>TransfoXLTokenizer</code> (Transformer-XL model)</li> <li><strong>tvp</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (TVP model)</li> <li><strong>umt5</strong> — <code>T5Tokenizer</code> or <code>T5TokenizerFast</code> (UMT5 model)</li> <li><strong>vilt</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (ViLT model)</li> <li><strong>vipllava</strong> — <code>LlamaTokenizer</code> or <code>LlamaTokenizerFast</code> (VipLlava model)</li> <li><strong>visual_bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> (VisualBERT model)</li> <li><strong>vits</strong> — <code>VitsTokenizer</code> (VITS model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2CTCTokenizer</code> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong> — <code>Wav2Vec2CTCTokenizer</code> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong> — <code>Wav2Vec2CTCTokenizer</code> (Wav2Vec2-Conformer model)</li> <li><strong>wav2vec2_phoneme</strong> — <code>Wav2Vec2PhonemeCTCTokenizer</code> (Wav2Vec2Phoneme model)</li> <li><strong>whisper</strong> — <code>WhisperTokenizer</code> or <code>WhisperTokenizerFast</code> (Whisper model)</li> <li><strong>xclip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a> (X-CLIP model)</li> <li><strong>xglm</strong> — <code>XGLMTokenizer</code> or <code>XGLMTokenizerFast</code> (XGLM model)</li> <li><strong>xlm</strong> — <code>XLMTokenizer</code> (XLM model)</li> <li><strong>xlm-prophetnet</strong> — <code>XLMProphetNetTokenizer</code> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong> — <code>XLMRobertaTokenizer</code> or <code>XLMRobertaTokenizerFast</code> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong> — <code>XLMRobertaTokenizer</code> or <code>XLMRobertaTokenizerFast</code> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong> — <code>XLNetTokenizer</code> or <code>XLNetTokenizerFast</code> (XLNet model)</li> <li><strong>xmod</strong> — <code>XLMRobertaTokenizer</code> or <code>XLMRobertaTokenizerFast</code> (X-MOD model)</li> <li><strong>yoso</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertTokenizer">AlbertTokenizer</a> or <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertTokenizerFast">AlbertTokenizerFast</a> (YOSO model)</li>',Oj,js,Kj,xs,Gd,ex,wu,rE="Register a new tokenizer in this mapping.",PC,Ed,SC,fe,Pd,ox,ju,aE=`This is a generic feature extractor class that will be instantiated as one of the feature extractor classes of the
library when created with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoFeatureExtractor.from_pretrained">AutoFeatureExtractor.from_pretrained()</a> class method.`,tx,xu,sE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",nx,L,Sd,rx,$u,iE="Instantiate one of the feature extractor classes of the library from a pretrained model vocabulary.",ax,ku,lE=`The feature extractor class to instantiate is selected based on the <code>model_type</code> property of the config object
(either passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s
missing, by falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,sx,Zu,dE='<li><strong>audio-spectrogram-transformer</strong> — <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor">ASTFeatureExtractor</a> (Audio Spectrogram Transformer model)</li> <li><strong>beit</strong> — <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitFeatureExtractor">BeitFeatureExtractor</a> (BEiT model)</li> <li><strong>chinese_clip</strong> — <a href="/docs/transformers/main/ja/model_doc/chinese_clip#transformers.ChineseCLIPFeatureExtractor">ChineseCLIPFeatureExtractor</a> (Chinese-CLIP model)</li> <li><strong>clap</strong> — <a href="/docs/transformers/main/ja/model_doc/clap#transformers.ClapFeatureExtractor">ClapFeatureExtractor</a> (CLAP model)</li> <li><strong>clip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPFeatureExtractor">CLIPFeatureExtractor</a> (CLIP model)</li> <li><strong>clipseg</strong> — <code>ViTFeatureExtractor</code> (CLIPSeg model)</li> <li><strong>clvp</strong> — <a href="/docs/transformers/main/ja/model_doc/clvp#transformers.ClvpFeatureExtractor">ClvpFeatureExtractor</a> (CLVP model)</li> <li><strong>conditional_detr</strong> — <a href="/docs/transformers/main/ja/model_doc/conditional_detr#transformers.ConditionalDetrFeatureExtractor">ConditionalDetrFeatureExtractor</a> (Conditional DETR model)</li> <li><strong>convnext</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (ConvNeXT model)</li> <li><strong>cvt</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (CvT model)</li> <li><strong>data2vec-audio</strong> — <code>Wav2Vec2FeatureExtractor</code> (Data2VecAudio model)</li> <li><strong>data2vec-vision</strong> — <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitFeatureExtractor">BeitFeatureExtractor</a> (Data2VecVision model)</li> <li><strong>deformable_detr</strong> — <a href="/docs/transformers/main/ja/model_doc/deformable_detr#transformers.DeformableDetrFeatureExtractor">DeformableDetrFeatureExtractor</a> (Deformable DETR model)</li> <li><strong>deit</strong> — <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor">DeiTFeatureExtractor</a> (DeiT model)</li> <li><strong>detr</strong> — <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrFeatureExtractor">DetrFeatureExtractor</a> (DETR model)</li> <li><strong>dinat</strong> — <code>ViTFeatureExtractor</code> (DiNAT model)</li> <li><strong>donut-swin</strong> — <code>DonutFeatureExtractor</code> (DonutSwin model)</li> <li><strong>dpt</strong> — <code>DPTFeatureExtractor</code> (DPT model)</li> <li><strong>encodec</strong> — <code>EncodecFeatureExtractor</code> (EnCodec model)</li> <li><strong>flava</strong> — <code>FlavaFeatureExtractor</code> (FLAVA model)</li> <li><strong>glpn</strong> — <code>GLPNFeatureExtractor</code> (GLPN model)</li> <li><strong>groupvit</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPFeatureExtractor">CLIPFeatureExtractor</a> (GroupViT model)</li> <li><strong>hubert</strong> — <code>Wav2Vec2FeatureExtractor</code> (Hubert model)</li> <li><strong>imagegpt</strong> — <code>ImageGPTFeatureExtractor</code> (ImageGPT model)</li> <li><strong>layoutlmv2</strong> — <code>LayoutLMv2FeatureExtractor</code> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong> — <code>LayoutLMv3FeatureExtractor</code> (LayoutLMv3 model)</li> <li><strong>levit</strong> — <code>LevitFeatureExtractor</code> (LeViT model)</li> <li><strong>maskformer</strong> — <code>MaskFormerFeatureExtractor</code> (MaskFormer model)</li> <li><strong>mctct</strong> — <code>MCTCTFeatureExtractor</code> (M-CTC-T model)</li> <li><strong>mobilenet_v1</strong> — <code>MobileNetV1FeatureExtractor</code> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong> — <code>MobileNetV2FeatureExtractor</code> (MobileNetV2 model)</li> <li><strong>mobilevit</strong> — <code>MobileViTFeatureExtractor</code> (MobileViT model)</li> <li><strong>nat</strong> — <code>ViTFeatureExtractor</code> (NAT model)</li> <li><strong>owlvit</strong> — <code>OwlViTFeatureExtractor</code> (OWL-ViT model)</li> <li><strong>perceiver</strong> — <code>PerceiverFeatureExtractor</code> (Perceiver model)</li> <li><strong>poolformer</strong> — <code>PoolFormerFeatureExtractor</code> (PoolFormer model)</li> <li><strong>pop2piano</strong> — <code>Pop2PianoFeatureExtractor</code> (Pop2Piano model)</li> <li><strong>regnet</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (RegNet model)</li> <li><strong>resnet</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (ResNet model)</li> <li><strong>seamless_m4t</strong> — <code>SeamlessM4TFeatureExtractor</code> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong> — <code>SeamlessM4TFeatureExtractor</code> (SeamlessM4Tv2 model)</li> <li><strong>segformer</strong> — <code>SegformerFeatureExtractor</code> (SegFormer model)</li> <li><strong>sew</strong> — <code>Wav2Vec2FeatureExtractor</code> (SEW model)</li> <li><strong>sew-d</strong> — <code>Wav2Vec2FeatureExtractor</code> (SEW-D model)</li> <li><strong>speech_to_text</strong> — <code>Speech2TextFeatureExtractor</code> (Speech2Text model)</li> <li><strong>speecht5</strong> — <code>SpeechT5FeatureExtractor</code> (SpeechT5 model)</li> <li><strong>swiftformer</strong> — <code>ViTFeatureExtractor</code> (SwiftFormer model)</li> <li><strong>swin</strong> — <code>ViTFeatureExtractor</code> (Swin Transformer model)</li> <li><strong>swinv2</strong> — <code>ViTFeatureExtractor</code> (Swin Transformer V2 model)</li> <li><strong>table-transformer</strong> — <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrFeatureExtractor">DetrFeatureExtractor</a> (Table Transformer model)</li> <li><strong>timesformer</strong> — <code>VideoMAEFeatureExtractor</code> (TimeSformer model)</li> <li><strong>tvlt</strong> — <code>TvltFeatureExtractor</code> (TVLT model)</li> <li><strong>unispeech</strong> — <code>Wav2Vec2FeatureExtractor</code> (UniSpeech model)</li> <li><strong>unispeech-sat</strong> — <code>Wav2Vec2FeatureExtractor</code> (UniSpeechSat model)</li> <li><strong>univnet</strong> — <code>UnivNetFeatureExtractor</code> (UnivNet model)</li> <li><strong>van</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextFeatureExtractor">ConvNextFeatureExtractor</a> (VAN model)</li> <li><strong>videomae</strong> — <code>VideoMAEFeatureExtractor</code> (VideoMAE model)</li> <li><strong>vilt</strong> — <code>ViltFeatureExtractor</code> (ViLT model)</li> <li><strong>vit</strong> — <code>ViTFeatureExtractor</code> (ViT model)</li> <li><strong>vit_mae</strong> — <code>ViTFeatureExtractor</code> (ViTMAE model)</li> <li><strong>vit_msn</strong> — <code>ViTFeatureExtractor</code> (ViTMSN model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2FeatureExtractor</code> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong> — <code>Wav2Vec2FeatureExtractor</code> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong> — <code>Wav2Vec2FeatureExtractor</code> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong> — <code>Wav2Vec2FeatureExtractor</code> (WavLM model)</li> <li><strong>whisper</strong> — <code>WhisperFeatureExtractor</code> (Whisper model)</li> <li><strong>xclip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPFeatureExtractor">CLIPFeatureExtractor</a> (X-CLIP model)</li> <li><strong>yolos</strong> — <code>YolosFeatureExtractor</code> (YOLOS model)</li>',ix,$s,lx,ks,dx,Zs,Ud,cx,Lu,cE="Register a new feature extractor for this class.",UC,Id,IC,ge,Nd,mx,Bu,mE=`This is a generic image processor class that will be instantiated as one of the image processor classes of the
library when created with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor.from_pretrained">AutoImageProcessor.from_pretrained()</a> class method.`,fx,Au,fE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",gx,B,Xd,px,Ru,gE="Instantiate one of the image processor classes of the library from a pretrained model vocabulary.",ux,Wu,pE=`The image processor class to instantiate is selected based on the <code>model_type</code> property of the config object
(either passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s
missing, by falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,hx,Ju,uE='<li><strong>align</strong> — <code>EfficientNetImageProcessor</code> (ALIGN model)</li> <li><strong>beit</strong> — <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitImageProcessor">BeitImageProcessor</a> (BEiT model)</li> <li><strong>bit</strong> — <a href="/docs/transformers/main/ja/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> (BiT model)</li> <li><strong>blip</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> (BLIP model)</li> <li><strong>blip-2</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> (BLIP-2 model)</li> <li><strong>bridgetower</strong> — <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerImageProcessor">BridgeTowerImageProcessor</a> (BridgeTower model)</li> <li><strong>chinese_clip</strong> — <a href="/docs/transformers/main/ja/model_doc/chinese_clip#transformers.ChineseCLIPImageProcessor">ChineseCLIPImageProcessor</a> (Chinese-CLIP model)</li> <li><strong>clip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (CLIP model)</li> <li><strong>clipseg</strong> — <code>ViTImageProcessor</code> (CLIPSeg model)</li> <li><strong>conditional_detr</strong> — <a href="/docs/transformers/main/ja/model_doc/conditional_detr#transformers.ConditionalDetrImageProcessor">ConditionalDetrImageProcessor</a> (Conditional DETR model)</li> <li><strong>convnext</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (ConvNeXTV2 model)</li> <li><strong>cvt</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (CvT model)</li> <li><strong>data2vec-vision</strong> — <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitImageProcessor">BeitImageProcessor</a> (Data2VecVision model)</li> <li><strong>deformable_detr</strong> — <a href="/docs/transformers/main/ja/model_doc/deformable_detr#transformers.DeformableDetrImageProcessor">DeformableDetrImageProcessor</a> (Deformable DETR model)</li> <li><strong>deit</strong> — <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTImageProcessor">DeiTImageProcessor</a> (DeiT model)</li> <li><strong>depth_anything</strong> — <code>DPTImageProcessor</code> (Depth Anything model)</li> <li><strong>deta</strong> — <a href="/docs/transformers/main/ja/model_doc/deta#transformers.DetaImageProcessor">DetaImageProcessor</a> (DETA model)</li> <li><strong>detr</strong> — <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrImageProcessor">DetrImageProcessor</a> (DETR model)</li> <li><strong>dinat</strong> — <code>ViTImageProcessor</code> (DiNAT model)</li> <li><strong>dinov2</strong> — <a href="/docs/transformers/main/ja/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> (DINOv2 model)</li> <li><strong>donut-swin</strong> — <code>DonutImageProcessor</code> (DonutSwin model)</li> <li><strong>dpt</strong> — <code>DPTImageProcessor</code> (DPT model)</li> <li><strong>efficientformer</strong> — <code>EfficientFormerImageProcessor</code> (EfficientFormer model)</li> <li><strong>efficientnet</strong> — <code>EfficientNetImageProcessor</code> (EfficientNet model)</li> <li><strong>flava</strong> — <code>FlavaImageProcessor</code> (FLAVA model)</li> <li><strong>focalnet</strong> — <a href="/docs/transformers/main/ja/model_doc/bit#transformers.BitImageProcessor">BitImageProcessor</a> (FocalNet model)</li> <li><strong>fuyu</strong> — <code>FuyuImageProcessor</code> (Fuyu model)</li> <li><strong>git</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (GIT model)</li> <li><strong>glpn</strong> — <code>GLPNImageProcessor</code> (GLPN model)</li> <li><strong>groupvit</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (GroupViT model)</li> <li><strong>idefics</strong> — <code>IdeficsImageProcessor</code> (IDEFICS model)</li> <li><strong>imagegpt</strong> — <code>ImageGPTImageProcessor</code> (ImageGPT model)</li> <li><strong>instructblip</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipImageProcessor">BlipImageProcessor</a> (InstructBLIP model)</li> <li><strong>kosmos-2</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (KOSMOS-2 model)</li> <li><strong>layoutlmv2</strong> — <code>LayoutLMv2ImageProcessor</code> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong> — <code>LayoutLMv3ImageProcessor</code> (LayoutLMv3 model)</li> <li><strong>levit</strong> — <code>LevitImageProcessor</code> (LeViT model)</li> <li><strong>llava</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (LLaVa model)</li> <li><strong>mask2former</strong> — <code>Mask2FormerImageProcessor</code> (Mask2Former model)</li> <li><strong>maskformer</strong> — <code>MaskFormerImageProcessor</code> (MaskFormer model)</li> <li><strong>mgp-str</strong> — <code>ViTImageProcessor</code> (MGP-STR model)</li> <li><strong>mobilenet_v1</strong> — <code>MobileNetV1ImageProcessor</code> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong> — <code>MobileNetV2ImageProcessor</code> (MobileNetV2 model)</li> <li><strong>mobilevit</strong> — <code>MobileViTImageProcessor</code> (MobileViT model)</li> <li><strong>mobilevitv2</strong> — <code>MobileViTImageProcessor</code> (MobileViTV2 model)</li> <li><strong>nat</strong> — <code>ViTImageProcessor</code> (NAT model)</li> <li><strong>nougat</strong> — <code>NougatImageProcessor</code> (Nougat model)</li> <li><strong>oneformer</strong> — <code>OneFormerImageProcessor</code> (OneFormer model)</li> <li><strong>owlv2</strong> — <code>Owlv2ImageProcessor</code> (OWLv2 model)</li> <li><strong>owlvit</strong> — <code>OwlViTImageProcessor</code> (OWL-ViT model)</li> <li><strong>perceiver</strong> — <code>PerceiverImageProcessor</code> (Perceiver model)</li> <li><strong>pix2struct</strong> — <code>Pix2StructImageProcessor</code> (Pix2Struct model)</li> <li><strong>poolformer</strong> — <code>PoolFormerImageProcessor</code> (PoolFormer model)</li> <li><strong>pvt</strong> — <code>PvtImageProcessor</code> (PVT model)</li> <li><strong>regnet</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (RegNet model)</li> <li><strong>resnet</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (ResNet model)</li> <li><strong>sam</strong> — <code>SamImageProcessor</code> (SAM model)</li> <li><strong>segformer</strong> — <code>SegformerImageProcessor</code> (SegFormer model)</li> <li><strong>siglip</strong> — <code>SiglipImageProcessor</code> (SigLIP model)</li> <li><strong>swiftformer</strong> — <code>ViTImageProcessor</code> (SwiftFormer model)</li> <li><strong>swin</strong> — <code>ViTImageProcessor</code> (Swin Transformer model)</li> <li><strong>swin2sr</strong> — <code>Swin2SRImageProcessor</code> (Swin2SR model)</li> <li><strong>swinv2</strong> — <code>ViTImageProcessor</code> (Swin Transformer V2 model)</li> <li><strong>table-transformer</strong> — <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrImageProcessor">DetrImageProcessor</a> (Table Transformer model)</li> <li><strong>timesformer</strong> — <code>VideoMAEImageProcessor</code> (TimeSformer model)</li> <li><strong>tvlt</strong> — <code>TvltImageProcessor</code> (TVLT model)</li> <li><strong>tvp</strong> — <code>TvpImageProcessor</code> (TVP model)</li> <li><strong>upernet</strong> — <code>SegformerImageProcessor</code> (UPerNet model)</li> <li><strong>van</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a> (VAN model)</li> <li><strong>videomae</strong> — <code>VideoMAEImageProcessor</code> (VideoMAE model)</li> <li><strong>vilt</strong> — <code>ViltImageProcessor</code> (ViLT model)</li> <li><strong>vipllava</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (VipLlava model)</li> <li><strong>vit</strong> — <code>ViTImageProcessor</code> (ViT model)</li> <li><strong>vit_hybrid</strong> — <code>ViTHybridImageProcessor</code> (ViT Hybrid model)</li> <li><strong>vit_mae</strong> — <code>ViTImageProcessor</code> (ViTMAE model)</li> <li><strong>vit_msn</strong> — <code>ViTImageProcessor</code> (ViTMSN model)</li> <li><strong>vitmatte</strong> — <code>VitMatteImageProcessor</code> (ViTMatte model)</li> <li><strong>xclip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> (X-CLIP model)</li> <li><strong>yolos</strong> — <code>YolosImageProcessor</code> (YOLOS model)</li>',bx,Ls,_x,Bs,Mx,As,qd,Tx,Vu,hE="Register a new image processor for this class.",NC,Qd,XC,pe,Hd,yx,Gu,bE=`This is a generic processor class that will be instantiated as one of the processor classes of the library when
created with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoProcessor.from_pretrained">AutoProcessor.from_pretrained()</a> class method.`,Fx,Eu,_E="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",vx,A,Dd,Cx,Pu,ME="Instantiate one of the processor classes of the library from a pretrained model vocabulary.",wx,Su,TE=`The processor class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible):`,jx,Uu,yE='<li><strong>align</strong> — <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignProcessor">AlignProcessor</a> (ALIGN model)</li> <li><strong>altclip</strong> — <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a> (AltCLIP model)</li> <li><strong>bark</strong> — <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkProcessor">BarkProcessor</a> (Bark model)</li> <li><strong>blip</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipProcessor">BlipProcessor</a> (BLIP model)</li> <li><strong>blip-2</strong> — <a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2Processor">Blip2Processor</a> (BLIP-2 model)</li> <li><strong>bridgetower</strong> — <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerProcessor">BridgeTowerProcessor</a> (BridgeTower model)</li> <li><strong>chinese_clip</strong> — <a href="/docs/transformers/main/ja/model_doc/chinese_clip#transformers.ChineseCLIPProcessor">ChineseCLIPProcessor</a> (Chinese-CLIP model)</li> <li><strong>clap</strong> — <a href="/docs/transformers/main/ja/model_doc/clap#transformers.ClapProcessor">ClapProcessor</a> (CLAP model)</li> <li><strong>clip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> (CLIP model)</li> <li><strong>clipseg</strong> — <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> (CLIPSeg model)</li> <li><strong>clvp</strong> — <a href="/docs/transformers/main/ja/model_doc/clvp#transformers.ClvpProcessor">ClvpProcessor</a> (CLVP model)</li> <li><strong>flava</strong> — <code>FlavaProcessor</code> (FLAVA model)</li> <li><strong>fuyu</strong> — <code>FuyuProcessor</code> (Fuyu model)</li> <li><strong>git</strong> — <code>GitProcessor</code> (GIT model)</li> <li><strong>groupvit</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> (GroupViT model)</li> <li><strong>hubert</strong> — <code>Wav2Vec2Processor</code> (Hubert model)</li> <li><strong>idefics</strong> — <code>IdeficsProcessor</code> (IDEFICS model)</li> <li><strong>instructblip</strong> — <code>InstructBlipProcessor</code> (InstructBLIP model)</li> <li><strong>kosmos-2</strong> — <code>Kosmos2Processor</code> (KOSMOS-2 model)</li> <li><strong>layoutlmv2</strong> — <code>LayoutLMv2Processor</code> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong> — <code>LayoutLMv3Processor</code> (LayoutLMv3 model)</li> <li><strong>llava</strong> — <code>LlavaProcessor</code> (LLaVa model)</li> <li><strong>markuplm</strong> — <code>MarkupLMProcessor</code> (MarkupLM model)</li> <li><strong>mctct</strong> — <code>MCTCTProcessor</code> (M-CTC-T model)</li> <li><strong>mgp-str</strong> — <code>MgpstrProcessor</code> (MGP-STR model)</li> <li><strong>oneformer</strong> — <code>OneFormerProcessor</code> (OneFormer model)</li> <li><strong>owlv2</strong> — <code>Owlv2Processor</code> (OWLv2 model)</li> <li><strong>owlvit</strong> — <code>OwlViTProcessor</code> (OWL-ViT model)</li> <li><strong>pix2struct</strong> — <code>Pix2StructProcessor</code> (Pix2Struct model)</li> <li><strong>pop2piano</strong> — <code>Pop2PianoProcessor</code> (Pop2Piano model)</li> <li><strong>sam</strong> — <code>SamProcessor</code> (SAM model)</li> <li><strong>seamless_m4t</strong> — <code>SeamlessM4TProcessor</code> (SeamlessM4T model)</li> <li><strong>sew</strong> — <code>Wav2Vec2Processor</code> (SEW model)</li> <li><strong>sew-d</strong> — <code>Wav2Vec2Processor</code> (SEW-D model)</li> <li><strong>siglip</strong> — <code>SiglipProcessor</code> (SigLIP model)</li> <li><strong>speech_to_text</strong> — <code>Speech2TextProcessor</code> (Speech2Text model)</li> <li><strong>speech_to_text_2</strong> — <code>Speech2Text2Processor</code> (Speech2Text2 model)</li> <li><strong>speecht5</strong> — <code>SpeechT5Processor</code> (SpeechT5 model)</li> <li><strong>trocr</strong> — <code>TrOCRProcessor</code> (TrOCR model)</li> <li><strong>tvlt</strong> — <code>TvltProcessor</code> (TVLT model)</li> <li><strong>tvp</strong> — <code>TvpProcessor</code> (TVP model)</li> <li><strong>unispeech</strong> — <code>Wav2Vec2Processor</code> (UniSpeech model)</li> <li><strong>unispeech-sat</strong> — <code>Wav2Vec2Processor</code> (UniSpeechSat model)</li> <li><strong>vilt</strong> — <code>ViltProcessor</code> (ViLT model)</li> <li><strong>vipllava</strong> — <code>LlavaProcessor</code> (VipLlava model)</li> <li><strong>vision-text-dual-encoder</strong> — <code>VisionTextDualEncoderProcessor</code> (VisionTextDualEncoder model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2Processor</code> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong> — <code>Wav2Vec2Processor</code> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong> — <code>Wav2Vec2Processor</code> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong> — <code>Wav2Vec2Processor</code> (WavLM model)</li> <li><strong>whisper</strong> — <code>WhisperProcessor</code> (Whisper model)</li> <li><strong>xclip</strong> — <code>XCLIPProcessor</code> (X-CLIP model)</li>',xx,Rs,$x,Ws,kx,Js,Yd,Zx,Iu,FE="Register a new processor for this class.",qC,zd,QC,Od,vE="以下の自動クラスは、特定のヘッドを持たないベースモデルクラスをインスタンス化するために利用可能です。",HC,Kd,DC,ue,ec,Lx,Nu,CE=`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,Bx,Xu,wE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Ax,Vn,oc,Rx,qu,jE="Instantiates one of the base model classes of the library from a configuration.",Wx,Qu,xE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Jx,Vs,Vx,R,tc,Gx,Hu,$E="Instantiate one of the base model classes of the library from a pretrained model.",Ex,Du,kE=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Px,Yu,ZE='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li> <li><strong>align</strong> — <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li> <li><strong>altclip</strong> — <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li> <li><strong>audio-spectrogram-transformer</strong> — <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTModel">ASTModel</a> (Audio Spectrogram Transformer model)</li> <li><strong>autoformer</strong> — <a href="/docs/transformers/main/ja/model_doc/autoformer#transformers.AutoformerModel">AutoformerModel</a> (Autoformer model)</li> <li><strong>bark</strong> — <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkModel">BarkModel</a> (Bark model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li> <li><strong>beit</strong> — <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li> <li><strong>bert-generation</strong> — <a href="/docs/transformers/main/ja/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong> — <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong> — <a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptModel">BioGptModel</a> (BioGpt model)</li> <li><strong>bit</strong> — <a href="/docs/transformers/main/ja/model_doc/bit#transformers.BitModel">BitModel</a> (BiT model)</li> <li><strong>blenderbot</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li> <li><strong>blip</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li> <li><strong>blip-2</strong> — <a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> (BLIP-2 model)</li> <li><strong>bloom</strong> — <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomModel">BloomModel</a> (BLOOM model)</li> <li><strong>bridgetower</strong> — <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a> (BridgeTower model)</li> <li><strong>bros</strong> — <a href="/docs/transformers/main/ja/model_doc/bros#transformers.BrosModel">BrosModel</a> (BROS model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li> <li><strong>canine</strong> — <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineModel">CanineModel</a> (CANINE model)</li> <li><strong>chinese_clip</strong> — <a href="/docs/transformers/main/ja/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li> <li><strong>clap</strong> — <a href="/docs/transformers/main/ja/model_doc/clap#transformers.ClapModel">ClapModel</a> (CLAP model)</li> <li><strong>clip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li> <li><strong>clip_vision_model</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionModel">CLIPVisionModel</a> (CLIPVisionModel model)</li> <li><strong>clipseg</strong> — <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li> <li><strong>clvp</strong> — <a href="/docs/transformers/main/ja/model_doc/clvp#transformers.ClvpModelForConditionalGeneration">ClvpModelForConditionalGeneration</a> (CLVP model)</li> <li><strong>code_llama</strong> — <code>LlamaModel</code> (CodeLlama model)</li> <li><strong>codegen</strong> — <a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenModel">CodeGenModel</a> (CodeGen model)</li> <li><strong>conditional_detr</strong> — <a href="/docs/transformers/main/ja/model_doc/conditional_detr#transformers.ConditionalDetrModel">ConditionalDetrModel</a> (Conditional DETR model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li> <li><strong>convnext</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong> — <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Model">ConvNextV2Model</a> (ConvNeXTV2 model)</li> <li><strong>cpmant</strong> — <a href="/docs/transformers/main/ja/model_doc/cpmant#transformers.CpmAntModel">CpmAntModel</a> (CPM-Ant model)</li> <li><strong>ctrl</strong> — <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li> <li><strong>cvt</strong> — <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtModel">CvtModel</a> (CvT model)</li> <li><strong>data2vec-audio</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li> <li><strong>data2vec-text</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li> <li><strong>data2vec-vision</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionModel">Data2VecVisionModel</a> (Data2VecVision model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li> <li><strong>decision_transformer</strong> — <a href="/docs/transformers/main/ja/model_doc/decision_transformer#transformers.DecisionTransformerModel">DecisionTransformerModel</a> (Decision Transformer model)</li> <li><strong>deformable_detr</strong> — <a href="/docs/transformers/main/ja/model_doc/deformable_detr#transformers.DeformableDetrModel">DeformableDetrModel</a> (Deformable DETR model)</li> <li><strong>deit</strong> — <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li> <li><strong>deta</strong> — <a href="/docs/transformers/main/ja/model_doc/deta#transformers.DetaModel">DetaModel</a> (DETA model)</li> <li><strong>detr</strong> — <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li> <li><strong>dinat</strong> — <a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatModel">DinatModel</a> (DiNAT model)</li> <li><strong>dinov2</strong> — <code>Dinov2Model</code> (DINOv2 model)</li> <li><strong>distilbert</strong> — <code>DistilBertModel</code> (DistilBERT model)</li> <li><strong>donut-swin</strong> — <code>DonutSwinModel</code> (DonutSwin model)</li> <li><strong>dpr</strong> — <code>DPRQuestionEncoder</code> (DPR model)</li> <li><strong>dpt</strong> — <code>DPTModel</code> (DPT model)</li> <li><strong>efficientformer</strong> — <code>EfficientFormerModel</code> (EfficientFormer model)</li> <li><strong>efficientnet</strong> — <code>EfficientNetModel</code> (EfficientNet model)</li> <li><strong>electra</strong> — <code>ElectraModel</code> (ELECTRA model)</li> <li><strong>encodec</strong> — <code>EncodecModel</code> (EnCodec model)</li> <li><strong>ernie</strong> — <code>ErnieModel</code> (ERNIE model)</li> <li><strong>ernie_m</strong> — <code>ErnieMModel</code> (ErnieM model)</li> <li><strong>esm</strong> — <code>EsmModel</code> (ESM model)</li> <li><strong>falcon</strong> — <code>FalconModel</code> (Falcon model)</li> <li><strong>fastspeech2_conformer</strong> — <code>FastSpeech2ConformerModel</code> (FastSpeech2Conformer model)</li> <li><strong>flaubert</strong> — <code>FlaubertModel</code> (FlauBERT model)</li> <li><strong>flava</strong> — <code>FlavaModel</code> (FLAVA model)</li> <li><strong>fnet</strong> — <code>FNetModel</code> (FNet model)</li> <li><strong>focalnet</strong> — <code>FocalNetModel</code> (FocalNet model)</li> <li><strong>fsmt</strong> — <code>FSMTModel</code> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong> — <code>FunnelModel</code> or <code>FunnelBaseModel</code> (Funnel Transformer model)</li> <li><strong>git</strong> — <code>GitModel</code> (GIT model)</li> <li><strong>glpn</strong> — <code>GLPNModel</code> (GLPN model)</li> <li><strong>gpt-sw3</strong> — <code>GPT2Model</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>GPT2Model</code> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong> — <code>GPTBigCodeModel</code> (GPTBigCode model)</li> <li><strong>gpt_neo</strong> — <code>GPTNeoModel</code> (GPT Neo model)</li> <li><strong>gpt_neox</strong> — <code>GPTNeoXModel</code> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong> — <code>GPTNeoXJapaneseModel</code> (GPT NeoX Japanese model)</li> <li><strong>gptj</strong> — <code>GPTJModel</code> (GPT-J model)</li> <li><strong>gptsan-japanese</strong> — <code>GPTSanJapaneseForConditionalGeneration</code> (GPTSAN-japanese model)</li> <li><strong>graphormer</strong> — <code>GraphormerModel</code> (Graphormer model)</li> <li><strong>groupvit</strong> — <code>GroupViTModel</code> (GroupViT model)</li> <li><strong>hubert</strong> — <code>HubertModel</code> (Hubert model)</li> <li><strong>ibert</strong> — <code>IBertModel</code> (I-BERT model)</li> <li><strong>idefics</strong> — <code>IdeficsModel</code> (IDEFICS model)</li> <li><strong>imagegpt</strong> — <code>ImageGPTModel</code> (ImageGPT model)</li> <li><strong>informer</strong> — <code>InformerModel</code> (Informer model)</li> <li><strong>jukebox</strong> — <code>JukeboxModel</code> (Jukebox model)</li> <li><strong>kosmos-2</strong> — <code>Kosmos2Model</code> (KOSMOS-2 model)</li> <li><strong>layoutlm</strong> — <code>LayoutLMModel</code> (LayoutLM model)</li> <li><strong>layoutlmv2</strong> — <code>LayoutLMv2Model</code> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong> — <code>LayoutLMv3Model</code> (LayoutLMv3 model)</li> <li><strong>led</strong> — <code>LEDModel</code> (LED model)</li> <li><strong>levit</strong> — <code>LevitModel</code> (LeViT model)</li> <li><strong>lilt</strong> — <code>LiltModel</code> (LiLT model)</li> <li><strong>llama</strong> — <code>LlamaModel</code> (LLaMA model)</li> <li><strong>longformer</strong> — <code>LongformerModel</code> (Longformer model)</li> <li><strong>longt5</strong> — <code>LongT5Model</code> (LongT5 model)</li> <li><strong>luke</strong> — <code>LukeModel</code> (LUKE model)</li> <li><strong>lxmert</strong> — <code>LxmertModel</code> (LXMERT model)</li> <li><strong>m2m_100</strong> — <code>M2M100Model</code> (M2M100 model)</li> <li><strong>marian</strong> — <code>MarianModel</code> (Marian model)</li> <li><strong>markuplm</strong> — <code>MarkupLMModel</code> (MarkupLM model)</li> <li><strong>mask2former</strong> — <code>Mask2FormerModel</code> (Mask2Former model)</li> <li><strong>maskformer</strong> — <code>MaskFormerModel</code> (MaskFormer model)</li> <li><strong>maskformer-swin</strong> — <code>MaskFormerSwinModel</code> (MaskFormerSwin model)</li> <li><strong>mbart</strong> — <code>MBartModel</code> (mBART model)</li> <li><strong>mctct</strong> — <code>MCTCTModel</code> (M-CTC-T model)</li> <li><strong>mega</strong> — <code>MegaModel</code> (MEGA model)</li> <li><strong>megatron-bert</strong> — <code>MegatronBertModel</code> (Megatron-BERT model)</li> <li><strong>mgp-str</strong> — <code>MgpstrForSceneTextRecognition</code> (MGP-STR model)</li> <li><strong>mistral</strong> — <code>MistralModel</code> (Mistral model)</li> <li><strong>mixtral</strong> — <code>MixtralModel</code> (Mixtral model)</li> <li><strong>mobilebert</strong> — <code>MobileBertModel</code> (MobileBERT model)</li> <li><strong>mobilenet_v1</strong> — <code>MobileNetV1Model</code> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong> — <code>MobileNetV2Model</code> (MobileNetV2 model)</li> <li><strong>mobilevit</strong> — <code>MobileViTModel</code> (MobileViT model)</li> <li><strong>mobilevitv2</strong> — <code>MobileViTV2Model</code> (MobileViTV2 model)</li> <li><strong>mpnet</strong> — <code>MPNetModel</code> (MPNet model)</li> <li><strong>mpt</strong> — <code>MptModel</code> (MPT model)</li> <li><strong>mra</strong> — <code>MraModel</code> (MRA model)</li> <li><strong>mt5</strong> — <code>MT5Model</code> (MT5 model)</li> <li><strong>mvp</strong> — <code>MvpModel</code> (MVP model)</li> <li><strong>nat</strong> — <code>NatModel</code> (NAT model)</li> <li><strong>nezha</strong> — <code>NezhaModel</code> (Nezha model)</li> <li><strong>nllb-moe</strong> — <code>NllbMoeModel</code> (NLLB-MOE model)</li> <li><strong>nystromformer</strong> — <code>NystromformerModel</code> (Nyströmformer model)</li> <li><strong>oneformer</strong> — <code>OneFormerModel</code> (OneFormer model)</li> <li><strong>open-llama</strong> — <code>OpenLlamaModel</code> (OpenLlama model)</li> <li><strong>openai-gpt</strong> — <code>OpenAIGPTModel</code> (OpenAI GPT model)</li> <li><strong>opt</strong> — <code>OPTModel</code> (OPT model)</li> <li><strong>owlv2</strong> — <code>Owlv2Model</code> (OWLv2 model)</li> <li><strong>owlvit</strong> — <code>OwlViTModel</code> (OWL-ViT model)</li> <li><strong>patchtsmixer</strong> — <code>PatchTSMixerModel</code> (PatchTSMixer model)</li> <li><strong>patchtst</strong> — <code>PatchTSTModel</code> (PatchTST model)</li> <li><strong>pegasus</strong> — <code>PegasusModel</code> (Pegasus model)</li> <li><strong>pegasus_x</strong> — <code>PegasusXModel</code> (PEGASUS-X model)</li> <li><strong>perceiver</strong> — <code>PerceiverModel</code> (Perceiver model)</li> <li><strong>persimmon</strong> — <code>PersimmonModel</code> (Persimmon model)</li> <li><strong>phi</strong> — <code>PhiModel</code> (Phi model)</li> <li><strong>plbart</strong> — <code>PLBartModel</code> (PLBart model)</li> <li><strong>poolformer</strong> — <code>PoolFormerModel</code> (PoolFormer model)</li> <li><strong>prophetnet</strong> — <code>ProphetNetModel</code> (ProphetNet model)</li> <li><strong>pvt</strong> — <code>PvtModel</code> (PVT model)</li> <li><strong>qdqbert</strong> — <code>QDQBertModel</code> (QDQBert model)</li> <li><strong>qwen2</strong> — <code>Qwen2Model</code> (Qwen2 model)</li> <li><strong>reformer</strong> — <code>ReformerModel</code> (Reformer model)</li> <li><strong>regnet</strong> — <code>RegNetModel</code> (RegNet model)</li> <li><strong>rembert</strong> — <code>RemBertModel</code> (RemBERT model)</li> <li><strong>resnet</strong> — <code>ResNetModel</code> (ResNet model)</li> <li><strong>retribert</strong> — <code>RetriBertModel</code> (RetriBERT model)</li> <li><strong>roberta</strong> — <code>RobertaModel</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>RobertaPreLayerNormModel</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong> — <code>RoCBertModel</code> (RoCBert model)</li> <li><strong>roformer</strong> — <code>RoFormerModel</code> (RoFormer model)</li> <li><strong>rwkv</strong> — <code>RwkvModel</code> (RWKV model)</li> <li><strong>sam</strong> — <code>SamModel</code> (SAM model)</li> <li><strong>seamless_m4t</strong> — <code>SeamlessM4TModel</code> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong> — <code>SeamlessM4Tv2Model</code> (SeamlessM4Tv2 model)</li> <li><strong>segformer</strong> — <code>SegformerModel</code> (SegFormer model)</li> <li><strong>sew</strong> — <code>SEWModel</code> (SEW model)</li> <li><strong>sew-d</strong> — <code>SEWDModel</code> (SEW-D model)</li> <li><strong>siglip</strong> — <code>SiglipModel</code> (SigLIP model)</li> <li><strong>siglip_vision_model</strong> — <code>SiglipVisionModel</code> (SiglipVisionModel model)</li> <li><strong>speech_to_text</strong> — <code>Speech2TextModel</code> (Speech2Text model)</li> <li><strong>speecht5</strong> — <code>SpeechT5Model</code> (SpeechT5 model)</li> <li><strong>splinter</strong> — <code>SplinterModel</code> (Splinter model)</li> <li><strong>squeezebert</strong> — <code>SqueezeBertModel</code> (SqueezeBERT model)</li> <li><strong>stablelm</strong> — <code>StableLmModel</code> (StableLm model)</li> <li><strong>swiftformer</strong> — <code>SwiftFormerModel</code> (SwiftFormer model)</li> <li><strong>swin</strong> — <code>SwinModel</code> (Swin Transformer model)</li> <li><strong>swin2sr</strong> — <code>Swin2SRModel</code> (Swin2SR model)</li> <li><strong>swinv2</strong> — <code>Swinv2Model</code> (Swin Transformer V2 model)</li> <li><strong>switch_transformers</strong> — <code>SwitchTransformersModel</code> (SwitchTransformers model)</li> <li><strong>t5</strong> — <code>T5Model</code> (T5 model)</li> <li><strong>table-transformer</strong> — <code>TableTransformerModel</code> (Table Transformer model)</li> <li><strong>tapas</strong> — <code>TapasModel</code> (TAPAS model)</li> <li><strong>time_series_transformer</strong> — <code>TimeSeriesTransformerModel</code> (Time Series Transformer model)</li> <li><strong>timesformer</strong> — <code>TimesformerModel</code> (TimeSformer model)</li> <li><strong>timm_backbone</strong> — <code>TimmBackbone</code> (TimmBackbone model)</li> <li><strong>trajectory_transformer</strong> — <code>TrajectoryTransformerModel</code> (Trajectory Transformer model)</li> <li><strong>transfo-xl</strong> — <code>TransfoXLModel</code> (Transformer-XL model)</li> <li><strong>tvlt</strong> — <code>TvltModel</code> (TVLT model)</li> <li><strong>tvp</strong> — <code>TvpModel</code> (TVP model)</li> <li><strong>umt5</strong> — <code>UMT5Model</code> (UMT5 model)</li> <li><strong>unispeech</strong> — <code>UniSpeechModel</code> (UniSpeech model)</li> <li><strong>unispeech-sat</strong> — <code>UniSpeechSatModel</code> (UniSpeechSat model)</li> <li><strong>univnet</strong> — <code>UnivNetModel</code> (UnivNet model)</li> <li><strong>van</strong> — <code>VanModel</code> (VAN model)</li> <li><strong>videomae</strong> — <code>VideoMAEModel</code> (VideoMAE model)</li> <li><strong>vilt</strong> — <code>ViltModel</code> (ViLT model)</li> <li><strong>vision-text-dual-encoder</strong> — <code>VisionTextDualEncoderModel</code> (VisionTextDualEncoder model)</li> <li><strong>visual_bert</strong> — <code>VisualBertModel</code> (VisualBERT model)</li> <li><strong>vit</strong> — <code>ViTModel</code> (ViT model)</li> <li><strong>vit_hybrid</strong> — <code>ViTHybridModel</code> (ViT Hybrid model)</li> <li><strong>vit_mae</strong> — <code>ViTMAEModel</code> (ViTMAE model)</li> <li><strong>vit_msn</strong> — <code>ViTMSNModel</code> (ViTMSN model)</li> <li><strong>vitdet</strong> — <code>VitDetModel</code> (VitDet model)</li> <li><strong>vits</strong> — <code>VitsModel</code> (VITS model)</li> <li><strong>vivit</strong> — <code>VivitModel</code> (ViViT model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2Model</code> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong> — <code>Wav2Vec2BertModel</code> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong> — <code>Wav2Vec2ConformerModel</code> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong> — <code>WavLMModel</code> (WavLM model)</li> <li><strong>whisper</strong> — <code>WhisperModel</code> (Whisper model)</li> <li><strong>xclip</strong> — <code>XCLIPModel</code> (X-CLIP model)</li> <li><strong>xglm</strong> — <code>XGLMModel</code> (XGLM model)</li> <li><strong>xlm</strong> — <code>XLMModel</code> (XLM model)</li> <li><strong>xlm-prophetnet</strong> — <code>XLMProphetNetModel</code> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong> — <code>XLMRobertaModel</code> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong> — <code>XLMRobertaXLModel</code> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong> — <code>XLNetModel</code> (XLNet model)</li> <li><strong>xmod</strong> — <code>XmodModel</code> (X-MOD model)</li> <li><strong>yolos</strong> — <code>YolosModel</code> (YOLOS model)</li> <li><strong>yoso</strong> — <code>YosoModel</code> (YOSO model)</li>',Sx,zu,LE=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,Ux,Gs,YC,nc,zC,he,rc,Ix,Ou,BE=`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,Nx,Ku,AE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Xx,Gn,ac,qx,eh,RE="Instantiates one of the base model classes of the library from a configuration.",Qx,oh,WE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Hx,Es,Dx,No,sc,Yx,th,JE="Instantiate one of the base model classes of the library from a pretrained model.",zx,nh,VE=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Ox,rh,GE='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li> <li><strong>blenderbot</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li> <li><strong>blip</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.TFBlipModel">TFBlipModel</a> (BLIP model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li> <li><strong>clip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li> <li><strong>convnext</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong> — <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.TFConvNextV2Model">TFConvNextV2Model</a> (ConvNeXTV2 model)</li> <li><strong>ctrl</strong> — <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li> <li><strong>cvt</strong> — <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.TFCvtModel">TFCvtModel</a> (CvT model)</li> <li><strong>data2vec-vision</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.TFData2VecVisionModel">TFData2VecVisionModel</a> (Data2VecVision model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li> <li><strong>deit</strong> — <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTModel">TFDeiTModel</a> (DeiT model)</li> <li><strong>distilbert</strong> — <code>TFDistilBertModel</code> (DistilBERT model)</li> <li><strong>dpr</strong> — <code>TFDPRQuestionEncoder</code> (DPR model)</li> <li><strong>efficientformer</strong> — <code>TFEfficientFormerModel</code> (EfficientFormer model)</li> <li><strong>electra</strong> — <code>TFElectraModel</code> (ELECTRA model)</li> <li><strong>esm</strong> — <code>TFEsmModel</code> (ESM model)</li> <li><strong>flaubert</strong> — <code>TFFlaubertModel</code> (FlauBERT model)</li> <li><strong>funnel</strong> — <code>TFFunnelModel</code> or <code>TFFunnelBaseModel</code> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong> — <code>TFGPT2Model</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>TFGPT2Model</code> (OpenAI GPT-2 model)</li> <li><strong>gptj</strong> — <code>TFGPTJModel</code> (GPT-J model)</li> <li><strong>groupvit</strong> — <code>TFGroupViTModel</code> (GroupViT model)</li> <li><strong>hubert</strong> — <code>TFHubertModel</code> (Hubert model)</li> <li><strong>layoutlm</strong> — <code>TFLayoutLMModel</code> (LayoutLM model)</li> <li><strong>layoutlmv3</strong> — <code>TFLayoutLMv3Model</code> (LayoutLMv3 model)</li> <li><strong>led</strong> — <code>TFLEDModel</code> (LED model)</li> <li><strong>longformer</strong> — <code>TFLongformerModel</code> (Longformer model)</li> <li><strong>lxmert</strong> — <code>TFLxmertModel</code> (LXMERT model)</li> <li><strong>marian</strong> — <code>TFMarianModel</code> (Marian model)</li> <li><strong>mbart</strong> — <code>TFMBartModel</code> (mBART model)</li> <li><strong>mobilebert</strong> — <code>TFMobileBertModel</code> (MobileBERT model)</li> <li><strong>mobilevit</strong> — <code>TFMobileViTModel</code> (MobileViT model)</li> <li><strong>mpnet</strong> — <code>TFMPNetModel</code> (MPNet model)</li> <li><strong>mt5</strong> — <code>TFMT5Model</code> (MT5 model)</li> <li><strong>openai-gpt</strong> — <code>TFOpenAIGPTModel</code> (OpenAI GPT model)</li> <li><strong>opt</strong> — <code>TFOPTModel</code> (OPT model)</li> <li><strong>pegasus</strong> — <code>TFPegasusModel</code> (Pegasus model)</li> <li><strong>regnet</strong> — <code>TFRegNetModel</code> (RegNet model)</li> <li><strong>rembert</strong> — <code>TFRemBertModel</code> (RemBERT model)</li> <li><strong>resnet</strong> — <code>TFResNetModel</code> (ResNet model)</li> <li><strong>roberta</strong> — <code>TFRobertaModel</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>TFRobertaPreLayerNormModel</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>TFRoFormerModel</code> (RoFormer model)</li> <li><strong>sam</strong> — <code>TFSamModel</code> (SAM model)</li> <li><strong>segformer</strong> — <code>TFSegformerModel</code> (SegFormer model)</li> <li><strong>speech_to_text</strong> — <code>TFSpeech2TextModel</code> (Speech2Text model)</li> <li><strong>swin</strong> — <code>TFSwinModel</code> (Swin Transformer model)</li> <li><strong>t5</strong> — <code>TFT5Model</code> (T5 model)</li> <li><strong>tapas</strong> — <code>TFTapasModel</code> (TAPAS model)</li> <li><strong>transfo-xl</strong> — <code>TFTransfoXLModel</code> (Transformer-XL model)</li> <li><strong>vision-text-dual-encoder</strong> — <code>TFVisionTextDualEncoderModel</code> (VisionTextDualEncoder model)</li> <li><strong>vit</strong> — <code>TFViTModel</code> (ViT model)</li> <li><strong>vit_mae</strong> — <code>TFViTMAEModel</code> (ViTMAE model)</li> <li><strong>wav2vec2</strong> — <code>TFWav2Vec2Model</code> (Wav2Vec2 model)</li> <li><strong>whisper</strong> — <code>TFWhisperModel</code> (Whisper model)</li> <li><strong>xglm</strong> — <code>TFXGLMModel</code> (XGLM model)</li> <li><strong>xlm</strong> — <code>TFXLMModel</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>TFXLMRobertaModel</code> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong> — <code>TFXLNetModel</code> (XLNet model)</li>',Kx,Ps,OC,ic,KC,be,lc,e1,ah,EE=`This is a generic model class that will be instantiated as one of the base model classes of the library when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,o1,sh,PE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",t1,En,dc,n1,ih,SE="Instantiates one of the base model classes of the library from a configuration.",r1,lh,UE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,a1,Ss,s1,Xo,cc,i1,dh,IE="Instantiate one of the base model classes of the library from a pretrained model.",l1,ch,NE=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,d1,mh,XE='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li> <li><strong>beit</strong> — <a href="/docs/transformers/main/ja/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li> <li><strong>blenderbot</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li> <li><strong>bloom</strong> — <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.FlaxBloomModel">FlaxBloomModel</a> (BLOOM model)</li> <li><strong>clip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li> <li><strong>distilbert</strong> — <code>FlaxDistilBertModel</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>FlaxElectraModel</code> (ELECTRA model)</li> <li><strong>gpt-sw3</strong> — <code>FlaxGPT2Model</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>FlaxGPT2Model</code> (OpenAI GPT-2 model)</li> <li><strong>gpt_neo</strong> — <code>FlaxGPTNeoModel</code> (GPT Neo model)</li> <li><strong>gptj</strong> — <code>FlaxGPTJModel</code> (GPT-J model)</li> <li><strong>llama</strong> — <code>FlaxLlamaModel</code> (LLaMA model)</li> <li><strong>longt5</strong> — <code>FlaxLongT5Model</code> (LongT5 model)</li> <li><strong>marian</strong> — <code>FlaxMarianModel</code> (Marian model)</li> <li><strong>mbart</strong> — <code>FlaxMBartModel</code> (mBART model)</li> <li><strong>mistral</strong> — <code>FlaxMistralModel</code> (Mistral model)</li> <li><strong>mt5</strong> — <code>FlaxMT5Model</code> (MT5 model)</li> <li><strong>opt</strong> — <code>FlaxOPTModel</code> (OPT model)</li> <li><strong>pegasus</strong> — <code>FlaxPegasusModel</code> (Pegasus model)</li> <li><strong>regnet</strong> — <code>FlaxRegNetModel</code> (RegNet model)</li> <li><strong>resnet</strong> — <code>FlaxResNetModel</code> (ResNet model)</li> <li><strong>roberta</strong> — <code>FlaxRobertaModel</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>FlaxRobertaPreLayerNormModel</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>FlaxRoFormerModel</code> (RoFormer model)</li> <li><strong>t5</strong> — <code>FlaxT5Model</code> (T5 model)</li> <li><strong>vision-text-dual-encoder</strong> — <code>FlaxVisionTextDualEncoderModel</code> (VisionTextDualEncoder model)</li> <li><strong>vit</strong> — <code>FlaxViTModel</code> (ViT model)</li> <li><strong>wav2vec2</strong> — <code>FlaxWav2Vec2Model</code> (Wav2Vec2 model)</li> <li><strong>whisper</strong> — <code>FlaxWhisperModel</code> (Whisper model)</li> <li><strong>xglm</strong> — <code>FlaxXGLMModel</code> (XGLM model)</li> <li><strong>xlm-roberta</strong> — <code>FlaxXLMRobertaModel</code> (XLM-RoBERTa model)</li>',c1,Us,e2,mc,o2,fc,qE="以下の自動クラスは、事前学習ヘッドを持つモデルをインスタンス化するために利用可能です。",t2,gc,n2,_e,pc,m1,fh,QE=`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,f1,gh,HE="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",g1,Pn,uc,p1,ph,DE="Instantiates one of the model classes of the library (with a pretraining head) from a configuration.",u1,uh,YE=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,h1,Is,b1,W,hc,_1,hh,zE="Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model.",M1,bh,OE=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,T1,_h,KE='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li> <li><strong>bloom</strong> — <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li> <li><strong>ctrl</strong> — <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li> <li><strong>data2vec-text</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>DistilBertForMaskedLM</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>ElectraForPreTraining</code> (ELECTRA model)</li> <li><strong>ernie</strong> — <code>ErnieForPreTraining</code> (ERNIE model)</li> <li><strong>flaubert</strong> — <code>FlaubertWithLMHeadModel</code> (FlauBERT model)</li> <li><strong>flava</strong> — <code>FlavaForPreTraining</code> (FLAVA model)</li> <li><strong>fnet</strong> — <code>FNetForPreTraining</code> (FNet model)</li> <li><strong>fsmt</strong> — <code>FSMTForConditionalGeneration</code> (FairSeq Machine-Translation model)</li> <li><strong>funnel</strong> — <code>FunnelForPreTraining</code> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong> — <code>GPT2LMHeadModel</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>GPT2LMHeadModel</code> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong> — <code>GPTBigCodeForCausalLM</code> (GPTBigCode model)</li> <li><strong>gptsan-japanese</strong> — <code>GPTSanJapaneseForConditionalGeneration</code> (GPTSAN-japanese model)</li> <li><strong>ibert</strong> — <code>IBertForMaskedLM</code> (I-BERT model)</li> <li><strong>idefics</strong> — <code>IdeficsForVisionText2Text</code> (IDEFICS model)</li> <li><strong>layoutlm</strong> — <code>LayoutLMForMaskedLM</code> (LayoutLM model)</li> <li><strong>llava</strong> — <code>LlavaForConditionalGeneration</code> (LLaVa model)</li> <li><strong>longformer</strong> — <code>LongformerForMaskedLM</code> (Longformer model)</li> <li><strong>luke</strong> — <code>LukeForMaskedLM</code> (LUKE model)</li> <li><strong>lxmert</strong> — <code>LxmertForPreTraining</code> (LXMERT model)</li> <li><strong>mega</strong> — <code>MegaForMaskedLM</code> (MEGA model)</li> <li><strong>megatron-bert</strong> — <code>MegatronBertForPreTraining</code> (Megatron-BERT model)</li> <li><strong>mobilebert</strong> — <code>MobileBertForPreTraining</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>MPNetForMaskedLM</code> (MPNet model)</li> <li><strong>mpt</strong> — <code>MptForCausalLM</code> (MPT model)</li> <li><strong>mra</strong> — <code>MraForMaskedLM</code> (MRA model)</li> <li><strong>mvp</strong> — <code>MvpForConditionalGeneration</code> (MVP model)</li> <li><strong>nezha</strong> — <code>NezhaForPreTraining</code> (Nezha model)</li> <li><strong>nllb-moe</strong> — <code>NllbMoeForConditionalGeneration</code> (NLLB-MOE model)</li> <li><strong>openai-gpt</strong> — <code>OpenAIGPTLMHeadModel</code> (OpenAI GPT model)</li> <li><strong>retribert</strong> — <code>RetriBertModel</code> (RetriBERT model)</li> <li><strong>roberta</strong> — <code>RobertaForMaskedLM</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>RobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong> — <code>RoCBertForPreTraining</code> (RoCBert model)</li> <li><strong>rwkv</strong> — <code>RwkvForCausalLM</code> (RWKV model)</li> <li><strong>splinter</strong> — <code>SplinterForPreTraining</code> (Splinter model)</li> <li><strong>squeezebert</strong> — <code>SqueezeBertForMaskedLM</code> (SqueezeBERT model)</li> <li><strong>switch_transformers</strong> — <code>SwitchTransformersForConditionalGeneration</code> (SwitchTransformers model)</li> <li><strong>t5</strong> — <code>T5ForConditionalGeneration</code> (T5 model)</li> <li><strong>tapas</strong> — <code>TapasForMaskedLM</code> (TAPAS model)</li> <li><strong>transfo-xl</strong> — <code>TransfoXLLMHeadModel</code> (Transformer-XL model)</li> <li><strong>tvlt</strong> — <code>TvltForPreTraining</code> (TVLT model)</li> <li><strong>unispeech</strong> — <code>UniSpeechForPreTraining</code> (UniSpeech model)</li> <li><strong>unispeech-sat</strong> — <code>UniSpeechSatForPreTraining</code> (UniSpeechSat model)</li> <li><strong>videomae</strong> — <code>VideoMAEForPreTraining</code> (VideoMAE model)</li> <li><strong>vipllava</strong> — <code>VipLlavaForConditionalGeneration</code> (VipLlava model)</li> <li><strong>visual_bert</strong> — <code>VisualBertForPreTraining</code> (VisualBERT model)</li> <li><strong>vit_mae</strong> — <code>ViTMAEForPreTraining</code> (ViTMAE model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2ForPreTraining</code> (Wav2Vec2 model)</li> <li><strong>wav2vec2-conformer</strong> — <code>Wav2Vec2ConformerForPreTraining</code> (Wav2Vec2-Conformer model)</li> <li><strong>xlm</strong> — <code>XLMWithLMHeadModel</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>XLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong> — <code>XLMRobertaXLForMaskedLM</code> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong> — <code>XLNetLMHeadModel</code> (XLNet model)</li> <li><strong>xmod</strong> — <code>XmodForMaskedLM</code> (X-MOD model)</li>',y1,Mh,eP=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,F1,Ns,r2,bc,a2,Me,_c,v1,Th,oP=`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,C1,yh,tP="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",w1,Sn,Mc,j1,Fh,nP="Instantiates one of the model classes of the library (with a pretraining head) from a configuration.",x1,vh,rP=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,$1,Xs,k1,qo,Tc,Z1,Ch,aP="Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model.",L1,wh,sP=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,B1,jh,iP='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li> <li><strong>ctrl</strong> — <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li> <li><strong>distilbert</strong> — <code>TFDistilBertForMaskedLM</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>TFElectraForPreTraining</code> (ELECTRA model)</li> <li><strong>flaubert</strong> — <code>TFFlaubertWithLMHeadModel</code> (FlauBERT model)</li> <li><strong>funnel</strong> — <code>TFFunnelForPreTraining</code> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong> — <code>TFGPT2LMHeadModel</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>TFGPT2LMHeadModel</code> (OpenAI GPT-2 model)</li> <li><strong>layoutlm</strong> — <code>TFLayoutLMForMaskedLM</code> (LayoutLM model)</li> <li><strong>lxmert</strong> — <code>TFLxmertForPreTraining</code> (LXMERT model)</li> <li><strong>mobilebert</strong> — <code>TFMobileBertForPreTraining</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>TFMPNetForMaskedLM</code> (MPNet model)</li> <li><strong>openai-gpt</strong> — <code>TFOpenAIGPTLMHeadModel</code> (OpenAI GPT model)</li> <li><strong>roberta</strong> — <code>TFRobertaForMaskedLM</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>TFRobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>t5</strong> — <code>TFT5ForConditionalGeneration</code> (T5 model)</li> <li><strong>tapas</strong> — <code>TFTapasForMaskedLM</code> (TAPAS model)</li> <li><strong>transfo-xl</strong> — <code>TFTransfoXLLMHeadModel</code> (Transformer-XL model)</li> <li><strong>vit_mae</strong> — <code>TFViTMAEForPreTraining</code> (ViTMAE model)</li> <li><strong>xlm</strong> — <code>TFXLMWithLMHeadModel</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>TFXLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong> — <code>TFXLNetLMHeadModel</code> (XLNet model)</li>',A1,qs,s2,yc,i2,Te,Fc,R1,xh,lP=`This is a generic model class that will be instantiated as one of the model classes of the library (with a pretraining head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,W1,$h,dP="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",J1,Un,vc,V1,kh,cP="Instantiates one of the model classes of the library (with a pretraining head) from a configuration.",G1,Zh,mP=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,E1,Qs,P1,Qo,Cc,S1,Lh,fP="Instantiate one of the model classes of the library (with a pretraining head) from a pretrained model.",U1,Bh,gP=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,I1,Ah,pP='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li> <li><strong>electra</strong> — <code>FlaxElectraForPreTraining</code> (ELECTRA model)</li> <li><strong>longt5</strong> — <code>FlaxLongT5ForConditionalGeneration</code> (LongT5 model)</li> <li><strong>mbart</strong> — <code>FlaxMBartForConditionalGeneration</code> (mBART model)</li> <li><strong>mt5</strong> — <code>FlaxMT5ForConditionalGeneration</code> (MT5 model)</li> <li><strong>roberta</strong> — <code>FlaxRobertaForMaskedLM</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>FlaxRobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>FlaxRoFormerForMaskedLM</code> (RoFormer model)</li> <li><strong>t5</strong> — <code>FlaxT5ForConditionalGeneration</code> (T5 model)</li> <li><strong>wav2vec2</strong> — <code>FlaxWav2Vec2ForPreTraining</code> (Wav2Vec2 model)</li> <li><strong>whisper</strong> — <code>FlaxWhisperForConditionalGeneration</code> (Whisper model)</li> <li><strong>xlm-roberta</strong> — <code>FlaxXLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li>',N1,Hs,l2,wc,d2,jc,uP="以下の自動クラスは、次の自然言語処理タスクに利用可能です。",c2,xc,m2,ye,$c,X1,Rh,hP=`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,q1,Wh,bP="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Q1,In,kc,H1,Jh,_P="Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration.",D1,Vh,MP=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Y1,Ds,z1,J,Zc,O1,Gh,TP="Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.",K1,Eh,yP=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,e$,Ph,FP='<li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li> <li><strong>bert-generation</strong> — <a href="/docs/transformers/main/ja/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong> — <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong> — <a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptForCausalLM">BioGptForCausalLM</a> (BioGpt model)</li> <li><strong>blenderbot</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li> <li><strong>bloom</strong> — <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li> <li><strong>code_llama</strong> — <code>LlamaForCausalLM</code> (CodeLlama model)</li> <li><strong>codegen</strong> — <a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenForCausalLM">CodeGenForCausalLM</a> (CodeGen model)</li> <li><strong>cpmant</strong> — <a href="/docs/transformers/main/ja/model_doc/cpmant#transformers.CpmAntForCausalLM">CpmAntForCausalLM</a> (CPM-Ant model)</li> <li><strong>ctrl</strong> — <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li> <li><strong>data2vec-text</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li> <li><strong>electra</strong> — <code>ElectraForCausalLM</code> (ELECTRA model)</li> <li><strong>ernie</strong> — <code>ErnieForCausalLM</code> (ERNIE model)</li> <li><strong>falcon</strong> — <code>FalconForCausalLM</code> (Falcon model)</li> <li><strong>fuyu</strong> — <code>FuyuForCausalLM</code> (Fuyu model)</li> <li><strong>git</strong> — <code>GitForCausalLM</code> (GIT model)</li> <li><strong>gpt-sw3</strong> — <code>GPT2LMHeadModel</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>GPT2LMHeadModel</code> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong> — <code>GPTBigCodeForCausalLM</code> (GPTBigCode model)</li> <li><strong>gpt_neo</strong> — <code>GPTNeoForCausalLM</code> (GPT Neo model)</li> <li><strong>gpt_neox</strong> — <code>GPTNeoXForCausalLM</code> (GPT NeoX model)</li> <li><strong>gpt_neox_japanese</strong> — <code>GPTNeoXJapaneseForCausalLM</code> (GPT NeoX Japanese model)</li> <li><strong>gptj</strong> — <code>GPTJForCausalLM</code> (GPT-J model)</li> <li><strong>llama</strong> — <code>LlamaForCausalLM</code> (LLaMA model)</li> <li><strong>marian</strong> — <code>MarianForCausalLM</code> (Marian model)</li> <li><strong>mbart</strong> — <code>MBartForCausalLM</code> (mBART model)</li> <li><strong>mega</strong> — <code>MegaForCausalLM</code> (MEGA model)</li> <li><strong>megatron-bert</strong> — <code>MegatronBertForCausalLM</code> (Megatron-BERT model)</li> <li><strong>mistral</strong> — <code>MistralForCausalLM</code> (Mistral model)</li> <li><strong>mixtral</strong> — <code>MixtralForCausalLM</code> (Mixtral model)</li> <li><strong>mpt</strong> — <code>MptForCausalLM</code> (MPT model)</li> <li><strong>musicgen</strong> — <code>MusicgenForCausalLM</code> (MusicGen model)</li> <li><strong>mvp</strong> — <code>MvpForCausalLM</code> (MVP model)</li> <li><strong>open-llama</strong> — <code>OpenLlamaForCausalLM</code> (OpenLlama model)</li> <li><strong>openai-gpt</strong> — <code>OpenAIGPTLMHeadModel</code> (OpenAI GPT model)</li> <li><strong>opt</strong> — <code>OPTForCausalLM</code> (OPT model)</li> <li><strong>pegasus</strong> — <code>PegasusForCausalLM</code> (Pegasus model)</li> <li><strong>persimmon</strong> — <code>PersimmonForCausalLM</code> (Persimmon model)</li> <li><strong>phi</strong> — <code>PhiForCausalLM</code> (Phi model)</li> <li><strong>plbart</strong> — <code>PLBartForCausalLM</code> (PLBart model)</li> <li><strong>prophetnet</strong> — <code>ProphetNetForCausalLM</code> (ProphetNet model)</li> <li><strong>qdqbert</strong> — <code>QDQBertLMHeadModel</code> (QDQBert model)</li> <li><strong>qwen2</strong> — <code>Qwen2ForCausalLM</code> (Qwen2 model)</li> <li><strong>reformer</strong> — <code>ReformerModelWithLMHead</code> (Reformer model)</li> <li><strong>rembert</strong> — <code>RemBertForCausalLM</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>RobertaForCausalLM</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>RobertaPreLayerNormForCausalLM</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong> — <code>RoCBertForCausalLM</code> (RoCBert model)</li> <li><strong>roformer</strong> — <code>RoFormerForCausalLM</code> (RoFormer model)</li> <li><strong>rwkv</strong> — <code>RwkvForCausalLM</code> (RWKV model)</li> <li><strong>speech_to_text_2</strong> — <code>Speech2Text2ForCausalLM</code> (Speech2Text2 model)</li> <li><strong>stablelm</strong> — <code>StableLmForCausalLM</code> (StableLm model)</li> <li><strong>transfo-xl</strong> — <code>TransfoXLLMHeadModel</code> (Transformer-XL model)</li> <li><strong>trocr</strong> — <code>TrOCRForCausalLM</code> (TrOCR model)</li> <li><strong>whisper</strong> — <code>WhisperForCausalLM</code> (Whisper model)</li> <li><strong>xglm</strong> — <code>XGLMForCausalLM</code> (XGLM model)</li> <li><strong>xlm</strong> — <code>XLMWithLMHeadModel</code> (XLM model)</li> <li><strong>xlm-prophetnet</strong> — <code>XLMProphetNetForCausalLM</code> (XLM-ProphetNet model)</li> <li><strong>xlm-roberta</strong> — <code>XLMRobertaForCausalLM</code> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong> — <code>XLMRobertaXLForCausalLM</code> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong> — <code>XLNetLMHeadModel</code> (XLNet model)</li> <li><strong>xmod</strong> — <code>XmodForCausalLM</code> (X-MOD model)</li>',o$,Sh,vP=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,t$,Ys,f2,Lc,g2,Fe,Bc,n$,Uh,CP=`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,r$,Ih,wP="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",a$,Nn,Ac,s$,Nh,jP="Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration.",i$,Xh,xP=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,l$,zs,d$,Ho,Rc,c$,qh,$P="Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.",m$,Qh,kP=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,f$,Hh,ZP='<li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForCausalLM">TFCamembertForCausalLM</a> (CamemBERT model)</li> <li><strong>ctrl</strong> — <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li> <li><strong>gpt-sw3</strong> — <code>TFGPT2LMHeadModel</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>TFGPT2LMHeadModel</code> (OpenAI GPT-2 model)</li> <li><strong>gptj</strong> — <code>TFGPTJForCausalLM</code> (GPT-J model)</li> <li><strong>openai-gpt</strong> — <code>TFOpenAIGPTLMHeadModel</code> (OpenAI GPT model)</li> <li><strong>opt</strong> — <code>TFOPTForCausalLM</code> (OPT model)</li> <li><strong>rembert</strong> — <code>TFRemBertForCausalLM</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>TFRobertaForCausalLM</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>TFRobertaPreLayerNormForCausalLM</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>TFRoFormerForCausalLM</code> (RoFormer model)</li> <li><strong>transfo-xl</strong> — <code>TFTransfoXLLMHeadModel</code> (Transformer-XL model)</li> <li><strong>xglm</strong> — <code>TFXGLMForCausalLM</code> (XGLM model)</li> <li><strong>xlm</strong> — <code>TFXLMWithLMHeadModel</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>TFXLMRobertaForCausalLM</code> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong> — <code>TFXLNetLMHeadModel</code> (XLNet model)</li>',g$,Os,p2,Wc,u2,ve,Jc,p$,Dh,LP=`This is a generic model class that will be instantiated as one of the model classes of the library (with a causal language modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,u$,Yh,BP="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",h$,Xn,Vc,b$,zh,AP="Instantiates one of the model classes of the library (with a causal language modeling head) from a configuration.",_$,Oh,RP=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,M$,Ks,T$,Do,Gc,y$,Kh,WP="Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.",F$,eb,JP=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,v$,ob,VP='<li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForCausalLM">FlaxBartForCausalLM</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForCausalLM">FlaxBertForCausalLM</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForCausalLM">FlaxBigBirdForCausalLM</a> (BigBird model)</li> <li><strong>bloom</strong> — <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.FlaxBloomForCausalLM">FlaxBloomForCausalLM</a> (BLOOM model)</li> <li><strong>electra</strong> — <code>FlaxElectraForCausalLM</code> (ELECTRA model)</li> <li><strong>gpt-sw3</strong> — <code>FlaxGPT2LMHeadModel</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>FlaxGPT2LMHeadModel</code> (OpenAI GPT-2 model)</li> <li><strong>gpt_neo</strong> — <code>FlaxGPTNeoForCausalLM</code> (GPT Neo model)</li> <li><strong>gptj</strong> — <code>FlaxGPTJForCausalLM</code> (GPT-J model)</li> <li><strong>llama</strong> — <code>FlaxLlamaForCausalLM</code> (LLaMA model)</li> <li><strong>mistral</strong> — <code>FlaxMistralForCausalLM</code> (Mistral model)</li> <li><strong>opt</strong> — <code>FlaxOPTForCausalLM</code> (OPT model)</li> <li><strong>roberta</strong> — <code>FlaxRobertaForCausalLM</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>FlaxRobertaPreLayerNormForCausalLM</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>xglm</strong> — <code>FlaxXGLMForCausalLM</code> (XGLM model)</li> <li><strong>xlm-roberta</strong> — <code>FlaxXLMRobertaForCausalLM</code> (XLM-RoBERTa model)</li>',C$,ei,h2,Ec,b2,Ce,Pc,w$,tb,GP=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,j$,nb,EP="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",x$,qn,Sc,$$,rb,PP="Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration.",k$,ab,SP=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Z$,oi,L$,V,Uc,B$,sb,UP="Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model.",A$,ib,IP=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,R$,lb,NP='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>DistilBertForMaskedLM</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>ElectraForMaskedLM</code> (ELECTRA model)</li> <li><strong>ernie</strong> — <code>ErnieForMaskedLM</code> (ERNIE model)</li> <li><strong>esm</strong> — <code>EsmForMaskedLM</code> (ESM model)</li> <li><strong>flaubert</strong> — <code>FlaubertWithLMHeadModel</code> (FlauBERT model)</li> <li><strong>fnet</strong> — <code>FNetForMaskedLM</code> (FNet model)</li> <li><strong>funnel</strong> — <code>FunnelForMaskedLM</code> (Funnel Transformer model)</li> <li><strong>ibert</strong> — <code>IBertForMaskedLM</code> (I-BERT model)</li> <li><strong>layoutlm</strong> — <code>LayoutLMForMaskedLM</code> (LayoutLM model)</li> <li><strong>longformer</strong> — <code>LongformerForMaskedLM</code> (Longformer model)</li> <li><strong>luke</strong> — <code>LukeForMaskedLM</code> (LUKE model)</li> <li><strong>mbart</strong> — <code>MBartForConditionalGeneration</code> (mBART model)</li> <li><strong>mega</strong> — <code>MegaForMaskedLM</code> (MEGA model)</li> <li><strong>megatron-bert</strong> — <code>MegatronBertForMaskedLM</code> (Megatron-BERT model)</li> <li><strong>mobilebert</strong> — <code>MobileBertForMaskedLM</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>MPNetForMaskedLM</code> (MPNet model)</li> <li><strong>mra</strong> — <code>MraForMaskedLM</code> (MRA model)</li> <li><strong>mvp</strong> — <code>MvpForConditionalGeneration</code> (MVP model)</li> <li><strong>nezha</strong> — <code>NezhaForMaskedLM</code> (Nezha model)</li> <li><strong>nystromformer</strong> — <code>NystromformerForMaskedLM</code> (Nyströmformer model)</li> <li><strong>perceiver</strong> — <code>PerceiverForMaskedLM</code> (Perceiver model)</li> <li><strong>qdqbert</strong> — <code>QDQBertForMaskedLM</code> (QDQBert model)</li> <li><strong>reformer</strong> — <code>ReformerForMaskedLM</code> (Reformer model)</li> <li><strong>rembert</strong> — <code>RemBertForMaskedLM</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>RobertaForMaskedLM</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>RobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong> — <code>RoCBertForMaskedLM</code> (RoCBert model)</li> <li><strong>roformer</strong> — <code>RoFormerForMaskedLM</code> (RoFormer model)</li> <li><strong>squeezebert</strong> — <code>SqueezeBertForMaskedLM</code> (SqueezeBERT model)</li> <li><strong>tapas</strong> — <code>TapasForMaskedLM</code> (TAPAS model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2ForMaskedLM</code> (Wav2Vec2 model)</li> <li><strong>xlm</strong> — <code>XLMWithLMHeadModel</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>XLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong> — <code>XLMRobertaXLForMaskedLM</code> (XLM-RoBERTa-XL model)</li> <li><strong>xmod</strong> — <code>XmodForMaskedLM</code> (X-MOD model)</li> <li><strong>yoso</strong> — <code>YosoForMaskedLM</code> (YOSO model)</li>',W$,db,XP=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,J$,ti,_2,Ic,M2,we,Nc,V$,cb,qP=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,G$,mb,QP="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",E$,Qn,Xc,P$,fb,HP="Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration.",S$,gb,DP=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,U$,ni,I$,Yo,qc,N$,pb,YP="Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model.",X$,ub,zP=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,q$,hb,OP='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>TFDistilBertForMaskedLM</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>TFElectraForMaskedLM</code> (ELECTRA model)</li> <li><strong>esm</strong> — <code>TFEsmForMaskedLM</code> (ESM model)</li> <li><strong>flaubert</strong> — <code>TFFlaubertWithLMHeadModel</code> (FlauBERT model)</li> <li><strong>funnel</strong> — <code>TFFunnelForMaskedLM</code> (Funnel Transformer model)</li> <li><strong>layoutlm</strong> — <code>TFLayoutLMForMaskedLM</code> (LayoutLM model)</li> <li><strong>longformer</strong> — <code>TFLongformerForMaskedLM</code> (Longformer model)</li> <li><strong>mobilebert</strong> — <code>TFMobileBertForMaskedLM</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>TFMPNetForMaskedLM</code> (MPNet model)</li> <li><strong>rembert</strong> — <code>TFRemBertForMaskedLM</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>TFRobertaForMaskedLM</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>TFRobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>TFRoFormerForMaskedLM</code> (RoFormer model)</li> <li><strong>tapas</strong> — <code>TFTapasForMaskedLM</code> (TAPAS model)</li> <li><strong>xlm</strong> — <code>TFXLMWithLMHeadModel</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>TFXLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li>',Q$,ri,T2,Qc,y2,je,Hc,H$,bb,KP=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked language modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,D$,_b,eS="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Y$,Hn,Dc,z$,Mb,oS="Instantiates one of the model classes of the library (with a masked language modeling head) from a configuration.",O$,Tb,tS=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,K$,ai,ek,zo,Yc,ok,yb,nS="Instantiate one of the model classes of the library (with a masked language modeling head) from a pretrained model.",tk,Fb,rS=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,nk,vb,aS='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li> <li><strong>distilbert</strong> — <code>FlaxDistilBertForMaskedLM</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>FlaxElectraForMaskedLM</code> (ELECTRA model)</li> <li><strong>mbart</strong> — <code>FlaxMBartForConditionalGeneration</code> (mBART model)</li> <li><strong>roberta</strong> — <code>FlaxRobertaForMaskedLM</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>FlaxRobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>FlaxRoFormerForMaskedLM</code> (RoFormer model)</li> <li><strong>xlm-roberta</strong> — <code>FlaxXLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li>',rk,si,F2,zc,v2,Oc,Kc,C2,em,w2,om,tm,j2,nm,x2,xe,rm,ak,Cb,sS=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,sk,wb,iS="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",ik,Dn,am,lk,jb,lS="Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration.",dk,xb,dS=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,ck,ii,mk,G,sm,fk,$b,cS="Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model.",gk,kb,mS=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,pk,Zb,fS='<li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li> <li><strong>bigbird_pegasus</strong> — <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBird-Pegasus model)</li> <li><strong>blenderbot</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li> <li><strong>encoder-decoder</strong> — <code>EncoderDecoderModel</code> (Encoder decoder model)</li> <li><strong>fsmt</strong> — <code>FSMTForConditionalGeneration</code> (FairSeq Machine-Translation model)</li> <li><strong>gptsan-japanese</strong> — <code>GPTSanJapaneseForConditionalGeneration</code> (GPTSAN-japanese model)</li> <li><strong>led</strong> — <code>LEDForConditionalGeneration</code> (LED model)</li> <li><strong>longt5</strong> — <code>LongT5ForConditionalGeneration</code> (LongT5 model)</li> <li><strong>m2m_100</strong> — <code>M2M100ForConditionalGeneration</code> (M2M100 model)</li> <li><strong>marian</strong> — <code>MarianMTModel</code> (Marian model)</li> <li><strong>mbart</strong> — <code>MBartForConditionalGeneration</code> (mBART model)</li> <li><strong>mt5</strong> — <code>MT5ForConditionalGeneration</code> (MT5 model)</li> <li><strong>mvp</strong> — <code>MvpForConditionalGeneration</code> (MVP model)</li> <li><strong>nllb-moe</strong> — <code>NllbMoeForConditionalGeneration</code> (NLLB-MOE model)</li> <li><strong>pegasus</strong> — <code>PegasusForConditionalGeneration</code> (Pegasus model)</li> <li><strong>pegasus_x</strong> — <code>PegasusXForConditionalGeneration</code> (PEGASUS-X model)</li> <li><strong>plbart</strong> — <code>PLBartForConditionalGeneration</code> (PLBart model)</li> <li><strong>prophetnet</strong> — <code>ProphetNetForConditionalGeneration</code> (ProphetNet model)</li> <li><strong>seamless_m4t</strong> — <code>SeamlessM4TForTextToText</code> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong> — <code>SeamlessM4Tv2ForTextToText</code> (SeamlessM4Tv2 model)</li> <li><strong>switch_transformers</strong> — <code>SwitchTransformersForConditionalGeneration</code> (SwitchTransformers model)</li> <li><strong>t5</strong> — <code>T5ForConditionalGeneration</code> (T5 model)</li> <li><strong>umt5</strong> — <code>UMT5ForConditionalGeneration</code> (UMT5 model)</li> <li><strong>xlm-prophetnet</strong> — <code>XLMProphetNetForConditionalGeneration</code> (XLM-ProphetNet model)</li>',uk,Lb,gS=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,hk,li,$2,im,k2,$e,lm,bk,Bb,pS=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,_k,Ab,uS="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Mk,Yn,dm,Tk,Rb,hS="Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration.",yk,Wb,bS=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Fk,di,vk,Oo,cm,Ck,Jb,_S="Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model.",wk,Vb,MS=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,jk,Gb,TS='<li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li> <li><strong>blenderbot</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li> <li><strong>encoder-decoder</strong> — <code>TFEncoderDecoderModel</code> (Encoder decoder model)</li> <li><strong>led</strong> — <code>TFLEDForConditionalGeneration</code> (LED model)</li> <li><strong>marian</strong> — <code>TFMarianMTModel</code> (Marian model)</li> <li><strong>mbart</strong> — <code>TFMBartForConditionalGeneration</code> (mBART model)</li> <li><strong>mt5</strong> — <code>TFMT5ForConditionalGeneration</code> (MT5 model)</li> <li><strong>pegasus</strong> — <code>TFPegasusForConditionalGeneration</code> (Pegasus model)</li> <li><strong>t5</strong> — <code>TFT5ForConditionalGeneration</code> (T5 model)</li>',xk,ci,Z2,mm,L2,ke,fm,$k,Eb,yS=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence language modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,kk,Pb,FS="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Zk,zn,gm,Lk,Sb,vS="Instantiates one of the model classes of the library (with a sequence-to-sequence language modeling head) from a configuration.",Bk,Ub,CS=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Ak,mi,Rk,Ko,pm,Wk,Ib,wS="Instantiate one of the model classes of the library (with a sequence-to-sequence language modeling head) from a pretrained model.",Jk,Nb,jS=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Vk,Xb,xS='<li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li> <li><strong>blenderbot</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li> <li><strong>blenderbot-small</strong> — <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li> <li><strong>encoder-decoder</strong> — <code>FlaxEncoderDecoderModel</code> (Encoder decoder model)</li> <li><strong>longt5</strong> — <code>FlaxLongT5ForConditionalGeneration</code> (LongT5 model)</li> <li><strong>marian</strong> — <code>FlaxMarianMTModel</code> (Marian model)</li> <li><strong>mbart</strong> — <code>FlaxMBartForConditionalGeneration</code> (mBART model)</li> <li><strong>mt5</strong> — <code>FlaxMT5ForConditionalGeneration</code> (MT5 model)</li> <li><strong>pegasus</strong> — <code>FlaxPegasusForConditionalGeneration</code> (Pegasus model)</li> <li><strong>t5</strong> — <code>FlaxT5ForConditionalGeneration</code> (T5 model)</li>',Gk,fi,B2,um,A2,Ze,hm,Ek,qb,$S=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,Pk,Qb,kS="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Sk,On,bm,Uk,Hb,ZS="Instantiates one of the model classes of the library (with a sequence classification head) from a configuration.",Ik,Db,LS=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,Nk,gi,Xk,E,_m,qk,Yb,BS="Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.",Qk,zb,AS=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Hk,Ob,RS='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong> — <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBird-Pegasus model)</li> <li><strong>biogpt</strong> — <a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptForSequenceClassification">BioGptForSequenceClassification</a> (BioGpt model)</li> <li><strong>bloom</strong> — <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomForSequenceClassification">BloomForSequenceClassification</a> (BLOOM model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li> <li><strong>canine</strong> — <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (CANINE model)</li> <li><strong>code_llama</strong> — <code>LlamaForSequenceClassification</code> (CodeLlama model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li> <li><strong>ctrl</strong> — <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li> <li><strong>data2vec-text</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>DistilBertForSequenceClassification</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>ElectraForSequenceClassification</code> (ELECTRA model)</li> <li><strong>ernie</strong> — <code>ErnieForSequenceClassification</code> (ERNIE model)</li> <li><strong>ernie_m</strong> — <code>ErnieMForSequenceClassification</code> (ErnieM model)</li> <li><strong>esm</strong> — <code>EsmForSequenceClassification</code> (ESM model)</li> <li><strong>falcon</strong> — <code>FalconForSequenceClassification</code> (Falcon model)</li> <li><strong>flaubert</strong> — <code>FlaubertForSequenceClassification</code> (FlauBERT model)</li> <li><strong>fnet</strong> — <code>FNetForSequenceClassification</code> (FNet model)</li> <li><strong>funnel</strong> — <code>FunnelForSequenceClassification</code> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong> — <code>GPT2ForSequenceClassification</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>GPT2ForSequenceClassification</code> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong> — <code>GPTBigCodeForSequenceClassification</code> (GPTBigCode model)</li> <li><strong>gpt_neo</strong> — <code>GPTNeoForSequenceClassification</code> (GPT Neo model)</li> <li><strong>gpt_neox</strong> — <code>GPTNeoXForSequenceClassification</code> (GPT NeoX model)</li> <li><strong>gptj</strong> — <code>GPTJForSequenceClassification</code> (GPT-J model)</li> <li><strong>ibert</strong> — <code>IBertForSequenceClassification</code> (I-BERT model)</li> <li><strong>layoutlm</strong> — <code>LayoutLMForSequenceClassification</code> (LayoutLM model)</li> <li><strong>layoutlmv2</strong> — <code>LayoutLMv2ForSequenceClassification</code> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong> — <code>LayoutLMv3ForSequenceClassification</code> (LayoutLMv3 model)</li> <li><strong>led</strong> — <code>LEDForSequenceClassification</code> (LED model)</li> <li><strong>lilt</strong> — <code>LiltForSequenceClassification</code> (LiLT model)</li> <li><strong>llama</strong> — <code>LlamaForSequenceClassification</code> (LLaMA model)</li> <li><strong>longformer</strong> — <code>LongformerForSequenceClassification</code> (Longformer model)</li> <li><strong>luke</strong> — <code>LukeForSequenceClassification</code> (LUKE model)</li> <li><strong>markuplm</strong> — <code>MarkupLMForSequenceClassification</code> (MarkupLM model)</li> <li><strong>mbart</strong> — <code>MBartForSequenceClassification</code> (mBART model)</li> <li><strong>mega</strong> — <code>MegaForSequenceClassification</code> (MEGA model)</li> <li><strong>megatron-bert</strong> — <code>MegatronBertForSequenceClassification</code> (Megatron-BERT model)</li> <li><strong>mistral</strong> — <code>MistralForSequenceClassification</code> (Mistral model)</li> <li><strong>mixtral</strong> — <code>MixtralForSequenceClassification</code> (Mixtral model)</li> <li><strong>mobilebert</strong> — <code>MobileBertForSequenceClassification</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>MPNetForSequenceClassification</code> (MPNet model)</li> <li><strong>mpt</strong> — <code>MptForSequenceClassification</code> (MPT model)</li> <li><strong>mra</strong> — <code>MraForSequenceClassification</code> (MRA model)</li> <li><strong>mt5</strong> — <code>MT5ForSequenceClassification</code> (MT5 model)</li> <li><strong>mvp</strong> — <code>MvpForSequenceClassification</code> (MVP model)</li> <li><strong>nezha</strong> — <code>NezhaForSequenceClassification</code> (Nezha model)</li> <li><strong>nystromformer</strong> — <code>NystromformerForSequenceClassification</code> (Nyströmformer model)</li> <li><strong>open-llama</strong> — <code>OpenLlamaForSequenceClassification</code> (OpenLlama model)</li> <li><strong>openai-gpt</strong> — <code>OpenAIGPTForSequenceClassification</code> (OpenAI GPT model)</li> <li><strong>opt</strong> — <code>OPTForSequenceClassification</code> (OPT model)</li> <li><strong>perceiver</strong> — <code>PerceiverForSequenceClassification</code> (Perceiver model)</li> <li><strong>persimmon</strong> — <code>PersimmonForSequenceClassification</code> (Persimmon model)</li> <li><strong>phi</strong> — <code>PhiForSequenceClassification</code> (Phi model)</li> <li><strong>plbart</strong> — <code>PLBartForSequenceClassification</code> (PLBart model)</li> <li><strong>qdqbert</strong> — <code>QDQBertForSequenceClassification</code> (QDQBert model)</li> <li><strong>qwen2</strong> — <code>Qwen2ForSequenceClassification</code> (Qwen2 model)</li> <li><strong>reformer</strong> — <code>ReformerForSequenceClassification</code> (Reformer model)</li> <li><strong>rembert</strong> — <code>RemBertForSequenceClassification</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>RobertaForSequenceClassification</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>RobertaPreLayerNormForSequenceClassification</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong> — <code>RoCBertForSequenceClassification</code> (RoCBert model)</li> <li><strong>roformer</strong> — <code>RoFormerForSequenceClassification</code> (RoFormer model)</li> <li><strong>squeezebert</strong> — <code>SqueezeBertForSequenceClassification</code> (SqueezeBERT model)</li> <li><strong>stablelm</strong> — <code>StableLmForSequenceClassification</code> (StableLm model)</li> <li><strong>t5</strong> — <code>T5ForSequenceClassification</code> (T5 model)</li> <li><strong>tapas</strong> — <code>TapasForSequenceClassification</code> (TAPAS model)</li> <li><strong>transfo-xl</strong> — <code>TransfoXLForSequenceClassification</code> (Transformer-XL model)</li> <li><strong>umt5</strong> — <code>UMT5ForSequenceClassification</code> (UMT5 model)</li> <li><strong>xlm</strong> — <code>XLMForSequenceClassification</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>XLMRobertaForSequenceClassification</code> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong> — <code>XLMRobertaXLForSequenceClassification</code> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong> — <code>XLNetForSequenceClassification</code> (XLNet model)</li> <li><strong>xmod</strong> — <code>XmodForSequenceClassification</code> (X-MOD model)</li> <li><strong>yoso</strong> — <code>YosoForSequenceClassification</code> (YOSO model)</li>',Dk,Kb,WS=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,Yk,pi,R2,Mm,W2,Le,Tm,zk,e_,JS=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,Ok,o_,VS="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",Kk,Kn,ym,eZ,t_,GS="Instantiates one of the model classes of the library (with a sequence classification head) from a configuration.",oZ,n_,ES=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,tZ,ui,nZ,et,Fm,rZ,r_,PS="Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.",aZ,a_,SS=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,sZ,s_,US='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.TFBartForSequenceClassification">TFBartForSequenceClassification</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li> <li><strong>ctrl</strong> — <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>TFDistilBertForSequenceClassification</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>TFElectraForSequenceClassification</code> (ELECTRA model)</li> <li><strong>esm</strong> — <code>TFEsmForSequenceClassification</code> (ESM model)</li> <li><strong>flaubert</strong> — <code>TFFlaubertForSequenceClassification</code> (FlauBERT model)</li> <li><strong>funnel</strong> — <code>TFFunnelForSequenceClassification</code> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong> — <code>TFGPT2ForSequenceClassification</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>TFGPT2ForSequenceClassification</code> (OpenAI GPT-2 model)</li> <li><strong>gptj</strong> — <code>TFGPTJForSequenceClassification</code> (GPT-J model)</li> <li><strong>layoutlm</strong> — <code>TFLayoutLMForSequenceClassification</code> (LayoutLM model)</li> <li><strong>layoutlmv3</strong> — <code>TFLayoutLMv3ForSequenceClassification</code> (LayoutLMv3 model)</li> <li><strong>longformer</strong> — <code>TFLongformerForSequenceClassification</code> (Longformer model)</li> <li><strong>mobilebert</strong> — <code>TFMobileBertForSequenceClassification</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>TFMPNetForSequenceClassification</code> (MPNet model)</li> <li><strong>openai-gpt</strong> — <code>TFOpenAIGPTForSequenceClassification</code> (OpenAI GPT model)</li> <li><strong>rembert</strong> — <code>TFRemBertForSequenceClassification</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>TFRobertaForSequenceClassification</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>TFRobertaPreLayerNormForSequenceClassification</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>TFRoFormerForSequenceClassification</code> (RoFormer model)</li> <li><strong>tapas</strong> — <code>TFTapasForSequenceClassification</code> (TAPAS model)</li> <li><strong>transfo-xl</strong> — <code>TFTransfoXLForSequenceClassification</code> (Transformer-XL model)</li> <li><strong>xlm</strong> — <code>TFXLMForSequenceClassification</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>TFXLMRobertaForSequenceClassification</code> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong> — <code>TFXLNetForSequenceClassification</code> (XLNet model)</li>',iZ,hi,J2,vm,V2,Be,Cm,lZ,i_,IS=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,dZ,l_,NS="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",cZ,er,wm,mZ,d_,XS="Instantiates one of the model classes of the library (with a sequence classification head) from a configuration.",fZ,c_,qS=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,gZ,bi,pZ,ot,jm,uZ,m_,QS="Instantiate one of the model classes of the library (with a sequence classification head) from a pretrained model.",hZ,f_,HS=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,bZ,g_,DS='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li> <li><strong>distilbert</strong> — <code>FlaxDistilBertForSequenceClassification</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>FlaxElectraForSequenceClassification</code> (ELECTRA model)</li> <li><strong>mbart</strong> — <code>FlaxMBartForSequenceClassification</code> (mBART model)</li> <li><strong>roberta</strong> — <code>FlaxRobertaForSequenceClassification</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>FlaxRobertaPreLayerNormForSequenceClassification</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>FlaxRoFormerForSequenceClassification</code> (RoFormer model)</li> <li><strong>xlm-roberta</strong> — <code>FlaxXLMRobertaForSequenceClassification</code> (XLM-RoBERTa model)</li>',_Z,_i,G2,xm,E2,Ae,$m,MZ,p_,YS=`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,TZ,u_,zS="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",yZ,or,km,FZ,h_,OS="Instantiates one of the model classes of the library (with a multiple choice head) from a configuration.",vZ,b_,KS=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,CZ,Mi,wZ,P,Zm,jZ,__,eU="Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model.",xZ,M_,oU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,$Z,T_,tU='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li> <li><strong>canine</strong> — <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (CANINE model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForMultipleChoice">DebertaV2ForMultipleChoice</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>DistilBertForMultipleChoice</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>ElectraForMultipleChoice</code> (ELECTRA model)</li> <li><strong>ernie</strong> — <code>ErnieForMultipleChoice</code> (ERNIE model)</li> <li><strong>ernie_m</strong> — <code>ErnieMForMultipleChoice</code> (ErnieM model)</li> <li><strong>flaubert</strong> — <code>FlaubertForMultipleChoice</code> (FlauBERT model)</li> <li><strong>fnet</strong> — <code>FNetForMultipleChoice</code> (FNet model)</li> <li><strong>funnel</strong> — <code>FunnelForMultipleChoice</code> (Funnel Transformer model)</li> <li><strong>ibert</strong> — <code>IBertForMultipleChoice</code> (I-BERT model)</li> <li><strong>longformer</strong> — <code>LongformerForMultipleChoice</code> (Longformer model)</li> <li><strong>luke</strong> — <code>LukeForMultipleChoice</code> (LUKE model)</li> <li><strong>mega</strong> — <code>MegaForMultipleChoice</code> (MEGA model)</li> <li><strong>megatron-bert</strong> — <code>MegatronBertForMultipleChoice</code> (Megatron-BERT model)</li> <li><strong>mobilebert</strong> — <code>MobileBertForMultipleChoice</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>MPNetForMultipleChoice</code> (MPNet model)</li> <li><strong>mra</strong> — <code>MraForMultipleChoice</code> (MRA model)</li> <li><strong>nezha</strong> — <code>NezhaForMultipleChoice</code> (Nezha model)</li> <li><strong>nystromformer</strong> — <code>NystromformerForMultipleChoice</code> (Nyströmformer model)</li> <li><strong>qdqbert</strong> — <code>QDQBertForMultipleChoice</code> (QDQBert model)</li> <li><strong>rembert</strong> — <code>RemBertForMultipleChoice</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>RobertaForMultipleChoice</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>RobertaPreLayerNormForMultipleChoice</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong> — <code>RoCBertForMultipleChoice</code> (RoCBert model)</li> <li><strong>roformer</strong> — <code>RoFormerForMultipleChoice</code> (RoFormer model)</li> <li><strong>squeezebert</strong> — <code>SqueezeBertForMultipleChoice</code> (SqueezeBERT model)</li> <li><strong>xlm</strong> — <code>XLMForMultipleChoice</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>XLMRobertaForMultipleChoice</code> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong> — <code>XLMRobertaXLForMultipleChoice</code> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong> — <code>XLNetForMultipleChoice</code> (XLNet model)</li> <li><strong>xmod</strong> — <code>XmodForMultipleChoice</code> (X-MOD model)</li> <li><strong>yoso</strong> — <code>YosoForMultipleChoice</code> (YOSO model)</li>',kZ,y_,nU=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,ZZ,Ti,P2,Lm,S2,Re,Bm,LZ,F_,rU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,BZ,v_,aU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",AZ,tr,Am,RZ,C_,sU="Instantiates one of the model classes of the library (with a multiple choice head) from a configuration.",WZ,w_,iU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,JZ,yi,VZ,tt,Rm,GZ,j_,lU="Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model.",EZ,x_,dU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,PZ,$_,cU='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2ForMultipleChoice">TFDebertaV2ForMultipleChoice</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>TFDistilBertForMultipleChoice</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>TFElectraForMultipleChoice</code> (ELECTRA model)</li> <li><strong>flaubert</strong> — <code>TFFlaubertForMultipleChoice</code> (FlauBERT model)</li> <li><strong>funnel</strong> — <code>TFFunnelForMultipleChoice</code> (Funnel Transformer model)</li> <li><strong>longformer</strong> — <code>TFLongformerForMultipleChoice</code> (Longformer model)</li> <li><strong>mobilebert</strong> — <code>TFMobileBertForMultipleChoice</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>TFMPNetForMultipleChoice</code> (MPNet model)</li> <li><strong>rembert</strong> — <code>TFRemBertForMultipleChoice</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>TFRobertaForMultipleChoice</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>TFRobertaPreLayerNormForMultipleChoice</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>TFRoFormerForMultipleChoice</code> (RoFormer model)</li> <li><strong>xlm</strong> — <code>TFXLMForMultipleChoice</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>TFXLMRobertaForMultipleChoice</code> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong> — <code>TFXLNetForMultipleChoice</code> (XLNet model)</li>',SZ,Fi,U2,Wm,I2,We,Jm,UZ,k_,mU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a multiple choice head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,IZ,Z_,fU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",NZ,nr,Vm,XZ,L_,gU="Instantiates one of the model classes of the library (with a multiple choice head) from a configuration.",qZ,B_,pU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,QZ,vi,HZ,nt,Gm,DZ,A_,uU="Instantiate one of the model classes of the library (with a multiple choice head) from a pretrained model.",YZ,R_,hU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,zZ,W_,bU='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li> <li><strong>distilbert</strong> — <code>FlaxDistilBertForMultipleChoice</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>FlaxElectraForMultipleChoice</code> (ELECTRA model)</li> <li><strong>roberta</strong> — <code>FlaxRobertaForMultipleChoice</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>FlaxRobertaPreLayerNormForMultipleChoice</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>FlaxRoFormerForMultipleChoice</code> (RoFormer model)</li> <li><strong>xlm-roberta</strong> — <code>FlaxXLMRobertaForMultipleChoice</code> (XLM-RoBERTa model)</li>',OZ,Ci,N2,Em,X2,Je,Pm,KZ,J_,_U=`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,eL,V_,MU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",oL,rr,Sm,tL,G_,TU="Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration.",nL,E_,yU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,rL,wi,aL,S,Um,sL,P_,FU="Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model.",iL,S_,vU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,lL,U_,CU='<li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li> <li><strong>ernie</strong> — <code>ErnieForNextSentencePrediction</code> (ERNIE model)</li> <li><strong>fnet</strong> — <code>FNetForNextSentencePrediction</code> (FNet model)</li> <li><strong>megatron-bert</strong> — <code>MegatronBertForNextSentencePrediction</code> (Megatron-BERT model)</li> <li><strong>mobilebert</strong> — <code>MobileBertForNextSentencePrediction</code> (MobileBERT model)</li> <li><strong>nezha</strong> — <code>NezhaForNextSentencePrediction</code> (Nezha model)</li> <li><strong>qdqbert</strong> — <code>QDQBertForNextSentencePrediction</code> (QDQBert model)</li>',dL,I_,wU=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,cL,ji,q2,Im,Q2,Ve,Nm,mL,N_,jU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,fL,X_,xU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",gL,ar,Xm,pL,q_,$U="Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration.",uL,Q_,kU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,hL,xi,bL,rt,qm,_L,H_,ZU="Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model.",ML,D_,LU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,TL,Y_,BU='<li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForNextSentencePrediction">TFBertForNextSentencePrediction</a> (BERT model)</li> <li><strong>mobilebert</strong> — <code>TFMobileBertForNextSentencePrediction</code> (MobileBERT model)</li>',yL,$i,H2,Qm,D2,Ge,Hm,FL,z_,AU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a next sentence prediction head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,vL,O_,RU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",CL,sr,Dm,wL,K_,WU="Instantiates one of the model classes of the library (with a next sentence prediction head) from a configuration.",jL,eM,JU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,xL,ki,$L,at,Ym,kL,oM,VU="Instantiate one of the model classes of the library (with a next sentence prediction head) from a pretrained model.",ZL,tM,GU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,LL,nM,EU='<li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>',BL,Zi,Y2,zm,z2,Ee,Om,AL,rM,PU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,RL,aM,SU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",WL,ir,Km,JL,sM,UU="Instantiates one of the model classes of the library (with a token classification head) from a configuration.",VL,iM,IU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,GL,Li,EL,U,ef,PL,lM,NU="Instantiate one of the model classes of the library (with a token classification head) from a pretrained model.",SL,dM,XU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,UL,cM,qU='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li> <li><strong>biogpt</strong> — <a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptForTokenClassification">BioGptForTokenClassification</a> (BioGpt model)</li> <li><strong>bloom</strong> — <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomForTokenClassification">BloomForTokenClassification</a> (BLOOM model)</li> <li><strong>bros</strong> — <a href="/docs/transformers/main/ja/model_doc/bros#transformers.BrosForTokenClassification">BrosForTokenClassification</a> (BROS model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li> <li><strong>canine</strong> — <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (CANINE model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>DistilBertForTokenClassification</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>ElectraForTokenClassification</code> (ELECTRA model)</li> <li><strong>ernie</strong> — <code>ErnieForTokenClassification</code> (ERNIE model)</li> <li><strong>ernie_m</strong> — <code>ErnieMForTokenClassification</code> (ErnieM model)</li> <li><strong>esm</strong> — <code>EsmForTokenClassification</code> (ESM model)</li> <li><strong>falcon</strong> — <code>FalconForTokenClassification</code> (Falcon model)</li> <li><strong>flaubert</strong> — <code>FlaubertForTokenClassification</code> (FlauBERT model)</li> <li><strong>fnet</strong> — <code>FNetForTokenClassification</code> (FNet model)</li> <li><strong>funnel</strong> — <code>FunnelForTokenClassification</code> (Funnel Transformer model)</li> <li><strong>gpt-sw3</strong> — <code>GPT2ForTokenClassification</code> (GPT-Sw3 model)</li> <li><strong>gpt2</strong> — <code>GPT2ForTokenClassification</code> (OpenAI GPT-2 model)</li> <li><strong>gpt_bigcode</strong> — <code>GPTBigCodeForTokenClassification</code> (GPTBigCode model)</li> <li><strong>gpt_neo</strong> — <code>GPTNeoForTokenClassification</code> (GPT Neo model)</li> <li><strong>gpt_neox</strong> — <code>GPTNeoXForTokenClassification</code> (GPT NeoX model)</li> <li><strong>ibert</strong> — <code>IBertForTokenClassification</code> (I-BERT model)</li> <li><strong>layoutlm</strong> — <code>LayoutLMForTokenClassification</code> (LayoutLM model)</li> <li><strong>layoutlmv2</strong> — <code>LayoutLMv2ForTokenClassification</code> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong> — <code>LayoutLMv3ForTokenClassification</code> (LayoutLMv3 model)</li> <li><strong>lilt</strong> — <code>LiltForTokenClassification</code> (LiLT model)</li> <li><strong>longformer</strong> — <code>LongformerForTokenClassification</code> (Longformer model)</li> <li><strong>luke</strong> — <code>LukeForTokenClassification</code> (LUKE model)</li> <li><strong>markuplm</strong> — <code>MarkupLMForTokenClassification</code> (MarkupLM model)</li> <li><strong>mega</strong> — <code>MegaForTokenClassification</code> (MEGA model)</li> <li><strong>megatron-bert</strong> — <code>MegatronBertForTokenClassification</code> (Megatron-BERT model)</li> <li><strong>mobilebert</strong> — <code>MobileBertForTokenClassification</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>MPNetForTokenClassification</code> (MPNet model)</li> <li><strong>mpt</strong> — <code>MptForTokenClassification</code> (MPT model)</li> <li><strong>mra</strong> — <code>MraForTokenClassification</code> (MRA model)</li> <li><strong>mt5</strong> — <code>MT5ForTokenClassification</code> (MT5 model)</li> <li><strong>nezha</strong> — <code>NezhaForTokenClassification</code> (Nezha model)</li> <li><strong>nystromformer</strong> — <code>NystromformerForTokenClassification</code> (Nyströmformer model)</li> <li><strong>phi</strong> — <code>PhiForTokenClassification</code> (Phi model)</li> <li><strong>qdqbert</strong> — <code>QDQBertForTokenClassification</code> (QDQBert model)</li> <li><strong>rembert</strong> — <code>RemBertForTokenClassification</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>RobertaForTokenClassification</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>RobertaPreLayerNormForTokenClassification</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong> — <code>RoCBertForTokenClassification</code> (RoCBert model)</li> <li><strong>roformer</strong> — <code>RoFormerForTokenClassification</code> (RoFormer model)</li> <li><strong>squeezebert</strong> — <code>SqueezeBertForTokenClassification</code> (SqueezeBERT model)</li> <li><strong>t5</strong> — <code>T5ForTokenClassification</code> (T5 model)</li> <li><strong>umt5</strong> — <code>UMT5ForTokenClassification</code> (UMT5 model)</li> <li><strong>xlm</strong> — <code>XLMForTokenClassification</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>XLMRobertaForTokenClassification</code> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong> — <code>XLMRobertaXLForTokenClassification</code> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong> — <code>XLNetForTokenClassification</code> (XLNet model)</li> <li><strong>xmod</strong> — <code>XmodForTokenClassification</code> (X-MOD model)</li> <li><strong>yoso</strong> — <code>YosoForTokenClassification</code> (YOSO model)</li>',IL,mM,QU=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,NL,Bi,O2,of,K2,Pe,tf,XL,fM,HU=`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,qL,gM,DU="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",QL,lr,nf,HL,pM,YU="Instantiates one of the model classes of the library (with a token classification head) from a configuration.",DL,uM,zU=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,YL,Ai,zL,st,rf,OL,hM,OU="Instantiate one of the model classes of the library (with a token classification head) from a pretrained model.",KL,bM,KU=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,eB,_M,e9='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>TFDistilBertForTokenClassification</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>TFElectraForTokenClassification</code> (ELECTRA model)</li> <li><strong>esm</strong> — <code>TFEsmForTokenClassification</code> (ESM model)</li> <li><strong>flaubert</strong> — <code>TFFlaubertForTokenClassification</code> (FlauBERT model)</li> <li><strong>funnel</strong> — <code>TFFunnelForTokenClassification</code> (Funnel Transformer model)</li> <li><strong>layoutlm</strong> — <code>TFLayoutLMForTokenClassification</code> (LayoutLM model)</li> <li><strong>layoutlmv3</strong> — <code>TFLayoutLMv3ForTokenClassification</code> (LayoutLMv3 model)</li> <li><strong>longformer</strong> — <code>TFLongformerForTokenClassification</code> (Longformer model)</li> <li><strong>mobilebert</strong> — <code>TFMobileBertForTokenClassification</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>TFMPNetForTokenClassification</code> (MPNet model)</li> <li><strong>rembert</strong> — <code>TFRemBertForTokenClassification</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>TFRobertaForTokenClassification</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>TFRobertaPreLayerNormForTokenClassification</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>TFRoFormerForTokenClassification</code> (RoFormer model)</li> <li><strong>xlm</strong> — <code>TFXLMForTokenClassification</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>TFXLMRobertaForTokenClassification</code> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong> — <code>TFXLNetForTokenClassification</code> (XLNet model)</li>',oB,Ri,ew,af,ow,Se,sf,tB,MM,o9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a token classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,nB,TM,t9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",rB,dr,lf,aB,yM,n9="Instantiates one of the model classes of the library (with a token classification head) from a configuration.",sB,FM,r9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,iB,Wi,lB,it,df,dB,vM,a9="Instantiate one of the model classes of the library (with a token classification head) from a pretrained model.",cB,CM,s9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,mB,wM,i9='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li> <li><strong>distilbert</strong> — <code>FlaxDistilBertForTokenClassification</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>FlaxElectraForTokenClassification</code> (ELECTRA model)</li> <li><strong>roberta</strong> — <code>FlaxRobertaForTokenClassification</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>FlaxRobertaPreLayerNormForTokenClassification</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>FlaxRoFormerForTokenClassification</code> (RoFormer model)</li> <li><strong>xlm-roberta</strong> — <code>FlaxXLMRobertaForTokenClassification</code> (XLM-RoBERTa model)</li>',fB,Ji,tw,cf,nw,Ue,mf,gB,jM,l9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,pB,xM,d9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",uB,cr,ff,hB,$M,c9="Instantiates one of the model classes of the library (with a question answering head) from a configuration.",bB,kM,m9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,_B,Vi,MB,I,gf,TB,ZM,f9="Instantiate one of the model classes of the library (with a question answering head) from a pretrained model.",yB,LM,g9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,FB,BM,p9='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li> <li><strong>bigbird_pegasus</strong> — <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBird-Pegasus model)</li> <li><strong>bloom</strong> — <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomForQuestionAnswering">BloomForQuestionAnswering</a> (BLOOM model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li> <li><strong>canine</strong> — <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (CANINE model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li> <li><strong>data2vec-text</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>DistilBertForQuestionAnswering</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>ElectraForQuestionAnswering</code> (ELECTRA model)</li> <li><strong>ernie</strong> — <code>ErnieForQuestionAnswering</code> (ERNIE model)</li> <li><strong>ernie_m</strong> — <code>ErnieMForQuestionAnswering</code> (ErnieM model)</li> <li><strong>falcon</strong> — <code>FalconForQuestionAnswering</code> (Falcon model)</li> <li><strong>flaubert</strong> — <code>FlaubertForQuestionAnsweringSimple</code> (FlauBERT model)</li> <li><strong>fnet</strong> — <code>FNetForQuestionAnswering</code> (FNet model)</li> <li><strong>funnel</strong> — <code>FunnelForQuestionAnswering</code> (Funnel Transformer model)</li> <li><strong>gpt2</strong> — <code>GPT2ForQuestionAnswering</code> (OpenAI GPT-2 model)</li> <li><strong>gpt_neo</strong> — <code>GPTNeoForQuestionAnswering</code> (GPT Neo model)</li> <li><strong>gpt_neox</strong> — <code>GPTNeoXForQuestionAnswering</code> (GPT NeoX model)</li> <li><strong>gptj</strong> — <code>GPTJForQuestionAnswering</code> (GPT-J model)</li> <li><strong>ibert</strong> — <code>IBertForQuestionAnswering</code> (I-BERT model)</li> <li><strong>layoutlmv2</strong> — <code>LayoutLMv2ForQuestionAnswering</code> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong> — <code>LayoutLMv3ForQuestionAnswering</code> (LayoutLMv3 model)</li> <li><strong>led</strong> — <code>LEDForQuestionAnswering</code> (LED model)</li> <li><strong>lilt</strong> — <code>LiltForQuestionAnswering</code> (LiLT model)</li> <li><strong>llama</strong> — <code>LlamaForQuestionAnswering</code> (LLaMA model)</li> <li><strong>longformer</strong> — <code>LongformerForQuestionAnswering</code> (Longformer model)</li> <li><strong>luke</strong> — <code>LukeForQuestionAnswering</code> (LUKE model)</li> <li><strong>lxmert</strong> — <code>LxmertForQuestionAnswering</code> (LXMERT model)</li> <li><strong>markuplm</strong> — <code>MarkupLMForQuestionAnswering</code> (MarkupLM model)</li> <li><strong>mbart</strong> — <code>MBartForQuestionAnswering</code> (mBART model)</li> <li><strong>mega</strong> — <code>MegaForQuestionAnswering</code> (MEGA model)</li> <li><strong>megatron-bert</strong> — <code>MegatronBertForQuestionAnswering</code> (Megatron-BERT model)</li> <li><strong>mobilebert</strong> — <code>MobileBertForQuestionAnswering</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>MPNetForQuestionAnswering</code> (MPNet model)</li> <li><strong>mpt</strong> — <code>MptForQuestionAnswering</code> (MPT model)</li> <li><strong>mra</strong> — <code>MraForQuestionAnswering</code> (MRA model)</li> <li><strong>mt5</strong> — <code>MT5ForQuestionAnswering</code> (MT5 model)</li> <li><strong>mvp</strong> — <code>MvpForQuestionAnswering</code> (MVP model)</li> <li><strong>nezha</strong> — <code>NezhaForQuestionAnswering</code> (Nezha model)</li> <li><strong>nystromformer</strong> — <code>NystromformerForQuestionAnswering</code> (Nyströmformer model)</li> <li><strong>opt</strong> — <code>OPTForQuestionAnswering</code> (OPT model)</li> <li><strong>qdqbert</strong> — <code>QDQBertForQuestionAnswering</code> (QDQBert model)</li> <li><strong>reformer</strong> — <code>ReformerForQuestionAnswering</code> (Reformer model)</li> <li><strong>rembert</strong> — <code>RemBertForQuestionAnswering</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>RobertaForQuestionAnswering</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>RobertaPreLayerNormForQuestionAnswering</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roc_bert</strong> — <code>RoCBertForQuestionAnswering</code> (RoCBert model)</li> <li><strong>roformer</strong> — <code>RoFormerForQuestionAnswering</code> (RoFormer model)</li> <li><strong>splinter</strong> — <code>SplinterForQuestionAnswering</code> (Splinter model)</li> <li><strong>squeezebert</strong> — <code>SqueezeBertForQuestionAnswering</code> (SqueezeBERT model)</li> <li><strong>t5</strong> — <code>T5ForQuestionAnswering</code> (T5 model)</li> <li><strong>umt5</strong> — <code>UMT5ForQuestionAnswering</code> (UMT5 model)</li> <li><strong>xlm</strong> — <code>XLMForQuestionAnsweringSimple</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>XLMRobertaForQuestionAnswering</code> (XLM-RoBERTa model)</li> <li><strong>xlm-roberta-xl</strong> — <code>XLMRobertaXLForQuestionAnswering</code> (XLM-RoBERTa-XL model)</li> <li><strong>xlnet</strong> — <code>XLNetForQuestionAnsweringSimple</code> (XLNet model)</li> <li><strong>xmod</strong> — <code>XmodForQuestionAnswering</code> (X-MOD model)</li> <li><strong>yoso</strong> — <code>YosoForQuestionAnswering</code> (YOSO model)</li>',vB,AM,u9=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,CB,Gi,rw,pf,aw,Ie,uf,wB,RM,h9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,jB,WM,b9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",xB,mr,hf,$B,JM,_9="Instantiates one of the model classes of the library (with a question answering head) from a configuration.",kB,VM,M9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,ZB,Ei,LB,lt,bf,BB,GM,T9="Instantiate one of the model classes of the library (with a question answering head) from a pretrained model.",AB,EM,y9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,RB,PM,F9='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li> <li><strong>camembert</strong> — <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li> <li><strong>convbert</strong> — <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li> <li><strong>deberta</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li> <li><strong>deberta-v2</strong> — <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li> <li><strong>distilbert</strong> — <code>TFDistilBertForQuestionAnswering</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>TFElectraForQuestionAnswering</code> (ELECTRA model)</li> <li><strong>flaubert</strong> — <code>TFFlaubertForQuestionAnsweringSimple</code> (FlauBERT model)</li> <li><strong>funnel</strong> — <code>TFFunnelForQuestionAnswering</code> (Funnel Transformer model)</li> <li><strong>gptj</strong> — <code>TFGPTJForQuestionAnswering</code> (GPT-J model)</li> <li><strong>layoutlmv3</strong> — <code>TFLayoutLMv3ForQuestionAnswering</code> (LayoutLMv3 model)</li> <li><strong>longformer</strong> — <code>TFLongformerForQuestionAnswering</code> (Longformer model)</li> <li><strong>mobilebert</strong> — <code>TFMobileBertForQuestionAnswering</code> (MobileBERT model)</li> <li><strong>mpnet</strong> — <code>TFMPNetForQuestionAnswering</code> (MPNet model)</li> <li><strong>rembert</strong> — <code>TFRemBertForQuestionAnswering</code> (RemBERT model)</li> <li><strong>roberta</strong> — <code>TFRobertaForQuestionAnswering</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>TFRobertaPreLayerNormForQuestionAnswering</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>TFRoFormerForQuestionAnswering</code> (RoFormer model)</li> <li><strong>xlm</strong> — <code>TFXLMForQuestionAnsweringSimple</code> (XLM model)</li> <li><strong>xlm-roberta</strong> — <code>TFXLMRobertaForQuestionAnswering</code> (XLM-RoBERTa model)</li> <li><strong>xlnet</strong> — <code>TFXLNetForQuestionAnsweringSimple</code> (XLNet model)</li>',WB,Pi,sw,_f,iw,Ne,Mf,JB,SM,v9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a question answering head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,VB,UM,C9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",GB,fr,Tf,EB,IM,w9="Instantiates one of the model classes of the library (with a question answering head) from a configuration.",PB,NM,j9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,SB,Si,UB,dt,yf,IB,XM,x9="Instantiate one of the model classes of the library (with a question answering head) from a pretrained model.",NB,qM,$9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,XB,QM,k9='<li><strong>albert</strong> — <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li> <li><strong>bart</strong> — <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li> <li><strong>bert</strong> — <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li> <li><strong>big_bird</strong> — <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li> <li><strong>distilbert</strong> — <code>FlaxDistilBertForQuestionAnswering</code> (DistilBERT model)</li> <li><strong>electra</strong> — <code>FlaxElectraForQuestionAnswering</code> (ELECTRA model)</li> <li><strong>mbart</strong> — <code>FlaxMBartForQuestionAnswering</code> (mBART model)</li> <li><strong>roberta</strong> — <code>FlaxRobertaForQuestionAnswering</code> (RoBERTa model)</li> <li><strong>roberta-prelayernorm</strong> — <code>FlaxRobertaPreLayerNormForQuestionAnswering</code> (RoBERTa-PreLayerNorm model)</li> <li><strong>roformer</strong> — <code>FlaxRoFormerForQuestionAnswering</code> (RoFormer model)</li> <li><strong>xlm-roberta</strong> — <code>FlaxXLMRobertaForQuestionAnswering</code> (XLM-RoBERTa model)</li>',qB,Ui,lw,Ff,dw,vf,Cf,cw,wf,mw,jf,xf,fw,$f,gw,kf,Z9="以下の自動クラスは、次のコンピュータービジョンタスクに利用可能です。",pw,Zf,uw,Xe,Lf,QB,HM,L9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a depth estimation head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,HB,DM,B9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",DB,gr,Bf,YB,YM,A9="Instantiates one of the model classes of the library (with a depth estimation head) from a configuration.",zB,zM,R9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,OB,Ii,KB,N,Af,eA,OM,W9="Instantiate one of the model classes of the library (with a depth estimation head) from a pretrained model.",oA,KM,J9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,tA,eT,V9="<li><strong>depth_anything</strong> — <code>DepthAnythingForDepthEstimation</code> (Depth Anything model)</li> <li><strong>dpt</strong> — <code>DPTForDepthEstimation</code> (DPT model)</li> <li><strong>glpn</strong> — <code>GLPNForDepthEstimation</code> (GLPN model)</li>",nA,oT,G9=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,rA,Ni,hw,Rf,bw,qe,Wf,aA,tT,E9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,sA,nT,P9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",iA,pr,Jf,lA,rT,S9="Instantiates one of the model classes of the library (with a image classification head) from a configuration.",dA,aT,U9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,cA,Xi,mA,X,Vf,fA,sT,I9="Instantiate one of the model classes of the library (with a image classification head) from a pretrained model.",gA,iT,N9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,pA,lT,X9='<li><strong>beit</strong> — <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li> <li><strong>bit</strong> — <a href="/docs/transformers/main/ja/model_doc/bit#transformers.BitForImageClassification">BitForImageClassification</a> (BiT model)</li> <li><strong>clip</strong> — <code>CLIPForImageClassification</code> (CLIP model)</li> <li><strong>convnext</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong> — <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification">ConvNextV2ForImageClassification</a> (ConvNeXTV2 model)</li> <li><strong>cvt</strong> — <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtForImageClassification">CvtForImageClassification</a> (CvT model)</li> <li><strong>data2vec-vision</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionForImageClassification">Data2VecVisionForImageClassification</a> (Data2VecVision model)</li> <li><strong>deit</strong> — <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li> <li><strong>dinat</strong> — <a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatForImageClassification">DinatForImageClassification</a> (DiNAT model)</li> <li><strong>dinov2</strong> — <code>Dinov2ForImageClassification</code> (DINOv2 model)</li> <li><strong>efficientformer</strong> — <code>EfficientFormerForImageClassification</code> or <code>EfficientFormerForImageClassificationWithTeacher</code> (EfficientFormer model)</li> <li><strong>efficientnet</strong> — <code>EfficientNetForImageClassification</code> (EfficientNet model)</li> <li><strong>focalnet</strong> — <code>FocalNetForImageClassification</code> (FocalNet model)</li> <li><strong>imagegpt</strong> — <code>ImageGPTForImageClassification</code> (ImageGPT model)</li> <li><strong>levit</strong> — <code>LevitForImageClassification</code> or <code>LevitForImageClassificationWithTeacher</code> (LeViT model)</li> <li><strong>mobilenet_v1</strong> — <code>MobileNetV1ForImageClassification</code> (MobileNetV1 model)</li> <li><strong>mobilenet_v2</strong> — <code>MobileNetV2ForImageClassification</code> (MobileNetV2 model)</li> <li><strong>mobilevit</strong> — <code>MobileViTForImageClassification</code> (MobileViT model)</li> <li><strong>mobilevitv2</strong> — <code>MobileViTV2ForImageClassification</code> (MobileViTV2 model)</li> <li><strong>nat</strong> — <code>NatForImageClassification</code> (NAT model)</li> <li><strong>perceiver</strong> — <code>PerceiverForImageClassificationLearned</code> or <code>PerceiverForImageClassificationFourier</code> or <code>PerceiverForImageClassificationConvProcessing</code> (Perceiver model)</li> <li><strong>poolformer</strong> — <code>PoolFormerForImageClassification</code> (PoolFormer model)</li> <li><strong>pvt</strong> — <code>PvtForImageClassification</code> (PVT model)</li> <li><strong>regnet</strong> — <code>RegNetForImageClassification</code> (RegNet model)</li> <li><strong>resnet</strong> — <code>ResNetForImageClassification</code> (ResNet model)</li> <li><strong>segformer</strong> — <code>SegformerForImageClassification</code> (SegFormer model)</li> <li><strong>siglip</strong> — <code>SiglipForImageClassification</code> (SigLIP model)</li> <li><strong>swiftformer</strong> — <code>SwiftFormerForImageClassification</code> (SwiftFormer model)</li> <li><strong>swin</strong> — <code>SwinForImageClassification</code> (Swin Transformer model)</li> <li><strong>swinv2</strong> — <code>Swinv2ForImageClassification</code> (Swin Transformer V2 model)</li> <li><strong>van</strong> — <code>VanForImageClassification</code> (VAN model)</li> <li><strong>vit</strong> — <code>ViTForImageClassification</code> (ViT model)</li> <li><strong>vit_hybrid</strong> — <code>ViTHybridForImageClassification</code> (ViT Hybrid model)</li> <li><strong>vit_msn</strong> — <code>ViTMSNForImageClassification</code> (ViTMSN model)</li>',uA,dT,q9=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,hA,qi,_w,Gf,Mw,Qe,Ef,bA,cT,Q9=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,_A,mT,H9="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",MA,ur,Pf,TA,fT,D9="Instantiates one of the model classes of the library (with a image classification head) from a configuration.",yA,gT,Y9=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,FA,Qi,vA,ct,Sf,CA,pT,z9="Instantiate one of the model classes of the library (with a image classification head) from a pretrained model.",wA,uT,O9=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,jA,hT,K9='<li><strong>convnext</strong> — <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNeXT model)</li> <li><strong>convnextv2</strong> — <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.TFConvNextV2ForImageClassification">TFConvNextV2ForImageClassification</a> (ConvNeXTV2 model)</li> <li><strong>cvt</strong> — <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.TFCvtForImageClassification">TFCvtForImageClassification</a> (CvT model)</li> <li><strong>data2vec-vision</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.TFData2VecVisionForImageClassification">TFData2VecVisionForImageClassification</a> (Data2VecVision model)</li> <li><strong>deit</strong> — <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTForImageClassification">TFDeiTForImageClassification</a> or <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTForImageClassificationWithTeacher">TFDeiTForImageClassificationWithTeacher</a> (DeiT model)</li> <li><strong>efficientformer</strong> — <code>TFEfficientFormerForImageClassification</code> or <code>TFEfficientFormerForImageClassificationWithTeacher</code> (EfficientFormer model)</li> <li><strong>mobilevit</strong> — <code>TFMobileViTForImageClassification</code> (MobileViT model)</li> <li><strong>regnet</strong> — <code>TFRegNetForImageClassification</code> (RegNet model)</li> <li><strong>resnet</strong> — <code>TFResNetForImageClassification</code> (ResNet model)</li> <li><strong>segformer</strong> — <code>TFSegformerForImageClassification</code> (SegFormer model)</li> <li><strong>swin</strong> — <code>TFSwinForImageClassification</code> (Swin Transformer model)</li> <li><strong>vit</strong> — <code>TFViTForImageClassification</code> (ViT model)</li>',xA,Hi,Tw,Uf,yw,He,If,$A,bT,eI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,kA,_T,oI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",ZA,hr,Nf,LA,MT,tI="Instantiates one of the model classes of the library (with a image classification head) from a configuration.",BA,TT,nI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,AA,Di,RA,mt,Xf,WA,yT,rI="Instantiate one of the model classes of the library (with a image classification head) from a pretrained model.",JA,FT,aI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,VA,vT,sI='<li><strong>beit</strong> — <a href="/docs/transformers/main/ja/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li> <li><strong>regnet</strong> — <code>FlaxRegNetForImageClassification</code> (RegNet model)</li> <li><strong>resnet</strong> — <code>FlaxResNetForImageClassification</code> (ResNet model)</li> <li><strong>vit</strong> — <code>FlaxViTForImageClassification</code> (ViT model)</li>',GA,Yi,Fw,qf,vw,De,Qf,EA,CT,iI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a video classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,PA,wT,lI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",SA,br,Hf,UA,jT,dI="Instantiates one of the model classes of the library (with a video classification head) from a configuration.",IA,xT,cI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,NA,zi,XA,q,Df,qA,$T,mI="Instantiate one of the model classes of the library (with a video classification head) from a pretrained model.",QA,kT,fI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,HA,ZT,gI="<li><strong>timesformer</strong> — <code>TimesformerForVideoClassification</code> (TimeSformer model)</li> <li><strong>videomae</strong> — <code>VideoMAEForVideoClassification</code> (VideoMAE model)</li> <li><strong>vivit</strong> — <code>VivitForVideoClassification</code> (ViViT model)</li>",DA,LT,pI=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,YA,Oi,Cw,Yf,ww,Ye,zf,zA,BT,uI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,OA,AT,hI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",KA,_r,Of,eR,RT,bI="Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration.",oR,WT,_I=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,tR,Ki,nR,Q,Kf,rR,JT,MI="Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model.",aR,VT,TI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,sR,GT,yI='<li><strong>deit</strong> — <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li> <li><strong>focalnet</strong> — <code>FocalNetForMaskedImageModeling</code> (FocalNet model)</li> <li><strong>swin</strong> — <code>SwinForMaskedImageModeling</code> (Swin Transformer model)</li> <li><strong>swinv2</strong> — <code>Swinv2ForMaskedImageModeling</code> (Swin Transformer V2 model)</li> <li><strong>vit</strong> — <code>ViTForMaskedImageModeling</code> (ViT model)</li>',iR,ET,FI=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,lR,el,jw,eg,xw,ze,og,dR,PT,vI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a masked image modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,cR,ST,CI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",mR,Mr,tg,fR,UT,wI="Instantiates one of the model classes of the library (with a masked image modeling head) from a configuration.",gR,IT,jI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,pR,ol,uR,ft,ng,hR,NT,xI="Instantiate one of the model classes of the library (with a masked image modeling head) from a pretrained model.",bR,XT,$I=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,_R,qT,kI='<li><strong>deit</strong> — <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTForMaskedImageModeling">TFDeiTForMaskedImageModeling</a> (DeiT model)</li> <li><strong>swin</strong> — <code>TFSwinForMaskedImageModeling</code> (Swin Transformer model)</li>',MR,tl,$w,rg,kw,Oe,ag,TR,QT,ZI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a object detection head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,yR,HT,LI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",FR,Tr,sg,vR,DT,BI="Instantiates one of the model classes of the library (with a object detection head) from a configuration.",CR,YT,AI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,wR,nl,jR,H,ig,xR,zT,RI="Instantiate one of the model classes of the library (with a object detection head) from a pretrained model.",$R,OT,WI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,kR,KT,JI='<li><strong>conditional_detr</strong> — <a href="/docs/transformers/main/ja/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection">ConditionalDetrForObjectDetection</a> (Conditional DETR model)</li> <li><strong>deformable_detr</strong> — <a href="/docs/transformers/main/ja/model_doc/deformable_detr#transformers.DeformableDetrForObjectDetection">DeformableDetrForObjectDetection</a> (Deformable DETR model)</li> <li><strong>deta</strong> — <a href="/docs/transformers/main/ja/model_doc/deta#transformers.DetaForObjectDetection">DetaForObjectDetection</a> (DETA model)</li> <li><strong>detr</strong> — <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li> <li><strong>table-transformer</strong> — <code>TableTransformerForObjectDetection</code> (Table Transformer model)</li> <li><strong>yolos</strong> — <code>YolosForObjectDetection</code> (YOLOS model)</li>',ZR,ey,VI=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,LR,rl,Zw,lg,Lw,Ke,dg,BR,oy,GI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a image segmentation head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,AR,ty,EI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",RR,yr,cg,WR,ny,PI="Instantiates one of the model classes of the library (with a image segmentation head) from a configuration.",JR,ry,SI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,VR,al,GR,D,mg,ER,ay,UI="Instantiate one of the model classes of the library (with a image segmentation head) from a pretrained model.",PR,sy,II=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,SR,iy,NI='<li><strong>detr</strong> — <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>',UR,ly,XI=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,IR,sl,Bw,fg,Aw,gg,pg,Rw,ug,Ww,eo,hg,NR,dy,qI=`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,XR,cy,QI="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",qR,Fr,bg,QR,my,HI="Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration.",HR,fy,DI=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,DR,il,YR,Y,_g,zR,gy,YI="Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model.",OR,py,zI=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,KR,uy,OI='<li><strong>beit</strong> — <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li> <li><strong>data2vec-vision</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionForSemanticSegmentation">Data2VecVisionForSemanticSegmentation</a> (Data2VecVision model)</li> <li><strong>dpt</strong> — <code>DPTForSemanticSegmentation</code> (DPT model)</li> <li><strong>mobilenet_v2</strong> — <code>MobileNetV2ForSemanticSegmentation</code> (MobileNetV2 model)</li> <li><strong>mobilevit</strong> — <code>MobileViTForSemanticSegmentation</code> (MobileViT model)</li> <li><strong>mobilevitv2</strong> — <code>MobileViTV2ForSemanticSegmentation</code> (MobileViTV2 model)</li> <li><strong>segformer</strong> — <code>SegformerForSemanticSegmentation</code> (SegFormer model)</li> <li><strong>upernet</strong> — <code>UperNetForSemanticSegmentation</code> (UPerNet model)</li>',eW,hy,KI=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,oW,ll,Jw,Mg,Vw,oo,Tg,tW,by,e5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a semantic segmentation head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,nW,_y,o5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",rW,vr,yg,aW,My,t5="Instantiates one of the model classes of the library (with a semantic segmentation head) from a configuration.",sW,Ty,n5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,iW,dl,lW,gt,Fg,dW,yy,r5="Instantiate one of the model classes of the library (with a semantic segmentation head) from a pretrained model.",cW,Fy,a5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,mW,vy,s5='<li><strong>data2vec-vision</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.TFData2VecVisionForSemanticSegmentation">TFData2VecVisionForSemanticSegmentation</a> (Data2VecVision model)</li> <li><strong>mobilevit</strong> — <code>TFMobileViTForSemanticSegmentation</code> (MobileViT model)</li> <li><strong>segformer</strong> — <code>TFSegformerForSemanticSegmentation</code> (SegFormer model)</li>',fW,cl,Gw,vg,Ew,to,Cg,gW,Cy,i5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a instance segmentation head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,pW,wy,l5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",uW,Cr,wg,hW,jy,d5="Instantiates one of the model classes of the library (with a instance segmentation head) from a configuration.",bW,xy,c5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,_W,ml,MW,z,jg,TW,$y,m5="Instantiate one of the model classes of the library (with a instance segmentation head) from a pretrained model.",yW,ky,f5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,FW,Zy,g5="<li><strong>maskformer</strong> — <code>MaskFormerForInstanceSegmentation</code> (MaskFormer model)</li>",vW,Ly,p5=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,CW,fl,Pw,xg,Sw,no,$g,wW,By,u5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a universal image segmentation head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,jW,Ay,h5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",xW,wr,kg,$W,Ry,b5="Instantiates one of the model classes of the library (with a universal image segmentation head) from a configuration.",kW,Wy,_5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,ZW,gl,LW,O,Zg,BW,Jy,M5="Instantiate one of the model classes of the library (with a universal image segmentation head) from a pretrained model.",AW,Vy,T5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,RW,Gy,y5='<li><strong>detr</strong> — <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li> <li><strong>mask2former</strong> — <code>Mask2FormerForUniversalSegmentation</code> (Mask2Former model)</li> <li><strong>maskformer</strong> — <code>MaskFormerForInstanceSegmentation</code> (MaskFormer model)</li> <li><strong>oneformer</strong> — <code>OneFormerForUniversalSegmentation</code> (OneFormer model)</li>',WW,Ey,F5=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,JW,pl,Uw,Lg,Iw,ro,Bg,VW,Py,v5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a zero-shot image classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,GW,Sy,C5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",EW,jr,Ag,PW,Uy,w5="Instantiates one of the model classes of the library (with a zero-shot image classification head) from a configuration.",SW,Iy,j5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,UW,ul,IW,K,Rg,NW,Ny,x5="Instantiate one of the model classes of the library (with a zero-shot image classification head) from a pretrained model.",XW,Xy,$5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,qW,qy,k5='<li><strong>align</strong> — <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li> <li><strong>altclip</strong> — <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li> <li><strong>blip</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li> <li><strong>chinese_clip</strong> — <a href="/docs/transformers/main/ja/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li> <li><strong>clip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li> <li><strong>clipseg</strong> — <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li> <li><strong>siglip</strong> — <code>SiglipModel</code> (SigLIP model)</li>',QW,Qy,Z5=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,HW,hl,Nw,Wg,Xw,ao,Jg,DW,Hy,L5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a zero-shot image classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,YW,Dy,B5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",zW,xr,Vg,OW,Yy,A5="Instantiates one of the model classes of the library (with a zero-shot image classification head) from a configuration.",KW,zy,R5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,e0,bl,o0,pt,Gg,t0,Oy,W5="Instantiate one of the model classes of the library (with a zero-shot image classification head) from a pretrained model.",n0,Ky,J5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,r0,eF,V5='<li><strong>blip</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.TFBlipModel">TFBlipModel</a> (BLIP model)</li> <li><strong>clip</strong> — <a href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>',a0,_l,qw,Eg,Qw,so,Pg,s0,oF,G5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a zero-shot object detection head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,i0,tF,E5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",l0,$r,Sg,d0,nF,P5="Instantiates one of the model classes of the library (with a zero-shot object detection head) from a configuration.",c0,rF,S5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,m0,Ml,f0,ee,Ug,g0,aF,U5="Instantiate one of the model classes of the library (with a zero-shot object detection head) from a pretrained model.",p0,sF,I5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,u0,iF,N5="<li><strong>owlv2</strong> — <code>Owlv2ForObjectDetection</code> (OWLv2 model)</li> <li><strong>owlvit</strong> — <code>OwlViTForObjectDetection</code> (OWL-ViT model)</li>",h0,lF,X5=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,b0,Tl,Hw,Ig,Dw,Ng,q5="以下の自動クラスは、次の音声タスクに利用可能です。",Yw,Xg,zw,io,qg,_0,dF,Q5=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,M0,cF,H5="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",T0,kr,Qg,y0,mF,D5="Instantiates one of the model classes of the library (with a audio classification head) from a configuration.",F0,fF,Y5=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,v0,yl,C0,oe,Hg,w0,gF,z5="Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model.",j0,pF,O5=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,x0,uF,K5='<li><strong>audio-spectrogram-transformer</strong> — <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification">ASTForAudioClassification</a> (Audio Spectrogram Transformer model)</li> <li><strong>data2vec-audio</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li> <li><strong>hubert</strong> — <code>HubertForSequenceClassification</code> (Hubert model)</li> <li><strong>sew</strong> — <code>SEWForSequenceClassification</code> (SEW model)</li> <li><strong>sew-d</strong> — <code>SEWDForSequenceClassification</code> (SEW-D model)</li> <li><strong>unispeech</strong> — <code>UniSpeechForSequenceClassification</code> (UniSpeech model)</li> <li><strong>unispeech-sat</strong> — <code>UniSpeechSatForSequenceClassification</code> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2ForSequenceClassification</code> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong> — <code>Wav2Vec2BertForSequenceClassification</code> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong> — <code>Wav2Vec2ConformerForSequenceClassification</code> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong> — <code>WavLMForSequenceClassification</code> (WavLM model)</li> <li><strong>whisper</strong> — <code>WhisperForAudioClassification</code> (Whisper model)</li>',$0,hF,eN=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,k0,Fl,Ow,Dg,Kw,lo,Yg,Z0,bF,oN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,L0,_F,tN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",B0,Zr,zg,A0,MF,nN="Instantiates one of the model classes of the library (with a audio classification head) from a configuration.",R0,TF,rN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,W0,vl,J0,ut,Og,V0,yF,aN="Instantiate one of the model classes of the library (with a audio classification head) from a pretrained model.",G0,FF,sN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,E0,vF,iN="<li><strong>wav2vec2</strong> — <code>TFWav2Vec2ForSequenceClassification</code> (Wav2Vec2 model)</li>",P0,Cl,ej,Kg,oj,co,ep,S0,CF,lN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio frame (token) classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,U0,wF,dN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",I0,Lr,op,N0,jF,cN="Instantiates one of the model classes of the library (with a audio frame (token) classification head) from a configuration.",X0,xF,mN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,q0,wl,Q0,te,tp,H0,$F,fN="Instantiate one of the model classes of the library (with a audio frame (token) classification head) from a pretrained model.",D0,kF,gN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,Y0,ZF,pN='<li><strong>data2vec-audio</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li> <li><strong>unispeech-sat</strong> — <code>UniSpeechSatForAudioFrameClassification</code> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2ForAudioFrameClassification</code> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong> — <code>Wav2Vec2BertForAudioFrameClassification</code> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong> — <code>Wav2Vec2ConformerForAudioFrameClassification</code> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong> — <code>WavLMForAudioFrameClassification</code> (WavLM model)</li>',z0,LF,uN=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,O0,jl,tj,np,nj,mo,rp,K0,BF,hN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a connectionist temporal classification head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,eJ,AF,bN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",oJ,Br,ap,tJ,RF,_N="Instantiates one of the model classes of the library (with a connectionist temporal classification head) from a configuration.",nJ,WF,MN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,rJ,xl,aJ,ne,sp,sJ,JF,TN="Instantiate one of the model classes of the library (with a connectionist temporal classification head) from a pretrained model.",iJ,VF,yN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,lJ,GF,FN='<li><strong>data2vec-audio</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li> <li><strong>hubert</strong> — <code>HubertForCTC</code> (Hubert model)</li> <li><strong>mctct</strong> — <code>MCTCTForCTC</code> (M-CTC-T model)</li> <li><strong>sew</strong> — <code>SEWForCTC</code> (SEW model)</li> <li><strong>sew-d</strong> — <code>SEWDForCTC</code> (SEW-D model)</li> <li><strong>unispeech</strong> — <code>UniSpeechForCTC</code> (UniSpeech model)</li> <li><strong>unispeech-sat</strong> — <code>UniSpeechSatForCTC</code> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2ForCTC</code> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong> — <code>Wav2Vec2BertForCTC</code> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong> — <code>Wav2Vec2ConformerForCTC</code> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong> — <code>WavLMForCTC</code> (WavLM model)</li>',dJ,EF,vN=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,cJ,$l,rj,ip,aj,fo,lp,mJ,PF,CN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,fJ,SF,wN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",gJ,Ar,dp,pJ,UF,jN="Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration.",uJ,IF,xN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,hJ,kl,bJ,re,cp,_J,NF,$N="Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model.",MJ,XF,kN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,TJ,qF,ZN="<li><strong>pop2piano</strong> — <code>Pop2PianoForConditionalGeneration</code> (Pop2Piano model)</li> <li><strong>seamless_m4t</strong> — <code>SeamlessM4TForSpeechToText</code> (SeamlessM4T model)</li> <li><strong>seamless_m4t_v2</strong> — <code>SeamlessM4Tv2ForSpeechToText</code> (SeamlessM4Tv2 model)</li> <li><strong>speech-encoder-decoder</strong> — <code>SpeechEncoderDecoderModel</code> (Speech Encoder decoder model)</li> <li><strong>speech_to_text</strong> — <code>Speech2TextForConditionalGeneration</code> (Speech2Text model)</li> <li><strong>speecht5</strong> — <code>SpeechT5ForSpeechToText</code> (SpeechT5 model)</li> <li><strong>whisper</strong> — <code>WhisperForConditionalGeneration</code> (Whisper model)</li>",yJ,QF,LN=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,FJ,Zl,sj,mp,ij,go,fp,vJ,HF,BN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,CJ,DF,AN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",wJ,Rr,gp,jJ,YF,RN="Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration.",xJ,zF,WN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,$J,Ll,kJ,ht,pp,ZJ,OF,JN="Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model.",LJ,KF,VN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,BJ,ev,GN="<li><strong>speech_to_text</strong> — <code>TFSpeech2TextForConditionalGeneration</code> (Speech2Text model)</li> <li><strong>whisper</strong> — <code>TFWhisperForConditionalGeneration</code> (Whisper model)</li>",AJ,Bl,lj,up,dj,po,hp,RJ,ov,EN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,WJ,tv,PN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",JJ,Wr,bp,VJ,nv,SN="Instantiates one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a configuration.",GJ,rv,UN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,EJ,Al,PJ,bt,_p,SJ,av,IN="Instantiate one of the model classes of the library (with a sequence-to-sequence speech-to-text modeling head) from a pretrained model.",UJ,sv,NN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,IJ,iv,XN="<li><strong>speech-encoder-decoder</strong> — <code>FlaxSpeechEncoderDecoderModel</code> (Speech Encoder decoder model)</li> <li><strong>whisper</strong> — <code>FlaxWhisperForConditionalGeneration</code> (Whisper model)</li>",NJ,Rl,cj,Mp,mj,uo,Tp,XJ,lv,qN=`This is a generic model class that will be instantiated as one of the model classes of the library (with a audio retrieval via x-vector head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,qJ,dv,QN="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",QJ,Jr,yp,HJ,cv,HN="Instantiates one of the model classes of the library (with a audio retrieval via x-vector head) from a configuration.",DJ,mv,DN=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,YJ,Wl,zJ,ae,Fp,OJ,fv,YN="Instantiate one of the model classes of the library (with a audio retrieval via x-vector head) from a pretrained model.",KJ,gv,zN=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,eV,pv,ON='<li><strong>data2vec-audio</strong> — <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li> <li><strong>unispeech-sat</strong> — <code>UniSpeechSatForXVector</code> (UniSpeechSat model)</li> <li><strong>wav2vec2</strong> — <code>Wav2Vec2ForXVector</code> (Wav2Vec2 model)</li> <li><strong>wav2vec2-bert</strong> — <code>Wav2Vec2BertForXVector</code> (Wav2Vec2-BERT model)</li> <li><strong>wav2vec2-conformer</strong> — <code>Wav2Vec2ConformerForXVector</code> (Wav2Vec2-Conformer model)</li> <li><strong>wavlm</strong> — <code>WavLMForXVector</code> (WavLM model)</li>',oV,uv,KN=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,tV,Jl,fj,vp,gj,Cp,wp,pj,jp,uj,xp,$p,hj,kp,bj,Zp,e4="以下の自動クラスは、次のマルチモーダルタスクに利用可能です。",_j,Lp,Mj,ho,Bp,nV,hv,o4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,rV,bv,t4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",aV,Vr,Ap,sV,_v,n4="Instantiates one of the model classes of the library (with a table question answering head) from a configuration.",iV,Mv,r4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,lV,Vl,dV,se,Rp,cV,Tv,a4="Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model.",mV,yv,s4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,fV,Fv,i4="<li><strong>tapas</strong> — <code>TapasForQuestionAnswering</code> (TAPAS model)</li>",gV,vv,l4=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,pV,Gl,Tj,Wp,yj,bo,Jp,uV,Cv,d4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a table question answering head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,hV,wv,c4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",bV,Gr,Vp,_V,jv,m4="Instantiates one of the model classes of the library (with a table question answering head) from a configuration.",MV,xv,f4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,TV,El,yV,_t,Gp,FV,$v,g4="Instantiate one of the model classes of the library (with a table question answering head) from a pretrained model.",vV,kv,p4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,CV,Zv,u4="<li><strong>tapas</strong> — <code>TFTapasForQuestionAnswering</code> (TAPAS model)</li>",wV,Pl,Fj,Ep,vj,_o,Pp,jV,Lv,h4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a document question answering head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,xV,Bv,b4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",$V,Er,Sp,kV,Av,_4="Instantiates one of the model classes of the library (with a document question answering head) from a configuration.",ZV,Rv,M4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,LV,Sl,BV,ie,Up,AV,Wv,T4="Instantiate one of the model classes of the library (with a document question answering head) from a pretrained model.",RV,Jv,y4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,WV,Vv,F4="<li><strong>layoutlm</strong> — <code>LayoutLMForQuestionAnswering</code> (LayoutLM model)</li> <li><strong>layoutlmv2</strong> — <code>LayoutLMv2ForQuestionAnswering</code> (LayoutLMv2 model)</li> <li><strong>layoutlmv3</strong> — <code>LayoutLMv3ForQuestionAnswering</code> (LayoutLMv3 model)</li>",JV,Gv,v4=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,VV,Ul,Cj,Ip,wj,Mo,Np,GV,Ev,C4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a document question answering head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,EV,Pv,w4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",PV,Pr,Xp,SV,Sv,j4="Instantiates one of the model classes of the library (with a document question answering head) from a configuration.",UV,Uv,x4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,IV,Il,NV,Mt,qp,XV,Iv,$4="Instantiate one of the model classes of the library (with a document question answering head) from a pretrained model.",qV,Nv,k4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,QV,Xv,Z4="<li><strong>layoutlm</strong> — <code>TFLayoutLMForQuestionAnswering</code> (LayoutLM model)</li> <li><strong>layoutlmv3</strong> — <code>TFLayoutLMv3ForQuestionAnswering</code> (LayoutLMv3 model)</li>",HV,Nl,jj,Qp,xj,To,Hp,DV,qv,L4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a visual question answering head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,YV,Qv,B4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",zV,Sr,Dp,OV,Hv,A4="Instantiates one of the model classes of the library (with a visual question answering head) from a configuration.",KV,Dv,R4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,eG,Xl,oG,le,Yp,tG,Yv,W4="Instantiate one of the model classes of the library (with a visual question answering head) from a pretrained model.",nG,zv,J4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,rG,Ov,V4='<li><strong>blip-2</strong> — <a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li> <li><strong>vilt</strong> — <code>ViltForQuestionAnswering</code> (ViLT model)</li>',aG,Kv,G4=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,sG,ql,$j,zp,kj,yo,Op,iG,eC,E4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,lG,oC,P4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",dG,Ur,Kp,cG,tC,S4="Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration.",mG,nC,U4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,fG,Ql,gG,de,eu,pG,rC,I4="Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model.",uG,aC,N4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,hG,sC,X4='<li><strong>blip</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipForConditionalGeneration">BlipForConditionalGeneration</a> (BLIP model)</li> <li><strong>blip-2</strong> — <a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li> <li><strong>git</strong> — <code>GitForCausalLM</code> (GIT model)</li> <li><strong>instructblip</strong> — <code>InstructBlipForConditionalGeneration</code> (InstructBLIP model)</li> <li><strong>kosmos-2</strong> — <code>Kosmos2ForConditionalGeneration</code> (KOSMOS-2 model)</li> <li><strong>llava</strong> — <code>LlavaForConditionalGeneration</code> (LLaVa model)</li> <li><strong>pix2struct</strong> — <code>Pix2StructForConditionalGeneration</code> (Pix2Struct model)</li> <li><strong>vipllava</strong> — <code>VipLlavaForConditionalGeneration</code> (VipLlava model)</li> <li><strong>vision-encoder-decoder</strong> — <code>VisionEncoderDecoderModel</code> (Vision Encoder decoder model)</li>',bG,iC,q4=`The model is set in evaluation mode by default using <code>model.eval()</code> (so for instance, dropout modules are
deactivated). To train the model, you should first set it back in training mode with <code>model.train()</code>`,_G,Hl,Zj,ou,Lj,Fo,tu,MG,lC,Q4=`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,TG,dC,H4="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",yG,Ir,nu,FG,cC,D4="Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration.",vG,mC,Y4=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,CG,Dl,wG,Tt,ru,jG,fC,z4="Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model.",xG,gC,O4=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,$G,pC,K4='<li><strong>blip</strong> — <a href="/docs/transformers/main/ja/model_doc/blip#transformers.TFBlipForConditionalGeneration">TFBlipForConditionalGeneration</a> (BLIP model)</li> <li><strong>vision-encoder-decoder</strong> — <code>TFVisionEncoderDecoderModel</code> (Vision Encoder decoder model)</li>',kG,Yl,Bj,au,Aj,vo,su,ZG,uC,eX=`This is a generic model class that will be instantiated as one of the model classes of the library (with a vision-to-text modeling head) when created
with the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> class method or the <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_config">from_config()</a> class
method.`,LG,hC,oX="This class cannot be instantiated directly using <code>__init__()</code> (throws an error).",BG,Nr,iu,AG,bC,tX="Instantiates one of the model classes of the library (with a vision-to-text modeling head) from a configuration.",RG,_C,nX=`Note:
Loading a model from its configuration file does <strong>not</strong> load the model weights. It only affects the
model’s configuration. Use <a href="/docs/transformers/main/ja/model_doc/auto#transformers.FlaxAutoModelForVision2Seq.from_pretrained">from_pretrained()</a> to load the model weights.`,WG,zl,JG,yt,lu,VG,MC,rX="Instantiate one of the model classes of the library (with a vision-to-text modeling head) from a pretrained model.",GG,TC,aX=`The model class to instantiate is selected based on the <code>model_type</code> property of the config object (either
passed as an argument or loaded from <code>pretrained_model_name_or_path</code> if possible), or when it’s missing, by
falling back to using pattern matching on <code>pretrained_model_name_or_path</code>:`,EG,yC,sX="<li><strong>vision-encoder-decoder</strong> — <code>FlaxVisionEncoderDecoderModel</code> (Vision Encoder decoder model)</li>",PG,Ol,Rj,FC,Wj;return m=new Z({props:{title:"Auto Classes",local:"auto-classes",headingTag:"h1"}}),Cd=new $({props:{code:"bW9kZWwlMjAlM0QlMjBBdXRvTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKQ==",highlighted:'model = AutoModel.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)',wrap:!1}}),xd=new Z({props:{title:"自動クラスの拡張",local:"自動クラスの拡張",headingTag:"h2"}}),kd=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Db25maWclMkMlMjBBdXRvTW9kZWwlMEElMEFBdXRvQ29uZmlnLnJlZ2lzdGVyKCUyMm5ldy1tb2RlbCUyMiUyQyUyME5ld01vZGVsQ29uZmlnKSUwQUF1dG9Nb2RlbC5yZWdpc3RlcihOZXdNb2RlbENvbmZpZyUyQyUyME5ld01vZGVsKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoConfig, AutoModel

AutoConfig.register(<span class="hljs-string">&quot;new-model&quot;</span>, NewModelConfig)
AutoModel.register(NewModelConfig, NewModel)`,wrap:!1}}),vs=new Jj({props:{warning:!0,$$slots:{default:[uX]},$$scope:{ctx:v}}}),Ld=new Z({props:{title:"AutoConfig",local:"transformers.AutoConfig",headingTag:"h2"}}),Bd=new j({props:{name:"class transformers.AutoConfig",anchor:"transformers.AutoConfig",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/configuration_auto.py#L981"}}),Ad=new j({props:{name:"from_pretrained",anchor:"transformers.AutoConfig.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoConfig.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model configuration hosted inside a model repo on
huggingface.co.</li>
<li>A path to a <em>directory</em> containing a configuration file saved using the
<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.save_pretrained">save_pretrained()</a> method, or the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> method,
e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a saved configuration JSON <em>file</em>, e.g.,
<code>./my_model_directory/configuration.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoConfig.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoConfig.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoConfig.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoConfig.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoConfig.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoConfig.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final configuration object.</p>
<p>If <code>True</code>, then this functions returns a <code>Tuple(config, unused_kwargs)</code> where <em>unused_kwargs</em> is a
dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the
part of <code>kwargs</code> which has not been used to update <code>config</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoConfig.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoConfig.from_pretrained.kwargs(additional",description:`<strong>kwargs(additional</strong> keyword arguments, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are configuration attributes will be used to override the loaded
values. Behavior concerning key/value pairs whose keys are <em>not</em> configuration attributes is controlled
by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs(additional"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/configuration_auto.py#L1004"}}),Cs=new k({props:{anchor:"transformers.AutoConfig.from_pretrained.example",$$slots:{default:[hX]},$$scope:{ctx:v}}}),Rd=new j({props:{name:"register",anchor:"transformers.AutoConfig.register",parameters:[{name:"model_type",val:""},{name:"config",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoConfig.register.model_type",description:"<strong>model_type</strong> (<code>str</code>) &#x2014; The model type like &#x201C;bert&#x201D; or &#x201C;gpt&#x201D;.",name:"model_type"},{anchor:"transformers.AutoConfig.register.config",description:'<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014; The config to register.',name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/configuration_auto.py#L1143"}}),Wd=new Z({props:{title:"AutoTokenizer",local:"transformers.AutoTokenizer",headingTag:"h2"}}),Jd=new j({props:{name:"class transformers.AutoTokenizer",anchor:"transformers.AutoTokenizer",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/tokenization_auto.py#L620"}}),Vd=new j({props:{name:"from_pretrained",anchor:"transformers.AutoTokenizer.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoTokenizer.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/main/ja/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a single saved vocabulary file if and only if the tokenizer only requires a
single vocabulary file (like Bert or XLNet), e.g.: <code>./my_model_directory/vocab.txt</code>. (Not
applicable to all derived classes)</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoTokenizer.from_pretrained.inputs",description:`<strong>inputs</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the Tokenizer <code>__init__()</code> method.`,name:"inputs"},{anchor:"transformers.AutoTokenizer.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
The configuration object used to determine the tokenizer class to instantiate.`,name:"config"},{anchor:"transformers.AutoTokenizer.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoTokenizer.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download the model weights and configuration files and override the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoTokenizer.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoTokenizer.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoTokenizer.from_pretrained.subfolder",description:`<strong>subfolder</strong> (<code>str</code>, <em>optional</em>) &#x2014;
In case the relevant files are located inside a subfolder of the model repo on huggingface.co (e.g. for
facebook/rag-token-base), specify it here.`,name:"subfolder"},{anchor:"transformers.AutoTokenizer.from_pretrained.use_fast",description:`<strong>use_fast</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Use a <a href="https://huggingface.co/docs/tokenizers/index" rel="nofollow">fast Rust-based tokenizer</a> if it is supported for
a given model. If a fast tokenizer is not available for a given model, a normal Python-based tokenizer
is returned instead.`,name:"use_fast"},{anchor:"transformers.AutoTokenizer.from_pretrained.tokenizer_type",description:`<strong>tokenizer_type</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Tokenizer type to be loaded.`,name:"tokenizer_type"},{anchor:"transformers.AutoTokenizer.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoTokenizer.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Will be passed to the Tokenizer <code>__init__()</code> method. Can be used to set special tokens like
<code>bos_token</code>, <code>eos_token</code>, <code>unk_token</code>, <code>sep_token</code>, <code>pad_token</code>, <code>cls_token</code>, <code>mask_token</code>,
<code>additional_special_tokens</code>. See parameters in the <code>__init__()</code> for more details.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/tokenization_auto.py#L634"}}),js=new k({props:{anchor:"transformers.AutoTokenizer.from_pretrained.example",$$slots:{default:[bX]},$$scope:{ctx:v}}}),Gd=new j({props:{name:"register",anchor:"transformers.AutoTokenizer.register",parameters:[{name:"config_class",val:""},{name:"slow_tokenizer_class",val:" = None"},{name:"fast_tokenizer_class",val:" = None"},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoTokenizer.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoTokenizer.register.slow_tokenizer_class",description:`<strong>slow_tokenizer_class</strong> (<code>PretrainedTokenizer</code>, <em>optional</em>) &#x2014;
The slow tokenizer to register.`,name:"slow_tokenizer_class"},{anchor:"transformers.AutoTokenizer.register.fast_tokenizer_class",description:`<strong>fast_tokenizer_class</strong> (<code>PretrainedTokenizerFast</code>, <em>optional</em>) &#x2014;
The fast tokenizer to register.`,name:"fast_tokenizer_class"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/tokenization_auto.py#L851"}}),Ed=new Z({props:{title:"AutoFeatureExtractor",local:"transformers.AutoFeatureExtractor",headingTag:"h2"}}),Pd=new j({props:{name:"class transformers.AutoFeatureExtractor",anchor:"transformers.AutoFeatureExtractor",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/feature_extraction_auto.py#L238"}}),Sd=new j({props:{name:"from_pretrained",anchor:"transformers.AutoFeatureExtractor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a feature extractor file saved using the
<a href="/docs/transformers/main/ja/main_classes/feature_extractor#transformers.FeatureExtractionMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved feature extractor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoFeatureExtractor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/feature_extraction_auto.py#L252"}}),$s=new Jj({props:{$$slots:{default:[_X]},$$scope:{ctx:v}}}),ks=new k({props:{anchor:"transformers.AutoFeatureExtractor.from_pretrained.example",$$slots:{default:[MX]},$$scope:{ctx:v}}}),Ud=new j({props:{name:"register",anchor:"transformers.AutoFeatureExtractor.register",parameters:[{name:"config_class",val:""},{name:"feature_extractor_class",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoFeatureExtractor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoFeatureExtractor.register.feature_extractor_class",description:"<strong>feature_extractor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The feature extractor to register.",name:"feature_extractor_class"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/feature_extraction_auto.py#L386"}}),Id=new Z({props:{title:"AutoImageProcessor",local:"transformers.AutoImageProcessor",headingTag:"h2"}}),Nd=new j({props:{name:"class transformers.AutoImageProcessor",anchor:"transformers.AutoImageProcessor",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/image_processing_auto.py#L251"}}),Xd=new j({props:{name:"from_pretrained",anchor:"transformers.AutoImageProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoImageProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained image_processor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a image processor file saved using the
<a href="/docs/transformers/main/ja/main_classes/image_processor#transformers.ImageProcessingMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved image processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoImageProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model image processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoImageProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the image processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.AutoImageProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoImageProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoImageProcessor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.AutoImageProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoImageProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final image processor object. If <code>True</code>, then this
functions returns a <code>Tuple(image_processor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not image processor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>image_processor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoImageProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoImageProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are image processor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> image processor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/image_processing_auto.py#L265"}}),Ls=new Jj({props:{$$slots:{default:[TX]},$$scope:{ctx:v}}}),Bs=new k({props:{anchor:"transformers.AutoImageProcessor.from_pretrained.example",$$slots:{default:[yX]},$$scope:{ctx:v}}}),qd=new j({props:{name:"register",anchor:"transformers.AutoImageProcessor.register",parameters:[{name:"config_class",val:""},{name:"image_processor_class",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoImageProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoImageProcessor.register.image_processor_class",description:'<strong>image_processor_class</strong> (<a href="/docs/transformers/main/ja/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a>) &#x2014; The image processor to register.',name:"image_processor_class"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/image_processing_auto.py#L421"}}),Qd=new Z({props:{title:"AutoProcessor",local:"transformers.AutoProcessor",headingTag:"h2"}}),Hd=new j({props:{name:"class transformers.AutoProcessor",anchor:"transformers.AutoProcessor",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/processing_auto.py#L129"}}),Dd=new j({props:{name:"from_pretrained",anchor:"transformers.AutoProcessor.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained feature_extractor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a processor files saved using the <code>save_pretrained()</code> method,
e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoProcessor.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model feature extractor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoProcessor.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the feature extractor files and override the cached versions
if they exist.`,name:"force_download"},{anchor:"transformers.AutoProcessor.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.AutoProcessor.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoProcessor.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <em>bool</em>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, will use the token generated
when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.AutoProcessor.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoProcessor.from_pretrained.return_unused_kwargs",description:`<strong>return_unused_kwargs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
If <code>False</code>, then this function returns just the final feature extractor object. If <code>True</code>, then this
functions returns a <code>Tuple(feature_extractor, unused_kwargs)</code> where <em>unused_kwargs</em> is a dictionary
consisting of the key/value pairs whose keys are not feature extractor attributes: i.e., the part of
<code>kwargs</code> which has not been used to update <code>feature_extractor</code> and is otherwise ignored.`,name:"return_unused_kwargs"},{anchor:"transformers.AutoProcessor.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoProcessor.from_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The values in kwargs of any keys which are feature extractor attributes will be used to override the
loaded values. Behavior concerning key/value pairs whose keys are <em>not</em> feature extractor attributes is
controlled by the <code>return_unused_kwargs</code> keyword parameter.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/processing_auto.py#L143"}}),Rs=new Jj({props:{$$slots:{default:[FX]},$$scope:{ctx:v}}}),Ws=new k({props:{anchor:"transformers.AutoProcessor.from_pretrained.example",$$slots:{default:[vX]},$$scope:{ctx:v}}}),Yd=new j({props:{name:"register",anchor:"transformers.AutoProcessor.register",parameters:[{name:"config_class",val:""},{name:"processor_class",val:""},{name:"exist_ok",val:" = False"}],parametersDescription:[{anchor:"transformers.AutoProcessor.register.config_class",description:`<strong>config_class</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The configuration corresponding to the model to register.`,name:"config_class"},{anchor:"transformers.AutoProcessor.register.processor_class",description:"<strong>processor_class</strong> (<code>FeatureExtractorMixin</code>) &#x2014; The processor to register.",name:"processor_class"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/processing_auto.py#L346"}}),zd=new Z({props:{title:"Generic model classes",local:"generic-model-classes",headingTag:"h2"}}),Kd=new Z({props:{title:"AutoModel",local:"transformers.AutoModel",headingTag:"h3"}}),ec=new j({props:{name:"class transformers.AutoModel",anchor:"transformers.AutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1314"}}),oc=new j({props:{name:"from_config",anchor:"transformers.AutoModel.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModel.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTModel">ASTModel</a> (Audio Spectrogram Transformer model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertModel">AlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPConfig">AltCLIPConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/autoformer#transformers.AutoformerConfig">AutoformerConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/autoformer#transformers.AutoformerModel">AutoformerModel</a> (Autoformer model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkConfig">BarkConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkModel">BarkModel</a> (Bark model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartModel">BartModel</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitModel">BeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertModel">BertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert-generation#transformers.BertGenerationEncoder">BertGenerationEncoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdModel">BigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusModel">BigBirdPegasusModel</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptModel">BioGptModel</a> (BioGpt model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bit#transformers.BitConfig">BitConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bit#transformers.BitModel">BitModel</a> (BiT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotModel">BlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallModel">BlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2Model">Blip2Model</a> (BLIP-2 model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomModel">BloomModel</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerConfig">BridgeTowerConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bridgetower#transformers.BridgeTowerModel">BridgeTowerModel</a> (BridgeTower model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bros#transformers.BrosConfig">BrosConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bros#transformers.BrosModel">BrosModel</a> (BROS model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionConfig">CLIPVisionConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionModel">CLIPVisionModel</a> (CLIPVisionModel model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLModel">CTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertModel">CamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineModel">CanineModel</a> (CANINE model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/chinese_clip#transformers.ChineseCLIPConfig">ChineseCLIPConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clap#transformers.ClapConfig">ClapConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/clap#transformers.ClapModel">ClapModel</a> (CLAP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clvp#transformers.ClvpConfig">ClvpConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/clvp#transformers.ClvpModelForConditionalGeneration">ClvpModelForConditionalGeneration</a> (CLVP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenConfig">CodeGenConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenModel">CodeGenModel</a> (CodeGen model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/conditional_detr#transformers.ConditionalDetrConfig">ConditionalDetrConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/conditional_detr#transformers.ConditionalDetrModel">ConditionalDetrModel</a> (Conditional DETR model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertModel">ConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextModel">ConvNextModel</a> (ConvNeXT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Model">ConvNextV2Model</a> (ConvNeXTV2 model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/cpmant#transformers.CpmAntModel">CpmAntModel</a> (CPM-Ant model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtModel">CvtModel</a> (CvT model)</li>
<li><code>DPRConfig</code> configuration class: <code>DPRQuestionEncoder</code> (DPR model)</li>
<li><code>DPTConfig</code> configuration class: <code>DPTModel</code> (DPT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioModel">Data2VecAudioModel</a> (Data2VecAudio model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextModel">Data2VecTextModel</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionModel">Data2VecVisionModel</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaModel">DebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Model">DebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/decision_transformer#transformers.DecisionTransformerConfig">DecisionTransformerConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/decision_transformer#transformers.DecisionTransformerModel">DecisionTransformerModel</a> (Decision Transformer model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deformable_detr#transformers.DeformableDetrConfig">DeformableDetrConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deformable_detr#transformers.DeformableDetrModel">DeformableDetrModel</a> (Deformable DETR model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTModel">DeiTModel</a> (DeiT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deta#transformers.DetaConfig">DetaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deta#transformers.DetaModel">DetaModel</a> (DETA model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrModel">DetrModel</a> (DETR model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatConfig">DinatConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatModel">DinatModel</a> (DiNAT model)</li>
<li><code>Dinov2Config</code> configuration class: <code>Dinov2Model</code> (DINOv2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>DistilBertModel</code> (DistilBERT model)</li>
<li><code>DonutSwinConfig</code> configuration class: <code>DonutSwinModel</code> (DonutSwin model)</li>
<li><code>EfficientFormerConfig</code> configuration class: <code>EfficientFormerModel</code> (EfficientFormer model)</li>
<li><code>EfficientNetConfig</code> configuration class: <code>EfficientNetModel</code> (EfficientNet model)</li>
<li><code>ElectraConfig</code> configuration class: <code>ElectraModel</code> (ELECTRA model)</li>
<li><code>EncodecConfig</code> configuration class: <code>EncodecModel</code> (EnCodec model)</li>
<li><code>ErnieConfig</code> configuration class: <code>ErnieModel</code> (ERNIE model)</li>
<li><code>ErnieMConfig</code> configuration class: <code>ErnieMModel</code> (ErnieM model)</li>
<li><code>EsmConfig</code> configuration class: <code>EsmModel</code> (ESM model)</li>
<li><code>FNetConfig</code> configuration class: <code>FNetModel</code> (FNet model)</li>
<li><code>FSMTConfig</code> configuration class: <code>FSMTModel</code> (FairSeq Machine-Translation model)</li>
<li><code>FalconConfig</code> configuration class: <code>FalconModel</code> (Falcon model)</li>
<li><code>FastSpeech2ConformerConfig</code> configuration class: <code>FastSpeech2ConformerModel</code> (FastSpeech2Conformer model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>FlaubertModel</code> (FlauBERT model)</li>
<li><code>FlavaConfig</code> configuration class: <code>FlavaModel</code> (FLAVA model)</li>
<li><code>FocalNetConfig</code> configuration class: <code>FocalNetModel</code> (FocalNet model)</li>
<li><code>FunnelConfig</code> configuration class: <code>FunnelModel</code> or <code>FunnelBaseModel</code> (Funnel Transformer model)</li>
<li><code>GLPNConfig</code> configuration class: <code>GLPNModel</code> (GLPN model)</li>
<li><code>GPT2Config</code> configuration class: <code>GPT2Model</code> (OpenAI GPT-2 model)</li>
<li><code>GPTBigCodeConfig</code> configuration class: <code>GPTBigCodeModel</code> (GPTBigCode model)</li>
<li><code>GPTJConfig</code> configuration class: <code>GPTJModel</code> (GPT-J model)</li>
<li><code>GPTNeoConfig</code> configuration class: <code>GPTNeoModel</code> (GPT Neo model)</li>
<li><code>GPTNeoXConfig</code> configuration class: <code>GPTNeoXModel</code> (GPT NeoX model)</li>
<li><code>GPTNeoXJapaneseConfig</code> configuration class: <code>GPTNeoXJapaneseModel</code> (GPT NeoX Japanese model)</li>
<li><code>GPTSanJapaneseConfig</code> configuration class: <code>GPTSanJapaneseForConditionalGeneration</code> (GPTSAN-japanese model)</li>
<li><code>GitConfig</code> configuration class: <code>GitModel</code> (GIT model)</li>
<li><code>GraphormerConfig</code> configuration class: <code>GraphormerModel</code> (Graphormer model)</li>
<li><code>GroupViTConfig</code> configuration class: <code>GroupViTModel</code> (GroupViT model)</li>
<li><code>HubertConfig</code> configuration class: <code>HubertModel</code> (Hubert model)</li>
<li><code>IBertConfig</code> configuration class: <code>IBertModel</code> (I-BERT model)</li>
<li><code>IdeficsConfig</code> configuration class: <code>IdeficsModel</code> (IDEFICS model)</li>
<li><code>ImageGPTConfig</code> configuration class: <code>ImageGPTModel</code> (ImageGPT model)</li>
<li><code>InformerConfig</code> configuration class: <code>InformerModel</code> (Informer model)</li>
<li><code>JukeboxConfig</code> configuration class: <code>JukeboxModel</code> (Jukebox model)</li>
<li><code>Kosmos2Config</code> configuration class: <code>Kosmos2Model</code> (KOSMOS-2 model)</li>
<li><code>LEDConfig</code> configuration class: <code>LEDModel</code> (LED model)</li>
<li><code>LayoutLMConfig</code> configuration class: <code>LayoutLMModel</code> (LayoutLM model)</li>
<li><code>LayoutLMv2Config</code> configuration class: <code>LayoutLMv2Model</code> (LayoutLMv2 model)</li>
<li><code>LayoutLMv3Config</code> configuration class: <code>LayoutLMv3Model</code> (LayoutLMv3 model)</li>
<li><code>LevitConfig</code> configuration class: <code>LevitModel</code> (LeViT model)</li>
<li><code>LiltConfig</code> configuration class: <code>LiltModel</code> (LiLT model)</li>
<li><code>LlamaConfig</code> configuration class: <code>LlamaModel</code> (LLaMA model)</li>
<li><code>LongT5Config</code> configuration class: <code>LongT5Model</code> (LongT5 model)</li>
<li><code>LongformerConfig</code> configuration class: <code>LongformerModel</code> (Longformer model)</li>
<li><code>LukeConfig</code> configuration class: <code>LukeModel</code> (LUKE model)</li>
<li><code>LxmertConfig</code> configuration class: <code>LxmertModel</code> (LXMERT model)</li>
<li><code>M2M100Config</code> configuration class: <code>M2M100Model</code> (M2M100 model)</li>
<li><code>MBartConfig</code> configuration class: <code>MBartModel</code> (mBART model)</li>
<li><code>MCTCTConfig</code> configuration class: <code>MCTCTModel</code> (M-CTC-T model)</li>
<li><code>MPNetConfig</code> configuration class: <code>MPNetModel</code> (MPNet model)</li>
<li><code>MT5Config</code> configuration class: <code>MT5Model</code> (MT5 model)</li>
<li><code>MarianConfig</code> configuration class: <code>MarianModel</code> (Marian model)</li>
<li><code>MarkupLMConfig</code> configuration class: <code>MarkupLMModel</code> (MarkupLM model)</li>
<li><code>Mask2FormerConfig</code> configuration class: <code>Mask2FormerModel</code> (Mask2Former model)</li>
<li><code>MaskFormerConfig</code> configuration class: <code>MaskFormerModel</code> (MaskFormer model)</li>
<li><code>MaskFormerSwinConfig</code> configuration class: <code>MaskFormerSwinModel</code> (MaskFormerSwin model)</li>
<li><code>MegaConfig</code> configuration class: <code>MegaModel</code> (MEGA model)</li>
<li><code>MegatronBertConfig</code> configuration class: <code>MegatronBertModel</code> (Megatron-BERT model)</li>
<li><code>MgpstrConfig</code> configuration class: <code>MgpstrForSceneTextRecognition</code> (MGP-STR model)</li>
<li><code>MistralConfig</code> configuration class: <code>MistralModel</code> (Mistral model)</li>
<li><code>MixtralConfig</code> configuration class: <code>MixtralModel</code> (Mixtral model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>MobileBertModel</code> (MobileBERT model)</li>
<li><code>MobileNetV1Config</code> configuration class: <code>MobileNetV1Model</code> (MobileNetV1 model)</li>
<li><code>MobileNetV2Config</code> configuration class: <code>MobileNetV2Model</code> (MobileNetV2 model)</li>
<li><code>MobileViTConfig</code> configuration class: <code>MobileViTModel</code> (MobileViT model)</li>
<li><code>MobileViTV2Config</code> configuration class: <code>MobileViTV2Model</code> (MobileViTV2 model)</li>
<li><code>MptConfig</code> configuration class: <code>MptModel</code> (MPT model)</li>
<li><code>MraConfig</code> configuration class: <code>MraModel</code> (MRA model)</li>
<li><code>MvpConfig</code> configuration class: <code>MvpModel</code> (MVP model)</li>
<li><code>NatConfig</code> configuration class: <code>NatModel</code> (NAT model)</li>
<li><code>NezhaConfig</code> configuration class: <code>NezhaModel</code> (Nezha model)</li>
<li><code>NllbMoeConfig</code> configuration class: <code>NllbMoeModel</code> (NLLB-MOE model)</li>
<li><code>NystromformerConfig</code> configuration class: <code>NystromformerModel</code> (Nystr&#xF6;mformer model)</li>
<li><code>OPTConfig</code> configuration class: <code>OPTModel</code> (OPT model)</li>
<li><code>OneFormerConfig</code> configuration class: <code>OneFormerModel</code> (OneFormer model)</li>
<li><code>OpenAIGPTConfig</code> configuration class: <code>OpenAIGPTModel</code> (OpenAI GPT model)</li>
<li><code>OpenLlamaConfig</code> configuration class: <code>OpenLlamaModel</code> (OpenLlama model)</li>
<li><code>OwlViTConfig</code> configuration class: <code>OwlViTModel</code> (OWL-ViT model)</li>
<li><code>Owlv2Config</code> configuration class: <code>Owlv2Model</code> (OWLv2 model)</li>
<li><code>PLBartConfig</code> configuration class: <code>PLBartModel</code> (PLBart model)</li>
<li><code>PatchTSMixerConfig</code> configuration class: <code>PatchTSMixerModel</code> (PatchTSMixer model)</li>
<li><code>PatchTSTConfig</code> configuration class: <code>PatchTSTModel</code> (PatchTST model)</li>
<li><code>PegasusConfig</code> configuration class: <code>PegasusModel</code> (Pegasus model)</li>
<li><code>PegasusXConfig</code> configuration class: <code>PegasusXModel</code> (PEGASUS-X model)</li>
<li><code>PerceiverConfig</code> configuration class: <code>PerceiverModel</code> (Perceiver model)</li>
<li><code>PersimmonConfig</code> configuration class: <code>PersimmonModel</code> (Persimmon model)</li>
<li><code>PhiConfig</code> configuration class: <code>PhiModel</code> (Phi model)</li>
<li><code>PoolFormerConfig</code> configuration class: <code>PoolFormerModel</code> (PoolFormer model)</li>
<li><code>ProphetNetConfig</code> configuration class: <code>ProphetNetModel</code> (ProphetNet model)</li>
<li><code>PvtConfig</code> configuration class: <code>PvtModel</code> (PVT model)</li>
<li><code>QDQBertConfig</code> configuration class: <code>QDQBertModel</code> (QDQBert model)</li>
<li><code>Qwen2Config</code> configuration class: <code>Qwen2Model</code> (Qwen2 model)</li>
<li><code>ReformerConfig</code> configuration class: <code>ReformerModel</code> (Reformer model)</li>
<li><code>RegNetConfig</code> configuration class: <code>RegNetModel</code> (RegNet model)</li>
<li><code>RemBertConfig</code> configuration class: <code>RemBertModel</code> (RemBERT model)</li>
<li><code>ResNetConfig</code> configuration class: <code>ResNetModel</code> (ResNet model)</li>
<li><code>RetriBertConfig</code> configuration class: <code>RetriBertModel</code> (RetriBERT model)</li>
<li><code>RoCBertConfig</code> configuration class: <code>RoCBertModel</code> (RoCBert model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>RoFormerModel</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>RobertaModel</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>RobertaPreLayerNormModel</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>RwkvConfig</code> configuration class: <code>RwkvModel</code> (RWKV model)</li>
<li><code>SEWConfig</code> configuration class: <code>SEWModel</code> (SEW model)</li>
<li><code>SEWDConfig</code> configuration class: <code>SEWDModel</code> (SEW-D model)</li>
<li><code>SamConfig</code> configuration class: <code>SamModel</code> (SAM model)</li>
<li><code>SeamlessM4TConfig</code> configuration class: <code>SeamlessM4TModel</code> (SeamlessM4T model)</li>
<li><code>SeamlessM4Tv2Config</code> configuration class: <code>SeamlessM4Tv2Model</code> (SeamlessM4Tv2 model)</li>
<li><code>SegformerConfig</code> configuration class: <code>SegformerModel</code> (SegFormer model)</li>
<li><code>SiglipConfig</code> configuration class: <code>SiglipModel</code> (SigLIP model)</li>
<li><code>SiglipVisionConfig</code> configuration class: <code>SiglipVisionModel</code> (SiglipVisionModel model)</li>
<li><code>Speech2TextConfig</code> configuration class: <code>Speech2TextModel</code> (Speech2Text model)</li>
<li><code>SpeechT5Config</code> configuration class: <code>SpeechT5Model</code> (SpeechT5 model)</li>
<li><code>SplinterConfig</code> configuration class: <code>SplinterModel</code> (Splinter model)</li>
<li><code>SqueezeBertConfig</code> configuration class: <code>SqueezeBertModel</code> (SqueezeBERT model)</li>
<li><code>StableLmConfig</code> configuration class: <code>StableLmModel</code> (StableLm model)</li>
<li><code>SwiftFormerConfig</code> configuration class: <code>SwiftFormerModel</code> (SwiftFormer model)</li>
<li><code>Swin2SRConfig</code> configuration class: <code>Swin2SRModel</code> (Swin2SR model)</li>
<li><code>SwinConfig</code> configuration class: <code>SwinModel</code> (Swin Transformer model)</li>
<li><code>Swinv2Config</code> configuration class: <code>Swinv2Model</code> (Swin Transformer V2 model)</li>
<li><code>SwitchTransformersConfig</code> configuration class: <code>SwitchTransformersModel</code> (SwitchTransformers model)</li>
<li><code>T5Config</code> configuration class: <code>T5Model</code> (T5 model)</li>
<li><code>TableTransformerConfig</code> configuration class: <code>TableTransformerModel</code> (Table Transformer model)</li>
<li><code>TapasConfig</code> configuration class: <code>TapasModel</code> (TAPAS model)</li>
<li><code>TimeSeriesTransformerConfig</code> configuration class: <code>TimeSeriesTransformerModel</code> (Time Series Transformer model)</li>
<li><code>TimesformerConfig</code> configuration class: <code>TimesformerModel</code> (TimeSformer model)</li>
<li><code>TimmBackboneConfig</code> configuration class: <code>TimmBackbone</code> (TimmBackbone model)</li>
<li><code>TrajectoryTransformerConfig</code> configuration class: <code>TrajectoryTransformerModel</code> (Trajectory Transformer model)</li>
<li><code>TransfoXLConfig</code> configuration class: <code>TransfoXLModel</code> (Transformer-XL model)</li>
<li><code>TvltConfig</code> configuration class: <code>TvltModel</code> (TVLT model)</li>
<li><code>TvpConfig</code> configuration class: <code>TvpModel</code> (TVP model)</li>
<li><code>UMT5Config</code> configuration class: <code>UMT5Model</code> (UMT5 model)</li>
<li><code>UniSpeechConfig</code> configuration class: <code>UniSpeechModel</code> (UniSpeech model)</li>
<li><code>UniSpeechSatConfig</code> configuration class: <code>UniSpeechSatModel</code> (UniSpeechSat model)</li>
<li><code>UnivNetConfig</code> configuration class: <code>UnivNetModel</code> (UnivNet model)</li>
<li><code>VanConfig</code> configuration class: <code>VanModel</code> (VAN model)</li>
<li><code>ViTConfig</code> configuration class: <code>ViTModel</code> (ViT model)</li>
<li><code>ViTHybridConfig</code> configuration class: <code>ViTHybridModel</code> (ViT Hybrid model)</li>
<li><code>ViTMAEConfig</code> configuration class: <code>ViTMAEModel</code> (ViTMAE model)</li>
<li><code>ViTMSNConfig</code> configuration class: <code>ViTMSNModel</code> (ViTMSN model)</li>
<li><code>VideoMAEConfig</code> configuration class: <code>VideoMAEModel</code> (VideoMAE model)</li>
<li><code>ViltConfig</code> configuration class: <code>ViltModel</code> (ViLT model)</li>
<li><code>VisionTextDualEncoderConfig</code> configuration class: <code>VisionTextDualEncoderModel</code> (VisionTextDualEncoder model)</li>
<li><code>VisualBertConfig</code> configuration class: <code>VisualBertModel</code> (VisualBERT model)</li>
<li><code>VitDetConfig</code> configuration class: <code>VitDetModel</code> (VitDet model)</li>
<li><code>VitsConfig</code> configuration class: <code>VitsModel</code> (VITS model)</li>
<li><code>VivitConfig</code> configuration class: <code>VivitModel</code> (ViViT model)</li>
<li><code>Wav2Vec2BertConfig</code> configuration class: <code>Wav2Vec2BertModel</code> (Wav2Vec2-BERT model)</li>
<li><code>Wav2Vec2Config</code> configuration class: <code>Wav2Vec2Model</code> (Wav2Vec2 model)</li>
<li><code>Wav2Vec2ConformerConfig</code> configuration class: <code>Wav2Vec2ConformerModel</code> (Wav2Vec2-Conformer model)</li>
<li><code>WavLMConfig</code> configuration class: <code>WavLMModel</code> (WavLM model)</li>
<li><code>WhisperConfig</code> configuration class: <code>WhisperModel</code> (Whisper model)</li>
<li><code>XCLIPConfig</code> configuration class: <code>XCLIPModel</code> (X-CLIP model)</li>
<li><code>XGLMConfig</code> configuration class: <code>XGLMModel</code> (XGLM model)</li>
<li><code>XLMConfig</code> configuration class: <code>XLMModel</code> (XLM model)</li>
<li><code>XLMProphetNetConfig</code> configuration class: <code>XLMProphetNetModel</code> (XLM-ProphetNet model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>XLMRobertaModel</code> (XLM-RoBERTa model)</li>
<li><code>XLMRobertaXLConfig</code> configuration class: <code>XLMRobertaXLModel</code> (XLM-RoBERTa-XL model)</li>
<li><code>XLNetConfig</code> configuration class: <code>XLNetModel</code> (XLNet model)</li>
<li><code>XmodConfig</code> configuration class: <code>XmodModel</code> (X-MOD model)</li>
<li><code>YolosConfig</code> configuration class: <code>YolosModel</code> (YOLOS model)</li>
<li><code>YosoConfig</code> configuration class: <code>YosoModel</code> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Vs=new k({props:{anchor:"transformers.AutoModel.from_config.example",$$slots:{default:[CX]},$$scope:{ctx:v}}}),tc=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModel.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModel.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModel.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModel.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModel.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModel.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModel.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModel.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModel.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModel.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModel.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModel.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModel.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModel.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModel.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModel.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Gs=new k({props:{anchor:"transformers.AutoModel.from_pretrained.example",$$slots:{default:[wX]},$$scope:{ctx:v}}}),nc=new Z({props:{title:"TFAutoModel",local:"transformers.TFAutoModel",headingTag:"h3"}}),rc=new j({props:{name:"class transformers.TFAutoModel",anchor:"transformers.TFAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L531"}}),ac=new j({props:{name:"from_config",anchor:"transformers.TFAutoModel.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModel.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertModel">TFAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.TFBartModel">TFBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertModel">TFBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.TFBlenderbotModel">TFBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.TFBlenderbotSmallModel">TFBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blip#transformers.TFBlipModel">TFBlipModel</a> (BLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.TFCTRLModel">TFCTRLModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertModel">TFCamembertModel</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertModel">TFConvBertModel</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.TFConvNextModel">TFConvNextModel</a> (ConvNeXT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.TFConvNextV2Model">TFConvNextV2Model</a> (ConvNeXTV2 model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.TFCvtModel">TFCvtModel</a> (CvT model)</li>
<li><code>DPRConfig</code> configuration class: <code>TFDPRQuestionEncoder</code> (DPR model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.TFData2VecVisionModel">TFData2VecVisionModel</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.TFDebertaModel">TFDebertaModel</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2Model">TFDebertaV2Model</a> (DeBERTa-v2 model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTModel">TFDeiTModel</a> (DeiT model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>TFDistilBertModel</code> (DistilBERT model)</li>
<li><code>EfficientFormerConfig</code> configuration class: <code>TFEfficientFormerModel</code> (EfficientFormer model)</li>
<li><code>ElectraConfig</code> configuration class: <code>TFElectraModel</code> (ELECTRA model)</li>
<li><code>EsmConfig</code> configuration class: <code>TFEsmModel</code> (ESM model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>TFFlaubertModel</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>TFFunnelModel</code> or <code>TFFunnelBaseModel</code> (Funnel Transformer model)</li>
<li><code>GPT2Config</code> configuration class: <code>TFGPT2Model</code> (OpenAI GPT-2 model)</li>
<li><code>GPTJConfig</code> configuration class: <code>TFGPTJModel</code> (GPT-J model)</li>
<li><code>GroupViTConfig</code> configuration class: <code>TFGroupViTModel</code> (GroupViT model)</li>
<li><code>HubertConfig</code> configuration class: <code>TFHubertModel</code> (Hubert model)</li>
<li><code>LEDConfig</code> configuration class: <code>TFLEDModel</code> (LED model)</li>
<li><code>LayoutLMConfig</code> configuration class: <code>TFLayoutLMModel</code> (LayoutLM model)</li>
<li><code>LayoutLMv3Config</code> configuration class: <code>TFLayoutLMv3Model</code> (LayoutLMv3 model)</li>
<li><code>LongformerConfig</code> configuration class: <code>TFLongformerModel</code> (Longformer model)</li>
<li><code>LxmertConfig</code> configuration class: <code>TFLxmertModel</code> (LXMERT model)</li>
<li><code>MBartConfig</code> configuration class: <code>TFMBartModel</code> (mBART model)</li>
<li><code>MPNetConfig</code> configuration class: <code>TFMPNetModel</code> (MPNet model)</li>
<li><code>MT5Config</code> configuration class: <code>TFMT5Model</code> (MT5 model)</li>
<li><code>MarianConfig</code> configuration class: <code>TFMarianModel</code> (Marian model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>TFMobileBertModel</code> (MobileBERT model)</li>
<li><code>MobileViTConfig</code> configuration class: <code>TFMobileViTModel</code> (MobileViT model)</li>
<li><code>OPTConfig</code> configuration class: <code>TFOPTModel</code> (OPT model)</li>
<li><code>OpenAIGPTConfig</code> configuration class: <code>TFOpenAIGPTModel</code> (OpenAI GPT model)</li>
<li><code>PegasusConfig</code> configuration class: <code>TFPegasusModel</code> (Pegasus model)</li>
<li><code>RegNetConfig</code> configuration class: <code>TFRegNetModel</code> (RegNet model)</li>
<li><code>RemBertConfig</code> configuration class: <code>TFRemBertModel</code> (RemBERT model)</li>
<li><code>ResNetConfig</code> configuration class: <code>TFResNetModel</code> (ResNet model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>TFRoFormerModel</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>TFRobertaModel</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>TFRobertaPreLayerNormModel</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>SamConfig</code> configuration class: <code>TFSamModel</code> (SAM model)</li>
<li><code>SegformerConfig</code> configuration class: <code>TFSegformerModel</code> (SegFormer model)</li>
<li><code>Speech2TextConfig</code> configuration class: <code>TFSpeech2TextModel</code> (Speech2Text model)</li>
<li><code>SwinConfig</code> configuration class: <code>TFSwinModel</code> (Swin Transformer model)</li>
<li><code>T5Config</code> configuration class: <code>TFT5Model</code> (T5 model)</li>
<li><code>TapasConfig</code> configuration class: <code>TFTapasModel</code> (TAPAS model)</li>
<li><code>TransfoXLConfig</code> configuration class: <code>TFTransfoXLModel</code> (Transformer-XL model)</li>
<li><code>ViTConfig</code> configuration class: <code>TFViTModel</code> (ViT model)</li>
<li><code>ViTMAEConfig</code> configuration class: <code>TFViTMAEModel</code> (ViTMAE model)</li>
<li><code>VisionTextDualEncoderConfig</code> configuration class: <code>TFVisionTextDualEncoderModel</code> (VisionTextDualEncoder model)</li>
<li><code>Wav2Vec2Config</code> configuration class: <code>TFWav2Vec2Model</code> (Wav2Vec2 model)</li>
<li><code>WhisperConfig</code> configuration class: <code>TFWhisperModel</code> (Whisper model)</li>
<li><code>XGLMConfig</code> configuration class: <code>TFXGLMModel</code> (XGLM model)</li>
<li><code>XLMConfig</code> configuration class: <code>TFXLMModel</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>TFXLMRobertaModel</code> (XLM-RoBERTa model)</li>
<li><code>XLNetConfig</code> configuration class: <code>TFXLNetModel</code> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Es=new k({props:{anchor:"transformers.TFAutoModel.from_config.example",$$slots:{default:[jX]},$$scope:{ctx:v}}}),sc=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModel.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModel.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModel.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModel.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModel.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModel.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModel.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModel.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModel.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModel.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModel.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModel.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModel.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModel.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModel.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ps=new k({props:{anchor:"transformers.TFAutoModel.from_pretrained.example",$$slots:{default:[xX]},$$scope:{ctx:v}}}),ic=new Z({props:{title:"FlaxAutoModel",local:"transformers.FlaxAutoModel",headingTag:"h3"}}),lc=new j({props:{name:"class transformers.FlaxAutoModel",anchor:"transformers.FlaxAutoModel",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L278"}}),dc=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModel.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModel.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertModel">FlaxAlbertModel</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartModel">FlaxBartModel</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/beit#transformers.FlaxBeitModel">FlaxBeitModel</a> (BEiT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertModel">FlaxBertModel</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdModel">FlaxBigBirdModel</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.FlaxBlenderbotModel">FlaxBlenderbotModel</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallModel">FlaxBlenderbotSmallModel</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.FlaxBloomModel">FlaxBloomModel</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/clip#transformers.FlaxCLIPModel">FlaxCLIPModel</a> (CLIP model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>FlaxDistilBertModel</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>FlaxElectraModel</code> (ELECTRA model)</li>
<li><code>GPT2Config</code> configuration class: <code>FlaxGPT2Model</code> (OpenAI GPT-2 model)</li>
<li><code>GPTJConfig</code> configuration class: <code>FlaxGPTJModel</code> (GPT-J model)</li>
<li><code>GPTNeoConfig</code> configuration class: <code>FlaxGPTNeoModel</code> (GPT Neo model)</li>
<li><code>LlamaConfig</code> configuration class: <code>FlaxLlamaModel</code> (LLaMA model)</li>
<li><code>LongT5Config</code> configuration class: <code>FlaxLongT5Model</code> (LongT5 model)</li>
<li><code>MBartConfig</code> configuration class: <code>FlaxMBartModel</code> (mBART model)</li>
<li><code>MT5Config</code> configuration class: <code>FlaxMT5Model</code> (MT5 model)</li>
<li><code>MarianConfig</code> configuration class: <code>FlaxMarianModel</code> (Marian model)</li>
<li><code>MistralConfig</code> configuration class: <code>FlaxMistralModel</code> (Mistral model)</li>
<li><code>OPTConfig</code> configuration class: <code>FlaxOPTModel</code> (OPT model)</li>
<li><code>PegasusConfig</code> configuration class: <code>FlaxPegasusModel</code> (Pegasus model)</li>
<li><code>RegNetConfig</code> configuration class: <code>FlaxRegNetModel</code> (RegNet model)</li>
<li><code>ResNetConfig</code> configuration class: <code>FlaxResNetModel</code> (ResNet model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>FlaxRoFormerModel</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>FlaxRobertaModel</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>FlaxRobertaPreLayerNormModel</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>T5Config</code> configuration class: <code>FlaxT5Model</code> (T5 model)</li>
<li><code>ViTConfig</code> configuration class: <code>FlaxViTModel</code> (ViT model)</li>
<li><code>VisionTextDualEncoderConfig</code> configuration class: <code>FlaxVisionTextDualEncoderModel</code> (VisionTextDualEncoder model)</li>
<li><code>Wav2Vec2Config</code> configuration class: <code>FlaxWav2Vec2Model</code> (Wav2Vec2 model)</li>
<li><code>WhisperConfig</code> configuration class: <code>FlaxWhisperModel</code> (Whisper model)</li>
<li><code>XGLMConfig</code> configuration class: <code>FlaxXGLMModel</code> (XGLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>FlaxXLMRobertaModel</code> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ss=new k({props:{anchor:"transformers.FlaxAutoModel.from_config.example",$$slots:{default:[$X]},$$scope:{ctx:v}}}),cc=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModel.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModel.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModel.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModel.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModel.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModel.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModel.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModel.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModel.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModel.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModel.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModel.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModel.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModel.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModel.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Us=new k({props:{anchor:"transformers.FlaxAutoModel.from_pretrained.example",$$slots:{default:[kX]},$$scope:{ctx:v}}}),mc=new Z({props:{title:"Generic pretraining classes",local:"generic-pretraining-classes",headingTag:"h2"}}),gc=new Z({props:{title:"AutoModelForPreTraining",local:"transformers.AutoModelForPreTraining",headingTag:"h3"}}),pc=new j({props:{name:"class transformers.AutoModelForPreTraining",anchor:"transformers.AutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1321"}}),uc=new j({props:{name:"from_config",anchor:"transformers.AutoModelForPreTraining.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForPreTraining.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForPreTraining">AlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForPreTraining">BertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForPreTraining">BigBirdForPreTraining</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>DistilBertForMaskedLM</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>ElectraForPreTraining</code> (ELECTRA model)</li>
<li><code>ErnieConfig</code> configuration class: <code>ErnieForPreTraining</code> (ERNIE model)</li>
<li><code>FNetConfig</code> configuration class: <code>FNetForPreTraining</code> (FNet model)</li>
<li><code>FSMTConfig</code> configuration class: <code>FSMTForConditionalGeneration</code> (FairSeq Machine-Translation model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>FlaubertWithLMHeadModel</code> (FlauBERT model)</li>
<li><code>FlavaConfig</code> configuration class: <code>FlavaForPreTraining</code> (FLAVA model)</li>
<li><code>FunnelConfig</code> configuration class: <code>FunnelForPreTraining</code> (Funnel Transformer model)</li>
<li><code>GPT2Config</code> configuration class: <code>GPT2LMHeadModel</code> (OpenAI GPT-2 model)</li>
<li><code>GPTBigCodeConfig</code> configuration class: <code>GPTBigCodeForCausalLM</code> (GPTBigCode model)</li>
<li><code>GPTSanJapaneseConfig</code> configuration class: <code>GPTSanJapaneseForConditionalGeneration</code> (GPTSAN-japanese model)</li>
<li><code>IBertConfig</code> configuration class: <code>IBertForMaskedLM</code> (I-BERT model)</li>
<li><code>IdeficsConfig</code> configuration class: <code>IdeficsForVisionText2Text</code> (IDEFICS model)</li>
<li><code>LayoutLMConfig</code> configuration class: <code>LayoutLMForMaskedLM</code> (LayoutLM model)</li>
<li><code>LlavaConfig</code> configuration class: <code>LlavaForConditionalGeneration</code> (LLaVa model)</li>
<li><code>LongformerConfig</code> configuration class: <code>LongformerForMaskedLM</code> (Longformer model)</li>
<li><code>LukeConfig</code> configuration class: <code>LukeForMaskedLM</code> (LUKE model)</li>
<li><code>LxmertConfig</code> configuration class: <code>LxmertForPreTraining</code> (LXMERT model)</li>
<li><code>MPNetConfig</code> configuration class: <code>MPNetForMaskedLM</code> (MPNet model)</li>
<li><code>MegaConfig</code> configuration class: <code>MegaForMaskedLM</code> (MEGA model)</li>
<li><code>MegatronBertConfig</code> configuration class: <code>MegatronBertForPreTraining</code> (Megatron-BERT model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>MobileBertForPreTraining</code> (MobileBERT model)</li>
<li><code>MptConfig</code> configuration class: <code>MptForCausalLM</code> (MPT model)</li>
<li><code>MraConfig</code> configuration class: <code>MraForMaskedLM</code> (MRA model)</li>
<li><code>MvpConfig</code> configuration class: <code>MvpForConditionalGeneration</code> (MVP model)</li>
<li><code>NezhaConfig</code> configuration class: <code>NezhaForPreTraining</code> (Nezha model)</li>
<li><code>NllbMoeConfig</code> configuration class: <code>NllbMoeForConditionalGeneration</code> (NLLB-MOE model)</li>
<li><code>OpenAIGPTConfig</code> configuration class: <code>OpenAIGPTLMHeadModel</code> (OpenAI GPT model)</li>
<li><code>RetriBertConfig</code> configuration class: <code>RetriBertModel</code> (RetriBERT model)</li>
<li><code>RoCBertConfig</code> configuration class: <code>RoCBertForPreTraining</code> (RoCBert model)</li>
<li><code>RobertaConfig</code> configuration class: <code>RobertaForMaskedLM</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>RobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>RwkvConfig</code> configuration class: <code>RwkvForCausalLM</code> (RWKV model)</li>
<li><code>SplinterConfig</code> configuration class: <code>SplinterForPreTraining</code> (Splinter model)</li>
<li><code>SqueezeBertConfig</code> configuration class: <code>SqueezeBertForMaskedLM</code> (SqueezeBERT model)</li>
<li><code>SwitchTransformersConfig</code> configuration class: <code>SwitchTransformersForConditionalGeneration</code> (SwitchTransformers model)</li>
<li><code>T5Config</code> configuration class: <code>T5ForConditionalGeneration</code> (T5 model)</li>
<li><code>TapasConfig</code> configuration class: <code>TapasForMaskedLM</code> (TAPAS model)</li>
<li><code>TransfoXLConfig</code> configuration class: <code>TransfoXLLMHeadModel</code> (Transformer-XL model)</li>
<li><code>TvltConfig</code> configuration class: <code>TvltForPreTraining</code> (TVLT model)</li>
<li><code>UniSpeechConfig</code> configuration class: <code>UniSpeechForPreTraining</code> (UniSpeech model)</li>
<li><code>UniSpeechSatConfig</code> configuration class: <code>UniSpeechSatForPreTraining</code> (UniSpeechSat model)</li>
<li><code>ViTMAEConfig</code> configuration class: <code>ViTMAEForPreTraining</code> (ViTMAE model)</li>
<li><code>VideoMAEConfig</code> configuration class: <code>VideoMAEForPreTraining</code> (VideoMAE model)</li>
<li><code>VipLlavaConfig</code> configuration class: <code>VipLlavaForConditionalGeneration</code> (VipLlava model)</li>
<li><code>VisualBertConfig</code> configuration class: <code>VisualBertForPreTraining</code> (VisualBERT model)</li>
<li><code>Wav2Vec2Config</code> configuration class: <code>Wav2Vec2ForPreTraining</code> (Wav2Vec2 model)</li>
<li><code>Wav2Vec2ConformerConfig</code> configuration class: <code>Wav2Vec2ConformerForPreTraining</code> (Wav2Vec2-Conformer model)</li>
<li><code>XLMConfig</code> configuration class: <code>XLMWithLMHeadModel</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>XLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li>
<li><code>XLMRobertaXLConfig</code> configuration class: <code>XLMRobertaXLForMaskedLM</code> (XLM-RoBERTa-XL model)</li>
<li><code>XLNetConfig</code> configuration class: <code>XLNetLMHeadModel</code> (XLNet model)</li>
<li><code>XmodConfig</code> configuration class: <code>XmodForMaskedLM</code> (X-MOD model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Is=new k({props:{anchor:"transformers.AutoModelForPreTraining.from_config.example",$$slots:{default:[ZX]},$$scope:{ctx:v}}}),hc=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForPreTraining.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForPreTraining.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForPreTraining.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ns=new k({props:{anchor:"transformers.AutoModelForPreTraining.from_pretrained.example",$$slots:{default:[LX]},$$scope:{ctx:v}}}),bc=new Z({props:{title:"TFAutoModelForPreTraining",local:"transformers.TFAutoModelForPreTraining",headingTag:"h3"}}),_c=new j({props:{name:"class transformers.TFAutoModelForPreTraining",anchor:"transformers.TFAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L547"}}),Mc=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForPreTraining.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForPreTraining.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForPreTraining">TFAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForPreTraining">TFBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>TFDistilBertForMaskedLM</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>TFElectraForPreTraining</code> (ELECTRA model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>TFFlaubertWithLMHeadModel</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>TFFunnelForPreTraining</code> (Funnel Transformer model)</li>
<li><code>GPT2Config</code> configuration class: <code>TFGPT2LMHeadModel</code> (OpenAI GPT-2 model)</li>
<li><code>LayoutLMConfig</code> configuration class: <code>TFLayoutLMForMaskedLM</code> (LayoutLM model)</li>
<li><code>LxmertConfig</code> configuration class: <code>TFLxmertForPreTraining</code> (LXMERT model)</li>
<li><code>MPNetConfig</code> configuration class: <code>TFMPNetForMaskedLM</code> (MPNet model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>TFMobileBertForPreTraining</code> (MobileBERT model)</li>
<li><code>OpenAIGPTConfig</code> configuration class: <code>TFOpenAIGPTLMHeadModel</code> (OpenAI GPT model)</li>
<li><code>RobertaConfig</code> configuration class: <code>TFRobertaForMaskedLM</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>TFRobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>T5Config</code> configuration class: <code>TFT5ForConditionalGeneration</code> (T5 model)</li>
<li><code>TapasConfig</code> configuration class: <code>TFTapasForMaskedLM</code> (TAPAS model)</li>
<li><code>TransfoXLConfig</code> configuration class: <code>TFTransfoXLLMHeadModel</code> (Transformer-XL model)</li>
<li><code>ViTMAEConfig</code> configuration class: <code>TFViTMAEForPreTraining</code> (ViTMAE model)</li>
<li><code>XLMConfig</code> configuration class: <code>TFXLMWithLMHeadModel</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>TFXLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li>
<li><code>XLNetConfig</code> configuration class: <code>TFXLNetLMHeadModel</code> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Xs=new k({props:{anchor:"transformers.TFAutoModelForPreTraining.from_config.example",$$slots:{default:[BX]},$$scope:{ctx:v}}}),Tc=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForPreTraining.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),qs=new k({props:{anchor:"transformers.TFAutoModelForPreTraining.from_pretrained.example",$$slots:{default:[AX]},$$scope:{ctx:v}}}),yc=new Z({props:{title:"FlaxAutoModelForPreTraining",local:"transformers.FlaxAutoModelForPreTraining",headingTag:"h3"}}),Fc=new j({props:{name:"class transformers.FlaxAutoModelForPreTraining",anchor:"transformers.FlaxAutoModelForPreTraining",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L285"}}),vc=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForPreTraining.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForPreTraining.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForPreTraining">FlaxAlbertForPreTraining</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForPreTraining">FlaxBertForPreTraining</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForPreTraining">FlaxBigBirdForPreTraining</a> (BigBird model)</li>
<li><code>ElectraConfig</code> configuration class: <code>FlaxElectraForPreTraining</code> (ELECTRA model)</li>
<li><code>LongT5Config</code> configuration class: <code>FlaxLongT5ForConditionalGeneration</code> (LongT5 model)</li>
<li><code>MBartConfig</code> configuration class: <code>FlaxMBartForConditionalGeneration</code> (mBART model)</li>
<li><code>MT5Config</code> configuration class: <code>FlaxMT5ForConditionalGeneration</code> (MT5 model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>FlaxRoFormerForMaskedLM</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>FlaxRobertaForMaskedLM</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>FlaxRobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>T5Config</code> configuration class: <code>FlaxT5ForConditionalGeneration</code> (T5 model)</li>
<li><code>Wav2Vec2Config</code> configuration class: <code>FlaxWav2Vec2ForPreTraining</code> (Wav2Vec2 model)</li>
<li><code>WhisperConfig</code> configuration class: <code>FlaxWhisperForConditionalGeneration</code> (Whisper model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>FlaxXLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Qs=new k({props:{anchor:"transformers.FlaxAutoModelForPreTraining.from_config.example",$$slots:{default:[RX]},$$scope:{ctx:v}}}),Cc=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Hs=new k({props:{anchor:"transformers.FlaxAutoModelForPreTraining.from_pretrained.example",$$slots:{default:[WX]},$$scope:{ctx:v}}}),wc=new Z({props:{title:"Natural Language Processing",local:"natural-language-processing",headingTag:"h2"}}),xc=new Z({props:{title:"AutoModelForCausalLM",local:"transformers.AutoModelForCausalLM",headingTag:"h3"}}),$c=new j({props:{name:"class transformers.AutoModelForCausalLM",anchor:"transformers.AutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1336"}}),kc=new j({props:{name:"from_config",anchor:"transformers.AutoModelForCausalLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCausalLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForCausalLM">BartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertLMHeadModel">BertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert-generation#transformers.BertGenerationConfig">BertGenerationConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert-generation#transformers.BertGenerationDecoder">BertGenerationDecoder</a> (Bert Generation model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForCausalLM">BigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForCausalLM">BigBirdPegasusForCausalLM</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptForCausalLM">BioGptForCausalLM</a> (BioGpt model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotForCausalLM">BlenderbotForCausalLM</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallForCausalLM">BlenderbotSmallForCausalLM</a> (BlenderbotSmall model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomForCausalLM">BloomForCausalLM</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLLMHeadModel">CTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForCausalLM">CamembertForCausalLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenConfig">CodeGenConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/codegen#transformers.CodeGenForCausalLM">CodeGenForCausalLM</a> (CodeGen model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/cpmant#transformers.CpmAntConfig">CpmAntConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/cpmant#transformers.CpmAntForCausalLM">CpmAntForCausalLM</a> (CPM-Ant model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForCausalLM">Data2VecTextForCausalLM</a> (Data2VecText model)</li>
<li><code>ElectraConfig</code> configuration class: <code>ElectraForCausalLM</code> (ELECTRA model)</li>
<li><code>ErnieConfig</code> configuration class: <code>ErnieForCausalLM</code> (ERNIE model)</li>
<li><code>FalconConfig</code> configuration class: <code>FalconForCausalLM</code> (Falcon model)</li>
<li><code>FuyuConfig</code> configuration class: <code>FuyuForCausalLM</code> (Fuyu model)</li>
<li><code>GPT2Config</code> configuration class: <code>GPT2LMHeadModel</code> (OpenAI GPT-2 model)</li>
<li><code>GPTBigCodeConfig</code> configuration class: <code>GPTBigCodeForCausalLM</code> (GPTBigCode model)</li>
<li><code>GPTJConfig</code> configuration class: <code>GPTJForCausalLM</code> (GPT-J model)</li>
<li><code>GPTNeoConfig</code> configuration class: <code>GPTNeoForCausalLM</code> (GPT Neo model)</li>
<li><code>GPTNeoXConfig</code> configuration class: <code>GPTNeoXForCausalLM</code> (GPT NeoX model)</li>
<li><code>GPTNeoXJapaneseConfig</code> configuration class: <code>GPTNeoXJapaneseForCausalLM</code> (GPT NeoX Japanese model)</li>
<li><code>GitConfig</code> configuration class: <code>GitForCausalLM</code> (GIT model)</li>
<li><code>LlamaConfig</code> configuration class: <code>LlamaForCausalLM</code> (LLaMA model)</li>
<li><code>MBartConfig</code> configuration class: <code>MBartForCausalLM</code> (mBART model)</li>
<li><code>MarianConfig</code> configuration class: <code>MarianForCausalLM</code> (Marian model)</li>
<li><code>MegaConfig</code> configuration class: <code>MegaForCausalLM</code> (MEGA model)</li>
<li><code>MegatronBertConfig</code> configuration class: <code>MegatronBertForCausalLM</code> (Megatron-BERT model)</li>
<li><code>MistralConfig</code> configuration class: <code>MistralForCausalLM</code> (Mistral model)</li>
<li><code>MixtralConfig</code> configuration class: <code>MixtralForCausalLM</code> (Mixtral model)</li>
<li><code>MptConfig</code> configuration class: <code>MptForCausalLM</code> (MPT model)</li>
<li><code>MusicgenConfig</code> configuration class: <code>MusicgenForCausalLM</code> (MusicGen model)</li>
<li><code>MvpConfig</code> configuration class: <code>MvpForCausalLM</code> (MVP model)</li>
<li><code>OPTConfig</code> configuration class: <code>OPTForCausalLM</code> (OPT model)</li>
<li><code>OpenAIGPTConfig</code> configuration class: <code>OpenAIGPTLMHeadModel</code> (OpenAI GPT model)</li>
<li><code>OpenLlamaConfig</code> configuration class: <code>OpenLlamaForCausalLM</code> (OpenLlama model)</li>
<li><code>PLBartConfig</code> configuration class: <code>PLBartForCausalLM</code> (PLBart model)</li>
<li><code>PegasusConfig</code> configuration class: <code>PegasusForCausalLM</code> (Pegasus model)</li>
<li><code>PersimmonConfig</code> configuration class: <code>PersimmonForCausalLM</code> (Persimmon model)</li>
<li><code>PhiConfig</code> configuration class: <code>PhiForCausalLM</code> (Phi model)</li>
<li><code>ProphetNetConfig</code> configuration class: <code>ProphetNetForCausalLM</code> (ProphetNet model)</li>
<li><code>QDQBertConfig</code> configuration class: <code>QDQBertLMHeadModel</code> (QDQBert model)</li>
<li><code>Qwen2Config</code> configuration class: <code>Qwen2ForCausalLM</code> (Qwen2 model)</li>
<li><code>ReformerConfig</code> configuration class: <code>ReformerModelWithLMHead</code> (Reformer model)</li>
<li><code>RemBertConfig</code> configuration class: <code>RemBertForCausalLM</code> (RemBERT model)</li>
<li><code>RoCBertConfig</code> configuration class: <code>RoCBertForCausalLM</code> (RoCBert model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>RoFormerForCausalLM</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>RobertaForCausalLM</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>RobertaPreLayerNormForCausalLM</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>RwkvConfig</code> configuration class: <code>RwkvForCausalLM</code> (RWKV model)</li>
<li><code>Speech2Text2Config</code> configuration class: <code>Speech2Text2ForCausalLM</code> (Speech2Text2 model)</li>
<li><code>StableLmConfig</code> configuration class: <code>StableLmForCausalLM</code> (StableLm model)</li>
<li><code>TrOCRConfig</code> configuration class: <code>TrOCRForCausalLM</code> (TrOCR model)</li>
<li><code>TransfoXLConfig</code> configuration class: <code>TransfoXLLMHeadModel</code> (Transformer-XL model)</li>
<li><code>WhisperConfig</code> configuration class: <code>WhisperForCausalLM</code> (Whisper model)</li>
<li><code>XGLMConfig</code> configuration class: <code>XGLMForCausalLM</code> (XGLM model)</li>
<li><code>XLMConfig</code> configuration class: <code>XLMWithLMHeadModel</code> (XLM model)</li>
<li><code>XLMProphetNetConfig</code> configuration class: <code>XLMProphetNetForCausalLM</code> (XLM-ProphetNet model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>XLMRobertaForCausalLM</code> (XLM-RoBERTa model)</li>
<li><code>XLMRobertaXLConfig</code> configuration class: <code>XLMRobertaXLForCausalLM</code> (XLM-RoBERTa-XL model)</li>
<li><code>XLNetConfig</code> configuration class: <code>XLNetLMHeadModel</code> (XLNet model)</li>
<li><code>XmodConfig</code> configuration class: <code>XmodForCausalLM</code> (X-MOD model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ds=new k({props:{anchor:"transformers.AutoModelForCausalLM.from_config.example",$$slots:{default:[JX]},$$scope:{ctx:v}}}),Zc=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForCausalLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCausalLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForCausalLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ys=new k({props:{anchor:"transformers.AutoModelForCausalLM.from_pretrained.example",$$slots:{default:[VX]},$$scope:{ctx:v}}}),Lc=new Z({props:{title:"TFAutoModelForCausalLM",local:"transformers.TFAutoModelForCausalLM",headingTag:"h3"}}),Bc=new j({props:{name:"class transformers.TFAutoModelForCausalLM",anchor:"transformers.TFAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L562"}}),Ac=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForCausalLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForCausalLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertLMHeadModel">TFBertLMHeadModel</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.TFCTRLLMHeadModel">TFCTRLLMHeadModel</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForCausalLM">TFCamembertForCausalLM</a> (CamemBERT model)</li>
<li><code>GPT2Config</code> configuration class: <code>TFGPT2LMHeadModel</code> (OpenAI GPT-2 model)</li>
<li><code>GPTJConfig</code> configuration class: <code>TFGPTJForCausalLM</code> (GPT-J model)</li>
<li><code>OPTConfig</code> configuration class: <code>TFOPTForCausalLM</code> (OPT model)</li>
<li><code>OpenAIGPTConfig</code> configuration class: <code>TFOpenAIGPTLMHeadModel</code> (OpenAI GPT model)</li>
<li><code>RemBertConfig</code> configuration class: <code>TFRemBertForCausalLM</code> (RemBERT model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>TFRoFormerForCausalLM</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>TFRobertaForCausalLM</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>TFRobertaPreLayerNormForCausalLM</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>TransfoXLConfig</code> configuration class: <code>TFTransfoXLLMHeadModel</code> (Transformer-XL model)</li>
<li><code>XGLMConfig</code> configuration class: <code>TFXGLMForCausalLM</code> (XGLM model)</li>
<li><code>XLMConfig</code> configuration class: <code>TFXLMWithLMHeadModel</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>TFXLMRobertaForCausalLM</code> (XLM-RoBERTa model)</li>
<li><code>XLNetConfig</code> configuration class: <code>TFXLNetLMHeadModel</code> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),zs=new k({props:{anchor:"transformers.TFAutoModelForCausalLM.from_config.example",$$slots:{default:[GX]},$$scope:{ctx:v}}}),Rc=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForCausalLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Os=new k({props:{anchor:"transformers.TFAutoModelForCausalLM.from_pretrained.example",$$slots:{default:[EX]},$$scope:{ctx:v}}}),Wc=new Z({props:{title:"FlaxAutoModelForCausalLM",local:"transformers.FlaxAutoModelForCausalLM",headingTag:"h3"}}),Jc=new j({props:{name:"class transformers.FlaxAutoModelForCausalLM",anchor:"transformers.FlaxAutoModelForCausalLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L292"}}),Vc=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForCausalLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForCausalLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForCausalLM">FlaxBartForCausalLM</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForCausalLM">FlaxBertForCausalLM</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForCausalLM">FlaxBigBirdForCausalLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.FlaxBloomForCausalLM">FlaxBloomForCausalLM</a> (BLOOM model)</li>
<li><code>ElectraConfig</code> configuration class: <code>FlaxElectraForCausalLM</code> (ELECTRA model)</li>
<li><code>GPT2Config</code> configuration class: <code>FlaxGPT2LMHeadModel</code> (OpenAI GPT-2 model)</li>
<li><code>GPTJConfig</code> configuration class: <code>FlaxGPTJForCausalLM</code> (GPT-J model)</li>
<li><code>GPTNeoConfig</code> configuration class: <code>FlaxGPTNeoForCausalLM</code> (GPT Neo model)</li>
<li><code>LlamaConfig</code> configuration class: <code>FlaxLlamaForCausalLM</code> (LLaMA model)</li>
<li><code>MistralConfig</code> configuration class: <code>FlaxMistralForCausalLM</code> (Mistral model)</li>
<li><code>OPTConfig</code> configuration class: <code>FlaxOPTForCausalLM</code> (OPT model)</li>
<li><code>RobertaConfig</code> configuration class: <code>FlaxRobertaForCausalLM</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>FlaxRobertaPreLayerNormForCausalLM</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>XGLMConfig</code> configuration class: <code>FlaxXGLMForCausalLM</code> (XGLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>FlaxXLMRobertaForCausalLM</code> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ks=new k({props:{anchor:"transformers.FlaxAutoModelForCausalLM.from_config.example",$$slots:{default:[PX]},$$scope:{ctx:v}}}),Gc=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ei=new k({props:{anchor:"transformers.FlaxAutoModelForCausalLM.from_pretrained.example",$$slots:{default:[SX]},$$scope:{ctx:v}}}),Ec=new Z({props:{title:"AutoModelForMaskedLM",local:"transformers.AutoModelForMaskedLM",headingTag:"h3"}}),Pc=new j({props:{name:"class transformers.AutoModelForMaskedLM",anchor:"transformers.AutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1343"}}),Sc=new j({props:{name:"from_config",anchor:"transformers.AutoModelForMaskedLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForMaskedLM">AlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForMaskedLM">BertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForMaskedLM">BigBirdForMaskedLM</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForMaskedLM">CamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertForMaskedLM">ConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForMaskedLM">Data2VecTextForMaskedLM</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaForMaskedLM">DebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">DebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>DistilBertForMaskedLM</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>ElectraForMaskedLM</code> (ELECTRA model)</li>
<li><code>ErnieConfig</code> configuration class: <code>ErnieForMaskedLM</code> (ERNIE model)</li>
<li><code>EsmConfig</code> configuration class: <code>EsmForMaskedLM</code> (ESM model)</li>
<li><code>FNetConfig</code> configuration class: <code>FNetForMaskedLM</code> (FNet model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>FlaubertWithLMHeadModel</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>FunnelForMaskedLM</code> (Funnel Transformer model)</li>
<li><code>IBertConfig</code> configuration class: <code>IBertForMaskedLM</code> (I-BERT model)</li>
<li><code>LayoutLMConfig</code> configuration class: <code>LayoutLMForMaskedLM</code> (LayoutLM model)</li>
<li><code>LongformerConfig</code> configuration class: <code>LongformerForMaskedLM</code> (Longformer model)</li>
<li><code>LukeConfig</code> configuration class: <code>LukeForMaskedLM</code> (LUKE model)</li>
<li><code>MBartConfig</code> configuration class: <code>MBartForConditionalGeneration</code> (mBART model)</li>
<li><code>MPNetConfig</code> configuration class: <code>MPNetForMaskedLM</code> (MPNet model)</li>
<li><code>MegaConfig</code> configuration class: <code>MegaForMaskedLM</code> (MEGA model)</li>
<li><code>MegatronBertConfig</code> configuration class: <code>MegatronBertForMaskedLM</code> (Megatron-BERT model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>MobileBertForMaskedLM</code> (MobileBERT model)</li>
<li><code>MraConfig</code> configuration class: <code>MraForMaskedLM</code> (MRA model)</li>
<li><code>MvpConfig</code> configuration class: <code>MvpForConditionalGeneration</code> (MVP model)</li>
<li><code>NezhaConfig</code> configuration class: <code>NezhaForMaskedLM</code> (Nezha model)</li>
<li><code>NystromformerConfig</code> configuration class: <code>NystromformerForMaskedLM</code> (Nystr&#xF6;mformer model)</li>
<li><code>PerceiverConfig</code> configuration class: <code>PerceiverForMaskedLM</code> (Perceiver model)</li>
<li><code>QDQBertConfig</code> configuration class: <code>QDQBertForMaskedLM</code> (QDQBert model)</li>
<li><code>ReformerConfig</code> configuration class: <code>ReformerForMaskedLM</code> (Reformer model)</li>
<li><code>RemBertConfig</code> configuration class: <code>RemBertForMaskedLM</code> (RemBERT model)</li>
<li><code>RoCBertConfig</code> configuration class: <code>RoCBertForMaskedLM</code> (RoCBert model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>RoFormerForMaskedLM</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>RobertaForMaskedLM</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>RobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>SqueezeBertConfig</code> configuration class: <code>SqueezeBertForMaskedLM</code> (SqueezeBERT model)</li>
<li><code>TapasConfig</code> configuration class: <code>TapasForMaskedLM</code> (TAPAS model)</li>
<li><code>Wav2Vec2Config</code> configuration class: <code>Wav2Vec2ForMaskedLM</code> (Wav2Vec2 model)</li>
<li><code>XLMConfig</code> configuration class: <code>XLMWithLMHeadModel</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>XLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li>
<li><code>XLMRobertaXLConfig</code> configuration class: <code>XLMRobertaXLForMaskedLM</code> (XLM-RoBERTa-XL model)</li>
<li><code>XmodConfig</code> configuration class: <code>XmodForMaskedLM</code> (X-MOD model)</li>
<li><code>YosoConfig</code> configuration class: <code>YosoForMaskedLM</code> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),oi=new k({props:{anchor:"transformers.AutoModelForMaskedLM.from_config.example",$$slots:{default:[UX]},$$scope:{ctx:v}}}),Uc=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForMaskedLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ti=new k({props:{anchor:"transformers.AutoModelForMaskedLM.from_pretrained.example",$$slots:{default:[IX]},$$scope:{ctx:v}}}),Ic=new Z({props:{title:"TFAutoModelForMaskedLM",local:"transformers.TFAutoModelForMaskedLM",headingTag:"h3"}}),Nc=new j({props:{name:"class transformers.TFAutoModelForMaskedLM",anchor:"transformers.TFAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L612"}}),Xc=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForMaskedLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMaskedLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForMaskedLM">TFAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForMaskedLM">TFBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForMaskedLM">TFCamembertForMaskedLM</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertForMaskedLM">TFConvBertForMaskedLM</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.TFDebertaForMaskedLM">TFDebertaForMaskedLM</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">TFDebertaV2ForMaskedLM</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>TFDistilBertForMaskedLM</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>TFElectraForMaskedLM</code> (ELECTRA model)</li>
<li><code>EsmConfig</code> configuration class: <code>TFEsmForMaskedLM</code> (ESM model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>TFFlaubertWithLMHeadModel</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>TFFunnelForMaskedLM</code> (Funnel Transformer model)</li>
<li><code>LayoutLMConfig</code> configuration class: <code>TFLayoutLMForMaskedLM</code> (LayoutLM model)</li>
<li><code>LongformerConfig</code> configuration class: <code>TFLongformerForMaskedLM</code> (Longformer model)</li>
<li><code>MPNetConfig</code> configuration class: <code>TFMPNetForMaskedLM</code> (MPNet model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>TFMobileBertForMaskedLM</code> (MobileBERT model)</li>
<li><code>RemBertConfig</code> configuration class: <code>TFRemBertForMaskedLM</code> (RemBERT model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>TFRoFormerForMaskedLM</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>TFRobertaForMaskedLM</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>TFRobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>TapasConfig</code> configuration class: <code>TFTapasForMaskedLM</code> (TAPAS model)</li>
<li><code>XLMConfig</code> configuration class: <code>TFXLMWithLMHeadModel</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>TFXLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ni=new k({props:{anchor:"transformers.TFAutoModelForMaskedLM.from_config.example",$$slots:{default:[NX]},$$scope:{ctx:v}}}),qc=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ri=new k({props:{anchor:"transformers.TFAutoModelForMaskedLM.from_pretrained.example",$$slots:{default:[XX]},$$scope:{ctx:v}}}),Qc=new Z({props:{title:"FlaxAutoModelForMaskedLM",local:"transformers.FlaxAutoModelForMaskedLM",headingTag:"h3"}}),Hc=new j({props:{name:"class transformers.FlaxAutoModelForMaskedLM",anchor:"transformers.FlaxAutoModelForMaskedLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L299"}}),Dc=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForMaskedLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForMaskedLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForMaskedLM">FlaxAlbertForMaskedLM</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForMaskedLM">FlaxBertForMaskedLM</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForMaskedLM">FlaxBigBirdForMaskedLM</a> (BigBird model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>FlaxDistilBertForMaskedLM</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>FlaxElectraForMaskedLM</code> (ELECTRA model)</li>
<li><code>MBartConfig</code> configuration class: <code>FlaxMBartForConditionalGeneration</code> (mBART model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>FlaxRoFormerForMaskedLM</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>FlaxRobertaForMaskedLM</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>FlaxRobertaPreLayerNormForMaskedLM</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>FlaxXLMRobertaForMaskedLM</code> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ai=new k({props:{anchor:"transformers.FlaxAutoModelForMaskedLM.from_config.example",$$slots:{default:[qX]},$$scope:{ctx:v}}}),Yc=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),si=new k({props:{anchor:"transformers.FlaxAutoModelForMaskedLM.from_pretrained.example",$$slots:{default:[QX]},$$scope:{ctx:v}}}),zc=new Z({props:{title:"AutoModelForMaskGeneration",local:"transformers.AutoModelForMaskGeneration",headingTag:"h3"}}),Kc=new j({props:{name:"class transformers.AutoModelForMaskGeneration",anchor:"transformers.AutoModelForMaskGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1302"}}),em=new Z({props:{title:"TFAutoModelForMaskGeneration",local:"transformers.TFAutoModelForMaskGeneration",headingTag:"h3"}}),tm=new j({props:{name:"class transformers.TFAutoModelForMaskGeneration",anchor:"transformers.TFAutoModelForMaskGeneration",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L523"}}),nm=new Z({props:{title:"AutoModelForSeq2SeqLM",local:"transformers.AutoModelForSeq2SeqLM",headingTag:"h3"}}),rm=new j({props:{name:"class transformers.AutoModelForSeq2SeqLM",anchor:"transformers.AutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1350"}}),am=new j({props:{name:"from_config",anchor:"transformers.AutoModelForSeq2SeqLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSeq2SeqLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForConditionalGeneration">BartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForConditionalGeneration">BigBirdPegasusForConditionalGeneration</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotForConditionalGeneration">BlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallForConditionalGeneration">BlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><code>EncoderDecoderConfig</code> configuration class: <code>EncoderDecoderModel</code> (Encoder decoder model)</li>
<li><code>FSMTConfig</code> configuration class: <code>FSMTForConditionalGeneration</code> (FairSeq Machine-Translation model)</li>
<li><code>GPTSanJapaneseConfig</code> configuration class: <code>GPTSanJapaneseForConditionalGeneration</code> (GPTSAN-japanese model)</li>
<li><code>LEDConfig</code> configuration class: <code>LEDForConditionalGeneration</code> (LED model)</li>
<li><code>LongT5Config</code> configuration class: <code>LongT5ForConditionalGeneration</code> (LongT5 model)</li>
<li><code>M2M100Config</code> configuration class: <code>M2M100ForConditionalGeneration</code> (M2M100 model)</li>
<li><code>MBartConfig</code> configuration class: <code>MBartForConditionalGeneration</code> (mBART model)</li>
<li><code>MT5Config</code> configuration class: <code>MT5ForConditionalGeneration</code> (MT5 model)</li>
<li><code>MarianConfig</code> configuration class: <code>MarianMTModel</code> (Marian model)</li>
<li><code>MvpConfig</code> configuration class: <code>MvpForConditionalGeneration</code> (MVP model)</li>
<li><code>NllbMoeConfig</code> configuration class: <code>NllbMoeForConditionalGeneration</code> (NLLB-MOE model)</li>
<li><code>PLBartConfig</code> configuration class: <code>PLBartForConditionalGeneration</code> (PLBart model)</li>
<li><code>PegasusConfig</code> configuration class: <code>PegasusForConditionalGeneration</code> (Pegasus model)</li>
<li><code>PegasusXConfig</code> configuration class: <code>PegasusXForConditionalGeneration</code> (PEGASUS-X model)</li>
<li><code>ProphetNetConfig</code> configuration class: <code>ProphetNetForConditionalGeneration</code> (ProphetNet model)</li>
<li><code>SeamlessM4TConfig</code> configuration class: <code>SeamlessM4TForTextToText</code> (SeamlessM4T model)</li>
<li><code>SeamlessM4Tv2Config</code> configuration class: <code>SeamlessM4Tv2ForTextToText</code> (SeamlessM4Tv2 model)</li>
<li><code>SwitchTransformersConfig</code> configuration class: <code>SwitchTransformersForConditionalGeneration</code> (SwitchTransformers model)</li>
<li><code>T5Config</code> configuration class: <code>T5ForConditionalGeneration</code> (T5 model)</li>
<li><code>UMT5Config</code> configuration class: <code>UMT5ForConditionalGeneration</code> (UMT5 model)</li>
<li><code>XLMProphetNetConfig</code> configuration class: <code>XLMProphetNetForConditionalGeneration</code> (XLM-ProphetNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ii=new k({props:{anchor:"transformers.AutoModelForSeq2SeqLM.from_config.example",$$slots:{default:[HX]},$$scope:{ctx:v}}}),sm=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),li=new k({props:{anchor:"transformers.AutoModelForSeq2SeqLM.from_pretrained.example",$$slots:{default:[DX]},$$scope:{ctx:v}}}),im=new Z({props:{title:"TFAutoModelForSeq2SeqLM",local:"transformers.TFAutoModelForSeq2SeqLM",headingTag:"h3"}}),lm=new j({props:{name:"class transformers.TFAutoModelForSeq2SeqLM",anchor:"transformers.TFAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L619"}}),dm=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForSeq2SeqLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.TFBartForConditionalGeneration">TFBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.TFBlenderbotForConditionalGeneration">TFBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.TFBlenderbotSmallForConditionalGeneration">TFBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><code>EncoderDecoderConfig</code> configuration class: <code>TFEncoderDecoderModel</code> (Encoder decoder model)</li>
<li><code>LEDConfig</code> configuration class: <code>TFLEDForConditionalGeneration</code> (LED model)</li>
<li><code>MBartConfig</code> configuration class: <code>TFMBartForConditionalGeneration</code> (mBART model)</li>
<li><code>MT5Config</code> configuration class: <code>TFMT5ForConditionalGeneration</code> (MT5 model)</li>
<li><code>MarianConfig</code> configuration class: <code>TFMarianMTModel</code> (Marian model)</li>
<li><code>PegasusConfig</code> configuration class: <code>TFPegasusForConditionalGeneration</code> (Pegasus model)</li>
<li><code>T5Config</code> configuration class: <code>TFT5ForConditionalGeneration</code> (T5 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),di=new k({props:{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_config.example",$$slots:{default:[YX]},$$scope:{ctx:v}}}),cm=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ci=new k({props:{anchor:"transformers.TFAutoModelForSeq2SeqLM.from_pretrained.example",$$slots:{default:[zX]},$$scope:{ctx:v}}}),mm=new Z({props:{title:"FlaxAutoModelForSeq2SeqLM",local:"transformers.FlaxAutoModelForSeq2SeqLM",headingTag:"h3"}}),fm=new j({props:{name:"class transformers.FlaxAutoModelForSeq2SeqLM",anchor:"transformers.FlaxAutoModelForSeq2SeqLM",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L306"}}),gm=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForConditionalGeneration">FlaxBartForConditionalGeneration</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.BlenderbotConfig">BlenderbotConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot#transformers.FlaxBlenderbotForConditionalGeneration">FlaxBlenderbotForConditionalGeneration</a> (Blenderbot model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.BlenderbotSmallConfig">BlenderbotSmallConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blenderbot-small#transformers.FlaxBlenderbotSmallForConditionalGeneration">FlaxBlenderbotSmallForConditionalGeneration</a> (BlenderbotSmall model)</li>
<li><code>EncoderDecoderConfig</code> configuration class: <code>FlaxEncoderDecoderModel</code> (Encoder decoder model)</li>
<li><code>LongT5Config</code> configuration class: <code>FlaxLongT5ForConditionalGeneration</code> (LongT5 model)</li>
<li><code>MBartConfig</code> configuration class: <code>FlaxMBartForConditionalGeneration</code> (mBART model)</li>
<li><code>MT5Config</code> configuration class: <code>FlaxMT5ForConditionalGeneration</code> (MT5 model)</li>
<li><code>MarianConfig</code> configuration class: <code>FlaxMarianMTModel</code> (Marian model)</li>
<li><code>PegasusConfig</code> configuration class: <code>FlaxPegasusForConditionalGeneration</code> (Pegasus model)</li>
<li><code>T5Config</code> configuration class: <code>FlaxT5ForConditionalGeneration</code> (T5 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),mi=new k({props:{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_config.example",$$slots:{default:[OX]},$$scope:{ctx:v}}}),pm=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),fi=new k({props:{anchor:"transformers.FlaxAutoModelForSeq2SeqLM.from_pretrained.example",$$slots:{default:[KX]},$$scope:{ctx:v}}}),um=new Z({props:{title:"AutoModelForSequenceClassification",local:"transformers.AutoModelForSequenceClassification",headingTag:"h3"}}),hm=new j({props:{name:"class transformers.AutoModelForSequenceClassification",anchor:"transformers.AutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1361"}}),bm=new j({props:{name:"from_config",anchor:"transformers.AutoModelForSequenceClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSequenceClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForSequenceClassification">AlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForSequenceClassification">BartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForSequenceClassification">BertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForSequenceClassification">BigBirdForSequenceClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForSequenceClassification">BigBirdPegasusForSequenceClassification</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptForSequenceClassification">BioGptForSequenceClassification</a> (BioGpt model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomForSequenceClassification">BloomForSequenceClassification</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLForSequenceClassification">CTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForSequenceClassification">CamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineForSequenceClassification">CanineForSequenceClassification</a> (CANINE model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertForSequenceClassification">ConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForSequenceClassification">Data2VecTextForSequenceClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaForSequenceClassification">DebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForSequenceClassification">DebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>DistilBertForSequenceClassification</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>ElectraForSequenceClassification</code> (ELECTRA model)</li>
<li><code>ErnieConfig</code> configuration class: <code>ErnieForSequenceClassification</code> (ERNIE model)</li>
<li><code>ErnieMConfig</code> configuration class: <code>ErnieMForSequenceClassification</code> (ErnieM model)</li>
<li><code>EsmConfig</code> configuration class: <code>EsmForSequenceClassification</code> (ESM model)</li>
<li><code>FNetConfig</code> configuration class: <code>FNetForSequenceClassification</code> (FNet model)</li>
<li><code>FalconConfig</code> configuration class: <code>FalconForSequenceClassification</code> (Falcon model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>FlaubertForSequenceClassification</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>FunnelForSequenceClassification</code> (Funnel Transformer model)</li>
<li><code>GPT2Config</code> configuration class: <code>GPT2ForSequenceClassification</code> (OpenAI GPT-2 model)</li>
<li><code>GPTBigCodeConfig</code> configuration class: <code>GPTBigCodeForSequenceClassification</code> (GPTBigCode model)</li>
<li><code>GPTJConfig</code> configuration class: <code>GPTJForSequenceClassification</code> (GPT-J model)</li>
<li><code>GPTNeoConfig</code> configuration class: <code>GPTNeoForSequenceClassification</code> (GPT Neo model)</li>
<li><code>GPTNeoXConfig</code> configuration class: <code>GPTNeoXForSequenceClassification</code> (GPT NeoX model)</li>
<li><code>IBertConfig</code> configuration class: <code>IBertForSequenceClassification</code> (I-BERT model)</li>
<li><code>LEDConfig</code> configuration class: <code>LEDForSequenceClassification</code> (LED model)</li>
<li><code>LayoutLMConfig</code> configuration class: <code>LayoutLMForSequenceClassification</code> (LayoutLM model)</li>
<li><code>LayoutLMv2Config</code> configuration class: <code>LayoutLMv2ForSequenceClassification</code> (LayoutLMv2 model)</li>
<li><code>LayoutLMv3Config</code> configuration class: <code>LayoutLMv3ForSequenceClassification</code> (LayoutLMv3 model)</li>
<li><code>LiltConfig</code> configuration class: <code>LiltForSequenceClassification</code> (LiLT model)</li>
<li><code>LlamaConfig</code> configuration class: <code>LlamaForSequenceClassification</code> (LLaMA model)</li>
<li><code>LongformerConfig</code> configuration class: <code>LongformerForSequenceClassification</code> (Longformer model)</li>
<li><code>LukeConfig</code> configuration class: <code>LukeForSequenceClassification</code> (LUKE model)</li>
<li><code>MBartConfig</code> configuration class: <code>MBartForSequenceClassification</code> (mBART model)</li>
<li><code>MPNetConfig</code> configuration class: <code>MPNetForSequenceClassification</code> (MPNet model)</li>
<li><code>MT5Config</code> configuration class: <code>MT5ForSequenceClassification</code> (MT5 model)</li>
<li><code>MarkupLMConfig</code> configuration class: <code>MarkupLMForSequenceClassification</code> (MarkupLM model)</li>
<li><code>MegaConfig</code> configuration class: <code>MegaForSequenceClassification</code> (MEGA model)</li>
<li><code>MegatronBertConfig</code> configuration class: <code>MegatronBertForSequenceClassification</code> (Megatron-BERT model)</li>
<li><code>MistralConfig</code> configuration class: <code>MistralForSequenceClassification</code> (Mistral model)</li>
<li><code>MixtralConfig</code> configuration class: <code>MixtralForSequenceClassification</code> (Mixtral model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>MobileBertForSequenceClassification</code> (MobileBERT model)</li>
<li><code>MptConfig</code> configuration class: <code>MptForSequenceClassification</code> (MPT model)</li>
<li><code>MraConfig</code> configuration class: <code>MraForSequenceClassification</code> (MRA model)</li>
<li><code>MvpConfig</code> configuration class: <code>MvpForSequenceClassification</code> (MVP model)</li>
<li><code>NezhaConfig</code> configuration class: <code>NezhaForSequenceClassification</code> (Nezha model)</li>
<li><code>NystromformerConfig</code> configuration class: <code>NystromformerForSequenceClassification</code> (Nystr&#xF6;mformer model)</li>
<li><code>OPTConfig</code> configuration class: <code>OPTForSequenceClassification</code> (OPT model)</li>
<li><code>OpenAIGPTConfig</code> configuration class: <code>OpenAIGPTForSequenceClassification</code> (OpenAI GPT model)</li>
<li><code>OpenLlamaConfig</code> configuration class: <code>OpenLlamaForSequenceClassification</code> (OpenLlama model)</li>
<li><code>PLBartConfig</code> configuration class: <code>PLBartForSequenceClassification</code> (PLBart model)</li>
<li><code>PerceiverConfig</code> configuration class: <code>PerceiverForSequenceClassification</code> (Perceiver model)</li>
<li><code>PersimmonConfig</code> configuration class: <code>PersimmonForSequenceClassification</code> (Persimmon model)</li>
<li><code>PhiConfig</code> configuration class: <code>PhiForSequenceClassification</code> (Phi model)</li>
<li><code>QDQBertConfig</code> configuration class: <code>QDQBertForSequenceClassification</code> (QDQBert model)</li>
<li><code>Qwen2Config</code> configuration class: <code>Qwen2ForSequenceClassification</code> (Qwen2 model)</li>
<li><code>ReformerConfig</code> configuration class: <code>ReformerForSequenceClassification</code> (Reformer model)</li>
<li><code>RemBertConfig</code> configuration class: <code>RemBertForSequenceClassification</code> (RemBERT model)</li>
<li><code>RoCBertConfig</code> configuration class: <code>RoCBertForSequenceClassification</code> (RoCBert model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>RoFormerForSequenceClassification</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>RobertaForSequenceClassification</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>RobertaPreLayerNormForSequenceClassification</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>SqueezeBertConfig</code> configuration class: <code>SqueezeBertForSequenceClassification</code> (SqueezeBERT model)</li>
<li><code>StableLmConfig</code> configuration class: <code>StableLmForSequenceClassification</code> (StableLm model)</li>
<li><code>T5Config</code> configuration class: <code>T5ForSequenceClassification</code> (T5 model)</li>
<li><code>TapasConfig</code> configuration class: <code>TapasForSequenceClassification</code> (TAPAS model)</li>
<li><code>TransfoXLConfig</code> configuration class: <code>TransfoXLForSequenceClassification</code> (Transformer-XL model)</li>
<li><code>UMT5Config</code> configuration class: <code>UMT5ForSequenceClassification</code> (UMT5 model)</li>
<li><code>XLMConfig</code> configuration class: <code>XLMForSequenceClassification</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>XLMRobertaForSequenceClassification</code> (XLM-RoBERTa model)</li>
<li><code>XLMRobertaXLConfig</code> configuration class: <code>XLMRobertaXLForSequenceClassification</code> (XLM-RoBERTa-XL model)</li>
<li><code>XLNetConfig</code> configuration class: <code>XLNetForSequenceClassification</code> (XLNet model)</li>
<li><code>XmodConfig</code> configuration class: <code>XmodForSequenceClassification</code> (X-MOD model)</li>
<li><code>YosoConfig</code> configuration class: <code>YosoForSequenceClassification</code> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),gi=new k({props:{anchor:"transformers.AutoModelForSequenceClassification.from_config.example",$$slots:{default:[e3]},$$scope:{ctx:v}}}),_m=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSequenceClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),pi=new k({props:{anchor:"transformers.AutoModelForSequenceClassification.from_pretrained.example",$$slots:{default:[o3]},$$scope:{ctx:v}}}),Mm=new Z({props:{title:"TFAutoModelForSequenceClassification",local:"transformers.TFAutoModelForSequenceClassification",headingTag:"h3"}}),Tm=new j({props:{name:"class transformers.TFAutoModelForSequenceClassification",anchor:"transformers.TFAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L630"}}),ym=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForSequenceClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSequenceClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForSequenceClassification">TFAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.TFBartForSequenceClassification">TFBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForSequenceClassification">TFBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.CTRLConfig">CTRLConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/ctrl#transformers.TFCTRLForSequenceClassification">TFCTRLForSequenceClassification</a> (CTRL model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForSequenceClassification">TFCamembertForSequenceClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertForSequenceClassification">TFConvBertForSequenceClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.TFDebertaForSequenceClassification">TFDebertaForSequenceClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2ForSequenceClassification">TFDebertaV2ForSequenceClassification</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>TFDistilBertForSequenceClassification</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>TFElectraForSequenceClassification</code> (ELECTRA model)</li>
<li><code>EsmConfig</code> configuration class: <code>TFEsmForSequenceClassification</code> (ESM model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>TFFlaubertForSequenceClassification</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>TFFunnelForSequenceClassification</code> (Funnel Transformer model)</li>
<li><code>GPT2Config</code> configuration class: <code>TFGPT2ForSequenceClassification</code> (OpenAI GPT-2 model)</li>
<li><code>GPTJConfig</code> configuration class: <code>TFGPTJForSequenceClassification</code> (GPT-J model)</li>
<li><code>LayoutLMConfig</code> configuration class: <code>TFLayoutLMForSequenceClassification</code> (LayoutLM model)</li>
<li><code>LayoutLMv3Config</code> configuration class: <code>TFLayoutLMv3ForSequenceClassification</code> (LayoutLMv3 model)</li>
<li><code>LongformerConfig</code> configuration class: <code>TFLongformerForSequenceClassification</code> (Longformer model)</li>
<li><code>MPNetConfig</code> configuration class: <code>TFMPNetForSequenceClassification</code> (MPNet model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>TFMobileBertForSequenceClassification</code> (MobileBERT model)</li>
<li><code>OpenAIGPTConfig</code> configuration class: <code>TFOpenAIGPTForSequenceClassification</code> (OpenAI GPT model)</li>
<li><code>RemBertConfig</code> configuration class: <code>TFRemBertForSequenceClassification</code> (RemBERT model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>TFRoFormerForSequenceClassification</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>TFRobertaForSequenceClassification</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>TFRobertaPreLayerNormForSequenceClassification</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>TapasConfig</code> configuration class: <code>TFTapasForSequenceClassification</code> (TAPAS model)</li>
<li><code>TransfoXLConfig</code> configuration class: <code>TFTransfoXLForSequenceClassification</code> (Transformer-XL model)</li>
<li><code>XLMConfig</code> configuration class: <code>TFXLMForSequenceClassification</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>TFXLMRobertaForSequenceClassification</code> (XLM-RoBERTa model)</li>
<li><code>XLNetConfig</code> configuration class: <code>TFXLNetForSequenceClassification</code> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ui=new k({props:{anchor:"transformers.TFAutoModelForSequenceClassification.from_config.example",$$slots:{default:[t3]},$$scope:{ctx:v}}}),Fm=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),hi=new k({props:{anchor:"transformers.TFAutoModelForSequenceClassification.from_pretrained.example",$$slots:{default:[n3]},$$scope:{ctx:v}}}),vm=new Z({props:{title:"FlaxAutoModelForSequenceClassification",local:"transformers.FlaxAutoModelForSequenceClassification",headingTag:"h3"}}),Cm=new j({props:{name:"class transformers.FlaxAutoModelForSequenceClassification",anchor:"transformers.FlaxAutoModelForSequenceClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L317"}}),wm=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForSequenceClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForSequenceClassification">FlaxAlbertForSequenceClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForSequenceClassification">FlaxBartForSequenceClassification</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForSequenceClassification">FlaxBertForSequenceClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForSequenceClassification">FlaxBigBirdForSequenceClassification</a> (BigBird model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>FlaxDistilBertForSequenceClassification</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>FlaxElectraForSequenceClassification</code> (ELECTRA model)</li>
<li><code>MBartConfig</code> configuration class: <code>FlaxMBartForSequenceClassification</code> (mBART model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>FlaxRoFormerForSequenceClassification</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>FlaxRobertaForSequenceClassification</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>FlaxRobertaPreLayerNormForSequenceClassification</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>FlaxXLMRobertaForSequenceClassification</code> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),bi=new k({props:{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_config.example",$$slots:{default:[r3]},$$scope:{ctx:v}}}),jm=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),_i=new k({props:{anchor:"transformers.FlaxAutoModelForSequenceClassification.from_pretrained.example",$$slots:{default:[a3]},$$scope:{ctx:v}}}),xm=new Z({props:{title:"AutoModelForMultipleChoice",local:"transformers.AutoModelForMultipleChoice",headingTag:"h3"}}),$m=new j({props:{name:"class transformers.AutoModelForMultipleChoice",anchor:"transformers.AutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1417"}}),km=new j({props:{name:"from_config",anchor:"transformers.AutoModelForMultipleChoice.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMultipleChoice.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForMultipleChoice">AlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForMultipleChoice">BertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForMultipleChoice">BigBirdForMultipleChoice</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForMultipleChoice">CamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineForMultipleChoice">CanineForMultipleChoice</a> (CANINE model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertForMultipleChoice">ConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForMultipleChoice">Data2VecTextForMultipleChoice</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForMultipleChoice">DebertaV2ForMultipleChoice</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>DistilBertForMultipleChoice</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>ElectraForMultipleChoice</code> (ELECTRA model)</li>
<li><code>ErnieConfig</code> configuration class: <code>ErnieForMultipleChoice</code> (ERNIE model)</li>
<li><code>ErnieMConfig</code> configuration class: <code>ErnieMForMultipleChoice</code> (ErnieM model)</li>
<li><code>FNetConfig</code> configuration class: <code>FNetForMultipleChoice</code> (FNet model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>FlaubertForMultipleChoice</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>FunnelForMultipleChoice</code> (Funnel Transformer model)</li>
<li><code>IBertConfig</code> configuration class: <code>IBertForMultipleChoice</code> (I-BERT model)</li>
<li><code>LongformerConfig</code> configuration class: <code>LongformerForMultipleChoice</code> (Longformer model)</li>
<li><code>LukeConfig</code> configuration class: <code>LukeForMultipleChoice</code> (LUKE model)</li>
<li><code>MPNetConfig</code> configuration class: <code>MPNetForMultipleChoice</code> (MPNet model)</li>
<li><code>MegaConfig</code> configuration class: <code>MegaForMultipleChoice</code> (MEGA model)</li>
<li><code>MegatronBertConfig</code> configuration class: <code>MegatronBertForMultipleChoice</code> (Megatron-BERT model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>MobileBertForMultipleChoice</code> (MobileBERT model)</li>
<li><code>MraConfig</code> configuration class: <code>MraForMultipleChoice</code> (MRA model)</li>
<li><code>NezhaConfig</code> configuration class: <code>NezhaForMultipleChoice</code> (Nezha model)</li>
<li><code>NystromformerConfig</code> configuration class: <code>NystromformerForMultipleChoice</code> (Nystr&#xF6;mformer model)</li>
<li><code>QDQBertConfig</code> configuration class: <code>QDQBertForMultipleChoice</code> (QDQBert model)</li>
<li><code>RemBertConfig</code> configuration class: <code>RemBertForMultipleChoice</code> (RemBERT model)</li>
<li><code>RoCBertConfig</code> configuration class: <code>RoCBertForMultipleChoice</code> (RoCBert model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>RoFormerForMultipleChoice</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>RobertaForMultipleChoice</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>RobertaPreLayerNormForMultipleChoice</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>SqueezeBertConfig</code> configuration class: <code>SqueezeBertForMultipleChoice</code> (SqueezeBERT model)</li>
<li><code>XLMConfig</code> configuration class: <code>XLMForMultipleChoice</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>XLMRobertaForMultipleChoice</code> (XLM-RoBERTa model)</li>
<li><code>XLMRobertaXLConfig</code> configuration class: <code>XLMRobertaXLForMultipleChoice</code> (XLM-RoBERTa-XL model)</li>
<li><code>XLNetConfig</code> configuration class: <code>XLNetForMultipleChoice</code> (XLNet model)</li>
<li><code>XmodConfig</code> configuration class: <code>XmodForMultipleChoice</code> (X-MOD model)</li>
<li><code>YosoConfig</code> configuration class: <code>YosoForMultipleChoice</code> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Mi=new k({props:{anchor:"transformers.AutoModelForMultipleChoice.from_config.example",$$slots:{default:[s3]},$$scope:{ctx:v}}}),Zm=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForMultipleChoice.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ti=new k({props:{anchor:"transformers.AutoModelForMultipleChoice.from_pretrained.example",$$slots:{default:[i3]},$$scope:{ctx:v}}}),Lm=new Z({props:{title:"TFAutoModelForMultipleChoice",local:"transformers.TFAutoModelForMultipleChoice",headingTag:"h3"}}),Bm=new j({props:{name:"class transformers.TFAutoModelForMultipleChoice",anchor:"transformers.TFAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L677"}}),Am=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForMultipleChoice.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMultipleChoice.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForMultipleChoice">TFAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForMultipleChoice">TFBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForMultipleChoice">TFCamembertForMultipleChoice</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertForMultipleChoice">TFConvBertForMultipleChoice</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2ForMultipleChoice">TFDebertaV2ForMultipleChoice</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>TFDistilBertForMultipleChoice</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>TFElectraForMultipleChoice</code> (ELECTRA model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>TFFlaubertForMultipleChoice</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>TFFunnelForMultipleChoice</code> (Funnel Transformer model)</li>
<li><code>LongformerConfig</code> configuration class: <code>TFLongformerForMultipleChoice</code> (Longformer model)</li>
<li><code>MPNetConfig</code> configuration class: <code>TFMPNetForMultipleChoice</code> (MPNet model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>TFMobileBertForMultipleChoice</code> (MobileBERT model)</li>
<li><code>RemBertConfig</code> configuration class: <code>TFRemBertForMultipleChoice</code> (RemBERT model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>TFRoFormerForMultipleChoice</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>TFRobertaForMultipleChoice</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>TFRobertaPreLayerNormForMultipleChoice</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>XLMConfig</code> configuration class: <code>TFXLMForMultipleChoice</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>TFXLMRobertaForMultipleChoice</code> (XLM-RoBERTa model)</li>
<li><code>XLNetConfig</code> configuration class: <code>TFXLNetForMultipleChoice</code> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),yi=new k({props:{anchor:"transformers.TFAutoModelForMultipleChoice.from_config.example",$$slots:{default:[l3]},$$scope:{ctx:v}}}),Rm=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Fi=new k({props:{anchor:"transformers.TFAutoModelForMultipleChoice.from_pretrained.example",$$slots:{default:[d3]},$$scope:{ctx:v}}}),Wm=new Z({props:{title:"FlaxAutoModelForMultipleChoice",local:"transformers.FlaxAutoModelForMultipleChoice",headingTag:"h3"}}),Jm=new j({props:{name:"class transformers.FlaxAutoModelForMultipleChoice",anchor:"transformers.FlaxAutoModelForMultipleChoice",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L342"}}),Vm=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForMultipleChoice.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForMultipleChoice">FlaxAlbertForMultipleChoice</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForMultipleChoice">FlaxBertForMultipleChoice</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForMultipleChoice">FlaxBigBirdForMultipleChoice</a> (BigBird model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>FlaxDistilBertForMultipleChoice</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>FlaxElectraForMultipleChoice</code> (ELECTRA model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>FlaxRoFormerForMultipleChoice</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>FlaxRobertaForMultipleChoice</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>FlaxRobertaPreLayerNormForMultipleChoice</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>FlaxXLMRobertaForMultipleChoice</code> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),vi=new k({props:{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_config.example",$$slots:{default:[c3]},$$scope:{ctx:v}}}),Gm=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ci=new k({props:{anchor:"transformers.FlaxAutoModelForMultipleChoice.from_pretrained.example",$$slots:{default:[m3]},$$scope:{ctx:v}}}),Em=new Z({props:{title:"AutoModelForNextSentencePrediction",local:"transformers.AutoModelForNextSentencePrediction",headingTag:"h3"}}),Pm=new j({props:{name:"class transformers.AutoModelForNextSentencePrediction",anchor:"transformers.AutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1424"}}),Sm=new j({props:{name:"from_config",anchor:"transformers.AutoModelForNextSentencePrediction.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForNextSentencePrediction.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForNextSentencePrediction">BertForNextSentencePrediction</a> (BERT model)</li>
<li><code>ErnieConfig</code> configuration class: <code>ErnieForNextSentencePrediction</code> (ERNIE model)</li>
<li><code>FNetConfig</code> configuration class: <code>FNetForNextSentencePrediction</code> (FNet model)</li>
<li><code>MegatronBertConfig</code> configuration class: <code>MegatronBertForNextSentencePrediction</code> (Megatron-BERT model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>MobileBertForNextSentencePrediction</code> (MobileBERT model)</li>
<li><code>NezhaConfig</code> configuration class: <code>NezhaForNextSentencePrediction</code> (Nezha model)</li>
<li><code>QDQBertConfig</code> configuration class: <code>QDQBertForNextSentencePrediction</code> (QDQBert model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),wi=new k({props:{anchor:"transformers.AutoModelForNextSentencePrediction.from_config.example",$$slots:{default:[f3]},$$scope:{ctx:v}}}),Um=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ji=new k({props:{anchor:"transformers.AutoModelForNextSentencePrediction.from_pretrained.example",$$slots:{default:[g3]},$$scope:{ctx:v}}}),Im=new Z({props:{title:"TFAutoModelForNextSentencePrediction",local:"transformers.TFAutoModelForNextSentencePrediction",headingTag:"h3"}}),Nm=new j({props:{name:"class transformers.TFAutoModelForNextSentencePrediction",anchor:"transformers.TFAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L684"}}),Xm=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForNextSentencePrediction.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForNextSentencePrediction">TFBertForNextSentencePrediction</a> (BERT model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>TFMobileBertForNextSentencePrediction</code> (MobileBERT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),xi=new k({props:{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_config.example",$$slots:{default:[p3]},$$scope:{ctx:v}}}),qm=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),$i=new k({props:{anchor:"transformers.TFAutoModelForNextSentencePrediction.from_pretrained.example",$$slots:{default:[u3]},$$scope:{ctx:v}}}),Qm=new Z({props:{title:"FlaxAutoModelForNextSentencePrediction",local:"transformers.FlaxAutoModelForNextSentencePrediction",headingTag:"h3"}}),Hm=new j({props:{name:"class transformers.FlaxAutoModelForNextSentencePrediction",anchor:"transformers.FlaxAutoModelForNextSentencePrediction",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L349"}}),Dm=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForNextSentencePrediction">FlaxBertForNextSentencePrediction</a> (BERT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ki=new k({props:{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_config.example",$$slots:{default:[h3]},$$scope:{ctx:v}}}),Ym=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Zi=new k({props:{anchor:"transformers.FlaxAutoModelForNextSentencePrediction.from_pretrained.example",$$slots:{default:[b3]},$$scope:{ctx:v}}}),zm=new Z({props:{title:"AutoModelForTokenClassification",local:"transformers.AutoModelForTokenClassification",headingTag:"h3"}}),Om=new j({props:{name:"class transformers.AutoModelForTokenClassification",anchor:"transformers.AutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1410"}}),Km=new j({props:{name:"from_config",anchor:"transformers.AutoModelForTokenClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTokenClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForTokenClassification">AlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForTokenClassification">BertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForTokenClassification">BigBirdForTokenClassification</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptConfig">BioGptConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/biogpt#transformers.BioGptForTokenClassification">BioGptForTokenClassification</a> (BioGpt model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomForTokenClassification">BloomForTokenClassification</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bros#transformers.BrosConfig">BrosConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bros#transformers.BrosForTokenClassification">BrosForTokenClassification</a> (BROS model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForTokenClassification">CamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineForTokenClassification">CanineForTokenClassification</a> (CANINE model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertForTokenClassification">ConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForTokenClassification">Data2VecTextForTokenClassification</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaForTokenClassification">DebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForTokenClassification">DebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>DistilBertForTokenClassification</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>ElectraForTokenClassification</code> (ELECTRA model)</li>
<li><code>ErnieConfig</code> configuration class: <code>ErnieForTokenClassification</code> (ERNIE model)</li>
<li><code>ErnieMConfig</code> configuration class: <code>ErnieMForTokenClassification</code> (ErnieM model)</li>
<li><code>EsmConfig</code> configuration class: <code>EsmForTokenClassification</code> (ESM model)</li>
<li><code>FNetConfig</code> configuration class: <code>FNetForTokenClassification</code> (FNet model)</li>
<li><code>FalconConfig</code> configuration class: <code>FalconForTokenClassification</code> (Falcon model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>FlaubertForTokenClassification</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>FunnelForTokenClassification</code> (Funnel Transformer model)</li>
<li><code>GPT2Config</code> configuration class: <code>GPT2ForTokenClassification</code> (OpenAI GPT-2 model)</li>
<li><code>GPTBigCodeConfig</code> configuration class: <code>GPTBigCodeForTokenClassification</code> (GPTBigCode model)</li>
<li><code>GPTNeoConfig</code> configuration class: <code>GPTNeoForTokenClassification</code> (GPT Neo model)</li>
<li><code>GPTNeoXConfig</code> configuration class: <code>GPTNeoXForTokenClassification</code> (GPT NeoX model)</li>
<li><code>IBertConfig</code> configuration class: <code>IBertForTokenClassification</code> (I-BERT model)</li>
<li><code>LayoutLMConfig</code> configuration class: <code>LayoutLMForTokenClassification</code> (LayoutLM model)</li>
<li><code>LayoutLMv2Config</code> configuration class: <code>LayoutLMv2ForTokenClassification</code> (LayoutLMv2 model)</li>
<li><code>LayoutLMv3Config</code> configuration class: <code>LayoutLMv3ForTokenClassification</code> (LayoutLMv3 model)</li>
<li><code>LiltConfig</code> configuration class: <code>LiltForTokenClassification</code> (LiLT model)</li>
<li><code>LongformerConfig</code> configuration class: <code>LongformerForTokenClassification</code> (Longformer model)</li>
<li><code>LukeConfig</code> configuration class: <code>LukeForTokenClassification</code> (LUKE model)</li>
<li><code>MPNetConfig</code> configuration class: <code>MPNetForTokenClassification</code> (MPNet model)</li>
<li><code>MT5Config</code> configuration class: <code>MT5ForTokenClassification</code> (MT5 model)</li>
<li><code>MarkupLMConfig</code> configuration class: <code>MarkupLMForTokenClassification</code> (MarkupLM model)</li>
<li><code>MegaConfig</code> configuration class: <code>MegaForTokenClassification</code> (MEGA model)</li>
<li><code>MegatronBertConfig</code> configuration class: <code>MegatronBertForTokenClassification</code> (Megatron-BERT model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>MobileBertForTokenClassification</code> (MobileBERT model)</li>
<li><code>MptConfig</code> configuration class: <code>MptForTokenClassification</code> (MPT model)</li>
<li><code>MraConfig</code> configuration class: <code>MraForTokenClassification</code> (MRA model)</li>
<li><code>NezhaConfig</code> configuration class: <code>NezhaForTokenClassification</code> (Nezha model)</li>
<li><code>NystromformerConfig</code> configuration class: <code>NystromformerForTokenClassification</code> (Nystr&#xF6;mformer model)</li>
<li><code>PhiConfig</code> configuration class: <code>PhiForTokenClassification</code> (Phi model)</li>
<li><code>QDQBertConfig</code> configuration class: <code>QDQBertForTokenClassification</code> (QDQBert model)</li>
<li><code>RemBertConfig</code> configuration class: <code>RemBertForTokenClassification</code> (RemBERT model)</li>
<li><code>RoCBertConfig</code> configuration class: <code>RoCBertForTokenClassification</code> (RoCBert model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>RoFormerForTokenClassification</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>RobertaForTokenClassification</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>RobertaPreLayerNormForTokenClassification</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>SqueezeBertConfig</code> configuration class: <code>SqueezeBertForTokenClassification</code> (SqueezeBERT model)</li>
<li><code>T5Config</code> configuration class: <code>T5ForTokenClassification</code> (T5 model)</li>
<li><code>UMT5Config</code> configuration class: <code>UMT5ForTokenClassification</code> (UMT5 model)</li>
<li><code>XLMConfig</code> configuration class: <code>XLMForTokenClassification</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>XLMRobertaForTokenClassification</code> (XLM-RoBERTa model)</li>
<li><code>XLMRobertaXLConfig</code> configuration class: <code>XLMRobertaXLForTokenClassification</code> (XLM-RoBERTa-XL model)</li>
<li><code>XLNetConfig</code> configuration class: <code>XLNetForTokenClassification</code> (XLNet model)</li>
<li><code>XmodConfig</code> configuration class: <code>XmodForTokenClassification</code> (X-MOD model)</li>
<li><code>YosoConfig</code> configuration class: <code>YosoForTokenClassification</code> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Li=new k({props:{anchor:"transformers.AutoModelForTokenClassification.from_config.example",$$slots:{default:[_3]},$$scope:{ctx:v}}}),ef=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForTokenClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Bi=new k({props:{anchor:"transformers.AutoModelForTokenClassification.from_pretrained.example",$$slots:{default:[M3]},$$scope:{ctx:v}}}),of=new Z({props:{title:"TFAutoModelForTokenClassification",local:"transformers.TFAutoModelForTokenClassification",headingTag:"h3"}}),tf=new j({props:{name:"class transformers.TFAutoModelForTokenClassification",anchor:"transformers.TFAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L668"}}),nf=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForTokenClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForTokenClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForTokenClassification">TFAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForTokenClassification">TFBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForTokenClassification">TFCamembertForTokenClassification</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertForTokenClassification">TFConvBertForTokenClassification</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.TFDebertaForTokenClassification">TFDebertaForTokenClassification</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2ForTokenClassification">TFDebertaV2ForTokenClassification</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>TFDistilBertForTokenClassification</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>TFElectraForTokenClassification</code> (ELECTRA model)</li>
<li><code>EsmConfig</code> configuration class: <code>TFEsmForTokenClassification</code> (ESM model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>TFFlaubertForTokenClassification</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>TFFunnelForTokenClassification</code> (Funnel Transformer model)</li>
<li><code>LayoutLMConfig</code> configuration class: <code>TFLayoutLMForTokenClassification</code> (LayoutLM model)</li>
<li><code>LayoutLMv3Config</code> configuration class: <code>TFLayoutLMv3ForTokenClassification</code> (LayoutLMv3 model)</li>
<li><code>LongformerConfig</code> configuration class: <code>TFLongformerForTokenClassification</code> (Longformer model)</li>
<li><code>MPNetConfig</code> configuration class: <code>TFMPNetForTokenClassification</code> (MPNet model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>TFMobileBertForTokenClassification</code> (MobileBERT model)</li>
<li><code>RemBertConfig</code> configuration class: <code>TFRemBertForTokenClassification</code> (RemBERT model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>TFRoFormerForTokenClassification</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>TFRobertaForTokenClassification</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>TFRobertaPreLayerNormForTokenClassification</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>XLMConfig</code> configuration class: <code>TFXLMForTokenClassification</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>TFXLMRobertaForTokenClassification</code> (XLM-RoBERTa model)</li>
<li><code>XLNetConfig</code> configuration class: <code>TFXLNetForTokenClassification</code> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ai=new k({props:{anchor:"transformers.TFAutoModelForTokenClassification.from_config.example",$$slots:{default:[T3]},$$scope:{ctx:v}}}),rf=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ri=new k({props:{anchor:"transformers.TFAutoModelForTokenClassification.from_pretrained.example",$$slots:{default:[y3]},$$scope:{ctx:v}}}),af=new Z({props:{title:"FlaxAutoModelForTokenClassification",local:"transformers.FlaxAutoModelForTokenClassification",headingTag:"h3"}}),sf=new j({props:{name:"class transformers.FlaxAutoModelForTokenClassification",anchor:"transformers.FlaxAutoModelForTokenClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L333"}}),lf=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForTokenClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForTokenClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForTokenClassification">FlaxAlbertForTokenClassification</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForTokenClassification">FlaxBertForTokenClassification</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForTokenClassification">FlaxBigBirdForTokenClassification</a> (BigBird model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>FlaxDistilBertForTokenClassification</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>FlaxElectraForTokenClassification</code> (ELECTRA model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>FlaxRoFormerForTokenClassification</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>FlaxRobertaForTokenClassification</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>FlaxRobertaPreLayerNormForTokenClassification</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>FlaxXLMRobertaForTokenClassification</code> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Wi=new k({props:{anchor:"transformers.FlaxAutoModelForTokenClassification.from_config.example",$$slots:{default:[F3]},$$scope:{ctx:v}}}),df=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ji=new k({props:{anchor:"transformers.FlaxAutoModelForTokenClassification.from_pretrained.example",$$slots:{default:[v3]},$$scope:{ctx:v}}}),cf=new Z({props:{title:"AutoModelForQuestionAnswering",local:"transformers.AutoModelForQuestionAnswering",headingTag:"h3"}}),mf=new j({props:{name:"class transformers.AutoModelForQuestionAnswering",anchor:"transformers.AutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1370"}}),ff=new j({props:{name:"from_config",anchor:"transformers.AutoModelForQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertForQuestionAnswering">AlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartForQuestionAnswering">BartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertForQuestionAnswering">BertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdForQuestionAnswering">BigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusConfig">BigBirdPegasusConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bigbird_pegasus#transformers.BigBirdPegasusForQuestionAnswering">BigBirdPegasusForQuestionAnswering</a> (BigBird-Pegasus model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomConfig">BloomConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bloom#transformers.BloomForQuestionAnswering">BloomForQuestionAnswering</a> (BLOOM model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertForQuestionAnswering">CamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineConfig">CanineConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/canine#transformers.CanineForQuestionAnswering">CanineForQuestionAnswering</a> (CANINE model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertForQuestionAnswering">ConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextConfig">Data2VecTextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecTextForQuestionAnswering">Data2VecTextForQuestionAnswering</a> (Data2VecText model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaForQuestionAnswering">DebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2ForQuestionAnswering">DebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>DistilBertForQuestionAnswering</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>ElectraForQuestionAnswering</code> (ELECTRA model)</li>
<li><code>ErnieConfig</code> configuration class: <code>ErnieForQuestionAnswering</code> (ERNIE model)</li>
<li><code>ErnieMConfig</code> configuration class: <code>ErnieMForQuestionAnswering</code> (ErnieM model)</li>
<li><code>FNetConfig</code> configuration class: <code>FNetForQuestionAnswering</code> (FNet model)</li>
<li><code>FalconConfig</code> configuration class: <code>FalconForQuestionAnswering</code> (Falcon model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>FlaubertForQuestionAnsweringSimple</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>FunnelForQuestionAnswering</code> (Funnel Transformer model)</li>
<li><code>GPT2Config</code> configuration class: <code>GPT2ForQuestionAnswering</code> (OpenAI GPT-2 model)</li>
<li><code>GPTJConfig</code> configuration class: <code>GPTJForQuestionAnswering</code> (GPT-J model)</li>
<li><code>GPTNeoConfig</code> configuration class: <code>GPTNeoForQuestionAnswering</code> (GPT Neo model)</li>
<li><code>GPTNeoXConfig</code> configuration class: <code>GPTNeoXForQuestionAnswering</code> (GPT NeoX model)</li>
<li><code>IBertConfig</code> configuration class: <code>IBertForQuestionAnswering</code> (I-BERT model)</li>
<li><code>LEDConfig</code> configuration class: <code>LEDForQuestionAnswering</code> (LED model)</li>
<li><code>LayoutLMv2Config</code> configuration class: <code>LayoutLMv2ForQuestionAnswering</code> (LayoutLMv2 model)</li>
<li><code>LayoutLMv3Config</code> configuration class: <code>LayoutLMv3ForQuestionAnswering</code> (LayoutLMv3 model)</li>
<li><code>LiltConfig</code> configuration class: <code>LiltForQuestionAnswering</code> (LiLT model)</li>
<li><code>LlamaConfig</code> configuration class: <code>LlamaForQuestionAnswering</code> (LLaMA model)</li>
<li><code>LongformerConfig</code> configuration class: <code>LongformerForQuestionAnswering</code> (Longformer model)</li>
<li><code>LukeConfig</code> configuration class: <code>LukeForQuestionAnswering</code> (LUKE model)</li>
<li><code>LxmertConfig</code> configuration class: <code>LxmertForQuestionAnswering</code> (LXMERT model)</li>
<li><code>MBartConfig</code> configuration class: <code>MBartForQuestionAnswering</code> (mBART model)</li>
<li><code>MPNetConfig</code> configuration class: <code>MPNetForQuestionAnswering</code> (MPNet model)</li>
<li><code>MT5Config</code> configuration class: <code>MT5ForQuestionAnswering</code> (MT5 model)</li>
<li><code>MarkupLMConfig</code> configuration class: <code>MarkupLMForQuestionAnswering</code> (MarkupLM model)</li>
<li><code>MegaConfig</code> configuration class: <code>MegaForQuestionAnswering</code> (MEGA model)</li>
<li><code>MegatronBertConfig</code> configuration class: <code>MegatronBertForQuestionAnswering</code> (Megatron-BERT model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>MobileBertForQuestionAnswering</code> (MobileBERT model)</li>
<li><code>MptConfig</code> configuration class: <code>MptForQuestionAnswering</code> (MPT model)</li>
<li><code>MraConfig</code> configuration class: <code>MraForQuestionAnswering</code> (MRA model)</li>
<li><code>MvpConfig</code> configuration class: <code>MvpForQuestionAnswering</code> (MVP model)</li>
<li><code>NezhaConfig</code> configuration class: <code>NezhaForQuestionAnswering</code> (Nezha model)</li>
<li><code>NystromformerConfig</code> configuration class: <code>NystromformerForQuestionAnswering</code> (Nystr&#xF6;mformer model)</li>
<li><code>OPTConfig</code> configuration class: <code>OPTForQuestionAnswering</code> (OPT model)</li>
<li><code>QDQBertConfig</code> configuration class: <code>QDQBertForQuestionAnswering</code> (QDQBert model)</li>
<li><code>ReformerConfig</code> configuration class: <code>ReformerForQuestionAnswering</code> (Reformer model)</li>
<li><code>RemBertConfig</code> configuration class: <code>RemBertForQuestionAnswering</code> (RemBERT model)</li>
<li><code>RoCBertConfig</code> configuration class: <code>RoCBertForQuestionAnswering</code> (RoCBert model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>RoFormerForQuestionAnswering</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>RobertaForQuestionAnswering</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>RobertaPreLayerNormForQuestionAnswering</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>SplinterConfig</code> configuration class: <code>SplinterForQuestionAnswering</code> (Splinter model)</li>
<li><code>SqueezeBertConfig</code> configuration class: <code>SqueezeBertForQuestionAnswering</code> (SqueezeBERT model)</li>
<li><code>T5Config</code> configuration class: <code>T5ForQuestionAnswering</code> (T5 model)</li>
<li><code>UMT5Config</code> configuration class: <code>UMT5ForQuestionAnswering</code> (UMT5 model)</li>
<li><code>XLMConfig</code> configuration class: <code>XLMForQuestionAnsweringSimple</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>XLMRobertaForQuestionAnswering</code> (XLM-RoBERTa model)</li>
<li><code>XLMRobertaXLConfig</code> configuration class: <code>XLMRobertaXLForQuestionAnswering</code> (XLM-RoBERTa-XL model)</li>
<li><code>XLNetConfig</code> configuration class: <code>XLNetForQuestionAnsweringSimple</code> (XLNet model)</li>
<li><code>XmodConfig</code> configuration class: <code>XmodForQuestionAnswering</code> (X-MOD model)</li>
<li><code>YosoConfig</code> configuration class: <code>YosoForQuestionAnswering</code> (YOSO model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Vi=new k({props:{anchor:"transformers.AutoModelForQuestionAnswering.from_config.example",$$slots:{default:[C3]},$$scope:{ctx:v}}}),gf=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Gi=new k({props:{anchor:"transformers.AutoModelForQuestionAnswering.from_pretrained.example",$$slots:{default:[w3]},$$scope:{ctx:v}}}),pf=new Z({props:{title:"TFAutoModelForQuestionAnswering",local:"transformers.TFAutoModelForQuestionAnswering",headingTag:"h3"}}),uf=new j({props:{name:"class transformers.TFAutoModelForQuestionAnswering",anchor:"transformers.TFAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L639"}}),hf=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.TFAlbertForQuestionAnswering">TFAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.TFBertForQuestionAnswering">TFBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/camembert#transformers.CamembertConfig">CamembertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/camembert#transformers.TFCamembertForQuestionAnswering">TFCamembertForQuestionAnswering</a> (CamemBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convbert#transformers.ConvBertConfig">ConvBertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convbert#transformers.TFConvBertForQuestionAnswering">TFConvBertForQuestionAnswering</a> (ConvBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta#transformers.DebertaConfig">DebertaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta#transformers.TFDebertaForQuestionAnswering">TFDebertaForQuestionAnswering</a> (DeBERTa model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.DebertaV2Config">DebertaV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deberta-v2#transformers.TFDebertaV2ForQuestionAnswering">TFDebertaV2ForQuestionAnswering</a> (DeBERTa-v2 model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>TFDistilBertForQuestionAnswering</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>TFElectraForQuestionAnswering</code> (ELECTRA model)</li>
<li><code>FlaubertConfig</code> configuration class: <code>TFFlaubertForQuestionAnsweringSimple</code> (FlauBERT model)</li>
<li><code>FunnelConfig</code> configuration class: <code>TFFunnelForQuestionAnswering</code> (Funnel Transformer model)</li>
<li><code>GPTJConfig</code> configuration class: <code>TFGPTJForQuestionAnswering</code> (GPT-J model)</li>
<li><code>LayoutLMv3Config</code> configuration class: <code>TFLayoutLMv3ForQuestionAnswering</code> (LayoutLMv3 model)</li>
<li><code>LongformerConfig</code> configuration class: <code>TFLongformerForQuestionAnswering</code> (Longformer model)</li>
<li><code>MPNetConfig</code> configuration class: <code>TFMPNetForQuestionAnswering</code> (MPNet model)</li>
<li><code>MobileBertConfig</code> configuration class: <code>TFMobileBertForQuestionAnswering</code> (MobileBERT model)</li>
<li><code>RemBertConfig</code> configuration class: <code>TFRemBertForQuestionAnswering</code> (RemBERT model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>TFRoFormerForQuestionAnswering</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>TFRobertaForQuestionAnswering</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>TFRobertaPreLayerNormForQuestionAnswering</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>XLMConfig</code> configuration class: <code>TFXLMForQuestionAnsweringSimple</code> (XLM model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>TFXLMRobertaForQuestionAnswering</code> (XLM-RoBERTa model)</li>
<li><code>XLNetConfig</code> configuration class: <code>TFXLNetForQuestionAnsweringSimple</code> (XLNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ei=new k({props:{anchor:"transformers.TFAutoModelForQuestionAnswering.from_config.example",$$slots:{default:[j3]},$$scope:{ctx:v}}}),bf=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Pi=new k({props:{anchor:"transformers.TFAutoModelForQuestionAnswering.from_pretrained.example",$$slots:{default:[x3]},$$scope:{ctx:v}}}),_f=new Z({props:{title:"FlaxAutoModelForQuestionAnswering",local:"transformers.FlaxAutoModelForQuestionAnswering",headingTag:"h3"}}),Mf=new j({props:{name:"class transformers.FlaxAutoModelForQuestionAnswering",anchor:"transformers.FlaxAutoModelForQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L326"}}),Tf=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/albert#transformers.AlbertConfig">AlbertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/albert#transformers.FlaxAlbertForQuestionAnswering">FlaxAlbertForQuestionAnswering</a> (ALBERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bart#transformers.BartConfig">BartConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bart#transformers.FlaxBartForQuestionAnswering">FlaxBartForQuestionAnswering</a> (BART model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertConfig">BertConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bert#transformers.FlaxBertForQuestionAnswering">FlaxBertForQuestionAnswering</a> (BERT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.BigBirdConfig">BigBirdConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/big_bird#transformers.FlaxBigBirdForQuestionAnswering">FlaxBigBirdForQuestionAnswering</a> (BigBird model)</li>
<li><code>DistilBertConfig</code> configuration class: <code>FlaxDistilBertForQuestionAnswering</code> (DistilBERT model)</li>
<li><code>ElectraConfig</code> configuration class: <code>FlaxElectraForQuestionAnswering</code> (ELECTRA model)</li>
<li><code>MBartConfig</code> configuration class: <code>FlaxMBartForQuestionAnswering</code> (mBART model)</li>
<li><code>RoFormerConfig</code> configuration class: <code>FlaxRoFormerForQuestionAnswering</code> (RoFormer model)</li>
<li><code>RobertaConfig</code> configuration class: <code>FlaxRobertaForQuestionAnswering</code> (RoBERTa model)</li>
<li><code>RobertaPreLayerNormConfig</code> configuration class: <code>FlaxRobertaPreLayerNormForQuestionAnswering</code> (RoBERTa-PreLayerNorm model)</li>
<li><code>XLMRobertaConfig</code> configuration class: <code>FlaxXLMRobertaForQuestionAnswering</code> (XLM-RoBERTa model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Si=new k({props:{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_config.example",$$slots:{default:[$3]},$$scope:{ctx:v}}}),yf=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ui=new k({props:{anchor:"transformers.FlaxAutoModelForQuestionAnswering.from_pretrained.example",$$slots:{default:[k3]},$$scope:{ctx:v}}}),Ff=new Z({props:{title:"AutoModelForTextEncoding",local:"transformers.AutoModelForTextEncoding",headingTag:"h3"}}),Cf=new j({props:{name:"class transformers.AutoModelForTextEncoding",anchor:"transformers.AutoModelForTextEncoding",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1306"}}),wf=new Z({props:{title:"TFAutoModelForTextEncoding",local:"transformers.TFAutoModelForTextEncoding",headingTag:"h3"}}),xf=new j({props:{name:"class transformers.TFAutoModelForTextEncoding",anchor:"transformers.TFAutoModelForTextEncoding",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L527"}}),$f=new Z({props:{title:"Computer vision",local:"computer-vision",headingTag:"h2"}}),Zf=new Z({props:{title:"AutoModelForDepthEstimation",local:"transformers.AutoModelForDepthEstimation",headingTag:"h3"}}),Lf=new j({props:{name:"class transformers.AutoModelForDepthEstimation",anchor:"transformers.AutoModelForDepthEstimation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1499"}}),Bf=new j({props:{name:"from_config",anchor:"transformers.AutoModelForDepthEstimation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDepthEstimation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>DPTConfig</code> configuration class: <code>DPTForDepthEstimation</code> (DPT model)</li>
<li><code>DepthAnythingConfig</code> configuration class: <code>DepthAnythingForDepthEstimation</code> (Depth Anything model)</li>
<li><code>GLPNConfig</code> configuration class: <code>GLPNForDepthEstimation</code> (GLPN model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ii=new k({props:{anchor:"transformers.AutoModelForDepthEstimation.from_config.example",$$slots:{default:[Z3]},$$scope:{ctx:v}}}),Af=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForDepthEstimation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ni=new k({props:{anchor:"transformers.AutoModelForDepthEstimation.from_pretrained.example",$$slots:{default:[L3]},$$scope:{ctx:v}}}),Rf=new Z({props:{title:"AutoModelForImageClassification",local:"transformers.AutoModelForImageClassification",headingTag:"h3"}}),Wf=new j({props:{name:"class transformers.AutoModelForImageClassification",anchor:"transformers.AutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1433"}}),Jf=new j({props:{name:"from_config",anchor:"transformers.AutoModelForImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForImageClassification">BeitForImageClassification</a> (BEiT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/bit#transformers.BitConfig">BitConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/bit#transformers.BitForImageClassification">BitForImageClassification</a> (BiT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <code>CLIPForImageClassification</code> (CLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextForImageClassification">ConvNextForImageClassification</a> (ConvNeXT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification">ConvNextV2ForImageClassification</a> (ConvNeXTV2 model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtForImageClassification">CvtForImageClassification</a> (CvT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionForImageClassification">Data2VecVisionForImageClassification</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> or <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatConfig">DinatConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatForImageClassification">DinatForImageClassification</a> (DiNAT model)</li>
<li><code>Dinov2Config</code> configuration class: <code>Dinov2ForImageClassification</code> (DINOv2 model)</li>
<li><code>EfficientFormerConfig</code> configuration class: <code>EfficientFormerForImageClassification</code> or <code>EfficientFormerForImageClassificationWithTeacher</code> (EfficientFormer model)</li>
<li><code>EfficientNetConfig</code> configuration class: <code>EfficientNetForImageClassification</code> (EfficientNet model)</li>
<li><code>FocalNetConfig</code> configuration class: <code>FocalNetForImageClassification</code> (FocalNet model)</li>
<li><code>ImageGPTConfig</code> configuration class: <code>ImageGPTForImageClassification</code> (ImageGPT model)</li>
<li><code>LevitConfig</code> configuration class: <code>LevitForImageClassification</code> or <code>LevitForImageClassificationWithTeacher</code> (LeViT model)</li>
<li><code>MobileNetV1Config</code> configuration class: <code>MobileNetV1ForImageClassification</code> (MobileNetV1 model)</li>
<li><code>MobileNetV2Config</code> configuration class: <code>MobileNetV2ForImageClassification</code> (MobileNetV2 model)</li>
<li><code>MobileViTConfig</code> configuration class: <code>MobileViTForImageClassification</code> (MobileViT model)</li>
<li><code>MobileViTV2Config</code> configuration class: <code>MobileViTV2ForImageClassification</code> (MobileViTV2 model)</li>
<li><code>NatConfig</code> configuration class: <code>NatForImageClassification</code> (NAT model)</li>
<li><code>PerceiverConfig</code> configuration class: <code>PerceiverForImageClassificationLearned</code> or <code>PerceiverForImageClassificationFourier</code> or <code>PerceiverForImageClassificationConvProcessing</code> (Perceiver model)</li>
<li><code>PoolFormerConfig</code> configuration class: <code>PoolFormerForImageClassification</code> (PoolFormer model)</li>
<li><code>PvtConfig</code> configuration class: <code>PvtForImageClassification</code> (PVT model)</li>
<li><code>RegNetConfig</code> configuration class: <code>RegNetForImageClassification</code> (RegNet model)</li>
<li><code>ResNetConfig</code> configuration class: <code>ResNetForImageClassification</code> (ResNet model)</li>
<li><code>SegformerConfig</code> configuration class: <code>SegformerForImageClassification</code> (SegFormer model)</li>
<li><code>SiglipConfig</code> configuration class: <code>SiglipForImageClassification</code> (SigLIP model)</li>
<li><code>SwiftFormerConfig</code> configuration class: <code>SwiftFormerForImageClassification</code> (SwiftFormer model)</li>
<li><code>SwinConfig</code> configuration class: <code>SwinForImageClassification</code> (Swin Transformer model)</li>
<li><code>Swinv2Config</code> configuration class: <code>Swinv2ForImageClassification</code> (Swin Transformer V2 model)</li>
<li><code>VanConfig</code> configuration class: <code>VanForImageClassification</code> (VAN model)</li>
<li><code>ViTConfig</code> configuration class: <code>ViTForImageClassification</code> (ViT model)</li>
<li><code>ViTHybridConfig</code> configuration class: <code>ViTHybridForImageClassification</code> (ViT Hybrid model)</li>
<li><code>ViTMSNConfig</code> configuration class: <code>ViTMSNForImageClassification</code> (ViTMSN model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Xi=new k({props:{anchor:"transformers.AutoModelForImageClassification.from_config.example",$$slots:{default:[B3]},$$scope:{ctx:v}}}),Vf=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),qi=new k({props:{anchor:"transformers.AutoModelForImageClassification.from_pretrained.example",$$slots:{default:[A3]},$$scope:{ctx:v}}}),Gf=new Z({props:{title:"TFAutoModelForImageClassification",local:"transformers.TFAutoModelForImageClassification",headingTag:"h3"}}),Ef=new j({props:{name:"class transformers.TFAutoModelForImageClassification",anchor:"transformers.TFAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L578"}}),Pf=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextConfig">ConvNextConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.TFConvNextForImageClassification">TFConvNextForImageClassification</a> (ConvNeXT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.TFConvNextV2ForImageClassification">TFConvNextV2ForImageClassification</a> (ConvNeXTV2 model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig">CvtConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.TFCvtForImageClassification">TFCvtForImageClassification</a> (CvT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.TFData2VecVisionForImageClassification">TFData2VecVisionForImageClassification</a> (Data2VecVision model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTForImageClassification">TFDeiTForImageClassification</a> or <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTForImageClassificationWithTeacher">TFDeiTForImageClassificationWithTeacher</a> (DeiT model)</li>
<li><code>EfficientFormerConfig</code> configuration class: <code>TFEfficientFormerForImageClassification</code> or <code>TFEfficientFormerForImageClassificationWithTeacher</code> (EfficientFormer model)</li>
<li><code>MobileViTConfig</code> configuration class: <code>TFMobileViTForImageClassification</code> (MobileViT model)</li>
<li><code>RegNetConfig</code> configuration class: <code>TFRegNetForImageClassification</code> (RegNet model)</li>
<li><code>ResNetConfig</code> configuration class: <code>TFResNetForImageClassification</code> (ResNet model)</li>
<li><code>SegformerConfig</code> configuration class: <code>TFSegformerForImageClassification</code> (SegFormer model)</li>
<li><code>SwinConfig</code> configuration class: <code>TFSwinForImageClassification</code> (Swin Transformer model)</li>
<li><code>ViTConfig</code> configuration class: <code>TFViTForImageClassification</code> (ViT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Qi=new k({props:{anchor:"transformers.TFAutoModelForImageClassification.from_config.example",$$slots:{default:[R3]},$$scope:{ctx:v}}}),Sf=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Hi=new k({props:{anchor:"transformers.TFAutoModelForImageClassification.from_pretrained.example",$$slots:{default:[W3]},$$scope:{ctx:v}}}),Uf=new Z({props:{title:"FlaxAutoModelForImageClassification",local:"transformers.FlaxAutoModelForImageClassification",headingTag:"h3"}}),If=new j({props:{name:"class transformers.FlaxAutoModelForImageClassification",anchor:"transformers.FlaxAutoModelForImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L358"}}),Nf=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/beit#transformers.FlaxBeitForImageClassification">FlaxBeitForImageClassification</a> (BEiT model)</li>
<li><code>RegNetConfig</code> configuration class: <code>FlaxRegNetForImageClassification</code> (RegNet model)</li>
<li><code>ResNetConfig</code> configuration class: <code>FlaxResNetForImageClassification</code> (ResNet model)</li>
<li><code>ViTConfig</code> configuration class: <code>FlaxViTForImageClassification</code> (ViT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Di=new k({props:{anchor:"transformers.FlaxAutoModelForImageClassification.from_config.example",$$slots:{default:[J3]},$$scope:{ctx:v}}}),Xf=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Yi=new k({props:{anchor:"transformers.FlaxAutoModelForImageClassification.from_pretrained.example",$$slots:{default:[V3]},$$scope:{ctx:v}}}),qf=new Z({props:{title:"AutoModelForVideoClassification",local:"transformers.AutoModelForVideoClassification",headingTag:"h3"}}),Qf=new j({props:{name:"class transformers.AutoModelForVideoClassification",anchor:"transformers.AutoModelForVideoClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1506"}}),Hf=new j({props:{name:"from_config",anchor:"transformers.AutoModelForVideoClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVideoClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>TimesformerConfig</code> configuration class: <code>TimesformerForVideoClassification</code> (TimeSformer model)</li>
<li><code>VideoMAEConfig</code> configuration class: <code>VideoMAEForVideoClassification</code> (VideoMAE model)</li>
<li><code>VivitConfig</code> configuration class: <code>VivitForVideoClassification</code> (ViViT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),zi=new k({props:{anchor:"transformers.AutoModelForVideoClassification.from_config.example",$$slots:{default:[G3]},$$scope:{ctx:v}}}),Df=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForVideoClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Oi=new k({props:{anchor:"transformers.AutoModelForVideoClassification.from_pretrained.example",$$slots:{default:[E3]},$$scope:{ctx:v}}}),Yf=new Z({props:{title:"AutoModelForMaskedImageModeling",local:"transformers.AutoModelForMaskedImageModeling",headingTag:"h3"}}),zf=new j({props:{name:"class transformers.AutoModelForMaskedImageModeling",anchor:"transformers.AutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1571"}}),Of=new j({props:{name:"from_config",anchor:"transformers.AutoModelForMaskedImageModeling.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedImageModeling.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><code>FocalNetConfig</code> configuration class: <code>FocalNetForMaskedImageModeling</code> (FocalNet model)</li>
<li><code>SwinConfig</code> configuration class: <code>SwinForMaskedImageModeling</code> (Swin Transformer model)</li>
<li><code>Swinv2Config</code> configuration class: <code>Swinv2ForMaskedImageModeling</code> (Swin Transformer V2 model)</li>
<li><code>ViTConfig</code> configuration class: <code>ViTForMaskedImageModeling</code> (ViT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ki=new k({props:{anchor:"transformers.AutoModelForMaskedImageModeling.from_config.example",$$slots:{default:[P3]},$$scope:{ctx:v}}}),Kf=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),el=new k({props:{anchor:"transformers.AutoModelForMaskedImageModeling.from_pretrained.example",$$slots:{default:[S3]},$$scope:{ctx:v}}}),eg=new Z({props:{title:"TFAutoModelForMaskedImageModeling",local:"transformers.TFAutoModelForMaskedImageModeling",headingTag:"h3"}}),og=new j({props:{name:"class transformers.TFAutoModelForMaskedImageModeling",anchor:"transformers.TFAutoModelForMaskedImageModeling",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L569"}}),tg=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForMaskedImageModeling.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTForMaskedImageModeling">TFDeiTForMaskedImageModeling</a> (DeiT model)</li>
<li><code>SwinConfig</code> configuration class: <code>TFSwinForMaskedImageModeling</code> (Swin Transformer model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ol=new k({props:{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_config.example",$$slots:{default:[U3]},$$scope:{ctx:v}}}),ng=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),tl=new k({props:{anchor:"transformers.TFAutoModelForMaskedImageModeling.from_pretrained.example",$$slots:{default:[I3]},$$scope:{ctx:v}}}),rg=new Z({props:{title:"AutoModelForObjectDetection",local:"transformers.AutoModelForObjectDetection",headingTag:"h3"}}),ag=new j({props:{name:"class transformers.AutoModelForObjectDetection",anchor:"transformers.AutoModelForObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1483"}}),sg=new j({props:{name:"from_config",anchor:"transformers.AutoModelForObjectDetection.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForObjectDetection.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/conditional_detr#transformers.ConditionalDetrConfig">ConditionalDetrConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/conditional_detr#transformers.ConditionalDetrForObjectDetection">ConditionalDetrForObjectDetection</a> (Conditional DETR model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deformable_detr#transformers.DeformableDetrConfig">DeformableDetrConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deformable_detr#transformers.DeformableDetrForObjectDetection">DeformableDetrForObjectDetection</a> (Deformable DETR model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/deta#transformers.DetaConfig">DetaConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/deta#transformers.DetaForObjectDetection">DetaForObjectDetection</a> (DETA model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrForObjectDetection">DetrForObjectDetection</a> (DETR model)</li>
<li><code>TableTransformerConfig</code> configuration class: <code>TableTransformerForObjectDetection</code> (Table Transformer model)</li>
<li><code>YolosConfig</code> configuration class: <code>YolosForObjectDetection</code> (YOLOS model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),nl=new k({props:{anchor:"transformers.AutoModelForObjectDetection.from_config.example",$$slots:{default:[N3]},$$scope:{ctx:v}}}),ig=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForObjectDetection.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),rl=new k({props:{anchor:"transformers.AutoModelForObjectDetection.from_pretrained.example",$$slots:{default:[X3]},$$scope:{ctx:v}}}),lg=new Z({props:{title:"AutoModelForImageSegmentation",local:"transformers.AutoModelForImageSegmentation",headingTag:"h3"}}),dg=new j({props:{name:"class transformers.AutoModelForImageSegmentation",anchor:"transformers.AutoModelForImageSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1449"}}),cg=new j({props:{name:"from_config",anchor:"transformers.AutoModelForImageSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),al=new k({props:{anchor:"transformers.AutoModelForImageSegmentation.from_config.example",$$slots:{default:[q3]},$$scope:{ctx:v}}}),mg=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForImageSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),sl=new k({props:{anchor:"transformers.AutoModelForImageSegmentation.from_pretrained.example",$$slots:{default:[Q3]},$$scope:{ctx:v}}}),fg=new Z({props:{title:"AutoModelForImageToImage",local:"transformers.AutoModelForImageToImage",headingTag:"h3"}}),pg=new j({props:{name:"class transformers.AutoModelForImageToImage",anchor:"transformers.AutoModelForImageToImage",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1310"}}),ug=new Z({props:{title:"AutoModelForSemanticSegmentation",local:"transformers.AutoModelForSemanticSegmentation",headingTag:"h3"}}),hg=new j({props:{name:"class transformers.AutoModelForSemanticSegmentation",anchor:"transformers.AutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1456"}}),bg=new j({props:{name:"from_config",anchor:"transformers.AutoModelForSemanticSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSemanticSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitConfig">BeitConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/beit#transformers.BeitForSemanticSegmentation">BeitForSemanticSegmentation</a> (BEiT model)</li>
<li><code>DPTConfig</code> configuration class: <code>DPTForSemanticSegmentation</code> (DPT model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionForSemanticSegmentation">Data2VecVisionForSemanticSegmentation</a> (Data2VecVision model)</li>
<li><code>MobileNetV2Config</code> configuration class: <code>MobileNetV2ForSemanticSegmentation</code> (MobileNetV2 model)</li>
<li><code>MobileViTConfig</code> configuration class: <code>MobileViTForSemanticSegmentation</code> (MobileViT model)</li>
<li><code>MobileViTV2Config</code> configuration class: <code>MobileViTV2ForSemanticSegmentation</code> (MobileViTV2 model)</li>
<li><code>SegformerConfig</code> configuration class: <code>SegformerForSemanticSegmentation</code> (SegFormer model)</li>
<li><code>UperNetConfig</code> configuration class: <code>UperNetForSemanticSegmentation</code> (UPerNet model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),il=new k({props:{anchor:"transformers.AutoModelForSemanticSegmentation.from_config.example",$$slots:{default:[H3]},$$scope:{ctx:v}}}),_g=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ll=new k({props:{anchor:"transformers.AutoModelForSemanticSegmentation.from_pretrained.example",$$slots:{default:[D3]},$$scope:{ctx:v}}}),Mg=new Z({props:{title:"TFAutoModelForSemanticSegmentation",local:"transformers.TFAutoModelForSemanticSegmentation",headingTag:"h3"}}),Tg=new j({props:{name:"class transformers.TFAutoModelForSemanticSegmentation",anchor:"transformers.TFAutoModelForSemanticSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L596"}}),yg=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForSemanticSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecVisionConfig">Data2VecVisionConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.TFData2VecVisionForSemanticSegmentation">TFData2VecVisionForSemanticSegmentation</a> (Data2VecVision model)</li>
<li><code>MobileViTConfig</code> configuration class: <code>TFMobileViTForSemanticSegmentation</code> (MobileViT model)</li>
<li><code>SegformerConfig</code> configuration class: <code>TFSegformerForSemanticSegmentation</code> (SegFormer model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),dl=new k({props:{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_config.example",$$slots:{default:[Y3]},$$scope:{ctx:v}}}),Fg=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),cl=new k({props:{anchor:"transformers.TFAutoModelForSemanticSegmentation.from_pretrained.example",$$slots:{default:[z3]},$$scope:{ctx:v}}}),vg=new Z({props:{title:"AutoModelForInstanceSegmentation",local:"transformers.AutoModelForInstanceSegmentation",headingTag:"h3"}}),Cg=new j({props:{name:"class transformers.AutoModelForInstanceSegmentation",anchor:"transformers.AutoModelForInstanceSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1474"}}),wg=new j({props:{name:"from_config",anchor:"transformers.AutoModelForInstanceSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForInstanceSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>MaskFormerConfig</code> configuration class: <code>MaskFormerForInstanceSegmentation</code> (MaskFormer model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ml=new k({props:{anchor:"transformers.AutoModelForInstanceSegmentation.from_config.example",$$slots:{default:[O3]},$$scope:{ctx:v}}}),jg=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),fl=new k({props:{anchor:"transformers.AutoModelForInstanceSegmentation.from_pretrained.example",$$slots:{default:[K3]},$$scope:{ctx:v}}}),xg=new Z({props:{title:"AutoModelForUniversalSegmentation",local:"transformers.AutoModelForUniversalSegmentation",headingTag:"h3"}}),$g=new j({props:{name:"class transformers.AutoModelForUniversalSegmentation",anchor:"transformers.AutoModelForUniversalSegmentation",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1465"}}),kg=new j({props:{name:"from_config",anchor:"transformers.AutoModelForUniversalSegmentation.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForUniversalSegmentation.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrConfig">DetrConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/detr#transformers.DetrForSegmentation">DetrForSegmentation</a> (DETR model)</li>
<li><code>Mask2FormerConfig</code> configuration class: <code>Mask2FormerForUniversalSegmentation</code> (Mask2Former model)</li>
<li><code>MaskFormerConfig</code> configuration class: <code>MaskFormerForInstanceSegmentation</code> (MaskFormer model)</li>
<li><code>OneFormerConfig</code> configuration class: <code>OneFormerForUniversalSegmentation</code> (OneFormer model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),gl=new k({props:{anchor:"transformers.AutoModelForUniversalSegmentation.from_config.example",$$slots:{default:[eq]},$$scope:{ctx:v}}}),Zg=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),pl=new k({props:{anchor:"transformers.AutoModelForUniversalSegmentation.from_pretrained.example",$$slots:{default:[oq]},$$scope:{ctx:v}}}),Lg=new Z({props:{title:"AutoModelForZeroShotImageClassification",local:"transformers.AutoModelForZeroShotImageClassification",headingTag:"h3"}}),Bg=new j({props:{name:"class transformers.AutoModelForZeroShotImageClassification",anchor:"transformers.AutoModelForZeroShotImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1440"}}),Ag=new j({props:{name:"from_config",anchor:"transformers.AutoModelForZeroShotImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a> (ALIGN model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPConfig">AltCLIPConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> (AltCLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipModel">BlipModel</a> (BLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a> (CLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> (CLIPSeg model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/chinese_clip#transformers.ChineseCLIPConfig">ChineseCLIPConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/chinese_clip#transformers.ChineseCLIPModel">ChineseCLIPModel</a> (Chinese-CLIP model)</li>
<li><code>SiglipConfig</code> configuration class: <code>SiglipModel</code> (SigLIP model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),ul=new k({props:{anchor:"transformers.AutoModelForZeroShotImageClassification.from_config.example",$$slots:{default:[tq]},$$scope:{ctx:v}}}),Rg=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),hl=new k({props:{anchor:"transformers.AutoModelForZeroShotImageClassification.from_pretrained.example",$$slots:{default:[nq]},$$scope:{ctx:v}}}),Wg=new Z({props:{title:"TFAutoModelForZeroShotImageClassification",local:"transformers.TFAutoModelForZeroShotImageClassification",headingTag:"h3"}}),Jg=new j({props:{name:"class transformers.TFAutoModelForZeroShotImageClassification",anchor:"transformers.TFAutoModelForZeroShotImageClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L587"}}),Vg=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blip#transformers.TFBlipModel">TFBlipModel</a> (BLIP model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> (CLIP model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),bl=new k({props:{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_config.example",$$slots:{default:[rq]},$$scope:{ctx:v}}}),Gg=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),_l=new k({props:{anchor:"transformers.TFAutoModelForZeroShotImageClassification.from_pretrained.example",$$slots:{default:[aq]},$$scope:{ctx:v}}}),Eg=new Z({props:{title:"AutoModelForZeroShotObjectDetection",local:"transformers.AutoModelForZeroShotObjectDetection",headingTag:"h3"}}),Pg=new j({props:{name:"class transformers.AutoModelForZeroShotObjectDetection",anchor:"transformers.AutoModelForZeroShotObjectDetection",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1490"}}),Sg=new j({props:{name:"from_config",anchor:"transformers.AutoModelForZeroShotObjectDetection.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>OwlViTConfig</code> configuration class: <code>OwlViTForObjectDetection</code> (OWL-ViT model)</li>
<li><code>Owlv2Config</code> configuration class: <code>Owlv2ForObjectDetection</code> (OWLv2 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ml=new k({props:{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_config.example",$$slots:{default:[sq]},$$scope:{ctx:v}}}),Ug=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Tl=new k({props:{anchor:"transformers.AutoModelForZeroShotObjectDetection.from_pretrained.example",$$slots:{default:[iq]},$$scope:{ctx:v}}}),Ig=new Z({props:{title:"Audio",local:"audio",headingTag:"h2"}}),Xg=new Z({props:{title:"AutoModelForAudioClassification",local:"transformers.AutoModelForAudioClassification",headingTag:"h3"}}),qg=new j({props:{name:"class transformers.AutoModelForAudioClassification",anchor:"transformers.AutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1520"}}),Qg=new j({props:{name:"from_config",anchor:"transformers.AutoModelForAudioClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification">ASTForAudioClassification</a> (Audio Spectrogram Transformer model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioForSequenceClassification">Data2VecAudioForSequenceClassification</a> (Data2VecAudio model)</li>
<li><code>HubertConfig</code> configuration class: <code>HubertForSequenceClassification</code> (Hubert model)</li>
<li><code>SEWConfig</code> configuration class: <code>SEWForSequenceClassification</code> (SEW model)</li>
<li><code>SEWDConfig</code> configuration class: <code>SEWDForSequenceClassification</code> (SEW-D model)</li>
<li><code>UniSpeechConfig</code> configuration class: <code>UniSpeechForSequenceClassification</code> (UniSpeech model)</li>
<li><code>UniSpeechSatConfig</code> configuration class: <code>UniSpeechSatForSequenceClassification</code> (UniSpeechSat model)</li>
<li><code>Wav2Vec2BertConfig</code> configuration class: <code>Wav2Vec2BertForSequenceClassification</code> (Wav2Vec2-BERT model)</li>
<li><code>Wav2Vec2Config</code> configuration class: <code>Wav2Vec2ForSequenceClassification</code> (Wav2Vec2 model)</li>
<li><code>Wav2Vec2ConformerConfig</code> configuration class: <code>Wav2Vec2ConformerForSequenceClassification</code> (Wav2Vec2-Conformer model)</li>
<li><code>WavLMConfig</code> configuration class: <code>WavLMForSequenceClassification</code> (WavLM model)</li>
<li><code>WhisperConfig</code> configuration class: <code>WhisperForAudioClassification</code> (Whisper model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),yl=new k({props:{anchor:"transformers.AutoModelForAudioClassification.from_config.example",$$slots:{default:[lq]},$$scope:{ctx:v}}}),Hg=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForAudioClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Fl=new k({props:{anchor:"transformers.AutoModelForAudioClassification.from_pretrained.example",$$slots:{default:[dq]},$$scope:{ctx:v}}}),Dg=new Z({props:{title:"AutoModelForAudioFrameClassification",local:"transformers.TFAutoModelForAudioClassification",headingTag:"h3"}}),Yg=new j({props:{name:"class transformers.TFAutoModelForAudioClassification",anchor:"transformers.TFAutoModelForAudioClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L538"}}),zg=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForAudioClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForAudioClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>Wav2Vec2Config</code> configuration class: <code>TFWav2Vec2ForSequenceClassification</code> (Wav2Vec2 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),vl=new k({props:{anchor:"transformers.TFAutoModelForAudioClassification.from_config.example",$$slots:{default:[cq]},$$scope:{ctx:v}}}),Og=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Cl=new k({props:{anchor:"transformers.TFAutoModelForAudioClassification.from_pretrained.example",$$slots:{default:[mq]},$$scope:{ctx:v}}}),Kg=new Z({props:{title:"TFAutoModelForAudioFrameClassification",local:"transformers.AutoModelForAudioFrameClassification",headingTag:"h3"}}),ep=new j({props:{name:"class transformers.AutoModelForAudioFrameClassification",anchor:"transformers.AutoModelForAudioFrameClassification",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1543"}}),op=new j({props:{name:"from_config",anchor:"transformers.AutoModelForAudioFrameClassification.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioFrameClassification.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioForAudioFrameClassification">Data2VecAudioForAudioFrameClassification</a> (Data2VecAudio model)</li>
<li><code>UniSpeechSatConfig</code> configuration class: <code>UniSpeechSatForAudioFrameClassification</code> (UniSpeechSat model)</li>
<li><code>Wav2Vec2BertConfig</code> configuration class: <code>Wav2Vec2BertForAudioFrameClassification</code> (Wav2Vec2-BERT model)</li>
<li><code>Wav2Vec2Config</code> configuration class: <code>Wav2Vec2ForAudioFrameClassification</code> (Wav2Vec2 model)</li>
<li><code>Wav2Vec2ConformerConfig</code> configuration class: <code>Wav2Vec2ConformerForAudioFrameClassification</code> (Wav2Vec2-Conformer model)</li>
<li><code>WavLMConfig</code> configuration class: <code>WavLMForAudioFrameClassification</code> (WavLM model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),wl=new k({props:{anchor:"transformers.AutoModelForAudioFrameClassification.from_config.example",$$slots:{default:[fq]},$$scope:{ctx:v}}}),tp=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),jl=new k({props:{anchor:"transformers.AutoModelForAudioFrameClassification.from_pretrained.example",$$slots:{default:[gq]},$$scope:{ctx:v}}}),np=new Z({props:{title:"AutoModelForCTC",local:"transformers.AutoModelForCTC",headingTag:"h3"}}),rp=new j({props:{name:"class transformers.AutoModelForCTC",anchor:"transformers.AutoModelForCTC",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1527"}}),ap=new j({props:{name:"from_config",anchor:"transformers.AutoModelForCTC.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCTC.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioForCTC">Data2VecAudioForCTC</a> (Data2VecAudio model)</li>
<li><code>HubertConfig</code> configuration class: <code>HubertForCTC</code> (Hubert model)</li>
<li><code>MCTCTConfig</code> configuration class: <code>MCTCTForCTC</code> (M-CTC-T model)</li>
<li><code>SEWConfig</code> configuration class: <code>SEWForCTC</code> (SEW model)</li>
<li><code>SEWDConfig</code> configuration class: <code>SEWDForCTC</code> (SEW-D model)</li>
<li><code>UniSpeechConfig</code> configuration class: <code>UniSpeechForCTC</code> (UniSpeech model)</li>
<li><code>UniSpeechSatConfig</code> configuration class: <code>UniSpeechSatForCTC</code> (UniSpeechSat model)</li>
<li><code>Wav2Vec2BertConfig</code> configuration class: <code>Wav2Vec2BertForCTC</code> (Wav2Vec2-BERT model)</li>
<li><code>Wav2Vec2Config</code> configuration class: <code>Wav2Vec2ForCTC</code> (Wav2Vec2 model)</li>
<li><code>Wav2Vec2ConformerConfig</code> configuration class: <code>Wav2Vec2ConformerForCTC</code> (Wav2Vec2-Conformer model)</li>
<li><code>WavLMConfig</code> configuration class: <code>WavLMForCTC</code> (WavLM model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),xl=new k({props:{anchor:"transformers.AutoModelForCTC.from_config.example",$$slots:{default:[pq]},$$scope:{ctx:v}}}),sp=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForCTC.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForCTC.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForCTC.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForCTC.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForCTC.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForCTC.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForCTC.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForCTC.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForCTC.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForCTC.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForCTC.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForCTC.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForCTC.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForCTC.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForCTC.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForCTC.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),$l=new k({props:{anchor:"transformers.AutoModelForCTC.from_pretrained.example",$$slots:{default:[uq]},$$scope:{ctx:v}}}),ip=new Z({props:{title:"AutoModelForSpeechSeq2Seq",local:"transformers.AutoModelForSpeechSeq2Seq",headingTag:"h3"}}),lp=new j({props:{name:"class transformers.AutoModelForSpeechSeq2Seq",anchor:"transformers.AutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1534"}}),dp=new j({props:{name:"from_config",anchor:"transformers.AutoModelForSpeechSeq2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>Pop2PianoConfig</code> configuration class: <code>Pop2PianoForConditionalGeneration</code> (Pop2Piano model)</li>
<li><code>SeamlessM4TConfig</code> configuration class: <code>SeamlessM4TForSpeechToText</code> (SeamlessM4T model)</li>
<li><code>SeamlessM4Tv2Config</code> configuration class: <code>SeamlessM4Tv2ForSpeechToText</code> (SeamlessM4Tv2 model)</li>
<li><code>Speech2TextConfig</code> configuration class: <code>Speech2TextForConditionalGeneration</code> (Speech2Text model)</li>
<li><code>SpeechEncoderDecoderConfig</code> configuration class: <code>SpeechEncoderDecoderModel</code> (Speech Encoder decoder model)</li>
<li><code>SpeechT5Config</code> configuration class: <code>SpeechT5ForSpeechToText</code> (SpeechT5 model)</li>
<li><code>WhisperConfig</code> configuration class: <code>WhisperForConditionalGeneration</code> (Whisper model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),kl=new k({props:{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_config.example",$$slots:{default:[hq]},$$scope:{ctx:v}}}),cp=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Zl=new k({props:{anchor:"transformers.AutoModelForSpeechSeq2Seq.from_pretrained.example",$$slots:{default:[bq]},$$scope:{ctx:v}}}),mp=new Z({props:{title:"TFAutoModelForSpeechSeq2Seq",local:"transformers.TFAutoModelForSpeechSeq2Seq",headingTag:"h3"}}),fp=new j({props:{name:"class transformers.TFAutoModelForSpeechSeq2Seq",anchor:"transformers.TFAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L693"}}),gp=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>Speech2TextConfig</code> configuration class: <code>TFSpeech2TextForConditionalGeneration</code> (Speech2Text model)</li>
<li><code>WhisperConfig</code> configuration class: <code>TFWhisperForConditionalGeneration</code> (Whisper model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ll=new k({props:{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_config.example",$$slots:{default:[_q]},$$scope:{ctx:v}}}),pp=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Bl=new k({props:{anchor:"transformers.TFAutoModelForSpeechSeq2Seq.from_pretrained.example",$$slots:{default:[Mq]},$$scope:{ctx:v}}}),up=new Z({props:{title:"FlaxAutoModelForSpeechSeq2Seq",local:"transformers.FlaxAutoModelForSpeechSeq2Seq",headingTag:"h3"}}),hp=new j({props:{name:"class transformers.FlaxAutoModelForSpeechSeq2Seq",anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L374"}}),bp=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>SpeechEncoderDecoderConfig</code> configuration class: <code>FlaxSpeechEncoderDecoderModel</code> (Speech Encoder decoder model)</li>
<li><code>WhisperConfig</code> configuration class: <code>FlaxWhisperForConditionalGeneration</code> (Whisper model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Al=new k({props:{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_config.example",$$slots:{default:[Tq]},$$scope:{ctx:v}}}),_p=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Rl=new k({props:{anchor:"transformers.FlaxAutoModelForSpeechSeq2Seq.from_pretrained.example",$$slots:{default:[yq]},$$scope:{ctx:v}}}),Mp=new Z({props:{title:"AutoModelForAudioXVector",local:"transformers.AutoModelForAudioXVector",headingTag:"h3"}}),Tp=new j({props:{name:"class transformers.AutoModelForAudioXVector",anchor:"transformers.AutoModelForAudioXVector",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1552"}}),yp=new j({props:{name:"from_config",anchor:"transformers.AutoModelForAudioXVector.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioXVector.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioConfig">Data2VecAudioConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/data2vec#transformers.Data2VecAudioForXVector">Data2VecAudioForXVector</a> (Data2VecAudio model)</li>
<li><code>UniSpeechSatConfig</code> configuration class: <code>UniSpeechSatForXVector</code> (UniSpeechSat model)</li>
<li><code>Wav2Vec2BertConfig</code> configuration class: <code>Wav2Vec2BertForXVector</code> (Wav2Vec2-BERT model)</li>
<li><code>Wav2Vec2Config</code> configuration class: <code>Wav2Vec2ForXVector</code> (Wav2Vec2 model)</li>
<li><code>Wav2Vec2ConformerConfig</code> configuration class: <code>Wav2Vec2ConformerForXVector</code> (Wav2Vec2-Conformer model)</li>
<li><code>WavLMConfig</code> configuration class: <code>WavLMForXVector</code> (WavLM model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Wl=new k({props:{anchor:"transformers.AutoModelForAudioXVector.from_config.example",$$slots:{default:[Fq]},$$scope:{ctx:v}}}),Fp=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForAudioXVector.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Jl=new k({props:{anchor:"transformers.AutoModelForAudioXVector.from_pretrained.example",$$slots:{default:[vq]},$$scope:{ctx:v}}}),vp=new Z({props:{title:"AutoModelForTextToSpectrogram",local:"transformers.AutoModelForTextToSpectrogram",headingTag:"h3"}}),wp=new j({props:{name:"class transformers.AutoModelForTextToSpectrogram",anchor:"transformers.AutoModelForTextToSpectrogram",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1556"}}),jp=new Z({props:{title:"AutoModelForTextToWaveform",local:"transformers.AutoModelForTextToWaveform",headingTag:"h3"}}),$p=new j({props:{name:"class transformers.AutoModelForTextToWaveform",anchor:"transformers.AutoModelForTextToWaveform",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1560"}}),kp=new Z({props:{title:"Multimodal",local:"multimodal",headingTag:"h2"}}),Lp=new Z({props:{title:"AutoModelForTableQuestionAnswering",local:"transformers.AutoModelForTableQuestionAnswering",headingTag:"h3"}}),Bp=new j({props:{name:"class transformers.AutoModelForTableQuestionAnswering",anchor:"transformers.AutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1377"}}),Ap=new j({props:{name:"from_config",anchor:"transformers.AutoModelForTableQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTableQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>TapasConfig</code> configuration class: <code>TapasForQuestionAnswering</code> (TAPAS model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Vl=new k({props:{anchor:"transformers.AutoModelForTableQuestionAnswering.from_config.example",$$slots:{default:[Cq]},$$scope:{ctx:v}}}),Rp=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Gl=new k({props:{anchor:"transformers.AutoModelForTableQuestionAnswering.from_pretrained.example",$$slots:{default:[wq]},$$scope:{ctx:v}}}),Wp=new Z({props:{title:"TFAutoModelForTableQuestionAnswering",local:"transformers.TFAutoModelForTableQuestionAnswering",headingTag:"h3"}}),Jp=new j({props:{name:"class transformers.TFAutoModelForTableQuestionAnswering",anchor:"transformers.TFAutoModelForTableQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L657"}}),Vp=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>TapasConfig</code> configuration class: <code>TFTapasForQuestionAnswering</code> (TAPAS model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),El=new k({props:{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_config.example",$$slots:{default:[jq]},$$scope:{ctx:v}}}),Gp=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Pl=new k({props:{anchor:"transformers.TFAutoModelForTableQuestionAnswering.from_pretrained.example",$$slots:{default:[xq]},$$scope:{ctx:v}}}),Ep=new Z({props:{title:"AutoModelForDocumentQuestionAnswering",local:"transformers.AutoModelForDocumentQuestionAnswering",headingTag:"h3"}}),Pp=new j({props:{name:"class transformers.AutoModelForDocumentQuestionAnswering",anchor:"transformers.AutoModelForDocumentQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1399"}}),Sp=new j({props:{name:"from_config",anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>LayoutLMConfig</code> configuration class: <code>LayoutLMForQuestionAnswering</code> (LayoutLM model)</li>
<li><code>LayoutLMv2Config</code> configuration class: <code>LayoutLMv2ForQuestionAnswering</code> (LayoutLMv2 model)</li>
<li><code>LayoutLMv3Config</code> configuration class: <code>LayoutLMv3ForQuestionAnswering</code> (LayoutLMv3 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Sl=new k({props:{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_config.example",$$slots:{default:[$q]},$$scope:{ctx:v}}}),Up=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ul=new k({props:{anchor:"transformers.AutoModelForDocumentQuestionAnswering.from_pretrained.example",$$slots:{default:[kq]},$$scope:{ctx:v}}}),Ip=new Z({props:{title:"TFAutoModelForDocumentQuestionAnswering",local:"transformers.TFAutoModelForDocumentQuestionAnswering",headingTag:"h3"}}),Np=new j({props:{name:"class transformers.TFAutoModelForDocumentQuestionAnswering",anchor:"transformers.TFAutoModelForDocumentQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L646"}}),Xp=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>LayoutLMConfig</code> configuration class: <code>TFLayoutLMForQuestionAnswering</code> (LayoutLM model)</li>
<li><code>LayoutLMv3Config</code> configuration class: <code>TFLayoutLMv3ForQuestionAnswering</code> (LayoutLMv3 model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Il=new k({props:{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_config.example",$$slots:{default:[Zq]},$$scope:{ctx:v}}}),qp=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Nl=new k({props:{anchor:"transformers.TFAutoModelForDocumentQuestionAnswering.from_pretrained.example",$$slots:{default:[Lq]},$$scope:{ctx:v}}}),Qp=new Z({props:{title:"AutoModelForVisualQuestionAnswering",local:"transformers.AutoModelForVisualQuestionAnswering",headingTag:"h3"}}),Hp=new j({props:{name:"class transformers.AutoModelForVisualQuestionAnswering",anchor:"transformers.AutoModelForVisualQuestionAnswering",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1388"}}),Dp=new j({props:{name:"from_config",anchor:"transformers.AutoModelForVisualQuestionAnswering.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li>
<li><code>ViltConfig</code> configuration class: <code>ViltForQuestionAnswering</code> (ViLT model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Xl=new k({props:{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_config.example",$$slots:{default:[Bq]},$$scope:{ctx:v}}}),Yp=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),ql=new k({props:{anchor:"transformers.AutoModelForVisualQuestionAnswering.from_pretrained.example",$$slots:{default:[Aq]},$$scope:{ctx:v}}}),zp=new Z({props:{title:"AutoModelForVision2Seq",local:"transformers.AutoModelForVision2Seq",headingTag:"h3"}}),Op=new j({props:{name:"class transformers.AutoModelForVision2Seq",anchor:"transformers.AutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_auto.py#L1513"}}),Kp=new j({props:{name:"from_config",anchor:"transformers.AutoModelForVision2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVision2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2Config">Blip2Config</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blip-2#transformers.Blip2ForConditionalGeneration">Blip2ForConditionalGeneration</a> (BLIP-2 model)</li>
<li><a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipForConditionalGeneration">BlipForConditionalGeneration</a> (BLIP model)</li>
<li><code>GitConfig</code> configuration class: <code>GitForCausalLM</code> (GIT model)</li>
<li><code>InstructBlipConfig</code> configuration class: <code>InstructBlipForConditionalGeneration</code> (InstructBLIP model)</li>
<li><code>Kosmos2Config</code> configuration class: <code>Kosmos2ForConditionalGeneration</code> (KOSMOS-2 model)</li>
<li><code>LlavaConfig</code> configuration class: <code>LlavaForConditionalGeneration</code> (LLaVa model)</li>
<li><code>Pix2StructConfig</code> configuration class: <code>Pix2StructForConditionalGeneration</code> (Pix2Struct model)</li>
<li><code>VipLlavaConfig</code> configuration class: <code>VipLlavaForConditionalGeneration</code> (VipLlava model)</li>
<li><code>VisionEncoderDecoderConfig</code> configuration class: <code>VisionEncoderDecoderModel</code> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Ql=new k({props:{anchor:"transformers.AutoModelForVision2Seq.from_config.example",$$slots:{default:[Rq]},$$scope:{ctx:v}}}),eu=new j({props:{name:"from_pretrained",anchor:"transformers.AutoModelForVision2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>tensorflow index checkpoint file</em> (e.g, <code>./tf_model/model.ckpt.index</code>). In
this case, <code>from_tf</code> should be set to <code>True</code> and a configuration object should be provided as
<code>config</code> argument. This loading path is slower than converting the TensorFlow checkpoint in a
PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.state_dict",description:`<strong>state_dict</strong> (<em>Dict[str, torch.Tensor]</em>, <em>optional</em>) &#x2014;
A state dictionary to use instead of a state dictionary loaded from saved weights file.</p>
<p>This option can be used if you want to create a model from a pretrained configuration but load your own
weights. In this case though, you should check if using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> is not a simpler option.`,name:"state_dict"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.from_tf",description:`<strong>from_tf</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a TensorFlow checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_tf"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Hl=new k({props:{anchor:"transformers.AutoModelForVision2Seq.from_pretrained.example",$$slots:{default:[Wq]},$$scope:{ctx:v}}}),ou=new Z({props:{title:"TFAutoModelForVision2Seq",local:"transformers.TFAutoModelForVision2Seq",headingTag:"h3"}}),tu=new j({props:{name:"class transformers.TFAutoModelForVision2Seq",anchor:"transformers.TFAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_tf_auto.py#L605"}}),nu=new j({props:{name:"from_config",anchor:"transformers.TFAutoModelForVision2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForVision2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><a href="/docs/transformers/main/ja/model_doc/blip#transformers.BlipConfig">BlipConfig</a> configuration class: <a href="/docs/transformers/main/ja/model_doc/blip#transformers.TFBlipForConditionalGeneration">TFBlipForConditionalGeneration</a> (BLIP model)</li>
<li><code>VisionEncoderDecoderConfig</code> configuration class: <code>TFVisionEncoderDecoderModel</code> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),Dl=new k({props:{anchor:"transformers.TFAutoModelForVision2Seq.from_config.example",$$slots:{default:[Jq]},$$scope:{ctx:v}}}),ru=new j({props:{name:"from_pretrained",anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Yl=new k({props:{anchor:"transformers.TFAutoModelForVision2Seq.from_pretrained.example",$$slots:{default:[Vq]},$$scope:{ctx:v}}}),au=new Z({props:{title:"FlaxAutoModelForVision2Seq",local:"transformers.FlaxAutoModelForVision2Seq",headingTag:"h3"}}),su=new j({props:{name:"class transformers.FlaxAutoModelForVision2Seq",anchor:"transformers.FlaxAutoModelForVision2Seq",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/modeling_flax_auto.py#L367"}}),iu=new j({props:{name:"from_config",anchor:"transformers.FlaxAutoModelForVision2Seq.from_config",parameters:[{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForVision2Seq.from_config.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>) &#x2014;
The model class to instantiate is selected based on the configuration class:</p>
<ul>
<li><code>VisionEncoderDecoderConfig</code> configuration class: <code>FlaxVisionEncoderDecoderModel</code> (Vision Encoder decoder model)</li>
</ul>`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L411"}}),zl=new k({props:{anchor:"transformers.FlaxAutoModelForVision2Seq.from_config.example",$$slots:{default:[Gq]},$$scope:{ctx:v}}}),lu=new j({props:{name:"from_pretrained",anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained",parameters:[{name:"*model_args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Can be either:</p>
<ul>
<li>A string, the <em>model id</em> of a pretrained model hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing model weights saved using
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a>, e.g., <code>./my_model_directory/</code>.</li>
<li>A path or url to a <em>PyTorch state_dict save file</em> (e.g, <code>./pt_model/pytorch_model.bin</code>). In this
case, <code>from_pt</code> should be set to <code>True</code> and a configuration object should be provided as <code>config</code>
argument. This loading path is slower than converting the PyTorch model in a TensorFlow model
using the provided conversion scripts and loading the TensorFlow model afterwards.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.model_args",description:`<strong>model_args</strong> (additional positional arguments, <em>optional</em>) &#x2014;
Will be passed along to the underlying model <code>__init__()</code> method.`,name:"model_args"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a>, <em>optional</em>) &#x2014;
Configuration for the model to use instead of an automatically loaded configuration. Configuration can
be automatically loaded when:</p>
<ul>
<li>The model is a model provided by the library (loaded with the <em>model id</em> string of a pretrained
model).</li>
<li>The model was saved using <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.save_pretrained">save_pretrained()</a> and is reloaded by supplying the
save directory.</li>
<li>The model is loaded by supplying a local directory as <code>pretrained_model_name_or_path</code> and a
configuration JSON file named <em>config.json</em> is found in the directory.</li>
</ul>`,name:"config"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model configuration should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.from_pt",description:`<strong>from_pt</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Load the model weights from a PyTorch checkpoint save file (see docstring of
<code>pretrained_model_name_or_path</code> argument).`,name:"from_pt"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force the (re-)download of the model weights and configuration files, overriding the
cached versions if they exist.`,name:"force_download"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received files. Will attempt to resume the download if such a
file exists.`,name:"resume_download"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}</code>. The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.output_loading_info(bool,",description:`<strong>output_loading_info(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.`,name:"output_loading_info(bool,"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.local_files_only(bool,",description:`<strong>local_files_only(<code>bool</code>,</strong> <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to only look at local files (e.g., not try downloading the model).`,name:"local_files_only(bool,"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.trust_remote_code",description:`<strong>trust_remote_code</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to allow for custom models defined on the Hub in their own modeling files. This option
should only be set to <code>True</code> for repositories you trust and in which you have read the code, as it will
execute code present on the Hub on your local machine.`,name:"trust_remote_code"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.code_revision",description:`<strong>code_revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific revision to use for the code on the Hub, if the code leaves in a different repository than
the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based
system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any identifier
allowed by git.`,name:"code_revision"},{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.kwargs",description:`<strong>kwargs</strong> (additional keyword arguments, <em>optional</em>) &#x2014;
Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,
<code>output_attentions=True</code>). Behaves differently depending on whether a <code>config</code> is provided or
automatically loaded:</p>
<ul>
<li>If a configuration is provided with <code>config</code>, <code>**kwargs</code> will be directly passed to the
underlying model&#x2019;s <code>__init__</code> method (we assume all relevant updates to the configuration have
already been done)</li>
<li>If a configuration is not provided, <code>kwargs</code> will be first passed to the configuration class
initialization function (<a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig.from_pretrained">from_pretrained()</a>). Each key of <code>kwargs</code> that
corresponds to a configuration attribute will be used to override said attribute with the
supplied <code>kwargs</code> value. Remaining keys that do not correspond to any configuration attribute
will be passed to the underlying model&#x2019;s <code>__init__</code> function.</li>
</ul>`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/auto/auto_factory.py#L442"}}),Ol=new k({props:{anchor:"transformers.FlaxAutoModelForVision2Seq.from_pretrained.example",$$slots:{default:[Eq]},$$scope:{ctx:v}}}),{c(){n=i("meta"),F=r(),c=i("p"),s=r(),p(m.$$.fragment),e=r(),T=i("p"),T.innerHTML=SG,xC=r(),vd=i("p"),vd.innerHTML=UG,$C=r(),p(Cd.$$.fragment),kC=r(),wd=i("p"),wd.innerHTML=IG,ZC=r(),jd=i("p"),jd.innerHTML=NG,LC=r(),p(xd.$$.fragment),BC=r(),$d=i("p"),$d.innerHTML=XG,AC=r(),p(kd.$$.fragment),RC=r(),Zd=i("p"),Zd.textContent=qG,WC=r(),p(vs.$$.fragment),JC=r(),p(Ld.$$.fragment),VC=r(),ce=i("div"),p(Bd.$$.fragment),Vj=r(),pu=i("p"),pu.innerHTML=QG,Gj=r(),uu=i("p"),uu.innerHTML=HG,Ej=r(),Uo=i("div"),p(Ad.$$.fragment),Pj=r(),hu=i("p"),hu.textContent=DG,Sj=r(),bu=i("p"),bu.innerHTML=YG,Uj=r(),_u=i("ul"),_u.innerHTML=zG,Ij=r(),p(Cs.$$.fragment),Nj=r(),ws=i("div"),p(Rd.$$.fragment),Xj=r(),Mu=i("p"),Mu.textContent=OG,GC=r(),p(Wd.$$.fragment),EC=r(),me=i("div"),p(Jd.$$.fragment),qj=r(),Tu=i("p"),Tu.innerHTML=KG,Qj=r(),yu=i("p"),yu.innerHTML=eE,Hj=r(),Io=i("div"),p(Vd.$$.fragment),Dj=r(),Fu=i("p"),Fu.textContent=oE,Yj=r(),vu=i("p"),vu.innerHTML=tE,zj=r(),Cu=i("ul"),Cu.innerHTML=nE,Oj=r(),p(js.$$.fragment),Kj=r(),xs=i("div"),p(Gd.$$.fragment),ex=r(),wu=i("p"),wu.textContent=rE,PC=r(),p(Ed.$$.fragment),SC=r(),fe=i("div"),p(Pd.$$.fragment),ox=r(),ju=i("p"),ju.innerHTML=aE,tx=r(),xu=i("p"),xu.innerHTML=sE,nx=r(),L=i("div"),p(Sd.$$.fragment),rx=r(),$u=i("p"),$u.textContent=iE,ax=r(),ku=i("p"),ku.innerHTML=lE,sx=r(),Zu=i("ul"),Zu.innerHTML=dE,ix=r(),p($s.$$.fragment),lx=r(),p(ks.$$.fragment),dx=r(),Zs=i("div"),p(Ud.$$.fragment),cx=r(),Lu=i("p"),Lu.textContent=cE,UC=r(),p(Id.$$.fragment),IC=r(),ge=i("div"),p(Nd.$$.fragment),mx=r(),Bu=i("p"),Bu.innerHTML=mE,fx=r(),Au=i("p"),Au.innerHTML=fE,gx=r(),B=i("div"),p(Xd.$$.fragment),px=r(),Ru=i("p"),Ru.textContent=gE,ux=r(),Wu=i("p"),Wu.innerHTML=pE,hx=r(),Ju=i("ul"),Ju.innerHTML=uE,bx=r(),p(Ls.$$.fragment),_x=r(),p(Bs.$$.fragment),Mx=r(),As=i("div"),p(qd.$$.fragment),Tx=r(),Vu=i("p"),Vu.textContent=hE,NC=r(),p(Qd.$$.fragment),XC=r(),pe=i("div"),p(Hd.$$.fragment),yx=r(),Gu=i("p"),Gu.innerHTML=bE,Fx=r(),Eu=i("p"),Eu.innerHTML=_E,vx=r(),A=i("div"),p(Dd.$$.fragment),Cx=r(),Pu=i("p"),Pu.textContent=ME,wx=r(),Su=i("p"),Su.innerHTML=TE,jx=r(),Uu=i("ul"),Uu.innerHTML=yE,xx=r(),p(Rs.$$.fragment),$x=r(),p(Ws.$$.fragment),kx=r(),Js=i("div"),p(Yd.$$.fragment),Zx=r(),Iu=i("p"),Iu.textContent=FE,qC=r(),p(zd.$$.fragment),QC=r(),Od=i("p"),Od.textContent=vE,HC=r(),p(Kd.$$.fragment),DC=r(),ue=i("div"),p(ec.$$.fragment),Lx=r(),Nu=i("p"),Nu.innerHTML=CE,Bx=r(),Xu=i("p"),Xu.innerHTML=wE,Ax=r(),Vn=i("div"),p(oc.$$.fragment),Rx=r(),qu=i("p"),qu.textContent=jE,Wx=r(),Qu=i("p"),Qu.innerHTML=xE,Jx=r(),p(Vs.$$.fragment),Vx=r(),R=i("div"),p(tc.$$.fragment),Gx=r(),Hu=i("p"),Hu.textContent=$E,Ex=r(),Du=i("p"),Du.innerHTML=kE,Px=r(),Yu=i("ul"),Yu.innerHTML=ZE,Sx=r(),zu=i("p"),zu.innerHTML=LE,Ux=r(),p(Gs.$$.fragment),YC=r(),p(nc.$$.fragment),zC=r(),he=i("div"),p(rc.$$.fragment),Ix=r(),Ou=i("p"),Ou.innerHTML=BE,Nx=r(),Ku=i("p"),Ku.innerHTML=AE,Xx=r(),Gn=i("div"),p(ac.$$.fragment),qx=r(),eh=i("p"),eh.textContent=RE,Qx=r(),oh=i("p"),oh.innerHTML=WE,Hx=r(),p(Es.$$.fragment),Dx=r(),No=i("div"),p(sc.$$.fragment),Yx=r(),th=i("p"),th.textContent=JE,zx=r(),nh=i("p"),nh.innerHTML=VE,Ox=r(),rh=i("ul"),rh.innerHTML=GE,Kx=r(),p(Ps.$$.fragment),OC=r(),p(ic.$$.fragment),KC=r(),be=i("div"),p(lc.$$.fragment),e1=r(),ah=i("p"),ah.innerHTML=EE,o1=r(),sh=i("p"),sh.innerHTML=PE,t1=r(),En=i("div"),p(dc.$$.fragment),n1=r(),ih=i("p"),ih.textContent=SE,r1=r(),lh=i("p"),lh.innerHTML=UE,a1=r(),p(Ss.$$.fragment),s1=r(),Xo=i("div"),p(cc.$$.fragment),i1=r(),dh=i("p"),dh.textContent=IE,l1=r(),ch=i("p"),ch.innerHTML=NE,d1=r(),mh=i("ul"),mh.innerHTML=XE,c1=r(),p(Us.$$.fragment),e2=r(),p(mc.$$.fragment),o2=r(),fc=i("p"),fc.textContent=qE,t2=r(),p(gc.$$.fragment),n2=r(),_e=i("div"),p(pc.$$.fragment),m1=r(),fh=i("p"),fh.innerHTML=QE,f1=r(),gh=i("p"),gh.innerHTML=HE,g1=r(),Pn=i("div"),p(uc.$$.fragment),p1=r(),ph=i("p"),ph.textContent=DE,u1=r(),uh=i("p"),uh.innerHTML=YE,h1=r(),p(Is.$$.fragment),b1=r(),W=i("div"),p(hc.$$.fragment),_1=r(),hh=i("p"),hh.textContent=zE,M1=r(),bh=i("p"),bh.innerHTML=OE,T1=r(),_h=i("ul"),_h.innerHTML=KE,y1=r(),Mh=i("p"),Mh.innerHTML=eP,F1=r(),p(Ns.$$.fragment),r2=r(),p(bc.$$.fragment),a2=r(),Me=i("div"),p(_c.$$.fragment),v1=r(),Th=i("p"),Th.innerHTML=oP,C1=r(),yh=i("p"),yh.innerHTML=tP,w1=r(),Sn=i("div"),p(Mc.$$.fragment),j1=r(),Fh=i("p"),Fh.textContent=nP,x1=r(),vh=i("p"),vh.innerHTML=rP,$1=r(),p(Xs.$$.fragment),k1=r(),qo=i("div"),p(Tc.$$.fragment),Z1=r(),Ch=i("p"),Ch.textContent=aP,L1=r(),wh=i("p"),wh.innerHTML=sP,B1=r(),jh=i("ul"),jh.innerHTML=iP,A1=r(),p(qs.$$.fragment),s2=r(),p(yc.$$.fragment),i2=r(),Te=i("div"),p(Fc.$$.fragment),R1=r(),xh=i("p"),xh.innerHTML=lP,W1=r(),$h=i("p"),$h.innerHTML=dP,J1=r(),Un=i("div"),p(vc.$$.fragment),V1=r(),kh=i("p"),kh.textContent=cP,G1=r(),Zh=i("p"),Zh.innerHTML=mP,E1=r(),p(Qs.$$.fragment),P1=r(),Qo=i("div"),p(Cc.$$.fragment),S1=r(),Lh=i("p"),Lh.textContent=fP,U1=r(),Bh=i("p"),Bh.innerHTML=gP,I1=r(),Ah=i("ul"),Ah.innerHTML=pP,N1=r(),p(Hs.$$.fragment),l2=r(),p(wc.$$.fragment),d2=r(),jc=i("p"),jc.textContent=uP,c2=r(),p(xc.$$.fragment),m2=r(),ye=i("div"),p($c.$$.fragment),X1=r(),Rh=i("p"),Rh.innerHTML=hP,q1=r(),Wh=i("p"),Wh.innerHTML=bP,Q1=r(),In=i("div"),p(kc.$$.fragment),H1=r(),Jh=i("p"),Jh.textContent=_P,D1=r(),Vh=i("p"),Vh.innerHTML=MP,Y1=r(),p(Ds.$$.fragment),z1=r(),J=i("div"),p(Zc.$$.fragment),O1=r(),Gh=i("p"),Gh.textContent=TP,K1=r(),Eh=i("p"),Eh.innerHTML=yP,e$=r(),Ph=i("ul"),Ph.innerHTML=FP,o$=r(),Sh=i("p"),Sh.innerHTML=vP,t$=r(),p(Ys.$$.fragment),f2=r(),p(Lc.$$.fragment),g2=r(),Fe=i("div"),p(Bc.$$.fragment),n$=r(),Uh=i("p"),Uh.innerHTML=CP,r$=r(),Ih=i("p"),Ih.innerHTML=wP,a$=r(),Nn=i("div"),p(Ac.$$.fragment),s$=r(),Nh=i("p"),Nh.textContent=jP,i$=r(),Xh=i("p"),Xh.innerHTML=xP,l$=r(),p(zs.$$.fragment),d$=r(),Ho=i("div"),p(Rc.$$.fragment),c$=r(),qh=i("p"),qh.textContent=$P,m$=r(),Qh=i("p"),Qh.innerHTML=kP,f$=r(),Hh=i("ul"),Hh.innerHTML=ZP,g$=r(),p(Os.$$.fragment),p2=r(),p(Wc.$$.fragment),u2=r(),ve=i("div"),p(Jc.$$.fragment),p$=r(),Dh=i("p"),Dh.innerHTML=LP,u$=r(),Yh=i("p"),Yh.innerHTML=BP,h$=r(),Xn=i("div"),p(Vc.$$.fragment),b$=r(),zh=i("p"),zh.textContent=AP,_$=r(),Oh=i("p"),Oh.innerHTML=RP,M$=r(),p(Ks.$$.fragment),T$=r(),Do=i("div"),p(Gc.$$.fragment),y$=r(),Kh=i("p"),Kh.textContent=WP,F$=r(),eb=i("p"),eb.innerHTML=JP,v$=r(),ob=i("ul"),ob.innerHTML=VP,C$=r(),p(ei.$$.fragment),h2=r(),p(Ec.$$.fragment),b2=r(),Ce=i("div"),p(Pc.$$.fragment),w$=r(),tb=i("p"),tb.innerHTML=GP,j$=r(),nb=i("p"),nb.innerHTML=EP,x$=r(),qn=i("div"),p(Sc.$$.fragment),$$=r(),rb=i("p"),rb.textContent=PP,k$=r(),ab=i("p"),ab.innerHTML=SP,Z$=r(),p(oi.$$.fragment),L$=r(),V=i("div"),p(Uc.$$.fragment),B$=r(),sb=i("p"),sb.textContent=UP,A$=r(),ib=i("p"),ib.innerHTML=IP,R$=r(),lb=i("ul"),lb.innerHTML=NP,W$=r(),db=i("p"),db.innerHTML=XP,J$=r(),p(ti.$$.fragment),_2=r(),p(Ic.$$.fragment),M2=r(),we=i("div"),p(Nc.$$.fragment),V$=r(),cb=i("p"),cb.innerHTML=qP,G$=r(),mb=i("p"),mb.innerHTML=QP,E$=r(),Qn=i("div"),p(Xc.$$.fragment),P$=r(),fb=i("p"),fb.textContent=HP,S$=r(),gb=i("p"),gb.innerHTML=DP,U$=r(),p(ni.$$.fragment),I$=r(),Yo=i("div"),p(qc.$$.fragment),N$=r(),pb=i("p"),pb.textContent=YP,X$=r(),ub=i("p"),ub.innerHTML=zP,q$=r(),hb=i("ul"),hb.innerHTML=OP,Q$=r(),p(ri.$$.fragment),T2=r(),p(Qc.$$.fragment),y2=r(),je=i("div"),p(Hc.$$.fragment),H$=r(),bb=i("p"),bb.innerHTML=KP,D$=r(),_b=i("p"),_b.innerHTML=eS,Y$=r(),Hn=i("div"),p(Dc.$$.fragment),z$=r(),Mb=i("p"),Mb.textContent=oS,O$=r(),Tb=i("p"),Tb.innerHTML=tS,K$=r(),p(ai.$$.fragment),ek=r(),zo=i("div"),p(Yc.$$.fragment),ok=r(),yb=i("p"),yb.textContent=nS,tk=r(),Fb=i("p"),Fb.innerHTML=rS,nk=r(),vb=i("ul"),vb.innerHTML=aS,rk=r(),p(si.$$.fragment),F2=r(),p(zc.$$.fragment),v2=r(),Oc=i("div"),p(Kc.$$.fragment),C2=r(),p(em.$$.fragment),w2=r(),om=i("div"),p(tm.$$.fragment),j2=r(),p(nm.$$.fragment),x2=r(),xe=i("div"),p(rm.$$.fragment),ak=r(),Cb=i("p"),Cb.innerHTML=sS,sk=r(),wb=i("p"),wb.innerHTML=iS,ik=r(),Dn=i("div"),p(am.$$.fragment),lk=r(),jb=i("p"),jb.textContent=lS,dk=r(),xb=i("p"),xb.innerHTML=dS,ck=r(),p(ii.$$.fragment),mk=r(),G=i("div"),p(sm.$$.fragment),fk=r(),$b=i("p"),$b.textContent=cS,gk=r(),kb=i("p"),kb.innerHTML=mS,pk=r(),Zb=i("ul"),Zb.innerHTML=fS,uk=r(),Lb=i("p"),Lb.innerHTML=gS,hk=r(),p(li.$$.fragment),$2=r(),p(im.$$.fragment),k2=r(),$e=i("div"),p(lm.$$.fragment),bk=r(),Bb=i("p"),Bb.innerHTML=pS,_k=r(),Ab=i("p"),Ab.innerHTML=uS,Mk=r(),Yn=i("div"),p(dm.$$.fragment),Tk=r(),Rb=i("p"),Rb.textContent=hS,yk=r(),Wb=i("p"),Wb.innerHTML=bS,Fk=r(),p(di.$$.fragment),vk=r(),Oo=i("div"),p(cm.$$.fragment),Ck=r(),Jb=i("p"),Jb.textContent=_S,wk=r(),Vb=i("p"),Vb.innerHTML=MS,jk=r(),Gb=i("ul"),Gb.innerHTML=TS,xk=r(),p(ci.$$.fragment),Z2=r(),p(mm.$$.fragment),L2=r(),ke=i("div"),p(fm.$$.fragment),$k=r(),Eb=i("p"),Eb.innerHTML=yS,kk=r(),Pb=i("p"),Pb.innerHTML=FS,Zk=r(),zn=i("div"),p(gm.$$.fragment),Lk=r(),Sb=i("p"),Sb.textContent=vS,Bk=r(),Ub=i("p"),Ub.innerHTML=CS,Ak=r(),p(mi.$$.fragment),Rk=r(),Ko=i("div"),p(pm.$$.fragment),Wk=r(),Ib=i("p"),Ib.textContent=wS,Jk=r(),Nb=i("p"),Nb.innerHTML=jS,Vk=r(),Xb=i("ul"),Xb.innerHTML=xS,Gk=r(),p(fi.$$.fragment),B2=r(),p(um.$$.fragment),A2=r(),Ze=i("div"),p(hm.$$.fragment),Ek=r(),qb=i("p"),qb.innerHTML=$S,Pk=r(),Qb=i("p"),Qb.innerHTML=kS,Sk=r(),On=i("div"),p(bm.$$.fragment),Uk=r(),Hb=i("p"),Hb.textContent=ZS,Ik=r(),Db=i("p"),Db.innerHTML=LS,Nk=r(),p(gi.$$.fragment),Xk=r(),E=i("div"),p(_m.$$.fragment),qk=r(),Yb=i("p"),Yb.textContent=BS,Qk=r(),zb=i("p"),zb.innerHTML=AS,Hk=r(),Ob=i("ul"),Ob.innerHTML=RS,Dk=r(),Kb=i("p"),Kb.innerHTML=WS,Yk=r(),p(pi.$$.fragment),R2=r(),p(Mm.$$.fragment),W2=r(),Le=i("div"),p(Tm.$$.fragment),zk=r(),e_=i("p"),e_.innerHTML=JS,Ok=r(),o_=i("p"),o_.innerHTML=VS,Kk=r(),Kn=i("div"),p(ym.$$.fragment),eZ=r(),t_=i("p"),t_.textContent=GS,oZ=r(),n_=i("p"),n_.innerHTML=ES,tZ=r(),p(ui.$$.fragment),nZ=r(),et=i("div"),p(Fm.$$.fragment),rZ=r(),r_=i("p"),r_.textContent=PS,aZ=r(),a_=i("p"),a_.innerHTML=SS,sZ=r(),s_=i("ul"),s_.innerHTML=US,iZ=r(),p(hi.$$.fragment),J2=r(),p(vm.$$.fragment),V2=r(),Be=i("div"),p(Cm.$$.fragment),lZ=r(),i_=i("p"),i_.innerHTML=IS,dZ=r(),l_=i("p"),l_.innerHTML=NS,cZ=r(),er=i("div"),p(wm.$$.fragment),mZ=r(),d_=i("p"),d_.textContent=XS,fZ=r(),c_=i("p"),c_.innerHTML=qS,gZ=r(),p(bi.$$.fragment),pZ=r(),ot=i("div"),p(jm.$$.fragment),uZ=r(),m_=i("p"),m_.textContent=QS,hZ=r(),f_=i("p"),f_.innerHTML=HS,bZ=r(),g_=i("ul"),g_.innerHTML=DS,_Z=r(),p(_i.$$.fragment),G2=r(),p(xm.$$.fragment),E2=r(),Ae=i("div"),p($m.$$.fragment),MZ=r(),p_=i("p"),p_.innerHTML=YS,TZ=r(),u_=i("p"),u_.innerHTML=zS,yZ=r(),or=i("div"),p(km.$$.fragment),FZ=r(),h_=i("p"),h_.textContent=OS,vZ=r(),b_=i("p"),b_.innerHTML=KS,CZ=r(),p(Mi.$$.fragment),wZ=r(),P=i("div"),p(Zm.$$.fragment),jZ=r(),__=i("p"),__.textContent=eU,xZ=r(),M_=i("p"),M_.innerHTML=oU,$Z=r(),T_=i("ul"),T_.innerHTML=tU,kZ=r(),y_=i("p"),y_.innerHTML=nU,ZZ=r(),p(Ti.$$.fragment),P2=r(),p(Lm.$$.fragment),S2=r(),Re=i("div"),p(Bm.$$.fragment),LZ=r(),F_=i("p"),F_.innerHTML=rU,BZ=r(),v_=i("p"),v_.innerHTML=aU,AZ=r(),tr=i("div"),p(Am.$$.fragment),RZ=r(),C_=i("p"),C_.textContent=sU,WZ=r(),w_=i("p"),w_.innerHTML=iU,JZ=r(),p(yi.$$.fragment),VZ=r(),tt=i("div"),p(Rm.$$.fragment),GZ=r(),j_=i("p"),j_.textContent=lU,EZ=r(),x_=i("p"),x_.innerHTML=dU,PZ=r(),$_=i("ul"),$_.innerHTML=cU,SZ=r(),p(Fi.$$.fragment),U2=r(),p(Wm.$$.fragment),I2=r(),We=i("div"),p(Jm.$$.fragment),UZ=r(),k_=i("p"),k_.innerHTML=mU,IZ=r(),Z_=i("p"),Z_.innerHTML=fU,NZ=r(),nr=i("div"),p(Vm.$$.fragment),XZ=r(),L_=i("p"),L_.textContent=gU,qZ=r(),B_=i("p"),B_.innerHTML=pU,QZ=r(),p(vi.$$.fragment),HZ=r(),nt=i("div"),p(Gm.$$.fragment),DZ=r(),A_=i("p"),A_.textContent=uU,YZ=r(),R_=i("p"),R_.innerHTML=hU,zZ=r(),W_=i("ul"),W_.innerHTML=bU,OZ=r(),p(Ci.$$.fragment),N2=r(),p(Em.$$.fragment),X2=r(),Je=i("div"),p(Pm.$$.fragment),KZ=r(),J_=i("p"),J_.innerHTML=_U,eL=r(),V_=i("p"),V_.innerHTML=MU,oL=r(),rr=i("div"),p(Sm.$$.fragment),tL=r(),G_=i("p"),G_.textContent=TU,nL=r(),E_=i("p"),E_.innerHTML=yU,rL=r(),p(wi.$$.fragment),aL=r(),S=i("div"),p(Um.$$.fragment),sL=r(),P_=i("p"),P_.textContent=FU,iL=r(),S_=i("p"),S_.innerHTML=vU,lL=r(),U_=i("ul"),U_.innerHTML=CU,dL=r(),I_=i("p"),I_.innerHTML=wU,cL=r(),p(ji.$$.fragment),q2=r(),p(Im.$$.fragment),Q2=r(),Ve=i("div"),p(Nm.$$.fragment),mL=r(),N_=i("p"),N_.innerHTML=jU,fL=r(),X_=i("p"),X_.innerHTML=xU,gL=r(),ar=i("div"),p(Xm.$$.fragment),pL=r(),q_=i("p"),q_.textContent=$U,uL=r(),Q_=i("p"),Q_.innerHTML=kU,hL=r(),p(xi.$$.fragment),bL=r(),rt=i("div"),p(qm.$$.fragment),_L=r(),H_=i("p"),H_.textContent=ZU,ML=r(),D_=i("p"),D_.innerHTML=LU,TL=r(),Y_=i("ul"),Y_.innerHTML=BU,yL=r(),p($i.$$.fragment),H2=r(),p(Qm.$$.fragment),D2=r(),Ge=i("div"),p(Hm.$$.fragment),FL=r(),z_=i("p"),z_.innerHTML=AU,vL=r(),O_=i("p"),O_.innerHTML=RU,CL=r(),sr=i("div"),p(Dm.$$.fragment),wL=r(),K_=i("p"),K_.textContent=WU,jL=r(),eM=i("p"),eM.innerHTML=JU,xL=r(),p(ki.$$.fragment),$L=r(),at=i("div"),p(Ym.$$.fragment),kL=r(),oM=i("p"),oM.textContent=VU,ZL=r(),tM=i("p"),tM.innerHTML=GU,LL=r(),nM=i("ul"),nM.innerHTML=EU,BL=r(),p(Zi.$$.fragment),Y2=r(),p(zm.$$.fragment),z2=r(),Ee=i("div"),p(Om.$$.fragment),AL=r(),rM=i("p"),rM.innerHTML=PU,RL=r(),aM=i("p"),aM.innerHTML=SU,WL=r(),ir=i("div"),p(Km.$$.fragment),JL=r(),sM=i("p"),sM.textContent=UU,VL=r(),iM=i("p"),iM.innerHTML=IU,GL=r(),p(Li.$$.fragment),EL=r(),U=i("div"),p(ef.$$.fragment),PL=r(),lM=i("p"),lM.textContent=NU,SL=r(),dM=i("p"),dM.innerHTML=XU,UL=r(),cM=i("ul"),cM.innerHTML=qU,IL=r(),mM=i("p"),mM.innerHTML=QU,NL=r(),p(Bi.$$.fragment),O2=r(),p(of.$$.fragment),K2=r(),Pe=i("div"),p(tf.$$.fragment),XL=r(),fM=i("p"),fM.innerHTML=HU,qL=r(),gM=i("p"),gM.innerHTML=DU,QL=r(),lr=i("div"),p(nf.$$.fragment),HL=r(),pM=i("p"),pM.textContent=YU,DL=r(),uM=i("p"),uM.innerHTML=zU,YL=r(),p(Ai.$$.fragment),zL=r(),st=i("div"),p(rf.$$.fragment),OL=r(),hM=i("p"),hM.textContent=OU,KL=r(),bM=i("p"),bM.innerHTML=KU,eB=r(),_M=i("ul"),_M.innerHTML=e9,oB=r(),p(Ri.$$.fragment),ew=r(),p(af.$$.fragment),ow=r(),Se=i("div"),p(sf.$$.fragment),tB=r(),MM=i("p"),MM.innerHTML=o9,nB=r(),TM=i("p"),TM.innerHTML=t9,rB=r(),dr=i("div"),p(lf.$$.fragment),aB=r(),yM=i("p"),yM.textContent=n9,sB=r(),FM=i("p"),FM.innerHTML=r9,iB=r(),p(Wi.$$.fragment),lB=r(),it=i("div"),p(df.$$.fragment),dB=r(),vM=i("p"),vM.textContent=a9,cB=r(),CM=i("p"),CM.innerHTML=s9,mB=r(),wM=i("ul"),wM.innerHTML=i9,fB=r(),p(Ji.$$.fragment),tw=r(),p(cf.$$.fragment),nw=r(),Ue=i("div"),p(mf.$$.fragment),gB=r(),jM=i("p"),jM.innerHTML=l9,pB=r(),xM=i("p"),xM.innerHTML=d9,uB=r(),cr=i("div"),p(ff.$$.fragment),hB=r(),$M=i("p"),$M.textContent=c9,bB=r(),kM=i("p"),kM.innerHTML=m9,_B=r(),p(Vi.$$.fragment),MB=r(),I=i("div"),p(gf.$$.fragment),TB=r(),ZM=i("p"),ZM.textContent=f9,yB=r(),LM=i("p"),LM.innerHTML=g9,FB=r(),BM=i("ul"),BM.innerHTML=p9,vB=r(),AM=i("p"),AM.innerHTML=u9,CB=r(),p(Gi.$$.fragment),rw=r(),p(pf.$$.fragment),aw=r(),Ie=i("div"),p(uf.$$.fragment),wB=r(),RM=i("p"),RM.innerHTML=h9,jB=r(),WM=i("p"),WM.innerHTML=b9,xB=r(),mr=i("div"),p(hf.$$.fragment),$B=r(),JM=i("p"),JM.textContent=_9,kB=r(),VM=i("p"),VM.innerHTML=M9,ZB=r(),p(Ei.$$.fragment),LB=r(),lt=i("div"),p(bf.$$.fragment),BB=r(),GM=i("p"),GM.textContent=T9,AB=r(),EM=i("p"),EM.innerHTML=y9,RB=r(),PM=i("ul"),PM.innerHTML=F9,WB=r(),p(Pi.$$.fragment),sw=r(),p(_f.$$.fragment),iw=r(),Ne=i("div"),p(Mf.$$.fragment),JB=r(),SM=i("p"),SM.innerHTML=v9,VB=r(),UM=i("p"),UM.innerHTML=C9,GB=r(),fr=i("div"),p(Tf.$$.fragment),EB=r(),IM=i("p"),IM.textContent=w9,PB=r(),NM=i("p"),NM.innerHTML=j9,SB=r(),p(Si.$$.fragment),UB=r(),dt=i("div"),p(yf.$$.fragment),IB=r(),XM=i("p"),XM.textContent=x9,NB=r(),qM=i("p"),qM.innerHTML=$9,XB=r(),QM=i("ul"),QM.innerHTML=k9,qB=r(),p(Ui.$$.fragment),lw=r(),p(Ff.$$.fragment),dw=r(),vf=i("div"),p(Cf.$$.fragment),cw=r(),p(wf.$$.fragment),mw=r(),jf=i("div"),p(xf.$$.fragment),fw=r(),p($f.$$.fragment),gw=r(),kf=i("p"),kf.textContent=Z9,pw=r(),p(Zf.$$.fragment),uw=r(),Xe=i("div"),p(Lf.$$.fragment),QB=r(),HM=i("p"),HM.innerHTML=L9,HB=r(),DM=i("p"),DM.innerHTML=B9,DB=r(),gr=i("div"),p(Bf.$$.fragment),YB=r(),YM=i("p"),YM.textContent=A9,zB=r(),zM=i("p"),zM.innerHTML=R9,OB=r(),p(Ii.$$.fragment),KB=r(),N=i("div"),p(Af.$$.fragment),eA=r(),OM=i("p"),OM.textContent=W9,oA=r(),KM=i("p"),KM.innerHTML=J9,tA=r(),eT=i("ul"),eT.innerHTML=V9,nA=r(),oT=i("p"),oT.innerHTML=G9,rA=r(),p(Ni.$$.fragment),hw=r(),p(Rf.$$.fragment),bw=r(),qe=i("div"),p(Wf.$$.fragment),aA=r(),tT=i("p"),tT.innerHTML=E9,sA=r(),nT=i("p"),nT.innerHTML=P9,iA=r(),pr=i("div"),p(Jf.$$.fragment),lA=r(),rT=i("p"),rT.textContent=S9,dA=r(),aT=i("p"),aT.innerHTML=U9,cA=r(),p(Xi.$$.fragment),mA=r(),X=i("div"),p(Vf.$$.fragment),fA=r(),sT=i("p"),sT.textContent=I9,gA=r(),iT=i("p"),iT.innerHTML=N9,pA=r(),lT=i("ul"),lT.innerHTML=X9,uA=r(),dT=i("p"),dT.innerHTML=q9,hA=r(),p(qi.$$.fragment),_w=r(),p(Gf.$$.fragment),Mw=r(),Qe=i("div"),p(Ef.$$.fragment),bA=r(),cT=i("p"),cT.innerHTML=Q9,_A=r(),mT=i("p"),mT.innerHTML=H9,MA=r(),ur=i("div"),p(Pf.$$.fragment),TA=r(),fT=i("p"),fT.textContent=D9,yA=r(),gT=i("p"),gT.innerHTML=Y9,FA=r(),p(Qi.$$.fragment),vA=r(),ct=i("div"),p(Sf.$$.fragment),CA=r(),pT=i("p"),pT.textContent=z9,wA=r(),uT=i("p"),uT.innerHTML=O9,jA=r(),hT=i("ul"),hT.innerHTML=K9,xA=r(),p(Hi.$$.fragment),Tw=r(),p(Uf.$$.fragment),yw=r(),He=i("div"),p(If.$$.fragment),$A=r(),bT=i("p"),bT.innerHTML=eI,kA=r(),_T=i("p"),_T.innerHTML=oI,ZA=r(),hr=i("div"),p(Nf.$$.fragment),LA=r(),MT=i("p"),MT.textContent=tI,BA=r(),TT=i("p"),TT.innerHTML=nI,AA=r(),p(Di.$$.fragment),RA=r(),mt=i("div"),p(Xf.$$.fragment),WA=r(),yT=i("p"),yT.textContent=rI,JA=r(),FT=i("p"),FT.innerHTML=aI,VA=r(),vT=i("ul"),vT.innerHTML=sI,GA=r(),p(Yi.$$.fragment),Fw=r(),p(qf.$$.fragment),vw=r(),De=i("div"),p(Qf.$$.fragment),EA=r(),CT=i("p"),CT.innerHTML=iI,PA=r(),wT=i("p"),wT.innerHTML=lI,SA=r(),br=i("div"),p(Hf.$$.fragment),UA=r(),jT=i("p"),jT.textContent=dI,IA=r(),xT=i("p"),xT.innerHTML=cI,NA=r(),p(zi.$$.fragment),XA=r(),q=i("div"),p(Df.$$.fragment),qA=r(),$T=i("p"),$T.textContent=mI,QA=r(),kT=i("p"),kT.innerHTML=fI,HA=r(),ZT=i("ul"),ZT.innerHTML=gI,DA=r(),LT=i("p"),LT.innerHTML=pI,YA=r(),p(Oi.$$.fragment),Cw=r(),p(Yf.$$.fragment),ww=r(),Ye=i("div"),p(zf.$$.fragment),zA=r(),BT=i("p"),BT.innerHTML=uI,OA=r(),AT=i("p"),AT.innerHTML=hI,KA=r(),_r=i("div"),p(Of.$$.fragment),eR=r(),RT=i("p"),RT.textContent=bI,oR=r(),WT=i("p"),WT.innerHTML=_I,tR=r(),p(Ki.$$.fragment),nR=r(),Q=i("div"),p(Kf.$$.fragment),rR=r(),JT=i("p"),JT.textContent=MI,aR=r(),VT=i("p"),VT.innerHTML=TI,sR=r(),GT=i("ul"),GT.innerHTML=yI,iR=r(),ET=i("p"),ET.innerHTML=FI,lR=r(),p(el.$$.fragment),jw=r(),p(eg.$$.fragment),xw=r(),ze=i("div"),p(og.$$.fragment),dR=r(),PT=i("p"),PT.innerHTML=vI,cR=r(),ST=i("p"),ST.innerHTML=CI,mR=r(),Mr=i("div"),p(tg.$$.fragment),fR=r(),UT=i("p"),UT.textContent=wI,gR=r(),IT=i("p"),IT.innerHTML=jI,pR=r(),p(ol.$$.fragment),uR=r(),ft=i("div"),p(ng.$$.fragment),hR=r(),NT=i("p"),NT.textContent=xI,bR=r(),XT=i("p"),XT.innerHTML=$I,_R=r(),qT=i("ul"),qT.innerHTML=kI,MR=r(),p(tl.$$.fragment),$w=r(),p(rg.$$.fragment),kw=r(),Oe=i("div"),p(ag.$$.fragment),TR=r(),QT=i("p"),QT.innerHTML=ZI,yR=r(),HT=i("p"),HT.innerHTML=LI,FR=r(),Tr=i("div"),p(sg.$$.fragment),vR=r(),DT=i("p"),DT.textContent=BI,CR=r(),YT=i("p"),YT.innerHTML=AI,wR=r(),p(nl.$$.fragment),jR=r(),H=i("div"),p(ig.$$.fragment),xR=r(),zT=i("p"),zT.textContent=RI,$R=r(),OT=i("p"),OT.innerHTML=WI,kR=r(),KT=i("ul"),KT.innerHTML=JI,ZR=r(),ey=i("p"),ey.innerHTML=VI,LR=r(),p(rl.$$.fragment),Zw=r(),p(lg.$$.fragment),Lw=r(),Ke=i("div"),p(dg.$$.fragment),BR=r(),oy=i("p"),oy.innerHTML=GI,AR=r(),ty=i("p"),ty.innerHTML=EI,RR=r(),yr=i("div"),p(cg.$$.fragment),WR=r(),ny=i("p"),ny.textContent=PI,JR=r(),ry=i("p"),ry.innerHTML=SI,VR=r(),p(al.$$.fragment),GR=r(),D=i("div"),p(mg.$$.fragment),ER=r(),ay=i("p"),ay.textContent=UI,PR=r(),sy=i("p"),sy.innerHTML=II,SR=r(),iy=i("ul"),iy.innerHTML=NI,UR=r(),ly=i("p"),ly.innerHTML=XI,IR=r(),p(sl.$$.fragment),Bw=r(),p(fg.$$.fragment),Aw=r(),gg=i("div"),p(pg.$$.fragment),Rw=r(),p(ug.$$.fragment),Ww=r(),eo=i("div"),p(hg.$$.fragment),NR=r(),dy=i("p"),dy.innerHTML=qI,XR=r(),cy=i("p"),cy.innerHTML=QI,qR=r(),Fr=i("div"),p(bg.$$.fragment),QR=r(),my=i("p"),my.textContent=HI,HR=r(),fy=i("p"),fy.innerHTML=DI,DR=r(),p(il.$$.fragment),YR=r(),Y=i("div"),p(_g.$$.fragment),zR=r(),gy=i("p"),gy.textContent=YI,OR=r(),py=i("p"),py.innerHTML=zI,KR=r(),uy=i("ul"),uy.innerHTML=OI,eW=r(),hy=i("p"),hy.innerHTML=KI,oW=r(),p(ll.$$.fragment),Jw=r(),p(Mg.$$.fragment),Vw=r(),oo=i("div"),p(Tg.$$.fragment),tW=r(),by=i("p"),by.innerHTML=e5,nW=r(),_y=i("p"),_y.innerHTML=o5,rW=r(),vr=i("div"),p(yg.$$.fragment),aW=r(),My=i("p"),My.textContent=t5,sW=r(),Ty=i("p"),Ty.innerHTML=n5,iW=r(),p(dl.$$.fragment),lW=r(),gt=i("div"),p(Fg.$$.fragment),dW=r(),yy=i("p"),yy.textContent=r5,cW=r(),Fy=i("p"),Fy.innerHTML=a5,mW=r(),vy=i("ul"),vy.innerHTML=s5,fW=r(),p(cl.$$.fragment),Gw=r(),p(vg.$$.fragment),Ew=r(),to=i("div"),p(Cg.$$.fragment),gW=r(),Cy=i("p"),Cy.innerHTML=i5,pW=r(),wy=i("p"),wy.innerHTML=l5,uW=r(),Cr=i("div"),p(wg.$$.fragment),hW=r(),jy=i("p"),jy.textContent=d5,bW=r(),xy=i("p"),xy.innerHTML=c5,_W=r(),p(ml.$$.fragment),MW=r(),z=i("div"),p(jg.$$.fragment),TW=r(),$y=i("p"),$y.textContent=m5,yW=r(),ky=i("p"),ky.innerHTML=f5,FW=r(),Zy=i("ul"),Zy.innerHTML=g5,vW=r(),Ly=i("p"),Ly.innerHTML=p5,CW=r(),p(fl.$$.fragment),Pw=r(),p(xg.$$.fragment),Sw=r(),no=i("div"),p($g.$$.fragment),wW=r(),By=i("p"),By.innerHTML=u5,jW=r(),Ay=i("p"),Ay.innerHTML=h5,xW=r(),wr=i("div"),p(kg.$$.fragment),$W=r(),Ry=i("p"),Ry.textContent=b5,kW=r(),Wy=i("p"),Wy.innerHTML=_5,ZW=r(),p(gl.$$.fragment),LW=r(),O=i("div"),p(Zg.$$.fragment),BW=r(),Jy=i("p"),Jy.textContent=M5,AW=r(),Vy=i("p"),Vy.innerHTML=T5,RW=r(),Gy=i("ul"),Gy.innerHTML=y5,WW=r(),Ey=i("p"),Ey.innerHTML=F5,JW=r(),p(pl.$$.fragment),Uw=r(),p(Lg.$$.fragment),Iw=r(),ro=i("div"),p(Bg.$$.fragment),VW=r(),Py=i("p"),Py.innerHTML=v5,GW=r(),Sy=i("p"),Sy.innerHTML=C5,EW=r(),jr=i("div"),p(Ag.$$.fragment),PW=r(),Uy=i("p"),Uy.textContent=w5,SW=r(),Iy=i("p"),Iy.innerHTML=j5,UW=r(),p(ul.$$.fragment),IW=r(),K=i("div"),p(Rg.$$.fragment),NW=r(),Ny=i("p"),Ny.textContent=x5,XW=r(),Xy=i("p"),Xy.innerHTML=$5,qW=r(),qy=i("ul"),qy.innerHTML=k5,QW=r(),Qy=i("p"),Qy.innerHTML=Z5,HW=r(),p(hl.$$.fragment),Nw=r(),p(Wg.$$.fragment),Xw=r(),ao=i("div"),p(Jg.$$.fragment),DW=r(),Hy=i("p"),Hy.innerHTML=L5,YW=r(),Dy=i("p"),Dy.innerHTML=B5,zW=r(),xr=i("div"),p(Vg.$$.fragment),OW=r(),Yy=i("p"),Yy.textContent=A5,KW=r(),zy=i("p"),zy.innerHTML=R5,e0=r(),p(bl.$$.fragment),o0=r(),pt=i("div"),p(Gg.$$.fragment),t0=r(),Oy=i("p"),Oy.textContent=W5,n0=r(),Ky=i("p"),Ky.innerHTML=J5,r0=r(),eF=i("ul"),eF.innerHTML=V5,a0=r(),p(_l.$$.fragment),qw=r(),p(Eg.$$.fragment),Qw=r(),so=i("div"),p(Pg.$$.fragment),s0=r(),oF=i("p"),oF.innerHTML=G5,i0=r(),tF=i("p"),tF.innerHTML=E5,l0=r(),$r=i("div"),p(Sg.$$.fragment),d0=r(),nF=i("p"),nF.textContent=P5,c0=r(),rF=i("p"),rF.innerHTML=S5,m0=r(),p(Ml.$$.fragment),f0=r(),ee=i("div"),p(Ug.$$.fragment),g0=r(),aF=i("p"),aF.textContent=U5,p0=r(),sF=i("p"),sF.innerHTML=I5,u0=r(),iF=i("ul"),iF.innerHTML=N5,h0=r(),lF=i("p"),lF.innerHTML=X5,b0=r(),p(Tl.$$.fragment),Hw=r(),p(Ig.$$.fragment),Dw=r(),Ng=i("p"),Ng.textContent=q5,Yw=r(),p(Xg.$$.fragment),zw=r(),io=i("div"),p(qg.$$.fragment),_0=r(),dF=i("p"),dF.innerHTML=Q5,M0=r(),cF=i("p"),cF.innerHTML=H5,T0=r(),kr=i("div"),p(Qg.$$.fragment),y0=r(),mF=i("p"),mF.textContent=D5,F0=r(),fF=i("p"),fF.innerHTML=Y5,v0=r(),p(yl.$$.fragment),C0=r(),oe=i("div"),p(Hg.$$.fragment),w0=r(),gF=i("p"),gF.textContent=z5,j0=r(),pF=i("p"),pF.innerHTML=O5,x0=r(),uF=i("ul"),uF.innerHTML=K5,$0=r(),hF=i("p"),hF.innerHTML=eN,k0=r(),p(Fl.$$.fragment),Ow=r(),p(Dg.$$.fragment),Kw=r(),lo=i("div"),p(Yg.$$.fragment),Z0=r(),bF=i("p"),bF.innerHTML=oN,L0=r(),_F=i("p"),_F.innerHTML=tN,B0=r(),Zr=i("div"),p(zg.$$.fragment),A0=r(),MF=i("p"),MF.textContent=nN,R0=r(),TF=i("p"),TF.innerHTML=rN,W0=r(),p(vl.$$.fragment),J0=r(),ut=i("div"),p(Og.$$.fragment),V0=r(),yF=i("p"),yF.textContent=aN,G0=r(),FF=i("p"),FF.innerHTML=sN,E0=r(),vF=i("ul"),vF.innerHTML=iN,P0=r(),p(Cl.$$.fragment),ej=r(),p(Kg.$$.fragment),oj=r(),co=i("div"),p(ep.$$.fragment),S0=r(),CF=i("p"),CF.innerHTML=lN,U0=r(),wF=i("p"),wF.innerHTML=dN,I0=r(),Lr=i("div"),p(op.$$.fragment),N0=r(),jF=i("p"),jF.textContent=cN,X0=r(),xF=i("p"),xF.innerHTML=mN,q0=r(),p(wl.$$.fragment),Q0=r(),te=i("div"),p(tp.$$.fragment),H0=r(),$F=i("p"),$F.textContent=fN,D0=r(),kF=i("p"),kF.innerHTML=gN,Y0=r(),ZF=i("ul"),ZF.innerHTML=pN,z0=r(),LF=i("p"),LF.innerHTML=uN,O0=r(),p(jl.$$.fragment),tj=r(),p(np.$$.fragment),nj=r(),mo=i("div"),p(rp.$$.fragment),K0=r(),BF=i("p"),BF.innerHTML=hN,eJ=r(),AF=i("p"),AF.innerHTML=bN,oJ=r(),Br=i("div"),p(ap.$$.fragment),tJ=r(),RF=i("p"),RF.textContent=_N,nJ=r(),WF=i("p"),WF.innerHTML=MN,rJ=r(),p(xl.$$.fragment),aJ=r(),ne=i("div"),p(sp.$$.fragment),sJ=r(),JF=i("p"),JF.textContent=TN,iJ=r(),VF=i("p"),VF.innerHTML=yN,lJ=r(),GF=i("ul"),GF.innerHTML=FN,dJ=r(),EF=i("p"),EF.innerHTML=vN,cJ=r(),p($l.$$.fragment),rj=r(),p(ip.$$.fragment),aj=r(),fo=i("div"),p(lp.$$.fragment),mJ=r(),PF=i("p"),PF.innerHTML=CN,fJ=r(),SF=i("p"),SF.innerHTML=wN,gJ=r(),Ar=i("div"),p(dp.$$.fragment),pJ=r(),UF=i("p"),UF.textContent=jN,uJ=r(),IF=i("p"),IF.innerHTML=xN,hJ=r(),p(kl.$$.fragment),bJ=r(),re=i("div"),p(cp.$$.fragment),_J=r(),NF=i("p"),NF.textContent=$N,MJ=r(),XF=i("p"),XF.innerHTML=kN,TJ=r(),qF=i("ul"),qF.innerHTML=ZN,yJ=r(),QF=i("p"),QF.innerHTML=LN,FJ=r(),p(Zl.$$.fragment),sj=r(),p(mp.$$.fragment),ij=r(),go=i("div"),p(fp.$$.fragment),vJ=r(),HF=i("p"),HF.innerHTML=BN,CJ=r(),DF=i("p"),DF.innerHTML=AN,wJ=r(),Rr=i("div"),p(gp.$$.fragment),jJ=r(),YF=i("p"),YF.textContent=RN,xJ=r(),zF=i("p"),zF.innerHTML=WN,$J=r(),p(Ll.$$.fragment),kJ=r(),ht=i("div"),p(pp.$$.fragment),ZJ=r(),OF=i("p"),OF.textContent=JN,LJ=r(),KF=i("p"),KF.innerHTML=VN,BJ=r(),ev=i("ul"),ev.innerHTML=GN,AJ=r(),p(Bl.$$.fragment),lj=r(),p(up.$$.fragment),dj=r(),po=i("div"),p(hp.$$.fragment),RJ=r(),ov=i("p"),ov.innerHTML=EN,WJ=r(),tv=i("p"),tv.innerHTML=PN,JJ=r(),Wr=i("div"),p(bp.$$.fragment),VJ=r(),nv=i("p"),nv.textContent=SN,GJ=r(),rv=i("p"),rv.innerHTML=UN,EJ=r(),p(Al.$$.fragment),PJ=r(),bt=i("div"),p(_p.$$.fragment),SJ=r(),av=i("p"),av.textContent=IN,UJ=r(),sv=i("p"),sv.innerHTML=NN,IJ=r(),iv=i("ul"),iv.innerHTML=XN,NJ=r(),p(Rl.$$.fragment),cj=r(),p(Mp.$$.fragment),mj=r(),uo=i("div"),p(Tp.$$.fragment),XJ=r(),lv=i("p"),lv.innerHTML=qN,qJ=r(),dv=i("p"),dv.innerHTML=QN,QJ=r(),Jr=i("div"),p(yp.$$.fragment),HJ=r(),cv=i("p"),cv.textContent=HN,DJ=r(),mv=i("p"),mv.innerHTML=DN,YJ=r(),p(Wl.$$.fragment),zJ=r(),ae=i("div"),p(Fp.$$.fragment),OJ=r(),fv=i("p"),fv.textContent=YN,KJ=r(),gv=i("p"),gv.innerHTML=zN,eV=r(),pv=i("ul"),pv.innerHTML=ON,oV=r(),uv=i("p"),uv.innerHTML=KN,tV=r(),p(Jl.$$.fragment),fj=r(),p(vp.$$.fragment),gj=r(),Cp=i("div"),p(wp.$$.fragment),pj=r(),p(jp.$$.fragment),uj=r(),xp=i("div"),p($p.$$.fragment),hj=r(),p(kp.$$.fragment),bj=r(),Zp=i("p"),Zp.textContent=e4,_j=r(),p(Lp.$$.fragment),Mj=r(),ho=i("div"),p(Bp.$$.fragment),nV=r(),hv=i("p"),hv.innerHTML=o4,rV=r(),bv=i("p"),bv.innerHTML=t4,aV=r(),Vr=i("div"),p(Ap.$$.fragment),sV=r(),_v=i("p"),_v.textContent=n4,iV=r(),Mv=i("p"),Mv.innerHTML=r4,lV=r(),p(Vl.$$.fragment),dV=r(),se=i("div"),p(Rp.$$.fragment),cV=r(),Tv=i("p"),Tv.textContent=a4,mV=r(),yv=i("p"),yv.innerHTML=s4,fV=r(),Fv=i("ul"),Fv.innerHTML=i4,gV=r(),vv=i("p"),vv.innerHTML=l4,pV=r(),p(Gl.$$.fragment),Tj=r(),p(Wp.$$.fragment),yj=r(),bo=i("div"),p(Jp.$$.fragment),uV=r(),Cv=i("p"),Cv.innerHTML=d4,hV=r(),wv=i("p"),wv.innerHTML=c4,bV=r(),Gr=i("div"),p(Vp.$$.fragment),_V=r(),jv=i("p"),jv.textContent=m4,MV=r(),xv=i("p"),xv.innerHTML=f4,TV=r(),p(El.$$.fragment),yV=r(),_t=i("div"),p(Gp.$$.fragment),FV=r(),$v=i("p"),$v.textContent=g4,vV=r(),kv=i("p"),kv.innerHTML=p4,CV=r(),Zv=i("ul"),Zv.innerHTML=u4,wV=r(),p(Pl.$$.fragment),Fj=r(),p(Ep.$$.fragment),vj=r(),_o=i("div"),p(Pp.$$.fragment),jV=r(),Lv=i("p"),Lv.innerHTML=h4,xV=r(),Bv=i("p"),Bv.innerHTML=b4,$V=r(),Er=i("div"),p(Sp.$$.fragment),kV=r(),Av=i("p"),Av.textContent=_4,ZV=r(),Rv=i("p"),Rv.innerHTML=M4,LV=r(),p(Sl.$$.fragment),BV=r(),ie=i("div"),p(Up.$$.fragment),AV=r(),Wv=i("p"),Wv.textContent=T4,RV=r(),Jv=i("p"),Jv.innerHTML=y4,WV=r(),Vv=i("ul"),Vv.innerHTML=F4,JV=r(),Gv=i("p"),Gv.innerHTML=v4,VV=r(),p(Ul.$$.fragment),Cj=r(),p(Ip.$$.fragment),wj=r(),Mo=i("div"),p(Np.$$.fragment),GV=r(),Ev=i("p"),Ev.innerHTML=C4,EV=r(),Pv=i("p"),Pv.innerHTML=w4,PV=r(),Pr=i("div"),p(Xp.$$.fragment),SV=r(),Sv=i("p"),Sv.textContent=j4,UV=r(),Uv=i("p"),Uv.innerHTML=x4,IV=r(),p(Il.$$.fragment),NV=r(),Mt=i("div"),p(qp.$$.fragment),XV=r(),Iv=i("p"),Iv.textContent=$4,qV=r(),Nv=i("p"),Nv.innerHTML=k4,QV=r(),Xv=i("ul"),Xv.innerHTML=Z4,HV=r(),p(Nl.$$.fragment),jj=r(),p(Qp.$$.fragment),xj=r(),To=i("div"),p(Hp.$$.fragment),DV=r(),qv=i("p"),qv.innerHTML=L4,YV=r(),Qv=i("p"),Qv.innerHTML=B4,zV=r(),Sr=i("div"),p(Dp.$$.fragment),OV=r(),Hv=i("p"),Hv.textContent=A4,KV=r(),Dv=i("p"),Dv.innerHTML=R4,eG=r(),p(Xl.$$.fragment),oG=r(),le=i("div"),p(Yp.$$.fragment),tG=r(),Yv=i("p"),Yv.textContent=W4,nG=r(),zv=i("p"),zv.innerHTML=J4,rG=r(),Ov=i("ul"),Ov.innerHTML=V4,aG=r(),Kv=i("p"),Kv.innerHTML=G4,sG=r(),p(ql.$$.fragment),$j=r(),p(zp.$$.fragment),kj=r(),yo=i("div"),p(Op.$$.fragment),iG=r(),eC=i("p"),eC.innerHTML=E4,lG=r(),oC=i("p"),oC.innerHTML=P4,dG=r(),Ur=i("div"),p(Kp.$$.fragment),cG=r(),tC=i("p"),tC.textContent=S4,mG=r(),nC=i("p"),nC.innerHTML=U4,fG=r(),p(Ql.$$.fragment),gG=r(),de=i("div"),p(eu.$$.fragment),pG=r(),rC=i("p"),rC.textContent=I4,uG=r(),aC=i("p"),aC.innerHTML=N4,hG=r(),sC=i("ul"),sC.innerHTML=X4,bG=r(),iC=i("p"),iC.innerHTML=q4,_G=r(),p(Hl.$$.fragment),Zj=r(),p(ou.$$.fragment),Lj=r(),Fo=i("div"),p(tu.$$.fragment),MG=r(),lC=i("p"),lC.innerHTML=Q4,TG=r(),dC=i("p"),dC.innerHTML=H4,yG=r(),Ir=i("div"),p(nu.$$.fragment),FG=r(),cC=i("p"),cC.textContent=D4,vG=r(),mC=i("p"),mC.innerHTML=Y4,CG=r(),p(Dl.$$.fragment),wG=r(),Tt=i("div"),p(ru.$$.fragment),jG=r(),fC=i("p"),fC.textContent=z4,xG=r(),gC=i("p"),gC.innerHTML=O4,$G=r(),pC=i("ul"),pC.innerHTML=K4,kG=r(),p(Yl.$$.fragment),Bj=r(),p(au.$$.fragment),Aj=r(),vo=i("div"),p(su.$$.fragment),ZG=r(),uC=i("p"),uC.innerHTML=eX,LG=r(),hC=i("p"),hC.innerHTML=oX,BG=r(),Nr=i("div"),p(iu.$$.fragment),AG=r(),bC=i("p"),bC.textContent=tX,RG=r(),_C=i("p"),_C.innerHTML=nX,WG=r(),p(zl.$$.fragment),JG=r(),yt=i("div"),p(lu.$$.fragment),VG=r(),MC=i("p"),MC.textContent=rX,GG=r(),TC=i("p"),TC.innerHTML=aX,EG=r(),yC=i("ul"),yC.innerHTML=sX,PG=r(),p(Ol.$$.fragment),Rj=r(),FC=i("p"),this.h()},l(o){const g=pX("svelte-u9bgzb",document.head);n=l(g,"META",{name:!0,content:!0}),g.forEach(d),F=a(o),c=l(o,"P",{}),C(c).forEach(d),s=a(o),u(m.$$.fragment,o),e=a(o),T=l(o,"P",{"data-svelte-h":!0}),f(T)!=="svelte-tc8zjp"&&(T.innerHTML=SG),xC=a(o),vd=l(o,"P",{"data-svelte-h":!0}),f(vd)!=="svelte-1ct80so"&&(vd.innerHTML=UG),$C=a(o),u(Cd.$$.fragment,o),kC=a(o),wd=l(o,"P",{"data-svelte-h":!0}),f(wd)!=="svelte-1oxk3t"&&(wd.innerHTML=IG),ZC=a(o),jd=l(o,"P",{"data-svelte-h":!0}),f(jd)!=="svelte-wrj7bb"&&(jd.innerHTML=NG),LC=a(o),u(xd.$$.fragment,o),BC=a(o),$d=l(o,"P",{"data-svelte-h":!0}),f($d)!=="svelte-mqt3q8"&&($d.innerHTML=XG),AC=a(o),u(kd.$$.fragment,o),RC=a(o),Zd=l(o,"P",{"data-svelte-h":!0}),f(Zd)!=="svelte-fic0ug"&&(Zd.textContent=qG),WC=a(o),u(vs.$$.fragment,o),JC=a(o),u(Ld.$$.fragment,o),VC=a(o),ce=l(o,"DIV",{class:!0});var Ft=C(ce);u(Bd.$$.fragment,Ft),Vj=a(Ft),pu=l(Ft,"P",{"data-svelte-h":!0}),f(pu)!=="svelte-1ug5ri1"&&(pu.innerHTML=QG),Gj=a(Ft),uu=l(Ft,"P",{"data-svelte-h":!0}),f(uu)!=="svelte-1n2cphd"&&(uu.innerHTML=HG),Ej=a(Ft),Uo=l(Ft,"DIV",{class:!0});var vt=C(Uo);u(Ad.$$.fragment,vt),Pj=a(vt),hu=l(vt,"P",{"data-svelte-h":!0}),f(hu)!=="svelte-102bmt5"&&(hu.textContent=DG),Sj=a(vt),bu=l(vt,"P",{"data-svelte-h":!0}),f(bu)!=="svelte-dfwk4w"&&(bu.innerHTML=YG),Uj=a(vt),_u=l(vt,"UL",{"data-svelte-h":!0}),f(_u)!=="svelte-6o0o7e"&&(_u.innerHTML=zG),Ij=a(vt),u(Cs.$$.fragment,vt),vt.forEach(d),Nj=a(Ft),ws=l(Ft,"DIV",{class:!0});var du=C(ws);u(Rd.$$.fragment,du),Xj=a(du),Mu=l(du,"P",{"data-svelte-h":!0}),f(Mu)!=="svelte-3x5rn5"&&(Mu.textContent=OG),du.forEach(d),Ft.forEach(d),GC=a(o),u(Wd.$$.fragment,o),EC=a(o),me=l(o,"DIV",{class:!0});var Ct=C(me);u(Jd.$$.fragment,Ct),qj=a(Ct),Tu=l(Ct,"P",{"data-svelte-h":!0}),f(Tu)!=="svelte-1kffp3a"&&(Tu.innerHTML=KG),Qj=a(Ct),yu=l(Ct,"P",{"data-svelte-h":!0}),f(yu)!=="svelte-1n2cphd"&&(yu.innerHTML=eE),Hj=a(Ct),Io=l(Ct,"DIV",{class:!0});var wt=C(Io);u(Vd.$$.fragment,wt),Dj=a(wt),Fu=l(wt,"P",{"data-svelte-h":!0}),f(Fu)!=="svelte-1cgdf6a"&&(Fu.textContent=oE),Yj=a(wt),vu=l(wt,"P",{"data-svelte-h":!0}),f(vu)!=="svelte-yjv30u"&&(vu.innerHTML=tE),zj=a(wt),Cu=l(wt,"UL",{"data-svelte-h":!0}),f(Cu)!=="svelte-189qntz"&&(Cu.innerHTML=nE),Oj=a(wt),u(js.$$.fragment,wt),wt.forEach(d),Kj=a(Ct),xs=l(Ct,"DIV",{class:!0});var cu=C(xs);u(Gd.$$.fragment,cu),ex=a(cu),wu=l(cu,"P",{"data-svelte-h":!0}),f(wu)!=="svelte-1dj9ip8"&&(wu.textContent=rE),cu.forEach(d),Ct.forEach(d),PC=a(o),u(Ed.$$.fragment,o),SC=a(o),fe=l(o,"DIV",{class:!0});var jt=C(fe);u(Pd.$$.fragment,jt),ox=a(jt),ju=l(jt,"P",{"data-svelte-h":!0}),f(ju)!=="svelte-1m9iyda"&&(ju.innerHTML=aE),tx=a(jt),xu=l(jt,"P",{"data-svelte-h":!0}),f(xu)!=="svelte-1n2cphd"&&(xu.innerHTML=sE),nx=a(jt),L=l(jt,"DIV",{class:!0});var Co=C(L);u(Sd.$$.fragment,Co),rx=a(Co),$u=l(Co,"P",{"data-svelte-h":!0}),f($u)!=="svelte-brndlr"&&($u.textContent=iE),ax=a(Co),ku=l(Co,"P",{"data-svelte-h":!0}),f(ku)!=="svelte-8hskw5"&&(ku.innerHTML=lE),sx=a(Co),Zu=l(Co,"UL",{"data-svelte-h":!0}),f(Zu)!=="svelte-1gzeaul"&&(Zu.innerHTML=dE),ix=a(Co),u($s.$$.fragment,Co),lx=a(Co),u(ks.$$.fragment,Co),Co.forEach(d),dx=a(jt),Zs=l(jt,"DIV",{class:!0});var mu=C(Zs);u(Ud.$$.fragment,mu),cx=a(mu),Lu=l(mu,"P",{"data-svelte-h":!0}),f(Lu)!=="svelte-1a46i61"&&(Lu.textContent=cE),mu.forEach(d),jt.forEach(d),UC=a(o),u(Id.$$.fragment,o),IC=a(o),ge=l(o,"DIV",{class:!0});var xt=C(ge);u(Nd.$$.fragment,xt),mx=a(xt),Bu=l(xt,"P",{"data-svelte-h":!0}),f(Bu)!=="svelte-gn0fm4"&&(Bu.innerHTML=mE),fx=a(xt),Au=l(xt,"P",{"data-svelte-h":!0}),f(Au)!=="svelte-1n2cphd"&&(Au.innerHTML=fE),gx=a(xt),B=l(xt,"DIV",{class:!0});var wo=C(B);u(Xd.$$.fragment,wo),px=a(wo),Ru=l(wo,"P",{"data-svelte-h":!0}),f(Ru)!=="svelte-cyiy9y"&&(Ru.textContent=gE),ux=a(wo),Wu=l(wo,"P",{"data-svelte-h":!0}),f(Wu)!=="svelte-1vjaq5m"&&(Wu.innerHTML=pE),hx=a(wo),Ju=l(wo,"UL",{"data-svelte-h":!0}),f(Ju)!=="svelte-4rawf"&&(Ju.innerHTML=uE),bx=a(wo),u(Ls.$$.fragment,wo),_x=a(wo),u(Bs.$$.fragment,wo),wo.forEach(d),Mx=a(xt),As=l(xt,"DIV",{class:!0});var fu=C(As);u(qd.$$.fragment,fu),Tx=a(fu),Vu=l(fu,"P",{"data-svelte-h":!0}),f(Vu)!=="svelte-8qw1k0"&&(Vu.textContent=hE),fu.forEach(d),xt.forEach(d),NC=a(o),u(Qd.$$.fragment,o),XC=a(o),pe=l(o,"DIV",{class:!0});var $t=C(pe);u(Hd.$$.fragment,$t),yx=a($t),Gu=l($t,"P",{"data-svelte-h":!0}),f(Gu)!=="svelte-8fyp1u"&&(Gu.innerHTML=bE),Fx=a($t),Eu=l($t,"P",{"data-svelte-h":!0}),f(Eu)!=="svelte-1n2cphd"&&(Eu.innerHTML=_E),vx=a($t),A=l($t,"DIV",{class:!0});var jo=C(A);u(Dd.$$.fragment,jo),Cx=a(jo),Pu=l(jo,"P",{"data-svelte-h":!0}),f(Pu)!=="svelte-ampft1"&&(Pu.textContent=ME),wx=a(jo),Su=l(jo,"P",{"data-svelte-h":!0}),f(Su)!=="svelte-1lz4kgk"&&(Su.innerHTML=TE),jx=a(jo),Uu=l(jo,"UL",{"data-svelte-h":!0}),f(Uu)!=="svelte-ptljz0"&&(Uu.innerHTML=yE),xx=a(jo),u(Rs.$$.fragment,jo),$x=a(jo),u(Ws.$$.fragment,jo),jo.forEach(d),kx=a($t),Js=l($t,"DIV",{class:!0});var gu=C(Js);u(Yd.$$.fragment,gu),Zx=a(gu),Iu=l(gu,"P",{"data-svelte-h":!0}),f(Iu)!=="svelte-1rfmb83"&&(Iu.textContent=FE),gu.forEach(d),$t.forEach(d),qC=a(o),u(zd.$$.fragment,o),QC=a(o),Od=l(o,"P",{"data-svelte-h":!0}),f(Od)!=="svelte-18q6swv"&&(Od.textContent=vE),HC=a(o),u(Kd.$$.fragment,o),DC=a(o),ue=l(o,"DIV",{class:!0});var kt=C(ue);u(ec.$$.fragment,kt),Lx=a(kt),Nu=l(kt,"P",{"data-svelte-h":!0}),f(Nu)!=="svelte-5qy2p"&&(Nu.innerHTML=CE),Bx=a(kt),Xu=l(kt,"P",{"data-svelte-h":!0}),f(Xu)!=="svelte-1n2cphd"&&(Xu.innerHTML=wE),Ax=a(kt),Vn=l(kt,"DIV",{class:!0});var sa=C(Vn);u(oc.$$.fragment,sa),Rx=a(sa),qu=l(sa,"P",{"data-svelte-h":!0}),f(qu)!=="svelte-ws3pq1"&&(qu.textContent=jE),Wx=a(sa),Qu=l(sa,"P",{"data-svelte-h":!0}),f(Qu)!=="svelte-6ap5g8"&&(Qu.innerHTML=xE),Jx=a(sa),u(Vs.$$.fragment,sa),sa.forEach(d),Vx=a(kt),R=l(kt,"DIV",{class:!0});var xo=C(R);u(tc.$$.fragment,xo),Gx=a(xo),Hu=l(xo,"P",{"data-svelte-h":!0}),f(Hu)!=="svelte-mt76tr"&&(Hu.textContent=$E),Ex=a(xo),Du=l(xo,"P",{"data-svelte-h":!0}),f(Du)!=="svelte-1enn9p2"&&(Du.innerHTML=kE),Px=a(xo),Yu=l(xo,"UL",{"data-svelte-h":!0}),f(Yu)!=="svelte-1v45mi0"&&(Yu.innerHTML=ZE),Sx=a(xo),zu=l(xo,"P",{"data-svelte-h":!0}),f(zu)!=="svelte-tebhz6"&&(zu.innerHTML=LE),Ux=a(xo),u(Gs.$$.fragment,xo),xo.forEach(d),kt.forEach(d),YC=a(o),u(nc.$$.fragment,o),zC=a(o),he=l(o,"DIV",{class:!0});var Zt=C(he);u(rc.$$.fragment,Zt),Ix=a(Zt),Ou=l(Zt,"P",{"data-svelte-h":!0}),f(Ou)!=="svelte-5qy2p"&&(Ou.innerHTML=BE),Nx=a(Zt),Ku=l(Zt,"P",{"data-svelte-h":!0}),f(Ku)!=="svelte-1n2cphd"&&(Ku.innerHTML=AE),Xx=a(Zt),Gn=l(Zt,"DIV",{class:!0});var ia=C(Gn);u(ac.$$.fragment,ia),qx=a(ia),eh=l(ia,"P",{"data-svelte-h":!0}),f(eh)!=="svelte-ws3pq1"&&(eh.textContent=RE),Qx=a(ia),oh=l(ia,"P",{"data-svelte-h":!0}),f(oh)!=="svelte-6ap5g8"&&(oh.innerHTML=WE),Hx=a(ia),u(Es.$$.fragment,ia),ia.forEach(d),Dx=a(Zt),No=l(Zt,"DIV",{class:!0});var Lt=C(No);u(sc.$$.fragment,Lt),Yx=a(Lt),th=l(Lt,"P",{"data-svelte-h":!0}),f(th)!=="svelte-mt76tr"&&(th.textContent=JE),zx=a(Lt),nh=l(Lt,"P",{"data-svelte-h":!0}),f(nh)!=="svelte-1enn9p2"&&(nh.innerHTML=VE),Ox=a(Lt),rh=l(Lt,"UL",{"data-svelte-h":!0}),f(rh)!=="svelte-1f5gf02"&&(rh.innerHTML=GE),Kx=a(Lt),u(Ps.$$.fragment,Lt),Lt.forEach(d),Zt.forEach(d),OC=a(o),u(ic.$$.fragment,o),KC=a(o),be=l(o,"DIV",{class:!0});var Bt=C(be);u(lc.$$.fragment,Bt),e1=a(Bt),ah=l(Bt,"P",{"data-svelte-h":!0}),f(ah)!=="svelte-5qy2p"&&(ah.innerHTML=EE),o1=a(Bt),sh=l(Bt,"P",{"data-svelte-h":!0}),f(sh)!=="svelte-1n2cphd"&&(sh.innerHTML=PE),t1=a(Bt),En=l(Bt,"DIV",{class:!0});var la=C(En);u(dc.$$.fragment,la),n1=a(la),ih=l(la,"P",{"data-svelte-h":!0}),f(ih)!=="svelte-ws3pq1"&&(ih.textContent=SE),r1=a(la),lh=l(la,"P",{"data-svelte-h":!0}),f(lh)!=="svelte-6ap5g8"&&(lh.innerHTML=UE),a1=a(la),u(Ss.$$.fragment,la),la.forEach(d),s1=a(Bt),Xo=l(Bt,"DIV",{class:!0});var At=C(Xo);u(cc.$$.fragment,At),i1=a(At),dh=l(At,"P",{"data-svelte-h":!0}),f(dh)!=="svelte-mt76tr"&&(dh.textContent=IE),l1=a(At),ch=l(At,"P",{"data-svelte-h":!0}),f(ch)!=="svelte-1enn9p2"&&(ch.innerHTML=NE),d1=a(At),mh=l(At,"UL",{"data-svelte-h":!0}),f(mh)!=="svelte-l0hjyc"&&(mh.innerHTML=XE),c1=a(At),u(Us.$$.fragment,At),At.forEach(d),Bt.forEach(d),e2=a(o),u(mc.$$.fragment,o),o2=a(o),fc=l(o,"P",{"data-svelte-h":!0}),f(fc)!=="svelte-bem2jy"&&(fc.textContent=qE),t2=a(o),u(gc.$$.fragment,o),n2=a(o),_e=l(o,"DIV",{class:!0});var Rt=C(_e);u(pc.$$.fragment,Rt),m1=a(Rt),fh=l(Rt,"P",{"data-svelte-h":!0}),f(fh)!=="svelte-1bhcgfn"&&(fh.innerHTML=QE),f1=a(Rt),gh=l(Rt,"P",{"data-svelte-h":!0}),f(gh)!=="svelte-1n2cphd"&&(gh.innerHTML=HE),g1=a(Rt),Pn=l(Rt,"DIV",{class:!0});var da=C(Pn);u(uc.$$.fragment,da),p1=a(da),ph=l(da,"P",{"data-svelte-h":!0}),f(ph)!=="svelte-qeb48j"&&(ph.textContent=DE),u1=a(da),uh=l(da,"P",{"data-svelte-h":!0}),f(uh)!=="svelte-6ap5g8"&&(uh.innerHTML=YE),h1=a(da),u(Is.$$.fragment,da),da.forEach(d),b1=a(Rt),W=l(Rt,"DIV",{class:!0});var $o=C(W);u(hc.$$.fragment,$o),_1=a($o),hh=l($o,"P",{"data-svelte-h":!0}),f(hh)!=="svelte-141b7sb"&&(hh.textContent=zE),M1=a($o),bh=l($o,"P",{"data-svelte-h":!0}),f(bh)!=="svelte-1enn9p2"&&(bh.innerHTML=OE),T1=a($o),_h=l($o,"UL",{"data-svelte-h":!0}),f(_h)!=="svelte-1kc0uj0"&&(_h.innerHTML=KE),y1=a($o),Mh=l($o,"P",{"data-svelte-h":!0}),f(Mh)!=="svelte-tebhz6"&&(Mh.innerHTML=eP),F1=a($o),u(Ns.$$.fragment,$o),$o.forEach(d),Rt.forEach(d),r2=a(o),u(bc.$$.fragment,o),a2=a(o),Me=l(o,"DIV",{class:!0});var Wt=C(Me);u(_c.$$.fragment,Wt),v1=a(Wt),Th=l(Wt,"P",{"data-svelte-h":!0}),f(Th)!=="svelte-1bhcgfn"&&(Th.innerHTML=oP),C1=a(Wt),yh=l(Wt,"P",{"data-svelte-h":!0}),f(yh)!=="svelte-1n2cphd"&&(yh.innerHTML=tP),w1=a(Wt),Sn=l(Wt,"DIV",{class:!0});var ca=C(Sn);u(Mc.$$.fragment,ca),j1=a(ca),Fh=l(ca,"P",{"data-svelte-h":!0}),f(Fh)!=="svelte-qeb48j"&&(Fh.textContent=nP),x1=a(ca),vh=l(ca,"P",{"data-svelte-h":!0}),f(vh)!=="svelte-6ap5g8"&&(vh.innerHTML=rP),$1=a(ca),u(Xs.$$.fragment,ca),ca.forEach(d),k1=a(Wt),qo=l(Wt,"DIV",{class:!0});var Jt=C(qo);u(Tc.$$.fragment,Jt),Z1=a(Jt),Ch=l(Jt,"P",{"data-svelte-h":!0}),f(Ch)!=="svelte-141b7sb"&&(Ch.textContent=aP),L1=a(Jt),wh=l(Jt,"P",{"data-svelte-h":!0}),f(wh)!=="svelte-1enn9p2"&&(wh.innerHTML=sP),B1=a(Jt),jh=l(Jt,"UL",{"data-svelte-h":!0}),f(jh)!=="svelte-1aju47t"&&(jh.innerHTML=iP),A1=a(Jt),u(qs.$$.fragment,Jt),Jt.forEach(d),Wt.forEach(d),s2=a(o),u(yc.$$.fragment,o),i2=a(o),Te=l(o,"DIV",{class:!0});var Vt=C(Te);u(Fc.$$.fragment,Vt),R1=a(Vt),xh=l(Vt,"P",{"data-svelte-h":!0}),f(xh)!=="svelte-1bhcgfn"&&(xh.innerHTML=lP),W1=a(Vt),$h=l(Vt,"P",{"data-svelte-h":!0}),f($h)!=="svelte-1n2cphd"&&($h.innerHTML=dP),J1=a(Vt),Un=l(Vt,"DIV",{class:!0});var ma=C(Un);u(vc.$$.fragment,ma),V1=a(ma),kh=l(ma,"P",{"data-svelte-h":!0}),f(kh)!=="svelte-qeb48j"&&(kh.textContent=cP),G1=a(ma),Zh=l(ma,"P",{"data-svelte-h":!0}),f(Zh)!=="svelte-6ap5g8"&&(Zh.innerHTML=mP),E1=a(ma),u(Qs.$$.fragment,ma),ma.forEach(d),P1=a(Vt),Qo=l(Vt,"DIV",{class:!0});var Gt=C(Qo);u(Cc.$$.fragment,Gt),S1=a(Gt),Lh=l(Gt,"P",{"data-svelte-h":!0}),f(Lh)!=="svelte-141b7sb"&&(Lh.textContent=fP),U1=a(Gt),Bh=l(Gt,"P",{"data-svelte-h":!0}),f(Bh)!=="svelte-1enn9p2"&&(Bh.innerHTML=gP),I1=a(Gt),Ah=l(Gt,"UL",{"data-svelte-h":!0}),f(Ah)!=="svelte-18lsb0e"&&(Ah.innerHTML=pP),N1=a(Gt),u(Hs.$$.fragment,Gt),Gt.forEach(d),Vt.forEach(d),l2=a(o),u(wc.$$.fragment,o),d2=a(o),jc=l(o,"P",{"data-svelte-h":!0}),f(jc)!=="svelte-q6wav2"&&(jc.textContent=uP),c2=a(o),u(xc.$$.fragment,o),m2=a(o),ye=l(o,"DIV",{class:!0});var Et=C(ye);u($c.$$.fragment,Et),X1=a(Et),Rh=l(Et,"P",{"data-svelte-h":!0}),f(Rh)!=="svelte-qlgs7y"&&(Rh.innerHTML=hP),q1=a(Et),Wh=l(Et,"P",{"data-svelte-h":!0}),f(Wh)!=="svelte-1n2cphd"&&(Wh.innerHTML=bP),Q1=a(Et),In=l(Et,"DIV",{class:!0});var fa=C(In);u(kc.$$.fragment,fa),H1=a(fa),Jh=l(fa,"P",{"data-svelte-h":!0}),f(Jh)!=="svelte-6spxaa"&&(Jh.textContent=_P),D1=a(fa),Vh=l(fa,"P",{"data-svelte-h":!0}),f(Vh)!=="svelte-6ap5g8"&&(Vh.innerHTML=MP),Y1=a(fa),u(Ds.$$.fragment,fa),fa.forEach(d),z1=a(Et),J=l(Et,"DIV",{class:!0});var ko=C(J);u(Zc.$$.fragment,ko),O1=a(ko),Gh=l(ko,"P",{"data-svelte-h":!0}),f(Gh)!=="svelte-1b19um"&&(Gh.textContent=TP),K1=a(ko),Eh=l(ko,"P",{"data-svelte-h":!0}),f(Eh)!=="svelte-1enn9p2"&&(Eh.innerHTML=yP),e$=a(ko),Ph=l(ko,"UL",{"data-svelte-h":!0}),f(Ph)!=="svelte-1d74qd6"&&(Ph.innerHTML=FP),o$=a(ko),Sh=l(ko,"P",{"data-svelte-h":!0}),f(Sh)!=="svelte-tebhz6"&&(Sh.innerHTML=vP),t$=a(ko),u(Ys.$$.fragment,ko),ko.forEach(d),Et.forEach(d),f2=a(o),u(Lc.$$.fragment,o),g2=a(o),Fe=l(o,"DIV",{class:!0});var Pt=C(Fe);u(Bc.$$.fragment,Pt),n$=a(Pt),Uh=l(Pt,"P",{"data-svelte-h":!0}),f(Uh)!=="svelte-qlgs7y"&&(Uh.innerHTML=CP),r$=a(Pt),Ih=l(Pt,"P",{"data-svelte-h":!0}),f(Ih)!=="svelte-1n2cphd"&&(Ih.innerHTML=wP),a$=a(Pt),Nn=l(Pt,"DIV",{class:!0});var ga=C(Nn);u(Ac.$$.fragment,ga),s$=a(ga),Nh=l(ga,"P",{"data-svelte-h":!0}),f(Nh)!=="svelte-6spxaa"&&(Nh.textContent=jP),i$=a(ga),Xh=l(ga,"P",{"data-svelte-h":!0}),f(Xh)!=="svelte-6ap5g8"&&(Xh.innerHTML=xP),l$=a(ga),u(zs.$$.fragment,ga),ga.forEach(d),d$=a(Pt),Ho=l(Pt,"DIV",{class:!0});var St=C(Ho);u(Rc.$$.fragment,St),c$=a(St),qh=l(St,"P",{"data-svelte-h":!0}),f(qh)!=="svelte-1b19um"&&(qh.textContent=$P),m$=a(St),Qh=l(St,"P",{"data-svelte-h":!0}),f(Qh)!=="svelte-1enn9p2"&&(Qh.innerHTML=kP),f$=a(St),Hh=l(St,"UL",{"data-svelte-h":!0}),f(Hh)!=="svelte-twdln1"&&(Hh.innerHTML=ZP),g$=a(St),u(Os.$$.fragment,St),St.forEach(d),Pt.forEach(d),p2=a(o),u(Wc.$$.fragment,o),u2=a(o),ve=l(o,"DIV",{class:!0});var Ut=C(ve);u(Jc.$$.fragment,Ut),p$=a(Ut),Dh=l(Ut,"P",{"data-svelte-h":!0}),f(Dh)!=="svelte-qlgs7y"&&(Dh.innerHTML=LP),u$=a(Ut),Yh=l(Ut,"P",{"data-svelte-h":!0}),f(Yh)!=="svelte-1n2cphd"&&(Yh.innerHTML=BP),h$=a(Ut),Xn=l(Ut,"DIV",{class:!0});var pa=C(Xn);u(Vc.$$.fragment,pa),b$=a(pa),zh=l(pa,"P",{"data-svelte-h":!0}),f(zh)!=="svelte-6spxaa"&&(zh.textContent=AP),_$=a(pa),Oh=l(pa,"P",{"data-svelte-h":!0}),f(Oh)!=="svelte-6ap5g8"&&(Oh.innerHTML=RP),M$=a(pa),u(Ks.$$.fragment,pa),pa.forEach(d),T$=a(Ut),Do=l(Ut,"DIV",{class:!0});var It=C(Do);u(Gc.$$.fragment,It),y$=a(It),Kh=l(It,"P",{"data-svelte-h":!0}),f(Kh)!=="svelte-1b19um"&&(Kh.textContent=WP),F$=a(It),eb=l(It,"P",{"data-svelte-h":!0}),f(eb)!=="svelte-1enn9p2"&&(eb.innerHTML=JP),v$=a(It),ob=l(It,"UL",{"data-svelte-h":!0}),f(ob)!=="svelte-1ig7csh"&&(ob.innerHTML=VP),C$=a(It),u(ei.$$.fragment,It),It.forEach(d),Ut.forEach(d),h2=a(o),u(Ec.$$.fragment,o),b2=a(o),Ce=l(o,"DIV",{class:!0});var Nt=C(Ce);u(Pc.$$.fragment,Nt),w$=a(Nt),tb=l(Nt,"P",{"data-svelte-h":!0}),f(tb)!=="svelte-1xavzi6"&&(tb.innerHTML=GP),j$=a(Nt),nb=l(Nt,"P",{"data-svelte-h":!0}),f(nb)!=="svelte-1n2cphd"&&(nb.innerHTML=EP),x$=a(Nt),qn=l(Nt,"DIV",{class:!0});var ua=C(qn);u(Sc.$$.fragment,ua),$$=a(ua),rb=l(ua,"P",{"data-svelte-h":!0}),f(rb)!=="svelte-1l4n5m2"&&(rb.textContent=PP),k$=a(ua),ab=l(ua,"P",{"data-svelte-h":!0}),f(ab)!=="svelte-6ap5g8"&&(ab.innerHTML=SP),Z$=a(ua),u(oi.$$.fragment,ua),ua.forEach(d),L$=a(Nt),V=l(Nt,"DIV",{class:!0});var Zo=C(V);u(Uc.$$.fragment,Zo),B$=a(Zo),sb=l(Zo,"P",{"data-svelte-h":!0}),f(sb)!=="svelte-1hs4l6u"&&(sb.textContent=UP),A$=a(Zo),ib=l(Zo,"P",{"data-svelte-h":!0}),f(ib)!=="svelte-1enn9p2"&&(ib.innerHTML=IP),R$=a(Zo),lb=l(Zo,"UL",{"data-svelte-h":!0}),f(lb)!=="svelte-1rnnn6v"&&(lb.innerHTML=NP),W$=a(Zo),db=l(Zo,"P",{"data-svelte-h":!0}),f(db)!=="svelte-tebhz6"&&(db.innerHTML=XP),J$=a(Zo),u(ti.$$.fragment,Zo),Zo.forEach(d),Nt.forEach(d),_2=a(o),u(Ic.$$.fragment,o),M2=a(o),we=l(o,"DIV",{class:!0});var Xt=C(we);u(Nc.$$.fragment,Xt),V$=a(Xt),cb=l(Xt,"P",{"data-svelte-h":!0}),f(cb)!=="svelte-1xavzi6"&&(cb.innerHTML=qP),G$=a(Xt),mb=l(Xt,"P",{"data-svelte-h":!0}),f(mb)!=="svelte-1n2cphd"&&(mb.innerHTML=QP),E$=a(Xt),Qn=l(Xt,"DIV",{class:!0});var ha=C(Qn);u(Xc.$$.fragment,ha),P$=a(ha),fb=l(ha,"P",{"data-svelte-h":!0}),f(fb)!=="svelte-1l4n5m2"&&(fb.textContent=HP),S$=a(ha),gb=l(ha,"P",{"data-svelte-h":!0}),f(gb)!=="svelte-6ap5g8"&&(gb.innerHTML=DP),U$=a(ha),u(ni.$$.fragment,ha),ha.forEach(d),I$=a(Xt),Yo=l(Xt,"DIV",{class:!0});var qt=C(Yo);u(qc.$$.fragment,qt),N$=a(qt),pb=l(qt,"P",{"data-svelte-h":!0}),f(pb)!=="svelte-1hs4l6u"&&(pb.textContent=YP),X$=a(qt),ub=l(qt,"P",{"data-svelte-h":!0}),f(ub)!=="svelte-1enn9p2"&&(ub.innerHTML=zP),q$=a(qt),hb=l(qt,"UL",{"data-svelte-h":!0}),f(hb)!=="svelte-1qn5yke"&&(hb.innerHTML=OP),Q$=a(qt),u(ri.$$.fragment,qt),qt.forEach(d),Xt.forEach(d),T2=a(o),u(Qc.$$.fragment,o),y2=a(o),je=l(o,"DIV",{class:!0});var Qt=C(je);u(Hc.$$.fragment,Qt),H$=a(Qt),bb=l(Qt,"P",{"data-svelte-h":!0}),f(bb)!=="svelte-1xavzi6"&&(bb.innerHTML=KP),D$=a(Qt),_b=l(Qt,"P",{"data-svelte-h":!0}),f(_b)!=="svelte-1n2cphd"&&(_b.innerHTML=eS),Y$=a(Qt),Hn=l(Qt,"DIV",{class:!0});var ba=C(Hn);u(Dc.$$.fragment,ba),z$=a(ba),Mb=l(ba,"P",{"data-svelte-h":!0}),f(Mb)!=="svelte-1l4n5m2"&&(Mb.textContent=oS),O$=a(ba),Tb=l(ba,"P",{"data-svelte-h":!0}),f(Tb)!=="svelte-6ap5g8"&&(Tb.innerHTML=tS),K$=a(ba),u(ai.$$.fragment,ba),ba.forEach(d),ek=a(Qt),zo=l(Qt,"DIV",{class:!0});var Ht=C(zo);u(Yc.$$.fragment,Ht),ok=a(Ht),yb=l(Ht,"P",{"data-svelte-h":!0}),f(yb)!=="svelte-1hs4l6u"&&(yb.textContent=nS),tk=a(Ht),Fb=l(Ht,"P",{"data-svelte-h":!0}),f(Fb)!=="svelte-1enn9p2"&&(Fb.innerHTML=rS),nk=a(Ht),vb=l(Ht,"UL",{"data-svelte-h":!0}),f(vb)!=="svelte-bmur6s"&&(vb.innerHTML=aS),rk=a(Ht),u(si.$$.fragment,Ht),Ht.forEach(d),Qt.forEach(d),F2=a(o),u(zc.$$.fragment,o),v2=a(o),Oc=l(o,"DIV",{class:!0});var vC=C(Oc);u(Kc.$$.fragment,vC),vC.forEach(d),C2=a(o),u(em.$$.fragment,o),w2=a(o),om=l(o,"DIV",{class:!0});var CC=C(om);u(tm.$$.fragment,CC),CC.forEach(d),j2=a(o),u(nm.$$.fragment,o),x2=a(o),xe=l(o,"DIV",{class:!0});var Dt=C(xe);u(rm.$$.fragment,Dt),ak=a(Dt),Cb=l(Dt,"P",{"data-svelte-h":!0}),f(Cb)!=="svelte-u8o82m"&&(Cb.innerHTML=sS),sk=a(Dt),wb=l(Dt,"P",{"data-svelte-h":!0}),f(wb)!=="svelte-1n2cphd"&&(wb.innerHTML=iS),ik=a(Dt),Dn=l(Dt,"DIV",{class:!0});var _a=C(Dn);u(am.$$.fragment,_a),lk=a(_a),jb=l(_a,"P",{"data-svelte-h":!0}),f(jb)!=="svelte-1d0i3aa"&&(jb.textContent=lS),dk=a(_a),xb=l(_a,"P",{"data-svelte-h":!0}),f(xb)!=="svelte-6ap5g8"&&(xb.innerHTML=dS),ck=a(_a),u(ii.$$.fragment,_a),_a.forEach(d),mk=a(Dt),G=l(Dt,"DIV",{class:!0});var Lo=C(G);u(sm.$$.fragment,Lo),fk=a(Lo),$b=l(Lo,"P",{"data-svelte-h":!0}),f($b)!=="svelte-196fe96"&&($b.textContent=cS),gk=a(Lo),kb=l(Lo,"P",{"data-svelte-h":!0}),f(kb)!=="svelte-1enn9p2"&&(kb.innerHTML=mS),pk=a(Lo),Zb=l(Lo,"UL",{"data-svelte-h":!0}),f(Zb)!=="svelte-9qf4t4"&&(Zb.innerHTML=fS),uk=a(Lo),Lb=l(Lo,"P",{"data-svelte-h":!0}),f(Lb)!=="svelte-tebhz6"&&(Lb.innerHTML=gS),hk=a(Lo),u(li.$$.fragment,Lo),Lo.forEach(d),Dt.forEach(d),$2=a(o),u(im.$$.fragment,o),k2=a(o),$e=l(o,"DIV",{class:!0});var Yt=C($e);u(lm.$$.fragment,Yt),bk=a(Yt),Bb=l(Yt,"P",{"data-svelte-h":!0}),f(Bb)!=="svelte-u8o82m"&&(Bb.innerHTML=pS),_k=a(Yt),Ab=l(Yt,"P",{"data-svelte-h":!0}),f(Ab)!=="svelte-1n2cphd"&&(Ab.innerHTML=uS),Mk=a(Yt),Yn=l(Yt,"DIV",{class:!0});var Ma=C(Yn);u(dm.$$.fragment,Ma),Tk=a(Ma),Rb=l(Ma,"P",{"data-svelte-h":!0}),f(Rb)!=="svelte-1d0i3aa"&&(Rb.textContent=hS),yk=a(Ma),Wb=l(Ma,"P",{"data-svelte-h":!0}),f(Wb)!=="svelte-6ap5g8"&&(Wb.innerHTML=bS),Fk=a(Ma),u(di.$$.fragment,Ma),Ma.forEach(d),vk=a(Yt),Oo=l(Yt,"DIV",{class:!0});var zt=C(Oo);u(cm.$$.fragment,zt),Ck=a(zt),Jb=l(zt,"P",{"data-svelte-h":!0}),f(Jb)!=="svelte-196fe96"&&(Jb.textContent=_S),wk=a(zt),Vb=l(zt,"P",{"data-svelte-h":!0}),f(Vb)!=="svelte-1enn9p2"&&(Vb.innerHTML=MS),jk=a(zt),Gb=l(zt,"UL",{"data-svelte-h":!0}),f(Gb)!=="svelte-18c2iox"&&(Gb.innerHTML=TS),xk=a(zt),u(ci.$$.fragment,zt),zt.forEach(d),Yt.forEach(d),Z2=a(o),u(mm.$$.fragment,o),L2=a(o),ke=l(o,"DIV",{class:!0});var Ot=C(ke);u(fm.$$.fragment,Ot),$k=a(Ot),Eb=l(Ot,"P",{"data-svelte-h":!0}),f(Eb)!=="svelte-u8o82m"&&(Eb.innerHTML=yS),kk=a(Ot),Pb=l(Ot,"P",{"data-svelte-h":!0}),f(Pb)!=="svelte-1n2cphd"&&(Pb.innerHTML=FS),Zk=a(Ot),zn=l(Ot,"DIV",{class:!0});var Ta=C(zn);u(gm.$$.fragment,Ta),Lk=a(Ta),Sb=l(Ta,"P",{"data-svelte-h":!0}),f(Sb)!=="svelte-1d0i3aa"&&(Sb.textContent=vS),Bk=a(Ta),Ub=l(Ta,"P",{"data-svelte-h":!0}),f(Ub)!=="svelte-6ap5g8"&&(Ub.innerHTML=CS),Ak=a(Ta),u(mi.$$.fragment,Ta),Ta.forEach(d),Rk=a(Ot),Ko=l(Ot,"DIV",{class:!0});var Kt=C(Ko);u(pm.$$.fragment,Kt),Wk=a(Kt),Ib=l(Kt,"P",{"data-svelte-h":!0}),f(Ib)!=="svelte-196fe96"&&(Ib.textContent=wS),Jk=a(Kt),Nb=l(Kt,"P",{"data-svelte-h":!0}),f(Nb)!=="svelte-1enn9p2"&&(Nb.innerHTML=jS),Vk=a(Kt),Xb=l(Kt,"UL",{"data-svelte-h":!0}),f(Xb)!=="svelte-11fmtl4"&&(Xb.innerHTML=xS),Gk=a(Kt),u(fi.$$.fragment,Kt),Kt.forEach(d),Ot.forEach(d),B2=a(o),u(um.$$.fragment,o),A2=a(o),Ze=l(o,"DIV",{class:!0});var en=C(Ze);u(hm.$$.fragment,en),Ek=a(en),qb=l(en,"P",{"data-svelte-h":!0}),f(qb)!=="svelte-bs2pyf"&&(qb.innerHTML=$S),Pk=a(en),Qb=l(en,"P",{"data-svelte-h":!0}),f(Qb)!=="svelte-1n2cphd"&&(Qb.innerHTML=kS),Sk=a(en),On=l(en,"DIV",{class:!0});var ya=C(On);u(bm.$$.fragment,ya),Uk=a(ya),Hb=l(ya,"P",{"data-svelte-h":!0}),f(Hb)!=="svelte-6xp6fj"&&(Hb.textContent=ZS),Ik=a(ya),Db=l(ya,"P",{"data-svelte-h":!0}),f(Db)!=="svelte-6ap5g8"&&(Db.innerHTML=LS),Nk=a(ya),u(gi.$$.fragment,ya),ya.forEach(d),Xk=a(en),E=l(en,"DIV",{class:!0});var Bo=C(E);u(_m.$$.fragment,Bo),qk=a(Bo),Yb=l(Bo,"P",{"data-svelte-h":!0}),f(Yb)!=="svelte-10e8w47"&&(Yb.textContent=BS),Qk=a(Bo),zb=l(Bo,"P",{"data-svelte-h":!0}),f(zb)!=="svelte-1enn9p2"&&(zb.innerHTML=AS),Hk=a(Bo),Ob=l(Bo,"UL",{"data-svelte-h":!0}),f(Ob)!=="svelte-u5en2s"&&(Ob.innerHTML=RS),Dk=a(Bo),Kb=l(Bo,"P",{"data-svelte-h":!0}),f(Kb)!=="svelte-tebhz6"&&(Kb.innerHTML=WS),Yk=a(Bo),u(pi.$$.fragment,Bo),Bo.forEach(d),en.forEach(d),R2=a(o),u(Mm.$$.fragment,o),W2=a(o),Le=l(o,"DIV",{class:!0});var on=C(Le);u(Tm.$$.fragment,on),zk=a(on),e_=l(on,"P",{"data-svelte-h":!0}),f(e_)!=="svelte-bs2pyf"&&(e_.innerHTML=JS),Ok=a(on),o_=l(on,"P",{"data-svelte-h":!0}),f(o_)!=="svelte-1n2cphd"&&(o_.innerHTML=VS),Kk=a(on),Kn=l(on,"DIV",{class:!0});var Fa=C(Kn);u(ym.$$.fragment,Fa),eZ=a(Fa),t_=l(Fa,"P",{"data-svelte-h":!0}),f(t_)!=="svelte-6xp6fj"&&(t_.textContent=GS),oZ=a(Fa),n_=l(Fa,"P",{"data-svelte-h":!0}),f(n_)!=="svelte-6ap5g8"&&(n_.innerHTML=ES),tZ=a(Fa),u(ui.$$.fragment,Fa),Fa.forEach(d),nZ=a(on),et=l(on,"DIV",{class:!0});var tn=C(et);u(Fm.$$.fragment,tn),rZ=a(tn),r_=l(tn,"P",{"data-svelte-h":!0}),f(r_)!=="svelte-10e8w47"&&(r_.textContent=PS),aZ=a(tn),a_=l(tn,"P",{"data-svelte-h":!0}),f(a_)!=="svelte-1enn9p2"&&(a_.innerHTML=SS),sZ=a(tn),s_=l(tn,"UL",{"data-svelte-h":!0}),f(s_)!=="svelte-toypg2"&&(s_.innerHTML=US),iZ=a(tn),u(hi.$$.fragment,tn),tn.forEach(d),on.forEach(d),J2=a(o),u(vm.$$.fragment,o),V2=a(o),Be=l(o,"DIV",{class:!0});var nn=C(Be);u(Cm.$$.fragment,nn),lZ=a(nn),i_=l(nn,"P",{"data-svelte-h":!0}),f(i_)!=="svelte-bs2pyf"&&(i_.innerHTML=IS),dZ=a(nn),l_=l(nn,"P",{"data-svelte-h":!0}),f(l_)!=="svelte-1n2cphd"&&(l_.innerHTML=NS),cZ=a(nn),er=l(nn,"DIV",{class:!0});var va=C(er);u(wm.$$.fragment,va),mZ=a(va),d_=l(va,"P",{"data-svelte-h":!0}),f(d_)!=="svelte-6xp6fj"&&(d_.textContent=XS),fZ=a(va),c_=l(va,"P",{"data-svelte-h":!0}),f(c_)!=="svelte-6ap5g8"&&(c_.innerHTML=qS),gZ=a(va),u(bi.$$.fragment,va),va.forEach(d),pZ=a(nn),ot=l(nn,"DIV",{class:!0});var rn=C(ot);u(jm.$$.fragment,rn),uZ=a(rn),m_=l(rn,"P",{"data-svelte-h":!0}),f(m_)!=="svelte-10e8w47"&&(m_.textContent=QS),hZ=a(rn),f_=l(rn,"P",{"data-svelte-h":!0}),f(f_)!=="svelte-1enn9p2"&&(f_.innerHTML=HS),bZ=a(rn),g_=l(rn,"UL",{"data-svelte-h":!0}),f(g_)!=="svelte-8a0335"&&(g_.innerHTML=DS),_Z=a(rn),u(_i.$$.fragment,rn),rn.forEach(d),nn.forEach(d),G2=a(o),u(xm.$$.fragment,o),E2=a(o),Ae=l(o,"DIV",{class:!0});var an=C(Ae);u($m.$$.fragment,an),MZ=a(an),p_=l(an,"P",{"data-svelte-h":!0}),f(p_)!=="svelte-1pyjvqn"&&(p_.innerHTML=YS),TZ=a(an),u_=l(an,"P",{"data-svelte-h":!0}),f(u_)!=="svelte-1n2cphd"&&(u_.innerHTML=zS),yZ=a(an),or=l(an,"DIV",{class:!0});var Ca=C(or);u(km.$$.fragment,Ca),FZ=a(Ca),h_=l(Ca,"P",{"data-svelte-h":!0}),f(h_)!=="svelte-snlhsn"&&(h_.textContent=OS),vZ=a(Ca),b_=l(Ca,"P",{"data-svelte-h":!0}),f(b_)!=="svelte-6ap5g8"&&(b_.innerHTML=KS),CZ=a(Ca),u(Mi.$$.fragment,Ca),Ca.forEach(d),wZ=a(an),P=l(an,"DIV",{class:!0});var Ao=C(P);u(Zm.$$.fragment,Ao),jZ=a(Ao),__=l(Ao,"P",{"data-svelte-h":!0}),f(__)!=="svelte-jtqyi7"&&(__.textContent=eU),xZ=a(Ao),M_=l(Ao,"P",{"data-svelte-h":!0}),f(M_)!=="svelte-1enn9p2"&&(M_.innerHTML=oU),$Z=a(Ao),T_=l(Ao,"UL",{"data-svelte-h":!0}),f(T_)!=="svelte-9dj0u7"&&(T_.innerHTML=tU),kZ=a(Ao),y_=l(Ao,"P",{"data-svelte-h":!0}),f(y_)!=="svelte-tebhz6"&&(y_.innerHTML=nU),ZZ=a(Ao),u(Ti.$$.fragment,Ao),Ao.forEach(d),an.forEach(d),P2=a(o),u(Lm.$$.fragment,o),S2=a(o),Re=l(o,"DIV",{class:!0});var sn=C(Re);u(Bm.$$.fragment,sn),LZ=a(sn),F_=l(sn,"P",{"data-svelte-h":!0}),f(F_)!=="svelte-1pyjvqn"&&(F_.innerHTML=rU),BZ=a(sn),v_=l(sn,"P",{"data-svelte-h":!0}),f(v_)!=="svelte-1n2cphd"&&(v_.innerHTML=aU),AZ=a(sn),tr=l(sn,"DIV",{class:!0});var wa=C(tr);u(Am.$$.fragment,wa),RZ=a(wa),C_=l(wa,"P",{"data-svelte-h":!0}),f(C_)!=="svelte-snlhsn"&&(C_.textContent=sU),WZ=a(wa),w_=l(wa,"P",{"data-svelte-h":!0}),f(w_)!=="svelte-6ap5g8"&&(w_.innerHTML=iU),JZ=a(wa),u(yi.$$.fragment,wa),wa.forEach(d),VZ=a(sn),tt=l(sn,"DIV",{class:!0});var ln=C(tt);u(Rm.$$.fragment,ln),GZ=a(ln),j_=l(ln,"P",{"data-svelte-h":!0}),f(j_)!=="svelte-jtqyi7"&&(j_.textContent=lU),EZ=a(ln),x_=l(ln,"P",{"data-svelte-h":!0}),f(x_)!=="svelte-1enn9p2"&&(x_.innerHTML=dU),PZ=a(ln),$_=l(ln,"UL",{"data-svelte-h":!0}),f($_)!=="svelte-d4qbny"&&($_.innerHTML=cU),SZ=a(ln),u(Fi.$$.fragment,ln),ln.forEach(d),sn.forEach(d),U2=a(o),u(Wm.$$.fragment,o),I2=a(o),We=l(o,"DIV",{class:!0});var dn=C(We);u(Jm.$$.fragment,dn),UZ=a(dn),k_=l(dn,"P",{"data-svelte-h":!0}),f(k_)!=="svelte-1pyjvqn"&&(k_.innerHTML=mU),IZ=a(dn),Z_=l(dn,"P",{"data-svelte-h":!0}),f(Z_)!=="svelte-1n2cphd"&&(Z_.innerHTML=fU),NZ=a(dn),nr=l(dn,"DIV",{class:!0});var ja=C(nr);u(Vm.$$.fragment,ja),XZ=a(ja),L_=l(ja,"P",{"data-svelte-h":!0}),f(L_)!=="svelte-snlhsn"&&(L_.textContent=gU),qZ=a(ja),B_=l(ja,"P",{"data-svelte-h":!0}),f(B_)!=="svelte-6ap5g8"&&(B_.innerHTML=pU),QZ=a(ja),u(vi.$$.fragment,ja),ja.forEach(d),HZ=a(dn),nt=l(dn,"DIV",{class:!0});var cn=C(nt);u(Gm.$$.fragment,cn),DZ=a(cn),A_=l(cn,"P",{"data-svelte-h":!0}),f(A_)!=="svelte-jtqyi7"&&(A_.textContent=uU),YZ=a(cn),R_=l(cn,"P",{"data-svelte-h":!0}),f(R_)!=="svelte-1enn9p2"&&(R_.innerHTML=hU),zZ=a(cn),W_=l(cn,"UL",{"data-svelte-h":!0}),f(W_)!=="svelte-162mdc5"&&(W_.innerHTML=bU),OZ=a(cn),u(Ci.$$.fragment,cn),cn.forEach(d),dn.forEach(d),N2=a(o),u(Em.$$.fragment,o),X2=a(o),Je=l(o,"DIV",{class:!0});var mn=C(Je);u(Pm.$$.fragment,mn),KZ=a(mn),J_=l(mn,"P",{"data-svelte-h":!0}),f(J_)!=="svelte-jgr8ed"&&(J_.innerHTML=_U),eL=a(mn),V_=l(mn,"P",{"data-svelte-h":!0}),f(V_)!=="svelte-1n2cphd"&&(V_.innerHTML=MU),oL=a(mn),rr=l(mn,"DIV",{class:!0});var xa=C(rr);u(Sm.$$.fragment,xa),tL=a(xa),G_=l(xa,"P",{"data-svelte-h":!0}),f(G_)!=="svelte-1yu679h"&&(G_.textContent=TU),nL=a(xa),E_=l(xa,"P",{"data-svelte-h":!0}),f(E_)!=="svelte-6ap5g8"&&(E_.innerHTML=yU),rL=a(xa),u(wi.$$.fragment,xa),xa.forEach(d),aL=a(mn),S=l(mn,"DIV",{class:!0});var Ro=C(S);u(Um.$$.fragment,Ro),sL=a(Ro),P_=l(Ro,"P",{"data-svelte-h":!0}),f(P_)!=="svelte-hx8awn"&&(P_.textContent=FU),iL=a(Ro),S_=l(Ro,"P",{"data-svelte-h":!0}),f(S_)!=="svelte-1enn9p2"&&(S_.innerHTML=vU),lL=a(Ro),U_=l(Ro,"UL",{"data-svelte-h":!0}),f(U_)!=="svelte-1910gna"&&(U_.innerHTML=CU),dL=a(Ro),I_=l(Ro,"P",{"data-svelte-h":!0}),f(I_)!=="svelte-tebhz6"&&(I_.innerHTML=wU),cL=a(Ro),u(ji.$$.fragment,Ro),Ro.forEach(d),mn.forEach(d),q2=a(o),u(Im.$$.fragment,o),Q2=a(o),Ve=l(o,"DIV",{class:!0});var fn=C(Ve);u(Nm.$$.fragment,fn),mL=a(fn),N_=l(fn,"P",{"data-svelte-h":!0}),f(N_)!=="svelte-jgr8ed"&&(N_.innerHTML=jU),fL=a(fn),X_=l(fn,"P",{"data-svelte-h":!0}),f(X_)!=="svelte-1n2cphd"&&(X_.innerHTML=xU),gL=a(fn),ar=l(fn,"DIV",{class:!0});var $a=C(ar);u(Xm.$$.fragment,$a),pL=a($a),q_=l($a,"P",{"data-svelte-h":!0}),f(q_)!=="svelte-1yu679h"&&(q_.textContent=$U),uL=a($a),Q_=l($a,"P",{"data-svelte-h":!0}),f(Q_)!=="svelte-6ap5g8"&&(Q_.innerHTML=kU),hL=a($a),u(xi.$$.fragment,$a),$a.forEach(d),bL=a(fn),rt=l(fn,"DIV",{class:!0});var gn=C(rt);u(qm.$$.fragment,gn),_L=a(gn),H_=l(gn,"P",{"data-svelte-h":!0}),f(H_)!=="svelte-hx8awn"&&(H_.textContent=ZU),ML=a(gn),D_=l(gn,"P",{"data-svelte-h":!0}),f(D_)!=="svelte-1enn9p2"&&(D_.innerHTML=LU),TL=a(gn),Y_=l(gn,"UL",{"data-svelte-h":!0}),f(Y_)!=="svelte-46kt5y"&&(Y_.innerHTML=BU),yL=a(gn),u($i.$$.fragment,gn),gn.forEach(d),fn.forEach(d),H2=a(o),u(Qm.$$.fragment,o),D2=a(o),Ge=l(o,"DIV",{class:!0});var pn=C(Ge);u(Hm.$$.fragment,pn),FL=a(pn),z_=l(pn,"P",{"data-svelte-h":!0}),f(z_)!=="svelte-jgr8ed"&&(z_.innerHTML=AU),vL=a(pn),O_=l(pn,"P",{"data-svelte-h":!0}),f(O_)!=="svelte-1n2cphd"&&(O_.innerHTML=RU),CL=a(pn),sr=l(pn,"DIV",{class:!0});var ka=C(sr);u(Dm.$$.fragment,ka),wL=a(ka),K_=l(ka,"P",{"data-svelte-h":!0}),f(K_)!=="svelte-1yu679h"&&(K_.textContent=WU),jL=a(ka),eM=l(ka,"P",{"data-svelte-h":!0}),f(eM)!=="svelte-6ap5g8"&&(eM.innerHTML=JU),xL=a(ka),u(ki.$$.fragment,ka),ka.forEach(d),$L=a(pn),at=l(pn,"DIV",{class:!0});var un=C(at);u(Ym.$$.fragment,un),kL=a(un),oM=l(un,"P",{"data-svelte-h":!0}),f(oM)!=="svelte-hx8awn"&&(oM.textContent=VU),ZL=a(un),tM=l(un,"P",{"data-svelte-h":!0}),f(tM)!=="svelte-1enn9p2"&&(tM.innerHTML=GU),LL=a(un),nM=l(un,"UL",{"data-svelte-h":!0}),f(nM)!=="svelte-z0ross"&&(nM.innerHTML=EU),BL=a(un),u(Zi.$$.fragment,un),un.forEach(d),pn.forEach(d),Y2=a(o),u(zm.$$.fragment,o),z2=a(o),Ee=l(o,"DIV",{class:!0});var hn=C(Ee);u(Om.$$.fragment,hn),AL=a(hn),rM=l(hn,"P",{"data-svelte-h":!0}),f(rM)!=="svelte-lquxh9"&&(rM.innerHTML=PU),RL=a(hn),aM=l(hn,"P",{"data-svelte-h":!0}),f(aM)!=="svelte-1n2cphd"&&(aM.innerHTML=SU),WL=a(hn),ir=l(hn,"DIV",{class:!0});var Za=C(ir);u(Km.$$.fragment,Za),JL=a(Za),sM=l(Za,"P",{"data-svelte-h":!0}),f(sM)!=="svelte-81lf85"&&(sM.textContent=UU),VL=a(Za),iM=l(Za,"P",{"data-svelte-h":!0}),f(iM)!=="svelte-6ap5g8"&&(iM.innerHTML=IU),GL=a(Za),u(Li.$$.fragment,Za),Za.forEach(d),EL=a(hn),U=l(hn,"DIV",{class:!0});var Wo=C(U);u(ef.$$.fragment,Wo),PL=a(Wo),lM=l(Wo,"P",{"data-svelte-h":!0}),f(lM)!=="svelte-16vvmzn"&&(lM.textContent=NU),SL=a(Wo),dM=l(Wo,"P",{"data-svelte-h":!0}),f(dM)!=="svelte-1enn9p2"&&(dM.innerHTML=XU),UL=a(Wo),cM=l(Wo,"UL",{"data-svelte-h":!0}),f(cM)!=="svelte-xco91f"&&(cM.innerHTML=qU),IL=a(Wo),mM=l(Wo,"P",{"data-svelte-h":!0}),f(mM)!=="svelte-tebhz6"&&(mM.innerHTML=QU),NL=a(Wo),u(Bi.$$.fragment,Wo),Wo.forEach(d),hn.forEach(d),O2=a(o),u(of.$$.fragment,o),K2=a(o),Pe=l(o,"DIV",{class:!0});var bn=C(Pe);u(tf.$$.fragment,bn),XL=a(bn),fM=l(bn,"P",{"data-svelte-h":!0}),f(fM)!=="svelte-lquxh9"&&(fM.innerHTML=HU),qL=a(bn),gM=l(bn,"P",{"data-svelte-h":!0}),f(gM)!=="svelte-1n2cphd"&&(gM.innerHTML=DU),QL=a(bn),lr=l(bn,"DIV",{class:!0});var La=C(lr);u(nf.$$.fragment,La),HL=a(La),pM=l(La,"P",{"data-svelte-h":!0}),f(pM)!=="svelte-81lf85"&&(pM.textContent=YU),DL=a(La),uM=l(La,"P",{"data-svelte-h":!0}),f(uM)!=="svelte-6ap5g8"&&(uM.innerHTML=zU),YL=a(La),u(Ai.$$.fragment,La),La.forEach(d),zL=a(bn),st=l(bn,"DIV",{class:!0});var _n=C(st);u(rf.$$.fragment,_n),OL=a(_n),hM=l(_n,"P",{"data-svelte-h":!0}),f(hM)!=="svelte-16vvmzn"&&(hM.textContent=OU),KL=a(_n),bM=l(_n,"P",{"data-svelte-h":!0}),f(bM)!=="svelte-1enn9p2"&&(bM.innerHTML=KU),eB=a(_n),_M=l(_n,"UL",{"data-svelte-h":!0}),f(_M)!=="svelte-1gqc5od"&&(_M.innerHTML=e9),oB=a(_n),u(Ri.$$.fragment,_n),_n.forEach(d),bn.forEach(d),ew=a(o),u(af.$$.fragment,o),ow=a(o),Se=l(o,"DIV",{class:!0});var Mn=C(Se);u(sf.$$.fragment,Mn),tB=a(Mn),MM=l(Mn,"P",{"data-svelte-h":!0}),f(MM)!=="svelte-lquxh9"&&(MM.innerHTML=o9),nB=a(Mn),TM=l(Mn,"P",{"data-svelte-h":!0}),f(TM)!=="svelte-1n2cphd"&&(TM.innerHTML=t9),rB=a(Mn),dr=l(Mn,"DIV",{class:!0});var Ba=C(dr);u(lf.$$.fragment,Ba),aB=a(Ba),yM=l(Ba,"P",{"data-svelte-h":!0}),f(yM)!=="svelte-81lf85"&&(yM.textContent=n9),sB=a(Ba),FM=l(Ba,"P",{"data-svelte-h":!0}),f(FM)!=="svelte-6ap5g8"&&(FM.innerHTML=r9),iB=a(Ba),u(Wi.$$.fragment,Ba),Ba.forEach(d),lB=a(Mn),it=l(Mn,"DIV",{class:!0});var Tn=C(it);u(df.$$.fragment,Tn),dB=a(Tn),vM=l(Tn,"P",{"data-svelte-h":!0}),f(vM)!=="svelte-16vvmzn"&&(vM.textContent=a9),cB=a(Tn),CM=l(Tn,"P",{"data-svelte-h":!0}),f(CM)!=="svelte-1enn9p2"&&(CM.innerHTML=s9),mB=a(Tn),wM=l(Tn,"UL",{"data-svelte-h":!0}),f(wM)!=="svelte-j6wr59"&&(wM.innerHTML=i9),fB=a(Tn),u(Ji.$$.fragment,Tn),Tn.forEach(d),Mn.forEach(d),tw=a(o),u(cf.$$.fragment,o),nw=a(o),Ue=l(o,"DIV",{class:!0});var yn=C(Ue);u(mf.$$.fragment,yn),gB=a(yn),jM=l(yn,"P",{"data-svelte-h":!0}),f(jM)!=="svelte-1x34mmw"&&(jM.innerHTML=l9),pB=a(yn),xM=l(yn,"P",{"data-svelte-h":!0}),f(xM)!=="svelte-1n2cphd"&&(xM.innerHTML=d9),uB=a(yn),cr=l(yn,"DIV",{class:!0});var Aa=C(cr);u(ff.$$.fragment,Aa),hB=a(Aa),$M=l(Aa,"P",{"data-svelte-h":!0}),f($M)!=="svelte-n52yuc"&&($M.textContent=c9),bB=a(Aa),kM=l(Aa,"P",{"data-svelte-h":!0}),f(kM)!=="svelte-6ap5g8"&&(kM.innerHTML=m9),_B=a(Aa),u(Vi.$$.fragment,Aa),Aa.forEach(d),MB=a(yn),I=l(yn,"DIV",{class:!0});var Jo=C(I);u(gf.$$.fragment,Jo),TB=a(Jo),ZM=l(Jo,"P",{"data-svelte-h":!0}),f(ZM)!=="svelte-1h7oepk"&&(ZM.textContent=f9),yB=a(Jo),LM=l(Jo,"P",{"data-svelte-h":!0}),f(LM)!=="svelte-1enn9p2"&&(LM.innerHTML=g9),FB=a(Jo),BM=l(Jo,"UL",{"data-svelte-h":!0}),f(BM)!=="svelte-12nhs1g"&&(BM.innerHTML=p9),vB=a(Jo),AM=l(Jo,"P",{"data-svelte-h":!0}),f(AM)!=="svelte-tebhz6"&&(AM.innerHTML=u9),CB=a(Jo),u(Gi.$$.fragment,Jo),Jo.forEach(d),yn.forEach(d),rw=a(o),u(pf.$$.fragment,o),aw=a(o),Ie=l(o,"DIV",{class:!0});var Fn=C(Ie);u(uf.$$.fragment,Fn),wB=a(Fn),RM=l(Fn,"P",{"data-svelte-h":!0}),f(RM)!=="svelte-1x34mmw"&&(RM.innerHTML=h9),jB=a(Fn),WM=l(Fn,"P",{"data-svelte-h":!0}),f(WM)!=="svelte-1n2cphd"&&(WM.innerHTML=b9),xB=a(Fn),mr=l(Fn,"DIV",{class:!0});var Ra=C(mr);u(hf.$$.fragment,Ra),$B=a(Ra),JM=l(Ra,"P",{"data-svelte-h":!0}),f(JM)!=="svelte-n52yuc"&&(JM.textContent=_9),kB=a(Ra),VM=l(Ra,"P",{"data-svelte-h":!0}),f(VM)!=="svelte-6ap5g8"&&(VM.innerHTML=M9),ZB=a(Ra),u(Ei.$$.fragment,Ra),Ra.forEach(d),LB=a(Fn),lt=l(Fn,"DIV",{class:!0});var vn=C(lt);u(bf.$$.fragment,vn),BB=a(vn),GM=l(vn,"P",{"data-svelte-h":!0}),f(GM)!=="svelte-1h7oepk"&&(GM.textContent=T9),AB=a(vn),EM=l(vn,"P",{"data-svelte-h":!0}),f(EM)!=="svelte-1enn9p2"&&(EM.innerHTML=y9),RB=a(vn),PM=l(vn,"UL",{"data-svelte-h":!0}),f(PM)!=="svelte-1uxbjwa"&&(PM.innerHTML=F9),WB=a(vn),u(Pi.$$.fragment,vn),vn.forEach(d),Fn.forEach(d),sw=a(o),u(_f.$$.fragment,o),iw=a(o),Ne=l(o,"DIV",{class:!0});var Cn=C(Ne);u(Mf.$$.fragment,Cn),JB=a(Cn),SM=l(Cn,"P",{"data-svelte-h":!0}),f(SM)!=="svelte-1x34mmw"&&(SM.innerHTML=v9),VB=a(Cn),UM=l(Cn,"P",{"data-svelte-h":!0}),f(UM)!=="svelte-1n2cphd"&&(UM.innerHTML=C9),GB=a(Cn),fr=l(Cn,"DIV",{class:!0});var Wa=C(fr);u(Tf.$$.fragment,Wa),EB=a(Wa),IM=l(Wa,"P",{"data-svelte-h":!0}),f(IM)!=="svelte-n52yuc"&&(IM.textContent=w9),PB=a(Wa),NM=l(Wa,"P",{"data-svelte-h":!0}),f(NM)!=="svelte-6ap5g8"&&(NM.innerHTML=j9),SB=a(Wa),u(Si.$$.fragment,Wa),Wa.forEach(d),UB=a(Cn),dt=l(Cn,"DIV",{class:!0});var wn=C(dt);u(yf.$$.fragment,wn),IB=a(wn),XM=l(wn,"P",{"data-svelte-h":!0}),f(XM)!=="svelte-1h7oepk"&&(XM.textContent=x9),NB=a(wn),qM=l(wn,"P",{"data-svelte-h":!0}),f(qM)!=="svelte-1enn9p2"&&(qM.innerHTML=$9),XB=a(wn),QM=l(wn,"UL",{"data-svelte-h":!0}),f(QM)!=="svelte-v3x8xw"&&(QM.innerHTML=k9),qB=a(wn),u(Ui.$$.fragment,wn),wn.forEach(d),Cn.forEach(d),lw=a(o),u(Ff.$$.fragment,o),dw=a(o),vf=l(o,"DIV",{class:!0});var wC=C(vf);u(Cf.$$.fragment,wC),wC.forEach(d),cw=a(o),u(wf.$$.fragment,o),mw=a(o),jf=l(o,"DIV",{class:!0});var jC=C(jf);u(xf.$$.fragment,jC),jC.forEach(d),fw=a(o),u($f.$$.fragment,o),gw=a(o),kf=l(o,"P",{"data-svelte-h":!0}),f(kf)!=="svelte-1winzdz"&&(kf.textContent=Z9),pw=a(o),u(Zf.$$.fragment,o),uw=a(o),Xe=l(o,"DIV",{class:!0});var jn=C(Xe);u(Lf.$$.fragment,jn),QB=a(jn),HM=l(jn,"P",{"data-svelte-h":!0}),f(HM)!=="svelte-174cwl2"&&(HM.innerHTML=L9),HB=a(jn),DM=l(jn,"P",{"data-svelte-h":!0}),f(DM)!=="svelte-1n2cphd"&&(DM.innerHTML=B9),DB=a(jn),gr=l(jn,"DIV",{class:!0});var Ja=C(gr);u(Bf.$$.fragment,Ja),YB=a(Ja),YM=l(Ja,"P",{"data-svelte-h":!0}),f(YM)!=="svelte-1o4g80a"&&(YM.textContent=A9),zB=a(Ja),zM=l(Ja,"P",{"data-svelte-h":!0}),f(zM)!=="svelte-6ap5g8"&&(zM.innerHTML=R9),OB=a(Ja),u(Ii.$$.fragment,Ja),Ja.forEach(d),KB=a(jn),N=l(jn,"DIV",{class:!0});var Vo=C(N);u(Af.$$.fragment,Vo),eA=a(Vo),OM=l(Vo,"P",{"data-svelte-h":!0}),f(OM)!=="svelte-100to3e"&&(OM.textContent=W9),oA=a(Vo),KM=l(Vo,"P",{"data-svelte-h":!0}),f(KM)!=="svelte-1enn9p2"&&(KM.innerHTML=J9),tA=a(Vo),eT=l(Vo,"UL",{"data-svelte-h":!0}),f(eT)!=="svelte-r4gkin"&&(eT.innerHTML=V9),nA=a(Vo),oT=l(Vo,"P",{"data-svelte-h":!0}),f(oT)!=="svelte-tebhz6"&&(oT.innerHTML=G9),rA=a(Vo),u(Ni.$$.fragment,Vo),Vo.forEach(d),jn.forEach(d),hw=a(o),u(Rf.$$.fragment,o),bw=a(o),qe=l(o,"DIV",{class:!0});var xn=C(qe);u(Wf.$$.fragment,xn),aA=a(xn),tT=l(xn,"P",{"data-svelte-h":!0}),f(tT)!=="svelte-p5cf2d"&&(tT.innerHTML=E9),sA=a(xn),nT=l(xn,"P",{"data-svelte-h":!0}),f(nT)!=="svelte-1n2cphd"&&(nT.innerHTML=P9),iA=a(xn),pr=l(xn,"DIV",{class:!0});var Va=C(pr);u(Jf.$$.fragment,Va),lA=a(Va),rT=l(Va,"P",{"data-svelte-h":!0}),f(rT)!=="svelte-3mnohx"&&(rT.textContent=S9),dA=a(Va),aT=l(Va,"P",{"data-svelte-h":!0}),f(aT)!=="svelte-6ap5g8"&&(aT.innerHTML=U9),cA=a(Va),u(Xi.$$.fragment,Va),Va.forEach(d),mA=a(xn),X=l(xn,"DIV",{class:!0});var Go=C(X);u(Vf.$$.fragment,Go),fA=a(Go),sT=l(Go,"P",{"data-svelte-h":!0}),f(sT)!=="svelte-1ezjudv"&&(sT.textContent=I9),gA=a(Go),iT=l(Go,"P",{"data-svelte-h":!0}),f(iT)!=="svelte-1enn9p2"&&(iT.innerHTML=N9),pA=a(Go),lT=l(Go,"UL",{"data-svelte-h":!0}),f(lT)!=="svelte-s2nutl"&&(lT.innerHTML=X9),uA=a(Go),dT=l(Go,"P",{"data-svelte-h":!0}),f(dT)!=="svelte-tebhz6"&&(dT.innerHTML=q9),hA=a(Go),u(qi.$$.fragment,Go),Go.forEach(d),xn.forEach(d),_w=a(o),u(Gf.$$.fragment,o),Mw=a(o),Qe=l(o,"DIV",{class:!0});var $n=C(Qe);u(Ef.$$.fragment,$n),bA=a($n),cT=l($n,"P",{"data-svelte-h":!0}),f(cT)!=="svelte-p5cf2d"&&(cT.innerHTML=Q9),_A=a($n),mT=l($n,"P",{"data-svelte-h":!0}),f(mT)!=="svelte-1n2cphd"&&(mT.innerHTML=H9),MA=a($n),ur=l($n,"DIV",{class:!0});var Ga=C(ur);u(Pf.$$.fragment,Ga),TA=a(Ga),fT=l(Ga,"P",{"data-svelte-h":!0}),f(fT)!=="svelte-3mnohx"&&(fT.textContent=D9),yA=a(Ga),gT=l(Ga,"P",{"data-svelte-h":!0}),f(gT)!=="svelte-6ap5g8"&&(gT.innerHTML=Y9),FA=a(Ga),u(Qi.$$.fragment,Ga),Ga.forEach(d),vA=a($n),ct=l($n,"DIV",{class:!0});var kn=C(ct);u(Sf.$$.fragment,kn),CA=a(kn),pT=l(kn,"P",{"data-svelte-h":!0}),f(pT)!=="svelte-1ezjudv"&&(pT.textContent=z9),wA=a(kn),uT=l(kn,"P",{"data-svelte-h":!0}),f(uT)!=="svelte-1enn9p2"&&(uT.innerHTML=O9),jA=a(kn),hT=l(kn,"UL",{"data-svelte-h":!0}),f(hT)!=="svelte-15bjwjg"&&(hT.innerHTML=K9),xA=a(kn),u(Hi.$$.fragment,kn),kn.forEach(d),$n.forEach(d),Tw=a(o),u(Uf.$$.fragment,o),yw=a(o),He=l(o,"DIV",{class:!0});var Zn=C(He);u(If.$$.fragment,Zn),$A=a(Zn),bT=l(Zn,"P",{"data-svelte-h":!0}),f(bT)!=="svelte-p5cf2d"&&(bT.innerHTML=eI),kA=a(Zn),_T=l(Zn,"P",{"data-svelte-h":!0}),f(_T)!=="svelte-1n2cphd"&&(_T.innerHTML=oI),ZA=a(Zn),hr=l(Zn,"DIV",{class:!0});var Ea=C(hr);u(Nf.$$.fragment,Ea),LA=a(Ea),MT=l(Ea,"P",{"data-svelte-h":!0}),f(MT)!=="svelte-3mnohx"&&(MT.textContent=tI),BA=a(Ea),TT=l(Ea,"P",{"data-svelte-h":!0}),f(TT)!=="svelte-6ap5g8"&&(TT.innerHTML=nI),AA=a(Ea),u(Di.$$.fragment,Ea),Ea.forEach(d),RA=a(Zn),mt=l(Zn,"DIV",{class:!0});var Ln=C(mt);u(Xf.$$.fragment,Ln),WA=a(Ln),yT=l(Ln,"P",{"data-svelte-h":!0}),f(yT)!=="svelte-1ezjudv"&&(yT.textContent=rI),JA=a(Ln),FT=l(Ln,"P",{"data-svelte-h":!0}),f(FT)!=="svelte-1enn9p2"&&(FT.innerHTML=aI),VA=a(Ln),vT=l(Ln,"UL",{"data-svelte-h":!0}),f(vT)!=="svelte-5iphzy"&&(vT.innerHTML=sI),GA=a(Ln),u(Yi.$$.fragment,Ln),Ln.forEach(d),Zn.forEach(d),Fw=a(o),u(qf.$$.fragment,o),vw=a(o),De=l(o,"DIV",{class:!0});var Bn=C(De);u(Qf.$$.fragment,Bn),EA=a(Bn),CT=l(Bn,"P",{"data-svelte-h":!0}),f(CT)!=="svelte-d685lj"&&(CT.innerHTML=iI),PA=a(Bn),wT=l(Bn,"P",{"data-svelte-h":!0}),f(wT)!=="svelte-1n2cphd"&&(wT.innerHTML=lI),SA=a(Bn),br=l(Bn,"DIV",{class:!0});var Pa=C(br);u(Hf.$$.fragment,Pa),UA=a(Pa),jT=l(Pa,"P",{"data-svelte-h":!0}),f(jT)!=="svelte-1269scv"&&(jT.textContent=dI),IA=a(Pa),xT=l(Pa,"P",{"data-svelte-h":!0}),f(xT)!=="svelte-6ap5g8"&&(xT.innerHTML=cI),NA=a(Pa),u(zi.$$.fragment,Pa),Pa.forEach(d),XA=a(Bn),q=l(Bn,"DIV",{class:!0});var Eo=C(q);u(Df.$$.fragment,Eo),qA=a(Eo),$T=l(Eo,"P",{"data-svelte-h":!0}),f($T)!=="svelte-xeutyd"&&($T.textContent=mI),QA=a(Eo),kT=l(Eo,"P",{"data-svelte-h":!0}),f(kT)!=="svelte-1enn9p2"&&(kT.innerHTML=fI),HA=a(Eo),ZT=l(Eo,"UL",{"data-svelte-h":!0}),f(ZT)!=="svelte-dsbz7i"&&(ZT.innerHTML=gI),DA=a(Eo),LT=l(Eo,"P",{"data-svelte-h":!0}),f(LT)!=="svelte-tebhz6"&&(LT.innerHTML=pI),YA=a(Eo),u(Oi.$$.fragment,Eo),Eo.forEach(d),Bn.forEach(d),Cw=a(o),u(Yf.$$.fragment,o),ww=a(o),Ye=l(o,"DIV",{class:!0});var An=C(Ye);u(zf.$$.fragment,An),zA=a(An),BT=l(An,"P",{"data-svelte-h":!0}),f(BT)!=="svelte-1jdj5df"&&(BT.innerHTML=uI),OA=a(An),AT=l(An,"P",{"data-svelte-h":!0}),f(AT)!=="svelte-1n2cphd"&&(AT.innerHTML=hI),KA=a(An),_r=l(An,"DIV",{class:!0});var Sa=C(_r);u(Of.$$.fragment,Sa),eR=a(Sa),RT=l(Sa,"P",{"data-svelte-h":!0}),f(RT)!=="svelte-vmkzub"&&(RT.textContent=bI),oR=a(Sa),WT=l(Sa,"P",{"data-svelte-h":!0}),f(WT)!=="svelte-6ap5g8"&&(WT.innerHTML=_I),tR=a(Sa),u(Ki.$$.fragment,Sa),Sa.forEach(d),nR=a(An),Q=l(An,"DIV",{class:!0});var Po=C(Q);u(Kf.$$.fragment,Po),rR=a(Po),JT=l(Po,"P",{"data-svelte-h":!0}),f(JT)!=="svelte-up2an7"&&(JT.textContent=MI),aR=a(Po),VT=l(Po,"P",{"data-svelte-h":!0}),f(VT)!=="svelte-1enn9p2"&&(VT.innerHTML=TI),sR=a(Po),GT=l(Po,"UL",{"data-svelte-h":!0}),f(GT)!=="svelte-ut874g"&&(GT.innerHTML=yI),iR=a(Po),ET=l(Po,"P",{"data-svelte-h":!0}),f(ET)!=="svelte-tebhz6"&&(ET.innerHTML=FI),lR=a(Po),u(el.$$.fragment,Po),Po.forEach(d),An.forEach(d),jw=a(o),u(eg.$$.fragment,o),xw=a(o),ze=l(o,"DIV",{class:!0});var Rn=C(ze);u(og.$$.fragment,Rn),dR=a(Rn),PT=l(Rn,"P",{"data-svelte-h":!0}),f(PT)!=="svelte-1jdj5df"&&(PT.innerHTML=vI),cR=a(Rn),ST=l(Rn,"P",{"data-svelte-h":!0}),f(ST)!=="svelte-1n2cphd"&&(ST.innerHTML=CI),mR=a(Rn),Mr=l(Rn,"DIV",{class:!0});var Ua=C(Mr);u(tg.$$.fragment,Ua),fR=a(Ua),UT=l(Ua,"P",{"data-svelte-h":!0}),f(UT)!=="svelte-vmkzub"&&(UT.textContent=wI),gR=a(Ua),IT=l(Ua,"P",{"data-svelte-h":!0}),f(IT)!=="svelte-6ap5g8"&&(IT.innerHTML=jI),pR=a(Ua),u(ol.$$.fragment,Ua),Ua.forEach(d),uR=a(Rn),ft=l(Rn,"DIV",{class:!0});var Wn=C(ft);u(ng.$$.fragment,Wn),hR=a(Wn),NT=l(Wn,"P",{"data-svelte-h":!0}),f(NT)!=="svelte-up2an7"&&(NT.textContent=xI),bR=a(Wn),XT=l(Wn,"P",{"data-svelte-h":!0}),f(XT)!=="svelte-1enn9p2"&&(XT.innerHTML=$I),_R=a(Wn),qT=l(Wn,"UL",{"data-svelte-h":!0}),f(qT)!=="svelte-rb4m7e"&&(qT.innerHTML=kI),MR=a(Wn),u(tl.$$.fragment,Wn),Wn.forEach(d),Rn.forEach(d),$w=a(o),u(rg.$$.fragment,o),kw=a(o),Oe=l(o,"DIV",{class:!0});var Jn=C(Oe);u(ag.$$.fragment,Jn),TR=a(Jn),QT=l(Jn,"P",{"data-svelte-h":!0}),f(QT)!=="svelte-bvqary"&&(QT.innerHTML=ZI),yR=a(Jn),HT=l(Jn,"P",{"data-svelte-h":!0}),f(HT)!=="svelte-1n2cphd"&&(HT.innerHTML=LI),FR=a(Jn),Tr=l(Jn,"DIV",{class:!0});var Ia=C(Tr);u(sg.$$.fragment,Ia),vR=a(Ia),DT=l(Ia,"P",{"data-svelte-h":!0}),f(DT)!=="svelte-1dstum2"&&(DT.textContent=BI),CR=a(Ia),YT=l(Ia,"P",{"data-svelte-h":!0}),f(YT)!=="svelte-6ap5g8"&&(YT.innerHTML=AI),wR=a(Ia),u(nl.$$.fragment,Ia),Ia.forEach(d),jR=a(Jn),H=l(Jn,"DIV",{class:!0});var So=C(H);u(ig.$$.fragment,So),xR=a(So),zT=l(So,"P",{"data-svelte-h":!0}),f(zT)!=="svelte-7xylam"&&(zT.textContent=RI),$R=a(So),OT=l(So,"P",{"data-svelte-h":!0}),f(OT)!=="svelte-1enn9p2"&&(OT.innerHTML=WI),kR=a(So),KT=l(So,"UL",{"data-svelte-h":!0}),f(KT)!=="svelte-1w2fh1c"&&(KT.innerHTML=JI),ZR=a(So),ey=l(So,"P",{"data-svelte-h":!0}),f(ey)!=="svelte-tebhz6"&&(ey.innerHTML=VI),LR=a(So),u(rl.$$.fragment,So),So.forEach(d),Jn.forEach(d),Zw=a(o),u(lg.$$.fragment,o),Lw=a(o),Ke=l(o,"DIV",{class:!0});var Na=C(Ke);u(dg.$$.fragment,Na),BR=a(Na),oy=l(Na,"P",{"data-svelte-h":!0}),f(oy)!=="svelte-18jcj53"&&(oy.innerHTML=GI),AR=a(Na),ty=l(Na,"P",{"data-svelte-h":!0}),f(ty)!=="svelte-1n2cphd"&&(ty.innerHTML=EI),RR=a(Na),yr=l(Na,"DIV",{class:!0});var Kl=C(yr);u(cg.$$.fragment,Kl),WR=a(Kl),ny=l(Kl,"P",{"data-svelte-h":!0}),f(ny)!=="svelte-a638l3"&&(ny.textContent=PI),JR=a(Kl),ry=l(Kl,"P",{"data-svelte-h":!0}),f(ry)!=="svelte-6ap5g8"&&(ry.innerHTML=SI),VR=a(Kl),u(al.$$.fragment,Kl),Kl.forEach(d),GR=a(Na),D=l(Na,"DIV",{class:!0});var Xr=C(D);u(mg.$$.fragment,Xr),ER=a(Xr),ay=l(Xr,"P",{"data-svelte-h":!0}),f(ay)!=="svelte-1rj5rvl"&&(ay.textContent=UI),PR=a(Xr),sy=l(Xr,"P",{"data-svelte-h":!0}),f(sy)!=="svelte-1enn9p2"&&(sy.innerHTML=II),SR=a(Xr),iy=l(Xr,"UL",{"data-svelte-h":!0}),f(iy)!=="svelte-1wddeb6"&&(iy.innerHTML=NI),UR=a(Xr),ly=l(Xr,"P",{"data-svelte-h":!0}),f(ly)!=="svelte-tebhz6"&&(ly.innerHTML=XI),IR=a(Xr),u(sl.$$.fragment,Xr),Xr.forEach(d),Na.forEach(d),Bw=a(o),u(fg.$$.fragment,o),Aw=a(o),gg=l(o,"DIV",{class:!0});var iX=C(gg);u(pg.$$.fragment,iX),iX.forEach(d),Rw=a(o),u(ug.$$.fragment,o),Ww=a(o),eo=l(o,"DIV",{class:!0});var Xa=C(eo);u(hg.$$.fragment,Xa),NR=a(Xa),dy=l(Xa,"P",{"data-svelte-h":!0}),f(dy)!=="svelte-1bqddtc"&&(dy.innerHTML=qI),XR=a(Xa),cy=l(Xa,"P",{"data-svelte-h":!0}),f(cy)!=="svelte-1n2cphd"&&(cy.innerHTML=QI),qR=a(Xa),Fr=l(Xa,"DIV",{class:!0});var ed=C(Fr);u(bg.$$.fragment,ed),QR=a(ed),my=l(ed,"P",{"data-svelte-h":!0}),f(my)!=="svelte-ne8wj0"&&(my.textContent=HI),HR=a(ed),fy=l(ed,"P",{"data-svelte-h":!0}),f(fy)!=="svelte-6ap5g8"&&(fy.innerHTML=DI),DR=a(ed),u(il.$$.fragment,ed),ed.forEach(d),YR=a(Xa),Y=l(Xa,"DIV",{class:!0});var qr=C(Y);u(_g.$$.fragment,qr),zR=a(qr),gy=l(qr,"P",{"data-svelte-h":!0}),f(gy)!=="svelte-xit1mq"&&(gy.textContent=YI),OR=a(qr),py=l(qr,"P",{"data-svelte-h":!0}),f(py)!=="svelte-1enn9p2"&&(py.innerHTML=zI),KR=a(qr),uy=l(qr,"UL",{"data-svelte-h":!0}),f(uy)!=="svelte-15ams7e"&&(uy.innerHTML=OI),eW=a(qr),hy=l(qr,"P",{"data-svelte-h":!0}),f(hy)!=="svelte-tebhz6"&&(hy.innerHTML=KI),oW=a(qr),u(ll.$$.fragment,qr),qr.forEach(d),Xa.forEach(d),Jw=a(o),u(Mg.$$.fragment,o),Vw=a(o),oo=l(o,"DIV",{class:!0});var qa=C(oo);u(Tg.$$.fragment,qa),tW=a(qa),by=l(qa,"P",{"data-svelte-h":!0}),f(by)!=="svelte-1bqddtc"&&(by.innerHTML=e5),nW=a(qa),_y=l(qa,"P",{"data-svelte-h":!0}),f(_y)!=="svelte-1n2cphd"&&(_y.innerHTML=o5),rW=a(qa),vr=l(qa,"DIV",{class:!0});var od=C(vr);u(yg.$$.fragment,od),aW=a(od),My=l(od,"P",{"data-svelte-h":!0}),f(My)!=="svelte-ne8wj0"&&(My.textContent=t5),sW=a(od),Ty=l(od,"P",{"data-svelte-h":!0}),f(Ty)!=="svelte-6ap5g8"&&(Ty.innerHTML=n5),iW=a(od),u(dl.$$.fragment,od),od.forEach(d),lW=a(qa),gt=l(qa,"DIV",{class:!0});var Qa=C(gt);u(Fg.$$.fragment,Qa),dW=a(Qa),yy=l(Qa,"P",{"data-svelte-h":!0}),f(yy)!=="svelte-xit1mq"&&(yy.textContent=r5),cW=a(Qa),Fy=l(Qa,"P",{"data-svelte-h":!0}),f(Fy)!=="svelte-1enn9p2"&&(Fy.innerHTML=a5),mW=a(Qa),vy=l(Qa,"UL",{"data-svelte-h":!0}),f(vy)!=="svelte-1uzizsl"&&(vy.innerHTML=s5),fW=a(Qa),u(cl.$$.fragment,Qa),Qa.forEach(d),qa.forEach(d),Gw=a(o),u(vg.$$.fragment,o),Ew=a(o),to=l(o,"DIV",{class:!0});var Ha=C(to);u(Cg.$$.fragment,Ha),gW=a(Ha),Cy=l(Ha,"P",{"data-svelte-h":!0}),f(Cy)!=="svelte-95ycjf"&&(Cy.innerHTML=i5),pW=a(Ha),wy=l(Ha,"P",{"data-svelte-h":!0}),f(wy)!=="svelte-1n2cphd"&&(wy.innerHTML=l5),uW=a(Ha),Cr=l(Ha,"DIV",{class:!0});var td=C(Cr);u(wg.$$.fragment,td),hW=a(td),jy=l(td,"P",{"data-svelte-h":!0}),f(jy)!=="svelte-15xznv"&&(jy.textContent=d5),bW=a(td),xy=l(td,"P",{"data-svelte-h":!0}),f(xy)!=="svelte-6ap5g8"&&(xy.innerHTML=c5),_W=a(td),u(ml.$$.fragment,td),td.forEach(d),MW=a(Ha),z=l(Ha,"DIV",{class:!0});var Qr=C(z);u(jg.$$.fragment,Qr),TW=a(Qr),$y=l(Qr,"P",{"data-svelte-h":!0}),f($y)!=="svelte-1r6a2oz"&&($y.textContent=m5),yW=a(Qr),ky=l(Qr,"P",{"data-svelte-h":!0}),f(ky)!=="svelte-1enn9p2"&&(ky.innerHTML=f5),FW=a(Qr),Zy=l(Qr,"UL",{"data-svelte-h":!0}),f(Zy)!=="svelte-1tundgo"&&(Zy.innerHTML=g5),vW=a(Qr),Ly=l(Qr,"P",{"data-svelte-h":!0}),f(Ly)!=="svelte-tebhz6"&&(Ly.innerHTML=p5),CW=a(Qr),u(fl.$$.fragment,Qr),Qr.forEach(d),Ha.forEach(d),Pw=a(o),u(xg.$$.fragment,o),Sw=a(o),no=l(o,"DIV",{class:!0});var Da=C(no);u($g.$$.fragment,Da),wW=a(Da),By=l(Da,"P",{"data-svelte-h":!0}),f(By)!=="svelte-1nyb1ac"&&(By.innerHTML=u5),jW=a(Da),Ay=l(Da,"P",{"data-svelte-h":!0}),f(Ay)!=="svelte-1n2cphd"&&(Ay.innerHTML=h5),xW=a(Da),wr=l(Da,"DIV",{class:!0});var nd=C(wr);u(kg.$$.fragment,nd),$W=a(nd),Ry=l(nd,"P",{"data-svelte-h":!0}),f(Ry)!=="svelte-zddwso"&&(Ry.textContent=b5),kW=a(nd),Wy=l(nd,"P",{"data-svelte-h":!0}),f(Wy)!=="svelte-6ap5g8"&&(Wy.innerHTML=_5),ZW=a(nd),u(gl.$$.fragment,nd),nd.forEach(d),LW=a(Da),O=l(Da,"DIV",{class:!0});var Hr=C(O);u(Zg.$$.fragment,Hr),BW=a(Hr),Jy=l(Hr,"P",{"data-svelte-h":!0}),f(Jy)!=="svelte-1wyb2t8"&&(Jy.textContent=M5),AW=a(Hr),Vy=l(Hr,"P",{"data-svelte-h":!0}),f(Vy)!=="svelte-1enn9p2"&&(Vy.innerHTML=T5),RW=a(Hr),Gy=l(Hr,"UL",{"data-svelte-h":!0}),f(Gy)!=="svelte-9m0tlk"&&(Gy.innerHTML=y5),WW=a(Hr),Ey=l(Hr,"P",{"data-svelte-h":!0}),f(Ey)!=="svelte-tebhz6"&&(Ey.innerHTML=F5),JW=a(Hr),u(pl.$$.fragment,Hr),Hr.forEach(d),Da.forEach(d),Uw=a(o),u(Lg.$$.fragment,o),Iw=a(o),ro=l(o,"DIV",{class:!0});var Ya=C(ro);u(Bg.$$.fragment,Ya),VW=a(Ya),Py=l(Ya,"P",{"data-svelte-h":!0}),f(Py)!=="svelte-7hwhxa"&&(Py.innerHTML=v5),GW=a(Ya),Sy=l(Ya,"P",{"data-svelte-h":!0}),f(Sy)!=="svelte-1n2cphd"&&(Sy.innerHTML=C5),EW=a(Ya),jr=l(Ya,"DIV",{class:!0});var rd=C(jr);u(Ag.$$.fragment,rd),PW=a(rd),Uy=l(rd,"P",{"data-svelte-h":!0}),f(Uy)!=="svelte-1xqzdzm"&&(Uy.textContent=w5),SW=a(rd),Iy=l(rd,"P",{"data-svelte-h":!0}),f(Iy)!=="svelte-6ap5g8"&&(Iy.innerHTML=j5),UW=a(rd),u(ul.$$.fragment,rd),rd.forEach(d),IW=a(Ya),K=l(Ya,"DIV",{class:!0});var Dr=C(K);u(Rg.$$.fragment,Dr),NW=a(Dr),Ny=l(Dr,"P",{"data-svelte-h":!0}),f(Ny)!=="svelte-1kp6e9m"&&(Ny.textContent=x5),XW=a(Dr),Xy=l(Dr,"P",{"data-svelte-h":!0}),f(Xy)!=="svelte-1enn9p2"&&(Xy.innerHTML=$5),qW=a(Dr),qy=l(Dr,"UL",{"data-svelte-h":!0}),f(qy)!=="svelte-4zad5m"&&(qy.innerHTML=k5),QW=a(Dr),Qy=l(Dr,"P",{"data-svelte-h":!0}),f(Qy)!=="svelte-tebhz6"&&(Qy.innerHTML=Z5),HW=a(Dr),u(hl.$$.fragment,Dr),Dr.forEach(d),Ya.forEach(d),Nw=a(o),u(Wg.$$.fragment,o),Xw=a(o),ao=l(o,"DIV",{class:!0});var za=C(ao);u(Jg.$$.fragment,za),DW=a(za),Hy=l(za,"P",{"data-svelte-h":!0}),f(Hy)!=="svelte-7hwhxa"&&(Hy.innerHTML=L5),YW=a(za),Dy=l(za,"P",{"data-svelte-h":!0}),f(Dy)!=="svelte-1n2cphd"&&(Dy.innerHTML=B5),zW=a(za),xr=l(za,"DIV",{class:!0});var ad=C(xr);u(Vg.$$.fragment,ad),OW=a(ad),Yy=l(ad,"P",{"data-svelte-h":!0}),f(Yy)!=="svelte-1xqzdzm"&&(Yy.textContent=A5),KW=a(ad),zy=l(ad,"P",{"data-svelte-h":!0}),f(zy)!=="svelte-6ap5g8"&&(zy.innerHTML=R5),e0=a(ad),u(bl.$$.fragment,ad),ad.forEach(d),o0=a(za),pt=l(za,"DIV",{class:!0});var Oa=C(pt);u(Gg.$$.fragment,Oa),t0=a(Oa),Oy=l(Oa,"P",{"data-svelte-h":!0}),f(Oy)!=="svelte-1kp6e9m"&&(Oy.textContent=W5),n0=a(Oa),Ky=l(Oa,"P",{"data-svelte-h":!0}),f(Ky)!=="svelte-1enn9p2"&&(Ky.innerHTML=J5),r0=a(Oa),eF=l(Oa,"UL",{"data-svelte-h":!0}),f(eF)!=="svelte-hu4lyh"&&(eF.innerHTML=V5),a0=a(Oa),u(_l.$$.fragment,Oa),Oa.forEach(d),za.forEach(d),qw=a(o),u(Eg.$$.fragment,o),Qw=a(o),so=l(o,"DIV",{class:!0});var Ka=C(so);u(Pg.$$.fragment,Ka),s0=a(Ka),oF=l(Ka,"P",{"data-svelte-h":!0}),f(oF)!=="svelte-1l5rdlt"&&(oF.innerHTML=G5),i0=a(Ka),tF=l(Ka,"P",{"data-svelte-h":!0}),f(tF)!=="svelte-1n2cphd"&&(tF.innerHTML=E5),l0=a(Ka),$r=l(Ka,"DIV",{class:!0});var sd=C($r);u(Sg.$$.fragment,sd),d0=a(sd),nF=l(sd,"P",{"data-svelte-h":!0}),f(nF)!=="svelte-1t88i0p"&&(nF.textContent=P5),c0=a(sd),rF=l(sd,"P",{"data-svelte-h":!0}),f(rF)!=="svelte-6ap5g8"&&(rF.innerHTML=S5),m0=a(sd),u(Ml.$$.fragment,sd),sd.forEach(d),f0=a(Ka),ee=l(Ka,"DIV",{class:!0});var Yr=C(ee);u(Ug.$$.fragment,Yr),g0=a(Yr),aF=l(Yr,"P",{"data-svelte-h":!0}),f(aF)!=="svelte-xtgygr"&&(aF.textContent=U5),p0=a(Yr),sF=l(Yr,"P",{"data-svelte-h":!0}),f(sF)!=="svelte-1enn9p2"&&(sF.innerHTML=I5),u0=a(Yr),iF=l(Yr,"UL",{"data-svelte-h":!0}),f(iF)!=="svelte-mza580"&&(iF.innerHTML=N5),h0=a(Yr),lF=l(Yr,"P",{"data-svelte-h":!0}),f(lF)!=="svelte-tebhz6"&&(lF.innerHTML=X5),b0=a(Yr),u(Tl.$$.fragment,Yr),Yr.forEach(d),Ka.forEach(d),Hw=a(o),u(Ig.$$.fragment,o),Dw=a(o),Ng=l(o,"P",{"data-svelte-h":!0}),f(Ng)!=="svelte-1tuoohd"&&(Ng.textContent=q5),Yw=a(o),u(Xg.$$.fragment,o),zw=a(o),io=l(o,"DIV",{class:!0});var es=C(io);u(qg.$$.fragment,es),_0=a(es),dF=l(es,"P",{"data-svelte-h":!0}),f(dF)!=="svelte-pvc5c4"&&(dF.innerHTML=Q5),M0=a(es),cF=l(es,"P",{"data-svelte-h":!0}),f(cF)!=="svelte-1n2cphd"&&(cF.innerHTML=H5),T0=a(es),kr=l(es,"DIV",{class:!0});var id=C(kr);u(Qg.$$.fragment,id),y0=a(id),mF=l(id,"P",{"data-svelte-h":!0}),f(mF)!=="svelte-1jtlz68"&&(mF.textContent=D5),F0=a(id),fF=l(id,"P",{"data-svelte-h":!0}),f(fF)!=="svelte-6ap5g8"&&(fF.innerHTML=Y5),v0=a(id),u(yl.$$.fragment,id),id.forEach(d),C0=a(es),oe=l(es,"DIV",{class:!0});var zr=C(oe);u(Hg.$$.fragment,zr),w0=a(zr),gF=l(zr,"P",{"data-svelte-h":!0}),f(gF)!=="svelte-mj39cs"&&(gF.textContent=z5),j0=a(zr),pF=l(zr,"P",{"data-svelte-h":!0}),f(pF)!=="svelte-1enn9p2"&&(pF.innerHTML=O5),x0=a(zr),uF=l(zr,"UL",{"data-svelte-h":!0}),f(uF)!=="svelte-do3xwe"&&(uF.innerHTML=K5),$0=a(zr),hF=l(zr,"P",{"data-svelte-h":!0}),f(hF)!=="svelte-tebhz6"&&(hF.innerHTML=eN),k0=a(zr),u(Fl.$$.fragment,zr),zr.forEach(d),es.forEach(d),Ow=a(o),u(Dg.$$.fragment,o),Kw=a(o),lo=l(o,"DIV",{class:!0});var os=C(lo);u(Yg.$$.fragment,os),Z0=a(os),bF=l(os,"P",{"data-svelte-h":!0}),f(bF)!=="svelte-pvc5c4"&&(bF.innerHTML=oN),L0=a(os),_F=l(os,"P",{"data-svelte-h":!0}),f(_F)!=="svelte-1n2cphd"&&(_F.innerHTML=tN),B0=a(os),Zr=l(os,"DIV",{class:!0});var ld=C(Zr);u(zg.$$.fragment,ld),A0=a(ld),MF=l(ld,"P",{"data-svelte-h":!0}),f(MF)!=="svelte-1jtlz68"&&(MF.textContent=nN),R0=a(ld),TF=l(ld,"P",{"data-svelte-h":!0}),f(TF)!=="svelte-6ap5g8"&&(TF.innerHTML=rN),W0=a(ld),u(vl.$$.fragment,ld),ld.forEach(d),J0=a(os),ut=l(os,"DIV",{class:!0});var ts=C(ut);u(Og.$$.fragment,ts),V0=a(ts),yF=l(ts,"P",{"data-svelte-h":!0}),f(yF)!=="svelte-mj39cs"&&(yF.textContent=aN),G0=a(ts),FF=l(ts,"P",{"data-svelte-h":!0}),f(FF)!=="svelte-1enn9p2"&&(FF.innerHTML=sN),E0=a(ts),vF=l(ts,"UL",{"data-svelte-h":!0}),f(vF)!=="svelte-17vf7rp"&&(vF.innerHTML=iN),P0=a(ts),u(Cl.$$.fragment,ts),ts.forEach(d),os.forEach(d),ej=a(o),u(Kg.$$.fragment,o),oj=a(o),co=l(o,"DIV",{class:!0});var ns=C(co);u(ep.$$.fragment,ns),S0=a(ns),CF=l(ns,"P",{"data-svelte-h":!0}),f(CF)!=="svelte-8kp6on"&&(CF.innerHTML=lN),U0=a(ns),wF=l(ns,"P",{"data-svelte-h":!0}),f(wF)!=="svelte-1n2cphd"&&(wF.innerHTML=dN),I0=a(ns),Lr=l(ns,"DIV",{class:!0});var dd=C(Lr);u(op.$$.fragment,dd),N0=a(dd),jF=l(dd,"P",{"data-svelte-h":!0}),f(jF)!=="svelte-qoovm7"&&(jF.textContent=cN),X0=a(dd),xF=l(dd,"P",{"data-svelte-h":!0}),f(xF)!=="svelte-6ap5g8"&&(xF.innerHTML=mN),q0=a(dd),u(wl.$$.fragment,dd),dd.forEach(d),Q0=a(ns),te=l(ns,"DIV",{class:!0});var Or=C(te);u(tp.$$.fragment,Or),H0=a(Or),$F=l(Or,"P",{"data-svelte-h":!0}),f($F)!=="svelte-1ldaoop"&&($F.textContent=fN),D0=a(Or),kF=l(Or,"P",{"data-svelte-h":!0}),f(kF)!=="svelte-1enn9p2"&&(kF.innerHTML=gN),Y0=a(Or),ZF=l(Or,"UL",{"data-svelte-h":!0}),f(ZF)!=="svelte-1jbcnel"&&(ZF.innerHTML=pN),z0=a(Or),LF=l(Or,"P",{"data-svelte-h":!0}),f(LF)!=="svelte-tebhz6"&&(LF.innerHTML=uN),O0=a(Or),u(jl.$$.fragment,Or),Or.forEach(d),ns.forEach(d),tj=a(o),u(np.$$.fragment,o),nj=a(o),mo=l(o,"DIV",{class:!0});var rs=C(mo);u(rp.$$.fragment,rs),K0=a(rs),BF=l(rs,"P",{"data-svelte-h":!0}),f(BF)!=="svelte-1porvao"&&(BF.innerHTML=hN),eJ=a(rs),AF=l(rs,"P",{"data-svelte-h":!0}),f(AF)!=="svelte-1n2cphd"&&(AF.innerHTML=bN),oJ=a(rs),Br=l(rs,"DIV",{class:!0});var cd=C(Br);u(ap.$$.fragment,cd),tJ=a(cd),RF=l(cd,"P",{"data-svelte-h":!0}),f(RF)!=="svelte-ovue0k"&&(RF.textContent=_N),nJ=a(cd),WF=l(cd,"P",{"data-svelte-h":!0}),f(WF)!=="svelte-6ap5g8"&&(WF.innerHTML=MN),rJ=a(cd),u(xl.$$.fragment,cd),cd.forEach(d),aJ=a(rs),ne=l(rs,"DIV",{class:!0});var Kr=C(ne);u(sp.$$.fragment,Kr),sJ=a(Kr),JF=l(Kr,"P",{"data-svelte-h":!0}),f(JF)!=="svelte-1jrxhfe"&&(JF.textContent=TN),iJ=a(Kr),VF=l(Kr,"P",{"data-svelte-h":!0}),f(VF)!=="svelte-1enn9p2"&&(VF.innerHTML=yN),lJ=a(Kr),GF=l(Kr,"UL",{"data-svelte-h":!0}),f(GF)!=="svelte-95o8xr"&&(GF.innerHTML=FN),dJ=a(Kr),EF=l(Kr,"P",{"data-svelte-h":!0}),f(EF)!=="svelte-tebhz6"&&(EF.innerHTML=vN),cJ=a(Kr),u($l.$$.fragment,Kr),Kr.forEach(d),rs.forEach(d),rj=a(o),u(ip.$$.fragment,o),aj=a(o),fo=l(o,"DIV",{class:!0});var as=C(fo);u(lp.$$.fragment,as),mJ=a(as),PF=l(as,"P",{"data-svelte-h":!0}),f(PF)!=="svelte-1gn9ts8"&&(PF.innerHTML=CN),fJ=a(as),SF=l(as,"P",{"data-svelte-h":!0}),f(SF)!=="svelte-1n2cphd"&&(SF.innerHTML=wN),gJ=a(as),Ar=l(as,"DIV",{class:!0});var md=C(Ar);u(dp.$$.fragment,md),pJ=a(md),UF=l(md,"P",{"data-svelte-h":!0}),f(UF)!=="svelte-1aufdt0"&&(UF.textContent=jN),uJ=a(md),IF=l(md,"P",{"data-svelte-h":!0}),f(IF)!=="svelte-6ap5g8"&&(IF.innerHTML=xN),hJ=a(md),u(kl.$$.fragment,md),md.forEach(d),bJ=a(as),re=l(as,"DIV",{class:!0});var ea=C(re);u(cp.$$.fragment,ea),_J=a(ea),NF=l(ea,"P",{"data-svelte-h":!0}),f(NF)!=="svelte-73ck9g"&&(NF.textContent=$N),MJ=a(ea),XF=l(ea,"P",{"data-svelte-h":!0}),f(XF)!=="svelte-1enn9p2"&&(XF.innerHTML=kN),TJ=a(ea),qF=l(ea,"UL",{"data-svelte-h":!0}),f(qF)!=="svelte-mgm2zu"&&(qF.innerHTML=ZN),yJ=a(ea),QF=l(ea,"P",{"data-svelte-h":!0}),f(QF)!=="svelte-tebhz6"&&(QF.innerHTML=LN),FJ=a(ea),u(Zl.$$.fragment,ea),ea.forEach(d),as.forEach(d),sj=a(o),u(mp.$$.fragment,o),ij=a(o),go=l(o,"DIV",{class:!0});var ss=C(go);u(fp.$$.fragment,ss),vJ=a(ss),HF=l(ss,"P",{"data-svelte-h":!0}),f(HF)!=="svelte-1gn9ts8"&&(HF.innerHTML=BN),CJ=a(ss),DF=l(ss,"P",{"data-svelte-h":!0}),f(DF)!=="svelte-1n2cphd"&&(DF.innerHTML=AN),wJ=a(ss),Rr=l(ss,"DIV",{class:!0});var fd=C(Rr);u(gp.$$.fragment,fd),jJ=a(fd),YF=l(fd,"P",{"data-svelte-h":!0}),f(YF)!=="svelte-1aufdt0"&&(YF.textContent=RN),xJ=a(fd),zF=l(fd,"P",{"data-svelte-h":!0}),f(zF)!=="svelte-6ap5g8"&&(zF.innerHTML=WN),$J=a(fd),u(Ll.$$.fragment,fd),fd.forEach(d),kJ=a(ss),ht=l(ss,"DIV",{class:!0});var is=C(ht);u(pp.$$.fragment,is),ZJ=a(is),OF=l(is,"P",{"data-svelte-h":!0}),f(OF)!=="svelte-73ck9g"&&(OF.textContent=JN),LJ=a(is),KF=l(is,"P",{"data-svelte-h":!0}),f(KF)!=="svelte-1enn9p2"&&(KF.innerHTML=VN),BJ=a(is),ev=l(is,"UL",{"data-svelte-h":!0}),f(ev)!=="svelte-1xbpdmk"&&(ev.innerHTML=GN),AJ=a(is),u(Bl.$$.fragment,is),is.forEach(d),ss.forEach(d),lj=a(o),u(up.$$.fragment,o),dj=a(o),po=l(o,"DIV",{class:!0});var ls=C(po);u(hp.$$.fragment,ls),RJ=a(ls),ov=l(ls,"P",{"data-svelte-h":!0}),f(ov)!=="svelte-1gn9ts8"&&(ov.innerHTML=EN),WJ=a(ls),tv=l(ls,"P",{"data-svelte-h":!0}),f(tv)!=="svelte-1n2cphd"&&(tv.innerHTML=PN),JJ=a(ls),Wr=l(ls,"DIV",{class:!0});var gd=C(Wr);u(bp.$$.fragment,gd),VJ=a(gd),nv=l(gd,"P",{"data-svelte-h":!0}),f(nv)!=="svelte-1aufdt0"&&(nv.textContent=SN),GJ=a(gd),rv=l(gd,"P",{"data-svelte-h":!0}),f(rv)!=="svelte-6ap5g8"&&(rv.innerHTML=UN),EJ=a(gd),u(Al.$$.fragment,gd),gd.forEach(d),PJ=a(ls),bt=l(ls,"DIV",{class:!0});var ds=C(bt);u(_p.$$.fragment,ds),SJ=a(ds),av=l(ds,"P",{"data-svelte-h":!0}),f(av)!=="svelte-73ck9g"&&(av.textContent=IN),UJ=a(ds),sv=l(ds,"P",{"data-svelte-h":!0}),f(sv)!=="svelte-1enn9p2"&&(sv.innerHTML=NN),IJ=a(ds),iv=l(ds,"UL",{"data-svelte-h":!0}),f(iv)!=="svelte-rcb4sm"&&(iv.innerHTML=XN),NJ=a(ds),u(Rl.$$.fragment,ds),ds.forEach(d),ls.forEach(d),cj=a(o),u(Mp.$$.fragment,o),mj=a(o),uo=l(o,"DIV",{class:!0});var cs=C(uo);u(Tp.$$.fragment,cs),XJ=a(cs),lv=l(cs,"P",{"data-svelte-h":!0}),f(lv)!=="svelte-tluy8s"&&(lv.innerHTML=qN),qJ=a(cs),dv=l(cs,"P",{"data-svelte-h":!0}),f(dv)!=="svelte-1n2cphd"&&(dv.innerHTML=QN),QJ=a(cs),Jr=l(cs,"DIV",{class:!0});var pd=C(Jr);u(yp.$$.fragment,pd),HJ=a(pd),cv=l(pd,"P",{"data-svelte-h":!0}),f(cv)!=="svelte-1wxna3s"&&(cv.textContent=HN),DJ=a(pd),mv=l(pd,"P",{"data-svelte-h":!0}),f(mv)!=="svelte-6ap5g8"&&(mv.innerHTML=DN),YJ=a(pd),u(Wl.$$.fragment,pd),pd.forEach(d),zJ=a(cs),ae=l(cs,"DIV",{class:!0});var oa=C(ae);u(Fp.$$.fragment,oa),OJ=a(oa),fv=l(oa,"P",{"data-svelte-h":!0}),f(fv)!=="svelte-1n6mjno"&&(fv.textContent=YN),KJ=a(oa),gv=l(oa,"P",{"data-svelte-h":!0}),f(gv)!=="svelte-1enn9p2"&&(gv.innerHTML=zN),eV=a(oa),pv=l(oa,"UL",{"data-svelte-h":!0}),f(pv)!=="svelte-1gynvt9"&&(pv.innerHTML=ON),oV=a(oa),uv=l(oa,"P",{"data-svelte-h":!0}),f(uv)!=="svelte-tebhz6"&&(uv.innerHTML=KN),tV=a(oa),u(Jl.$$.fragment,oa),oa.forEach(d),cs.forEach(d),fj=a(o),u(vp.$$.fragment,o),gj=a(o),Cp=l(o,"DIV",{class:!0});var lX=C(Cp);u(wp.$$.fragment,lX),lX.forEach(d),pj=a(o),u(jp.$$.fragment,o),uj=a(o),xp=l(o,"DIV",{class:!0});var dX=C(xp);u($p.$$.fragment,dX),dX.forEach(d),hj=a(o),u(kp.$$.fragment,o),bj=a(o),Zp=l(o,"P",{"data-svelte-h":!0}),f(Zp)!=="svelte-fculmf"&&(Zp.textContent=e4),_j=a(o),u(Lp.$$.fragment,o),Mj=a(o),ho=l(o,"DIV",{class:!0});var ms=C(ho);u(Bp.$$.fragment,ms),nV=a(ms),hv=l(ms,"P",{"data-svelte-h":!0}),f(hv)!=="svelte-qqrhso"&&(hv.innerHTML=o4),rV=a(ms),bv=l(ms,"P",{"data-svelte-h":!0}),f(bv)!=="svelte-1n2cphd"&&(bv.innerHTML=t4),aV=a(ms),Vr=l(ms,"DIV",{class:!0});var ud=C(Vr);u(Ap.$$.fragment,ud),sV=a(ud),_v=l(ud,"P",{"data-svelte-h":!0}),f(_v)!=="svelte-syrugc"&&(_v.textContent=n4),iV=a(ud),Mv=l(ud,"P",{"data-svelte-h":!0}),f(Mv)!=="svelte-6ap5g8"&&(Mv.innerHTML=r4),lV=a(ud),u(Vl.$$.fragment,ud),ud.forEach(d),dV=a(ms),se=l(ms,"DIV",{class:!0});var ta=C(se);u(Rp.$$.fragment,ta),cV=a(ta),Tv=l(ta,"P",{"data-svelte-h":!0}),f(Tv)!=="svelte-1ontymo"&&(Tv.textContent=a4),mV=a(ta),yv=l(ta,"P",{"data-svelte-h":!0}),f(yv)!=="svelte-1enn9p2"&&(yv.innerHTML=s4),fV=a(ta),Fv=l(ta,"UL",{"data-svelte-h":!0}),f(Fv)!=="svelte-d66073"&&(Fv.innerHTML=i4),gV=a(ta),vv=l(ta,"P",{"data-svelte-h":!0}),f(vv)!=="svelte-tebhz6"&&(vv.innerHTML=l4),pV=a(ta),u(Gl.$$.fragment,ta),ta.forEach(d),ms.forEach(d),Tj=a(o),u(Wp.$$.fragment,o),yj=a(o),bo=l(o,"DIV",{class:!0});var fs=C(bo);u(Jp.$$.fragment,fs),uV=a(fs),Cv=l(fs,"P",{"data-svelte-h":!0}),f(Cv)!=="svelte-qqrhso"&&(Cv.innerHTML=d4),hV=a(fs),wv=l(fs,"P",{"data-svelte-h":!0}),f(wv)!=="svelte-1n2cphd"&&(wv.innerHTML=c4),bV=a(fs),Gr=l(fs,"DIV",{class:!0});var hd=C(Gr);u(Vp.$$.fragment,hd),_V=a(hd),jv=l(hd,"P",{"data-svelte-h":!0}),f(jv)!=="svelte-syrugc"&&(jv.textContent=m4),MV=a(hd),xv=l(hd,"P",{"data-svelte-h":!0}),f(xv)!=="svelte-6ap5g8"&&(xv.innerHTML=f4),TV=a(hd),u(El.$$.fragment,hd),hd.forEach(d),yV=a(fs),_t=l(fs,"DIV",{class:!0});var gs=C(_t);u(Gp.$$.fragment,gs),FV=a(gs),$v=l(gs,"P",{"data-svelte-h":!0}),f($v)!=="svelte-1ontymo"&&($v.textContent=g4),vV=a(gs),kv=l(gs,"P",{"data-svelte-h":!0}),f(kv)!=="svelte-1enn9p2"&&(kv.innerHTML=p4),CV=a(gs),Zv=l(gs,"UL",{"data-svelte-h":!0}),f(Zv)!=="svelte-ieksgl"&&(Zv.innerHTML=u4),wV=a(gs),u(Pl.$$.fragment,gs),gs.forEach(d),fs.forEach(d),Fj=a(o),u(Ep.$$.fragment,o),vj=a(o),_o=l(o,"DIV",{class:!0});var ps=C(_o);u(Pp.$$.fragment,ps),jV=a(ps),Lv=l(ps,"P",{"data-svelte-h":!0}),f(Lv)!=="svelte-uog3vd"&&(Lv.innerHTML=h4),xV=a(ps),Bv=l(ps,"P",{"data-svelte-h":!0}),f(Bv)!=="svelte-1n2cphd"&&(Bv.innerHTML=b4),$V=a(ps),Er=l(ps,"DIV",{class:!0});var bd=C(Er);u(Sp.$$.fragment,bd),kV=a(bd),Av=l(bd,"P",{"data-svelte-h":!0}),f(Av)!=="svelte-axumax"&&(Av.textContent=_4),ZV=a(bd),Rv=l(bd,"P",{"data-svelte-h":!0}),f(Rv)!=="svelte-6ap5g8"&&(Rv.innerHTML=M4),LV=a(bd),u(Sl.$$.fragment,bd),bd.forEach(d),BV=a(ps),ie=l(ps,"DIV",{class:!0});var na=C(ie);u(Up.$$.fragment,na),AV=a(na),Wv=l(na,"P",{"data-svelte-h":!0}),f(Wv)!=="svelte-dbgbst"&&(Wv.textContent=T4),RV=a(na),Jv=l(na,"P",{"data-svelte-h":!0}),f(Jv)!=="svelte-1enn9p2"&&(Jv.innerHTML=y4),WV=a(na),Vv=l(na,"UL",{"data-svelte-h":!0}),f(Vv)!=="svelte-1bmtkwo"&&(Vv.innerHTML=F4),JV=a(na),Gv=l(na,"P",{"data-svelte-h":!0}),f(Gv)!=="svelte-tebhz6"&&(Gv.innerHTML=v4),VV=a(na),u(Ul.$$.fragment,na),na.forEach(d),ps.forEach(d),Cj=a(o),u(Ip.$$.fragment,o),wj=a(o),Mo=l(o,"DIV",{class:!0});var us=C(Mo);u(Np.$$.fragment,us),GV=a(us),Ev=l(us,"P",{"data-svelte-h":!0}),f(Ev)!=="svelte-uog3vd"&&(Ev.innerHTML=C4),EV=a(us),Pv=l(us,"P",{"data-svelte-h":!0}),f(Pv)!=="svelte-1n2cphd"&&(Pv.innerHTML=w4),PV=a(us),Pr=l(us,"DIV",{class:!0});var _d=C(Pr);u(Xp.$$.fragment,_d),SV=a(_d),Sv=l(_d,"P",{"data-svelte-h":!0}),f(Sv)!=="svelte-axumax"&&(Sv.textContent=j4),UV=a(_d),Uv=l(_d,"P",{"data-svelte-h":!0}),f(Uv)!=="svelte-6ap5g8"&&(Uv.innerHTML=x4),IV=a(_d),u(Il.$$.fragment,_d),_d.forEach(d),NV=a(us),Mt=l(us,"DIV",{class:!0});var hs=C(Mt);u(qp.$$.fragment,hs),XV=a(hs),Iv=l(hs,"P",{"data-svelte-h":!0}),f(Iv)!=="svelte-dbgbst"&&(Iv.textContent=$4),qV=a(hs),Nv=l(hs,"P",{"data-svelte-h":!0}),f(Nv)!=="svelte-1enn9p2"&&(Nv.innerHTML=k4),QV=a(hs),Xv=l(hs,"UL",{"data-svelte-h":!0}),f(Xv)!=="svelte-mbd7pr"&&(Xv.innerHTML=Z4),HV=a(hs),u(Nl.$$.fragment,hs),hs.forEach(d),us.forEach(d),jj=a(o),u(Qp.$$.fragment,o),xj=a(o),To=l(o,"DIV",{class:!0});var bs=C(To);u(Hp.$$.fragment,bs),DV=a(bs),qv=l(bs,"P",{"data-svelte-h":!0}),f(qv)!=="svelte-7w2dbw"&&(qv.innerHTML=L4),YV=a(bs),Qv=l(bs,"P",{"data-svelte-h":!0}),f(Qv)!=="svelte-1n2cphd"&&(Qv.innerHTML=B4),zV=a(bs),Sr=l(bs,"DIV",{class:!0});var Md=C(Sr);u(Dp.$$.fragment,Md),OV=a(Md),Hv=l(Md,"P",{"data-svelte-h":!0}),f(Hv)!=="svelte-1nom7tk"&&(Hv.textContent=A4),KV=a(Md),Dv=l(Md,"P",{"data-svelte-h":!0}),f(Dv)!=="svelte-6ap5g8"&&(Dv.innerHTML=R4),eG=a(Md),u(Xl.$$.fragment,Md),Md.forEach(d),oG=a(bs),le=l(bs,"DIV",{class:!0});var ra=C(le);u(Yp.$$.fragment,ra),tG=a(ra),Yv=l(ra,"P",{"data-svelte-h":!0}),f(Yv)!=="svelte-7pdp5q"&&(Yv.textContent=W4),nG=a(ra),zv=l(ra,"P",{"data-svelte-h":!0}),f(zv)!=="svelte-1enn9p2"&&(zv.innerHTML=J4),rG=a(ra),Ov=l(ra,"UL",{"data-svelte-h":!0}),f(Ov)!=="svelte-1fbwj54"&&(Ov.innerHTML=V4),aG=a(ra),Kv=l(ra,"P",{"data-svelte-h":!0}),f(Kv)!=="svelte-tebhz6"&&(Kv.innerHTML=G4),sG=a(ra),u(ql.$$.fragment,ra),ra.forEach(d),bs.forEach(d),$j=a(o),u(zp.$$.fragment,o),kj=a(o),yo=l(o,"DIV",{class:!0});var _s=C(yo);u(Op.$$.fragment,_s),iG=a(_s),eC=l(_s,"P",{"data-svelte-h":!0}),f(eC)!=="svelte-cw6e2x"&&(eC.innerHTML=E4),lG=a(_s),oC=l(_s,"P",{"data-svelte-h":!0}),f(oC)!=="svelte-1n2cphd"&&(oC.innerHTML=P4),dG=a(_s),Ur=l(_s,"DIV",{class:!0});var Td=C(Ur);u(Kp.$$.fragment,Td),cG=a(Td),tC=l(Td,"P",{"data-svelte-h":!0}),f(tC)!=="svelte-10jw89"&&(tC.textContent=S4),mG=a(Td),nC=l(Td,"P",{"data-svelte-h":!0}),f(nC)!=="svelte-6ap5g8"&&(nC.innerHTML=U4),fG=a(Td),u(Ql.$$.fragment,Td),Td.forEach(d),gG=a(_s),de=l(_s,"DIV",{class:!0});var aa=C(de);u(eu.$$.fragment,aa),pG=a(aa),rC=l(aa,"P",{"data-svelte-h":!0}),f(rC)!=="svelte-1mjzf2l"&&(rC.textContent=I4),uG=a(aa),aC=l(aa,"P",{"data-svelte-h":!0}),f(aC)!=="svelte-1enn9p2"&&(aC.innerHTML=N4),hG=a(aa),sC=l(aa,"UL",{"data-svelte-h":!0}),f(sC)!=="svelte-1bm4l9n"&&(sC.innerHTML=X4),bG=a(aa),iC=l(aa,"P",{"data-svelte-h":!0}),f(iC)!=="svelte-tebhz6"&&(iC.innerHTML=q4),_G=a(aa),u(Hl.$$.fragment,aa),aa.forEach(d),_s.forEach(d),Zj=a(o),u(ou.$$.fragment,o),Lj=a(o),Fo=l(o,"DIV",{class:!0});var Ms=C(Fo);u(tu.$$.fragment,Ms),MG=a(Ms),lC=l(Ms,"P",{"data-svelte-h":!0}),f(lC)!=="svelte-cw6e2x"&&(lC.innerHTML=Q4),TG=a(Ms),dC=l(Ms,"P",{"data-svelte-h":!0}),f(dC)!=="svelte-1n2cphd"&&(dC.innerHTML=H4),yG=a(Ms),Ir=l(Ms,"DIV",{class:!0});var yd=C(Ir);u(nu.$$.fragment,yd),FG=a(yd),cC=l(yd,"P",{"data-svelte-h":!0}),f(cC)!=="svelte-10jw89"&&(cC.textContent=D4),vG=a(yd),mC=l(yd,"P",{"data-svelte-h":!0}),f(mC)!=="svelte-6ap5g8"&&(mC.innerHTML=Y4),CG=a(yd),u(Dl.$$.fragment,yd),yd.forEach(d),wG=a(Ms),Tt=l(Ms,"DIV",{class:!0});var Ts=C(Tt);u(ru.$$.fragment,Ts),jG=a(Ts),fC=l(Ts,"P",{"data-svelte-h":!0}),f(fC)!=="svelte-1mjzf2l"&&(fC.textContent=z4),xG=a(Ts),gC=l(Ts,"P",{"data-svelte-h":!0}),f(gC)!=="svelte-1enn9p2"&&(gC.innerHTML=O4),$G=a(Ts),pC=l(Ts,"UL",{"data-svelte-h":!0}),f(pC)!=="svelte-sghd02"&&(pC.innerHTML=K4),kG=a(Ts),u(Yl.$$.fragment,Ts),Ts.forEach(d),Ms.forEach(d),Bj=a(o),u(au.$$.fragment,o),Aj=a(o),vo=l(o,"DIV",{class:!0});var ys=C(vo);u(su.$$.fragment,ys),ZG=a(ys),uC=l(ys,"P",{"data-svelte-h":!0}),f(uC)!=="svelte-cw6e2x"&&(uC.innerHTML=eX),LG=a(ys),hC=l(ys,"P",{"data-svelte-h":!0}),f(hC)!=="svelte-1n2cphd"&&(hC.innerHTML=oX),BG=a(ys),Nr=l(ys,"DIV",{class:!0});var Fd=C(Nr);u(iu.$$.fragment,Fd),AG=a(Fd),bC=l(Fd,"P",{"data-svelte-h":!0}),f(bC)!=="svelte-10jw89"&&(bC.textContent=tX),RG=a(Fd),_C=l(Fd,"P",{"data-svelte-h":!0}),f(_C)!=="svelte-6ap5g8"&&(_C.innerHTML=nX),WG=a(Fd),u(zl.$$.fragment,Fd),Fd.forEach(d),JG=a(ys),yt=l(ys,"DIV",{class:!0});var Fs=C(yt);u(lu.$$.fragment,Fs),VG=a(Fs),MC=l(Fs,"P",{"data-svelte-h":!0}),f(MC)!=="svelte-1mjzf2l"&&(MC.textContent=rX),GG=a(Fs),TC=l(Fs,"P",{"data-svelte-h":!0}),f(TC)!=="svelte-1enn9p2"&&(TC.innerHTML=aX),EG=a(Fs),yC=l(Fs,"UL",{"data-svelte-h":!0}),f(yC)!=="svelte-1ywmboh"&&(yC.innerHTML=sX),PG=a(Fs),u(Ol.$$.fragment,Fs),Fs.forEach(d),ys.forEach(d),Rj=a(o),FC=l(o,"P",{}),C(FC).forEach(d),this.h()},h(){w(n,"name","hf:doc:metadata"),w(n,"content",Sq),w(Uo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ws,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Io,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(xs,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Zs,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(As,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Js,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Vn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Gn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(No,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(En,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Xo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Pn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Sn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(qo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Me,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Un,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Qo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(In,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Nn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ho,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Fe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Xn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Do,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(qn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Qn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Yo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(we,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Hn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(zo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Oc,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(om,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Dn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Yn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Oo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w($e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(zn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ko,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(On,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Kn,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(et,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(er,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ot,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Be,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(or,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(tr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(nr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(We,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(rr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Je,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ar,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(rt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ve,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(sr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(at,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ir,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(lr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(st,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Pe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(dr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(it,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(cr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(mr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(lt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(fr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(dt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(vf,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(jf,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(gr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(pr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ur,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Qe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(hr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(He,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(br,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(De,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(_r,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ye,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Mr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ze,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Tr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(yr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ke,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(gg,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Fr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(eo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(vr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(gt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(oo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Cr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(to,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(wr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(no,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(jr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ro,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(xr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ao,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w($r,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(so,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(kr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(io,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Zr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ut,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(lo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Lr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(co,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Br,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(mo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ar,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(fo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Rr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ht,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(go,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Wr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(po,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Jr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(uo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Cp,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(xp,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Vr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ho,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Gr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(_t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(bo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Er,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(_o,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Pr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Mo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Sr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(le,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(To,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ur,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(de,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(yo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Ir,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Fo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(Nr,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),w(vo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(o,g){t(document.head,n),y(o,F,g),y(o,c,g),y(o,s,g),h(m,o,g),y(o,e,g),y(o,T,g),y(o,xC,g),y(o,vd,g),y(o,$C,g),h(Cd,o,g),y(o,kC,g),y(o,wd,g),y(o,ZC,g),y(o,jd,g),y(o,LC,g),h(xd,o,g),y(o,BC,g),y(o,$d,g),y(o,AC,g),h(kd,o,g),y(o,RC,g),y(o,Zd,g),y(o,WC,g),h(vs,o,g),y(o,JC,g),h(Ld,o,g),y(o,VC,g),y(o,ce,g),h(Bd,ce,null),t(ce,Vj),t(ce,pu),t(ce,Gj),t(ce,uu),t(ce,Ej),t(ce,Uo),h(Ad,Uo,null),t(Uo,Pj),t(Uo,hu),t(Uo,Sj),t(Uo,bu),t(Uo,Uj),t(Uo,_u),t(Uo,Ij),h(Cs,Uo,null),t(ce,Nj),t(ce,ws),h(Rd,ws,null),t(ws,Xj),t(ws,Mu),y(o,GC,g),h(Wd,o,g),y(o,EC,g),y(o,me,g),h(Jd,me,null),t(me,qj),t(me,Tu),t(me,Qj),t(me,yu),t(me,Hj),t(me,Io),h(Vd,Io,null),t(Io,Dj),t(Io,Fu),t(Io,Yj),t(Io,vu),t(Io,zj),t(Io,Cu),t(Io,Oj),h(js,Io,null),t(me,Kj),t(me,xs),h(Gd,xs,null),t(xs,ex),t(xs,wu),y(o,PC,g),h(Ed,o,g),y(o,SC,g),y(o,fe,g),h(Pd,fe,null),t(fe,ox),t(fe,ju),t(fe,tx),t(fe,xu),t(fe,nx),t(fe,L),h(Sd,L,null),t(L,rx),t(L,$u),t(L,ax),t(L,ku),t(L,sx),t(L,Zu),t(L,ix),h($s,L,null),t(L,lx),h(ks,L,null),t(fe,dx),t(fe,Zs),h(Ud,Zs,null),t(Zs,cx),t(Zs,Lu),y(o,UC,g),h(Id,o,g),y(o,IC,g),y(o,ge,g),h(Nd,ge,null),t(ge,mx),t(ge,Bu),t(ge,fx),t(ge,Au),t(ge,gx),t(ge,B),h(Xd,B,null),t(B,px),t(B,Ru),t(B,ux),t(B,Wu),t(B,hx),t(B,Ju),t(B,bx),h(Ls,B,null),t(B,_x),h(Bs,B,null),t(ge,Mx),t(ge,As),h(qd,As,null),t(As,Tx),t(As,Vu),y(o,NC,g),h(Qd,o,g),y(o,XC,g),y(o,pe,g),h(Hd,pe,null),t(pe,yx),t(pe,Gu),t(pe,Fx),t(pe,Eu),t(pe,vx),t(pe,A),h(Dd,A,null),t(A,Cx),t(A,Pu),t(A,wx),t(A,Su),t(A,jx),t(A,Uu),t(A,xx),h(Rs,A,null),t(A,$x),h(Ws,A,null),t(pe,kx),t(pe,Js),h(Yd,Js,null),t(Js,Zx),t(Js,Iu),y(o,qC,g),h(zd,o,g),y(o,QC,g),y(o,Od,g),y(o,HC,g),h(Kd,o,g),y(o,DC,g),y(o,ue,g),h(ec,ue,null),t(ue,Lx),t(ue,Nu),t(ue,Bx),t(ue,Xu),t(ue,Ax),t(ue,Vn),h(oc,Vn,null),t(Vn,Rx),t(Vn,qu),t(Vn,Wx),t(Vn,Qu),t(Vn,Jx),h(Vs,Vn,null),t(ue,Vx),t(ue,R),h(tc,R,null),t(R,Gx),t(R,Hu),t(R,Ex),t(R,Du),t(R,Px),t(R,Yu),t(R,Sx),t(R,zu),t(R,Ux),h(Gs,R,null),y(o,YC,g),h(nc,o,g),y(o,zC,g),y(o,he,g),h(rc,he,null),t(he,Ix),t(he,Ou),t(he,Nx),t(he,Ku),t(he,Xx),t(he,Gn),h(ac,Gn,null),t(Gn,qx),t(Gn,eh),t(Gn,Qx),t(Gn,oh),t(Gn,Hx),h(Es,Gn,null),t(he,Dx),t(he,No),h(sc,No,null),t(No,Yx),t(No,th),t(No,zx),t(No,nh),t(No,Ox),t(No,rh),t(No,Kx),h(Ps,No,null),y(o,OC,g),h(ic,o,g),y(o,KC,g),y(o,be,g),h(lc,be,null),t(be,e1),t(be,ah),t(be,o1),t(be,sh),t(be,t1),t(be,En),h(dc,En,null),t(En,n1),t(En,ih),t(En,r1),t(En,lh),t(En,a1),h(Ss,En,null),t(be,s1),t(be,Xo),h(cc,Xo,null),t(Xo,i1),t(Xo,dh),t(Xo,l1),t(Xo,ch),t(Xo,d1),t(Xo,mh),t(Xo,c1),h(Us,Xo,null),y(o,e2,g),h(mc,o,g),y(o,o2,g),y(o,fc,g),y(o,t2,g),h(gc,o,g),y(o,n2,g),y(o,_e,g),h(pc,_e,null),t(_e,m1),t(_e,fh),t(_e,f1),t(_e,gh),t(_e,g1),t(_e,Pn),h(uc,Pn,null),t(Pn,p1),t(Pn,ph),t(Pn,u1),t(Pn,uh),t(Pn,h1),h(Is,Pn,null),t(_e,b1),t(_e,W),h(hc,W,null),t(W,_1),t(W,hh),t(W,M1),t(W,bh),t(W,T1),t(W,_h),t(W,y1),t(W,Mh),t(W,F1),h(Ns,W,null),y(o,r2,g),h(bc,o,g),y(o,a2,g),y(o,Me,g),h(_c,Me,null),t(Me,v1),t(Me,Th),t(Me,C1),t(Me,yh),t(Me,w1),t(Me,Sn),h(Mc,Sn,null),t(Sn,j1),t(Sn,Fh),t(Sn,x1),t(Sn,vh),t(Sn,$1),h(Xs,Sn,null),t(Me,k1),t(Me,qo),h(Tc,qo,null),t(qo,Z1),t(qo,Ch),t(qo,L1),t(qo,wh),t(qo,B1),t(qo,jh),t(qo,A1),h(qs,qo,null),y(o,s2,g),h(yc,o,g),y(o,i2,g),y(o,Te,g),h(Fc,Te,null),t(Te,R1),t(Te,xh),t(Te,W1),t(Te,$h),t(Te,J1),t(Te,Un),h(vc,Un,null),t(Un,V1),t(Un,kh),t(Un,G1),t(Un,Zh),t(Un,E1),h(Qs,Un,null),t(Te,P1),t(Te,Qo),h(Cc,Qo,null),t(Qo,S1),t(Qo,Lh),t(Qo,U1),t(Qo,Bh),t(Qo,I1),t(Qo,Ah),t(Qo,N1),h(Hs,Qo,null),y(o,l2,g),h(wc,o,g),y(o,d2,g),y(o,jc,g),y(o,c2,g),h(xc,o,g),y(o,m2,g),y(o,ye,g),h($c,ye,null),t(ye,X1),t(ye,Rh),t(ye,q1),t(ye,Wh),t(ye,Q1),t(ye,In),h(kc,In,null),t(In,H1),t(In,Jh),t(In,D1),t(In,Vh),t(In,Y1),h(Ds,In,null),t(ye,z1),t(ye,J),h(Zc,J,null),t(J,O1),t(J,Gh),t(J,K1),t(J,Eh),t(J,e$),t(J,Ph),t(J,o$),t(J,Sh),t(J,t$),h(Ys,J,null),y(o,f2,g),h(Lc,o,g),y(o,g2,g),y(o,Fe,g),h(Bc,Fe,null),t(Fe,n$),t(Fe,Uh),t(Fe,r$),t(Fe,Ih),t(Fe,a$),t(Fe,Nn),h(Ac,Nn,null),t(Nn,s$),t(Nn,Nh),t(Nn,i$),t(Nn,Xh),t(Nn,l$),h(zs,Nn,null),t(Fe,d$),t(Fe,Ho),h(Rc,Ho,null),t(Ho,c$),t(Ho,qh),t(Ho,m$),t(Ho,Qh),t(Ho,f$),t(Ho,Hh),t(Ho,g$),h(Os,Ho,null),y(o,p2,g),h(Wc,o,g),y(o,u2,g),y(o,ve,g),h(Jc,ve,null),t(ve,p$),t(ve,Dh),t(ve,u$),t(ve,Yh),t(ve,h$),t(ve,Xn),h(Vc,Xn,null),t(Xn,b$),t(Xn,zh),t(Xn,_$),t(Xn,Oh),t(Xn,M$),h(Ks,Xn,null),t(ve,T$),t(ve,Do),h(Gc,Do,null),t(Do,y$),t(Do,Kh),t(Do,F$),t(Do,eb),t(Do,v$),t(Do,ob),t(Do,C$),h(ei,Do,null),y(o,h2,g),h(Ec,o,g),y(o,b2,g),y(o,Ce,g),h(Pc,Ce,null),t(Ce,w$),t(Ce,tb),t(Ce,j$),t(Ce,nb),t(Ce,x$),t(Ce,qn),h(Sc,qn,null),t(qn,$$),t(qn,rb),t(qn,k$),t(qn,ab),t(qn,Z$),h(oi,qn,null),t(Ce,L$),t(Ce,V),h(Uc,V,null),t(V,B$),t(V,sb),t(V,A$),t(V,ib),t(V,R$),t(V,lb),t(V,W$),t(V,db),t(V,J$),h(ti,V,null),y(o,_2,g),h(Ic,o,g),y(o,M2,g),y(o,we,g),h(Nc,we,null),t(we,V$),t(we,cb),t(we,G$),t(we,mb),t(we,E$),t(we,Qn),h(Xc,Qn,null),t(Qn,P$),t(Qn,fb),t(Qn,S$),t(Qn,gb),t(Qn,U$),h(ni,Qn,null),t(we,I$),t(we,Yo),h(qc,Yo,null),t(Yo,N$),t(Yo,pb),t(Yo,X$),t(Yo,ub),t(Yo,q$),t(Yo,hb),t(Yo,Q$),h(ri,Yo,null),y(o,T2,g),h(Qc,o,g),y(o,y2,g),y(o,je,g),h(Hc,je,null),t(je,H$),t(je,bb),t(je,D$),t(je,_b),t(je,Y$),t(je,Hn),h(Dc,Hn,null),t(Hn,z$),t(Hn,Mb),t(Hn,O$),t(Hn,Tb),t(Hn,K$),h(ai,Hn,null),t(je,ek),t(je,zo),h(Yc,zo,null),t(zo,ok),t(zo,yb),t(zo,tk),t(zo,Fb),t(zo,nk),t(zo,vb),t(zo,rk),h(si,zo,null),y(o,F2,g),h(zc,o,g),y(o,v2,g),y(o,Oc,g),h(Kc,Oc,null),y(o,C2,g),h(em,o,g),y(o,w2,g),y(o,om,g),h(tm,om,null),y(o,j2,g),h(nm,o,g),y(o,x2,g),y(o,xe,g),h(rm,xe,null),t(xe,ak),t(xe,Cb),t(xe,sk),t(xe,wb),t(xe,ik),t(xe,Dn),h(am,Dn,null),t(Dn,lk),t(Dn,jb),t(Dn,dk),t(Dn,xb),t(Dn,ck),h(ii,Dn,null),t(xe,mk),t(xe,G),h(sm,G,null),t(G,fk),t(G,$b),t(G,gk),t(G,kb),t(G,pk),t(G,Zb),t(G,uk),t(G,Lb),t(G,hk),h(li,G,null),y(o,$2,g),h(im,o,g),y(o,k2,g),y(o,$e,g),h(lm,$e,null),t($e,bk),t($e,Bb),t($e,_k),t($e,Ab),t($e,Mk),t($e,Yn),h(dm,Yn,null),t(Yn,Tk),t(Yn,Rb),t(Yn,yk),t(Yn,Wb),t(Yn,Fk),h(di,Yn,null),t($e,vk),t($e,Oo),h(cm,Oo,null),t(Oo,Ck),t(Oo,Jb),t(Oo,wk),t(Oo,Vb),t(Oo,jk),t(Oo,Gb),t(Oo,xk),h(ci,Oo,null),y(o,Z2,g),h(mm,o,g),y(o,L2,g),y(o,ke,g),h(fm,ke,null),t(ke,$k),t(ke,Eb),t(ke,kk),t(ke,Pb),t(ke,Zk),t(ke,zn),h(gm,zn,null),t(zn,Lk),t(zn,Sb),t(zn,Bk),t(zn,Ub),t(zn,Ak),h(mi,zn,null),t(ke,Rk),t(ke,Ko),h(pm,Ko,null),t(Ko,Wk),t(Ko,Ib),t(Ko,Jk),t(Ko,Nb),t(Ko,Vk),t(Ko,Xb),t(Ko,Gk),h(fi,Ko,null),y(o,B2,g),h(um,o,g),y(o,A2,g),y(o,Ze,g),h(hm,Ze,null),t(Ze,Ek),t(Ze,qb),t(Ze,Pk),t(Ze,Qb),t(Ze,Sk),t(Ze,On),h(bm,On,null),t(On,Uk),t(On,Hb),t(On,Ik),t(On,Db),t(On,Nk),h(gi,On,null),t(Ze,Xk),t(Ze,E),h(_m,E,null),t(E,qk),t(E,Yb),t(E,Qk),t(E,zb),t(E,Hk),t(E,Ob),t(E,Dk),t(E,Kb),t(E,Yk),h(pi,E,null),y(o,R2,g),h(Mm,o,g),y(o,W2,g),y(o,Le,g),h(Tm,Le,null),t(Le,zk),t(Le,e_),t(Le,Ok),t(Le,o_),t(Le,Kk),t(Le,Kn),h(ym,Kn,null),t(Kn,eZ),t(Kn,t_),t(Kn,oZ),t(Kn,n_),t(Kn,tZ),h(ui,Kn,null),t(Le,nZ),t(Le,et),h(Fm,et,null),t(et,rZ),t(et,r_),t(et,aZ),t(et,a_),t(et,sZ),t(et,s_),t(et,iZ),h(hi,et,null),y(o,J2,g),h(vm,o,g),y(o,V2,g),y(o,Be,g),h(Cm,Be,null),t(Be,lZ),t(Be,i_),t(Be,dZ),t(Be,l_),t(Be,cZ),t(Be,er),h(wm,er,null),t(er,mZ),t(er,d_),t(er,fZ),t(er,c_),t(er,gZ),h(bi,er,null),t(Be,pZ),t(Be,ot),h(jm,ot,null),t(ot,uZ),t(ot,m_),t(ot,hZ),t(ot,f_),t(ot,bZ),t(ot,g_),t(ot,_Z),h(_i,ot,null),y(o,G2,g),h(xm,o,g),y(o,E2,g),y(o,Ae,g),h($m,Ae,null),t(Ae,MZ),t(Ae,p_),t(Ae,TZ),t(Ae,u_),t(Ae,yZ),t(Ae,or),h(km,or,null),t(or,FZ),t(or,h_),t(or,vZ),t(or,b_),t(or,CZ),h(Mi,or,null),t(Ae,wZ),t(Ae,P),h(Zm,P,null),t(P,jZ),t(P,__),t(P,xZ),t(P,M_),t(P,$Z),t(P,T_),t(P,kZ),t(P,y_),t(P,ZZ),h(Ti,P,null),y(o,P2,g),h(Lm,o,g),y(o,S2,g),y(o,Re,g),h(Bm,Re,null),t(Re,LZ),t(Re,F_),t(Re,BZ),t(Re,v_),t(Re,AZ),t(Re,tr),h(Am,tr,null),t(tr,RZ),t(tr,C_),t(tr,WZ),t(tr,w_),t(tr,JZ),h(yi,tr,null),t(Re,VZ),t(Re,tt),h(Rm,tt,null),t(tt,GZ),t(tt,j_),t(tt,EZ),t(tt,x_),t(tt,PZ),t(tt,$_),t(tt,SZ),h(Fi,tt,null),y(o,U2,g),h(Wm,o,g),y(o,I2,g),y(o,We,g),h(Jm,We,null),t(We,UZ),t(We,k_),t(We,IZ),t(We,Z_),t(We,NZ),t(We,nr),h(Vm,nr,null),t(nr,XZ),t(nr,L_),t(nr,qZ),t(nr,B_),t(nr,QZ),h(vi,nr,null),t(We,HZ),t(We,nt),h(Gm,nt,null),t(nt,DZ),t(nt,A_),t(nt,YZ),t(nt,R_),t(nt,zZ),t(nt,W_),t(nt,OZ),h(Ci,nt,null),y(o,N2,g),h(Em,o,g),y(o,X2,g),y(o,Je,g),h(Pm,Je,null),t(Je,KZ),t(Je,J_),t(Je,eL),t(Je,V_),t(Je,oL),t(Je,rr),h(Sm,rr,null),t(rr,tL),t(rr,G_),t(rr,nL),t(rr,E_),t(rr,rL),h(wi,rr,null),t(Je,aL),t(Je,S),h(Um,S,null),t(S,sL),t(S,P_),t(S,iL),t(S,S_),t(S,lL),t(S,U_),t(S,dL),t(S,I_),t(S,cL),h(ji,S,null),y(o,q2,g),h(Im,o,g),y(o,Q2,g),y(o,Ve,g),h(Nm,Ve,null),t(Ve,mL),t(Ve,N_),t(Ve,fL),t(Ve,X_),t(Ve,gL),t(Ve,ar),h(Xm,ar,null),t(ar,pL),t(ar,q_),t(ar,uL),t(ar,Q_),t(ar,hL),h(xi,ar,null),t(Ve,bL),t(Ve,rt),h(qm,rt,null),t(rt,_L),t(rt,H_),t(rt,ML),t(rt,D_),t(rt,TL),t(rt,Y_),t(rt,yL),h($i,rt,null),y(o,H2,g),h(Qm,o,g),y(o,D2,g),y(o,Ge,g),h(Hm,Ge,null),t(Ge,FL),t(Ge,z_),t(Ge,vL),t(Ge,O_),t(Ge,CL),t(Ge,sr),h(Dm,sr,null),t(sr,wL),t(sr,K_),t(sr,jL),t(sr,eM),t(sr,xL),h(ki,sr,null),t(Ge,$L),t(Ge,at),h(Ym,at,null),t(at,kL),t(at,oM),t(at,ZL),t(at,tM),t(at,LL),t(at,nM),t(at,BL),h(Zi,at,null),y(o,Y2,g),h(zm,o,g),y(o,z2,g),y(o,Ee,g),h(Om,Ee,null),t(Ee,AL),t(Ee,rM),t(Ee,RL),t(Ee,aM),t(Ee,WL),t(Ee,ir),h(Km,ir,null),t(ir,JL),t(ir,sM),t(ir,VL),t(ir,iM),t(ir,GL),h(Li,ir,null),t(Ee,EL),t(Ee,U),h(ef,U,null),t(U,PL),t(U,lM),t(U,SL),t(U,dM),t(U,UL),t(U,cM),t(U,IL),t(U,mM),t(U,NL),h(Bi,U,null),y(o,O2,g),h(of,o,g),y(o,K2,g),y(o,Pe,g),h(tf,Pe,null),t(Pe,XL),t(Pe,fM),t(Pe,qL),t(Pe,gM),t(Pe,QL),t(Pe,lr),h(nf,lr,null),t(lr,HL),t(lr,pM),t(lr,DL),t(lr,uM),t(lr,YL),h(Ai,lr,null),t(Pe,zL),t(Pe,st),h(rf,st,null),t(st,OL),t(st,hM),t(st,KL),t(st,bM),t(st,eB),t(st,_M),t(st,oB),h(Ri,st,null),y(o,ew,g),h(af,o,g),y(o,ow,g),y(o,Se,g),h(sf,Se,null),t(Se,tB),t(Se,MM),t(Se,nB),t(Se,TM),t(Se,rB),t(Se,dr),h(lf,dr,null),t(dr,aB),t(dr,yM),t(dr,sB),t(dr,FM),t(dr,iB),h(Wi,dr,null),t(Se,lB),t(Se,it),h(df,it,null),t(it,dB),t(it,vM),t(it,cB),t(it,CM),t(it,mB),t(it,wM),t(it,fB),h(Ji,it,null),y(o,tw,g),h(cf,o,g),y(o,nw,g),y(o,Ue,g),h(mf,Ue,null),t(Ue,gB),t(Ue,jM),t(Ue,pB),t(Ue,xM),t(Ue,uB),t(Ue,cr),h(ff,cr,null),t(cr,hB),t(cr,$M),t(cr,bB),t(cr,kM),t(cr,_B),h(Vi,cr,null),t(Ue,MB),t(Ue,I),h(gf,I,null),t(I,TB),t(I,ZM),t(I,yB),t(I,LM),t(I,FB),t(I,BM),t(I,vB),t(I,AM),t(I,CB),h(Gi,I,null),y(o,rw,g),h(pf,o,g),y(o,aw,g),y(o,Ie,g),h(uf,Ie,null),t(Ie,wB),t(Ie,RM),t(Ie,jB),t(Ie,WM),t(Ie,xB),t(Ie,mr),h(hf,mr,null),t(mr,$B),t(mr,JM),t(mr,kB),t(mr,VM),t(mr,ZB),h(Ei,mr,null),t(Ie,LB),t(Ie,lt),h(bf,lt,null),t(lt,BB),t(lt,GM),t(lt,AB),t(lt,EM),t(lt,RB),t(lt,PM),t(lt,WB),h(Pi,lt,null),y(o,sw,g),h(_f,o,g),y(o,iw,g),y(o,Ne,g),h(Mf,Ne,null),t(Ne,JB),t(Ne,SM),t(Ne,VB),t(Ne,UM),t(Ne,GB),t(Ne,fr),h(Tf,fr,null),t(fr,EB),t(fr,IM),t(fr,PB),t(fr,NM),t(fr,SB),h(Si,fr,null),t(Ne,UB),t(Ne,dt),h(yf,dt,null),t(dt,IB),t(dt,XM),t(dt,NB),t(dt,qM),t(dt,XB),t(dt,QM),t(dt,qB),h(Ui,dt,null),y(o,lw,g),h(Ff,o,g),y(o,dw,g),y(o,vf,g),h(Cf,vf,null),y(o,cw,g),h(wf,o,g),y(o,mw,g),y(o,jf,g),h(xf,jf,null),y(o,fw,g),h($f,o,g),y(o,gw,g),y(o,kf,g),y(o,pw,g),h(Zf,o,g),y(o,uw,g),y(o,Xe,g),h(Lf,Xe,null),t(Xe,QB),t(Xe,HM),t(Xe,HB),t(Xe,DM),t(Xe,DB),t(Xe,gr),h(Bf,gr,null),t(gr,YB),t(gr,YM),t(gr,zB),t(gr,zM),t(gr,OB),h(Ii,gr,null),t(Xe,KB),t(Xe,N),h(Af,N,null),t(N,eA),t(N,OM),t(N,oA),t(N,KM),t(N,tA),t(N,eT),t(N,nA),t(N,oT),t(N,rA),h(Ni,N,null),y(o,hw,g),h(Rf,o,g),y(o,bw,g),y(o,qe,g),h(Wf,qe,null),t(qe,aA),t(qe,tT),t(qe,sA),t(qe,nT),t(qe,iA),t(qe,pr),h(Jf,pr,null),t(pr,lA),t(pr,rT),t(pr,dA),t(pr,aT),t(pr,cA),h(Xi,pr,null),t(qe,mA),t(qe,X),h(Vf,X,null),t(X,fA),t(X,sT),t(X,gA),t(X,iT),t(X,pA),t(X,lT),t(X,uA),t(X,dT),t(X,hA),h(qi,X,null),y(o,_w,g),h(Gf,o,g),y(o,Mw,g),y(o,Qe,g),h(Ef,Qe,null),t(Qe,bA),t(Qe,cT),t(Qe,_A),t(Qe,mT),t(Qe,MA),t(Qe,ur),h(Pf,ur,null),t(ur,TA),t(ur,fT),t(ur,yA),t(ur,gT),t(ur,FA),h(Qi,ur,null),t(Qe,vA),t(Qe,ct),h(Sf,ct,null),t(ct,CA),t(ct,pT),t(ct,wA),t(ct,uT),t(ct,jA),t(ct,hT),t(ct,xA),h(Hi,ct,null),y(o,Tw,g),h(Uf,o,g),y(o,yw,g),y(o,He,g),h(If,He,null),t(He,$A),t(He,bT),t(He,kA),t(He,_T),t(He,ZA),t(He,hr),h(Nf,hr,null),t(hr,LA),t(hr,MT),t(hr,BA),t(hr,TT),t(hr,AA),h(Di,hr,null),t(He,RA),t(He,mt),h(Xf,mt,null),t(mt,WA),t(mt,yT),t(mt,JA),t(mt,FT),t(mt,VA),t(mt,vT),t(mt,GA),h(Yi,mt,null),y(o,Fw,g),h(qf,o,g),y(o,vw,g),y(o,De,g),h(Qf,De,null),t(De,EA),t(De,CT),t(De,PA),t(De,wT),t(De,SA),t(De,br),h(Hf,br,null),t(br,UA),t(br,jT),t(br,IA),t(br,xT),t(br,NA),h(zi,br,null),t(De,XA),t(De,q),h(Df,q,null),t(q,qA),t(q,$T),t(q,QA),t(q,kT),t(q,HA),t(q,ZT),t(q,DA),t(q,LT),t(q,YA),h(Oi,q,null),y(o,Cw,g),h(Yf,o,g),y(o,ww,g),y(o,Ye,g),h(zf,Ye,null),t(Ye,zA),t(Ye,BT),t(Ye,OA),t(Ye,AT),t(Ye,KA),t(Ye,_r),h(Of,_r,null),t(_r,eR),t(_r,RT),t(_r,oR),t(_r,WT),t(_r,tR),h(Ki,_r,null),t(Ye,nR),t(Ye,Q),h(Kf,Q,null),t(Q,rR),t(Q,JT),t(Q,aR),t(Q,VT),t(Q,sR),t(Q,GT),t(Q,iR),t(Q,ET),t(Q,lR),h(el,Q,null),y(o,jw,g),h(eg,o,g),y(o,xw,g),y(o,ze,g),h(og,ze,null),t(ze,dR),t(ze,PT),t(ze,cR),t(ze,ST),t(ze,mR),t(ze,Mr),h(tg,Mr,null),t(Mr,fR),t(Mr,UT),t(Mr,gR),t(Mr,IT),t(Mr,pR),h(ol,Mr,null),t(ze,uR),t(ze,ft),h(ng,ft,null),t(ft,hR),t(ft,NT),t(ft,bR),t(ft,XT),t(ft,_R),t(ft,qT),t(ft,MR),h(tl,ft,null),y(o,$w,g),h(rg,o,g),y(o,kw,g),y(o,Oe,g),h(ag,Oe,null),t(Oe,TR),t(Oe,QT),t(Oe,yR),t(Oe,HT),t(Oe,FR),t(Oe,Tr),h(sg,Tr,null),t(Tr,vR),t(Tr,DT),t(Tr,CR),t(Tr,YT),t(Tr,wR),h(nl,Tr,null),t(Oe,jR),t(Oe,H),h(ig,H,null),t(H,xR),t(H,zT),t(H,$R),t(H,OT),t(H,kR),t(H,KT),t(H,ZR),t(H,ey),t(H,LR),h(rl,H,null),y(o,Zw,g),h(lg,o,g),y(o,Lw,g),y(o,Ke,g),h(dg,Ke,null),t(Ke,BR),t(Ke,oy),t(Ke,AR),t(Ke,ty),t(Ke,RR),t(Ke,yr),h(cg,yr,null),t(yr,WR),t(yr,ny),t(yr,JR),t(yr,ry),t(yr,VR),h(al,yr,null),t(Ke,GR),t(Ke,D),h(mg,D,null),t(D,ER),t(D,ay),t(D,PR),t(D,sy),t(D,SR),t(D,iy),t(D,UR),t(D,ly),t(D,IR),h(sl,D,null),y(o,Bw,g),h(fg,o,g),y(o,Aw,g),y(o,gg,g),h(pg,gg,null),y(o,Rw,g),h(ug,o,g),y(o,Ww,g),y(o,eo,g),h(hg,eo,null),t(eo,NR),t(eo,dy),t(eo,XR),t(eo,cy),t(eo,qR),t(eo,Fr),h(bg,Fr,null),t(Fr,QR),t(Fr,my),t(Fr,HR),t(Fr,fy),t(Fr,DR),h(il,Fr,null),t(eo,YR),t(eo,Y),h(_g,Y,null),t(Y,zR),t(Y,gy),t(Y,OR),t(Y,py),t(Y,KR),t(Y,uy),t(Y,eW),t(Y,hy),t(Y,oW),h(ll,Y,null),y(o,Jw,g),h(Mg,o,g),y(o,Vw,g),y(o,oo,g),h(Tg,oo,null),t(oo,tW),t(oo,by),t(oo,nW),t(oo,_y),t(oo,rW),t(oo,vr),h(yg,vr,null),t(vr,aW),t(vr,My),t(vr,sW),t(vr,Ty),t(vr,iW),h(dl,vr,null),t(oo,lW),t(oo,gt),h(Fg,gt,null),t(gt,dW),t(gt,yy),t(gt,cW),t(gt,Fy),t(gt,mW),t(gt,vy),t(gt,fW),h(cl,gt,null),y(o,Gw,g),h(vg,o,g),y(o,Ew,g),y(o,to,g),h(Cg,to,null),t(to,gW),t(to,Cy),t(to,pW),t(to,wy),t(to,uW),t(to,Cr),h(wg,Cr,null),t(Cr,hW),t(Cr,jy),t(Cr,bW),t(Cr,xy),t(Cr,_W),h(ml,Cr,null),t(to,MW),t(to,z),h(jg,z,null),t(z,TW),t(z,$y),t(z,yW),t(z,ky),t(z,FW),t(z,Zy),t(z,vW),t(z,Ly),t(z,CW),h(fl,z,null),y(o,Pw,g),h(xg,o,g),y(o,Sw,g),y(o,no,g),h($g,no,null),t(no,wW),t(no,By),t(no,jW),t(no,Ay),t(no,xW),t(no,wr),h(kg,wr,null),t(wr,$W),t(wr,Ry),t(wr,kW),t(wr,Wy),t(wr,ZW),h(gl,wr,null),t(no,LW),t(no,O),h(Zg,O,null),t(O,BW),t(O,Jy),t(O,AW),t(O,Vy),t(O,RW),t(O,Gy),t(O,WW),t(O,Ey),t(O,JW),h(pl,O,null),y(o,Uw,g),h(Lg,o,g),y(o,Iw,g),y(o,ro,g),h(Bg,ro,null),t(ro,VW),t(ro,Py),t(ro,GW),t(ro,Sy),t(ro,EW),t(ro,jr),h(Ag,jr,null),t(jr,PW),t(jr,Uy),t(jr,SW),t(jr,Iy),t(jr,UW),h(ul,jr,null),t(ro,IW),t(ro,K),h(Rg,K,null),t(K,NW),t(K,Ny),t(K,XW),t(K,Xy),t(K,qW),t(K,qy),t(K,QW),t(K,Qy),t(K,HW),h(hl,K,null),y(o,Nw,g),h(Wg,o,g),y(o,Xw,g),y(o,ao,g),h(Jg,ao,null),t(ao,DW),t(ao,Hy),t(ao,YW),t(ao,Dy),t(ao,zW),t(ao,xr),h(Vg,xr,null),t(xr,OW),t(xr,Yy),t(xr,KW),t(xr,zy),t(xr,e0),h(bl,xr,null),t(ao,o0),t(ao,pt),h(Gg,pt,null),t(pt,t0),t(pt,Oy),t(pt,n0),t(pt,Ky),t(pt,r0),t(pt,eF),t(pt,a0),h(_l,pt,null),y(o,qw,g),h(Eg,o,g),y(o,Qw,g),y(o,so,g),h(Pg,so,null),t(so,s0),t(so,oF),t(so,i0),t(so,tF),t(so,l0),t(so,$r),h(Sg,$r,null),t($r,d0),t($r,nF),t($r,c0),t($r,rF),t($r,m0),h(Ml,$r,null),t(so,f0),t(so,ee),h(Ug,ee,null),t(ee,g0),t(ee,aF),t(ee,p0),t(ee,sF),t(ee,u0),t(ee,iF),t(ee,h0),t(ee,lF),t(ee,b0),h(Tl,ee,null),y(o,Hw,g),h(Ig,o,g),y(o,Dw,g),y(o,Ng,g),y(o,Yw,g),h(Xg,o,g),y(o,zw,g),y(o,io,g),h(qg,io,null),t(io,_0),t(io,dF),t(io,M0),t(io,cF),t(io,T0),t(io,kr),h(Qg,kr,null),t(kr,y0),t(kr,mF),t(kr,F0),t(kr,fF),t(kr,v0),h(yl,kr,null),t(io,C0),t(io,oe),h(Hg,oe,null),t(oe,w0),t(oe,gF),t(oe,j0),t(oe,pF),t(oe,x0),t(oe,uF),t(oe,$0),t(oe,hF),t(oe,k0),h(Fl,oe,null),y(o,Ow,g),h(Dg,o,g),y(o,Kw,g),y(o,lo,g),h(Yg,lo,null),t(lo,Z0),t(lo,bF),t(lo,L0),t(lo,_F),t(lo,B0),t(lo,Zr),h(zg,Zr,null),t(Zr,A0),t(Zr,MF),t(Zr,R0),t(Zr,TF),t(Zr,W0),h(vl,Zr,null),t(lo,J0),t(lo,ut),h(Og,ut,null),t(ut,V0),t(ut,yF),t(ut,G0),t(ut,FF),t(ut,E0),t(ut,vF),t(ut,P0),h(Cl,ut,null),y(o,ej,g),h(Kg,o,g),y(o,oj,g),y(o,co,g),h(ep,co,null),t(co,S0),t(co,CF),t(co,U0),t(co,wF),t(co,I0),t(co,Lr),h(op,Lr,null),t(Lr,N0),t(Lr,jF),t(Lr,X0),t(Lr,xF),t(Lr,q0),h(wl,Lr,null),t(co,Q0),t(co,te),h(tp,te,null),t(te,H0),t(te,$F),t(te,D0),t(te,kF),t(te,Y0),t(te,ZF),t(te,z0),t(te,LF),t(te,O0),h(jl,te,null),y(o,tj,g),h(np,o,g),y(o,nj,g),y(o,mo,g),h(rp,mo,null),t(mo,K0),t(mo,BF),t(mo,eJ),t(mo,AF),t(mo,oJ),t(mo,Br),h(ap,Br,null),t(Br,tJ),t(Br,RF),t(Br,nJ),t(Br,WF),t(Br,rJ),h(xl,Br,null),t(mo,aJ),t(mo,ne),h(sp,ne,null),t(ne,sJ),t(ne,JF),t(ne,iJ),t(ne,VF),t(ne,lJ),t(ne,GF),t(ne,dJ),t(ne,EF),t(ne,cJ),h($l,ne,null),y(o,rj,g),h(ip,o,g),y(o,aj,g),y(o,fo,g),h(lp,fo,null),t(fo,mJ),t(fo,PF),t(fo,fJ),t(fo,SF),t(fo,gJ),t(fo,Ar),h(dp,Ar,null),t(Ar,pJ),t(Ar,UF),t(Ar,uJ),t(Ar,IF),t(Ar,hJ),h(kl,Ar,null),t(fo,bJ),t(fo,re),h(cp,re,null),t(re,_J),t(re,NF),t(re,MJ),t(re,XF),t(re,TJ),t(re,qF),t(re,yJ),t(re,QF),t(re,FJ),h(Zl,re,null),y(o,sj,g),h(mp,o,g),y(o,ij,g),y(o,go,g),h(fp,go,null),t(go,vJ),t(go,HF),t(go,CJ),t(go,DF),t(go,wJ),t(go,Rr),h(gp,Rr,null),t(Rr,jJ),t(Rr,YF),t(Rr,xJ),t(Rr,zF),t(Rr,$J),h(Ll,Rr,null),t(go,kJ),t(go,ht),h(pp,ht,null),t(ht,ZJ),t(ht,OF),t(ht,LJ),t(ht,KF),t(ht,BJ),t(ht,ev),t(ht,AJ),h(Bl,ht,null),y(o,lj,g),h(up,o,g),y(o,dj,g),y(o,po,g),h(hp,po,null),t(po,RJ),t(po,ov),t(po,WJ),t(po,tv),t(po,JJ),t(po,Wr),h(bp,Wr,null),t(Wr,VJ),t(Wr,nv),t(Wr,GJ),t(Wr,rv),t(Wr,EJ),h(Al,Wr,null),t(po,PJ),t(po,bt),h(_p,bt,null),t(bt,SJ),t(bt,av),t(bt,UJ),t(bt,sv),t(bt,IJ),t(bt,iv),t(bt,NJ),h(Rl,bt,null),y(o,cj,g),h(Mp,o,g),y(o,mj,g),y(o,uo,g),h(Tp,uo,null),t(uo,XJ),t(uo,lv),t(uo,qJ),t(uo,dv),t(uo,QJ),t(uo,Jr),h(yp,Jr,null),t(Jr,HJ),t(Jr,cv),t(Jr,DJ),t(Jr,mv),t(Jr,YJ),h(Wl,Jr,null),t(uo,zJ),t(uo,ae),h(Fp,ae,null),t(ae,OJ),t(ae,fv),t(ae,KJ),t(ae,gv),t(ae,eV),t(ae,pv),t(ae,oV),t(ae,uv),t(ae,tV),h(Jl,ae,null),y(o,fj,g),h(vp,o,g),y(o,gj,g),y(o,Cp,g),h(wp,Cp,null),y(o,pj,g),h(jp,o,g),y(o,uj,g),y(o,xp,g),h($p,xp,null),y(o,hj,g),h(kp,o,g),y(o,bj,g),y(o,Zp,g),y(o,_j,g),h(Lp,o,g),y(o,Mj,g),y(o,ho,g),h(Bp,ho,null),t(ho,nV),t(ho,hv),t(ho,rV),t(ho,bv),t(ho,aV),t(ho,Vr),h(Ap,Vr,null),t(Vr,sV),t(Vr,_v),t(Vr,iV),t(Vr,Mv),t(Vr,lV),h(Vl,Vr,null),t(ho,dV),t(ho,se),h(Rp,se,null),t(se,cV),t(se,Tv),t(se,mV),t(se,yv),t(se,fV),t(se,Fv),t(se,gV),t(se,vv),t(se,pV),h(Gl,se,null),y(o,Tj,g),h(Wp,o,g),y(o,yj,g),y(o,bo,g),h(Jp,bo,null),t(bo,uV),t(bo,Cv),t(bo,hV),t(bo,wv),t(bo,bV),t(bo,Gr),h(Vp,Gr,null),t(Gr,_V),t(Gr,jv),t(Gr,MV),t(Gr,xv),t(Gr,TV),h(El,Gr,null),t(bo,yV),t(bo,_t),h(Gp,_t,null),t(_t,FV),t(_t,$v),t(_t,vV),t(_t,kv),t(_t,CV),t(_t,Zv),t(_t,wV),h(Pl,_t,null),y(o,Fj,g),h(Ep,o,g),y(o,vj,g),y(o,_o,g),h(Pp,_o,null),t(_o,jV),t(_o,Lv),t(_o,xV),t(_o,Bv),t(_o,$V),t(_o,Er),h(Sp,Er,null),t(Er,kV),t(Er,Av),t(Er,ZV),t(Er,Rv),t(Er,LV),h(Sl,Er,null),t(_o,BV),t(_o,ie),h(Up,ie,null),t(ie,AV),t(ie,Wv),t(ie,RV),t(ie,Jv),t(ie,WV),t(ie,Vv),t(ie,JV),t(ie,Gv),t(ie,VV),h(Ul,ie,null),y(o,Cj,g),h(Ip,o,g),y(o,wj,g),y(o,Mo,g),h(Np,Mo,null),t(Mo,GV),t(Mo,Ev),t(Mo,EV),t(Mo,Pv),t(Mo,PV),t(Mo,Pr),h(Xp,Pr,null),t(Pr,SV),t(Pr,Sv),t(Pr,UV),t(Pr,Uv),t(Pr,IV),h(Il,Pr,null),t(Mo,NV),t(Mo,Mt),h(qp,Mt,null),t(Mt,XV),t(Mt,Iv),t(Mt,qV),t(Mt,Nv),t(Mt,QV),t(Mt,Xv),t(Mt,HV),h(Nl,Mt,null),y(o,jj,g),h(Qp,o,g),y(o,xj,g),y(o,To,g),h(Hp,To,null),t(To,DV),t(To,qv),t(To,YV),t(To,Qv),t(To,zV),t(To,Sr),h(Dp,Sr,null),t(Sr,OV),t(Sr,Hv),t(Sr,KV),t(Sr,Dv),t(Sr,eG),h(Xl,Sr,null),t(To,oG),t(To,le),h(Yp,le,null),t(le,tG),t(le,Yv),t(le,nG),t(le,zv),t(le,rG),t(le,Ov),t(le,aG),t(le,Kv),t(le,sG),h(ql,le,null),y(o,$j,g),h(zp,o,g),y(o,kj,g),y(o,yo,g),h(Op,yo,null),t(yo,iG),t(yo,eC),t(yo,lG),t(yo,oC),t(yo,dG),t(yo,Ur),h(Kp,Ur,null),t(Ur,cG),t(Ur,tC),t(Ur,mG),t(Ur,nC),t(Ur,fG),h(Ql,Ur,null),t(yo,gG),t(yo,de),h(eu,de,null),t(de,pG),t(de,rC),t(de,uG),t(de,aC),t(de,hG),t(de,sC),t(de,bG),t(de,iC),t(de,_G),h(Hl,de,null),y(o,Zj,g),h(ou,o,g),y(o,Lj,g),y(o,Fo,g),h(tu,Fo,null),t(Fo,MG),t(Fo,lC),t(Fo,TG),t(Fo,dC),t(Fo,yG),t(Fo,Ir),h(nu,Ir,null),t(Ir,FG),t(Ir,cC),t(Ir,vG),t(Ir,mC),t(Ir,CG),h(Dl,Ir,null),t(Fo,wG),t(Fo,Tt),h(ru,Tt,null),t(Tt,jG),t(Tt,fC),t(Tt,xG),t(Tt,gC),t(Tt,$G),t(Tt,pC),t(Tt,kG),h(Yl,Tt,null),y(o,Bj,g),h(au,o,g),y(o,Aj,g),y(o,vo,g),h(su,vo,null),t(vo,ZG),t(vo,uC),t(vo,LG),t(vo,hC),t(vo,BG),t(vo,Nr),h(iu,Nr,null),t(Nr,AG),t(Nr,bC),t(Nr,RG),t(Nr,_C),t(Nr,WG),h(zl,Nr,null),t(vo,JG),t(vo,yt),h(lu,yt,null),t(yt,VG),t(yt,MC),t(yt,GG),t(yt,TC),t(yt,EG),t(yt,yC),t(yt,PG),h(Ol,yt,null),y(o,Rj,g),y(o,FC,g),Wj=!0},p(o,[g]){const Ft={};g&2&&(Ft.$$scope={dirty:g,ctx:o}),vs.$set(Ft);const vt={};g&2&&(vt.$$scope={dirty:g,ctx:o}),Cs.$set(vt);const du={};g&2&&(du.$$scope={dirty:g,ctx:o}),js.$set(du);const Ct={};g&2&&(Ct.$$scope={dirty:g,ctx:o}),$s.$set(Ct);const wt={};g&2&&(wt.$$scope={dirty:g,ctx:o}),ks.$set(wt);const cu={};g&2&&(cu.$$scope={dirty:g,ctx:o}),Ls.$set(cu);const jt={};g&2&&(jt.$$scope={dirty:g,ctx:o}),Bs.$set(jt);const Co={};g&2&&(Co.$$scope={dirty:g,ctx:o}),Rs.$set(Co);const mu={};g&2&&(mu.$$scope={dirty:g,ctx:o}),Ws.$set(mu);const xt={};g&2&&(xt.$$scope={dirty:g,ctx:o}),Vs.$set(xt);const wo={};g&2&&(wo.$$scope={dirty:g,ctx:o}),Gs.$set(wo);const fu={};g&2&&(fu.$$scope={dirty:g,ctx:o}),Es.$set(fu);const $t={};g&2&&($t.$$scope={dirty:g,ctx:o}),Ps.$set($t);const jo={};g&2&&(jo.$$scope={dirty:g,ctx:o}),Ss.$set(jo);const gu={};g&2&&(gu.$$scope={dirty:g,ctx:o}),Us.$set(gu);const kt={};g&2&&(kt.$$scope={dirty:g,ctx:o}),Is.$set(kt);const sa={};g&2&&(sa.$$scope={dirty:g,ctx:o}),Ns.$set(sa);const xo={};g&2&&(xo.$$scope={dirty:g,ctx:o}),Xs.$set(xo);const Zt={};g&2&&(Zt.$$scope={dirty:g,ctx:o}),qs.$set(Zt);const ia={};g&2&&(ia.$$scope={dirty:g,ctx:o}),Qs.$set(ia);const Lt={};g&2&&(Lt.$$scope={dirty:g,ctx:o}),Hs.$set(Lt);const Bt={};g&2&&(Bt.$$scope={dirty:g,ctx:o}),Ds.$set(Bt);const la={};g&2&&(la.$$scope={dirty:g,ctx:o}),Ys.$set(la);const At={};g&2&&(At.$$scope={dirty:g,ctx:o}),zs.$set(At);const Rt={};g&2&&(Rt.$$scope={dirty:g,ctx:o}),Os.$set(Rt);const da={};g&2&&(da.$$scope={dirty:g,ctx:o}),Ks.$set(da);const $o={};g&2&&($o.$$scope={dirty:g,ctx:o}),ei.$set($o);const Wt={};g&2&&(Wt.$$scope={dirty:g,ctx:o}),oi.$set(Wt);const ca={};g&2&&(ca.$$scope={dirty:g,ctx:o}),ti.$set(ca);const Jt={};g&2&&(Jt.$$scope={dirty:g,ctx:o}),ni.$set(Jt);const Vt={};g&2&&(Vt.$$scope={dirty:g,ctx:o}),ri.$set(Vt);const ma={};g&2&&(ma.$$scope={dirty:g,ctx:o}),ai.$set(ma);const Gt={};g&2&&(Gt.$$scope={dirty:g,ctx:o}),si.$set(Gt);const Et={};g&2&&(Et.$$scope={dirty:g,ctx:o}),ii.$set(Et);const fa={};g&2&&(fa.$$scope={dirty:g,ctx:o}),li.$set(fa);const ko={};g&2&&(ko.$$scope={dirty:g,ctx:o}),di.$set(ko);const Pt={};g&2&&(Pt.$$scope={dirty:g,ctx:o}),ci.$set(Pt);const ga={};g&2&&(ga.$$scope={dirty:g,ctx:o}),mi.$set(ga);const St={};g&2&&(St.$$scope={dirty:g,ctx:o}),fi.$set(St);const Ut={};g&2&&(Ut.$$scope={dirty:g,ctx:o}),gi.$set(Ut);const pa={};g&2&&(pa.$$scope={dirty:g,ctx:o}),pi.$set(pa);const It={};g&2&&(It.$$scope={dirty:g,ctx:o}),ui.$set(It);const Nt={};g&2&&(Nt.$$scope={dirty:g,ctx:o}),hi.$set(Nt);const ua={};g&2&&(ua.$$scope={dirty:g,ctx:o}),bi.$set(ua);const Zo={};g&2&&(Zo.$$scope={dirty:g,ctx:o}),_i.$set(Zo);const Xt={};g&2&&(Xt.$$scope={dirty:g,ctx:o}),Mi.$set(Xt);const ha={};g&2&&(ha.$$scope={dirty:g,ctx:o}),Ti.$set(ha);const qt={};g&2&&(qt.$$scope={dirty:g,ctx:o}),yi.$set(qt);const Qt={};g&2&&(Qt.$$scope={dirty:g,ctx:o}),Fi.$set(Qt);const ba={};g&2&&(ba.$$scope={dirty:g,ctx:o}),vi.$set(ba);const Ht={};g&2&&(Ht.$$scope={dirty:g,ctx:o}),Ci.$set(Ht);const vC={};g&2&&(vC.$$scope={dirty:g,ctx:o}),wi.$set(vC);const CC={};g&2&&(CC.$$scope={dirty:g,ctx:o}),ji.$set(CC);const Dt={};g&2&&(Dt.$$scope={dirty:g,ctx:o}),xi.$set(Dt);const _a={};g&2&&(_a.$$scope={dirty:g,ctx:o}),$i.$set(_a);const Lo={};g&2&&(Lo.$$scope={dirty:g,ctx:o}),ki.$set(Lo);const Yt={};g&2&&(Yt.$$scope={dirty:g,ctx:o}),Zi.$set(Yt);const Ma={};g&2&&(Ma.$$scope={dirty:g,ctx:o}),Li.$set(Ma);const zt={};g&2&&(zt.$$scope={dirty:g,ctx:o}),Bi.$set(zt);const Ot={};g&2&&(Ot.$$scope={dirty:g,ctx:o}),Ai.$set(Ot);const Ta={};g&2&&(Ta.$$scope={dirty:g,ctx:o}),Ri.$set(Ta);const Kt={};g&2&&(Kt.$$scope={dirty:g,ctx:o}),Wi.$set(Kt);const en={};g&2&&(en.$$scope={dirty:g,ctx:o}),Ji.$set(en);const ya={};g&2&&(ya.$$scope={dirty:g,ctx:o}),Vi.$set(ya);const Bo={};g&2&&(Bo.$$scope={dirty:g,ctx:o}),Gi.$set(Bo);const on={};g&2&&(on.$$scope={dirty:g,ctx:o}),Ei.$set(on);const Fa={};g&2&&(Fa.$$scope={dirty:g,ctx:o}),Pi.$set(Fa);const tn={};g&2&&(tn.$$scope={dirty:g,ctx:o}),Si.$set(tn);const nn={};g&2&&(nn.$$scope={dirty:g,ctx:o}),Ui.$set(nn);const va={};g&2&&(va.$$scope={dirty:g,ctx:o}),Ii.$set(va);const rn={};g&2&&(rn.$$scope={dirty:g,ctx:o}),Ni.$set(rn);const an={};g&2&&(an.$$scope={dirty:g,ctx:o}),Xi.$set(an);const Ca={};g&2&&(Ca.$$scope={dirty:g,ctx:o}),qi.$set(Ca);const Ao={};g&2&&(Ao.$$scope={dirty:g,ctx:o}),Qi.$set(Ao);const sn={};g&2&&(sn.$$scope={dirty:g,ctx:o}),Hi.$set(sn);const wa={};g&2&&(wa.$$scope={dirty:g,ctx:o}),Di.$set(wa);const ln={};g&2&&(ln.$$scope={dirty:g,ctx:o}),Yi.$set(ln);const dn={};g&2&&(dn.$$scope={dirty:g,ctx:o}),zi.$set(dn);const ja={};g&2&&(ja.$$scope={dirty:g,ctx:o}),Oi.$set(ja);const cn={};g&2&&(cn.$$scope={dirty:g,ctx:o}),Ki.$set(cn);const mn={};g&2&&(mn.$$scope={dirty:g,ctx:o}),el.$set(mn);const xa={};g&2&&(xa.$$scope={dirty:g,ctx:o}),ol.$set(xa);const Ro={};g&2&&(Ro.$$scope={dirty:g,ctx:o}),tl.$set(Ro);const fn={};g&2&&(fn.$$scope={dirty:g,ctx:o}),nl.$set(fn);const $a={};g&2&&($a.$$scope={dirty:g,ctx:o}),rl.$set($a);const gn={};g&2&&(gn.$$scope={dirty:g,ctx:o}),al.$set(gn);const pn={};g&2&&(pn.$$scope={dirty:g,ctx:o}),sl.$set(pn);const ka={};g&2&&(ka.$$scope={dirty:g,ctx:o}),il.$set(ka);const un={};g&2&&(un.$$scope={dirty:g,ctx:o}),ll.$set(un);const hn={};g&2&&(hn.$$scope={dirty:g,ctx:o}),dl.$set(hn);const Za={};g&2&&(Za.$$scope={dirty:g,ctx:o}),cl.$set(Za);const Wo={};g&2&&(Wo.$$scope={dirty:g,ctx:o}),ml.$set(Wo);const bn={};g&2&&(bn.$$scope={dirty:g,ctx:o}),fl.$set(bn);const La={};g&2&&(La.$$scope={dirty:g,ctx:o}),gl.$set(La);const _n={};g&2&&(_n.$$scope={dirty:g,ctx:o}),pl.$set(_n);const Mn={};g&2&&(Mn.$$scope={dirty:g,ctx:o}),ul.$set(Mn);const Ba={};g&2&&(Ba.$$scope={dirty:g,ctx:o}),hl.$set(Ba);const Tn={};g&2&&(Tn.$$scope={dirty:g,ctx:o}),bl.$set(Tn);const yn={};g&2&&(yn.$$scope={dirty:g,ctx:o}),_l.$set(yn);const Aa={};g&2&&(Aa.$$scope={dirty:g,ctx:o}),Ml.$set(Aa);const Jo={};g&2&&(Jo.$$scope={dirty:g,ctx:o}),Tl.$set(Jo);const Fn={};g&2&&(Fn.$$scope={dirty:g,ctx:o}),yl.$set(Fn);const Ra={};g&2&&(Ra.$$scope={dirty:g,ctx:o}),Fl.$set(Ra);const vn={};g&2&&(vn.$$scope={dirty:g,ctx:o}),vl.$set(vn);const Cn={};g&2&&(Cn.$$scope={dirty:g,ctx:o}),Cl.$set(Cn);const Wa={};g&2&&(Wa.$$scope={dirty:g,ctx:o}),wl.$set(Wa);const wn={};g&2&&(wn.$$scope={dirty:g,ctx:o}),jl.$set(wn);const wC={};g&2&&(wC.$$scope={dirty:g,ctx:o}),xl.$set(wC);const jC={};g&2&&(jC.$$scope={dirty:g,ctx:o}),$l.$set(jC);const jn={};g&2&&(jn.$$scope={dirty:g,ctx:o}),kl.$set(jn);const Ja={};g&2&&(Ja.$$scope={dirty:g,ctx:o}),Zl.$set(Ja);const Vo={};g&2&&(Vo.$$scope={dirty:g,ctx:o}),Ll.$set(Vo);const xn={};g&2&&(xn.$$scope={dirty:g,ctx:o}),Bl.$set(xn);const Va={};g&2&&(Va.$$scope={dirty:g,ctx:o}),Al.$set(Va);const Go={};g&2&&(Go.$$scope={dirty:g,ctx:o}),Rl.$set(Go);const $n={};g&2&&($n.$$scope={dirty:g,ctx:o}),Wl.$set($n);const Ga={};g&2&&(Ga.$$scope={dirty:g,ctx:o}),Jl.$set(Ga);const kn={};g&2&&(kn.$$scope={dirty:g,ctx:o}),Vl.$set(kn);const Zn={};g&2&&(Zn.$$scope={dirty:g,ctx:o}),Gl.$set(Zn);const Ea={};g&2&&(Ea.$$scope={dirty:g,ctx:o}),El.$set(Ea);const Ln={};g&2&&(Ln.$$scope={dirty:g,ctx:o}),Pl.$set(Ln);const Bn={};g&2&&(Bn.$$scope={dirty:g,ctx:o}),Sl.$set(Bn);const Pa={};g&2&&(Pa.$$scope={dirty:g,ctx:o}),Ul.$set(Pa);const Eo={};g&2&&(Eo.$$scope={dirty:g,ctx:o}),Il.$set(Eo);const An={};g&2&&(An.$$scope={dirty:g,ctx:o}),Nl.$set(An);const Sa={};g&2&&(Sa.$$scope={dirty:g,ctx:o}),Xl.$set(Sa);const Po={};g&2&&(Po.$$scope={dirty:g,ctx:o}),ql.$set(Po);const Rn={};g&2&&(Rn.$$scope={dirty:g,ctx:o}),Ql.$set(Rn);const Ua={};g&2&&(Ua.$$scope={dirty:g,ctx:o}),Hl.$set(Ua);const Wn={};g&2&&(Wn.$$scope={dirty:g,ctx:o}),Dl.$set(Wn);const Jn={};g&2&&(Jn.$$scope={dirty:g,ctx:o}),Yl.$set(Jn);const Ia={};g&2&&(Ia.$$scope={dirty:g,ctx:o}),zl.$set(Ia);const So={};g&2&&(So.$$scope={dirty:g,ctx:o}),Ol.$set(So)},i(o){Wj||(b(m.$$.fragment,o),b(Cd.$$.fragment,o),b(xd.$$.fragment,o),b(kd.$$.fragment,o),b(vs.$$.fragment,o),b(Ld.$$.fragment,o),b(Bd.$$.fragment,o),b(Ad.$$.fragment,o),b(Cs.$$.fragment,o),b(Rd.$$.fragment,o),b(Wd.$$.fragment,o),b(Jd.$$.fragment,o),b(Vd.$$.fragment,o),b(js.$$.fragment,o),b(Gd.$$.fragment,o),b(Ed.$$.fragment,o),b(Pd.$$.fragment,o),b(Sd.$$.fragment,o),b($s.$$.fragment,o),b(ks.$$.fragment,o),b(Ud.$$.fragment,o),b(Id.$$.fragment,o),b(Nd.$$.fragment,o),b(Xd.$$.fragment,o),b(Ls.$$.fragment,o),b(Bs.$$.fragment,o),b(qd.$$.fragment,o),b(Qd.$$.fragment,o),b(Hd.$$.fragment,o),b(Dd.$$.fragment,o),b(Rs.$$.fragment,o),b(Ws.$$.fragment,o),b(Yd.$$.fragment,o),b(zd.$$.fragment,o),b(Kd.$$.fragment,o),b(ec.$$.fragment,o),b(oc.$$.fragment,o),b(Vs.$$.fragment,o),b(tc.$$.fragment,o),b(Gs.$$.fragment,o),b(nc.$$.fragment,o),b(rc.$$.fragment,o),b(ac.$$.fragment,o),b(Es.$$.fragment,o),b(sc.$$.fragment,o),b(Ps.$$.fragment,o),b(ic.$$.fragment,o),b(lc.$$.fragment,o),b(dc.$$.fragment,o),b(Ss.$$.fragment,o),b(cc.$$.fragment,o),b(Us.$$.fragment,o),b(mc.$$.fragment,o),b(gc.$$.fragment,o),b(pc.$$.fragment,o),b(uc.$$.fragment,o),b(Is.$$.fragment,o),b(hc.$$.fragment,o),b(Ns.$$.fragment,o),b(bc.$$.fragment,o),b(_c.$$.fragment,o),b(Mc.$$.fragment,o),b(Xs.$$.fragment,o),b(Tc.$$.fragment,o),b(qs.$$.fragment,o),b(yc.$$.fragment,o),b(Fc.$$.fragment,o),b(vc.$$.fragment,o),b(Qs.$$.fragment,o),b(Cc.$$.fragment,o),b(Hs.$$.fragment,o),b(wc.$$.fragment,o),b(xc.$$.fragment,o),b($c.$$.fragment,o),b(kc.$$.fragment,o),b(Ds.$$.fragment,o),b(Zc.$$.fragment,o),b(Ys.$$.fragment,o),b(Lc.$$.fragment,o),b(Bc.$$.fragment,o),b(Ac.$$.fragment,o),b(zs.$$.fragment,o),b(Rc.$$.fragment,o),b(Os.$$.fragment,o),b(Wc.$$.fragment,o),b(Jc.$$.fragment,o),b(Vc.$$.fragment,o),b(Ks.$$.fragment,o),b(Gc.$$.fragment,o),b(ei.$$.fragment,o),b(Ec.$$.fragment,o),b(Pc.$$.fragment,o),b(Sc.$$.fragment,o),b(oi.$$.fragment,o),b(Uc.$$.fragment,o),b(ti.$$.fragment,o),b(Ic.$$.fragment,o),b(Nc.$$.fragment,o),b(Xc.$$.fragment,o),b(ni.$$.fragment,o),b(qc.$$.fragment,o),b(ri.$$.fragment,o),b(Qc.$$.fragment,o),b(Hc.$$.fragment,o),b(Dc.$$.fragment,o),b(ai.$$.fragment,o),b(Yc.$$.fragment,o),b(si.$$.fragment,o),b(zc.$$.fragment,o),b(Kc.$$.fragment,o),b(em.$$.fragment,o),b(tm.$$.fragment,o),b(nm.$$.fragment,o),b(rm.$$.fragment,o),b(am.$$.fragment,o),b(ii.$$.fragment,o),b(sm.$$.fragment,o),b(li.$$.fragment,o),b(im.$$.fragment,o),b(lm.$$.fragment,o),b(dm.$$.fragment,o),b(di.$$.fragment,o),b(cm.$$.fragment,o),b(ci.$$.fragment,o),b(mm.$$.fragment,o),b(fm.$$.fragment,o),b(gm.$$.fragment,o),b(mi.$$.fragment,o),b(pm.$$.fragment,o),b(fi.$$.fragment,o),b(um.$$.fragment,o),b(hm.$$.fragment,o),b(bm.$$.fragment,o),b(gi.$$.fragment,o),b(_m.$$.fragment,o),b(pi.$$.fragment,o),b(Mm.$$.fragment,o),b(Tm.$$.fragment,o),b(ym.$$.fragment,o),b(ui.$$.fragment,o),b(Fm.$$.fragment,o),b(hi.$$.fragment,o),b(vm.$$.fragment,o),b(Cm.$$.fragment,o),b(wm.$$.fragment,o),b(bi.$$.fragment,o),b(jm.$$.fragment,o),b(_i.$$.fragment,o),b(xm.$$.fragment,o),b($m.$$.fragment,o),b(km.$$.fragment,o),b(Mi.$$.fragment,o),b(Zm.$$.fragment,o),b(Ti.$$.fragment,o),b(Lm.$$.fragment,o),b(Bm.$$.fragment,o),b(Am.$$.fragment,o),b(yi.$$.fragment,o),b(Rm.$$.fragment,o),b(Fi.$$.fragment,o),b(Wm.$$.fragment,o),b(Jm.$$.fragment,o),b(Vm.$$.fragment,o),b(vi.$$.fragment,o),b(Gm.$$.fragment,o),b(Ci.$$.fragment,o),b(Em.$$.fragment,o),b(Pm.$$.fragment,o),b(Sm.$$.fragment,o),b(wi.$$.fragment,o),b(Um.$$.fragment,o),b(ji.$$.fragment,o),b(Im.$$.fragment,o),b(Nm.$$.fragment,o),b(Xm.$$.fragment,o),b(xi.$$.fragment,o),b(qm.$$.fragment,o),b($i.$$.fragment,o),b(Qm.$$.fragment,o),b(Hm.$$.fragment,o),b(Dm.$$.fragment,o),b(ki.$$.fragment,o),b(Ym.$$.fragment,o),b(Zi.$$.fragment,o),b(zm.$$.fragment,o),b(Om.$$.fragment,o),b(Km.$$.fragment,o),b(Li.$$.fragment,o),b(ef.$$.fragment,o),b(Bi.$$.fragment,o),b(of.$$.fragment,o),b(tf.$$.fragment,o),b(nf.$$.fragment,o),b(Ai.$$.fragment,o),b(rf.$$.fragment,o),b(Ri.$$.fragment,o),b(af.$$.fragment,o),b(sf.$$.fragment,o),b(lf.$$.fragment,o),b(Wi.$$.fragment,o),b(df.$$.fragment,o),b(Ji.$$.fragment,o),b(cf.$$.fragment,o),b(mf.$$.fragment,o),b(ff.$$.fragment,o),b(Vi.$$.fragment,o),b(gf.$$.fragment,o),b(Gi.$$.fragment,o),b(pf.$$.fragment,o),b(uf.$$.fragment,o),b(hf.$$.fragment,o),b(Ei.$$.fragment,o),b(bf.$$.fragment,o),b(Pi.$$.fragment,o),b(_f.$$.fragment,o),b(Mf.$$.fragment,o),b(Tf.$$.fragment,o),b(Si.$$.fragment,o),b(yf.$$.fragment,o),b(Ui.$$.fragment,o),b(Ff.$$.fragment,o),b(Cf.$$.fragment,o),b(wf.$$.fragment,o),b(xf.$$.fragment,o),b($f.$$.fragment,o),b(Zf.$$.fragment,o),b(Lf.$$.fragment,o),b(Bf.$$.fragment,o),b(Ii.$$.fragment,o),b(Af.$$.fragment,o),b(Ni.$$.fragment,o),b(Rf.$$.fragment,o),b(Wf.$$.fragment,o),b(Jf.$$.fragment,o),b(Xi.$$.fragment,o),b(Vf.$$.fragment,o),b(qi.$$.fragment,o),b(Gf.$$.fragment,o),b(Ef.$$.fragment,o),b(Pf.$$.fragment,o),b(Qi.$$.fragment,o),b(Sf.$$.fragment,o),b(Hi.$$.fragment,o),b(Uf.$$.fragment,o),b(If.$$.fragment,o),b(Nf.$$.fragment,o),b(Di.$$.fragment,o),b(Xf.$$.fragment,o),b(Yi.$$.fragment,o),b(qf.$$.fragment,o),b(Qf.$$.fragment,o),b(Hf.$$.fragment,o),b(zi.$$.fragment,o),b(Df.$$.fragment,o),b(Oi.$$.fragment,o),b(Yf.$$.fragment,o),b(zf.$$.fragment,o),b(Of.$$.fragment,o),b(Ki.$$.fragment,o),b(Kf.$$.fragment,o),b(el.$$.fragment,o),b(eg.$$.fragment,o),b(og.$$.fragment,o),b(tg.$$.fragment,o),b(ol.$$.fragment,o),b(ng.$$.fragment,o),b(tl.$$.fragment,o),b(rg.$$.fragment,o),b(ag.$$.fragment,o),b(sg.$$.fragment,o),b(nl.$$.fragment,o),b(ig.$$.fragment,o),b(rl.$$.fragment,o),b(lg.$$.fragment,o),b(dg.$$.fragment,o),b(cg.$$.fragment,o),b(al.$$.fragment,o),b(mg.$$.fragment,o),b(sl.$$.fragment,o),b(fg.$$.fragment,o),b(pg.$$.fragment,o),b(ug.$$.fragment,o),b(hg.$$.fragment,o),b(bg.$$.fragment,o),b(il.$$.fragment,o),b(_g.$$.fragment,o),b(ll.$$.fragment,o),b(Mg.$$.fragment,o),b(Tg.$$.fragment,o),b(yg.$$.fragment,o),b(dl.$$.fragment,o),b(Fg.$$.fragment,o),b(cl.$$.fragment,o),b(vg.$$.fragment,o),b(Cg.$$.fragment,o),b(wg.$$.fragment,o),b(ml.$$.fragment,o),b(jg.$$.fragment,o),b(fl.$$.fragment,o),b(xg.$$.fragment,o),b($g.$$.fragment,o),b(kg.$$.fragment,o),b(gl.$$.fragment,o),b(Zg.$$.fragment,o),b(pl.$$.fragment,o),b(Lg.$$.fragment,o),b(Bg.$$.fragment,o),b(Ag.$$.fragment,o),b(ul.$$.fragment,o),b(Rg.$$.fragment,o),b(hl.$$.fragment,o),b(Wg.$$.fragment,o),b(Jg.$$.fragment,o),b(Vg.$$.fragment,o),b(bl.$$.fragment,o),b(Gg.$$.fragment,o),b(_l.$$.fragment,o),b(Eg.$$.fragment,o),b(Pg.$$.fragment,o),b(Sg.$$.fragment,o),b(Ml.$$.fragment,o),b(Ug.$$.fragment,o),b(Tl.$$.fragment,o),b(Ig.$$.fragment,o),b(Xg.$$.fragment,o),b(qg.$$.fragment,o),b(Qg.$$.fragment,o),b(yl.$$.fragment,o),b(Hg.$$.fragment,o),b(Fl.$$.fragment,o),b(Dg.$$.fragment,o),b(Yg.$$.fragment,o),b(zg.$$.fragment,o),b(vl.$$.fragment,o),b(Og.$$.fragment,o),b(Cl.$$.fragment,o),b(Kg.$$.fragment,o),b(ep.$$.fragment,o),b(op.$$.fragment,o),b(wl.$$.fragment,o),b(tp.$$.fragment,o),b(jl.$$.fragment,o),b(np.$$.fragment,o),b(rp.$$.fragment,o),b(ap.$$.fragment,o),b(xl.$$.fragment,o),b(sp.$$.fragment,o),b($l.$$.fragment,o),b(ip.$$.fragment,o),b(lp.$$.fragment,o),b(dp.$$.fragment,o),b(kl.$$.fragment,o),b(cp.$$.fragment,o),b(Zl.$$.fragment,o),b(mp.$$.fragment,o),b(fp.$$.fragment,o),b(gp.$$.fragment,o),b(Ll.$$.fragment,o),b(pp.$$.fragment,o),b(Bl.$$.fragment,o),b(up.$$.fragment,o),b(hp.$$.fragment,o),b(bp.$$.fragment,o),b(Al.$$.fragment,o),b(_p.$$.fragment,o),b(Rl.$$.fragment,o),b(Mp.$$.fragment,o),b(Tp.$$.fragment,o),b(yp.$$.fragment,o),b(Wl.$$.fragment,o),b(Fp.$$.fragment,o),b(Jl.$$.fragment,o),b(vp.$$.fragment,o),b(wp.$$.fragment,o),b(jp.$$.fragment,o),b($p.$$.fragment,o),b(kp.$$.fragment,o),b(Lp.$$.fragment,o),b(Bp.$$.fragment,o),b(Ap.$$.fragment,o),b(Vl.$$.fragment,o),b(Rp.$$.fragment,o),b(Gl.$$.fragment,o),b(Wp.$$.fragment,o),b(Jp.$$.fragment,o),b(Vp.$$.fragment,o),b(El.$$.fragment,o),b(Gp.$$.fragment,o),b(Pl.$$.fragment,o),b(Ep.$$.fragment,o),b(Pp.$$.fragment,o),b(Sp.$$.fragment,o),b(Sl.$$.fragment,o),b(Up.$$.fragment,o),b(Ul.$$.fragment,o),b(Ip.$$.fragment,o),b(Np.$$.fragment,o),b(Xp.$$.fragment,o),b(Il.$$.fragment,o),b(qp.$$.fragment,o),b(Nl.$$.fragment,o),b(Qp.$$.fragment,o),b(Hp.$$.fragment,o),b(Dp.$$.fragment,o),b(Xl.$$.fragment,o),b(Yp.$$.fragment,o),b(ql.$$.fragment,o),b(zp.$$.fragment,o),b(Op.$$.fragment,o),b(Kp.$$.fragment,o),b(Ql.$$.fragment,o),b(eu.$$.fragment,o),b(Hl.$$.fragment,o),b(ou.$$.fragment,o),b(tu.$$.fragment,o),b(nu.$$.fragment,o),b(Dl.$$.fragment,o),b(ru.$$.fragment,o),b(Yl.$$.fragment,o),b(au.$$.fragment,o),b(su.$$.fragment,o),b(iu.$$.fragment,o),b(zl.$$.fragment,o),b(lu.$$.fragment,o),b(Ol.$$.fragment,o),Wj=!0)},o(o){_(m.$$.fragment,o),_(Cd.$$.fragment,o),_(xd.$$.fragment,o),_(kd.$$.fragment,o),_(vs.$$.fragment,o),_(Ld.$$.fragment,o),_(Bd.$$.fragment,o),_(Ad.$$.fragment,o),_(Cs.$$.fragment,o),_(Rd.$$.fragment,o),_(Wd.$$.fragment,o),_(Jd.$$.fragment,o),_(Vd.$$.fragment,o),_(js.$$.fragment,o),_(Gd.$$.fragment,o),_(Ed.$$.fragment,o),_(Pd.$$.fragment,o),_(Sd.$$.fragment,o),_($s.$$.fragment,o),_(ks.$$.fragment,o),_(Ud.$$.fragment,o),_(Id.$$.fragment,o),_(Nd.$$.fragment,o),_(Xd.$$.fragment,o),_(Ls.$$.fragment,o),_(Bs.$$.fragment,o),_(qd.$$.fragment,o),_(Qd.$$.fragment,o),_(Hd.$$.fragment,o),_(Dd.$$.fragment,o),_(Rs.$$.fragment,o),_(Ws.$$.fragment,o),_(Yd.$$.fragment,o),_(zd.$$.fragment,o),_(Kd.$$.fragment,o),_(ec.$$.fragment,o),_(oc.$$.fragment,o),_(Vs.$$.fragment,o),_(tc.$$.fragment,o),_(Gs.$$.fragment,o),_(nc.$$.fragment,o),_(rc.$$.fragment,o),_(ac.$$.fragment,o),_(Es.$$.fragment,o),_(sc.$$.fragment,o),_(Ps.$$.fragment,o),_(ic.$$.fragment,o),_(lc.$$.fragment,o),_(dc.$$.fragment,o),_(Ss.$$.fragment,o),_(cc.$$.fragment,o),_(Us.$$.fragment,o),_(mc.$$.fragment,o),_(gc.$$.fragment,o),_(pc.$$.fragment,o),_(uc.$$.fragment,o),_(Is.$$.fragment,o),_(hc.$$.fragment,o),_(Ns.$$.fragment,o),_(bc.$$.fragment,o),_(_c.$$.fragment,o),_(Mc.$$.fragment,o),_(Xs.$$.fragment,o),_(Tc.$$.fragment,o),_(qs.$$.fragment,o),_(yc.$$.fragment,o),_(Fc.$$.fragment,o),_(vc.$$.fragment,o),_(Qs.$$.fragment,o),_(Cc.$$.fragment,o),_(Hs.$$.fragment,o),_(wc.$$.fragment,o),_(xc.$$.fragment,o),_($c.$$.fragment,o),_(kc.$$.fragment,o),_(Ds.$$.fragment,o),_(Zc.$$.fragment,o),_(Ys.$$.fragment,o),_(Lc.$$.fragment,o),_(Bc.$$.fragment,o),_(Ac.$$.fragment,o),_(zs.$$.fragment,o),_(Rc.$$.fragment,o),_(Os.$$.fragment,o),_(Wc.$$.fragment,o),_(Jc.$$.fragment,o),_(Vc.$$.fragment,o),_(Ks.$$.fragment,o),_(Gc.$$.fragment,o),_(ei.$$.fragment,o),_(Ec.$$.fragment,o),_(Pc.$$.fragment,o),_(Sc.$$.fragment,o),_(oi.$$.fragment,o),_(Uc.$$.fragment,o),_(ti.$$.fragment,o),_(Ic.$$.fragment,o),_(Nc.$$.fragment,o),_(Xc.$$.fragment,o),_(ni.$$.fragment,o),_(qc.$$.fragment,o),_(ri.$$.fragment,o),_(Qc.$$.fragment,o),_(Hc.$$.fragment,o),_(Dc.$$.fragment,o),_(ai.$$.fragment,o),_(Yc.$$.fragment,o),_(si.$$.fragment,o),_(zc.$$.fragment,o),_(Kc.$$.fragment,o),_(em.$$.fragment,o),_(tm.$$.fragment,o),_(nm.$$.fragment,o),_(rm.$$.fragment,o),_(am.$$.fragment,o),_(ii.$$.fragment,o),_(sm.$$.fragment,o),_(li.$$.fragment,o),_(im.$$.fragment,o),_(lm.$$.fragment,o),_(dm.$$.fragment,o),_(di.$$.fragment,o),_(cm.$$.fragment,o),_(ci.$$.fragment,o),_(mm.$$.fragment,o),_(fm.$$.fragment,o),_(gm.$$.fragment,o),_(mi.$$.fragment,o),_(pm.$$.fragment,o),_(fi.$$.fragment,o),_(um.$$.fragment,o),_(hm.$$.fragment,o),_(bm.$$.fragment,o),_(gi.$$.fragment,o),_(_m.$$.fragment,o),_(pi.$$.fragment,o),_(Mm.$$.fragment,o),_(Tm.$$.fragment,o),_(ym.$$.fragment,o),_(ui.$$.fragment,o),_(Fm.$$.fragment,o),_(hi.$$.fragment,o),_(vm.$$.fragment,o),_(Cm.$$.fragment,o),_(wm.$$.fragment,o),_(bi.$$.fragment,o),_(jm.$$.fragment,o),_(_i.$$.fragment,o),_(xm.$$.fragment,o),_($m.$$.fragment,o),_(km.$$.fragment,o),_(Mi.$$.fragment,o),_(Zm.$$.fragment,o),_(Ti.$$.fragment,o),_(Lm.$$.fragment,o),_(Bm.$$.fragment,o),_(Am.$$.fragment,o),_(yi.$$.fragment,o),_(Rm.$$.fragment,o),_(Fi.$$.fragment,o),_(Wm.$$.fragment,o),_(Jm.$$.fragment,o),_(Vm.$$.fragment,o),_(vi.$$.fragment,o),_(Gm.$$.fragment,o),_(Ci.$$.fragment,o),_(Em.$$.fragment,o),_(Pm.$$.fragment,o),_(Sm.$$.fragment,o),_(wi.$$.fragment,o),_(Um.$$.fragment,o),_(ji.$$.fragment,o),_(Im.$$.fragment,o),_(Nm.$$.fragment,o),_(Xm.$$.fragment,o),_(xi.$$.fragment,o),_(qm.$$.fragment,o),_($i.$$.fragment,o),_(Qm.$$.fragment,o),_(Hm.$$.fragment,o),_(Dm.$$.fragment,o),_(ki.$$.fragment,o),_(Ym.$$.fragment,o),_(Zi.$$.fragment,o),_(zm.$$.fragment,o),_(Om.$$.fragment,o),_(Km.$$.fragment,o),_(Li.$$.fragment,o),_(ef.$$.fragment,o),_(Bi.$$.fragment,o),_(of.$$.fragment,o),_(tf.$$.fragment,o),_(nf.$$.fragment,o),_(Ai.$$.fragment,o),_(rf.$$.fragment,o),_(Ri.$$.fragment,o),_(af.$$.fragment,o),_(sf.$$.fragment,o),_(lf.$$.fragment,o),_(Wi.$$.fragment,o),_(df.$$.fragment,o),_(Ji.$$.fragment,o),_(cf.$$.fragment,o),_(mf.$$.fragment,o),_(ff.$$.fragment,o),_(Vi.$$.fragment,o),_(gf.$$.fragment,o),_(Gi.$$.fragment,o),_(pf.$$.fragment,o),_(uf.$$.fragment,o),_(hf.$$.fragment,o),_(Ei.$$.fragment,o),_(bf.$$.fragment,o),_(Pi.$$.fragment,o),_(_f.$$.fragment,o),_(Mf.$$.fragment,o),_(Tf.$$.fragment,o),_(Si.$$.fragment,o),_(yf.$$.fragment,o),_(Ui.$$.fragment,o),_(Ff.$$.fragment,o),_(Cf.$$.fragment,o),_(wf.$$.fragment,o),_(xf.$$.fragment,o),_($f.$$.fragment,o),_(Zf.$$.fragment,o),_(Lf.$$.fragment,o),_(Bf.$$.fragment,o),_(Ii.$$.fragment,o),_(Af.$$.fragment,o),_(Ni.$$.fragment,o),_(Rf.$$.fragment,o),_(Wf.$$.fragment,o),_(Jf.$$.fragment,o),_(Xi.$$.fragment,o),_(Vf.$$.fragment,o),_(qi.$$.fragment,o),_(Gf.$$.fragment,o),_(Ef.$$.fragment,o),_(Pf.$$.fragment,o),_(Qi.$$.fragment,o),_(Sf.$$.fragment,o),_(Hi.$$.fragment,o),_(Uf.$$.fragment,o),_(If.$$.fragment,o),_(Nf.$$.fragment,o),_(Di.$$.fragment,o),_(Xf.$$.fragment,o),_(Yi.$$.fragment,o),_(qf.$$.fragment,o),_(Qf.$$.fragment,o),_(Hf.$$.fragment,o),_(zi.$$.fragment,o),_(Df.$$.fragment,o),_(Oi.$$.fragment,o),_(Yf.$$.fragment,o),_(zf.$$.fragment,o),_(Of.$$.fragment,o),_(Ki.$$.fragment,o),_(Kf.$$.fragment,o),_(el.$$.fragment,o),_(eg.$$.fragment,o),_(og.$$.fragment,o),_(tg.$$.fragment,o),_(ol.$$.fragment,o),_(ng.$$.fragment,o),_(tl.$$.fragment,o),_(rg.$$.fragment,o),_(ag.$$.fragment,o),_(sg.$$.fragment,o),_(nl.$$.fragment,o),_(ig.$$.fragment,o),_(rl.$$.fragment,o),_(lg.$$.fragment,o),_(dg.$$.fragment,o),_(cg.$$.fragment,o),_(al.$$.fragment,o),_(mg.$$.fragment,o),_(sl.$$.fragment,o),_(fg.$$.fragment,o),_(pg.$$.fragment,o),_(ug.$$.fragment,o),_(hg.$$.fragment,o),_(bg.$$.fragment,o),_(il.$$.fragment,o),_(_g.$$.fragment,o),_(ll.$$.fragment,o),_(Mg.$$.fragment,o),_(Tg.$$.fragment,o),_(yg.$$.fragment,o),_(dl.$$.fragment,o),_(Fg.$$.fragment,o),_(cl.$$.fragment,o),_(vg.$$.fragment,o),_(Cg.$$.fragment,o),_(wg.$$.fragment,o),_(ml.$$.fragment,o),_(jg.$$.fragment,o),_(fl.$$.fragment,o),_(xg.$$.fragment,o),_($g.$$.fragment,o),_(kg.$$.fragment,o),_(gl.$$.fragment,o),_(Zg.$$.fragment,o),_(pl.$$.fragment,o),_(Lg.$$.fragment,o),_(Bg.$$.fragment,o),_(Ag.$$.fragment,o),_(ul.$$.fragment,o),_(Rg.$$.fragment,o),_(hl.$$.fragment,o),_(Wg.$$.fragment,o),_(Jg.$$.fragment,o),_(Vg.$$.fragment,o),_(bl.$$.fragment,o),_(Gg.$$.fragment,o),_(_l.$$.fragment,o),_(Eg.$$.fragment,o),_(Pg.$$.fragment,o),_(Sg.$$.fragment,o),_(Ml.$$.fragment,o),_(Ug.$$.fragment,o),_(Tl.$$.fragment,o),_(Ig.$$.fragment,o),_(Xg.$$.fragment,o),_(qg.$$.fragment,o),_(Qg.$$.fragment,o),_(yl.$$.fragment,o),_(Hg.$$.fragment,o),_(Fl.$$.fragment,o),_(Dg.$$.fragment,o),_(Yg.$$.fragment,o),_(zg.$$.fragment,o),_(vl.$$.fragment,o),_(Og.$$.fragment,o),_(Cl.$$.fragment,o),_(Kg.$$.fragment,o),_(ep.$$.fragment,o),_(op.$$.fragment,o),_(wl.$$.fragment,o),_(tp.$$.fragment,o),_(jl.$$.fragment,o),_(np.$$.fragment,o),_(rp.$$.fragment,o),_(ap.$$.fragment,o),_(xl.$$.fragment,o),_(sp.$$.fragment,o),_($l.$$.fragment,o),_(ip.$$.fragment,o),_(lp.$$.fragment,o),_(dp.$$.fragment,o),_(kl.$$.fragment,o),_(cp.$$.fragment,o),_(Zl.$$.fragment,o),_(mp.$$.fragment,o),_(fp.$$.fragment,o),_(gp.$$.fragment,o),_(Ll.$$.fragment,o),_(pp.$$.fragment,o),_(Bl.$$.fragment,o),_(up.$$.fragment,o),_(hp.$$.fragment,o),_(bp.$$.fragment,o),_(Al.$$.fragment,o),_(_p.$$.fragment,o),_(Rl.$$.fragment,o),_(Mp.$$.fragment,o),_(Tp.$$.fragment,o),_(yp.$$.fragment,o),_(Wl.$$.fragment,o),_(Fp.$$.fragment,o),_(Jl.$$.fragment,o),_(vp.$$.fragment,o),_(wp.$$.fragment,o),_(jp.$$.fragment,o),_($p.$$.fragment,o),_(kp.$$.fragment,o),_(Lp.$$.fragment,o),_(Bp.$$.fragment,o),_(Ap.$$.fragment,o),_(Vl.$$.fragment,o),_(Rp.$$.fragment,o),_(Gl.$$.fragment,o),_(Wp.$$.fragment,o),_(Jp.$$.fragment,o),_(Vp.$$.fragment,o),_(El.$$.fragment,o),_(Gp.$$.fragment,o),_(Pl.$$.fragment,o),_(Ep.$$.fragment,o),_(Pp.$$.fragment,o),_(Sp.$$.fragment,o),_(Sl.$$.fragment,o),_(Up.$$.fragment,o),_(Ul.$$.fragment,o),_(Ip.$$.fragment,o),_(Np.$$.fragment,o),_(Xp.$$.fragment,o),_(Il.$$.fragment,o),_(qp.$$.fragment,o),_(Nl.$$.fragment,o),_(Qp.$$.fragment,o),_(Hp.$$.fragment,o),_(Dp.$$.fragment,o),_(Xl.$$.fragment,o),_(Yp.$$.fragment,o),_(ql.$$.fragment,o),_(zp.$$.fragment,o),_(Op.$$.fragment,o),_(Kp.$$.fragment,o),_(Ql.$$.fragment,o),_(eu.$$.fragment,o),_(Hl.$$.fragment,o),_(ou.$$.fragment,o),_(tu.$$.fragment,o),_(nu.$$.fragment,o),_(Dl.$$.fragment,o),_(ru.$$.fragment,o),_(Yl.$$.fragment,o),_(au.$$.fragment,o),_(su.$$.fragment,o),_(iu.$$.fragment,o),_(zl.$$.fragment,o),_(lu.$$.fragment,o),_(Ol.$$.fragment,o),Wj=!1},d(o){o&&(d(F),d(c),d(s),d(e),d(T),d(xC),d(vd),d($C),d(kC),d(wd),d(ZC),d(jd),d(LC),d(BC),d($d),d(AC),d(RC),d(Zd),d(WC),d(JC),d(VC),d(ce),d(GC),d(EC),d(me),d(PC),d(SC),d(fe),d(UC),d(IC),d(ge),d(NC),d(XC),d(pe),d(qC),d(QC),d(Od),d(HC),d(DC),d(ue),d(YC),d(zC),d(he),d(OC),d(KC),d(be),d(e2),d(o2),d(fc),d(t2),d(n2),d(_e),d(r2),d(a2),d(Me),d(s2),d(i2),d(Te),d(l2),d(d2),d(jc),d(c2),d(m2),d(ye),d(f2),d(g2),d(Fe),d(p2),d(u2),d(ve),d(h2),d(b2),d(Ce),d(_2),d(M2),d(we),d(T2),d(y2),d(je),d(F2),d(v2),d(Oc),d(C2),d(w2),d(om),d(j2),d(x2),d(xe),d($2),d(k2),d($e),d(Z2),d(L2),d(ke),d(B2),d(A2),d(Ze),d(R2),d(W2),d(Le),d(J2),d(V2),d(Be),d(G2),d(E2),d(Ae),d(P2),d(S2),d(Re),d(U2),d(I2),d(We),d(N2),d(X2),d(Je),d(q2),d(Q2),d(Ve),d(H2),d(D2),d(Ge),d(Y2),d(z2),d(Ee),d(O2),d(K2),d(Pe),d(ew),d(ow),d(Se),d(tw),d(nw),d(Ue),d(rw),d(aw),d(Ie),d(sw),d(iw),d(Ne),d(lw),d(dw),d(vf),d(cw),d(mw),d(jf),d(fw),d(gw),d(kf),d(pw),d(uw),d(Xe),d(hw),d(bw),d(qe),d(_w),d(Mw),d(Qe),d(Tw),d(yw),d(He),d(Fw),d(vw),d(De),d(Cw),d(ww),d(Ye),d(jw),d(xw),d(ze),d($w),d(kw),d(Oe),d(Zw),d(Lw),d(Ke),d(Bw),d(Aw),d(gg),d(Rw),d(Ww),d(eo),d(Jw),d(Vw),d(oo),d(Gw),d(Ew),d(to),d(Pw),d(Sw),d(no),d(Uw),d(Iw),d(ro),d(Nw),d(Xw),d(ao),d(qw),d(Qw),d(so),d(Hw),d(Dw),d(Ng),d(Yw),d(zw),d(io),d(Ow),d(Kw),d(lo),d(ej),d(oj),d(co),d(tj),d(nj),d(mo),d(rj),d(aj),d(fo),d(sj),d(ij),d(go),d(lj),d(dj),d(po),d(cj),d(mj),d(uo),d(fj),d(gj),d(Cp),d(pj),d(uj),d(xp),d(hj),d(bj),d(Zp),d(_j),d(Mj),d(ho),d(Tj),d(yj),d(bo),d(Fj),d(vj),d(_o),d(Cj),d(wj),d(Mo),d(jj),d(xj),d(To),d($j),d(kj),d(yo),d(Zj),d(Lj),d(Fo),d(Bj),d(Aj),d(vo),d(Rj),d(FC)),d(n),M(m,o),M(Cd,o),M(xd,o),M(kd,o),M(vs,o),M(Ld,o),M(Bd),M(Ad),M(Cs),M(Rd),M(Wd,o),M(Jd),M(Vd),M(js),M(Gd),M(Ed,o),M(Pd),M(Sd),M($s),M(ks),M(Ud),M(Id,o),M(Nd),M(Xd),M(Ls),M(Bs),M(qd),M(Qd,o),M(Hd),M(Dd),M(Rs),M(Ws),M(Yd),M(zd,o),M(Kd,o),M(ec),M(oc),M(Vs),M(tc),M(Gs),M(nc,o),M(rc),M(ac),M(Es),M(sc),M(Ps),M(ic,o),M(lc),M(dc),M(Ss),M(cc),M(Us),M(mc,o),M(gc,o),M(pc),M(uc),M(Is),M(hc),M(Ns),M(bc,o),M(_c),M(Mc),M(Xs),M(Tc),M(qs),M(yc,o),M(Fc),M(vc),M(Qs),M(Cc),M(Hs),M(wc,o),M(xc,o),M($c),M(kc),M(Ds),M(Zc),M(Ys),M(Lc,o),M(Bc),M(Ac),M(zs),M(Rc),M(Os),M(Wc,o),M(Jc),M(Vc),M(Ks),M(Gc),M(ei),M(Ec,o),M(Pc),M(Sc),M(oi),M(Uc),M(ti),M(Ic,o),M(Nc),M(Xc),M(ni),M(qc),M(ri),M(Qc,o),M(Hc),M(Dc),M(ai),M(Yc),M(si),M(zc,o),M(Kc),M(em,o),M(tm),M(nm,o),M(rm),M(am),M(ii),M(sm),M(li),M(im,o),M(lm),M(dm),M(di),M(cm),M(ci),M(mm,o),M(fm),M(gm),M(mi),M(pm),M(fi),M(um,o),M(hm),M(bm),M(gi),M(_m),M(pi),M(Mm,o),M(Tm),M(ym),M(ui),M(Fm),M(hi),M(vm,o),M(Cm),M(wm),M(bi),M(jm),M(_i),M(xm,o),M($m),M(km),M(Mi),M(Zm),M(Ti),M(Lm,o),M(Bm),M(Am),M(yi),M(Rm),M(Fi),M(Wm,o),M(Jm),M(Vm),M(vi),M(Gm),M(Ci),M(Em,o),M(Pm),M(Sm),M(wi),M(Um),M(ji),M(Im,o),M(Nm),M(Xm),M(xi),M(qm),M($i),M(Qm,o),M(Hm),M(Dm),M(ki),M(Ym),M(Zi),M(zm,o),M(Om),M(Km),M(Li),M(ef),M(Bi),M(of,o),M(tf),M(nf),M(Ai),M(rf),M(Ri),M(af,o),M(sf),M(lf),M(Wi),M(df),M(Ji),M(cf,o),M(mf),M(ff),M(Vi),M(gf),M(Gi),M(pf,o),M(uf),M(hf),M(Ei),M(bf),M(Pi),M(_f,o),M(Mf),M(Tf),M(Si),M(yf),M(Ui),M(Ff,o),M(Cf),M(wf,o),M(xf),M($f,o),M(Zf,o),M(Lf),M(Bf),M(Ii),M(Af),M(Ni),M(Rf,o),M(Wf),M(Jf),M(Xi),M(Vf),M(qi),M(Gf,o),M(Ef),M(Pf),M(Qi),M(Sf),M(Hi),M(Uf,o),M(If),M(Nf),M(Di),M(Xf),M(Yi),M(qf,o),M(Qf),M(Hf),M(zi),M(Df),M(Oi),M(Yf,o),M(zf),M(Of),M(Ki),M(Kf),M(el),M(eg,o),M(og),M(tg),M(ol),M(ng),M(tl),M(rg,o),M(ag),M(sg),M(nl),M(ig),M(rl),M(lg,o),M(dg),M(cg),M(al),M(mg),M(sl),M(fg,o),M(pg),M(ug,o),M(hg),M(bg),M(il),M(_g),M(ll),M(Mg,o),M(Tg),M(yg),M(dl),M(Fg),M(cl),M(vg,o),M(Cg),M(wg),M(ml),M(jg),M(fl),M(xg,o),M($g),M(kg),M(gl),M(Zg),M(pl),M(Lg,o),M(Bg),M(Ag),M(ul),M(Rg),M(hl),M(Wg,o),M(Jg),M(Vg),M(bl),M(Gg),M(_l),M(Eg,o),M(Pg),M(Sg),M(Ml),M(Ug),M(Tl),M(Ig,o),M(Xg,o),M(qg),M(Qg),M(yl),M(Hg),M(Fl),M(Dg,o),M(Yg),M(zg),M(vl),M(Og),M(Cl),M(Kg,o),M(ep),M(op),M(wl),M(tp),M(jl),M(np,o),M(rp),M(ap),M(xl),M(sp),M($l),M(ip,o),M(lp),M(dp),M(kl),M(cp),M(Zl),M(mp,o),M(fp),M(gp),M(Ll),M(pp),M(Bl),M(up,o),M(hp),M(bp),M(Al),M(_p),M(Rl),M(Mp,o),M(Tp),M(yp),M(Wl),M(Fp),M(Jl),M(vp,o),M(wp),M(jp,o),M($p),M(kp,o),M(Lp,o),M(Bp),M(Ap),M(Vl),M(Rp),M(Gl),M(Wp,o),M(Jp),M(Vp),M(El),M(Gp),M(Pl),M(Ep,o),M(Pp),M(Sp),M(Sl),M(Up),M(Ul),M(Ip,o),M(Np),M(Xp),M(Il),M(qp),M(Nl),M(Qp,o),M(Hp),M(Dp),M(Xl),M(Yp),M(ql),M(zp,o),M(Op),M(Kp),M(Ql),M(eu),M(Hl),M(ou,o),M(tu),M(nu),M(Dl),M(ru),M(Yl),M(au,o),M(su),M(iu),M(zl),M(lu),M(Ol)}}}const Sq='{"title":"Auto Classes","local":"auto-classes","sections":[{"title":"自動クラスの拡張","local":"自動クラスの拡張","sections":[],"depth":2},{"title":"AutoConfig","local":"transformers.AutoConfig","sections":[],"depth":2},{"title":"AutoTokenizer","local":"transformers.AutoTokenizer","sections":[],"depth":2},{"title":"AutoFeatureExtractor","local":"transformers.AutoFeatureExtractor","sections":[],"depth":2},{"title":"AutoImageProcessor","local":"transformers.AutoImageProcessor","sections":[],"depth":2},{"title":"AutoProcessor","local":"transformers.AutoProcessor","sections":[],"depth":2},{"title":"Generic model classes","local":"generic-model-classes","sections":[{"title":"AutoModel","local":"transformers.AutoModel","sections":[],"depth":3},{"title":"TFAutoModel","local":"transformers.TFAutoModel","sections":[],"depth":3},{"title":"FlaxAutoModel","local":"transformers.FlaxAutoModel","sections":[],"depth":3}],"depth":2},{"title":"Generic pretraining classes","local":"generic-pretraining-classes","sections":[{"title":"AutoModelForPreTraining","local":"transformers.AutoModelForPreTraining","sections":[],"depth":3},{"title":"TFAutoModelForPreTraining","local":"transformers.TFAutoModelForPreTraining","sections":[],"depth":3},{"title":"FlaxAutoModelForPreTraining","local":"transformers.FlaxAutoModelForPreTraining","sections":[],"depth":3}],"depth":2},{"title":"Natural Language Processing","local":"natural-language-processing","sections":[{"title":"AutoModelForCausalLM","local":"transformers.AutoModelForCausalLM","sections":[],"depth":3},{"title":"TFAutoModelForCausalLM","local":"transformers.TFAutoModelForCausalLM","sections":[],"depth":3},{"title":"FlaxAutoModelForCausalLM","local":"transformers.FlaxAutoModelForCausalLM","sections":[],"depth":3},{"title":"AutoModelForMaskedLM","local":"transformers.AutoModelForMaskedLM","sections":[],"depth":3},{"title":"TFAutoModelForMaskedLM","local":"transformers.TFAutoModelForMaskedLM","sections":[],"depth":3},{"title":"FlaxAutoModelForMaskedLM","local":"transformers.FlaxAutoModelForMaskedLM","sections":[],"depth":3},{"title":"AutoModelForMaskGeneration","local":"transformers.AutoModelForMaskGeneration","sections":[],"depth":3},{"title":"TFAutoModelForMaskGeneration","local":"transformers.TFAutoModelForMaskGeneration","sections":[],"depth":3},{"title":"AutoModelForSeq2SeqLM","local":"transformers.AutoModelForSeq2SeqLM","sections":[],"depth":3},{"title":"TFAutoModelForSeq2SeqLM","local":"transformers.TFAutoModelForSeq2SeqLM","sections":[],"depth":3},{"title":"FlaxAutoModelForSeq2SeqLM","local":"transformers.FlaxAutoModelForSeq2SeqLM","sections":[],"depth":3},{"title":"AutoModelForSequenceClassification","local":"transformers.AutoModelForSequenceClassification","sections":[],"depth":3},{"title":"TFAutoModelForSequenceClassification","local":"transformers.TFAutoModelForSequenceClassification","sections":[],"depth":3},{"title":"FlaxAutoModelForSequenceClassification","local":"transformers.FlaxAutoModelForSequenceClassification","sections":[],"depth":3},{"title":"AutoModelForMultipleChoice","local":"transformers.AutoModelForMultipleChoice","sections":[],"depth":3},{"title":"TFAutoModelForMultipleChoice","local":"transformers.TFAutoModelForMultipleChoice","sections":[],"depth":3},{"title":"FlaxAutoModelForMultipleChoice","local":"transformers.FlaxAutoModelForMultipleChoice","sections":[],"depth":3},{"title":"AutoModelForNextSentencePrediction","local":"transformers.AutoModelForNextSentencePrediction","sections":[],"depth":3},{"title":"TFAutoModelForNextSentencePrediction","local":"transformers.TFAutoModelForNextSentencePrediction","sections":[],"depth":3},{"title":"FlaxAutoModelForNextSentencePrediction","local":"transformers.FlaxAutoModelForNextSentencePrediction","sections":[],"depth":3},{"title":"AutoModelForTokenClassification","local":"transformers.AutoModelForTokenClassification","sections":[],"depth":3},{"title":"TFAutoModelForTokenClassification","local":"transformers.TFAutoModelForTokenClassification","sections":[],"depth":3},{"title":"FlaxAutoModelForTokenClassification","local":"transformers.FlaxAutoModelForTokenClassification","sections":[],"depth":3},{"title":"AutoModelForQuestionAnswering","local":"transformers.AutoModelForQuestionAnswering","sections":[],"depth":3},{"title":"TFAutoModelForQuestionAnswering","local":"transformers.TFAutoModelForQuestionAnswering","sections":[],"depth":3},{"title":"FlaxAutoModelForQuestionAnswering","local":"transformers.FlaxAutoModelForQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForTextEncoding","local":"transformers.AutoModelForTextEncoding","sections":[],"depth":3},{"title":"TFAutoModelForTextEncoding","local":"transformers.TFAutoModelForTextEncoding","sections":[],"depth":3}],"depth":2},{"title":"Computer vision","local":"computer-vision","sections":[{"title":"AutoModelForDepthEstimation","local":"transformers.AutoModelForDepthEstimation","sections":[],"depth":3},{"title":"AutoModelForImageClassification","local":"transformers.AutoModelForImageClassification","sections":[],"depth":3},{"title":"TFAutoModelForImageClassification","local":"transformers.TFAutoModelForImageClassification","sections":[],"depth":3},{"title":"FlaxAutoModelForImageClassification","local":"transformers.FlaxAutoModelForImageClassification","sections":[],"depth":3},{"title":"AutoModelForVideoClassification","local":"transformers.AutoModelForVideoClassification","sections":[],"depth":3},{"title":"AutoModelForMaskedImageModeling","local":"transformers.AutoModelForMaskedImageModeling","sections":[],"depth":3},{"title":"TFAutoModelForMaskedImageModeling","local":"transformers.TFAutoModelForMaskedImageModeling","sections":[],"depth":3},{"title":"AutoModelForObjectDetection","local":"transformers.AutoModelForObjectDetection","sections":[],"depth":3},{"title":"AutoModelForImageSegmentation","local":"transformers.AutoModelForImageSegmentation","sections":[],"depth":3},{"title":"AutoModelForImageToImage","local":"transformers.AutoModelForImageToImage","sections":[],"depth":3},{"title":"AutoModelForSemanticSegmentation","local":"transformers.AutoModelForSemanticSegmentation","sections":[],"depth":3},{"title":"TFAutoModelForSemanticSegmentation","local":"transformers.TFAutoModelForSemanticSegmentation","sections":[],"depth":3},{"title":"AutoModelForInstanceSegmentation","local":"transformers.AutoModelForInstanceSegmentation","sections":[],"depth":3},{"title":"AutoModelForUniversalSegmentation","local":"transformers.AutoModelForUniversalSegmentation","sections":[],"depth":3},{"title":"AutoModelForZeroShotImageClassification","local":"transformers.AutoModelForZeroShotImageClassification","sections":[],"depth":3},{"title":"TFAutoModelForZeroShotImageClassification","local":"transformers.TFAutoModelForZeroShotImageClassification","sections":[],"depth":3},{"title":"AutoModelForZeroShotObjectDetection","local":"transformers.AutoModelForZeroShotObjectDetection","sections":[],"depth":3}],"depth":2},{"title":"Audio","local":"audio","sections":[{"title":"AutoModelForAudioClassification","local":"transformers.AutoModelForAudioClassification","sections":[],"depth":3},{"title":"AutoModelForAudioFrameClassification","local":"transformers.TFAutoModelForAudioClassification","sections":[],"depth":3},{"title":"TFAutoModelForAudioFrameClassification","local":"transformers.AutoModelForAudioFrameClassification","sections":[],"depth":3},{"title":"AutoModelForCTC","local":"transformers.AutoModelForCTC","sections":[],"depth":3},{"title":"AutoModelForSpeechSeq2Seq","local":"transformers.AutoModelForSpeechSeq2Seq","sections":[],"depth":3},{"title":"TFAutoModelForSpeechSeq2Seq","local":"transformers.TFAutoModelForSpeechSeq2Seq","sections":[],"depth":3},{"title":"FlaxAutoModelForSpeechSeq2Seq","local":"transformers.FlaxAutoModelForSpeechSeq2Seq","sections":[],"depth":3},{"title":"AutoModelForAudioXVector","local":"transformers.AutoModelForAudioXVector","sections":[],"depth":3},{"title":"AutoModelForTextToSpectrogram","local":"transformers.AutoModelForTextToSpectrogram","sections":[],"depth":3},{"title":"AutoModelForTextToWaveform","local":"transformers.AutoModelForTextToWaveform","sections":[],"depth":3}],"depth":2},{"title":"Multimodal","local":"multimodal","sections":[{"title":"AutoModelForTableQuestionAnswering","local":"transformers.AutoModelForTableQuestionAnswering","sections":[],"depth":3},{"title":"TFAutoModelForTableQuestionAnswering","local":"transformers.TFAutoModelForTableQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForDocumentQuestionAnswering","local":"transformers.AutoModelForDocumentQuestionAnswering","sections":[],"depth":3},{"title":"TFAutoModelForDocumentQuestionAnswering","local":"transformers.TFAutoModelForDocumentQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForVisualQuestionAnswering","local":"transformers.AutoModelForVisualQuestionAnswering","sections":[],"depth":3},{"title":"AutoModelForVision2Seq","local":"transformers.AutoModelForVision2Seq","sections":[],"depth":3},{"title":"TFAutoModelForVision2Seq","local":"transformers.TFAutoModelForVision2Seq","sections":[],"depth":3},{"title":"FlaxAutoModelForVision2Seq","local":"transformers.FlaxAutoModelForVision2Seq","sections":[],"depth":3}],"depth":2}],"depth":1}';function Uq(v){return mX(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Yq extends fX{constructor(n){super(),gX(this,n,Uq,Pq,cX,{})}}export{Yq as component};
