import{s as ha,f as Ot,o as ua,n as Kt}from"../chunks/scheduler.9bc65507.js";import{S as fa,i as ga,g as l,s as a,r as d,A as _a,h as m,f as t,c as n,j as z,u as h,x as w,k as $,y as o,a as i,v as u,d as f,t as g,w as _}from"../chunks/index.707bf1b6.js";import{D as M}from"../chunks/Docstring.17db21ae.js";import{C as er}from"../chunks/CodeBlock.54a9f38d.js";import{E as Yt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as G}from"../chunks/Heading.342b1fa6.js";function wa(U){let c,A="Example:",x,p,v;return p=new er({props:{code:"QWRhZmFjdG9yKG1vZGVsLnBhcmFtZXRlcnMoKSUyQyUyMHNjYWxlX3BhcmFtZXRlciUzREZhbHNlJTJDJTIwcmVsYXRpdmVfc3RlcCUzREZhbHNlJTJDJTIwd2FybXVwX2luaXQlM0RGYWxzZSUyQyUyMGxyJTNEMWUtMyk=",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">False</span>, relative_step=<span class="hljs-literal">False</span>, warmup_init=<span class="hljs-literal">False</span>, lr=<span class="hljs-number">1e-3</span>)',wrap:!1}}),{c(){c=l("p"),c.textContent=A,x=a(),d(p.$$.fragment)},l(s){c=m(s,"P",{"data-svelte-h":!0}),w(c)!=="svelte-11lpom8"&&(c.textContent=A),x=n(s),h(p.$$.fragment,s)},m(s,T){i(s,c,T),i(s,x,T),u(p,s,T),v=!0},p:Kt,i(s){v||(f(p.$$.fragment,s),v=!0)},o(s){g(p.$$.fragment,s),v=!1},d(s){s&&(t(c),t(x)),_(p,s)}}}function ba(U){let c,A="Others reported the following combination to work well:",x,p,v;return p=new er({props:{code:"QWRhZmFjdG9yKG1vZGVsLnBhcmFtZXRlcnMoKSUyQyUyMHNjYWxlX3BhcmFtZXRlciUzRFRydWUlMkMlMjByZWxhdGl2ZV9zdGVwJTNEVHJ1ZSUyQyUyMHdhcm11cF9pbml0JTNEVHJ1ZSUyQyUyMGxyJTNETm9uZSk=",highlighted:'Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)',wrap:!1}}),{c(){c=l("p"),c.textContent=A,x=a(),d(p.$$.fragment)},l(s){c=m(s,"P",{"data-svelte-h":!0}),w(c)!=="svelte-mxeef1"&&(c.textContent=A),x=n(s),h(p.$$.fragment,s)},m(s,T){i(s,c,T),i(s,x,T),u(p,s,T),v=!0},p:Kt,i(s){v||(f(p.$$.fragment,s),v=!0)},o(s){g(p.$$.fragment,s),v=!1},d(s){s&&(t(c),t(x)),_(p,s)}}}function $a(U){let c,A="scheduler as following:",x,p,v;return p=new er({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5vcHRpbWl6YXRpb24lMjBpbXBvcnQlMjBBZGFmYWN0b3IlMkMlMjBBZGFmYWN0b3JTY2hlZHVsZSUwQSUwQW9wdGltaXplciUyMCUzRCUyMEFkYWZhY3Rvcihtb2RlbC5wYXJhbWV0ZXJzKCklMkMlMjBzY2FsZV9wYXJhbWV0ZXIlM0RUcnVlJTJDJTIwcmVsYXRpdmVfc3RlcCUzRFRydWUlMkMlMjB3YXJtdXBfaW5pdCUzRFRydWUlMkMlMjBsciUzRE5vbmUpJTBBbHJfc2NoZWR1bGVyJTIwJTNEJTIwQWRhZmFjdG9yU2NoZWR1bGUob3B0aW1pemVyKSUwQXRyYWluZXIlMjAlM0QlMjBUcmFpbmVyKC4uLiUyQyUyMG9wdGltaXplcnMlM0Qob3B0aW1pemVyJTJDJTIwbHJfc2NoZWR1bGVyKSk=",highlighted:`<span class="hljs-keyword">from</span> transformers.optimization <span class="hljs-keyword">import</span> Adafactor, AdafactorSchedule

optimizer = Adafactor(model.parameters(), scale_parameter=<span class="hljs-literal">True</span>, relative_step=<span class="hljs-literal">True</span>, warmup_init=<span class="hljs-literal">True</span>, lr=<span class="hljs-literal">None</span>)
lr_scheduler = AdafactorSchedule(optimizer)
trainer = Trainer(..., optimizers=(optimizer, lr_scheduler))`,wrap:!1}}),{c(){c=l("p"),c.textContent=A,x=a(),d(p.$$.fragment)},l(s){c=m(s,"P",{"data-svelte-h":!0}),w(c)!=="svelte-1eua222"&&(c.textContent=A),x=n(s),h(p.$$.fragment,s)},m(s,T){i(s,c,T),i(s,x,T),u(p,s,T),v=!0},p:Kt,i(s){v||(f(p.$$.fragment,s),v=!0)},o(s){g(p.$$.fragment,s),v=!1},d(s){s&&(t(c),t(x)),_(p,s)}}}function va(U){let c,A="Usage:",x,p,v;return p=new er({props:{code:"JTIzJTIwcmVwbGFjZSUyMEFkYW1XJTIwd2l0aCUyMEFkYWZhY3RvciUwQW9wdGltaXplciUyMCUzRCUyMEFkYWZhY3RvciglMEElMjAlMjAlMjAlMjBtb2RlbC5wYXJhbWV0ZXJzKCklMkMlMEElMjAlMjAlMjAlMjBsciUzRDFlLTMlMkMlMEElMjAlMjAlMjAlMjBlcHMlM0QoMWUtMzAlMkMlMjAxZS0zKSUyQyUwQSUyMCUyMCUyMCUyMGNsaXBfdGhyZXNob2xkJTNEMS4wJTJDJTBBJTIwJTIwJTIwJTIwZGVjYXlfcmF0ZSUzRC0wLjglMkMlMEElMjAlMjAlMjAlMjBiZXRhMSUzRE5vbmUlMkMlMEElMjAlMjAlMjAlMjB3ZWlnaHRfZGVjYXklM0QwLjAlMkMlMEElMjAlMjAlMjAlMjByZWxhdGl2ZV9zdGVwJTNERmFsc2UlMkMlMEElMjAlMjAlMjAlMjBzY2FsZV9wYXJhbWV0ZXIlM0RGYWxzZSUyQyUwQSUyMCUyMCUyMCUyMHdhcm11cF9pbml0JTNERmFsc2UlMkMlMEEp",highlighted:`<span class="hljs-comment"># replace AdamW with Adafactor</span>
optimizer = Adafactor(
    model.parameters(),
    lr=<span class="hljs-number">1e-3</span>,
    eps=(<span class="hljs-number">1e-30</span>, <span class="hljs-number">1e-3</span>),
    clip_threshold=<span class="hljs-number">1.0</span>,
    decay_rate=-<span class="hljs-number">0.8</span>,
    beta1=<span class="hljs-literal">None</span>,
    weight_decay=<span class="hljs-number">0.0</span>,
    relative_step=<span class="hljs-literal">False</span>,
    scale_parameter=<span class="hljs-literal">False</span>,
    warmup_init=<span class="hljs-literal">False</span>,
)`,wrap:!1}}),{c(){c=l("p"),c.textContent=A,x=a(),d(p.$$.fragment)},l(s){c=m(s,"P",{"data-svelte-h":!0}),w(c)!=="svelte-5wyjqd"&&(c.textContent=A),x=n(s),h(p.$$.fragment,s)},m(s,T){i(s,c,T),i(s,x,T),u(p,s,T),v=!0},p:Kt,i(s){v||(f(p.$$.fragment,s),v=!0)},o(s){g(p.$$.fragment,s),v=!1},d(s){s&&(t(c),t(x)),_(p,s)}}}function ya(U){let c,A,x,p,v,s,T,Rr="<code>.optimization</code> モジュールは以下を提供します。",mt,ee,Er="<li>モデルの微調整に使用できる重み減衰が修正されたオプティマイザー、および</li> <li><code>_LRSchedule</code> から継承するスケジュール オブジェクトの形式のいくつかのスケジュール:</li> <li>複数のバッチの勾配を累積するための勾配累積クラス</li>",ct,te,pt,W,re,tr,Pe,kr=`Implements Adam algorithm with weight decay fix as introduced in <a href="https://arxiv.org/abs/1711.05101" rel="nofollow">Decoupled Weight Decay
Regularization</a>.`,rr,Q,ae,ar,Re,Ir="Performs a single optimization step.",dt,ne,ht,b,oe,nr,Ee,Fr=`AdaFactor pytorch implementation can be used as a drop in replacement for Adam original fairseq code:
<a href="https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py" rel="nofollow">https://github.com/pytorch/fairseq/blob/master/fairseq/optim/adafactor.py</a>`,or,ke,Vr=`Paper: <em>Adafactor: Adaptive Learning Rates with Sublinear Memory Cost</em> <a href="https://arxiv.org/abs/1804.04235" rel="nofollow">https://arxiv.org/abs/1804.04235</a> Note that
this optimizer internally adjusts the learning rate depending on the <code>scale_parameter</code>, <code>relative_step</code> and
<code>warmup_init</code> options. To use a manual (external) learning rate schedule you should set <code>scale_parameter=False</code> and
<code>relative_step=False</code>.`,sr,Ie,Jr="This implementation handles low-precision (FP16, bfloat) values, but we have not thoroughly tested.",ir,Fe,Nr='Recommended T5 finetuning settings (<a href="https://discuss.huggingface.co/t/t5-finetuning-tips/684/3" rel="nofollow">https://discuss.huggingface.co/t/t5-finetuning-tips/684/3</a>):',lr,Ve,Zr='<li><p>Training without LR warmup or clip_threshold is not recommended.</p> <ul><li>use scheduled LR warm-up to fixed LR</li> <li>use clip_threshold=1.0 (<a href="https://arxiv.org/abs/1804.04235" rel="nofollow">https://arxiv.org/abs/1804.04235</a>)</li></ul></li> <li><p>Disable relative updates</p></li> <li><p>Use scale_parameter=False</p></li> <li><p>Additional optimizer operations like gradient clipping should not be used alongside Adafactor</p></li>',mr,S,cr,H,pr,Je,Gr='When using <code>lr=None</code> with <a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a> you will most likely need to use <code>AdafactorSchedule</code>',dr,q,hr,B,ur,X,se,fr,Ne,Qr="Performs a single optimization step",ut,ie,ft,C,le,gr,Ze,Sr=`Adam enables L2 weight decay and clip_by_global_norm on gradients. Just adding the square of the weights to the
loss function is <em>not</em> the correct way of using L2 regularization/weight decay with Adam, since that will interact
with the m and v parameters in strange ways as shown in <a href="https://arxiv.org/abs/1711.05101" rel="nofollow">Decoupled Weight Decay
Regularization</a>.`,_r,Ge,Hr=`Instead we want to decay the weights in a manner that doesn’t interact with the m/v parameters. This is equivalent
to adding the square of the weights to the loss with plain (non-momentum) SGD.`,wr,O,me,br,Qe,qr="Creates an optimizer from its config with WarmUp custom object.",gt,D,ce,$r,Se,Br="Creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay.",_t,pe,wt,de,bt,P,he,vr,He,Xr="An enumeration.",$t,R,ue,yr,qe,Or="Unified API to get any scheduler from its name.",vt,E,fe,xr,Be,Yr="Create a schedule with a constant learning rate, using the learning rate set in optimizer.",yt,k,ge,Tr,Xe,Kr=`Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate
increases linearly between 0 and the initial lr set in the optimizer.`,xt,_e,ea,Tt,I,we,zr,Oe,ta=`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the
initial lr set in the optimizer.`,zt,be,ra,Mt,F,$e,Mr,Ye,aa=`Create a schedule with a learning rate that decreases following the values of the cosine function between the
initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases
linearly between 0 and the initial lr set in the optimizer.`,At,ve,na,Ct,V,ye,Ar,Ke,oa=`Create a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after
a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.`,Wt,xe,sa,jt,j,Te,Cr,et,ia=`Create a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the
optimizer to end lr defined by <em>lr_end</em>, after a warmup period during which it increases linearly from 0 to the
initial lr set in the optimizer.`,Wr,tt,la=`Note: <em>power</em> defaults to 1.0 as in the fairseq implementation, which in turn is based on the original BERT
implementation at
<a href="https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37" rel="nofollow">https://github.com/google-research/bert/blob/f39e881b169b9d53bea03d2d341b31707a6c052b/optimization.py#L37</a>`,Lt,J,ze,jr,rt,ma=`Create a schedule with an inverse square-root learning rate, from the initial lr set in the optimizer, after a
warmup period which increases lr linearly from 0 to the initial lr set in the optimizer.`,Ut,Me,Dt,N,Ae,Lr,at,ca="Applies a warmup schedule on a given learning rate decay schedule.",Pt,Ce,Rt,We,Et,L,je,Ur,nt,pa=`Gradient accumulation utility. When used with a distribution strategy, the accumulator should be called in a
replica context. Gradients will be accumulated locally on each replica and without synchronization. Users should
then call <code>.gradients</code>, scale the gradients if required, and pass the result to <code>apply_gradients</code>.`,Dr,Y,Le,Pr,ot,da="Resets the accumulated gradients on the current replica.",kt,lt,It;return v=new G({props:{title:"Optimization",local:"optimization",headingTag:"h1"}}),te=new G({props:{title:"AdamW (PyTorch)",local:"transformers.AdamW",headingTag:"h2"}}),re=new M({props:{name:"class transformers.AdamW",anchor:"transformers.AdamW",parameters:[{name:"params",val:": Iterable"},{name:"lr",val:": float = 0.001"},{name:"betas",val:": Tuple = (0.9, 0.999)"},{name:"eps",val:": float = 1e-06"},{name:"weight_decay",val:": float = 0.0"},{name:"correct_bias",val:": bool = True"},{name:"no_deprecation_warning",val:": bool = False"}],parametersDescription:[{anchor:"transformers.AdamW.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.AdamW.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>, defaults to 0.001) &#x2014;
The learning rate to use.`,name:"lr"},{anchor:"transformers.AdamW.betas",description:`<strong>betas</strong> (<code>Tuple[float,float]</code>, <em>optional</em>, defaults to <code>(0.9, 0.999)</code>) &#x2014;
Adam&#x2019;s betas parameters (b1, b2).`,name:"betas"},{anchor:"transformers.AdamW.eps",description:`<strong>eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-06) &#x2014;
Adam&#x2019;s epsilon for numerical stability.`,name:"eps"},{anchor:"transformers.AdamW.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Decoupled weight decay to apply.`,name:"weight_decay"},{anchor:"transformers.AdamW.correct_bias",description:`<strong>correct_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to correct bias in Adam (for instance, in Bert TF repository they use <code>False</code>).`,name:"correct_bias"},{anchor:"transformers.AdamW.no_deprecation_warning",description:`<strong>no_deprecation_warning</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
A flag used to disable the deprecation warning (set to <code>True</code> to disable the warning).`,name:"no_deprecation_warning"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L396"}}),ae=new M({props:{name:"step",anchor:"transformers.AdamW.step",parameters:[{name:"closure",val:": Callable = None"}],parametersDescription:[{anchor:"transformers.AdamW.step.closure",description:"<strong>closure</strong> (<code>Callable</code>, <em>optional</em>) &#x2014; A closure that reevaluates the model and returns the loss.",name:"closure"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L447"}}),ne=new G({props:{title:"AdaFactor (PyTorch)",local:"transformers.Adafactor",headingTag:"h2"}}),oe=new M({props:{name:"class transformers.Adafactor",anchor:"transformers.Adafactor",parameters:[{name:"params",val:""},{name:"lr",val:" = None"},{name:"eps",val:" = (1e-30, 0.001)"},{name:"clip_threshold",val:" = 1.0"},{name:"decay_rate",val:" = -0.8"},{name:"beta1",val:" = None"},{name:"weight_decay",val:" = 0.0"},{name:"scale_parameter",val:" = True"},{name:"relative_step",val:" = True"},{name:"warmup_init",val:" = False"}],parametersDescription:[{anchor:"transformers.Adafactor.params",description:`<strong>params</strong> (<code>Iterable[nn.parameter.Parameter]</code>) &#x2014;
Iterable of parameters to optimize or dictionaries defining parameter groups.`,name:"params"},{anchor:"transformers.Adafactor.lr",description:`<strong>lr</strong> (<code>float</code>, <em>optional</em>) &#x2014;
The external learning rate.`,name:"lr"},{anchor:"transformers.Adafactor.eps",description:`<strong>eps</strong> (<code>Tuple[float, float]</code>, <em>optional</em>, defaults to <code>(1e-30, 0.001)</code>) &#x2014;
Regularization constants for square gradient and parameter scale respectively`,name:"eps"},{anchor:"transformers.Adafactor.clip_threshold",description:`<strong>clip_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Threshold of root mean square of final gradient update`,name:"clip_threshold"},{anchor:"transformers.Adafactor.decay_rate",description:`<strong>decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to -0.8) &#x2014;
Coefficient used to compute running averages of square`,name:"decay_rate"},{anchor:"transformers.Adafactor.beta1",description:`<strong>beta1</strong> (<code>float</code>, <em>optional</em>) &#x2014;
Coefficient used for computing running averages of gradient`,name:"beta1"},{anchor:"transformers.Adafactor.weight_decay",description:`<strong>weight_decay</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
Weight decay (L2 penalty)`,name:"weight_decay"},{anchor:"transformers.Adafactor.scale_parameter",description:`<strong>scale_parameter</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, learning rate is scaled by root mean square`,name:"scale_parameter"},{anchor:"transformers.Adafactor.relative_step",description:`<strong>relative_step</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
If True, time-dependent learning rate is computed instead of external learning rate`,name:"relative_step"},{anchor:"transformers.Adafactor.warmup_init",description:`<strong>warmup_init</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Time-dependent learning rate computation depends on whether warm-up initialization is being used`,name:"warmup_init"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L510"}}),S=new Yt({props:{anchor:"transformers.Adafactor.example",$$slots:{default:[wa]},$$scope:{ctx:U}}}),H=new Yt({props:{anchor:"transformers.Adafactor.example-2",$$slots:{default:[ba]},$$scope:{ctx:U}}}),q=new Yt({props:{anchor:"transformers.Adafactor.example-3",$$slots:{default:[$a]},$$scope:{ctx:U}}}),B=new Yt({props:{anchor:"transformers.Adafactor.example-4",$$slots:{default:[va]},$$scope:{ctx:U}}}),se=new M({props:{name:"step",anchor:"transformers.Adafactor.step",parameters:[{name:"closure",val:" = None"}],parametersDescription:[{anchor:"transformers.Adafactor.step.closure",description:`<strong>closure</strong> (callable, optional) &#x2014; A closure that reevaluates the model
and returns the loss.`,name:"closure"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L656"}}),ie=new G({props:{title:"AdamWeightDecay (TensorFlow)",local:"transformers.AdamWeightDecay",headingTag:"h2"}}),le=new M({props:{name:"class transformers.AdamWeightDecay",anchor:"transformers.AdamWeightDecay",parameters:[{name:"learning_rate",val:": Union = 0.001"},{name:"beta_1",val:": float = 0.9"},{name:"beta_2",val:": float = 0.999"},{name:"epsilon",val:": float = 1e-07"},{name:"amsgrad",val:": bool = False"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"include_in_weight_decay",val:": Optional = None"},{name:"exclude_from_weight_decay",val:": Optional = None"},{name:"name",val:": str = 'AdamWeightDecay'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AdamWeightDecay.learning_rate",description:`<strong>learning_rate</strong> (<code>Union[float, LearningRateSchedule]</code>, <em>optional</em>, defaults to 0.001) &#x2014;
The learning rate to use or a schedule.`,name:"learning_rate"},{anchor:"transformers.AdamWeightDecay.beta_1",description:`<strong>beta_1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 parameter in Adam, which is the exponential decay rate for the 1st momentum estimates.`,name:"beta_1"},{anchor:"transformers.AdamWeightDecay.beta_2",description:`<strong>beta_2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 parameter in Adam, which is the exponential decay rate for the 2nd momentum estimates.`,name:"beta_2"},{anchor:"transformers.AdamWeightDecay.epsilon",description:`<strong>epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-07) &#x2014;
The epsilon parameter in Adam, which is a small constant for numerical stability.`,name:"epsilon"},{anchor:"transformers.AdamWeightDecay.amsgrad",description:`<strong>amsgrad</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to apply AMSGrad variant of this algorithm or not, see <a href="https://arxiv.org/abs/1904.09237" rel="nofollow">On the Convergence of Adam and
Beyond</a>.`,name:"amsgrad"},{anchor:"transformers.AdamWeightDecay.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The weight decay to apply.`,name:"weight_decay_rate"},{anchor:"transformers.AdamWeightDecay.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters by default (unless they are in <code>exclude_from_weight_decay</code>).`,name:"include_in_weight_decay"},{anchor:"transformers.AdamWeightDecay.exclude_from_weight_decay",description:`<strong>exclude_from_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to exclude from applying weight decay to. If a
<code>include_in_weight_decay</code> is passed, the names in it will supersede this list.`,name:"exclude_from_weight_decay"},{anchor:"transformers.AdamWeightDecay.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;AdamWeightDecay&quot;</code>) &#x2014;
Optional name for the operations created when applying gradients.`,name:"name"},{anchor:"transformers.AdamWeightDecay.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Keyword arguments. Allowed to be {<code>clipnorm</code>, <code>clipvalue</code>, <code>lr</code>, <code>decay</code>}. <code>clipnorm</code> is clip gradients by
norm; <code>clipvalue</code> is clip gradients by value, <code>decay</code> is included for backward compatibility to allow time
inverse decay of learning rate. <code>lr</code> is included for backward compatibility, recommended to use
<code>learning_rate</code> instead.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization_tf.py#L181"}}),me=new M({props:{name:"from_config",anchor:"transformers.AdamWeightDecay.from_config",parameters:[{name:"config",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization_tf.py#L238"}}),ce=new M({props:{name:"transformers.create_optimizer",anchor:"transformers.create_optimizer",parameters:[{name:"init_lr",val:": float"},{name:"num_train_steps",val:": int"},{name:"num_warmup_steps",val:": int"},{name:"min_lr_ratio",val:": float = 0.0"},{name:"adam_beta1",val:": float = 0.9"},{name:"adam_beta2",val:": float = 0.999"},{name:"adam_epsilon",val:": float = 1e-08"},{name:"adam_clipnorm",val:": Optional = None"},{name:"adam_global_clipnorm",val:": Optional = None"},{name:"weight_decay_rate",val:": float = 0.0"},{name:"power",val:": float = 1.0"},{name:"include_in_weight_decay",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.create_optimizer.init_lr",description:`<strong>init_lr</strong> (<code>float</code>) &#x2014;
The desired learning rate at the end of the warmup phase.`,name:"init_lr"},{anchor:"transformers.create_optimizer.num_train_steps",description:`<strong>num_train_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_train_steps"},{anchor:"transformers.create_optimizer.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of warmup steps.`,name:"num_warmup_steps"},{anchor:"transformers.create_optimizer.min_lr_ratio",description:`<strong>min_lr_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The final learning rate at the end of the linear decay will be <code>init_lr * min_lr_ratio</code>.`,name:"min_lr_ratio"},{anchor:"transformers.create_optimizer.adam_beta1",description:`<strong>adam_beta1</strong> (<code>float</code>, <em>optional</em>, defaults to 0.9) &#x2014;
The beta1 to use in Adam.`,name:"adam_beta1"},{anchor:"transformers.create_optimizer.adam_beta2",description:`<strong>adam_beta2</strong> (<code>float</code>, <em>optional</em>, defaults to 0.999) &#x2014;
The beta2 to use in Adam.`,name:"adam_beta2"},{anchor:"transformers.create_optimizer.adam_epsilon",description:`<strong>adam_epsilon</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-8) &#x2014;
The epsilon to use in Adam.`,name:"adam_epsilon"},{anchor:"transformers.create_optimizer.adam_clipnorm",description:`<strong>adam_clipnorm</strong> (<code>float</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
If not <code>None</code>, clip the gradient norm for each weight tensor to this value.`,name:"adam_clipnorm"},{anchor:"transformers.create_optimizer.adam_global_clipnorm",description:`<strong>adam_global_clipnorm</strong> (<code>float</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
If not <code>None</code>, clip gradient norm to this value. When using this argument, the norm is computed over all
weight tensors, as if they were concatenated into a single vector.`,name:"adam_global_clipnorm"},{anchor:"transformers.create_optimizer.weight_decay_rate",description:`<strong>weight_decay_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0) &#x2014;
The weight decay to use.`,name:"weight_decay_rate"},{anchor:"transformers.create_optimizer.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The power to use for PolynomialDecay.`,name:"power"},{anchor:"transformers.create_optimizer.include_in_weight_decay",description:`<strong>include_in_weight_decay</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
List of the parameter names (or re patterns) to apply weight decay to. If none is passed, weight decay is
applied to all parameters except bias and layer norm parameters.`,name:"include_in_weight_decay"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization_tf.py#L97"}}),pe=new G({props:{title:"Schedules",local:"schedules",headingTag:"h2"}}),de=new G({props:{title:"Learning Rate Schedules (Pytorch)",local:"transformers.SchedulerType",headingTag:"h3"}}),he=new M({props:{name:"class transformers.SchedulerType",anchor:"transformers.SchedulerType",parameters:[{name:"value",val:""},{name:"names",val:" = None"},{name:"module",val:" = None"},{name:"qualname",val:" = None"},{name:"type",val:" = None"},{name:"start",val:" = 1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/trainer_utils.py#L395"}}),ue=new M({props:{name:"transformers.get_scheduler",anchor:"transformers.get_scheduler",parameters:[{name:"name",val:": Union"},{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": Optional = None"},{name:"num_training_steps",val:": Optional = None"},{name:"scheduler_specific_kwargs",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.get_scheduler.name",description:`<strong>name</strong> (<code>str</code> or <code>SchedulerType</code>) &#x2014;
The name of the scheduler to use.`,name:"name"},{anchor:"transformers.get_scheduler.optimizer",description:`<strong>optimizer</strong> (<code>torch.optim.Optimizer</code>) &#x2014;
The optimizer that will be used during training.`,name:"optimizer"},{anchor:"transformers.get_scheduler.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The number of warmup steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_warmup_steps"},{anchor:"transformers.get_scheduler.num_training_steps",description:`<strong>num_training_steps</strong> (\`int&#x201C;, <em>optional</em>) &#x2014;
The number of training steps to do. This is not required by all schedulers (hence the argument being
optional), the function will raise an error if it&#x2019;s unset and the scheduler type requires it.`,name:"num_training_steps"},{anchor:"transformers.get_scheduler.scheduler_specific_kwargs",description:`<strong>scheduler_specific_kwargs</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Extra parameters for schedulers such as cosine with restarts. Mismatched scheduler types and scheduler
parameters will cause the scheduler function to raise a TypeError.`,name:"scheduler_specific_kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L338"}}),fe=new M({props:{name:"transformers.get_constant_schedule",anchor:"transformers.get_constant_schedule",parameters:[{name:"optimizer",val:": Optimizer"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_constant_schedule.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L39",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),ge=new M({props:{name:"transformers.get_constant_schedule_with_warmup",anchor:"transformers.get_constant_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_constant_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_constant_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_constant_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L80",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),we=new M({props:{name:"transformers.get_cosine_schedule_with_warmup",anchor:"transformers.get_cosine_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": float = 0.5"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_cosine_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>float</code>, <em>optional</em>, defaults to 0.5) &#x2014;
The number of waves in the cosine schedule (the defaults is to just decrease from the max value to 0
following a half-cosine).`,name:"num_cycles"},{anchor:"transformers.get_cosine_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L143",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),$e=new M({props:{name:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"num_training_steps",val:": int"},{name:"num_cycles",val:": int = 1"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.num_cycles",description:`<strong>num_cycles</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of hard restarts to use.`,name:"num_cycles"},{anchor:"transformers.get_cosine_with_hard_restarts_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L188",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),ye=new M({props:{name:"transformers.get_linear_schedule_with_warmup",anchor:"transformers.get_linear_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"last_epoch",val:" = -1"}],parametersDescription:[{anchor:"transformers.get_linear_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_linear_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_linear_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L107",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Te=new M({props:{name:"transformers.get_polynomial_decay_schedule_with_warmup",anchor:"transformers.get_polynomial_decay_schedule_with_warmup",parameters:[{name:"optimizer",val:""},{name:"num_warmup_steps",val:""},{name:"num_training_steps",val:""},{name:"lr_end",val:" = 1e-07"},{name:"power",val:" = 1.0"},{name:"last_epoch",val:" = -1"}],parametersDescription:[{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.num_training_steps",description:`<strong>num_training_steps</strong> (<code>int</code>) &#x2014;
The total number of training steps.`,name:"num_training_steps"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.lr_end",description:`<strong>lr_end</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-7) &#x2014;
The end LR.`,name:"lr_end"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
Power factor.`,name:"power"},{anchor:"transformers.get_polynomial_decay_schedule_with_warmup.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L242",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),ze=new M({props:{name:"transformers.get_inverse_sqrt_schedule",anchor:"transformers.get_inverse_sqrt_schedule",parameters:[{name:"optimizer",val:": Optimizer"},{name:"num_warmup_steps",val:": int"},{name:"timescale",val:": int = None"},{name:"last_epoch",val:": int = -1"}],parametersDescription:[{anchor:"transformers.get_inverse_sqrt_schedule.optimizer",description:`<strong>optimizer</strong> (<code>~torch.optim.Optimizer</code>) &#x2014;
The optimizer for which to schedule the learning rate.`,name:"optimizer"},{anchor:"transformers.get_inverse_sqrt_schedule.num_warmup_steps",description:`<strong>num_warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup phase.`,name:"num_warmup_steps"},{anchor:"transformers.get_inverse_sqrt_schedule.timescale",description:`<strong>timescale</strong> (<code>int</code>, <em>optional</em>, defaults to <code>num_warmup_steps</code>) &#x2014;
Time scale.`,name:"timescale"},{anchor:"transformers.get_inverse_sqrt_schedule.last_epoch",description:`<strong>last_epoch</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The index of the last epoch when resuming training.`,name:"last_epoch"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization.py#L296",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>torch.optim.lr_scheduler.LambdaLR</code> with the appropriate schedule.</p>
`}}),Me=new G({props:{title:"Warmup (TensorFlow)",local:"transformers.WarmUp",headingTag:"h3"}}),Ae=new M({props:{name:"class transformers.WarmUp",anchor:"transformers.WarmUp",parameters:[{name:"initial_learning_rate",val:": float"},{name:"decay_schedule_fn",val:": Callable"},{name:"warmup_steps",val:": int"},{name:"power",val:": float = 1.0"},{name:"name",val:": str = None"}],parametersDescription:[{anchor:"transformers.WarmUp.initial_learning_rate",description:`<strong>initial_learning_rate</strong> (<code>float</code>) &#x2014;
The initial learning rate for the schedule after the warmup (so this will be the learning rate at the end
of the warmup).`,name:"initial_learning_rate"},{anchor:"transformers.WarmUp.decay_schedule_fn",description:`<strong>decay_schedule_fn</strong> (<code>Callable</code>) &#x2014;
The schedule function to apply after the warmup for the rest of training.`,name:"decay_schedule_fn"},{anchor:"transformers.WarmUp.warmup_steps",description:`<strong>warmup_steps</strong> (<code>int</code>) &#x2014;
The number of steps for the warmup part of training.`,name:"warmup_steps"},{anchor:"transformers.WarmUp.power",description:`<strong>power</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The power to use for the polynomial warmup (defaults is a linear warmup).`,name:"power"},{anchor:"transformers.WarmUp.name",description:`<strong>name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Optional name prefix for the returned tensors during the schedule.`,name:"name"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization_tf.py#L39"}}),Ce=new G({props:{title:"Gradient Strategies",local:"gradient-strategies",headingTag:"h2"}}),We=new G({props:{title:"GradientAccumulator (TensorFlow)",local:"transformers.GradientAccumulator",headingTag:"h3"}}),je=new M({props:{name:"class transformers.GradientAccumulator",anchor:"transformers.GradientAccumulator",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization_tf.py#L311"}}),Le=new M({props:{name:"reset",anchor:"transformers.GradientAccumulator.reset",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/optimization_tf.py#L373"}}),{c(){c=l("meta"),A=a(),x=l("p"),p=a(),d(v.$$.fragment),s=a(),T=l("p"),T.innerHTML=Rr,mt=a(),ee=l("ul"),ee.innerHTML=Er,ct=a(),d(te.$$.fragment),pt=a(),W=l("div"),d(re.$$.fragment),tr=a(),Pe=l("p"),Pe.innerHTML=kr,rr=a(),Q=l("div"),d(ae.$$.fragment),ar=a(),Re=l("p"),Re.textContent=Ir,dt=a(),d(ne.$$.fragment),ht=a(),b=l("div"),d(oe.$$.fragment),nr=a(),Ee=l("p"),Ee.innerHTML=Fr,or=a(),ke=l("p"),ke.innerHTML=Vr,sr=a(),Ie=l("p"),Ie.textContent=Jr,ir=a(),Fe=l("p"),Fe.innerHTML=Nr,lr=a(),Ve=l("ul"),Ve.innerHTML=Zr,mr=a(),d(S.$$.fragment),cr=a(),d(H.$$.fragment),pr=a(),Je=l("p"),Je.innerHTML=Gr,dr=a(),d(q.$$.fragment),hr=a(),d(B.$$.fragment),ur=a(),X=l("div"),d(se.$$.fragment),fr=a(),Ne=l("p"),Ne.textContent=Qr,ut=a(),d(ie.$$.fragment),ft=a(),C=l("div"),d(le.$$.fragment),gr=a(),Ze=l("p"),Ze.innerHTML=Sr,_r=a(),Ge=l("p"),Ge.textContent=Hr,wr=a(),O=l("div"),d(me.$$.fragment),br=a(),Qe=l("p"),Qe.textContent=qr,gt=a(),D=l("div"),d(ce.$$.fragment),$r=a(),Se=l("p"),Se.textContent=Br,_t=a(),d(pe.$$.fragment),wt=a(),d(de.$$.fragment),bt=a(),P=l("div"),d(he.$$.fragment),vr=a(),He=l("p"),He.textContent=Xr,$t=a(),R=l("div"),d(ue.$$.fragment),yr=a(),qe=l("p"),qe.textContent=Or,vt=a(),E=l("div"),d(fe.$$.fragment),xr=a(),Be=l("p"),Be.textContent=Yr,yt=a(),k=l("div"),d(ge.$$.fragment),Tr=a(),Xe=l("p"),Xe.textContent=Kr,xt=a(),_e=l("img"),Tt=a(),I=l("div"),d(we.$$.fragment),zr=a(),Oe=l("p"),Oe.textContent=ta,zt=a(),be=l("img"),Mt=a(),F=l("div"),d($e.$$.fragment),Mr=a(),Ye=l("p"),Ye.textContent=aa,At=a(),ve=l("img"),Ct=a(),V=l("div"),d(ye.$$.fragment),Ar=a(),Ke=l("p"),Ke.textContent=oa,Wt=a(),xe=l("img"),jt=a(),j=l("div"),d(Te.$$.fragment),Cr=a(),et=l("p"),et.innerHTML=ia,Wr=a(),tt=l("p"),tt.innerHTML=la,Lt=a(),J=l("div"),d(ze.$$.fragment),jr=a(),rt=l("p"),rt.textContent=ma,Ut=a(),d(Me.$$.fragment),Dt=a(),N=l("div"),d(Ae.$$.fragment),Lr=a(),at=l("p"),at.textContent=ca,Pt=a(),d(Ce.$$.fragment),Rt=a(),d(We.$$.fragment),Et=a(),L=l("div"),d(je.$$.fragment),Ur=a(),nt=l("p"),nt.innerHTML=pa,Dr=a(),Y=l("div"),d(Le.$$.fragment),Pr=a(),ot=l("p"),ot.textContent=da,kt=a(),lt=l("p"),this.h()},l(e){const r=_a("svelte-u9bgzb",document.head);c=m(r,"META",{name:!0,content:!0}),r.forEach(t),A=n(e),x=m(e,"P",{}),z(x).forEach(t),p=n(e),h(v.$$.fragment,e),s=n(e),T=m(e,"P",{"data-svelte-h":!0}),w(T)!=="svelte-17yfr74"&&(T.innerHTML=Rr),mt=n(e),ee=m(e,"UL",{"data-svelte-h":!0}),w(ee)!=="svelte-1rrczld"&&(ee.innerHTML=Er),ct=n(e),h(te.$$.fragment,e),pt=n(e),W=m(e,"DIV",{class:!0});var Z=z(W);h(re.$$.fragment,Z),tr=n(Z),Pe=m(Z,"P",{"data-svelte-h":!0}),w(Pe)!=="svelte-b86q21"&&(Pe.innerHTML=kr),rr=n(Z),Q=m(Z,"DIV",{class:!0});var Ue=z(Q);h(ae.$$.fragment,Ue),ar=n(Ue),Re=m(Ue,"P",{"data-svelte-h":!0}),w(Re)!=="svelte-1gr03w4"&&(Re.textContent=Ir),Ue.forEach(t),Z.forEach(t),dt=n(e),h(ne.$$.fragment,e),ht=n(e),b=m(e,"DIV",{class:!0});var y=z(b);h(oe.$$.fragment,y),nr=n(y),Ee=m(y,"P",{"data-svelte-h":!0}),w(Ee)!=="svelte-114ll6r"&&(Ee.innerHTML=Fr),or=n(y),ke=m(y,"P",{"data-svelte-h":!0}),w(ke)!=="svelte-9bvrpy"&&(ke.innerHTML=Vr),sr=n(y),Ie=m(y,"P",{"data-svelte-h":!0}),w(Ie)!=="svelte-1dxk71o"&&(Ie.textContent=Jr),ir=n(y),Fe=m(y,"P",{"data-svelte-h":!0}),w(Fe)!=="svelte-kc6644"&&(Fe.innerHTML=Nr),lr=n(y),Ve=m(y,"UL",{"data-svelte-h":!0}),w(Ve)!=="svelte-p81jc"&&(Ve.innerHTML=Zr),mr=n(y),h(S.$$.fragment,y),cr=n(y),h(H.$$.fragment,y),pr=n(y),Je=m(y,"P",{"data-svelte-h":!0}),w(Je)!=="svelte-11152nx"&&(Je.innerHTML=Gr),dr=n(y),h(q.$$.fragment,y),hr=n(y),h(B.$$.fragment,y),ur=n(y),X=m(y,"DIV",{class:!0});var De=z(X);h(se.$$.fragment,De),fr=n(De),Ne=m(De,"P",{"data-svelte-h":!0}),w(Ne)!=="svelte-6lkj7q"&&(Ne.textContent=Qr),De.forEach(t),y.forEach(t),ut=n(e),h(ie.$$.fragment,e),ft=n(e),C=m(e,"DIV",{class:!0});var K=z(C);h(le.$$.fragment,K),gr=n(K),Ze=m(K,"P",{"data-svelte-h":!0}),w(Ze)!=="svelte-towids"&&(Ze.innerHTML=Sr),_r=n(K),Ge=m(K,"P",{"data-svelte-h":!0}),w(Ge)!=="svelte-id029h"&&(Ge.textContent=Hr),wr=n(K),O=m(K,"DIV",{class:!0});var Ft=z(O);h(me.$$.fragment,Ft),br=n(Ft),Qe=m(Ft,"P",{"data-svelte-h":!0}),w(Qe)!=="svelte-3iict9"&&(Qe.textContent=qr),Ft.forEach(t),K.forEach(t),gt=n(e),D=m(e,"DIV",{class:!0});var Vt=z(D);h(ce.$$.fragment,Vt),$r=n(Vt),Se=m(Vt,"P",{"data-svelte-h":!0}),w(Se)!=="svelte-16qojcu"&&(Se.textContent=Br),Vt.forEach(t),_t=n(e),h(pe.$$.fragment,e),wt=n(e),h(de.$$.fragment,e),bt=n(e),P=m(e,"DIV",{class:!0});var Jt=z(P);h(he.$$.fragment,Jt),vr=n(Jt),He=m(Jt,"P",{"data-svelte-h":!0}),w(He)!=="svelte-ofujqi"&&(He.textContent=Xr),Jt.forEach(t),$t=n(e),R=m(e,"DIV",{class:!0});var Nt=z(R);h(ue.$$.fragment,Nt),yr=n(Nt),qe=m(Nt,"P",{"data-svelte-h":!0}),w(qe)!=="svelte-1oh310h"&&(qe.textContent=Or),Nt.forEach(t),vt=n(e),E=m(e,"DIV",{class:!0});var Zt=z(E);h(fe.$$.fragment,Zt),xr=n(Zt),Be=m(Zt,"P",{"data-svelte-h":!0}),w(Be)!=="svelte-cadr4a"&&(Be.textContent=Yr),Zt.forEach(t),yt=n(e),k=m(e,"DIV",{class:!0});var Gt=z(k);h(ge.$$.fragment,Gt),Tr=n(Gt),Xe=m(Gt,"P",{"data-svelte-h":!0}),w(Xe)!=="svelte-1lj8vcj"&&(Xe.textContent=Kr),Gt.forEach(t),xt=n(e),_e=m(e,"IMG",{alt:!0,src:!0}),Tt=n(e),I=m(e,"DIV",{class:!0});var Qt=z(I);h(we.$$.fragment,Qt),zr=n(Qt),Oe=m(Qt,"P",{"data-svelte-h":!0}),w(Oe)!=="svelte-1lhkhz0"&&(Oe.textContent=ta),Qt.forEach(t),zt=n(e),be=m(e,"IMG",{alt:!0,src:!0}),Mt=n(e),F=m(e,"DIV",{class:!0});var St=z(F);h($e.$$.fragment,St),Mr=n(St),Ye=m(St,"P",{"data-svelte-h":!0}),w(Ye)!=="svelte-1pclsvt"&&(Ye.textContent=aa),St.forEach(t),At=n(e),ve=m(e,"IMG",{alt:!0,src:!0}),Ct=n(e),V=m(e,"DIV",{class:!0});var Ht=z(V);h(ye.$$.fragment,Ht),Ar=n(Ht),Ke=m(Ht,"P",{"data-svelte-h":!0}),w(Ke)!=="svelte-17q4ov5"&&(Ke.textContent=oa),Ht.forEach(t),Wt=n(e),xe=m(e,"IMG",{alt:!0,src:!0}),jt=n(e),j=m(e,"DIV",{class:!0});var st=z(j);h(Te.$$.fragment,st),Cr=n(st),et=m(st,"P",{"data-svelte-h":!0}),w(et)!=="svelte-1y55gjq"&&(et.innerHTML=ia),Wr=n(st),tt=m(st,"P",{"data-svelte-h":!0}),w(tt)!=="svelte-1tn0l2l"&&(tt.innerHTML=la),st.forEach(t),Lt=n(e),J=m(e,"DIV",{class:!0});var qt=z(J);h(ze.$$.fragment,qt),jr=n(qt),rt=m(qt,"P",{"data-svelte-h":!0}),w(rt)!=="svelte-yp2yn6"&&(rt.textContent=ma),qt.forEach(t),Ut=n(e),h(Me.$$.fragment,e),Dt=n(e),N=m(e,"DIV",{class:!0});var Bt=z(N);h(Ae.$$.fragment,Bt),Lr=n(Bt),at=m(Bt,"P",{"data-svelte-h":!0}),w(at)!=="svelte-1mfn66a"&&(at.textContent=ca),Bt.forEach(t),Pt=n(e),h(Ce.$$.fragment,e),Rt=n(e),h(We.$$.fragment,e),Et=n(e),L=m(e,"DIV",{class:!0});var it=z(L);h(je.$$.fragment,it),Ur=n(it),nt=m(it,"P",{"data-svelte-h":!0}),w(nt)!=="svelte-8tkiw"&&(nt.innerHTML=pa),Dr=n(it),Y=m(it,"DIV",{class:!0});var Xt=z(Y);h(Le.$$.fragment,Xt),Pr=n(Xt),ot=m(Xt,"P",{"data-svelte-h":!0}),w(ot)!=="svelte-v0y289"&&(ot.textContent=da),Xt.forEach(t),it.forEach(t),kt=n(e),lt=m(e,"P",{}),z(lt).forEach(t),this.h()},h(){$(c,"name","hf:doc:metadata"),$(c,"content",xa),$(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(_e,"alt",""),Ot(_e.src,ea="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png")||$(_e,"src",ea),$(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(be,"alt",""),Ot(be.src,ra="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png")||$(be,"src",ra),$(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(ve,"alt",""),Ot(ve.src,na="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png")||$(ve,"src",na),$(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(xe,"alt",""),Ot(xe.src,sa="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png")||$(xe,"src",sa),$(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,r){o(document.head,c),i(e,A,r),i(e,x,r),i(e,p,r),u(v,e,r),i(e,s,r),i(e,T,r),i(e,mt,r),i(e,ee,r),i(e,ct,r),u(te,e,r),i(e,pt,r),i(e,W,r),u(re,W,null),o(W,tr),o(W,Pe),o(W,rr),o(W,Q),u(ae,Q,null),o(Q,ar),o(Q,Re),i(e,dt,r),u(ne,e,r),i(e,ht,r),i(e,b,r),u(oe,b,null),o(b,nr),o(b,Ee),o(b,or),o(b,ke),o(b,sr),o(b,Ie),o(b,ir),o(b,Fe),o(b,lr),o(b,Ve),o(b,mr),u(S,b,null),o(b,cr),u(H,b,null),o(b,pr),o(b,Je),o(b,dr),u(q,b,null),o(b,hr),u(B,b,null),o(b,ur),o(b,X),u(se,X,null),o(X,fr),o(X,Ne),i(e,ut,r),u(ie,e,r),i(e,ft,r),i(e,C,r),u(le,C,null),o(C,gr),o(C,Ze),o(C,_r),o(C,Ge),o(C,wr),o(C,O),u(me,O,null),o(O,br),o(O,Qe),i(e,gt,r),i(e,D,r),u(ce,D,null),o(D,$r),o(D,Se),i(e,_t,r),u(pe,e,r),i(e,wt,r),u(de,e,r),i(e,bt,r),i(e,P,r),u(he,P,null),o(P,vr),o(P,He),i(e,$t,r),i(e,R,r),u(ue,R,null),o(R,yr),o(R,qe),i(e,vt,r),i(e,E,r),u(fe,E,null),o(E,xr),o(E,Be),i(e,yt,r),i(e,k,r),u(ge,k,null),o(k,Tr),o(k,Xe),i(e,xt,r),i(e,_e,r),i(e,Tt,r),i(e,I,r),u(we,I,null),o(I,zr),o(I,Oe),i(e,zt,r),i(e,be,r),i(e,Mt,r),i(e,F,r),u($e,F,null),o(F,Mr),o(F,Ye),i(e,At,r),i(e,ve,r),i(e,Ct,r),i(e,V,r),u(ye,V,null),o(V,Ar),o(V,Ke),i(e,Wt,r),i(e,xe,r),i(e,jt,r),i(e,j,r),u(Te,j,null),o(j,Cr),o(j,et),o(j,Wr),o(j,tt),i(e,Lt,r),i(e,J,r),u(ze,J,null),o(J,jr),o(J,rt),i(e,Ut,r),u(Me,e,r),i(e,Dt,r),i(e,N,r),u(Ae,N,null),o(N,Lr),o(N,at),i(e,Pt,r),u(Ce,e,r),i(e,Rt,r),u(We,e,r),i(e,Et,r),i(e,L,r),u(je,L,null),o(L,Ur),o(L,nt),o(L,Dr),o(L,Y),u(Le,Y,null),o(Y,Pr),o(Y,ot),i(e,kt,r),i(e,lt,r),It=!0},p(e,[r]){const Z={};r&2&&(Z.$$scope={dirty:r,ctx:e}),S.$set(Z);const Ue={};r&2&&(Ue.$$scope={dirty:r,ctx:e}),H.$set(Ue);const y={};r&2&&(y.$$scope={dirty:r,ctx:e}),q.$set(y);const De={};r&2&&(De.$$scope={dirty:r,ctx:e}),B.$set(De)},i(e){It||(f(v.$$.fragment,e),f(te.$$.fragment,e),f(re.$$.fragment,e),f(ae.$$.fragment,e),f(ne.$$.fragment,e),f(oe.$$.fragment,e),f(S.$$.fragment,e),f(H.$$.fragment,e),f(q.$$.fragment,e),f(B.$$.fragment,e),f(se.$$.fragment,e),f(ie.$$.fragment,e),f(le.$$.fragment,e),f(me.$$.fragment,e),f(ce.$$.fragment,e),f(pe.$$.fragment,e),f(de.$$.fragment,e),f(he.$$.fragment,e),f(ue.$$.fragment,e),f(fe.$$.fragment,e),f(ge.$$.fragment,e),f(we.$$.fragment,e),f($e.$$.fragment,e),f(ye.$$.fragment,e),f(Te.$$.fragment,e),f(ze.$$.fragment,e),f(Me.$$.fragment,e),f(Ae.$$.fragment,e),f(Ce.$$.fragment,e),f(We.$$.fragment,e),f(je.$$.fragment,e),f(Le.$$.fragment,e),It=!0)},o(e){g(v.$$.fragment,e),g(te.$$.fragment,e),g(re.$$.fragment,e),g(ae.$$.fragment,e),g(ne.$$.fragment,e),g(oe.$$.fragment,e),g(S.$$.fragment,e),g(H.$$.fragment,e),g(q.$$.fragment,e),g(B.$$.fragment,e),g(se.$$.fragment,e),g(ie.$$.fragment,e),g(le.$$.fragment,e),g(me.$$.fragment,e),g(ce.$$.fragment,e),g(pe.$$.fragment,e),g(de.$$.fragment,e),g(he.$$.fragment,e),g(ue.$$.fragment,e),g(fe.$$.fragment,e),g(ge.$$.fragment,e),g(we.$$.fragment,e),g($e.$$.fragment,e),g(ye.$$.fragment,e),g(Te.$$.fragment,e),g(ze.$$.fragment,e),g(Me.$$.fragment,e),g(Ae.$$.fragment,e),g(Ce.$$.fragment,e),g(We.$$.fragment,e),g(je.$$.fragment,e),g(Le.$$.fragment,e),It=!1},d(e){e&&(t(A),t(x),t(p),t(s),t(T),t(mt),t(ee),t(ct),t(pt),t(W),t(dt),t(ht),t(b),t(ut),t(ft),t(C),t(gt),t(D),t(_t),t(wt),t(bt),t(P),t($t),t(R),t(vt),t(E),t(yt),t(k),t(xt),t(_e),t(Tt),t(I),t(zt),t(be),t(Mt),t(F),t(At),t(ve),t(Ct),t(V),t(Wt),t(xe),t(jt),t(j),t(Lt),t(J),t(Ut),t(Dt),t(N),t(Pt),t(Rt),t(Et),t(L),t(kt),t(lt)),t(c),_(v,e),_(te,e),_(re),_(ae),_(ne,e),_(oe),_(S),_(H),_(q),_(B),_(se),_(ie,e),_(le),_(me),_(ce),_(pe,e),_(de,e),_(he),_(ue),_(fe),_(ge),_(we),_($e),_(ye),_(Te),_(ze),_(Me,e),_(Ae),_(Ce,e),_(We,e),_(je),_(Le)}}}const xa='{"title":"Optimization","local":"optimization","sections":[{"title":"AdamW (PyTorch)","local":"transformers.AdamW","sections":[],"depth":2},{"title":"AdaFactor (PyTorch)","local":"transformers.Adafactor","sections":[],"depth":2},{"title":"AdamWeightDecay (TensorFlow)","local":"transformers.AdamWeightDecay","sections":[],"depth":2},{"title":"Schedules","local":"schedules","sections":[{"title":"Learning Rate Schedules (Pytorch)","local":"transformers.SchedulerType","sections":[],"depth":3},{"title":"Warmup (TensorFlow)","local":"transformers.WarmUp","sections":[],"depth":3}],"depth":2},{"title":"Gradient Strategies","local":"gradient-strategies","sections":[{"title":"GradientAccumulator (TensorFlow)","local":"transformers.GradientAccumulator","sections":[],"depth":3}],"depth":2}],"depth":1}';function Ta(U){return ua(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class La extends fa{constructor(c){super(),ga(this,c,Ta,ya,ha,{})}}export{La as component};
