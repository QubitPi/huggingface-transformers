import{s as So,f as xo,o as jo,n as j}from"../chunks/scheduler.9bc65507.js";import{S as Lo,i as Jo,g as m,s as i,r as g,A as Zo,h as p,f as s,c as l,j as v,u as f,x as y,k as $,y as c,a as d,v as u,d as h,t as _,w as M}from"../chunks/index.707bf1b6.js";import{T as xt}from"../chunks/Tip.c2ecdbf4.js";import{D as w}from"../chunks/Docstring.17db21ae.js";import{C as ee}from"../chunks/CodeBlock.54a9f38d.js";import{E as K}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as ko}from"../chunks/PipelineTag.44585822.js";import{H as Q}from"../chunks/Heading.342b1fa6.js";function Uo(I){let n,C="Example:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdDb25maWclMkMlMjBDTElQU2VnTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ0NvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ01vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ01vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZyUwQSUwQSUyMyUyMFdlJTIwY2FuJTIwYWxzbyUyMGluaXRpYWxpemUlMjBhJTIwQ0xJUFNlZ0NvbmZpZyUyMGZyb20lMjBhJTIwQ0xJUFNlZ1RleHRDb25maWclMjBhbmQlMjBhJTIwQ0xJUFNlZ1Zpc2lvbkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQU2VnVGV4dCUyMGFuZCUyMENMSVBTZWdWaXNpb24lMjBjb25maWd1cmF0aW9uJTBBY29uZmlnX3RleHQlMjAlM0QlMjBDTElQU2VnVGV4dENvbmZpZygpJTBBY29uZmlnX3Zpc2lvbiUyMCUzRCUyMENMSVBTZWdWaXNpb25Db25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMENMSVBTZWdDb25maWcuZnJvbV90ZXh0X3Zpc2lvbl9jb25maWdzKGNvbmZpZ190ZXh0JTJDJTIwY29uZmlnX3Zpc2lvbik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegConfig, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a CLIPSegConfig from a CLIPSegTextConfig and a CLIPSegVisionConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegText and CLIPSegVision configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = CLIPSegTextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = CLIPSegVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Wo(I){let n,C="Example:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdUZXh0Q29uZmlnJTJDJTIwQ0xJUFNlZ1RleHRNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQU2VnVGV4dENvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnVGV4dENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMENMSVBTZWdUZXh0TW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMENJREFTJTJGY2xpcHNlZy1yZDY0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnVGV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegTextConfig, CLIPSegTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegTextConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegTextModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function zo(I){let n,C="Example:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdWaXNpb25Db25maWclMkMlMjBDTElQU2VnVmlzaW9uTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ1Zpc2lvbkNvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnVmlzaW9uQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ1Zpc2lvbk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ1Zpc2lvbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegVisionConfig, CLIPSegVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegVisionConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegVisionModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Bo(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function Vo(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdNb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMEElMjAlMjAlMjAlMjB0ZXh0JTNEJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcGFkZGluZyUzRFRydWUlMEEpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0c19wZXJfaW1hZ2UlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0c19wZXJfaW1hZ2UlMjAlMjAlMjMlMjB0aGlzJTIwaXMlMjB0aGUlMjBpbWFnZS10ZXh0JTIwc2ltaWxhcml0eSUyMHNjb3JlJTBBcHJvYnMlMjAlM0QlMjBsb2dpdHNfcGVyX2ltYWdlLnNvZnRtYXgoZGltJTNEMSklMjAlMjAlMjMlMjB3ZSUyMGNhbiUyMHRha2UlMjB0aGUlMjBzb2Z0bWF4JTIwdG8lMjBnZXQlMjB0aGUlMjBsYWJlbCUyMHByb2JhYmlsaXRpZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function No(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function Fo(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQU2VnTW9kZWwlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJDSURBUyUyRmNsaXBzZWctcmQ2NC1yZWZpbmVkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJDSURBUyUyRmNsaXBzZWctcmQ2NC1yZWZpbmVkJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBdGV4dF9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF90ZXh0X2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Eo(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function Qo(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdNb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBaW1hZ2VfZmVhdHVyZXMlMjAlM0QlMjBtb2RlbC5nZXRfaW1hZ2VfZmVhdHVyZXMoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Ho(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function qo(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQU2VnVGV4dE1vZGVsJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyQ0lEQVMlMkZjbGlwc2VnLXJkNjQtcmVmaW5lZCUyMiklMEFtb2RlbCUyMCUzRCUyMENMSVBTZWdUZXh0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGUlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBcG9vbGVkX291dHB1dCUyMCUzRCUyMG91dHB1dHMucG9vbGVyX291dHB1dCUyMCUyMCUyMyUyMHBvb2xlZCUyMChFT1MlMjB0b2tlbiklMjBzdGF0ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPSegTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegTextModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Xo(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function Go(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdWaXNpb25Nb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnVmlzaW9uTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQXBvb2xlZF9vdXRwdXQlMjAlM0QlMjBvdXRwdXRzLnBvb2xlcl9vdXRwdXQlMjAlMjAlMjMlMjBwb29sZWQlMjBDTFMlMjBzdGF0ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegVisionModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Ro(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function Do(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBDTElQU2VnRm9ySW1hZ2VTZWdtZW50YXRpb24lMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnRm9ySW1hZ2VTZWdtZW50YXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQXRleHRzJTIwJTNEJTIwJTVCJTIyYSUyMGNhdCUyMiUyQyUyMCUyMmElMjByZW1vdGUlMjIlMkMlMjAlMjJhJTIwYmxhbmtldCUyMiU1RCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEdGV4dHMlMkMlMjBpbWFnZXMlM0QlNUJpbWFnZSU1RCUyMColMjBsZW4odGV4dHMpJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cyUwQXByaW50KGxvZ2l0cy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegForImageSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;a cat&quot;</span>, <span class="hljs-string">&quot;a remote&quot;</span>, <span class="hljs-string">&quot;a blanket&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=[image] * <span class="hljs-built_in">len</span>(texts), padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(logits.shape)
torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">352</span>, <span class="hljs-number">352</span>])`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Ao(I){let n,C,r,a,b,t,T,Lt,ye,On=`CLIPSeg モデルは、Timo Lüddecke, Alexander Ecker によって <a href="https://arxiv.org/abs/2112.10003" rel="nofollow">Image Segmentation using Text and Image Prompts</a> で提案されました。
そしてアレクサンダー・エッカー。 CLIPSeg は、ゼロショットおよびワンショット画像セグメンテーションのために、凍結された <a href="clip">CLIP</a> モデルの上に最小限のデコーダを追加します。`,Jt,Ie,Kn="論文の要約は次のとおりです。",Zt,$e,eo=`<em>画像のセグメンテーションは通常、トレーニングによって解決されます。
オブジェクト クラスの固定セットのモデル。後で追加のクラスやより複雑なクエリを組み込むとコストがかかります
これらの式を含むデータセットでモデルを再トレーニングする必要があるためです。ここでシステムを提案します
任意の情報に基づいて画像セグメンテーションを生成できます。
テスト時にプロンプ​​トが表示されます。プロンプトはテキストまたは
画像。このアプローチにより、統一されたモデルを作成できます。
3 つの一般的なセグメンテーション タスクについて (1 回トレーニング済み)
参照式のセグメンテーション、ゼロショット セグメンテーション、ワンショット セグメンテーションという明確な課題が伴います。
CLIP モデルをバックボーンとして構築し、これをトランスベースのデコーダで拡張して、高密度なデータ通信を可能にします。
予測。の拡張バージョンでトレーニングした後、
PhraseCut データセット、私たちのシステムは、フリーテキスト プロンプトまたは
クエリを表す追加の画像。後者の画像ベースのプロンプトのさまざまなバリエーションを詳細に分析します。
この新しいハイブリッド入力により、動的適応が可能になります。
前述の 3 つのセグメンテーション タスクのみですが、
テキストまたは画像をクエリするバイナリ セグメンテーション タスクに
定式化することができる。最後に、システムがうまく適応していることがわかりました
アフォーダンスまたはプロパティを含む一般化されたクエリ</em>`,kt,te,to,Ut,ve,no='CLIPSeg の概要。 <a href="https://arxiv.org/abs/2112.10003">元の論文から抜粋。</a>',Wt,we,oo=`このモデルは、<a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a> によって提供されました。
元のコードは <a href="https://github.com/timojl/clipseg" rel="nofollow">ここ</a> にあります。`,zt,Pe,Bt,Se,so=`<li><a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> は、<a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> の上にデコーダを追加します。後者は <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a> と同じです。</li> <li><a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> は、テスト時に任意のプロンプトに基づいて画像セグメンテーションを生成できます。プロンプトはテキストのいずれかです
(<code>input_ids</code> としてモデルに提供される) または画像 (<code>conditional_pixel_values</code> としてモデルに提供される)。カスタムを提供することもできます
条件付き埋め込み (<code>conditional_embeddings</code>としてモデルに提供されます)。</li>`,Vt,xe,Nt,je,ao="CLIPSeg の使用を開始するのに役立つ、公式 Hugging Face およびコミュニティ (🌎 で示されている) リソースのリスト。ここに含めるリソースの送信に興味がある場合は、お気軽にプル リクエストを開いてください。審査させていただきます。リソースは、既存のリソースを複製するのではなく、何か新しいものを示すことが理想的です。",Ft,Le,Et,Je,ro='<li><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb" rel="nofollow">CLIPSeg を使用したゼロショット画像セグメンテーション</a> を説明するノートブック。</li>',Qt,Ze,Ht,P,ke,ln,dt,io=`<a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to
instantiate a CLIPSeg model according to the specified arguments, defining the text model and vision model configs.
Instantiating a configuration with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,dn,ct,lo=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,cn,ne,mn,oe,Ue,pn,mt,co=`Instantiate a <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> (or a derived class) from clipseg text model configuration and clipseg vision
model configuration.`,qt,We,Xt,L,ze,gn,pt,mo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,fn,gt,po=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,un,se,Gt,Be,Rt,J,Ve,hn,ft,go=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,_n,ut,fo=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Mn,ae,Dt,Ne,At,S,Fe,bn,ht,uo="Constructs a CLIPSeg processor which wraps a CLIPSeg image processor and a CLIP tokenizer into a single processor.",Cn,_t,ho=`<a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> offers all the functionalities of <code>ViTImageProcessor</code> and <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegProcessor.decode">decode()</a> for more information.`,Tn,re,Ee,yn,Mt,_o=`This method forwards all its arguments to CLIPTokenizerFast’s <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.batch_decode">batch_decode()</a>. Please
refer to the docstring of this method for more information.`,In,ie,Qe,$n,bt,Mo=`This method forwards all its arguments to CLIPTokenizerFast’s <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,Yt,He,Ot,x,qe,vn,Ct,bo=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,wn,z,Xe,Pn,Tt,Co='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> forward method, overrides the <code>__call__</code> special method.',Sn,le,xn,de,jn,B,Ge,Ln,yt,To='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> forward method, overrides the <code>__call__</code> special method.',Jn,ce,Zn,me,kn,V,Re,Un,It,yo='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> forward method, overrides the <code>__call__</code> special method.',Wn,pe,zn,ge,Kt,De,en,Y,Ae,Bn,N,Ye,Vn,$t,Io='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegTextModel">CLIPSegTextModel</a> forward method, overrides the <code>__call__</code> special method.',Nn,fe,Fn,ue,tn,Oe,nn,O,Ke,En,F,et,Qn,vt,$o='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionModel">CLIPSegVisionModel</a> forward method, overrides the <code>__call__</code> special method.',Hn,he,qn,_e,on,tt,sn,Z,nt,Xn,wt,vo="CLIPSeg model with a Transformer-based decoder on top for zero-shot and one-shot image segmentation.",Gn,Pt,wo=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Rn,E,ot,Dn,St,Po='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> forward method, overrides the <code>__call__</code> special method.',An,Me,Yn,be,an,jt,rn;return b=new Q({props:{title:"CLIPSeg",local:"clipseg",headingTag:"h1"}}),T=new Q({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Pe=new Q({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),xe=new Q({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Le=new ko({props:{pipeline:"image-segmentation"}}),Ze=new Q({props:{title:"CLIPSegConfig",local:"transformers.CLIPSegConfig",headingTag:"h2"}}),ke=new w({props:{name:"class transformers.CLIPSegConfig",anchor:"transformers.CLIPSegConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"extract_layers",val:" = [3, 6, 9]"},{name:"reduce_dim",val:" = 64"},{name:"decoder_num_attention_heads",val:" = 4"},{name:"decoder_attention_dropout",val:" = 0.0"},{name:"decoder_hidden_act",val:" = 'quick_gelu'"},{name:"decoder_intermediate_size",val:" = 2048"},{name:"conditional_layer",val:" = 0"},{name:"use_complex_transposed_convolution",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegTextConfig">CLIPSegTextConfig</a>.`,name:"text_config"},{anchor:"transformers.CLIPSegConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionConfig">CLIPSegVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.CLIPSegConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.CLIPSegConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original CLIPSeg implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.CLIPSegConfig.extract_layers",description:`<strong>extract_layers</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 6, 9]</code>) &#x2014;
Layers to extract when forwarding the query image through the frozen visual backbone of CLIP.`,name:"extract_layers"},{anchor:"transformers.CLIPSegConfig.reduce_dim",description:`<strong>reduce_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality to reduce the CLIP vision embedding.`,name:"reduce_dim"},{anchor:"transformers.CLIPSegConfig.decoder_num_attention_heads",description:`<strong>decoder_num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of attention heads in the decoder of CLIPSeg.`,name:"decoder_num_attention_heads"},{anchor:"transformers.CLIPSegConfig.decoder_attention_dropout",description:`<strong>decoder_attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"decoder_attention_dropout"},{anchor:"transformers.CLIPSegConfig.decoder_hidden_act",description:`<strong>decoder_hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"decoder_hidden_act"},{anchor:"transformers.CLIPSegConfig.decoder_intermediate_size",description:`<strong>decoder_intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layers in the Transformer decoder.`,name:"decoder_intermediate_size"},{anchor:"transformers.CLIPSegConfig.conditional_layer",description:`<strong>conditional_layer</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The layer to use of the Transformer encoder whose activations will be combined with the condition
embeddings using FiLM (Feature-wise Linear Modulation). If 0, the last layer is used.`,name:"conditional_layer"},{anchor:"transformers.CLIPSegConfig.use_complex_transposed_convolution",description:`<strong>use_complex_transposed_convolution</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a more complex transposed convolution in the decoder, enabling more fine-grained
segmentation.`,name:"use_complex_transposed_convolution"},{anchor:"transformers.CLIPSegConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L248"}}),ne=new K({props:{anchor:"transformers.CLIPSegConfig.example",$$slots:{default:[Uo]},$$scope:{ctx:I}}}),Ue=new w({props:{name:"from_text_vision_configs",anchor:"transformers.CLIPSegConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": CLIPSegTextConfig"},{name:"vision_config",val:": CLIPSegVisionConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L423",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig"
>CLIPSegConfig</a></p>
`}}),We=new Q({props:{title:"CLIPSegTextConfig",local:"transformers.CLIPSegTextConfig",headingTag:"h2"}}),ze=new w({props:{name:"class transformers.CLIPSegTextConfig",anchor:"transformers.CLIPSegTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 77"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the CLIPSeg text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>.`,name:"vocab_size"},{anchor:"transformers.CLIPSegTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPSegTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPSegTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPSegTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPSegTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 77) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.CLIPSegTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.CLIPSegTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.CLIPSegTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPSegTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPSegTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.CLIPSegTextConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.CLIPSegTextConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49406) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.CLIPSegTextConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49407) &#x2014;
End of stream token id.`,name:"eos_token_id"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L31"}}),se=new K({props:{anchor:"transformers.CLIPSegTextConfig.example",$$slots:{default:[Wo]},$$scope:{ctx:I}}}),Be=new Q({props:{title:"CLIPSegVisionConfig",local:"transformers.CLIPSegVisionConfig",headingTag:"h2"}}),Ve=new w({props:{name:"class transformers.CLIPSegVisionConfig",anchor:"transformers.CLIPSegVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPSegVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPSegVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPSegVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPSegVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.CLIPSegVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.CLIPSegVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.CLIPSegVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.CLIPSegVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.CLIPSegVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPSegVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPSegVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L143"}}),ae=new K({props:{anchor:"transformers.CLIPSegVisionConfig.example",$$slots:{default:[zo]},$$scope:{ctx:I}}}),Ne=new Q({props:{title:"CLIPSegProcessor",local:"transformers.CLIPSegProcessor",headingTag:"h2"}}),Fe=new w({props:{name:"class transformers.CLIPSegProcessor",anchor:"transformers.CLIPSegProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegProcessor.image_processor",description:`<strong>image_processor</strong> (<code>ViTImageProcessor</code>, <em>optional</em>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.CLIPSegProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L25"}}),Ee=new w({props:{name:"batch_decode",anchor:"transformers.CLIPSegProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L134"}}),Qe=new w({props:{name:"decode",anchor:"transformers.CLIPSegProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L141"}}),He=new Q({props:{title:"CLIPSegModel",local:"transformers.CLIPSegModel",headingTag:"h2"}}),qe=new w({props:{name:"class transformers.CLIPSegModel",anchor:"transformers.CLIPSegModel",parameters:[{name:"config",val:": CLIPSegConfig"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L924"}}),Xe=new w({props:{name:"forward",anchor:"transformers.CLIPSegModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"return_loss",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.CLIPSegModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1056",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) — Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) — The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) — The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) — The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>image_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) — The image embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
<li><strong>text_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>vision_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),le=new xt({props:{$$slots:{default:[Bo]},$$scope:{ctx:I}}}),de=new K({props:{anchor:"transformers.CLIPSegModel.forward.example",$$slots:{default:[Vo]},$$scope:{ctx:I}}}),Ge=new w({props:{name:"get_text_features",anchor:"transformers.CLIPSegModel.get_text_features",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L960",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),ce=new xt({props:{$$slots:{default:[No]},$$scope:{ctx:I}}}),me=new K({props:{anchor:"transformers.CLIPSegModel.get_text_features.example",$$slots:{default:[Fo]},$$scope:{ctx:I}}}),Re=new w({props:{name:"get_image_features",anchor:"transformers.CLIPSegModel.get_image_features",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1007",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),pe=new xt({props:{$$slots:{default:[Eo]},$$scope:{ctx:I}}}),ge=new K({props:{anchor:"transformers.CLIPSegModel.get_image_features.example",$$slots:{default:[Qo]},$$scope:{ctx:I}}}),De=new Q({props:{title:"CLIPSegTextModel",local:"transformers.CLIPSegTextModel",headingTag:"h2"}}),Ae=new w({props:{name:"class transformers.CLIPSegTextModel",anchor:"transformers.CLIPSegTextModel",parameters:[{name:"config",val:": CLIPSegTextConfig"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L757"}}),Ye=new w({props:{name:"forward",anchor:"transformers.CLIPSegTextModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L774",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new xt({props:{$$slots:{default:[Ho]},$$scope:{ctx:I}}}),ue=new K({props:{anchor:"transformers.CLIPSegTextModel.forward.example",$$slots:{default:[qo]},$$scope:{ctx:I}}}),Oe=new Q({props:{title:"CLIPSegVisionModel",local:"transformers.CLIPSegVisionModel",headingTag:"h2"}}),Ke=new w({props:{name:"class transformers.CLIPSegVisionModel",anchor:"transformers.CLIPSegVisionModel",parameters:[{name:"config",val:": CLIPSegVisionConfig"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L872"}}),et=new w({props:{name:"forward",anchor:"transformers.CLIPSegVisionModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L885",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),he=new xt({props:{$$slots:{default:[Xo]},$$scope:{ctx:I}}}),_e=new K({props:{anchor:"transformers.CLIPSegVisionModel.forward.example",$$slots:{default:[Go]},$$scope:{ctx:I}}}),tt=new Q({props:{title:"CLIPSegForImageSegmentation",local:"transformers.CLIPSegForImageSegmentation",headingTag:"h2"}}),nt=new w({props:{name:"class transformers.CLIPSegForImageSegmentation",anchor:"transformers.CLIPSegForImageSegmentation",parameters:[{name:"config",val:": CLIPSegConfig"}],parametersDescription:[{anchor:"transformers.CLIPSegForImageSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1307"}}),ot=new w({props:{name:"forward",anchor:"transformers.CLIPSegForImageSegmentation.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"conditional_pixel_values",val:": Optional = None"},{name:"conditional_embeddings",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegForImageSegmentation.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1358",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) — Contrastive loss for image-text similarity.
…</li>
<li><strong>vision_model_output</strong> (<code>BaseModelOutputWithPooling</code>) — The output of the <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Me=new xt({props:{$$slots:{default:[Ro]},$$scope:{ctx:I}}}),be=new K({props:{anchor:"transformers.CLIPSegForImageSegmentation.forward.example",$$slots:{default:[Do]},$$scope:{ctx:I}}}),{c(){n=m("meta"),C=i(),r=m("p"),a=i(),g(b.$$.fragment),t=i(),g(T.$$.fragment),Lt=i(),ye=m("p"),ye.innerHTML=On,Jt=i(),Ie=m("p"),Ie.textContent=Kn,Zt=i(),$e=m("p"),$e.innerHTML=eo,kt=i(),te=m("img"),Ut=i(),ve=m("small"),ve.innerHTML=no,Wt=i(),we=m("p"),we.innerHTML=oo,zt=i(),g(Pe.$$.fragment),Bt=i(),Se=m("ul"),Se.innerHTML=so,Vt=i(),g(xe.$$.fragment),Nt=i(),je=m("p"),je.textContent=ao,Ft=i(),g(Le.$$.fragment),Et=i(),Je=m("ul"),Je.innerHTML=ro,Qt=i(),g(Ze.$$.fragment),Ht=i(),P=m("div"),g(ke.$$.fragment),ln=i(),dt=m("p"),dt.innerHTML=io,dn=i(),ct=m("p"),ct.innerHTML=lo,cn=i(),g(ne.$$.fragment),mn=i(),oe=m("div"),g(Ue.$$.fragment),pn=i(),mt=m("p"),mt.innerHTML=co,qt=i(),g(We.$$.fragment),Xt=i(),L=m("div"),g(ze.$$.fragment),gn=i(),pt=m("p"),pt.innerHTML=mo,fn=i(),gt=m("p"),gt.innerHTML=po,un=i(),g(se.$$.fragment),Gt=i(),g(Be.$$.fragment),Rt=i(),J=m("div"),g(Ve.$$.fragment),hn=i(),ft=m("p"),ft.innerHTML=go,_n=i(),ut=m("p"),ut.innerHTML=fo,Mn=i(),g(ae.$$.fragment),Dt=i(),g(Ne.$$.fragment),At=i(),S=m("div"),g(Fe.$$.fragment),bn=i(),ht=m("p"),ht.textContent=uo,Cn=i(),_t=m("p"),_t.innerHTML=ho,Tn=i(),re=m("div"),g(Ee.$$.fragment),yn=i(),Mt=m("p"),Mt.innerHTML=_o,In=i(),ie=m("div"),g(Qe.$$.fragment),$n=i(),bt=m("p"),bt.innerHTML=Mo,Yt=i(),g(He.$$.fragment),Ot=i(),x=m("div"),g(qe.$$.fragment),vn=i(),Ct=m("p"),Ct.innerHTML=bo,wn=i(),z=m("div"),g(Xe.$$.fragment),Pn=i(),Tt=m("p"),Tt.innerHTML=Co,Sn=i(),g(le.$$.fragment),xn=i(),g(de.$$.fragment),jn=i(),B=m("div"),g(Ge.$$.fragment),Ln=i(),yt=m("p"),yt.innerHTML=To,Jn=i(),g(ce.$$.fragment),Zn=i(),g(me.$$.fragment),kn=i(),V=m("div"),g(Re.$$.fragment),Un=i(),It=m("p"),It.innerHTML=yo,Wn=i(),g(pe.$$.fragment),zn=i(),g(ge.$$.fragment),Kt=i(),g(De.$$.fragment),en=i(),Y=m("div"),g(Ae.$$.fragment),Bn=i(),N=m("div"),g(Ye.$$.fragment),Vn=i(),$t=m("p"),$t.innerHTML=Io,Nn=i(),g(fe.$$.fragment),Fn=i(),g(ue.$$.fragment),tn=i(),g(Oe.$$.fragment),nn=i(),O=m("div"),g(Ke.$$.fragment),En=i(),F=m("div"),g(et.$$.fragment),Qn=i(),vt=m("p"),vt.innerHTML=$o,Hn=i(),g(he.$$.fragment),qn=i(),g(_e.$$.fragment),on=i(),g(tt.$$.fragment),sn=i(),Z=m("div"),g(nt.$$.fragment),Xn=i(),wt=m("p"),wt.textContent=vo,Gn=i(),Pt=m("p"),Pt.innerHTML=wo,Rn=i(),E=m("div"),g(ot.$$.fragment),Dn=i(),St=m("p"),St.innerHTML=Po,An=i(),g(Me.$$.fragment),Yn=i(),g(be.$$.fragment),an=i(),jt=m("p"),this.h()},l(e){const o=Zo("svelte-u9bgzb",document.head);n=p(o,"META",{name:!0,content:!0}),o.forEach(s),C=l(e),r=p(e,"P",{}),v(r).forEach(s),a=l(e),f(b.$$.fragment,e),t=l(e),f(T.$$.fragment,e),Lt=l(e),ye=p(e,"P",{"data-svelte-h":!0}),y(ye)!=="svelte-11pq5bj"&&(ye.innerHTML=On),Jt=l(e),Ie=p(e,"P",{"data-svelte-h":!0}),y(Ie)!=="svelte-1cv3nri"&&(Ie.textContent=Kn),Zt=l(e),$e=p(e,"P",{"data-svelte-h":!0}),y($e)!=="svelte-ulr7co"&&($e.innerHTML=eo),kt=l(e),te=p(e,"IMG",{src:!0,alt:!0,width:!0}),Ut=l(e),ve=p(e,"SMALL",{"data-svelte-h":!0}),y(ve)!=="svelte-16azl3n"&&(ve.innerHTML=no),Wt=l(e),we=p(e,"P",{"data-svelte-h":!0}),y(we)!=="svelte-5uei6k"&&(we.innerHTML=oo),zt=l(e),f(Pe.$$.fragment,e),Bt=l(e),Se=p(e,"UL",{"data-svelte-h":!0}),y(Se)!=="svelte-1ahyu4x"&&(Se.innerHTML=so),Vt=l(e),f(xe.$$.fragment,e),Nt=l(e),je=p(e,"P",{"data-svelte-h":!0}),y(je)!=="svelte-163rb1e"&&(je.textContent=ao),Ft=l(e),f(Le.$$.fragment,e),Et=l(e),Je=p(e,"UL",{"data-svelte-h":!0}),y(Je)!=="svelte-46rblz"&&(Je.innerHTML=ro),Qt=l(e),f(Ze.$$.fragment,e),Ht=l(e),P=p(e,"DIV",{class:!0});var k=v(P);f(ke.$$.fragment,k),ln=l(k),dt=p(k,"P",{"data-svelte-h":!0}),y(dt)!=="svelte-15m9iib"&&(dt.innerHTML=io),dn=l(k),ct=p(k,"P",{"data-svelte-h":!0}),y(ct)!=="svelte-1s6wgpv"&&(ct.innerHTML=lo),cn=l(k),f(ne.$$.fragment,k),mn=l(k),oe=p(k,"DIV",{class:!0});var st=v(oe);f(Ue.$$.fragment,st),pn=l(st),mt=p(st,"P",{"data-svelte-h":!0}),y(mt)!=="svelte-8hi51v"&&(mt.innerHTML=co),st.forEach(s),k.forEach(s),qt=l(e),f(We.$$.fragment,e),Xt=l(e),L=p(e,"DIV",{class:!0});var H=v(L);f(ze.$$.fragment,H),gn=l(H),pt=p(H,"P",{"data-svelte-h":!0}),y(pt)!=="svelte-ug4hle"&&(pt.innerHTML=mo),fn=l(H),gt=p(H,"P",{"data-svelte-h":!0}),y(gt)!=="svelte-1s6wgpv"&&(gt.innerHTML=po),un=l(H),f(se.$$.fragment,H),H.forEach(s),Gt=l(e),f(Be.$$.fragment,e),Rt=l(e),J=p(e,"DIV",{class:!0});var q=v(J);f(Ve.$$.fragment,q),hn=l(q),ft=p(q,"P",{"data-svelte-h":!0}),y(ft)!=="svelte-ug4hle"&&(ft.innerHTML=go),_n=l(q),ut=p(q,"P",{"data-svelte-h":!0}),y(ut)!=="svelte-1s6wgpv"&&(ut.innerHTML=fo),Mn=l(q),f(ae.$$.fragment,q),q.forEach(s),Dt=l(e),f(Ne.$$.fragment,e),At=l(e),S=p(e,"DIV",{class:!0});var U=v(S);f(Fe.$$.fragment,U),bn=l(U),ht=p(U,"P",{"data-svelte-h":!0}),y(ht)!=="svelte-9hszhx"&&(ht.textContent=uo),Cn=l(U),_t=p(U,"P",{"data-svelte-h":!0}),y(_t)!=="svelte-1w3xqxn"&&(_t.innerHTML=ho),Tn=l(U),re=p(U,"DIV",{class:!0});var at=v(re);f(Ee.$$.fragment,at),yn=l(at),Mt=p(at,"P",{"data-svelte-h":!0}),y(Mt)!=="svelte-vld6ul"&&(Mt.innerHTML=_o),at.forEach(s),In=l(U),ie=p(U,"DIV",{class:!0});var rt=v(ie);f(Qe.$$.fragment,rt),$n=l(rt),bt=p(rt,"P",{"data-svelte-h":!0}),y(bt)!=="svelte-1ovp6q3"&&(bt.innerHTML=Mo),rt.forEach(s),U.forEach(s),Yt=l(e),f(He.$$.fragment,e),Ot=l(e),x=p(e,"DIV",{class:!0});var W=v(x);f(qe.$$.fragment,W),vn=l(W),Ct=p(W,"P",{"data-svelte-h":!0}),y(Ct)!=="svelte-1gjh92c"&&(Ct.innerHTML=bo),wn=l(W),z=p(W,"DIV",{class:!0});var X=v(z);f(Xe.$$.fragment,X),Pn=l(X),Tt=p(X,"P",{"data-svelte-h":!0}),y(Tt)!=="svelte-552b0y"&&(Tt.innerHTML=Co),Sn=l(X),f(le.$$.fragment,X),xn=l(X),f(de.$$.fragment,X),X.forEach(s),jn=l(W),B=p(W,"DIV",{class:!0});var G=v(B);f(Ge.$$.fragment,G),Ln=l(G),yt=p(G,"P",{"data-svelte-h":!0}),y(yt)!=="svelte-552b0y"&&(yt.innerHTML=To),Jn=l(G),f(ce.$$.fragment,G),Zn=l(G),f(me.$$.fragment,G),G.forEach(s),kn=l(W),V=p(W,"DIV",{class:!0});var R=v(V);f(Re.$$.fragment,R),Un=l(R),It=p(R,"P",{"data-svelte-h":!0}),y(It)!=="svelte-552b0y"&&(It.innerHTML=yo),Wn=l(R),f(pe.$$.fragment,R),zn=l(R),f(ge.$$.fragment,R),R.forEach(s),W.forEach(s),Kt=l(e),f(De.$$.fragment,e),en=l(e),Y=p(e,"DIV",{class:!0});var it=v(Y);f(Ae.$$.fragment,it),Bn=l(it),N=p(it,"DIV",{class:!0});var D=v(N);f(Ye.$$.fragment,D),Vn=l(D),$t=p(D,"P",{"data-svelte-h":!0}),y($t)!=="svelte-1vuc6nc"&&($t.innerHTML=Io),Nn=l(D),f(fe.$$.fragment,D),Fn=l(D),f(ue.$$.fragment,D),D.forEach(s),it.forEach(s),tn=l(e),f(Oe.$$.fragment,e),nn=l(e),O=p(e,"DIV",{class:!0});var lt=v(O);f(Ke.$$.fragment,lt),En=l(lt),F=p(lt,"DIV",{class:!0});var A=v(F);f(et.$$.fragment,A),Qn=l(A),vt=p(A,"P",{"data-svelte-h":!0}),y(vt)!=="svelte-1cjdav6"&&(vt.innerHTML=$o),Hn=l(A),f(he.$$.fragment,A),qn=l(A),f(_e.$$.fragment,A),A.forEach(s),lt.forEach(s),on=l(e),f(tt.$$.fragment,e),sn=l(e),Z=p(e,"DIV",{class:!0});var Ce=v(Z);f(nt.$$.fragment,Ce),Xn=l(Ce),wt=p(Ce,"P",{"data-svelte-h":!0}),y(wt)!=="svelte-qndran"&&(wt.textContent=vo),Gn=l(Ce),Pt=p(Ce,"P",{"data-svelte-h":!0}),y(Pt)!=="svelte-1gjh92c"&&(Pt.innerHTML=wo),Rn=l(Ce),E=p(Ce,"DIV",{class:!0});var Te=v(E);f(ot.$$.fragment,Te),Dn=l(Te),St=p(Te,"P",{"data-svelte-h":!0}),y(St)!=="svelte-pqt806"&&(St.innerHTML=Po),An=l(Te),f(Me.$$.fragment,Te),Yn=l(Te),f(be.$$.fragment,Te),Te.forEach(s),Ce.forEach(s),an=l(e),jt=p(e,"P",{}),v(jt).forEach(s),this.h()},h(){$(n,"name","hf:doc:metadata"),$(n,"content",Yo),xo(te.src,to="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png")||$(te,"src",to),$(te,"alt","描画"),$(te,"width","600"),$(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){c(document.head,n),d(e,C,o),d(e,r,o),d(e,a,o),u(b,e,o),d(e,t,o),u(T,e,o),d(e,Lt,o),d(e,ye,o),d(e,Jt,o),d(e,Ie,o),d(e,Zt,o),d(e,$e,o),d(e,kt,o),d(e,te,o),d(e,Ut,o),d(e,ve,o),d(e,Wt,o),d(e,we,o),d(e,zt,o),u(Pe,e,o),d(e,Bt,o),d(e,Se,o),d(e,Vt,o),u(xe,e,o),d(e,Nt,o),d(e,je,o),d(e,Ft,o),u(Le,e,o),d(e,Et,o),d(e,Je,o),d(e,Qt,o),u(Ze,e,o),d(e,Ht,o),d(e,P,o),u(ke,P,null),c(P,ln),c(P,dt),c(P,dn),c(P,ct),c(P,cn),u(ne,P,null),c(P,mn),c(P,oe),u(Ue,oe,null),c(oe,pn),c(oe,mt),d(e,qt,o),u(We,e,o),d(e,Xt,o),d(e,L,o),u(ze,L,null),c(L,gn),c(L,pt),c(L,fn),c(L,gt),c(L,un),u(se,L,null),d(e,Gt,o),u(Be,e,o),d(e,Rt,o),d(e,J,o),u(Ve,J,null),c(J,hn),c(J,ft),c(J,_n),c(J,ut),c(J,Mn),u(ae,J,null),d(e,Dt,o),u(Ne,e,o),d(e,At,o),d(e,S,o),u(Fe,S,null),c(S,bn),c(S,ht),c(S,Cn),c(S,_t),c(S,Tn),c(S,re),u(Ee,re,null),c(re,yn),c(re,Mt),c(S,In),c(S,ie),u(Qe,ie,null),c(ie,$n),c(ie,bt),d(e,Yt,o),u(He,e,o),d(e,Ot,o),d(e,x,o),u(qe,x,null),c(x,vn),c(x,Ct),c(x,wn),c(x,z),u(Xe,z,null),c(z,Pn),c(z,Tt),c(z,Sn),u(le,z,null),c(z,xn),u(de,z,null),c(x,jn),c(x,B),u(Ge,B,null),c(B,Ln),c(B,yt),c(B,Jn),u(ce,B,null),c(B,Zn),u(me,B,null),c(x,kn),c(x,V),u(Re,V,null),c(V,Un),c(V,It),c(V,Wn),u(pe,V,null),c(V,zn),u(ge,V,null),d(e,Kt,o),u(De,e,o),d(e,en,o),d(e,Y,o),u(Ae,Y,null),c(Y,Bn),c(Y,N),u(Ye,N,null),c(N,Vn),c(N,$t),c(N,Nn),u(fe,N,null),c(N,Fn),u(ue,N,null),d(e,tn,o),u(Oe,e,o),d(e,nn,o),d(e,O,o),u(Ke,O,null),c(O,En),c(O,F),u(et,F,null),c(F,Qn),c(F,vt),c(F,Hn),u(he,F,null),c(F,qn),u(_e,F,null),d(e,on,o),u(tt,e,o),d(e,sn,o),d(e,Z,o),u(nt,Z,null),c(Z,Xn),c(Z,wt),c(Z,Gn),c(Z,Pt),c(Z,Rn),c(Z,E),u(ot,E,null),c(E,Dn),c(E,St),c(E,An),u(Me,E,null),c(E,Yn),u(be,E,null),d(e,an,o),d(e,jt,o),rn=!0},p(e,[o]){const k={};o&2&&(k.$$scope={dirty:o,ctx:e}),ne.$set(k);const st={};o&2&&(st.$$scope={dirty:o,ctx:e}),se.$set(st);const H={};o&2&&(H.$$scope={dirty:o,ctx:e}),ae.$set(H);const q={};o&2&&(q.$$scope={dirty:o,ctx:e}),le.$set(q);const U={};o&2&&(U.$$scope={dirty:o,ctx:e}),de.$set(U);const at={};o&2&&(at.$$scope={dirty:o,ctx:e}),ce.$set(at);const rt={};o&2&&(rt.$$scope={dirty:o,ctx:e}),me.$set(rt);const W={};o&2&&(W.$$scope={dirty:o,ctx:e}),pe.$set(W);const X={};o&2&&(X.$$scope={dirty:o,ctx:e}),ge.$set(X);const G={};o&2&&(G.$$scope={dirty:o,ctx:e}),fe.$set(G);const R={};o&2&&(R.$$scope={dirty:o,ctx:e}),ue.$set(R);const it={};o&2&&(it.$$scope={dirty:o,ctx:e}),he.$set(it);const D={};o&2&&(D.$$scope={dirty:o,ctx:e}),_e.$set(D);const lt={};o&2&&(lt.$$scope={dirty:o,ctx:e}),Me.$set(lt);const A={};o&2&&(A.$$scope={dirty:o,ctx:e}),be.$set(A)},i(e){rn||(h(b.$$.fragment,e),h(T.$$.fragment,e),h(Pe.$$.fragment,e),h(xe.$$.fragment,e),h(Le.$$.fragment,e),h(Ze.$$.fragment,e),h(ke.$$.fragment,e),h(ne.$$.fragment,e),h(Ue.$$.fragment,e),h(We.$$.fragment,e),h(ze.$$.fragment,e),h(se.$$.fragment,e),h(Be.$$.fragment,e),h(Ve.$$.fragment,e),h(ae.$$.fragment,e),h(Ne.$$.fragment,e),h(Fe.$$.fragment,e),h(Ee.$$.fragment,e),h(Qe.$$.fragment,e),h(He.$$.fragment,e),h(qe.$$.fragment,e),h(Xe.$$.fragment,e),h(le.$$.fragment,e),h(de.$$.fragment,e),h(Ge.$$.fragment,e),h(ce.$$.fragment,e),h(me.$$.fragment,e),h(Re.$$.fragment,e),h(pe.$$.fragment,e),h(ge.$$.fragment,e),h(De.$$.fragment,e),h(Ae.$$.fragment,e),h(Ye.$$.fragment,e),h(fe.$$.fragment,e),h(ue.$$.fragment,e),h(Oe.$$.fragment,e),h(Ke.$$.fragment,e),h(et.$$.fragment,e),h(he.$$.fragment,e),h(_e.$$.fragment,e),h(tt.$$.fragment,e),h(nt.$$.fragment,e),h(ot.$$.fragment,e),h(Me.$$.fragment,e),h(be.$$.fragment,e),rn=!0)},o(e){_(b.$$.fragment,e),_(T.$$.fragment,e),_(Pe.$$.fragment,e),_(xe.$$.fragment,e),_(Le.$$.fragment,e),_(Ze.$$.fragment,e),_(ke.$$.fragment,e),_(ne.$$.fragment,e),_(Ue.$$.fragment,e),_(We.$$.fragment,e),_(ze.$$.fragment,e),_(se.$$.fragment,e),_(Be.$$.fragment,e),_(Ve.$$.fragment,e),_(ae.$$.fragment,e),_(Ne.$$.fragment,e),_(Fe.$$.fragment,e),_(Ee.$$.fragment,e),_(Qe.$$.fragment,e),_(He.$$.fragment,e),_(qe.$$.fragment,e),_(Xe.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(Ge.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(Re.$$.fragment,e),_(pe.$$.fragment,e),_(ge.$$.fragment,e),_(De.$$.fragment,e),_(Ae.$$.fragment,e),_(Ye.$$.fragment,e),_(fe.$$.fragment,e),_(ue.$$.fragment,e),_(Oe.$$.fragment,e),_(Ke.$$.fragment,e),_(et.$$.fragment,e),_(he.$$.fragment,e),_(_e.$$.fragment,e),_(tt.$$.fragment,e),_(nt.$$.fragment,e),_(ot.$$.fragment,e),_(Me.$$.fragment,e),_(be.$$.fragment,e),rn=!1},d(e){e&&(s(C),s(r),s(a),s(t),s(Lt),s(ye),s(Jt),s(Ie),s(Zt),s($e),s(kt),s(te),s(Ut),s(ve),s(Wt),s(we),s(zt),s(Bt),s(Se),s(Vt),s(Nt),s(je),s(Ft),s(Et),s(Je),s(Qt),s(Ht),s(P),s(qt),s(Xt),s(L),s(Gt),s(Rt),s(J),s(Dt),s(At),s(S),s(Yt),s(Ot),s(x),s(Kt),s(en),s(Y),s(tn),s(nn),s(O),s(on),s(sn),s(Z),s(an),s(jt)),s(n),M(b,e),M(T,e),M(Pe,e),M(xe,e),M(Le,e),M(Ze,e),M(ke),M(ne),M(Ue),M(We,e),M(ze),M(se),M(Be,e),M(Ve),M(ae),M(Ne,e),M(Fe),M(Ee),M(Qe),M(He,e),M(qe),M(Xe),M(le),M(de),M(Ge),M(ce),M(me),M(Re),M(pe),M(ge),M(De,e),M(Ae),M(Ye),M(fe),M(ue),M(Oe,e),M(Ke),M(et),M(he),M(_e),M(tt,e),M(nt),M(ot),M(Me),M(be)}}}const Yo='{"title":"CLIPSeg","local":"clipseg","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"CLIPSegConfig","local":"transformers.CLIPSegConfig","sections":[],"depth":2},{"title":"CLIPSegTextConfig","local":"transformers.CLIPSegTextConfig","sections":[],"depth":2},{"title":"CLIPSegVisionConfig","local":"transformers.CLIPSegVisionConfig","sections":[],"depth":2},{"title":"CLIPSegProcessor","local":"transformers.CLIPSegProcessor","sections":[],"depth":2},{"title":"CLIPSegModel","local":"transformers.CLIPSegModel","sections":[],"depth":2},{"title":"CLIPSegTextModel","local":"transformers.CLIPSegTextModel","sections":[],"depth":2},{"title":"CLIPSegVisionModel","local":"transformers.CLIPSegVisionModel","sections":[],"depth":2},{"title":"CLIPSegForImageSegmentation","local":"transformers.CLIPSegForImageSegmentation","sections":[],"depth":2}],"depth":1}';function Oo(I){return jo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class is extends Lo{constructor(n){super(),Jo(this,n,Oo,Ao,So,{})}}export{is as component};
