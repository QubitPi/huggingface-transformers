import{s as So,f as xo,o as jo,n as j}from"../chunks/scheduler.9bc65507.js";import{S as Lo,i as Jo,g as m,s as i,r as g,A as Zo,h as p,f as s,c as l,j as v,u as f,x as y,k as $,y as c,a as d,v as u,d as h,t as _,w as M}from"../chunks/index.707bf1b6.js";import{T as xt}from"../chunks/Tip.c2ecdbf4.js";import{D as w}from"../chunks/Docstring.17db21ae.js";import{C as ee}from"../chunks/CodeBlock.54a9f38d.js";import{E as K}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as ko}from"../chunks/PipelineTag.44585822.js";import{H as Q}from"../chunks/Heading.342b1fa6.js";function Uo(I){let n,C="Example:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdDb25maWclMkMlMjBDTElQU2VnTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ0NvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ01vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ01vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZyUwQSUwQSUyMyUyMFdlJTIwY2FuJTIwYWxzbyUyMGluaXRpYWxpemUlMjBhJTIwQ0xJUFNlZ0NvbmZpZyUyMGZyb20lMjBhJTIwQ0xJUFNlZ1RleHRDb25maWclMjBhbmQlMjBhJTIwQ0xJUFNlZ1Zpc2lvbkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQU2VnVGV4dCUyMGFuZCUyMENMSVBTZWdWaXNpb24lMjBjb25maWd1cmF0aW9uJTBBY29uZmlnX3RleHQlMjAlM0QlMjBDTElQU2VnVGV4dENvbmZpZygpJTBBY29uZmlnX3Zpc2lvbiUyMCUzRCUyMENMSVBTZWdWaXNpb25Db25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMENMSVBTZWdDb25maWcuZnJvbV90ZXh0X3Zpc2lvbl9jb25maWdzKGNvbmZpZ190ZXh0JTJDJTIwY29uZmlnX3Zpc2lvbik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegConfig, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a CLIPSegConfig from a CLIPSegTextConfig and a CLIPSegVisionConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegText and CLIPSegVision configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = CLIPSegTextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = CLIPSegVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = CLIPSegConfig.from_text_vision_configs(config_text, config_vision)`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Wo(I){let n,C="Example:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdUZXh0Q29uZmlnJTJDJTIwQ0xJUFNlZ1RleHRNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQU2VnVGV4dENvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnVGV4dENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMENMSVBTZWdUZXh0TW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMENJREFTJTJGY2xpcHNlZy1yZDY0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnVGV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegTextConfig, CLIPSegTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegTextConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegTextModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function zo(I){let n,C="Example:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBTZWdWaXNpb25Db25maWclMkMlMjBDTElQU2VnVmlzaW9uTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ1Zpc2lvbkNvbmZpZyUyMHdpdGglMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBDTElQU2VnVmlzaW9uQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFNlZ1Zpc2lvbk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBDSURBUyUyRmNsaXBzZWctcmQ2NCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ1Zpc2lvbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPSegVisionConfig, CLIPSegVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegVisionConfig with CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPSegVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPSegVisionModel (with random weights) from the CIDAS/clipseg-rd64 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Bo(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function Vo(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdNb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMEElMjAlMjAlMjAlMjB0ZXh0JTNEJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyJTJDJTIwcGFkZGluZyUzRFRydWUlMEEpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0c19wZXJfaW1hZ2UlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0c19wZXJfaW1hZ2UlMjAlMjAlMjMlMjB0aGlzJTIwaXMlMjB0aGUlMjBpbWFnZS10ZXh0JTIwc2ltaWxhcml0eSUyMHNjb3JlJTBBcHJvYnMlMjAlM0QlMjBsb2dpdHNfcGVyX2ltYWdlLnNvZnRtYXgoZGltJTNEMSklMjAlMjAlMjMlMjB3ZSUyMGNhbiUyMHRha2UlMjB0aGUlMjBzb2Z0bWF4JTIwdG8lMjBnZXQlMjB0aGUlMjBsYWJlbCUyMHByb2JhYmlsaXRpZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function No(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function Fo(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQU2VnTW9kZWwlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJDSURBUyUyRmNsaXBzZWctcmQ2NC1yZWZpbmVkJTIyKSUwQW1vZGVsJTIwJTNEJTIwQ0xJUFNlZ01vZGVsLmZyb21fcHJldHJhaW5lZCglMjJDSURBUyUyRmNsaXBzZWctcmQ2NC1yZWZpbmVkJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBdGV4dF9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF90ZXh0X2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Eo(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function Qo(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdNb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBaW1hZ2VfZmVhdHVyZXMlMjAlM0QlMjBtb2RlbC5nZXRfaW1hZ2VfZmVhdHVyZXMoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Ho(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function qo(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQU2VnVGV4dE1vZGVsJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyQ0lEQVMlMkZjbGlwc2VnLXJkNjQtcmVmaW5lZCUyMiklMEFtb2RlbCUyMCUzRCUyMENMSVBTZWdUZXh0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGUlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBcG9vbGVkX291dHB1dCUyMCUzRCUyMG91dHB1dHMucG9vbGVyX291dHB1dCUyMCUyMCUyMyUyMHBvb2xlZCUyMChFT1MlMjB0b2tlbiklMjBzdGF0ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPSegTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegTextModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Xo(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function Go(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBTZWdWaXNpb25Nb2RlbCUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnVmlzaW9uTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQXBvb2xlZF9vdXRwdXQlMjAlM0QlMjBvdXRwdXRzLnBvb2xlcl9vdXRwdXQlMjAlMjAlMjMlMjBwb29sZWQlMjBDTFMlMjBzdGF0ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegVisionModel.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Ro(I){let n,C=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=C},l(r){n=p(r,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=C)},m(r,a){d(r,n,a)},p:j,d(r){r&&s(n)}}}function Do(I){let n,C="Examples:",r,a,b;return a=new ee({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBDTElQU2VnRm9ySW1hZ2VTZWdtZW50YXRpb24lMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDTElQU2VnRm9ySW1hZ2VTZWdtZW50YXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMkNJREFTJTJGY2xpcHNlZy1yZDY0LXJlZmluZWQlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQXRleHRzJTIwJTNEJTIwJTVCJTIyYSUyMGNhdCUyMiUyQyUyMCUyMmElMjByZW1vdGUlMjIlMkMlMjAlMjJhJTIwYmxhbmtldCUyMiU1RCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3Nvcih0ZXh0JTNEdGV4dHMlMkMlMjBpbWFnZXMlM0QlNUJpbWFnZSU1RCUyMColMjBsZW4odGV4dHMpJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cyUwQXByaW50KGxvZ2l0cy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPSegForImageSegmentation
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPSegForImageSegmentation.from_pretrained(<span class="hljs-string">&quot;CIDAS/clipseg-rd64-refined&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;a cat&quot;</span>, <span class="hljs-string">&quot;a remote&quot;</span>, <span class="hljs-string">&quot;a blanket&quot;</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, images=[image] * <span class="hljs-built_in">len</span>(texts), padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(logits.shape)
torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">352</span>, <span class="hljs-number">352</span>])`,wrap:!1}}),{c(){n=m("p"),n.textContent=C,r=i(),g(a.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=C),r=l(t),f(a.$$.fragment,t)},m(t,T){d(t,n,T),d(t,r,T),u(a,t,T),b=!0},p:j,i(t){b||(h(a.$$.fragment,t),b=!0)},o(t){_(a.$$.fragment,t),b=!1},d(t){t&&(s(n),s(r)),M(a,t)}}}function Ao(I){let n,C,r,a,b,t,T,Lt,ye,On=`CLIPSeg „É¢„Éá„É´„ÅØ„ÄÅTimo L√ºddecke, Alexander Ecker „Å´„Çà„Å£„Å¶ <a href="https://arxiv.org/abs/2112.10003" rel="nofollow">Image Segmentation using Text and Image Prompts</a> „ÅßÊèêÊ°à„Åï„Çå„Åæ„Åó„Åü„ÄÇ
„Åù„Åó„Å¶„Ç¢„É¨„ÇØ„Çµ„É≥„ÉÄ„Éº„Éª„Ç®„ÉÉ„Ç´„Éº„ÄÇ CLIPSeg „ÅØ„ÄÅ„Çº„É≠„Ç∑„Éß„ÉÉ„Éà„Åä„Çà„Å≥„ÉØ„É≥„Ç∑„Éß„ÉÉ„ÉàÁîªÂÉè„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„ÅÆ„Åü„ÇÅ„Å´„ÄÅÂáçÁµê„Åï„Çå„Åü <a href="clip">CLIP</a> „É¢„Éá„É´„ÅÆ‰∏ä„Å´ÊúÄÂ∞èÈôê„ÅÆ„Éá„Ç≥„Éº„ÉÄ„ÇíËøΩÂä†„Åó„Åæ„Åô„ÄÇ`,Jt,Ie,Kn="Ë´ñÊñá„ÅÆË¶ÅÁ¥Ñ„ÅØÊ¨°„ÅÆ„Å®„Åä„Çä„Åß„Åô„ÄÇ",Zt,$e,eo=`<em>ÁîªÂÉè„ÅÆ„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„ÅØÈÄöÂ∏∏„ÄÅ„Éà„É¨„Éº„Éã„É≥„Ç∞„Å´„Çà„Å£„Å¶Ëß£Ê±∫„Åï„Çå„Åæ„Åô„ÄÇ
„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà „ÇØ„É©„Çπ„ÅÆÂõ∫ÂÆö„Çª„ÉÉ„Éà„ÅÆ„É¢„Éá„É´„ÄÇÂæå„ÅßËøΩÂä†„ÅÆ„ÇØ„É©„Çπ„ÇÑ„Çà„ÇäË§áÈõë„Å™„ÇØ„Ç®„É™„ÇíÁµÑ„ÅøËæº„ÇÄ„Å®„Ç≥„Çπ„Éà„Åå„Åã„Åã„Çä„Åæ„Åô
„Åì„Çå„Çâ„ÅÆÂºè„ÇíÂê´„ÇÄ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Åß„É¢„Éá„É´„ÇíÂÜç„Éà„É¨„Éº„Éã„É≥„Ç∞„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„Åü„ÇÅ„Åß„Åô„ÄÇ„Åì„Åì„Åß„Ç∑„Çπ„ÉÜ„É†„ÇíÊèêÊ°à„Åó„Åæ„Åô
‰ªªÊÑè„ÅÆÊÉÖÂ†±„Å´Âü∫„Å•„ÅÑ„Å¶ÁîªÂÉè„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„ÇíÁîüÊàê„Åß„Åç„Åæ„Åô„ÄÇ
„ÉÜ„Çπ„ÉàÊôÇ„Å´„Éó„É≠„É≥„Éó‚Äã‚Äã„Éà„ÅåË°®Á§∫„Åï„Çå„Åæ„Åô„ÄÇ„Éó„É≠„É≥„Éó„Éà„ÅØ„ÉÜ„Ç≠„Çπ„Éà„Åæ„Åü„ÅØ
ÁîªÂÉè„ÄÇ„Åì„ÅÆ„Ç¢„Éó„É≠„Éº„ÉÅ„Å´„Çà„Çä„ÄÅÁµ±‰∏Ä„Åï„Çå„Åü„É¢„Éá„É´„Çí‰ΩúÊàê„Åß„Åç„Åæ„Åô„ÄÇ
3 „Å§„ÅÆ‰∏ÄËà¨ÁöÑ„Å™„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥ „Çø„Çπ„ÇØ„Å´„Å§„ÅÑ„Å¶ (1 Âõû„Éà„É¨„Éº„Éã„É≥„Ç∞Ê∏à„Åø)
ÂèÇÁÖßÂºè„ÅÆ„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„ÄÅ„Çº„É≠„Ç∑„Éß„ÉÉ„Éà „Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„ÄÅ„ÉØ„É≥„Ç∑„Éß„ÉÉ„Éà „Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„Å®„ÅÑ„ÅÜÊòéÁ¢∫„Å™Ë™≤È°å„Åå‰º¥„ÅÑ„Åæ„Åô„ÄÇ
CLIP „É¢„Éá„É´„Çí„Éê„ÉÉ„ÇØ„Éú„Éº„É≥„Å®„Åó„Å¶ÊßãÁØâ„Åó„ÄÅ„Åì„Çå„Çí„Éà„É©„É≥„Çπ„Éô„Éº„Çπ„ÅÆ„Éá„Ç≥„Éº„ÉÄ„ÅßÊã°Âºµ„Åó„Å¶„ÄÅÈ´òÂØÜÂ∫¶„Å™„Éá„Éº„ÇøÈÄö‰ø°„ÇíÂèØËÉΩ„Å´„Åó„Åæ„Åô„ÄÇ
‰∫àÊ∏¨„ÄÇ„ÅÆÊã°Âºµ„Éê„Éº„Ç∏„Éß„É≥„Åß„Éà„É¨„Éº„Éã„É≥„Ç∞„Åó„ÅüÂæå„ÄÅ
PhraseCut „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÅÁßÅ„Åü„Å°„ÅÆ„Ç∑„Çπ„ÉÜ„É†„ÅØ„ÄÅ„Éï„É™„Éº„ÉÜ„Ç≠„Çπ„Éà „Éó„É≠„É≥„Éó„Éà„Åæ„Åü„ÅØ
„ÇØ„Ç®„É™„ÇíË°®„ÅôËøΩÂä†„ÅÆÁîªÂÉè„ÄÇÂæåËÄÖ„ÅÆÁîªÂÉè„Éô„Éº„Çπ„ÅÆ„Éó„É≠„É≥„Éó„Éà„ÅÆ„Åï„Åæ„Åñ„Åæ„Å™„Éê„É™„Ç®„Éº„Ç∑„Éß„É≥„ÇíË©≥Á¥∞„Å´ÂàÜÊûê„Åó„Åæ„Åô„ÄÇ
„Åì„ÅÆÊñ∞„Åó„ÅÑ„Éè„Ç§„Éñ„É™„ÉÉ„ÉâÂÖ•Âäõ„Å´„Çà„Çä„ÄÅÂãïÁöÑÈÅ©Âøú„ÅåÂèØËÉΩ„Å´„Å™„Çä„Åæ„Åô„ÄÇ
ÂâçËø∞„ÅÆ 3 „Å§„ÅÆ„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥ „Çø„Çπ„ÇØ„ÅÆ„Åø„Åß„Åô„Åå„ÄÅ
„ÉÜ„Ç≠„Çπ„Éà„Åæ„Åü„ÅØÁîªÂÉè„Çí„ÇØ„Ç®„É™„Åô„Çã„Éê„Ç§„Éä„É™ „Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥ „Çø„Çπ„ÇØ„Å´
ÂÆöÂºèÂåñ„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Çã„ÄÇÊúÄÂæå„Å´„ÄÅ„Ç∑„Çπ„ÉÜ„É†„Åå„ÅÜ„Åæ„ÅèÈÅ©Âøú„Åó„Å¶„ÅÑ„Çã„Åì„Å®„Åå„Çè„Åã„Çä„Åæ„Åó„Åü
„Ç¢„Éï„Ç©„Éº„ÉÄ„É≥„Çπ„Åæ„Åü„ÅØ„Éó„É≠„Éë„ÉÜ„Ç£„ÇíÂê´„ÇÄ‰∏ÄËà¨Âåñ„Åï„Çå„Åü„ÇØ„Ç®„É™</em>`,kt,te,to,Ut,ve,no='CLIPSeg „ÅÆÊ¶ÇË¶Å„ÄÇ <a href="https://arxiv.org/abs/2112.10003">ÂÖÉ„ÅÆË´ñÊñá„Åã„ÇâÊäúÁ≤ã„ÄÇ</a>',Wt,we,oo=`„Åì„ÅÆ„É¢„Éá„É´„ÅØ„ÄÅ<a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a> „Å´„Çà„Å£„Å¶Êèê‰æõ„Åï„Çå„Åæ„Åó„Åü„ÄÇ
ÂÖÉ„ÅÆ„Ç≥„Éº„Éâ„ÅØ <a href="https://github.com/timojl/clipseg" rel="nofollow">„Åì„Åì</a> „Å´„ÅÇ„Çä„Åæ„Åô„ÄÇ`,zt,Pe,Bt,Se,so=`<li><a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> „ÅØ„ÄÅ<a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> „ÅÆ‰∏ä„Å´„Éá„Ç≥„Éº„ÉÄ„ÇíËøΩÂä†„Åó„Åæ„Åô„ÄÇÂæåËÄÖ„ÅØ <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a> „Å®Âêå„Åò„Åß„Åô„ÄÇ</li> <li><a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> „ÅØ„ÄÅ„ÉÜ„Çπ„ÉàÊôÇ„Å´‰ªªÊÑè„ÅÆ„Éó„É≠„É≥„Éó„Éà„Å´Âü∫„Å•„ÅÑ„Å¶ÁîªÂÉè„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„ÇíÁîüÊàê„Åß„Åç„Åæ„Åô„ÄÇ„Éó„É≠„É≥„Éó„Éà„ÅØ„ÉÜ„Ç≠„Çπ„Éà„ÅÆ„ÅÑ„Åö„Çå„Åã„Åß„Åô
(<code>input_ids</code> „Å®„Åó„Å¶„É¢„Éá„É´„Å´Êèê‰æõ„Åï„Çå„Çã) „Åæ„Åü„ÅØÁîªÂÉè (<code>conditional_pixel_values</code> „Å®„Åó„Å¶„É¢„Éá„É´„Å´Êèê‰æõ„Åï„Çå„Çã)„ÄÇ„Ç´„Çπ„Çø„É†„ÇíÊèê‰æõ„Åô„Çã„Åì„Å®„ÇÇ„Åß„Åç„Åæ„Åô
Êù°‰ª∂‰ªò„ÅçÂüã„ÇÅËæº„Åø (<code>conditional_embeddings</code>„Å®„Åó„Å¶„É¢„Éá„É´„Å´Êèê‰æõ„Åï„Çå„Åæ„Åô)„ÄÇ</li>`,Vt,xe,Nt,je,ao="CLIPSeg „ÅÆ‰ΩøÁî®„ÇíÈñãÂßã„Åô„Çã„ÅÆ„Å´ÂΩπÁ´ã„Å§„ÄÅÂÖ¨Âºè Hugging Face „Åä„Çà„Å≥„Ç≥„Éü„É•„Éã„ÉÜ„Ç£ (üåé „ÅßÁ§∫„Åï„Çå„Å¶„ÅÑ„Çã) „É™„ÇΩ„Éº„Çπ„ÅÆ„É™„Çπ„Éà„ÄÇ„Åì„Åì„Å´Âê´„ÇÅ„Çã„É™„ÇΩ„Éº„Çπ„ÅÆÈÄÅ‰ø°„Å´ËààÂë≥„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅ„ÅäÊ∞óËªΩ„Å´„Éó„É´ „É™„ÇØ„Ç®„Çπ„Éà„ÇíÈñã„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂØ©Êüª„Åï„Åõ„Å¶„ÅÑ„Åü„Å†„Åç„Åæ„Åô„ÄÇ„É™„ÇΩ„Éº„Çπ„ÅØ„ÄÅÊó¢Â≠ò„ÅÆ„É™„ÇΩ„Éº„Çπ„ÇíË§áË£Ω„Åô„Çã„ÅÆ„Åß„ÅØ„Å™„Åè„ÄÅ‰Ωï„ÅãÊñ∞„Åó„ÅÑ„ÇÇ„ÅÆ„ÇíÁ§∫„Åô„Åì„Å®„ÅåÁêÜÊÉ≥ÁöÑ„Åß„Åô„ÄÇ",Ft,Le,Et,Je,ro='<li><a href="https://github.com/NielsRogge/Transformers-Tutorials/blob/master/CLIPSeg/Zero_shot_image_segmentation_with_CLIPSeg.ipynb" rel="nofollow">CLIPSeg „Çí‰ΩøÁî®„Åó„Åü„Çº„É≠„Ç∑„Éß„ÉÉ„ÉàÁîªÂÉè„Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥</a> „ÇíË™¨Êòé„Åô„Çã„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ„ÄÇ</li>',Qt,Ze,Ht,P,ke,ln,dt,io=`<a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to
instantiate a CLIPSeg model according to the specified arguments, defining the text model and vision model configs.
Instantiating a configuration with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,dn,ct,lo=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,cn,ne,mn,oe,Ue,pn,mt,co=`Instantiate a <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a> (or a derived class) from clipseg text model configuration and clipseg vision
model configuration.`,qt,We,Xt,L,ze,gn,pt,mo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,fn,gt,po=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,un,se,Gt,Be,Rt,J,Ve,hn,ft,go=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>. It is used to instantiate an
CLIPSeg model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the CLIPSeg
<a href="https://huggingface.co/CIDAS/clipseg-rd64" rel="nofollow">CIDAS/clipseg-rd64</a> architecture.`,_n,ut,fo=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Mn,ae,Dt,Ne,At,S,Fe,bn,ht,uo="Constructs a CLIPSeg processor which wraps a CLIPSeg image processor and a CLIP tokenizer into a single processor.",Cn,_t,ho=`<a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegProcessor">CLIPSegProcessor</a> offers all the functionalities of <code>ViTImageProcessor</code> and <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegProcessor.decode">decode()</a> for more information.`,Tn,re,Ee,yn,Mt,_o=`This method forwards all its arguments to CLIPTokenizerFast‚Äôs <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.batch_decode">batch_decode()</a>. Please
refer to the docstring of this method for more information.`,In,ie,Qe,$n,bt,Mo=`This method forwards all its arguments to CLIPTokenizerFast‚Äôs <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,Yt,He,Ot,x,qe,vn,Ct,bo=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,wn,z,Xe,Pn,Tt,Co='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> forward method, overrides the <code>__call__</code> special method.',Sn,le,xn,de,jn,B,Ge,Ln,yt,To='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> forward method, overrides the <code>__call__</code> special method.',Jn,ce,Zn,me,kn,V,Re,Un,It,yo='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a> forward method, overrides the <code>__call__</code> special method.',Wn,pe,zn,ge,Kt,De,en,Y,Ae,Bn,N,Ye,Vn,$t,Io='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegTextModel">CLIPSegTextModel</a> forward method, overrides the <code>__call__</code> special method.',Nn,fe,Fn,ue,tn,Oe,nn,O,Ke,En,F,et,Qn,vt,$o='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionModel">CLIPSegVisionModel</a> forward method, overrides the <code>__call__</code> special method.',Hn,he,qn,_e,on,tt,sn,Z,nt,Xn,wt,vo="CLIPSeg model with a Transformer-based decoder on top for zero-shot and one-shot image segmentation.",Gn,Pt,wo=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Rn,E,ot,Dn,St,Po='The <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegForImageSegmentation">CLIPSegForImageSegmentation</a> forward method, overrides the <code>__call__</code> special method.',An,Me,Yn,be,an,jt,rn;return b=new Q({props:{title:"CLIPSeg",local:"clipseg",headingTag:"h1"}}),T=new Q({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Pe=new Q({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),xe=new Q({props:{title:"Resources",local:"resources",headingTag:"h2"}}),Le=new ko({props:{pipeline:"image-segmentation"}}),Ze=new Q({props:{title:"CLIPSegConfig",local:"transformers.CLIPSegConfig",headingTag:"h2"}}),ke=new w({props:{name:"class transformers.CLIPSegConfig",anchor:"transformers.CLIPSegConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"extract_layers",val:" = [3, 6, 9]"},{name:"reduce_dim",val:" = 64"},{name:"decoder_num_attention_heads",val:" = 4"},{name:"decoder_attention_dropout",val:" = 0.0"},{name:"decoder_hidden_act",val:" = 'quick_gelu'"},{name:"decoder_intermediate_size",val:" = 2048"},{name:"conditional_layer",val:" = 0"},{name:"use_complex_transposed_convolution",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegTextConfig">CLIPSegTextConfig</a>.`,name:"text_config"},{anchor:"transformers.CLIPSegConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionConfig">CLIPSegVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.CLIPSegConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.CLIPSegConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original CLIPSeg implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.CLIPSegConfig.extract_layers",description:`<strong>extract_layers</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 6, 9]</code>) &#x2014;
Layers to extract when forwarding the query image through the frozen visual backbone of CLIP.`,name:"extract_layers"},{anchor:"transformers.CLIPSegConfig.reduce_dim",description:`<strong>reduce_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality to reduce the CLIP vision embedding.`,name:"reduce_dim"},{anchor:"transformers.CLIPSegConfig.decoder_num_attention_heads",description:`<strong>decoder_num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
Number of attention heads in the decoder of CLIPSeg.`,name:"decoder_num_attention_heads"},{anchor:"transformers.CLIPSegConfig.decoder_attention_dropout",description:`<strong>decoder_attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"decoder_attention_dropout"},{anchor:"transformers.CLIPSegConfig.decoder_hidden_act",description:`<strong>decoder_hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"decoder_hidden_act"},{anchor:"transformers.CLIPSegConfig.decoder_intermediate_size",description:`<strong>decoder_intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layers in the Transformer decoder.`,name:"decoder_intermediate_size"},{anchor:"transformers.CLIPSegConfig.conditional_layer",description:`<strong>conditional_layer</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
The layer to use of the Transformer encoder whose activations will be combined with the condition
embeddings using FiLM (Feature-wise Linear Modulation). If 0, the last layer is used.`,name:"conditional_layer"},{anchor:"transformers.CLIPSegConfig.use_complex_transposed_convolution",description:`<strong>use_complex_transposed_convolution</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to use a more complex transposed convolution in the decoder, enabling more fine-grained
segmentation.`,name:"use_complex_transposed_convolution"},{anchor:"transformers.CLIPSegConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L248"}}),ne=new K({props:{anchor:"transformers.CLIPSegConfig.example",$$slots:{default:[Uo]},$$scope:{ctx:I}}}),Ue=new w({props:{name:"from_text_vision_configs",anchor:"transformers.CLIPSegConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": CLIPSegTextConfig"},{name:"vision_config",val:": CLIPSegVisionConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L423",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig"
>CLIPSegConfig</a></p>
`}}),We=new Q({props:{title:"CLIPSegTextConfig",local:"transformers.CLIPSegTextConfig",headingTag:"h2"}}),ze=new w({props:{name:"class transformers.CLIPSegTextConfig",anchor:"transformers.CLIPSegTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 77"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the CLIPSeg text model. Defines the number of different tokens that can be represented
by the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegModel">CLIPSegModel</a>.`,name:"vocab_size"},{anchor:"transformers.CLIPSegTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPSegTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPSegTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPSegTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPSegTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 77) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.CLIPSegTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.CLIPSegTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.CLIPSegTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPSegTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPSegTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.CLIPSegTextConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.CLIPSegTextConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49406) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.CLIPSegTextConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49407) &#x2014;
End of stream token id.`,name:"eos_token_id"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L31"}}),se=new K({props:{anchor:"transformers.CLIPSegTextConfig.example",$$slots:{default:[Wo]},$$scope:{ctx:I}}}),Be=new Q({props:{title:"CLIPSegVisionConfig",local:"transformers.CLIPSegVisionConfig",headingTag:"h2"}}),Ve=new w({props:{name:"class transformers.CLIPSegVisionConfig",anchor:"transformers.CLIPSegVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPSegVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPSegVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPSegVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPSegVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.CLIPSegVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.CLIPSegVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.CLIPSegVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.CLIPSegVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.CLIPSegVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPSegVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPSegVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/configuration_clipseg.py#L143"}}),ae=new K({props:{anchor:"transformers.CLIPSegVisionConfig.example",$$slots:{default:[zo]},$$scope:{ctx:I}}}),Ne=new Q({props:{title:"CLIPSegProcessor",local:"transformers.CLIPSegProcessor",headingTag:"h2"}}),Fe=new w({props:{name:"class transformers.CLIPSegProcessor",anchor:"transformers.CLIPSegProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPSegProcessor.image_processor",description:`<strong>image_processor</strong> (<code>ViTImageProcessor</code>, <em>optional</em>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.CLIPSegProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L25"}}),Ee=new w({props:{name:"batch_decode",anchor:"transformers.CLIPSegProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L134"}}),Qe=new w({props:{name:"decode",anchor:"transformers.CLIPSegProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/processing_clipseg.py#L141"}}),He=new Q({props:{title:"CLIPSegModel",local:"transformers.CLIPSegModel",headingTag:"h2"}}),qe=new w({props:{name:"class transformers.CLIPSegModel",anchor:"transformers.CLIPSegModel",parameters:[{name:"config",val:": CLIPSegConfig"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L924"}}),Xe=new w({props:{name:"forward",anchor:"transformers.CLIPSegModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"return_loss",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.CLIPSegModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1056",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) ‚Äî Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) ‚Äî The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) ‚Äî The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) ‚Äî The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>image_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) ‚Äî The image embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
<li><strong>text_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</li>
<li><strong>vision_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clipseg.modeling_clipseg.CLIPSegOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),le=new xt({props:{$$slots:{default:[Bo]},$$scope:{ctx:I}}}),de=new K({props:{anchor:"transformers.CLIPSegModel.forward.example",$$slots:{default:[Vo]},$$scope:{ctx:I}}}),Ge=new w({props:{name:"get_text_features",anchor:"transformers.CLIPSegModel.get_text_features",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L960",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegTextModel"
>CLIPSegTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),ce=new xt({props:{$$slots:{default:[No]},$$scope:{ctx:I}}}),me=new K({props:{anchor:"transformers.CLIPSegModel.get_text_features.example",$$slots:{default:[Fo]},$$scope:{ctx:I}}}),Re=new w({props:{name:"get_image_features",anchor:"transformers.CLIPSegModel.get_image_features",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1007",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),pe=new xt({props:{$$slots:{default:[Eo]},$$scope:{ctx:I}}}),ge=new K({props:{anchor:"transformers.CLIPSegModel.get_image_features.example",$$slots:{default:[Qo]},$$scope:{ctx:I}}}),De=new Q({props:{title:"CLIPSegTextModel",local:"transformers.CLIPSegTextModel",headingTag:"h2"}}),Ae=new w({props:{name:"class transformers.CLIPSegTextModel",anchor:"transformers.CLIPSegTextModel",parameters:[{name:"config",val:": CLIPSegTextConfig"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L757"}}),Ye=new w({props:{name:"forward",anchor:"transformers.CLIPSegTextModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L774",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) ‚Äî Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) ‚Äî Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new xt({props:{$$slots:{default:[Ho]},$$scope:{ctx:I}}}),ue=new K({props:{anchor:"transformers.CLIPSegTextModel.forward.example",$$slots:{default:[qo]},$$scope:{ctx:I}}}),Oe=new Q({props:{title:"CLIPSegVisionModel",local:"transformers.CLIPSegVisionModel",headingTag:"h2"}}),Ke=new w({props:{name:"class transformers.CLIPSegVisionModel",anchor:"transformers.CLIPSegVisionModel",parameters:[{name:"config",val:": CLIPSegVisionConfig"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L872"}}),et=new w({props:{name:"forward",anchor:"transformers.CLIPSegVisionModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L885",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) ‚Äî Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) ‚Äî Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),he=new xt({props:{$$slots:{default:[Xo]},$$scope:{ctx:I}}}),_e=new K({props:{anchor:"transformers.CLIPSegVisionModel.forward.example",$$slots:{default:[Go]},$$scope:{ctx:I}}}),tt=new Q({props:{title:"CLIPSegForImageSegmentation",local:"transformers.CLIPSegForImageSegmentation",headingTag:"h2"}}),nt=new w({props:{name:"class transformers.CLIPSegForImageSegmentation",anchor:"transformers.CLIPSegForImageSegmentation",parameters:[{name:"config",val:": CLIPSegConfig"}],parametersDescription:[{anchor:"transformers.CLIPSegForImageSegmentation.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegConfig">CLIPSegConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1307"}}),ot=new w({props:{name:"forward",anchor:"transformers.CLIPSegForImageSegmentation.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"conditional_pixel_values",val:": Optional = None"},{name:"conditional_embeddings",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPSegForImageSegmentation.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.CLIPSegForImageSegmentation.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the sequence classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clipseg/modeling_clipseg.py#L1358",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clipseg.configuration_clipseg.CLIPSegTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) ‚Äî Contrastive loss for image-text similarity.
‚Ä¶</li>
<li><strong>vision_model_output</strong> (<code>BaseModelOutputWithPooling</code>) ‚Äî The output of the <a
  href="/docs/transformers/main/ja/model_doc/clipseg#transformers.CLIPSegVisionModel"
>CLIPSegVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clipseg.modeling_clipseg.CLIPSegImageSegmentationOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Me=new xt({props:{$$slots:{default:[Ro]},$$scope:{ctx:I}}}),be=new K({props:{anchor:"transformers.CLIPSegForImageSegmentation.forward.example",$$slots:{default:[Do]},$$scope:{ctx:I}}}),{c(){n=m("meta"),C=i(),r=m("p"),a=i(),g(b.$$.fragment),t=i(),g(T.$$.fragment),Lt=i(),ye=m("p"),ye.innerHTML=On,Jt=i(),Ie=m("p"),Ie.textContent=Kn,Zt=i(),$e=m("p"),$e.innerHTML=eo,kt=i(),te=m("img"),Ut=i(),ve=m("small"),ve.innerHTML=no,Wt=i(),we=m("p"),we.innerHTML=oo,zt=i(),g(Pe.$$.fragment),Bt=i(),Se=m("ul"),Se.innerHTML=so,Vt=i(),g(xe.$$.fragment),Nt=i(),je=m("p"),je.textContent=ao,Ft=i(),g(Le.$$.fragment),Et=i(),Je=m("ul"),Je.innerHTML=ro,Qt=i(),g(Ze.$$.fragment),Ht=i(),P=m("div"),g(ke.$$.fragment),ln=i(),dt=m("p"),dt.innerHTML=io,dn=i(),ct=m("p"),ct.innerHTML=lo,cn=i(),g(ne.$$.fragment),mn=i(),oe=m("div"),g(Ue.$$.fragment),pn=i(),mt=m("p"),mt.innerHTML=co,qt=i(),g(We.$$.fragment),Xt=i(),L=m("div"),g(ze.$$.fragment),gn=i(),pt=m("p"),pt.innerHTML=mo,fn=i(),gt=m("p"),gt.innerHTML=po,un=i(),g(se.$$.fragment),Gt=i(),g(Be.$$.fragment),Rt=i(),J=m("div"),g(Ve.$$.fragment),hn=i(),ft=m("p"),ft.innerHTML=go,_n=i(),ut=m("p"),ut.innerHTML=fo,Mn=i(),g(ae.$$.fragment),Dt=i(),g(Ne.$$.fragment),At=i(),S=m("div"),g(Fe.$$.fragment),bn=i(),ht=m("p"),ht.textContent=uo,Cn=i(),_t=m("p"),_t.innerHTML=ho,Tn=i(),re=m("div"),g(Ee.$$.fragment),yn=i(),Mt=m("p"),Mt.innerHTML=_o,In=i(),ie=m("div"),g(Qe.$$.fragment),$n=i(),bt=m("p"),bt.innerHTML=Mo,Yt=i(),g(He.$$.fragment),Ot=i(),x=m("div"),g(qe.$$.fragment),vn=i(),Ct=m("p"),Ct.innerHTML=bo,wn=i(),z=m("div"),g(Xe.$$.fragment),Pn=i(),Tt=m("p"),Tt.innerHTML=Co,Sn=i(),g(le.$$.fragment),xn=i(),g(de.$$.fragment),jn=i(),B=m("div"),g(Ge.$$.fragment),Ln=i(),yt=m("p"),yt.innerHTML=To,Jn=i(),g(ce.$$.fragment),Zn=i(),g(me.$$.fragment),kn=i(),V=m("div"),g(Re.$$.fragment),Un=i(),It=m("p"),It.innerHTML=yo,Wn=i(),g(pe.$$.fragment),zn=i(),g(ge.$$.fragment),Kt=i(),g(De.$$.fragment),en=i(),Y=m("div"),g(Ae.$$.fragment),Bn=i(),N=m("div"),g(Ye.$$.fragment),Vn=i(),$t=m("p"),$t.innerHTML=Io,Nn=i(),g(fe.$$.fragment),Fn=i(),g(ue.$$.fragment),tn=i(),g(Oe.$$.fragment),nn=i(),O=m("div"),g(Ke.$$.fragment),En=i(),F=m("div"),g(et.$$.fragment),Qn=i(),vt=m("p"),vt.innerHTML=$o,Hn=i(),g(he.$$.fragment),qn=i(),g(_e.$$.fragment),on=i(),g(tt.$$.fragment),sn=i(),Z=m("div"),g(nt.$$.fragment),Xn=i(),wt=m("p"),wt.textContent=vo,Gn=i(),Pt=m("p"),Pt.innerHTML=wo,Rn=i(),E=m("div"),g(ot.$$.fragment),Dn=i(),St=m("p"),St.innerHTML=Po,An=i(),g(Me.$$.fragment),Yn=i(),g(be.$$.fragment),an=i(),jt=m("p"),this.h()},l(e){const o=Zo("svelte-u9bgzb",document.head);n=p(o,"META",{name:!0,content:!0}),o.forEach(s),C=l(e),r=p(e,"P",{}),v(r).forEach(s),a=l(e),f(b.$$.fragment,e),t=l(e),f(T.$$.fragment,e),Lt=l(e),ye=p(e,"P",{"data-svelte-h":!0}),y(ye)!=="svelte-11pq5bj"&&(ye.innerHTML=On),Jt=l(e),Ie=p(e,"P",{"data-svelte-h":!0}),y(Ie)!=="svelte-1cv3nri"&&(Ie.textContent=Kn),Zt=l(e),$e=p(e,"P",{"data-svelte-h":!0}),y($e)!=="svelte-ulr7co"&&($e.innerHTML=eo),kt=l(e),te=p(e,"IMG",{src:!0,alt:!0,width:!0}),Ut=l(e),ve=p(e,"SMALL",{"data-svelte-h":!0}),y(ve)!=="svelte-16azl3n"&&(ve.innerHTML=no),Wt=l(e),we=p(e,"P",{"data-svelte-h":!0}),y(we)!=="svelte-5uei6k"&&(we.innerHTML=oo),zt=l(e),f(Pe.$$.fragment,e),Bt=l(e),Se=p(e,"UL",{"data-svelte-h":!0}),y(Se)!=="svelte-1ahyu4x"&&(Se.innerHTML=so),Vt=l(e),f(xe.$$.fragment,e),Nt=l(e),je=p(e,"P",{"data-svelte-h":!0}),y(je)!=="svelte-163rb1e"&&(je.textContent=ao),Ft=l(e),f(Le.$$.fragment,e),Et=l(e),Je=p(e,"UL",{"data-svelte-h":!0}),y(Je)!=="svelte-46rblz"&&(Je.innerHTML=ro),Qt=l(e),f(Ze.$$.fragment,e),Ht=l(e),P=p(e,"DIV",{class:!0});var k=v(P);f(ke.$$.fragment,k),ln=l(k),dt=p(k,"P",{"data-svelte-h":!0}),y(dt)!=="svelte-15m9iib"&&(dt.innerHTML=io),dn=l(k),ct=p(k,"P",{"data-svelte-h":!0}),y(ct)!=="svelte-1s6wgpv"&&(ct.innerHTML=lo),cn=l(k),f(ne.$$.fragment,k),mn=l(k),oe=p(k,"DIV",{class:!0});var st=v(oe);f(Ue.$$.fragment,st),pn=l(st),mt=p(st,"P",{"data-svelte-h":!0}),y(mt)!=="svelte-8hi51v"&&(mt.innerHTML=co),st.forEach(s),k.forEach(s),qt=l(e),f(We.$$.fragment,e),Xt=l(e),L=p(e,"DIV",{class:!0});var H=v(L);f(ze.$$.fragment,H),gn=l(H),pt=p(H,"P",{"data-svelte-h":!0}),y(pt)!=="svelte-ug4hle"&&(pt.innerHTML=mo),fn=l(H),gt=p(H,"P",{"data-svelte-h":!0}),y(gt)!=="svelte-1s6wgpv"&&(gt.innerHTML=po),un=l(H),f(se.$$.fragment,H),H.forEach(s),Gt=l(e),f(Be.$$.fragment,e),Rt=l(e),J=p(e,"DIV",{class:!0});var q=v(J);f(Ve.$$.fragment,q),hn=l(q),ft=p(q,"P",{"data-svelte-h":!0}),y(ft)!=="svelte-ug4hle"&&(ft.innerHTML=go),_n=l(q),ut=p(q,"P",{"data-svelte-h":!0}),y(ut)!=="svelte-1s6wgpv"&&(ut.innerHTML=fo),Mn=l(q),f(ae.$$.fragment,q),q.forEach(s),Dt=l(e),f(Ne.$$.fragment,e),At=l(e),S=p(e,"DIV",{class:!0});var U=v(S);f(Fe.$$.fragment,U),bn=l(U),ht=p(U,"P",{"data-svelte-h":!0}),y(ht)!=="svelte-9hszhx"&&(ht.textContent=uo),Cn=l(U),_t=p(U,"P",{"data-svelte-h":!0}),y(_t)!=="svelte-1w3xqxn"&&(_t.innerHTML=ho),Tn=l(U),re=p(U,"DIV",{class:!0});var at=v(re);f(Ee.$$.fragment,at),yn=l(at),Mt=p(at,"P",{"data-svelte-h":!0}),y(Mt)!=="svelte-vld6ul"&&(Mt.innerHTML=_o),at.forEach(s),In=l(U),ie=p(U,"DIV",{class:!0});var rt=v(ie);f(Qe.$$.fragment,rt),$n=l(rt),bt=p(rt,"P",{"data-svelte-h":!0}),y(bt)!=="svelte-1ovp6q3"&&(bt.innerHTML=Mo),rt.forEach(s),U.forEach(s),Yt=l(e),f(He.$$.fragment,e),Ot=l(e),x=p(e,"DIV",{class:!0});var W=v(x);f(qe.$$.fragment,W),vn=l(W),Ct=p(W,"P",{"data-svelte-h":!0}),y(Ct)!=="svelte-1gjh92c"&&(Ct.innerHTML=bo),wn=l(W),z=p(W,"DIV",{class:!0});var X=v(z);f(Xe.$$.fragment,X),Pn=l(X),Tt=p(X,"P",{"data-svelte-h":!0}),y(Tt)!=="svelte-552b0y"&&(Tt.innerHTML=Co),Sn=l(X),f(le.$$.fragment,X),xn=l(X),f(de.$$.fragment,X),X.forEach(s),jn=l(W),B=p(W,"DIV",{class:!0});var G=v(B);f(Ge.$$.fragment,G),Ln=l(G),yt=p(G,"P",{"data-svelte-h":!0}),y(yt)!=="svelte-552b0y"&&(yt.innerHTML=To),Jn=l(G),f(ce.$$.fragment,G),Zn=l(G),f(me.$$.fragment,G),G.forEach(s),kn=l(W),V=p(W,"DIV",{class:!0});var R=v(V);f(Re.$$.fragment,R),Un=l(R),It=p(R,"P",{"data-svelte-h":!0}),y(It)!=="svelte-552b0y"&&(It.innerHTML=yo),Wn=l(R),f(pe.$$.fragment,R),zn=l(R),f(ge.$$.fragment,R),R.forEach(s),W.forEach(s),Kt=l(e),f(De.$$.fragment,e),en=l(e),Y=p(e,"DIV",{class:!0});var it=v(Y);f(Ae.$$.fragment,it),Bn=l(it),N=p(it,"DIV",{class:!0});var D=v(N);f(Ye.$$.fragment,D),Vn=l(D),$t=p(D,"P",{"data-svelte-h":!0}),y($t)!=="svelte-1vuc6nc"&&($t.innerHTML=Io),Nn=l(D),f(fe.$$.fragment,D),Fn=l(D),f(ue.$$.fragment,D),D.forEach(s),it.forEach(s),tn=l(e),f(Oe.$$.fragment,e),nn=l(e),O=p(e,"DIV",{class:!0});var lt=v(O);f(Ke.$$.fragment,lt),En=l(lt),F=p(lt,"DIV",{class:!0});var A=v(F);f(et.$$.fragment,A),Qn=l(A),vt=p(A,"P",{"data-svelte-h":!0}),y(vt)!=="svelte-1cjdav6"&&(vt.innerHTML=$o),Hn=l(A),f(he.$$.fragment,A),qn=l(A),f(_e.$$.fragment,A),A.forEach(s),lt.forEach(s),on=l(e),f(tt.$$.fragment,e),sn=l(e),Z=p(e,"DIV",{class:!0});var Ce=v(Z);f(nt.$$.fragment,Ce),Xn=l(Ce),wt=p(Ce,"P",{"data-svelte-h":!0}),y(wt)!=="svelte-qndran"&&(wt.textContent=vo),Gn=l(Ce),Pt=p(Ce,"P",{"data-svelte-h":!0}),y(Pt)!=="svelte-1gjh92c"&&(Pt.innerHTML=wo),Rn=l(Ce),E=p(Ce,"DIV",{class:!0});var Te=v(E);f(ot.$$.fragment,Te),Dn=l(Te),St=p(Te,"P",{"data-svelte-h":!0}),y(St)!=="svelte-pqt806"&&(St.innerHTML=Po),An=l(Te),f(Me.$$.fragment,Te),Yn=l(Te),f(be.$$.fragment,Te),Te.forEach(s),Ce.forEach(s),an=l(e),jt=p(e,"P",{}),v(jt).forEach(s),this.h()},h(){$(n,"name","hf:doc:metadata"),$(n,"content",Yo),xo(te.src,to="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png")||$(te,"src",to),$(te,"alt","ÊèèÁîª"),$(te,"width","600"),$(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(ie,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){c(document.head,n),d(e,C,o),d(e,r,o),d(e,a,o),u(b,e,o),d(e,t,o),u(T,e,o),d(e,Lt,o),d(e,ye,o),d(e,Jt,o),d(e,Ie,o),d(e,Zt,o),d(e,$e,o),d(e,kt,o),d(e,te,o),d(e,Ut,o),d(e,ve,o),d(e,Wt,o),d(e,we,o),d(e,zt,o),u(Pe,e,o),d(e,Bt,o),d(e,Se,o),d(e,Vt,o),u(xe,e,o),d(e,Nt,o),d(e,je,o),d(e,Ft,o),u(Le,e,o),d(e,Et,o),d(e,Je,o),d(e,Qt,o),u(Ze,e,o),d(e,Ht,o),d(e,P,o),u(ke,P,null),c(P,ln),c(P,dt),c(P,dn),c(P,ct),c(P,cn),u(ne,P,null),c(P,mn),c(P,oe),u(Ue,oe,null),c(oe,pn),c(oe,mt),d(e,qt,o),u(We,e,o),d(e,Xt,o),d(e,L,o),u(ze,L,null),c(L,gn),c(L,pt),c(L,fn),c(L,gt),c(L,un),u(se,L,null),d(e,Gt,o),u(Be,e,o),d(e,Rt,o),d(e,J,o),u(Ve,J,null),c(J,hn),c(J,ft),c(J,_n),c(J,ut),c(J,Mn),u(ae,J,null),d(e,Dt,o),u(Ne,e,o),d(e,At,o),d(e,S,o),u(Fe,S,null),c(S,bn),c(S,ht),c(S,Cn),c(S,_t),c(S,Tn),c(S,re),u(Ee,re,null),c(re,yn),c(re,Mt),c(S,In),c(S,ie),u(Qe,ie,null),c(ie,$n),c(ie,bt),d(e,Yt,o),u(He,e,o),d(e,Ot,o),d(e,x,o),u(qe,x,null),c(x,vn),c(x,Ct),c(x,wn),c(x,z),u(Xe,z,null),c(z,Pn),c(z,Tt),c(z,Sn),u(le,z,null),c(z,xn),u(de,z,null),c(x,jn),c(x,B),u(Ge,B,null),c(B,Ln),c(B,yt),c(B,Jn),u(ce,B,null),c(B,Zn),u(me,B,null),c(x,kn),c(x,V),u(Re,V,null),c(V,Un),c(V,It),c(V,Wn),u(pe,V,null),c(V,zn),u(ge,V,null),d(e,Kt,o),u(De,e,o),d(e,en,o),d(e,Y,o),u(Ae,Y,null),c(Y,Bn),c(Y,N),u(Ye,N,null),c(N,Vn),c(N,$t),c(N,Nn),u(fe,N,null),c(N,Fn),u(ue,N,null),d(e,tn,o),u(Oe,e,o),d(e,nn,o),d(e,O,o),u(Ke,O,null),c(O,En),c(O,F),u(et,F,null),c(F,Qn),c(F,vt),c(F,Hn),u(he,F,null),c(F,qn),u(_e,F,null),d(e,on,o),u(tt,e,o),d(e,sn,o),d(e,Z,o),u(nt,Z,null),c(Z,Xn),c(Z,wt),c(Z,Gn),c(Z,Pt),c(Z,Rn),c(Z,E),u(ot,E,null),c(E,Dn),c(E,St),c(E,An),u(Me,E,null),c(E,Yn),u(be,E,null),d(e,an,o),d(e,jt,o),rn=!0},p(e,[o]){const k={};o&2&&(k.$$scope={dirty:o,ctx:e}),ne.$set(k);const st={};o&2&&(st.$$scope={dirty:o,ctx:e}),se.$set(st);const H={};o&2&&(H.$$scope={dirty:o,ctx:e}),ae.$set(H);const q={};o&2&&(q.$$scope={dirty:o,ctx:e}),le.$set(q);const U={};o&2&&(U.$$scope={dirty:o,ctx:e}),de.$set(U);const at={};o&2&&(at.$$scope={dirty:o,ctx:e}),ce.$set(at);const rt={};o&2&&(rt.$$scope={dirty:o,ctx:e}),me.$set(rt);const W={};o&2&&(W.$$scope={dirty:o,ctx:e}),pe.$set(W);const X={};o&2&&(X.$$scope={dirty:o,ctx:e}),ge.$set(X);const G={};o&2&&(G.$$scope={dirty:o,ctx:e}),fe.$set(G);const R={};o&2&&(R.$$scope={dirty:o,ctx:e}),ue.$set(R);const it={};o&2&&(it.$$scope={dirty:o,ctx:e}),he.$set(it);const D={};o&2&&(D.$$scope={dirty:o,ctx:e}),_e.$set(D);const lt={};o&2&&(lt.$$scope={dirty:o,ctx:e}),Me.$set(lt);const A={};o&2&&(A.$$scope={dirty:o,ctx:e}),be.$set(A)},i(e){rn||(h(b.$$.fragment,e),h(T.$$.fragment,e),h(Pe.$$.fragment,e),h(xe.$$.fragment,e),h(Le.$$.fragment,e),h(Ze.$$.fragment,e),h(ke.$$.fragment,e),h(ne.$$.fragment,e),h(Ue.$$.fragment,e),h(We.$$.fragment,e),h(ze.$$.fragment,e),h(se.$$.fragment,e),h(Be.$$.fragment,e),h(Ve.$$.fragment,e),h(ae.$$.fragment,e),h(Ne.$$.fragment,e),h(Fe.$$.fragment,e),h(Ee.$$.fragment,e),h(Qe.$$.fragment,e),h(He.$$.fragment,e),h(qe.$$.fragment,e),h(Xe.$$.fragment,e),h(le.$$.fragment,e),h(de.$$.fragment,e),h(Ge.$$.fragment,e),h(ce.$$.fragment,e),h(me.$$.fragment,e),h(Re.$$.fragment,e),h(pe.$$.fragment,e),h(ge.$$.fragment,e),h(De.$$.fragment,e),h(Ae.$$.fragment,e),h(Ye.$$.fragment,e),h(fe.$$.fragment,e),h(ue.$$.fragment,e),h(Oe.$$.fragment,e),h(Ke.$$.fragment,e),h(et.$$.fragment,e),h(he.$$.fragment,e),h(_e.$$.fragment,e),h(tt.$$.fragment,e),h(nt.$$.fragment,e),h(ot.$$.fragment,e),h(Me.$$.fragment,e),h(be.$$.fragment,e),rn=!0)},o(e){_(b.$$.fragment,e),_(T.$$.fragment,e),_(Pe.$$.fragment,e),_(xe.$$.fragment,e),_(Le.$$.fragment,e),_(Ze.$$.fragment,e),_(ke.$$.fragment,e),_(ne.$$.fragment,e),_(Ue.$$.fragment,e),_(We.$$.fragment,e),_(ze.$$.fragment,e),_(se.$$.fragment,e),_(Be.$$.fragment,e),_(Ve.$$.fragment,e),_(ae.$$.fragment,e),_(Ne.$$.fragment,e),_(Fe.$$.fragment,e),_(Ee.$$.fragment,e),_(Qe.$$.fragment,e),_(He.$$.fragment,e),_(qe.$$.fragment,e),_(Xe.$$.fragment,e),_(le.$$.fragment,e),_(de.$$.fragment,e),_(Ge.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(Re.$$.fragment,e),_(pe.$$.fragment,e),_(ge.$$.fragment,e),_(De.$$.fragment,e),_(Ae.$$.fragment,e),_(Ye.$$.fragment,e),_(fe.$$.fragment,e),_(ue.$$.fragment,e),_(Oe.$$.fragment,e),_(Ke.$$.fragment,e),_(et.$$.fragment,e),_(he.$$.fragment,e),_(_e.$$.fragment,e),_(tt.$$.fragment,e),_(nt.$$.fragment,e),_(ot.$$.fragment,e),_(Me.$$.fragment,e),_(be.$$.fragment,e),rn=!1},d(e){e&&(s(C),s(r),s(a),s(t),s(Lt),s(ye),s(Jt),s(Ie),s(Zt),s($e),s(kt),s(te),s(Ut),s(ve),s(Wt),s(we),s(zt),s(Bt),s(Se),s(Vt),s(Nt),s(je),s(Ft),s(Et),s(Je),s(Qt),s(Ht),s(P),s(qt),s(Xt),s(L),s(Gt),s(Rt),s(J),s(Dt),s(At),s(S),s(Yt),s(Ot),s(x),s(Kt),s(en),s(Y),s(tn),s(nn),s(O),s(on),s(sn),s(Z),s(an),s(jt)),s(n),M(b,e),M(T,e),M(Pe,e),M(xe,e),M(Le,e),M(Ze,e),M(ke),M(ne),M(Ue),M(We,e),M(ze),M(se),M(Be,e),M(Ve),M(ae),M(Ne,e),M(Fe),M(Ee),M(Qe),M(He,e),M(qe),M(Xe),M(le),M(de),M(Ge),M(ce),M(me),M(Re),M(pe),M(ge),M(De,e),M(Ae),M(Ye),M(fe),M(ue),M(Oe,e),M(Ke),M(et),M(he),M(_e),M(tt,e),M(nt),M(ot),M(Me),M(be)}}}const Yo='{"title":"CLIPSeg","local":"clipseg","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"CLIPSegConfig","local":"transformers.CLIPSegConfig","sections":[],"depth":2},{"title":"CLIPSegTextConfig","local":"transformers.CLIPSegTextConfig","sections":[],"depth":2},{"title":"CLIPSegVisionConfig","local":"transformers.CLIPSegVisionConfig","sections":[],"depth":2},{"title":"CLIPSegProcessor","local":"transformers.CLIPSegProcessor","sections":[],"depth":2},{"title":"CLIPSegModel","local":"transformers.CLIPSegModel","sections":[],"depth":2},{"title":"CLIPSegTextModel","local":"transformers.CLIPSegTextModel","sections":[],"depth":2},{"title":"CLIPSegVisionModel","local":"transformers.CLIPSegVisionModel","sections":[],"depth":2},{"title":"CLIPSegForImageSegmentation","local":"transformers.CLIPSegForImageSegmentation","sections":[],"depth":2}],"depth":1}';function Oo(I){return jo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class is extends Lo{constructor(n){super(),Jo(this,n,Oo,Ao,So,{})}}export{is as component};
