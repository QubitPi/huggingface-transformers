import{s as nt,o as ot,n as rt}from"../chunks/scheduler.9bc65507.js";import{S as at,i as st,g as s,s as o,r as d,A as it,h as i,f as n,c as r,j as $,u as p,x as M,k as I,y as t,a as m,v as g,d as f,t as h,w as u}from"../chunks/index.707bf1b6.js";import{D as U}from"../chunks/Docstring.17db21ae.js";import{C as ct}from"../chunks/CodeBlock.54a9f38d.js";import{E as mt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as Te}from"../chunks/Heading.342b1fa6.js";function lt(de){let l,W="Examples:",P,v,x;return v=new ct({props:{code:"JTIzJTIwV2UlMjBjYW4ndCUyMGluc3RhbnRpYXRlJTIwZGlyZWN0bHklMjB0aGUlMjBiYXNlJTIwY2xhc3MlMjAqSW1hZ2VQcm9jZXNzaW5nTWl4aW4qJTIwc28lMjBsZXQncyUyMHNob3clMjB0aGUlMjBleGFtcGxlcyUyMG9uJTIwYSUwQSUyMyUyMGRlcml2ZWQlMjBjbGFzcyUzQSUyMCpDTElQSW1hZ2VQcm9jZXNzb3IqJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQ0xJUEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIlMEEpJTIwJTIwJTIzJTIwRG93bmxvYWQlMjBpbWFnZV9wcm9jZXNzaW5nX2NvbmZpZyUyMGZyb20lMjBodWdnaW5nZmFjZS5jbyUyMGFuZCUyMGNhY2hlLiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMENMSVBJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRiUyMiUwQSklMjAlMjAlMjMlMjBFLmcuJTIwaW1hZ2UlMjBwcm9jZXNzb3IlMjAob3IlMjBtb2RlbCklMjB3YXMlMjBzYXZlZCUyMHVzaW5nJTIwKnNhdmVfcHJldHJhaW5lZCgnLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRicpKiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMENMSVBJbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyLiUyRnRlc3QlMkZzYXZlZF9tb2RlbCUyRnByZXByb2Nlc3Nvcl9jb25maWcuanNvbiUyMiklMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBDTElQSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiUyQyUyMGRvX25vcm1hbGl6ZSUzREZhbHNlJTJDJTIwZm9vJTNERmFsc2UlMEEpJTBBYXNzZXJ0JTIwaW1hZ2VfcHJvY2Vzc29yLmRvX25vcm1hbGl6ZSUyMGlzJTIwRmFsc2UlMEFpbWFnZV9wcm9jZXNzb3IlMkMlMjB1bnVzZWRfa3dhcmdzJTIwJTNEJTIwQ0xJUEltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIlMkMlMjBkb19ub3JtYWxpemUlM0RGYWxzZSUyQyUyMGZvbyUzREZhbHNlJTJDJTIwcmV0dXJuX3VudXNlZF9rd2FyZ3MlM0RUcnVlJTBBKSUwQWFzc2VydCUyMGltYWdlX3Byb2Nlc3Nvci5kb19ub3JtYWxpemUlMjBpcyUyMEZhbHNlJTBBYXNzZXJ0JTIwdW51c2VkX2t3YXJncyUyMCUzRCUzRCUyMCU3QiUyMmZvbyUyMiUzQSUyMEZhbHNlJTdE",highlighted:`<span class="hljs-comment"># We can&#x27;t instantiate directly the base class *ImageProcessingMixin* so let&#x27;s show the examples on a</span>
<span class="hljs-comment"># derived class: *CLIPImageProcessor*</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>
)  <span class="hljs-comment"># Download image_processing_config from huggingface.co and cache.</span>
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;./test/saved_model/&quot;</span>
)  <span class="hljs-comment"># E.g. image processor (or model) was saved using *save_pretrained(&#x27;./test/saved_model/&#x27;)*</span>
image_processor = CLIPImageProcessor.from_pretrained(<span class="hljs-string">&quot;./test/saved_model/preprocessor_config.json&quot;</span>)
image_processor = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
image_processor, unused_kwargs = CLIPImageProcessor.from_pretrained(
    <span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>, do_normalize=<span class="hljs-literal">False</span>, foo=<span class="hljs-literal">False</span>, return_unused_kwargs=<span class="hljs-literal">True</span>
)
<span class="hljs-keyword">assert</span> image_processor.do_normalize <span class="hljs-keyword">is</span> <span class="hljs-literal">False</span>
<span class="hljs-keyword">assert</span> unused_kwargs == {<span class="hljs-string">&quot;foo&quot;</span>: <span class="hljs-literal">False</span>}`,wrap:!1}}),{c(){l=s("p"),l.textContent=W,P=o(),d(v.$$.fragment)},l(c){l=i(c,"P",{"data-svelte-h":!0}),M(l)!=="svelte-kvfsh7"&&(l.textContent=W),P=r(c),p(v.$$.fragment,c)},m(c,w){m(c,l,w),m(c,P,w),g(v,c,w),x=!0},p:rt,i(c){x||(f(v.$$.fragment,c),x=!0)},o(c){h(v.$$.fragment,c),x=!1},d(c){c&&(n(l),n(P)),u(v,c)}}}function dt(de){let l,W,P,v,x,c,w,Ve="画像プロセッサは、ビジョン モデルの入力特徴の準備とその出力の後処理を担当します。これには、サイズ変更、正規化、PyTorch、TensorFlow、Flax、Numpy テンソルへの変換などの変換が含まれます。ロジットをセグメンテーション マスクに変換するなど、モデル固有の後処理も含まれる場合があります。",pe,q,ge,b,L,Pe,O,Se=`This is an image processor mixin used to provide saving/loading functionality for sequential and image feature
extractors.`,Ce,C,E,je,ee,Qe='Instantiate a type of <a href="/docs/transformers/main/ja/main_classes/image_processor#transformers.ImageProcessingMixin">ImageProcessingMixin</a> from an image processor.',Ue,B,Be,Z,R,Ze,te,Ye=`Save an image processor object to the directory <code>save_directory</code>, so that it can be re-loaded using the
<a href="/docs/transformers/main/ja/main_classes/image_processor#transformers.ImageProcessingMixin.from_pretrained">from_pretrained()</a> class method.`,fe,X,he,_,V,ze,ne,He='Holds the output of the <a href="/docs/transformers/main/ja/main_classes/feature_extractor#transformers.SequenceFeatureExtractor.pad">pad()</a> and feature extractor specific <code>__call__</code> methods.',Je,oe,Ae="This class is derived from a python dictionary and can be used as a dictionary.",Fe,z,S,ke,re,Ge="Convert the inner content to tensors.",Ne,J,Q,De,ae,Ke=`Send all values to device by calling <code>v.to(*args, **kwargs)</code> (PyTorch only). This should support casting in
different <code>dtypes</code> and sending the <code>BatchFeature</code> to a different <code>device</code>.`,ue,Y,_e,y,H,We,F,A,qe,se,Oe=`Center crop an image to <code>(size[&quot;height&quot;], size[&quot;width&quot;])</code>. If the input size is smaller than <code>crop_size</code> along
any edge, the image is padded with 0’s and then center cropped.`,Le,k,G,Ee,ie,et="Normalize an image. image = (image - image_mean) / image_std.",Re,N,K,Xe,ce,tt="Rescale an image by a scale factor. image = image * scale.",be,le,ye;return x=new Te({props:{title:"Image Processor",local:"image-processor",headingTag:"h1"}}),q=new Te({props:{title:"ImageProcessingMixin",local:"transformers.ImageProcessingMixin",headingTag:"h2"}}),L=new U({props:{name:"class transformers.ImageProcessingMixin",anchor:"transformers.ImageProcessingMixin",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L68"}}),E=new U({props:{name:"from_pretrained",anchor:"transformers.ImageProcessingMixin.from_pretrained",parameters:[{name:"pretrained_model_name_or_path",val:": Union"},{name:"cache_dir",val:": Union = None"},{name:"force_download",val:": bool = False"},{name:"local_files_only",val:": bool = False"},{name:"token",val:": Union = None"},{name:"revision",val:": str = 'main'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained image_processor hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a image processor file saved using the
<a href="/docs/transformers/main/ja/main_classes/image_processor#transformers.ImageProcessingMixin.save_pretrained">save_pretrained()</a> method, e.g.,
<code>./my_model_directory/</code>.</li>
<li>a path or url to a saved image processor JSON <em>file</em>, e.g.,
<code>./my_model_directory/preprocessor_config.json</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.cache_dir",description:`<strong>cache_dir</strong> (<code>str</code> or <code>os.PathLike</code>, <em>optional</em>) &#x2014;
Path to a directory in which a downloaded pretrained model image processor should be cached if the
standard cache should not be used.`,name:"cache_dir"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.force_download",description:`<strong>force_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to force to (re-)download the image processor files and override the cached versions if
they exist.`,name:"force_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.resume_download",description:`<strong>resume_download</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to delete incompletely received file. Attempts to resume the download if such a file
exists.`,name:"resume_download"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.proxies",description:`<strong>proxies</strong> (<code>Dict[str, str]</code>, <em>optional</em>) &#x2014;
A dictionary of proxy servers to use by protocol or endpoint, e.g., <code>{&apos;http&apos;: &apos;foo.bar:3128&apos;, &apos;http://hostname&apos;: &apos;foo.bar:4012&apos;}.</code> The proxies are used on each request.`,name:"proxies"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.token",description:`<strong>token</strong> (<code>str</code> or <code>bool</code>, <em>optional</em>) &#x2014;
The token to use as HTTP bearer authorization for remote files. If <code>True</code>, or not specified, will use
the token generated when running <code>huggingface-cli login</code> (stored in <code>~/.huggingface</code>).`,name:"token"},{anchor:"transformers.ImageProcessingMixin.from_pretrained.revision",description:`<strong>revision</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;main&quot;</code>) &#x2014;
The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a
git-based system for storing models and other artifacts on huggingface.co, so <code>revision</code> can be any
identifier allowed by git.`,name:"revision"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L95",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A image processor of type <a
  href="/docs/transformers/main/ja/main_classes/image_processor#transformers.ImageProcessingMixin"
>ImageProcessingMixin</a>.</p>
`}}),B=new mt({props:{anchor:"transformers.ImageProcessingMixin.from_pretrained.example",$$slots:{default:[lt]},$$scope:{ctx:de}}}),R=new U({props:{name:"save_pretrained",anchor:"transformers.ImageProcessingMixin.save_pretrained",parameters:[{name:"save_directory",val:": Union"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ImageProcessingMixin.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the image processor JSON file will be saved (will be created if it does not exist).`,name:"save_directory"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).`,name:"push_to_hub"},{anchor:"transformers.ImageProcessingMixin.save_pretrained.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/ja/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L209"}}),X=new Te({props:{title:"BatchFeature",local:"transformers.BatchFeature",headingTag:"h2"}}),V=new U({props:{name:"class transformers.BatchFeature",anchor:"transformers.BatchFeature",parameters:[{name:"data",val:": Optional = None"},{name:"tensor_type",val:": Union = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.data",description:`<strong>data</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of lists/arrays/tensors returned by the <strong>call</strong>/pad methods (&#x2018;input_values&#x2019;, &#x2018;attention_mask&#x2019;,
etc.).`,name:"data"},{anchor:"transformers.BatchFeature.tensor_type",description:`<strong>tensor_type</strong> (<code>Union[None, str, TensorType]</code>, <em>optional</em>) &#x2014;
You can give a tensor_type here to convert the lists of integers in PyTorch/TensorFlow/Numpy Tensors at
initialization.`,name:"tensor_type"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L61"}}),S=new U({props:{name:"convert_to_tensors",anchor:"transformers.BatchFeature.convert_to_tensors",parameters:[{name:"tensor_type",val:": Union = None"}],parametersDescription:[{anchor:"transformers.BatchFeature.convert_to_tensors.tensor_type",description:`<strong>tensor_type</strong> (<code>str</code> or <a href="/docs/transformers/main/ja/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
The type of tensors to use. If <code>str</code>, should be one of the values of the enum <a href="/docs/transformers/main/ja/internal/file_utils#transformers.TensorType">TensorType</a>. If
<code>None</code>, no modification is done.`,name:"tensor_type"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L164"}}),Q=new U({props:{name:"to",anchor:"transformers.BatchFeature.to",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BatchFeature.to.args",description:`<strong>args</strong> (<code>Tuple</code>) &#x2014;
Will be passed to the <code>to(...)</code> function of the tensors.`,name:"args"},{anchor:"transformers.BatchFeature.to.kwargs",description:`<strong>kwargs</strong> (<code>Dict</code>, <em>optional</em>) &#x2014;
Will be passed to the <code>to(...)</code> function of the tensors.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/feature_extraction_utils.py#L195",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The same instance after modification.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a></p>
`}}),Y=new Te({props:{title:"BaseImageProcessor",local:"transformers.image_processing_utils.BaseImageProcessor",headingTag:"h2"}}),H=new U({props:{name:"class transformers.image_processing_utils.BaseImageProcessor",anchor:"transformers.image_processing_utils.BaseImageProcessor",parameters:[{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L545"}}),A=new U({props:{name:"center_crop",anchor:"transformers.image_processing_utils.BaseImageProcessor.center_crop",parameters:[{name:"image",val:": ndarray"},{name:"size",val:": Dict"},{name:"data_format",val:": Union = None"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.image_processing_utils.BaseImageProcessor.center_crop.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to center crop.`,name:"image"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.center_crop.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>) &#x2014;
Size of the output image.`,name:"size"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.center_crop.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.center_crop.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L625"}}),G=new U({props:{name:"normalize",anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize",parameters:[{name:"image",val:": ndarray"},{name:"mean",val:": Union"},{name:"std",val:": Union"},{name:"data_format",val:": Union = None"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to normalize.`,name:"image"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize.mean",description:`<strong>mean</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
Image mean to use for normalization.`,name:"mean"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize.std",description:`<strong>std</strong> (<code>float</code> or <code>Iterable[float]</code>) &#x2014;
Image standard deviation to use for normalization.`,name:"std"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.normalize.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L588",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The normalized image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),K=new U({props:{name:"rescale",anchor:"transformers.image_processing_utils.BaseImageProcessor.rescale",parameters:[{name:"image",val:": ndarray"},{name:"scale",val:": float"},{name:"data_format",val:": Union = None"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.image_processing_utils.BaseImageProcessor.rescale.image",description:`<strong>image</strong> (<code>np.ndarray</code>) &#x2014;
Image to rescale.`,name:"image"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.rescale.scale",description:`<strong>scale</strong> (<code>float</code>) &#x2014;
The scaling factor to rescale pixel values by.`,name:"scale"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.rescale.data_format",description:`<strong>data_format</strong> (<code>str</code> or <code>ChannelDimension</code>, <em>optional</em>) &#x2014;
The channel dimension format for the output image. If unset, the channel dimension format of the input
image is used. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.image_processing_utils.BaseImageProcessor.rescale.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L556",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The rescaled image.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>np.ndarray</code></p>
`}}),{c(){l=s("meta"),W=o(),P=s("p"),v=o(),d(x.$$.fragment),c=o(),w=s("p"),w.textContent=Ve,pe=o(),d(q.$$.fragment),ge=o(),b=s("div"),d(L.$$.fragment),Pe=o(),O=s("p"),O.textContent=Se,Ce=o(),C=s("div"),d(E.$$.fragment),je=o(),ee=s("p"),ee.innerHTML=Qe,Ue=o(),d(B.$$.fragment),Be=o(),Z=s("div"),d(R.$$.fragment),Ze=o(),te=s("p"),te.innerHTML=Ye,fe=o(),d(X.$$.fragment),he=o(),_=s("div"),d(V.$$.fragment),ze=o(),ne=s("p"),ne.innerHTML=He,Je=o(),oe=s("p"),oe.textContent=Ae,Fe=o(),z=s("div"),d(S.$$.fragment),ke=o(),re=s("p"),re.textContent=Ge,Ne=o(),J=s("div"),d(Q.$$.fragment),De=o(),ae=s("p"),ae.innerHTML=Ke,ue=o(),d(Y.$$.fragment),_e=o(),y=s("div"),d(H.$$.fragment),We=o(),F=s("div"),d(A.$$.fragment),qe=o(),se=s("p"),se.innerHTML=Oe,Le=o(),k=s("div"),d(G.$$.fragment),Ee=o(),ie=s("p"),ie.textContent=et,Re=o(),N=s("div"),d(K.$$.fragment),Xe=o(),ce=s("p"),ce.textContent=tt,be=o(),le=s("p"),this.h()},l(e){const a=it("svelte-u9bgzb",document.head);l=i(a,"META",{name:!0,content:!0}),a.forEach(n),W=r(e),P=i(e,"P",{}),$(P).forEach(n),v=r(e),p(x.$$.fragment,e),c=r(e),w=i(e,"P",{"data-svelte-h":!0}),M(w)!=="svelte-1yy2ecy"&&(w.textContent=Ve),pe=r(e),p(q.$$.fragment,e),ge=r(e),b=i(e,"DIV",{class:!0});var T=$(b);p(L.$$.fragment,T),Pe=r(T),O=i(T,"P",{"data-svelte-h":!0}),M(O)!=="svelte-16ht4m3"&&(O.textContent=Se),Ce=r(T),C=i(T,"DIV",{class:!0});var me=$(C);p(E.$$.fragment,me),je=r(me),ee=i(me,"P",{"data-svelte-h":!0}),M(ee)!=="svelte-12cb1uf"&&(ee.innerHTML=Qe),Ue=r(me),p(B.$$.fragment,me),me.forEach(n),Be=r(T),Z=i(T,"DIV",{class:!0});var ve=$(Z);p(R.$$.fragment,ve),Ze=r(ve),te=i(ve,"P",{"data-svelte-h":!0}),M(te)!=="svelte-lmzm7o"&&(te.innerHTML=Ye),ve.forEach(n),T.forEach(n),fe=r(e),p(X.$$.fragment,e),he=r(e),_=i(e,"DIV",{class:!0});var j=$(_);p(V.$$.fragment,j),ze=r(j),ne=i(j,"P",{"data-svelte-h":!0}),M(ne)!=="svelte-gmb53s"&&(ne.innerHTML=He),Je=r(j),oe=i(j,"P",{"data-svelte-h":!0}),M(oe)!=="svelte-saqdtk"&&(oe.textContent=Ae),Fe=r(j),z=i(j,"DIV",{class:!0});var xe=$(z);p(S.$$.fragment,xe),ke=r(xe),re=i(xe,"P",{"data-svelte-h":!0}),M(re)!=="svelte-pxfh9u"&&(re.textContent=Ge),xe.forEach(n),Ne=r(j),J=i(j,"DIV",{class:!0});var $e=$(J);p(Q.$$.fragment,$e),De=r($e),ae=i($e,"P",{"data-svelte-h":!0}),M(ae)!=="svelte-d0cfhs"&&(ae.innerHTML=Ke),$e.forEach(n),j.forEach(n),ue=r(e),p(Y.$$.fragment,e),_e=r(e),y=i(e,"DIV",{class:!0});var D=$(y);p(H.$$.fragment,D),We=r(D),F=i(D,"DIV",{class:!0});var Ie=$(F);p(A.$$.fragment,Ie),qe=r(Ie),se=i(Ie,"P",{"data-svelte-h":!0}),M(se)!=="svelte-193kiu8"&&(se.innerHTML=Oe),Ie.forEach(n),Le=r(D),k=i(D,"DIV",{class:!0});var Me=$(k);p(G.$$.fragment,Me),Ee=r(Me),ie=i(Me,"P",{"data-svelte-h":!0}),M(ie)!=="svelte-1e5okex"&&(ie.textContent=et),Me.forEach(n),Re=r(D),N=i(D,"DIV",{class:!0});var we=$(N);p(K.$$.fragment,we),Xe=r(we),ce=i(we,"P",{"data-svelte-h":!0}),M(ce)!=="svelte-qun0mt"&&(ce.textContent=tt),we.forEach(n),D.forEach(n),be=r(e),le=i(e,"P",{}),$(le).forEach(n),this.h()},h(){I(l,"name","hf:doc:metadata"),I(l,"content",pt),I(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(b,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(_,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),I(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,a){t(document.head,l),m(e,W,a),m(e,P,a),m(e,v,a),g(x,e,a),m(e,c,a),m(e,w,a),m(e,pe,a),g(q,e,a),m(e,ge,a),m(e,b,a),g(L,b,null),t(b,Pe),t(b,O),t(b,Ce),t(b,C),g(E,C,null),t(C,je),t(C,ee),t(C,Ue),g(B,C,null),t(b,Be),t(b,Z),g(R,Z,null),t(Z,Ze),t(Z,te),m(e,fe,a),g(X,e,a),m(e,he,a),m(e,_,a),g(V,_,null),t(_,ze),t(_,ne),t(_,Je),t(_,oe),t(_,Fe),t(_,z),g(S,z,null),t(z,ke),t(z,re),t(_,Ne),t(_,J),g(Q,J,null),t(J,De),t(J,ae),m(e,ue,a),g(Y,e,a),m(e,_e,a),m(e,y,a),g(H,y,null),t(y,We),t(y,F),g(A,F,null),t(F,qe),t(F,se),t(y,Le),t(y,k),g(G,k,null),t(k,Ee),t(k,ie),t(y,Re),t(y,N),g(K,N,null),t(N,Xe),t(N,ce),m(e,be,a),m(e,le,a),ye=!0},p(e,[a]){const T={};a&2&&(T.$$scope={dirty:a,ctx:e}),B.$set(T)},i(e){ye||(f(x.$$.fragment,e),f(q.$$.fragment,e),f(L.$$.fragment,e),f(E.$$.fragment,e),f(B.$$.fragment,e),f(R.$$.fragment,e),f(X.$$.fragment,e),f(V.$$.fragment,e),f(S.$$.fragment,e),f(Q.$$.fragment,e),f(Y.$$.fragment,e),f(H.$$.fragment,e),f(A.$$.fragment,e),f(G.$$.fragment,e),f(K.$$.fragment,e),ye=!0)},o(e){h(x.$$.fragment,e),h(q.$$.fragment,e),h(L.$$.fragment,e),h(E.$$.fragment,e),h(B.$$.fragment,e),h(R.$$.fragment,e),h(X.$$.fragment,e),h(V.$$.fragment,e),h(S.$$.fragment,e),h(Q.$$.fragment,e),h(Y.$$.fragment,e),h(H.$$.fragment,e),h(A.$$.fragment,e),h(G.$$.fragment,e),h(K.$$.fragment,e),ye=!1},d(e){e&&(n(W),n(P),n(v),n(c),n(w),n(pe),n(ge),n(b),n(fe),n(he),n(_),n(ue),n(_e),n(y),n(be),n(le)),n(l),u(x,e),u(q,e),u(L),u(E),u(B),u(R),u(X,e),u(V),u(S),u(Q),u(Y,e),u(H),u(A),u(G),u(K)}}}const pt='{"title":"Image Processor","local":"image-processor","sections":[{"title":"ImageProcessingMixin","local":"transformers.ImageProcessingMixin","sections":[],"depth":2},{"title":"BatchFeature","local":"transformers.BatchFeature","sections":[],"depth":2},{"title":"BaseImageProcessor","local":"transformers.image_processing_utils.BaseImageProcessor","sections":[],"depth":2}],"depth":1}';function gt(de){return ot(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class vt extends at{constructor(l){super(),st(this,l,gt,dt,nt,{})}}export{vt as component};
