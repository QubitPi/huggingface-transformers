import{s as Yn,o as Dn,n as W}from"../chunks/scheduler.9bc65507.js";import{S as An,i as On,g as d,s as i,r as u,A as Kn,h as c,f as a,c as l,j as L,u as f,x as $,k as J,y as r,a as p,v as h,d as g,t as _,w as b}from"../chunks/index.707bf1b6.js";import{T as gt}from"../chunks/Tip.c2ecdbf4.js";import{D as z}from"../chunks/Docstring.17db21ae.js";import{C as Ve}from"../chunks/CodeBlock.54a9f38d.js";import{F as es,M as Un}from"../chunks/Markdown.8ab98a13.js";import{E as Ge}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as ts}from"../chunks/PipelineTag.44585822.js";import{H as we}from"../chunks/Heading.342b1fa6.js";function os(C){let e,m="Example:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBDb25maWclMkMlMjBDTElQTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUENvbmZpZyUyMHdpdGglMjBvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQ0xJUENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMENMSVBNb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBDTElQTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmlnJTBBJTBBJTIzJTIwV2UlMjBjYW4lMjBhbHNvJTIwaW5pdGlhbGl6ZSUyMGElMjBDTElQQ29uZmlnJTIwZnJvbSUyMGElMjBDTElQVGV4dENvbmZpZyUyMGFuZCUyMGElMjBDTElQVmlzaW9uQ29uZmlnJTBBZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBUZXh0Q29uZmlnJTJDJTIwQ0xJUFZpc2lvbkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQVGV4dCUyMGFuZCUyMENMSVBWaXNpb24lMjBjb25maWd1cmF0aW9uJTBBY29uZmlnX3RleHQlMjAlM0QlMjBDTElQVGV4dENvbmZpZygpJTBBY29uZmlnX3Zpc2lvbiUyMCUzRCUyMENMSVBWaXNpb25Db25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMENMSVBDb25maWcuZnJvbV90ZXh0X3Zpc2lvbl9jb25maWdzKGNvbmZpZ190ZXh0JTJDJTIwY29uZmlnX3Zpc2lvbik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPConfig, CLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPConfig with openai/clip-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPModel (with random weights) from the openai/clip-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a CLIPConfig from a CLIPTextConfig and a CLIPVisionConfig</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPTextConfig, CLIPVisionConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPText and CLIPVision configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = CLIPTextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = CLIPVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = CLIPConfig.from_text_vision_configs(config_text, config_vision)`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-11lpom8"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function ns(C){let e,m="Example:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBUZXh0Q29uZmlnJTJDJTIwQ0xJUFRleHRNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQVGV4dENvbmZpZyUyMHdpdGglMjBvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQ0xJUFRleHRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBDTElQVGV4dE1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMENMSVBUZXh0TW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPTextConfig, CLIPTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPTextConfig with openai/clip-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPTextModel (with random weights) from the openai/clip-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-11lpom8"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function ss(C){let e,m="Example:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENMSVBWaXNpb25Db25maWclMkMlMjBDTElQVmlzaW9uTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ0xJUFZpc2lvbkNvbmZpZyUyMHdpdGglMjBvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQ0xJUFZpc2lvbkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMENMSVBWaXNpb25Nb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBDTElQVmlzaW9uTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPVisionConfig, CLIPVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPVisionConfig with openai/clip-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CLIPVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a CLIPVisionModel (with random weights) from the openai/clip-vit-base-patch32 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-11lpom8"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function as(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function rs(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQ0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoJTBBJTIwJTIwJTIwJTIwdGV4dCUzRCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUyMHBhZGRpbmclM0RUcnVlJTBBKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsb2dpdHNfcGVyX2ltYWdlJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHNfcGVyX2ltYWdlJTIwJTIwJTIzJTIwdGhpcyUyMGlzJTIwdGhlJTIwaW1hZ2UtdGV4dCUyMHNpbWlsYXJpdHklMjBzY29yZSUwQXByb2JzJTIwJTNEJTIwbG9naXRzX3Blcl9pbWFnZS5zb2Z0bWF4KGRpbSUzRDEpJTIwJTIwJTIzJTIwd2UlMjBjYW4lMjB0YWtlJTIwdGhlJTIwc29mdG1heCUyMHRvJTIwZ2V0JTIwdGhlJTIwbGFiZWwlMjBwcm9iYWJpbGl0aWVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function is(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function ls(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMENMSVBNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXRleHRfZmVhdHVyZXMlMjAlM0QlMjBtb2RlbC5nZXRfdGV4dF9mZWF0dXJlcygqKmlucHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function ds(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function cs(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQ0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQWltYWdlX2ZlYXR1cmVzJTIwJTNEJTIwbW9kZWwuZ2V0X2ltYWdlX2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function ps(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function ms(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQVGV4dE1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBDTElQVGV4dE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQXBvb2xlZF9vdXRwdXQlMjAlM0QlMjBvdXRwdXRzLnBvb2xlcl9vdXRwdXQlMjAlMjAlMjMlMjBwb29sZWQlMjAoRU9TJTIwdG9rZW4pJTIwc3RhdGVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPTextModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function us(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function fs(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBDTElQVGV4dE1vZGVsV2l0aFByb2plY3Rpb24lMEElMEFtb2RlbCUyMCUzRCUyMENMSVBUZXh0TW9kZWxXaXRoUHJvamVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEF0ZXh0X2VtYmVkcyUyMCUzRCUyMG91dHB1dHMudGV4dF9lbWJlZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, CLIPTextModelWithProjection

<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPTextModelWithProjection.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_embeds = outputs.text_embeds`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function hs(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function gs(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBWaXNpb25Nb2RlbFdpdGhQcm9qZWN0aW9uJTBBJTBBbW9kZWwlMjAlM0QlMjBDTElQVmlzaW9uTW9kZWxXaXRoUHJvamVjdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBaW1hZ2VfZW1iZWRzJTIwJTNEJTIwb3V0cHV0cy5pbWFnZV9lbWJlZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPVisionModelWithProjection

<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPVisionModelWithProjection.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_embeds = outputs.image_embeds`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function _s(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function bs(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMENMSVBWaXNpb25Nb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQ0xJUFZpc2lvbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZSUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFwb29sZWRfb3V0cHV0JTIwJTNEJTIwb3V0cHV0cy5wb29sZXJfb3V0cHV0JTIwJTIwJTIzJTIwcG9vbGVkJTIwQ0xTJTIwc3RhdGVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, CLIPVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPVisionModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function Ts(C){let e,m,o,s,T,t,w=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Se,N,Ye=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,De,F,fe,te,k,ze='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a> forward method, overrides the <code>__call__</code> special method.',Ae,q,at,ce,Ue,K,Ie,oe,ve,Ce='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a> forward method, overrides the <code>__call__</code> special method.',ut,he,xe,Me,Ze,G,ge,R,Pe,je='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a> forward method, overrides the <code>__call__</code> special method.',rt,ye,qe,Q,Be,ne,ae,V,Oe,_e,be,Wt=`The text model from CLIP without any head or projection on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,S,pe,it=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Re,H,Ke,re,me,lt='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTextModel">CLIPTextModel</a> forward method, overrides the <code>__call__</code> special method.',ue,E,It,Y,ke,Ee,U,Z,$e,Fe,He,vt="CLIP Text Model with a projection layer on top (a linear layer on top of the pooled output).",ee,Ne,et=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,D,I,j=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,A,X,O,Le,ie,le='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTextModelWithProjection">CLIPTextModelWithProjection</a> forward method, overrides the <code>__call__</code> special method.',Te,y,x,de,We,dt,_t,B,ct,tt,Je,ot="CLIP Vision Model with a projection layer on top (a linear layer on top of the pooled output).",kt,Gt,bt=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,qt,Ot,io=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Wo,Tt,Lt,Vo,Kt,Rt='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionModelWithProjection">CLIPVisionModelWithProjection</a> forward method, overrides the <code>__call__</code> special method.',po,Mt,mo,se,Zt,Et,Ht,yt,Xt,eo,lo,Zo=`The vision model from CLIP without any head or projection on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,$t,Jt,Bo=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,to,Ct,Qt,oo,co,No='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionModel">CLIPVisionModel</a> forward method, overrides the <code>__call__</code> special method.',no,St,Mo,ft,Bt;return e=new we({props:{title:"CLIPModel",local:"transformers.CLIPModel",headingTag:"h2"}}),s=new z({props:{name:"class transformers.CLIPModel",anchor:"transformers.CLIPModel",parameters:[{name:"config",val:": CLIPConfig"}],parametersDescription:[{anchor:"transformers.CLIPModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L933"}}),fe=new z({props:{name:"forward",anchor:"transformers.CLIPModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"return_loss",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.CLIPModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L1066",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clip.modeling_clip.CLIPOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) — Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) — The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) — The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) — The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTextModel"
>CLIPTextModel</a>.</li>
<li><strong>image_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) — The image embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionModel"
>CLIPVisionModel</a>.</li>
<li><strong>text_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTextModel"
>CLIPTextModel</a>.</li>
<li><strong>vision_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionModel"
>CLIPVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clip.modeling_clip.CLIPOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),q=new gt({props:{$$slots:{default:[as]},$$scope:{ctx:C}}}),ce=new Ge({props:{anchor:"transformers.CLIPModel.forward.example",$$slots:{default:[rs]},$$scope:{ctx:C}}}),Ie=new z({props:{name:"get_text_features",anchor:"transformers.CLIPModel.get_text_features",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L970",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTextModel"
>CLIPTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),he=new gt({props:{$$slots:{default:[is]},$$scope:{ctx:C}}}),Me=new Ge({props:{anchor:"transformers.CLIPModel.get_text_features.example",$$slots:{default:[ls]},$$scope:{ctx:C}}}),ge=new z({props:{name:"get_image_features",anchor:"transformers.CLIPModel.get_image_features",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L1017",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionModel"
>CLIPVisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),ye=new gt({props:{$$slots:{default:[ds]},$$scope:{ctx:C}}}),Q=new Ge({props:{anchor:"transformers.CLIPModel.get_image_features.example",$$slots:{default:[cs]},$$scope:{ctx:C}}}),ne=new we({props:{title:"CLIPTextModel",local:"transformers.CLIPTextModel",headingTag:"h2"}}),Oe=new z({props:{name:"class transformers.CLIPTextModel",anchor:"transformers.CLIPTextModel",parameters:[{name:"config",val:": CLIPTextConfig"}],parametersDescription:[{anchor:"transformers.CLIPTextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L755"}}),Ke=new z({props:{name:"forward",anchor:"transformers.CLIPTextModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L776",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),E=new gt({props:{$$slots:{default:[ps]},$$scope:{ctx:C}}}),Y=new Ge({props:{anchor:"transformers.CLIPTextModel.forward.example",$$slots:{default:[ms]},$$scope:{ctx:C}}}),Ee=new we({props:{title:"CLIPTextModelWithProjection",local:"transformers.CLIPTextModelWithProjection",headingTag:"h2"}}),$e=new z({props:{name:"class transformers.CLIPTextModelWithProjection",anchor:"transformers.CLIPTextModelWithProjection",parameters:[{name:"config",val:": CLIPTextConfig"}],parametersDescription:[{anchor:"transformers.CLIPTextModelWithProjection.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L1160"}}),O=new z({props:{name:"forward",anchor:"transformers.CLIPTextModelWithProjection.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPTextModelWithProjection.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.CLIPTextModelWithProjection.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.CLIPTextModelWithProjection.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.CLIPTextModelWithProjection.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPTextModelWithProjection.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPTextModelWithProjection.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L1187",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clip.modeling_clip.CLIPTextModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>text_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code> <em>optional</em> returned when model is initialized with <code>with_projection=True</code>) — The text embeddings obtained by applying the projection layer to the pooler_output.</p>
</li>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clip.modeling_clip.CLIPTextModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),y=new gt({props:{$$slots:{default:[us]},$$scope:{ctx:C}}}),de=new Ge({props:{anchor:"transformers.CLIPTextModelWithProjection.forward.example",$$slots:{default:[fs]},$$scope:{ctx:C}}}),dt=new we({props:{title:"CLIPVisionModelWithProjection",local:"transformers.CLIPVisionModelWithProjection",headingTag:"h2"}}),ct=new z({props:{name:"class transformers.CLIPVisionModelWithProjection",anchor:"transformers.CLIPVisionModelWithProjection",parameters:[{name:"config",val:": CLIPVisionConfig"}],parametersDescription:[{anchor:"transformers.CLIPVisionModelWithProjection.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L1241"}}),Lt=new z({props:{name:"forward",anchor:"transformers.CLIPVisionModelWithProjection.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPVisionModelWithProjection.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPVisionModelWithProjection.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPVisionModelWithProjection.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPVisionModelWithProjection.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L1264",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clip.modeling_clip.CLIPVisionModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>image_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim)</code> <em>optional</em> returned when model is initialized with <code>with_projection=True</code>) — The image embeddings obtained by applying the projection layer to the pooler_output.</p>
</li>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clip.modeling_clip.CLIPVisionModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Mt=new gt({props:{$$slots:{default:[hs]},$$scope:{ctx:C}}}),se=new Ge({props:{anchor:"transformers.CLIPVisionModelWithProjection.forward.example",$$slots:{default:[gs]},$$scope:{ctx:C}}}),Et=new we({props:{title:"CLIPVisionModel",local:"transformers.CLIPVisionModel",headingTag:"h2"}}),Xt=new z({props:{name:"class transformers.CLIPVisionModel",anchor:"transformers.CLIPVisionModel",parameters:[{name:"config",val:": CLIPVisionConfig"}],parametersDescription:[{anchor:"transformers.CLIPVisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L874"}}),Qt=new z({props:{name:"forward",anchor:"transformers.CLIPVisionModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.CLIPVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.CLIPVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CLIPVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_clip.py#L892",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),St=new gt({props:{$$slots:{default:[_s]},$$scope:{ctx:C}}}),ft=new Ge({props:{anchor:"transformers.CLIPVisionModel.forward.example",$$slots:{default:[bs]},$$scope:{ctx:C}}}),{c(){u(e.$$.fragment),m=i(),o=d("div"),u(s.$$.fragment),T=i(),t=d("p"),t.innerHTML=w,Se=i(),N=d("p"),N.innerHTML=Ye,De=i(),F=d("div"),u(fe.$$.fragment),te=i(),k=d("p"),k.innerHTML=ze,Ae=i(),u(q.$$.fragment),at=i(),u(ce.$$.fragment),Ue=i(),K=d("div"),u(Ie.$$.fragment),oe=i(),ve=d("p"),ve.innerHTML=Ce,ut=i(),u(he.$$.fragment),xe=i(),u(Me.$$.fragment),Ze=i(),G=d("div"),u(ge.$$.fragment),R=i(),Pe=d("p"),Pe.innerHTML=je,rt=i(),u(ye.$$.fragment),qe=i(),u(Q.$$.fragment),Be=i(),u(ne.$$.fragment),ae=i(),V=d("div"),u(Oe.$$.fragment),_e=i(),be=d("p"),be.innerHTML=Wt,S=i(),pe=d("p"),pe.innerHTML=it,Re=i(),H=d("div"),u(Ke.$$.fragment),re=i(),me=d("p"),me.innerHTML=lt,ue=i(),u(E.$$.fragment),It=i(),u(Y.$$.fragment),ke=i(),u(Ee.$$.fragment),U=i(),Z=d("div"),u($e.$$.fragment),Fe=i(),He=d("p"),He.textContent=vt,ee=i(),Ne=d("p"),Ne.innerHTML=et,D=i(),I=d("p"),I.innerHTML=j,A=i(),X=d("div"),u(O.$$.fragment),Le=i(),ie=d("p"),ie.innerHTML=le,Te=i(),u(y.$$.fragment),x=i(),u(de.$$.fragment),We=i(),u(dt.$$.fragment),_t=i(),B=d("div"),u(ct.$$.fragment),tt=i(),Je=d("p"),Je.textContent=ot,kt=i(),Gt=d("p"),Gt.innerHTML=bt,qt=i(),Ot=d("p"),Ot.innerHTML=io,Wo=i(),Tt=d("div"),u(Lt.$$.fragment),Vo=i(),Kt=d("p"),Kt.innerHTML=Rt,po=i(),u(Mt.$$.fragment),mo=i(),u(se.$$.fragment),Zt=i(),u(Et.$$.fragment),Ht=i(),yt=d("div"),u(Xt.$$.fragment),eo=i(),lo=d("p"),lo.innerHTML=Zo,$t=i(),Jt=d("p"),Jt.innerHTML=Bo,to=i(),Ct=d("div"),u(Qt.$$.fragment),oo=i(),co=d("p"),co.innerHTML=No,no=i(),u(St.$$.fragment),Mo=i(),u(ft.$$.fragment),this.h()},l(M){f(e.$$.fragment,M),m=l(M),o=c(M,"DIV",{class:!0});var P=L(o);f(s.$$.fragment,P),T=l(P),t=c(P,"P",{"data-svelte-h":!0}),$(t)!=="svelte-eisylu"&&(t.innerHTML=w),Se=l(P),N=c(P,"P",{"data-svelte-h":!0}),$(N)!=="svelte-hswkmf"&&(N.innerHTML=Ye),De=l(P),F=c(P,"DIV",{class:!0});var zt=L(F);f(fe.$$.fragment,zt),te=l(zt),k=c(zt,"P",{"data-svelte-h":!0}),$(k)!=="svelte-2sfugn"&&(k.innerHTML=ze),Ae=l(zt),f(q.$$.fragment,zt),at=l(zt),f(ce.$$.fragment,zt),zt.forEach(a),Ue=l(P),K=c(P,"DIV",{class:!0});var xt=L(K);f(Ie.$$.fragment,xt),oe=l(xt),ve=c(xt,"P",{"data-svelte-h":!0}),$(ve)!=="svelte-2sfugn"&&(ve.innerHTML=Ce),ut=l(xt),f(he.$$.fragment,xt),xe=l(xt),f(Me.$$.fragment,xt),xt.forEach(a),Ze=l(P),G=c(P,"DIV",{class:!0});var Xe=L(G);f(ge.$$.fragment,Xe),R=l(Xe),Pe=c(Xe,"P",{"data-svelte-h":!0}),$(Pe)!=="svelte-2sfugn"&&(Pe.innerHTML=je),rt=l(Xe),f(ye.$$.fragment,Xe),qe=l(Xe),f(Q.$$.fragment,Xe),Xe.forEach(a),P.forEach(a),Be=l(M),f(ne.$$.fragment,M),ae=l(M),V=c(M,"DIV",{class:!0});var nt=L(V);f(Oe.$$.fragment,nt),_e=l(nt),be=c(nt,"P",{"data-svelte-h":!0}),$(be)!=="svelte-1wbbgj5"&&(be.innerHTML=Wt),S=l(nt),pe=c(nt,"P",{"data-svelte-h":!0}),$(pe)!=="svelte-hswkmf"&&(pe.innerHTML=it),Re=l(nt),H=c(nt,"DIV",{class:!0});var Pt=L(H);f(Ke.$$.fragment,Pt),re=l(Pt),me=c(Pt,"P",{"data-svelte-h":!0}),$(me)!=="svelte-na6w4z"&&(me.innerHTML=lt),ue=l(Pt),f(E.$$.fragment,Pt),It=l(Pt),f(Y.$$.fragment,Pt),Pt.forEach(a),nt.forEach(a),ke=l(M),f(Ee.$$.fragment,M),U=l(M),Z=c(M,"DIV",{class:!0});var Qe=L(Z);f($e.$$.fragment,Qe),Fe=l(Qe),He=c(Qe,"P",{"data-svelte-h":!0}),$(He)!=="svelte-1gij27p"&&(He.textContent=vt),ee=l(Qe),Ne=c(Qe,"P",{"data-svelte-h":!0}),$(Ne)!=="svelte-eisylu"&&(Ne.innerHTML=et),D=l(Qe),I=c(Qe,"P",{"data-svelte-h":!0}),$(I)!=="svelte-hswkmf"&&(I.innerHTML=j),A=l(Qe),X=c(Qe,"DIV",{class:!0});var Ut=L(X);f(O.$$.fragment,Ut),Le=l(Ut),ie=c(Ut,"P",{"data-svelte-h":!0}),$(ie)!=="svelte-1fa6nq3"&&(ie.innerHTML=le),Te=l(Ut),f(y.$$.fragment,Ut),x=l(Ut),f(de.$$.fragment,Ut),Ut.forEach(a),Qe.forEach(a),We=l(M),f(dt.$$.fragment,M),_t=l(M),B=c(M,"DIV",{class:!0});var ht=L(B);f(ct.$$.fragment,ht),tt=l(ht),Je=c(ht,"P",{"data-svelte-h":!0}),$(Je)!=="svelte-sjom46"&&(Je.textContent=ot),kt=l(ht),Gt=c(ht,"P",{"data-svelte-h":!0}),$(Gt)!=="svelte-eisylu"&&(Gt.innerHTML=bt),qt=l(ht),Ot=c(ht,"P",{"data-svelte-h":!0}),$(Ot)!=="svelte-hswkmf"&&(Ot.innerHTML=io),Wo=l(ht),Tt=c(ht,"DIV",{class:!0});var pt=L(Tt);f(Lt.$$.fragment,pt),Vo=l(pt),Kt=c(pt,"P",{"data-svelte-h":!0}),$(Kt)!=="svelte-11wplub"&&(Kt.innerHTML=Rt),po=l(pt),f(Mt.$$.fragment,pt),mo=l(pt),f(se.$$.fragment,pt),pt.forEach(a),ht.forEach(a),Zt=l(M),f(Et.$$.fragment,M),Ht=l(M),yt=c(M,"DIV",{class:!0});var st=L(yt);f(Xt.$$.fragment,st),eo=l(st),lo=c(st,"P",{"data-svelte-h":!0}),$(lo)!=="svelte-1vmqigk"&&(lo.innerHTML=Zo),$t=l(st),Jt=c(st,"P",{"data-svelte-h":!0}),$(Jt)!=="svelte-hswkmf"&&(Jt.innerHTML=Bo),to=l(st),Ct=c(st,"DIV",{class:!0});var wt=L(Ct);f(Qt.$$.fragment,wt),oo=l(wt),co=c(wt,"P",{"data-svelte-h":!0}),$(co)!=="svelte-w3vuer"&&(co.innerHTML=No),no=l(wt),f(St.$$.fragment,wt),Mo=l(wt),f(ft.$$.fragment,wt),wt.forEach(a),st.forEach(a),this.h()},h(){J(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(o,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Tt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Ct,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(M,P){h(e,M,P),p(M,m,P),p(M,o,P),h(s,o,null),r(o,T),r(o,t),r(o,Se),r(o,N),r(o,De),r(o,F),h(fe,F,null),r(F,te),r(F,k),r(F,Ae),h(q,F,null),r(F,at),h(ce,F,null),r(o,Ue),r(o,K),h(Ie,K,null),r(K,oe),r(K,ve),r(K,ut),h(he,K,null),r(K,xe),h(Me,K,null),r(o,Ze),r(o,G),h(ge,G,null),r(G,R),r(G,Pe),r(G,rt),h(ye,G,null),r(G,qe),h(Q,G,null),p(M,Be,P),h(ne,M,P),p(M,ae,P),p(M,V,P),h(Oe,V,null),r(V,_e),r(V,be),r(V,S),r(V,pe),r(V,Re),r(V,H),h(Ke,H,null),r(H,re),r(H,me),r(H,ue),h(E,H,null),r(H,It),h(Y,H,null),p(M,ke,P),h(Ee,M,P),p(M,U,P),p(M,Z,P),h($e,Z,null),r(Z,Fe),r(Z,He),r(Z,ee),r(Z,Ne),r(Z,D),r(Z,I),r(Z,A),r(Z,X),h(O,X,null),r(X,Le),r(X,ie),r(X,Te),h(y,X,null),r(X,x),h(de,X,null),p(M,We,P),h(dt,M,P),p(M,_t,P),p(M,B,P),h(ct,B,null),r(B,tt),r(B,Je),r(B,kt),r(B,Gt),r(B,qt),r(B,Ot),r(B,Wo),r(B,Tt),h(Lt,Tt,null),r(Tt,Vo),r(Tt,Kt),r(Tt,po),h(Mt,Tt,null),r(Tt,mo),h(se,Tt,null),p(M,Zt,P),h(Et,M,P),p(M,Ht,P),p(M,yt,P),h(Xt,yt,null),r(yt,eo),r(yt,lo),r(yt,$t),r(yt,Jt),r(yt,to),r(yt,Ct),h(Qt,Ct,null),r(Ct,oo),r(Ct,co),r(Ct,no),h(St,Ct,null),r(Ct,Mo),h(ft,Ct,null),Bt=!0},p(M,P){const zt={};P&2&&(zt.$$scope={dirty:P,ctx:M}),q.$set(zt);const xt={};P&2&&(xt.$$scope={dirty:P,ctx:M}),ce.$set(xt);const Xe={};P&2&&(Xe.$$scope={dirty:P,ctx:M}),he.$set(Xe);const nt={};P&2&&(nt.$$scope={dirty:P,ctx:M}),Me.$set(nt);const Pt={};P&2&&(Pt.$$scope={dirty:P,ctx:M}),ye.$set(Pt);const Qe={};P&2&&(Qe.$$scope={dirty:P,ctx:M}),Q.$set(Qe);const Ut={};P&2&&(Ut.$$scope={dirty:P,ctx:M}),E.$set(Ut);const ht={};P&2&&(ht.$$scope={dirty:P,ctx:M}),Y.$set(ht);const pt={};P&2&&(pt.$$scope={dirty:P,ctx:M}),y.$set(pt);const st={};P&2&&(st.$$scope={dirty:P,ctx:M}),de.$set(st);const wt={};P&2&&(wt.$$scope={dirty:P,ctx:M}),Mt.$set(wt);const so={};P&2&&(so.$$scope={dirty:P,ctx:M}),se.$set(so);const yo={};P&2&&(yo.$$scope={dirty:P,ctx:M}),St.$set(yo);const mt={};P&2&&(mt.$$scope={dirty:P,ctx:M}),ft.$set(mt)},i(M){Bt||(g(e.$$.fragment,M),g(s.$$.fragment,M),g(fe.$$.fragment,M),g(q.$$.fragment,M),g(ce.$$.fragment,M),g(Ie.$$.fragment,M),g(he.$$.fragment,M),g(Me.$$.fragment,M),g(ge.$$.fragment,M),g(ye.$$.fragment,M),g(Q.$$.fragment,M),g(ne.$$.fragment,M),g(Oe.$$.fragment,M),g(Ke.$$.fragment,M),g(E.$$.fragment,M),g(Y.$$.fragment,M),g(Ee.$$.fragment,M),g($e.$$.fragment,M),g(O.$$.fragment,M),g(y.$$.fragment,M),g(de.$$.fragment,M),g(dt.$$.fragment,M),g(ct.$$.fragment,M),g(Lt.$$.fragment,M),g(Mt.$$.fragment,M),g(se.$$.fragment,M),g(Et.$$.fragment,M),g(Xt.$$.fragment,M),g(Qt.$$.fragment,M),g(St.$$.fragment,M),g(ft.$$.fragment,M),Bt=!0)},o(M){_(e.$$.fragment,M),_(s.$$.fragment,M),_(fe.$$.fragment,M),_(q.$$.fragment,M),_(ce.$$.fragment,M),_(Ie.$$.fragment,M),_(he.$$.fragment,M),_(Me.$$.fragment,M),_(ge.$$.fragment,M),_(ye.$$.fragment,M),_(Q.$$.fragment,M),_(ne.$$.fragment,M),_(Oe.$$.fragment,M),_(Ke.$$.fragment,M),_(E.$$.fragment,M),_(Y.$$.fragment,M),_(Ee.$$.fragment,M),_($e.$$.fragment,M),_(O.$$.fragment,M),_(y.$$.fragment,M),_(de.$$.fragment,M),_(dt.$$.fragment,M),_(ct.$$.fragment,M),_(Lt.$$.fragment,M),_(Mt.$$.fragment,M),_(se.$$.fragment,M),_(Et.$$.fragment,M),_(Xt.$$.fragment,M),_(Qt.$$.fragment,M),_(St.$$.fragment,M),_(ft.$$.fragment,M),Bt=!1},d(M){M&&(a(m),a(o),a(Be),a(ae),a(V),a(ke),a(U),a(Z),a(We),a(_t),a(B),a(Zt),a(Ht),a(yt)),b(e,M),b(s),b(fe),b(q),b(ce),b(Ie),b(he),b(Me),b(ge),b(ye),b(Q),b(ne,M),b(Oe),b(Ke),b(E),b(Y),b(Ee,M),b($e),b(O),b(y),b(de),b(dt,M),b(ct),b(Lt),b(Mt),b(se),b(Et,M),b(Xt),b(Qt),b(St),b(ft)}}}function Ms(C){let e,m;return e=new Un({props:{$$slots:{default:[Ts]},$$scope:{ctx:C}}}),{c(){u(e.$$.fragment)},l(o){f(e.$$.fragment,o)},m(o,s){h(e,o,s),m=!0},p(o,s){const T={};s&2&&(T.$$scope={dirty:s,ctx:o}),e.$set(T)},i(o){m||(g(e.$$.fragment,o),m=!0)},o(o){_(e.$$.fragment,o),m=!1},d(o){b(e,o)}}}function ys(C){let e,m="TensorFlow models and layers in <code>transformers</code> accept two formats as input:",o,s,T="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional argument.</li>",t,w,Se=`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like <code>model.fit()</code> things should “just work” for you - just
pass your inputs and labels in any format that <code>model.fit()</code> supports! If, however, you want to use the second
format outside of Keras methods like <code>fit()</code> and <code>predict()</code>, such as when creating your own layers or models with
the Keras <code>Functional</code> API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`,N,Ye,De=`<li>a single Tensor with <code>input_ids</code> only and nothing else: <code>model(input_ids)</code></li> <li>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code>model([input_ids, attention_mask])</code> or <code>model([input_ids, attention_mask, token_type_ids])</code></li> <li>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code>model({&quot;input_ids&quot;: input_ids, &quot;token_type_ids&quot;: token_type_ids})</code></li>`,F,fe,te=`Note that when creating models and layers with
<a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="nofollow">subclassing</a> then you don’t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`;return{c(){e=d("p"),e.innerHTML=m,o=i(),s=d("ul"),s.innerHTML=T,t=i(),w=d("p"),w.innerHTML=Se,N=i(),Ye=d("ul"),Ye.innerHTML=De,F=i(),fe=d("p"),fe.innerHTML=te},l(k){e=c(k,"P",{"data-svelte-h":!0}),$(e)!=="svelte-1ajbfxg"&&(e.innerHTML=m),o=l(k),s=c(k,"UL",{"data-svelte-h":!0}),$(s)!=="svelte-qm1t26"&&(s.innerHTML=T),t=l(k),w=c(k,"P",{"data-svelte-h":!0}),$(w)!=="svelte-1v9qsc5"&&(w.innerHTML=Se),N=l(k),Ye=c(k,"UL",{"data-svelte-h":!0}),$(Ye)!=="svelte-15scerc"&&(Ye.innerHTML=De),F=l(k),fe=c(k,"P",{"data-svelte-h":!0}),$(fe)!=="svelte-1an3odd"&&(fe.innerHTML=te)},m(k,ze){p(k,e,ze),p(k,o,ze),p(k,s,ze),p(k,t,ze),p(k,w,ze),p(k,N,ze),p(k,Ye,ze),p(k,F,ze),p(k,fe,ze)},p:W,d(k){k&&(a(e),a(o),a(s),a(t),a(w),a(N),a(Ye),a(F),a(fe))}}}function $s(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function ws(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBdXRvUHJvY2Vzc29yJTJDJTIwVEZDTElQTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMFRGQ0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoJTBBJTIwJTIwJTIwJTIwdGV4dCUzRCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMiUyQyUyMHBhZGRpbmclM0RUcnVlJTBBKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsb2dpdHNfcGVyX2ltYWdlJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHNfcGVyX2ltYWdlJTIwJTIwJTIzJTIwdGhpcyUyMGlzJTIwdGhlJTIwaW1hZ2UtdGV4dCUyMHNpbWlsYXJpdHklMjBzY29yZSUwQXByb2JzJTIwJTNEJTIwdGYubm4uc29mdG1heChsb2dpdHNfcGVyX2ltYWdlJTJDJTIwYXhpcyUzRDEpJTIwJTIwJTIzJTIwd2UlMjBjYW4lMjB0YWtlJTIwdGhlJTIwc29mdG1heCUyMHRvJTIwZ2V0JTIwdGhlJTIwbGFiZWwlMjBwcm9iYWJpbGl0aWVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, TFCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFCLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = tf.nn.softmax(logits_per_image, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function Is(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function vs(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBURkNMSVBNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwVEZDTElQTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMiklMEF0ZXh0X2ZlYXR1cmVzJTIwJTNEJTIwbW9kZWwuZ2V0X3RleHRfZmVhdHVyZXMoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFCLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function Cs(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function xs(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMFRGQ0xJUE1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBURkNMSVBNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMiklMEElMEFpbWFnZV9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF9pbWFnZV9mZWF0dXJlcygqKmlucHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, TFCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFCLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function Ps(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function js(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBURkNMSVBUZXh0TW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMFRGQ0xJUFRleHRNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZSUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFwb29sZWRfb3V0cHV0JTIwJTNEJTIwb3V0cHV0cy5wb29sZXJfb3V0cHV0JTIwJTIwJTIzJTIwcG9vbGVkJTIwKEVPUyUyMHRva2VuKSUyMHN0YXRlcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, TFCLIPTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFCLIPTextModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function ks(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function Ls(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMFRGQ0xJUFZpc2lvbk1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBURkNMSVBWaXNpb25Nb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGUlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBcG9vbGVkX291dHB1dCUyMCUzRCUyMG91dHB1dHMucG9vbGVyX291dHB1dCUyMCUyMCUyMyUyMHBvb2xlZCUyMENMUyUyMHN0YXRlcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, TFCLIPVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFCLIPVisionModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function Js(C){let e,m,o,s,T,t,w=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Se,N,Ye=`This model is also a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a> subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`,De,F,fe,te,k,ze,Ae,q='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> forward method, overrides the <code>__call__</code> special method.',at,ce,Ue,K,Ie,oe,ve,Ce,ut,he='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> forward method, overrides the <code>__call__</code> special method.',xe,Me,Ze,G,ge,R,Pe,je,rt,ye='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPModel">TFCLIPModel</a> forward method, overrides the <code>__call__</code> special method.',qe,Q,Be,ne,ae,V,Oe,_e,be,Wt,S,pe,it,Re,H='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPTextModel">TFCLIPTextModel</a> forward method, overrides the <code>__call__</code> special method.',Ke,re,me,lt,ue,E,It,Y,ke,Ee,U,Z,$e,Fe,He='The <a href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPVisionModel">TFCLIPVisionModel</a> forward method, overrides the <code>__call__</code> special method.',vt,ee,Ne,et,D;return e=new we({props:{title:"TFCLIPModel",local:"transformers.TFCLIPModel",headingTag:"h2"}}),s=new z({props:{name:"class transformers.TFCLIPModel",anchor:"transformers.TFCLIPModel",parameters:[{name:"config",val:": CLIPConfig"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFCLIPModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_tf_clip.py#L1306"}}),F=new gt({props:{$$slots:{default:[ys]},$$scope:{ctx:C}}}),k=new z({props:{name:"call",anchor:"transformers.TFCLIPModel.call",parameters:[{name:"input_ids",val:": TFModelInputType | None = None"},{name:"pixel_values",val:": TFModelInputType | None = None"},{name:"attention_mask",val:": np.ndarray | tf.Tensor | None = None"},{name:"position_ids",val:": np.ndarray | tf.Tensor | None = None"},{name:"return_loss",val:": Optional[bool] = None"},{name:"output_attentions",val:": Optional[bool] = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFCLIPModel.call.input_ids",description:`<strong>input_ids</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFCLIPModel.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> <code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFCLIPModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFCLIPModel.call.position_ids",description:`<strong>position_ids</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.TFCLIPModel.call.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.TFCLIPModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFCLIPModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFCLIPModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFCLIPModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to \`False&#x201C;) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_tf_clip.py#L1397",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clip.modeling_tf_clip.TFCLIPOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) — Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image:(<code>tf.Tensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) — The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text:(<code>tf.Tensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) — The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds(<code>tf.Tensor</code></strong> of shape <code>(batch_size, output_dim</code>) — The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPTextModel"
>TFCLIPTextModel</a>.</li>
<li><strong>image_embeds(<code>tf.Tensor</code></strong> of shape <code>(batch_size, output_dim</code>) — The image embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPVisionModel"
>TFCLIPVisionModel</a>.</li>
<li><strong>text_model_output(<code>~modeling_tf_utils.TFBaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPTextModel"
>TFCLIPTextModel</a>.</li>
<li><strong>vision_model_output(<code>~modeling_tf_utils.TFBaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPVisionModel"
>TFCLIPVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clip.modeling_tf_clip.TFCLIPOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),ce=new gt({props:{$$slots:{default:[$s]},$$scope:{ctx:C}}}),K=new Ge({props:{anchor:"transformers.TFCLIPModel.call.example",$$slots:{default:[ws]},$$scope:{ctx:C}}}),ve=new z({props:{name:"get_text_features",anchor:"transformers.TFCLIPModel.get_text_features",parameters:[{name:"input_ids",val:": TFModelInputType | None = None"},{name:"attention_mask",val:": np.ndarray | tf.Tensor | None = None"},{name:"position_ids",val:": np.ndarray | tf.Tensor | None = None"},{name:"output_attentions",val:": Optional[bool] = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFCLIPModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFCLIPModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFCLIPModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.TFCLIPModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFCLIPModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFCLIPModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFCLIPModel.get_text_features.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to \`False&#x201C;) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_tf_clip.py#L1315",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by applying
the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPTextModel"
>TFCLIPTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>tf.Tensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Me=new gt({props:{$$slots:{default:[Is]},$$scope:{ctx:C}}}),G=new Ge({props:{anchor:"transformers.TFCLIPModel.get_text_features.example",$$slots:{default:[vs]},$$scope:{ctx:C}}}),Pe=new z({props:{name:"get_image_features",anchor:"transformers.TFCLIPModel.get_image_features",parameters:[{name:"pixel_values",val:": TFModelInputType | None = None"},{name:"output_attentions",val:": Optional[bool] = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFCLIPModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details. output_attentions (<code>bool</code>, <em>optional</em>): Whether or not to
return the attentions tensors of all attention layers. See <code>attentions</code> under returned tensors for more
detail. This argument can be used only in eager mode, in graph mode the value in the config will be used
instead.`,name:"pixel_values"},{anchor:"transformers.TFCLIPModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFCLIPModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFCLIPModel.get_image_features.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to \`False&#x201C;) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_tf_clip.py#L1355",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by applying
the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.TFCLIPVisionModel"
>TFCLIPVisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>tf.Tensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Q=new gt({props:{$$slots:{default:[Cs]},$$scope:{ctx:C}}}),ne=new Ge({props:{anchor:"transformers.TFCLIPModel.get_image_features.example",$$slots:{default:[xs]},$$scope:{ctx:C}}}),V=new we({props:{title:"TFCLIPTextModel",local:"transformers.TFCLIPTextModel",headingTag:"h2"}}),be=new z({props:{name:"class transformers.TFCLIPTextModel",anchor:"transformers.TFCLIPTextModel",parameters:[{name:"config",val:": CLIPTextConfig"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_tf_clip.py#L1184"}}),pe=new z({props:{name:"call",anchor:"transformers.TFCLIPTextModel.call",parameters:[{name:"input_ids",val:": TFModelInputType | None = None"},{name:"attention_mask",val:": np.ndarray | tf.Tensor | None = None"},{name:"position_ids",val:": np.ndarray | tf.Tensor | None = None"},{name:"output_attentions",val:": Optional[bool] = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFCLIPTextModel.call.input_ids",description:`<strong>input_ids</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.TFCLIPTextModel.call.attention_mask",description:`<strong>attention_mask</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.TFCLIPTextModel.call.position_ids",description:`<strong>position_ids</strong> (<code>np.ndarray</code> or <code>tf.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.TFCLIPTextModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail. This argument can be used only in eager mode, in graph mode the value in the
config will be used instead.`,name:"output_attentions"},{anchor:"transformers.TFCLIPTextModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFCLIPTextModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFCLIPTextModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to \`False&#x201C;) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_tf_clip.py#L1192",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"
>transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you’re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"
>transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling</a> or <code>tuple(tf.Tensor)</code></p>
`}}),re=new gt({props:{$$slots:{default:[Ps]},$$scope:{ctx:C}}}),lt=new Ge({props:{anchor:"transformers.TFCLIPTextModel.call.example",$$slots:{default:[js]},$$scope:{ctx:C}}}),E=new we({props:{title:"TFCLIPVisionModel",local:"transformers.TFCLIPVisionModel",headingTag:"h2"}}),ke=new z({props:{name:"class transformers.TFCLIPVisionModel",anchor:"transformers.TFCLIPVisionModel",parameters:[{name:"config",val:": CLIPVisionConfig"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_tf_clip.py#L1244"}}),Z=new z({props:{name:"call",anchor:"transformers.TFCLIPVisionModel.call",parameters:[{name:"pixel_values",val:": TFModelInputType | None = None"},{name:"output_attentions",val:": Optional[bool] = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFCLIPVisionModel.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details. output_attentions (<code>bool</code>, <em>optional</em>): Whether or not to
return the attentions tensors of all attention layers. See <code>attentions</code> under returned tensors for more
detail. This argument can be used only in eager mode, in graph mode the value in the config will be used
instead.`,name:"pixel_values"},{anchor:"transformers.TFCLIPVisionModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFCLIPVisionModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFCLIPVisionModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to \`False&#x201C;) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_tf_clip.py#L1253",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"
>transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you’re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"
>transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling</a> or <code>tuple(tf.Tensor)</code></p>
`}}),ee=new gt({props:{$$slots:{default:[ks]},$$scope:{ctx:C}}}),et=new Ge({props:{anchor:"transformers.TFCLIPVisionModel.call.example",$$slots:{default:[Ls]},$$scope:{ctx:C}}}),{c(){u(e.$$.fragment),m=i(),o=d("div"),u(s.$$.fragment),T=i(),t=d("p"),t.innerHTML=w,Se=i(),N=d("p"),N.innerHTML=Ye,De=i(),u(F.$$.fragment),fe=i(),te=d("div"),u(k.$$.fragment),ze=i(),Ae=d("p"),Ae.innerHTML=q,at=i(),u(ce.$$.fragment),Ue=i(),u(K.$$.fragment),Ie=i(),oe=d("div"),u(ve.$$.fragment),Ce=i(),ut=d("p"),ut.innerHTML=he,xe=i(),u(Me.$$.fragment),Ze=i(),u(G.$$.fragment),ge=i(),R=d("div"),u(Pe.$$.fragment),je=i(),rt=d("p"),rt.innerHTML=ye,qe=i(),u(Q.$$.fragment),Be=i(),u(ne.$$.fragment),ae=i(),u(V.$$.fragment),Oe=i(),_e=d("div"),u(be.$$.fragment),Wt=i(),S=d("div"),u(pe.$$.fragment),it=i(),Re=d("p"),Re.innerHTML=H,Ke=i(),u(re.$$.fragment),me=i(),u(lt.$$.fragment),ue=i(),u(E.$$.fragment),It=i(),Y=d("div"),u(ke.$$.fragment),Ee=i(),U=d("div"),u(Z.$$.fragment),$e=i(),Fe=d("p"),Fe.innerHTML=He,vt=i(),u(ee.$$.fragment),Ne=i(),u(et.$$.fragment),this.h()},l(I){f(e.$$.fragment,I),m=l(I),o=c(I,"DIV",{class:!0});var j=L(o);f(s.$$.fragment,j),T=l(j),t=c(j,"P",{"data-svelte-h":!0}),$(t)!=="svelte-x53t1u"&&(t.innerHTML=w),Se=l(j),N=c(j,"P",{"data-svelte-h":!0}),$(N)!=="svelte-1be7e3c"&&(N.innerHTML=Ye),De=l(j),f(F.$$.fragment,j),fe=l(j),te=c(j,"DIV",{class:!0});var A=L(te);f(k.$$.fragment,A),ze=l(A),Ae=c(A,"P",{"data-svelte-h":!0}),$(Ae)!=="svelte-1c6odyb"&&(Ae.innerHTML=q),at=l(A),f(ce.$$.fragment,A),Ue=l(A),f(K.$$.fragment,A),A.forEach(a),Ie=l(j),oe=c(j,"DIV",{class:!0});var X=L(oe);f(ve.$$.fragment,X),Ce=l(X),ut=c(X,"P",{"data-svelte-h":!0}),$(ut)!=="svelte-1c6odyb"&&(ut.innerHTML=he),xe=l(X),f(Me.$$.fragment,X),Ze=l(X),f(G.$$.fragment,X),X.forEach(a),ge=l(j),R=c(j,"DIV",{class:!0});var O=L(R);f(Pe.$$.fragment,O),je=l(O),rt=c(O,"P",{"data-svelte-h":!0}),$(rt)!=="svelte-1c6odyb"&&(rt.innerHTML=ye),qe=l(O),f(Q.$$.fragment,O),Be=l(O),f(ne.$$.fragment,O),O.forEach(a),j.forEach(a),ae=l(I),f(V.$$.fragment,I),Oe=l(I),_e=c(I,"DIV",{class:!0});var Le=L(_e);f(be.$$.fragment,Le),Wt=l(Le),S=c(Le,"DIV",{class:!0});var ie=L(S);f(pe.$$.fragment,ie),it=l(ie),Re=c(ie,"P",{"data-svelte-h":!0}),$(Re)!=="svelte-12kltt3"&&(Re.innerHTML=H),Ke=l(ie),f(re.$$.fragment,ie),me=l(ie),f(lt.$$.fragment,ie),ie.forEach(a),Le.forEach(a),ue=l(I),f(E.$$.fragment,I),It=l(I),Y=c(I,"DIV",{class:!0});var le=L(Y);f(ke.$$.fragment,le),Ee=l(le),U=c(le,"DIV",{class:!0});var Te=L(U);f(Z.$$.fragment,Te),$e=l(Te),Fe=c(Te,"P",{"data-svelte-h":!0}),$(Fe)!=="svelte-wgaoen"&&(Fe.innerHTML=He),vt=l(Te),f(ee.$$.fragment,Te),Ne=l(Te),f(et.$$.fragment,Te),Te.forEach(a),le.forEach(a),this.h()},h(){J(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(o,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(_e,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(I,j){h(e,I,j),p(I,m,j),p(I,o,j),h(s,o,null),r(o,T),r(o,t),r(o,Se),r(o,N),r(o,De),h(F,o,null),r(o,fe),r(o,te),h(k,te,null),r(te,ze),r(te,Ae),r(te,at),h(ce,te,null),r(te,Ue),h(K,te,null),r(o,Ie),r(o,oe),h(ve,oe,null),r(oe,Ce),r(oe,ut),r(oe,xe),h(Me,oe,null),r(oe,Ze),h(G,oe,null),r(o,ge),r(o,R),h(Pe,R,null),r(R,je),r(R,rt),r(R,qe),h(Q,R,null),r(R,Be),h(ne,R,null),p(I,ae,j),h(V,I,j),p(I,Oe,j),p(I,_e,j),h(be,_e,null),r(_e,Wt),r(_e,S),h(pe,S,null),r(S,it),r(S,Re),r(S,Ke),h(re,S,null),r(S,me),h(lt,S,null),p(I,ue,j),h(E,I,j),p(I,It,j),p(I,Y,j),h(ke,Y,null),r(Y,Ee),r(Y,U),h(Z,U,null),r(U,$e),r(U,Fe),r(U,vt),h(ee,U,null),r(U,Ne),h(et,U,null),D=!0},p(I,j){const A={};j&2&&(A.$$scope={dirty:j,ctx:I}),F.$set(A);const X={};j&2&&(X.$$scope={dirty:j,ctx:I}),ce.$set(X);const O={};j&2&&(O.$$scope={dirty:j,ctx:I}),K.$set(O);const Le={};j&2&&(Le.$$scope={dirty:j,ctx:I}),Me.$set(Le);const ie={};j&2&&(ie.$$scope={dirty:j,ctx:I}),G.$set(ie);const le={};j&2&&(le.$$scope={dirty:j,ctx:I}),Q.$set(le);const Te={};j&2&&(Te.$$scope={dirty:j,ctx:I}),ne.$set(Te);const y={};j&2&&(y.$$scope={dirty:j,ctx:I}),re.$set(y);const x={};j&2&&(x.$$scope={dirty:j,ctx:I}),lt.$set(x);const de={};j&2&&(de.$$scope={dirty:j,ctx:I}),ee.$set(de);const We={};j&2&&(We.$$scope={dirty:j,ctx:I}),et.$set(We)},i(I){D||(g(e.$$.fragment,I),g(s.$$.fragment,I),g(F.$$.fragment,I),g(k.$$.fragment,I),g(ce.$$.fragment,I),g(K.$$.fragment,I),g(ve.$$.fragment,I),g(Me.$$.fragment,I),g(G.$$.fragment,I),g(Pe.$$.fragment,I),g(Q.$$.fragment,I),g(ne.$$.fragment,I),g(V.$$.fragment,I),g(be.$$.fragment,I),g(pe.$$.fragment,I),g(re.$$.fragment,I),g(lt.$$.fragment,I),g(E.$$.fragment,I),g(ke.$$.fragment,I),g(Z.$$.fragment,I),g(ee.$$.fragment,I),g(et.$$.fragment,I),D=!0)},o(I){_(e.$$.fragment,I),_(s.$$.fragment,I),_(F.$$.fragment,I),_(k.$$.fragment,I),_(ce.$$.fragment,I),_(K.$$.fragment,I),_(ve.$$.fragment,I),_(Me.$$.fragment,I),_(G.$$.fragment,I),_(Pe.$$.fragment,I),_(Q.$$.fragment,I),_(ne.$$.fragment,I),_(V.$$.fragment,I),_(be.$$.fragment,I),_(pe.$$.fragment,I),_(re.$$.fragment,I),_(lt.$$.fragment,I),_(E.$$.fragment,I),_(ke.$$.fragment,I),_(Z.$$.fragment,I),_(ee.$$.fragment,I),_(et.$$.fragment,I),D=!1},d(I){I&&(a(m),a(o),a(ae),a(Oe),a(_e),a(ue),a(It),a(Y)),b(e,I),b(s),b(F),b(k),b(ce),b(K),b(ve),b(Me),b(G),b(Pe),b(Q),b(ne),b(V,I),b(be),b(pe),b(re),b(lt),b(E,I),b(ke),b(Z),b(ee),b(et)}}}function zs(C){let e,m;return e=new Un({props:{$$slots:{default:[Js]},$$scope:{ctx:C}}}),{c(){u(e.$$.fragment)},l(o){f(e.$$.fragment,o)},m(o,s){h(e,o,s),m=!0},p(o,s){const T={};s&2&&(T.$$scope={dirty:s,ctx:o}),e.$set(T)},i(o){m||(g(e.$$.fragment,o),m=!0)},o(o){_(e.$$.fragment,o),m=!1},d(o){b(e,o)}}}function Us(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function Fs(C){let e,m="Example:",o,s,T;return s=new Ve({props:{code:"aW1wb3J0JTIwamF4JTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEZsYXhDTElQTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEZsYXhDTElQTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMEElMjAlMjAlMjAlMjB0ZXh0JTNEJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMm5wJTIyJTJDJTIwcGFkZGluZyUzRFRydWUlMEEpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0c19wZXJfaW1hZ2UlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0c19wZXJfaW1hZ2UlMjAlMjAlMjMlMjB0aGlzJTIwaXMlMjB0aGUlMjBpbWFnZS10ZXh0JTIwc2ltaWxhcml0eSUyMHNjb3JlJTBBcHJvYnMlMjAlM0QlMjBqYXgubm4uc29mdG1heChsb2dpdHNfcGVyX2ltYWdlJTJDJTIwYXhpcyUzRDEpJTIwJTIwJTIzJTIwd2UlMjBjYW4lMjB0YWtlJTIwdGhlJTIwc29mdG1heCUyMHRvJTIwZ2V0JTIwdGhlJTIwbGFiZWwlMjBwcm9iYWJpbGl0aWVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> jax
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, FlaxCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxCLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = jax.nn.softmax(logits_per_image, axis=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-11lpom8"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function Ws(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBGbGF4Q0xJUE1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBGbGF4Q0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIybnAlMjIpJTBBdGV4dF9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF90ZXh0X2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxCLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function Vs(C){let e,m="Examples:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEZsYXhDTElQTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEZsYXhDTElQTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIybnAlMjIpJTBBJTBBaW1hZ2VfZmVhdHVyZXMlMjAlM0QlMjBtb2RlbC5nZXRfaW1hZ2VfZmVhdHVyZXMoKippbnB1dHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, FlaxCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxCLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-kvfsh7"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function Zs(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function Bs(C){let e,m="Example:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBGbGF4Q0xJUFRleHRNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwRmxheENMSVBUZXh0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMHBhZGRpbmclM0RUcnVlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJucCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGUlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBcG9vbGVyX291dHB1dCUyMCUzRCUyMG91dHB1dHMucG9vbGVyX291dHB1dCUyMCUyMCUyMyUyMHBvb2xlZCUyMChFT1MlMjB0b2tlbiklMjBzdGF0ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxCLIPTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxCLIPTextModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooler_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-11lpom8"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function Ns(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function Gs(C){let e,m="Example:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBGbGF4Q0xJUFRleHRNb2RlbFdpdGhQcm9qZWN0aW9uJTBBJTBBbW9kZWwlMjAlM0QlMjBGbGF4Q0xJUFRleHRNb2RlbFdpdGhQcm9qZWN0aW9uLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIybnAlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQXRleHRfZW1iZWRzJTIwJTNEJTIwb3V0cHV0cy50ZXh0X2VtYmVkcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, FlaxCLIPTextModelWithProjection

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxCLIPTextModelWithProjection.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_embeds = outputs.text_embeds`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-11lpom8"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function qs(C){let e,m=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=d("p"),e.innerHTML=m},l(o){e=c(o,"P",{"data-svelte-h":!0}),$(e)!=="svelte-fincs2"&&(e.innerHTML=m)},m(o,s){p(o,e,s)},p:W,d(o){o&&a(e)}}}function Rs(C){let e,m="Example:",o,s,T;return s=new Ve({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEZsYXhDTElQVmlzaW9uTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEZsYXhDTElQVmlzaW9uTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm9wZW5haSUyRmNsaXAtdml0LWJhc2UtcGF0Y2gzMiUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIybnAlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQXBvb2xlcl9vdXRwdXQlMjAlM0QlMjBvdXRwdXRzLnBvb2xlcl9vdXRwdXQlMjAlMjAlMjMlMjBwb29sZWQlMjBDTFMlMjBzdGF0ZXM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, FlaxCLIPVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = FlaxCLIPVisionModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;np&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooler_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){e=d("p"),e.textContent=m,o=i(),u(s.$$.fragment)},l(t){e=c(t,"P",{"data-svelte-h":!0}),$(e)!=="svelte-11lpom8"&&(e.textContent=m),o=l(t),f(s.$$.fragment,t)},m(t,w){p(t,e,w),p(t,o,w),h(s,t,w),T=!0},p:W,i(t){T||(g(s.$$.fragment,t),T=!0)},o(t){_(s.$$.fragment,t),T=!1},d(t){t&&(a(e),a(o)),b(s,t)}}}function Es(C){let e,m,o,s,T,t,w=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel">FlaxPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading, saving and converting weights from PyTorch models)`,Se,N,Ye=`This model is also a
<a href="https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html" rel="nofollow">flax.linen.Module</a> subclass. Use it as
a regular Flax linen Module and refer to the Flax documentation for all matter related to general usage and
behavior.`,De,F,fe="Finally, this model supports inherent JAX features such as:",te,k,ze='<li><a href="https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit" rel="nofollow">Just-In-Time (JIT) compilation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation" rel="nofollow">Automatic Differentiation</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap" rel="nofollow">Vectorization</a></li> <li><a href="https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap" rel="nofollow">Parallelization</a></li>',Ae,q,at,ce,Ue,K="The <code>FlaxCLIPPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",Ie,oe,ve,Ce,ut,he,xe,Me,Ze,G,ge,R,Pe,je,rt,ye,qe,Q,Be,ne,ae,V,Oe,_e,be="The <code>FlaxCLIPTextPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",Wt,S,pe,it,Re,H,Ke,re,me,lt,ue,E,It,Y,ke="The <code>FlaxCLIPTextPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",Ee,U,Z,$e,Fe,He,vt,ee,Ne,et,D,I,j,A,X="The <code>FlaxCLIPVisionPreTrainedModel</code> forward method, overrides the <code>__call__</code> special method.",O,Le,ie,le,Te;return e=new we({props:{title:"FlaxCLIPModel",local:"transformers.FlaxCLIPModel",headingTag:"h2"}}),s=new z({props:{name:"class transformers.FlaxCLIPModel",anchor:"transformers.FlaxCLIPModel",parameters:[{name:"config",val:": CLIPConfig"},{name:"input_shape",val:": Optional = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.FlaxCLIPModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"},{anchor:"transformers.FlaxCLIPModel.dtype",description:`<strong>dtype</strong> (<code>jax.numpy.dtype</code>, <em>optional</em>, defaults to <code>jax.numpy.float32</code>) &#x2014;
The data type of the computation. Can be one of <code>jax.numpy.float32</code>, <code>jax.numpy.float16</code> (on GPUs) and
<code>jax.numpy.bfloat16</code> (on TPUs).</p>
<p>This can be used to enable mixed-precision training or half-precision inference on GPUs or TPUs. If
specified all the computation will be performed with the given <code>dtype</code>.</p>
<p><strong>Note that this only specifies the dtype of the computation and does not influence the dtype of model
parameters.</strong></p>
<p>If you wish to change the dtype of the model parameters, see <a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_fp16">to_fp16()</a> and
<a href="/docs/transformers/main/ja/main_classes/model#transformers.FlaxPreTrainedModel.to_bf16">to_bf16()</a>.`,name:"dtype"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py#L1262"}}),at=new z({props:{name:"__call__",anchor:"transformers.FlaxCLIPModel.__call__",parameters:[{name:"input_ids",val:""},{name:"pixel_values",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlaxCLIPModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxCLIPModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxCLIPModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlaxCLIPModel.__call__.pixel_values",description:`<strong>pixel_values</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlaxCLIPModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxCLIPModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxCLIPModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py#L820",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>logits_per_image:(<code>jnp.ndarray</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) — The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text:(<code>jnp.ndarray</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) — The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds(<code>jnp.ndarray</code></strong> of shape <code>(batch_size, output_dim</code>) — The text embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.FlaxCLIPTextModel"
>FlaxCLIPTextModel</a>.</li>
<li><strong>image_embeds(<code>jnp.ndarray</code></strong> of shape <code>(batch_size, output_dim</code>) — The image embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.FlaxCLIPVisionModel"
>FlaxCLIPVisionModel</a>.</li>
<li><strong>text_model_output(<code>FlaxBaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.FlaxCLIPTextModel"
>FlaxCLIPTextModel</a>.</li>
<li><strong>vision_model_output(<code>FlaxBaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.FlaxCLIPVisionModel"
>FlaxCLIPVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clip.modeling_flax_clip.FlaxCLIPOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oe=new gt({props:{$$slots:{default:[Us]},$$scope:{ctx:C}}}),Ce=new Ge({props:{anchor:"transformers.FlaxCLIPModel.__call__.example",$$slots:{default:[Fs]},$$scope:{ctx:C}}}),xe=new z({props:{name:"get_text_features",anchor:"transformers.FlaxCLIPModel.get_text_features",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:" = False"}],parametersDescription:[{anchor:"transformers.FlaxCLIPModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you
provide it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py#L865",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by applying
the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.FlaxCLIPTextModel"
>FlaxCLIPTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>jnp.ndarray</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),Ze=new Ge({props:{anchor:"transformers.FlaxCLIPModel.get_text_features.example",$$slots:{default:[Ws]},$$scope:{ctx:C}}}),R=new z({props:{name:"get_image_features",anchor:"transformers.FlaxCLIPModel.get_image_features",parameters:[{name:"pixel_values",val:""},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:" = False"}],parametersDescription:[{anchor:"transformers.FlaxCLIPModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained
using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py#L932",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.FlaxCLIPVisionModel"
>FlaxCLIPVisionModel</a></p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>jnp.ndarray</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),je=new Ge({props:{anchor:"transformers.FlaxCLIPModel.get_image_features.example",$$slots:{default:[Vs]},$$scope:{ctx:C}}}),ye=new we({props:{title:"FlaxCLIPTextModel",local:"transformers.FlaxCLIPTextModel",headingTag:"h2"}}),Be=new z({props:{name:"class transformers.FlaxCLIPTextModel",anchor:"transformers.FlaxCLIPTextModel",parameters:[{name:"config",val:": CLIPTextConfig"},{name:"input_shape",val:" = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py#L1012"}}),V=new z({props:{name:"__call__",anchor:"transformers.FlaxCLIPTextModel.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlaxCLIPTextModel.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxCLIPTextModel.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxCLIPTextModel.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlaxCLIPTextModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxCLIPTextModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxCLIPTextModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py#L665",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"
>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"
>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),S=new gt({props:{$$slots:{default:[Zs]},$$scope:{ctx:C}}}),it=new Ge({props:{anchor:"transformers.FlaxCLIPTextModel.__call__.example",$$slots:{default:[Bs]},$$scope:{ctx:C}}}),H=new we({props:{title:"FlaxCLIPTextModelWithProjection",local:"transformers.FlaxCLIPTextModelWithProjection",headingTag:"h2"}}),me=new z({props:{name:"class transformers.FlaxCLIPTextModelWithProjection",anchor:"transformers.FlaxCLIPTextModelWithProjection",parameters:[{name:"config",val:": CLIPTextConfig"},{name:"input_shape",val:" = (1, 1)"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py#L1083"}}),E=new z({props:{name:"__call__",anchor:"transformers.FlaxCLIPTextModelWithProjection.__call__",parameters:[{name:"input_ids",val:""},{name:"attention_mask",val:" = None"},{name:"position_ids",val:" = None"},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlaxCLIPTextModelWithProjection.__call__.input_ids",description:`<strong>input_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.FlaxCLIPTextModelWithProjection.__call__.attention_mask",description:`<strong>attention_mask</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.FlaxCLIPTextModelWithProjection.__call__.position_ids",description:`<strong>position_ids</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.FlaxCLIPTextModelWithProjection.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxCLIPTextModelWithProjection.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxCLIPTextModelWithProjection.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py#L665",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>text_embeds</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, output_dim</code>) — The text embeddings obtained by applying the projection layer to the pooled output of
<a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.FlaxCLIPTextModel"
>FlaxCLIPTextModel</a>.</p>
</li>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.clip.modeling_flax_clip.FlaxCLIPTextModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),U=new gt({props:{$$slots:{default:[Ns]},$$scope:{ctx:C}}}),$e=new Ge({props:{anchor:"transformers.FlaxCLIPTextModelWithProjection.__call__.example",$$slots:{default:[Gs]},$$scope:{ctx:C}}}),He=new we({props:{title:"FlaxCLIPVisionModel",local:"transformers.FlaxCLIPVisionModel",headingTag:"h2"}}),Ne=new z({props:{name:"class transformers.FlaxCLIPVisionModel",anchor:"transformers.FlaxCLIPVisionModel",parameters:[{name:"config",val:": CLIPVisionConfig"},{name:"input_shape",val:": Optional = None"},{name:"seed",val:": int = 0"},{name:"dtype",val:": dtype = <class 'jax.numpy.float32'>"},{name:"_do_init",val:": bool = True"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py#L1137"}}),I=new z({props:{name:"__call__",anchor:"transformers.FlaxCLIPVisionModel.__call__",parameters:[{name:"pixel_values",val:""},{name:"params",val:": dict = None"},{name:"dropout_rng",val:": PRNGKey = None"},{name:"train",val:": bool = False"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.FlaxCLIPVisionModel.__call__.pixel_values",description:`<strong>pixel_values</strong> (<code>numpy.ndarray</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.FlaxCLIPVisionModel.__call__.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.FlaxCLIPVisionModel.__call__.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.FlaxCLIPVisionModel.__call__.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/modeling_flax_clip.py#L745",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"
>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.clip.configuration_clip.CLIPVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>jnp.ndarray</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>jnp.ndarray</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(jnp.ndarray)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>jnp.ndarray</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling"
>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Le=new gt({props:{$$slots:{default:[qs]},$$scope:{ctx:C}}}),le=new Ge({props:{anchor:"transformers.FlaxCLIPVisionModel.__call__.example",$$slots:{default:[Rs]},$$scope:{ctx:C}}}),{c(){u(e.$$.fragment),m=i(),o=d("div"),u(s.$$.fragment),T=i(),t=d("p"),t.innerHTML=w,Se=i(),N=d("p"),N.innerHTML=Ye,De=i(),F=d("p"),F.textContent=fe,te=i(),k=d("ul"),k.innerHTML=ze,Ae=i(),q=d("div"),u(at.$$.fragment),ce=i(),Ue=d("p"),Ue.innerHTML=K,Ie=i(),u(oe.$$.fragment),ve=i(),u(Ce.$$.fragment),ut=i(),he=d("div"),u(xe.$$.fragment),Me=i(),u(Ze.$$.fragment),G=i(),ge=d("div"),u(R.$$.fragment),Pe=i(),u(je.$$.fragment),rt=i(),u(ye.$$.fragment),qe=i(),Q=d("div"),u(Be.$$.fragment),ne=i(),ae=d("div"),u(V.$$.fragment),Oe=i(),_e=d("p"),_e.innerHTML=be,Wt=i(),u(S.$$.fragment),pe=i(),u(it.$$.fragment),Re=i(),u(H.$$.fragment),Ke=i(),re=d("div"),u(me.$$.fragment),lt=i(),ue=d("div"),u(E.$$.fragment),It=i(),Y=d("p"),Y.innerHTML=ke,Ee=i(),u(U.$$.fragment),Z=i(),u($e.$$.fragment),Fe=i(),u(He.$$.fragment),vt=i(),ee=d("div"),u(Ne.$$.fragment),et=i(),D=d("div"),u(I.$$.fragment),j=i(),A=d("p"),A.innerHTML=X,O=i(),u(Le.$$.fragment),ie=i(),u(le.$$.fragment),this.h()},l(y){f(e.$$.fragment,y),m=l(y),o=c(y,"DIV",{class:!0});var x=L(o);f(s.$$.fragment,x),T=l(x),t=c(x,"P",{"data-svelte-h":!0}),$(t)!=="svelte-99cpmj"&&(t.innerHTML=w),Se=l(x),N=c(x,"P",{"data-svelte-h":!0}),$(N)!=="svelte-10nfsf3"&&(N.innerHTML=Ye),De=l(x),F=c(x,"P",{"data-svelte-h":!0}),$(F)!=="svelte-1pplc4a"&&(F.textContent=fe),te=l(x),k=c(x,"UL",{"data-svelte-h":!0}),$(k)!=="svelte-1w7z84m"&&(k.innerHTML=ze),Ae=l(x),q=c(x,"DIV",{class:!0});var de=L(q);f(at.$$.fragment,de),ce=l(de),Ue=c(de,"P",{"data-svelte-h":!0}),$(Ue)!=="svelte-qhvesh"&&(Ue.innerHTML=K),Ie=l(de),f(oe.$$.fragment,de),ve=l(de),f(Ce.$$.fragment,de),de.forEach(a),ut=l(x),he=c(x,"DIV",{class:!0});var We=L(he);f(xe.$$.fragment,We),Me=l(We),f(Ze.$$.fragment,We),We.forEach(a),G=l(x),ge=c(x,"DIV",{class:!0});var dt=L(ge);f(R.$$.fragment,dt),Pe=l(dt),f(je.$$.fragment,dt),dt.forEach(a),x.forEach(a),rt=l(y),f(ye.$$.fragment,y),qe=l(y),Q=c(y,"DIV",{class:!0});var _t=L(Q);f(Be.$$.fragment,_t),ne=l(_t),ae=c(_t,"DIV",{class:!0});var B=L(ae);f(V.$$.fragment,B),Oe=l(B),_e=c(B,"P",{"data-svelte-h":!0}),$(_e)!=="svelte-1v5d1sw"&&(_e.innerHTML=be),Wt=l(B),f(S.$$.fragment,B),pe=l(B),f(it.$$.fragment,B),B.forEach(a),_t.forEach(a),Re=l(y),f(H.$$.fragment,y),Ke=l(y),re=c(y,"DIV",{class:!0});var ct=L(re);f(me.$$.fragment,ct),lt=l(ct),ue=c(ct,"DIV",{class:!0});var tt=L(ue);f(E.$$.fragment,tt),It=l(tt),Y=c(tt,"P",{"data-svelte-h":!0}),$(Y)!=="svelte-1v5d1sw"&&(Y.innerHTML=ke),Ee=l(tt),f(U.$$.fragment,tt),Z=l(tt),f($e.$$.fragment,tt),tt.forEach(a),ct.forEach(a),Fe=l(y),f(He.$$.fragment,y),vt=l(y),ee=c(y,"DIV",{class:!0});var Je=L(ee);f(Ne.$$.fragment,Je),et=l(Je),D=c(Je,"DIV",{class:!0});var ot=L(D);f(I.$$.fragment,ot),j=l(ot),A=c(ot,"P",{"data-svelte-h":!0}),$(A)!=="svelte-tkfbrd"&&(A.innerHTML=X),O=l(ot),f(Le.$$.fragment,ot),ie=l(ot),f(le.$$.fragment,ot),ot.forEach(a),Je.forEach(a),this.h()},h(){J(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(he,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(ge,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(o,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(ae,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(y,x){h(e,y,x),p(y,m,x),p(y,o,x),h(s,o,null),r(o,T),r(o,t),r(o,Se),r(o,N),r(o,De),r(o,F),r(o,te),r(o,k),r(o,Ae),r(o,q),h(at,q,null),r(q,ce),r(q,Ue),r(q,Ie),h(oe,q,null),r(q,ve),h(Ce,q,null),r(o,ut),r(o,he),h(xe,he,null),r(he,Me),h(Ze,he,null),r(o,G),r(o,ge),h(R,ge,null),r(ge,Pe),h(je,ge,null),p(y,rt,x),h(ye,y,x),p(y,qe,x),p(y,Q,x),h(Be,Q,null),r(Q,ne),r(Q,ae),h(V,ae,null),r(ae,Oe),r(ae,_e),r(ae,Wt),h(S,ae,null),r(ae,pe),h(it,ae,null),p(y,Re,x),h(H,y,x),p(y,Ke,x),p(y,re,x),h(me,re,null),r(re,lt),r(re,ue),h(E,ue,null),r(ue,It),r(ue,Y),r(ue,Ee),h(U,ue,null),r(ue,Z),h($e,ue,null),p(y,Fe,x),h(He,y,x),p(y,vt,x),p(y,ee,x),h(Ne,ee,null),r(ee,et),r(ee,D),h(I,D,null),r(D,j),r(D,A),r(D,O),h(Le,D,null),r(D,ie),h(le,D,null),Te=!0},p(y,x){const de={};x&2&&(de.$$scope={dirty:x,ctx:y}),oe.$set(de);const We={};x&2&&(We.$$scope={dirty:x,ctx:y}),Ce.$set(We);const dt={};x&2&&(dt.$$scope={dirty:x,ctx:y}),Ze.$set(dt);const _t={};x&2&&(_t.$$scope={dirty:x,ctx:y}),je.$set(_t);const B={};x&2&&(B.$$scope={dirty:x,ctx:y}),S.$set(B);const ct={};x&2&&(ct.$$scope={dirty:x,ctx:y}),it.$set(ct);const tt={};x&2&&(tt.$$scope={dirty:x,ctx:y}),U.$set(tt);const Je={};x&2&&(Je.$$scope={dirty:x,ctx:y}),$e.$set(Je);const ot={};x&2&&(ot.$$scope={dirty:x,ctx:y}),Le.$set(ot);const kt={};x&2&&(kt.$$scope={dirty:x,ctx:y}),le.$set(kt)},i(y){Te||(g(e.$$.fragment,y),g(s.$$.fragment,y),g(at.$$.fragment,y),g(oe.$$.fragment,y),g(Ce.$$.fragment,y),g(xe.$$.fragment,y),g(Ze.$$.fragment,y),g(R.$$.fragment,y),g(je.$$.fragment,y),g(ye.$$.fragment,y),g(Be.$$.fragment,y),g(V.$$.fragment,y),g(S.$$.fragment,y),g(it.$$.fragment,y),g(H.$$.fragment,y),g(me.$$.fragment,y),g(E.$$.fragment,y),g(U.$$.fragment,y),g($e.$$.fragment,y),g(He.$$.fragment,y),g(Ne.$$.fragment,y),g(I.$$.fragment,y),g(Le.$$.fragment,y),g(le.$$.fragment,y),Te=!0)},o(y){_(e.$$.fragment,y),_(s.$$.fragment,y),_(at.$$.fragment,y),_(oe.$$.fragment,y),_(Ce.$$.fragment,y),_(xe.$$.fragment,y),_(Ze.$$.fragment,y),_(R.$$.fragment,y),_(je.$$.fragment,y),_(ye.$$.fragment,y),_(Be.$$.fragment,y),_(V.$$.fragment,y),_(S.$$.fragment,y),_(it.$$.fragment,y),_(H.$$.fragment,y),_(me.$$.fragment,y),_(E.$$.fragment,y),_(U.$$.fragment,y),_($e.$$.fragment,y),_(He.$$.fragment,y),_(Ne.$$.fragment,y),_(I.$$.fragment,y),_(Le.$$.fragment,y),_(le.$$.fragment,y),Te=!1},d(y){y&&(a(m),a(o),a(rt),a(qe),a(Q),a(Re),a(Ke),a(re),a(Fe),a(vt),a(ee)),b(e,y),b(s),b(at),b(oe),b(Ce),b(xe),b(Ze),b(R),b(je),b(ye,y),b(Be),b(V),b(S),b(it),b(H,y),b(me),b(E),b(U),b($e),b(He,y),b(Ne),b(I),b(Le),b(le)}}}function Hs(C){let e,m;return e=new Un({props:{$$slots:{default:[Es]},$$scope:{ctx:C}}}),{c(){u(e.$$.fragment)},l(o){f(e.$$.fragment,o)},m(o,s){h(e,o,s),m=!0},p(o,s){const T={};s&2&&(T.$$scope={dirty:s,ctx:o}),e.$set(T)},i(o){m||(g(e.$$.fragment,o),m=!0)},o(o){_(e.$$.fragment,o),m=!1},d(o){b(e,o)}}}function Xs(C){let e,m,o,s,T,t,w,Se,N,Ye=`CLIP モデルは、Alec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel Goh Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever <a href="https://arxiv.org/abs/2103.00020" rel="nofollow">Learning Transferable Visual Models From Natural Language Supervision</a> で提案されました。
サンディニ・アガルワル、ギリッシュ・サストリー、アマンダ・アスケル、パメラ・ミシュキン、ジャック・クラーク、グレッチェン・クルーガー、イリヤ・サツケヴァー。クリップ
(Contrastive Language-Image Pre-Training) は、さまざまな (画像、テキスト) ペアでトレーニングされたニューラル ネットワークです。かもね
直接最適化することなく、与えられた画像から最も関連性の高いテキスト スニペットを予測するように自然言語で指示されます。
GPT-2 および 3 のゼロショット機能と同様に、タスクに対して。`,De,F,fe="論文の要約は次のとおりです。",te,k,ze=`<em>最先端のコンピューター ビジョン システムは、あらかじめ定められたオブジェクト カテゴリの固定セットを予測するようにトレーニングされています。これ
制限された形式の監視では、指定するために追加のラベル付きデータが必要となるため、一般性と使いやすさが制限されます。
その他の視覚的なコンセプト。画像に関する生のテキストから直接学習することは、
より広範な監督源。どのキャプションが表示されるかを予測するという単純な事前トレーニング タスクが有効であることを示します。
400 のデータセットで SOTA 画像表現を最初から学習するための効率的かつスケーラブルな方法はどの画像ですか
インターネットから収集された数百万の（画像、テキスト）ペア。事前トレーニング後、自然言語を使用して参照します。
視覚的な概念を学習し（または新しい概念を説明し）、下流のタスクへのモデルのゼロショット転送を可能にします。私たちは勉強します
30 を超えるさまざまな既存のコンピューター ビジョン データセットでタスクをまたがってベンチマークを行うことにより、このアプローチのパフォーマンスを評価します。
OCR、ビデオ内のアクション認識、地理的位置特定、およびさまざまな種類のきめ細かいオブジェクト分類など。の
モデルはほとんどのタスクに簡単に移行でき、多くの場合、必要がなくても完全に監視されたベースラインと競合します。
データセット固有のトレーニングに適しています。たとえば、ImageNet ゼロショットではオリジナルの ResNet-50 の精度と一致します。
トレーニングに使用された 128 万のトレーニング サンプルを使用する必要はありません。コードをリリースし、事前トレーニング済み
モデルの重みはこの https URL で確認できます。</em>`,Ae,q,at='このモデルは <a href="https://huggingface.co/valhalla" rel="nofollow">valhalla</a> によって提供されました。元のコードは <a href="https://github.com/openai/CLIP" rel="nofollow">ここ</a> にあります。',ce,Ue,K,Ie,oe=`CLIP は、マルチモーダルなビジョンおよび言語モデルです。画像とテキストの類似性やゼロショット画像に使用できます。
分類。 CLIP は、ViT のようなトランスフォーマーを使用して視覚的特徴を取得し、因果言語モデルを使用してテキストを取得します
特徴。次に、テキストと視覚の両方の特徴が、同じ次元の潜在空間に投影されます。ドット
投影された画像とテキストの特徴間の積が同様のスコアとして使用されます。`,ve,Ce,ut=`画像を Transformer エンコーダに供給するために、各画像は固定サイズの重複しないパッチのシーケンスに分割されます。
これらは線形に埋め込まれます。 [CLS] トークンは、イメージ全体の表現として機能するために追加されます。作家たち
また、絶対位置埋め込みを追加し、結果として得られるベクトルのシーケンスを標準の Transformer エンコーダに供給します。
<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> を使用して、モデルの画像のサイズ変更 (または再スケール) および正規化を行うことができます。`,he,xe,Me=`<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> はテキストのエンコードに使用されます。 <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> はラップします
<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> と <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizer">CLIPTokenizer</a> を両方の単一インスタンスに統合
テキストをエンコードして画像を準備します。次の例は、次のメソッドを使用して画像とテキストの類似性スコアを取得する方法を示しています。
<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> と <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a>。`,Ze,G,ge,R,Pe,je,rt="CLIP を使い始めるのに役立つ公式 Hugging Face およびコミュニティ (🌎 で示されている) リソースのリスト。",ye,qe,Q='<li><a href="https://huggingface.co/blog/fine-tune-clip-rsicd" rel="nofollow">リモート センシング (衛星) 画像とキャプションを使用した CLIP の微調整</a>、[RSICD データセット] を使用して CLIP を微調整する方法に関するブログ投稿(<a href="https://github.com/201528014227051/RSICD_optimal" rel="nofollow">https://github.com/201528014227051/RSICD_optimal</a>) と、データ拡張によるパフォーマンスの変化の比較。</li> <li>この <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/contrastive-image-text" rel="nofollow">サンプル スクリプト</a> は、プレ- <a href="https://cocodataset.org/#home" rel="nofollow">COCO データセット</a> を使用してトレーニングされたビジョンおよびテキスト エンコーダー。</li>',Be,ne,ae,V,Oe='<li>画像キャプションのビーム検索による推論に事前トレーニング済み CLIP を使用する方法に関する <a href="https://colab.research.google.com/drive/1tuoAC5F4sC7qid56Z0ap-stR3rwdk0ZV?usp=sharing" rel="nofollow">ノートブック</a>。 🌎</li>',_e,be,Wt="<strong>画像検索</strong>",S,pe,it='<li>事前トレーニングされた CLIP を使用した画像検索と MRR (平均相互ランク) スコアの計算に関する <a href="https://colab.research.google.com/drive/1bLVwVKpAndpEDHqjzxVPr_9nGrSbuOQd?usp=sharing" rel="nofollow">ノートブック</a>。 🌎</li> <li>画像の取得と類似性スコアの表示に関する <a href="https://colab.research.google.com/github/deep-diver/image_search_with_natural_language/blob/main/notebooks/Image_Search_CLIP.ipynb" rel="nofollow">ノートブック</a>。 🌎</li> <li>多言語 CLIP を使用して画像とテキストを同じベクトル空間にマッピングする方法に関する <a href="https://colab.research.google.com/drive/1xO-wC_m_GNzgjIBQ4a4znvQkvDoZJvH4?usp=sharing" rel="nofollow">ノートブック</a>。 🌎</li> <li>を使用してセマンティック イメージ検索で CLIP を実行する方法に関する <a href="https://colab.research.google.com/github/vivien000/clip-demo/blob/master/clip.ipynb#scrollTo=uzdFhRGqiWkR" rel="nofollow">ノートブック</a> <a href="https://unsplash.com" rel="nofollow">Unsplash</a> および <a href="https://www.themoviedb.org/" rel="nofollow">TMDB</a> データセット。 🌎</li>',Re,H,Ke="<strong>説明可能性</strong>",re,me,lt='<li>入力トークンと画像セグメントの類似性を視覚化する方法に関する <a href="https://colab.research.google.com/github/hila-chefer/Transformer-MM-Explainability/blob/main/CLIP_explainability.ipynb" rel="nofollow">ノートブック</a>。 🌎</li>',ue,E,It=`ここに含めるリソースの送信に興味がある場合は、お気軽にプル リクエストを開いてください。審査させていただきます。
リソースは、既存のリソースを複製するのではなく、何か新しいものを示すことが理想的です。`,Y,ke,Ee,U,Z,$e,Fe,He=`<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a>. It is used to instantiate
a CLIP model according to the specified arguments, defining the text model and vision model configs. Instantiating
a configuration with the defaults will yield a similar configuration to that of the CLIP
<a href="https://huggingface.co/openai/clip-vit-base-patch32" rel="nofollow">openai/clip-vit-base-patch32</a> architecture.`,vt,ee,Ne=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,et,D,I,j,A,X,O,Le=`Instantiate a <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig">CLIPConfig</a> (or a derived class) from clip text model configuration and clip vision model
configuration.`,ie,le,Te,y,x,de,We,dt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTextModel">CLIPTextModel</a>. It is used to instantiate a CLIP
text encoder according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the text encoder of the CLIP
<a href="https://huggingface.co/openai/clip-vit-base-patch32" rel="nofollow">openai/clip-vit-base-patch32</a> architecture.`,_t,B,ct=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,tt,Je,ot,kt,Gt,bt,qt,Ot,io,Wo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionModel">CLIPVisionModel</a>. It is used to instantiate a
CLIP vision encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the vision encoder of the CLIP
<a href="https://huggingface.co/openai/clip-vit-base-patch32" rel="nofollow">openai/clip-vit-base-patch32</a> architecture.`,Tt,Lt,Vo=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Kt,Rt,po,Mt,mo,se,Zt,Et,Ht,yt="Construct a CLIP tokenizer. Based on byte-level Byte-Pair-Encoding.",Xt,eo,lo=`This tokenizer inherits from <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a> which contains most of the main methods. Users should refer to
this superclass for more information regarding those methods.`,Zo,$t,Jt,Bo,to,Ct=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A CLIP sequence has the following format:`,Qt,oo,co="<li>single sequence: <code>&lt;|startoftext|&gt; X &lt;|endoftext|&gt;</code></li>",No,no,St="Pairs of sequences are not the expected use case, but they will be handled without a separator.",Mo,ft,Bt,M,P,zt=`Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer <code>prepare_for_model</code> method.`,xt,Xe,nt,Pt,Qe,Ut=`Create a mask from the two sequences passed. CLIP does not make use of token type ids, therefore a list of
zeros is returned.`,ht,pt,st,wt,so,yo,mt,$o,_n,Go,Fn=`Construct a “fast” CLIP tokenizer (backed by HuggingFace’s <em>tokenizers</em> library). Based on byte-level
Byte-Pair-Encoding.`,bn,qo,Wn=`This tokenizer inherits from <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a> which contains most of the main methods. Users should
refer to this superclass for more information regarding those methods.`,Tn,Nt,wo,Mn,Ro,Vn=`Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A CLIP sequence has the following format:`,yn,Eo,Zn="<li>single sequence: <code>&lt;|startoftext|&gt; X &lt;|endoftext|&gt;</code></li>",$n,Ho,Bn="Pairs of sequences are not the expected use case, but they will be handled without a separator.",wn,uo,Io,In,Xo,Nn=`Create a mask from the two sequences passed. CLIP does not make use of token type ids, therefore a list of
zeros is returned.`,tn,vo,on,Yt,Co,vn,Qo,Gn="Constructs a CLIP image processor.",Cn,fo,xo,xn,So,qn="Preprocess an image or batch of images.",nn,Po,sn,jo,ko,an,Lo,rn,jt,Jo,Pn,Yo,Rn="Constructs a CLIP processor which wraps a CLIP image processor and a CLIP tokenizer into a single processor.",jn,Do,En=`<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPProcessor">CLIPProcessor</a> offers all the functionalities of <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> and <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>. See the
<code>__call__()</code> and <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPProcessor.decode">decode()</a> for more information.`,kn,ho,zo,Ln,Ao,Hn=`This method forwards all its arguments to CLIPTokenizerFast’s <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.batch_decode">batch_decode()</a>. Please
refer to the docstring of this method for more information.`,Jn,go,Uo,zn,Oo,Xn=`This method forwards all its arguments to CLIPTokenizerFast’s <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,ln,_o,dn,en,cn;return T=new we({props:{title:"CLIP",local:"clip",headingTag:"h1"}}),w=new we({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Ue=new we({props:{title:"Usage tips and example",local:"usage-tips-and-example",headingTag:"h2"}}),G=new Ve({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQ0xJUFByb2Nlc3NvciUyQyUyMENMSVBNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQ0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJvcGVuYWklMkZjbGlwLXZpdC1iYXNlLXBhdGNoMzIlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQ0xJUFByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyb3BlbmFpJTJGY2xpcC12aXQtYmFzZS1wYXRjaDMyJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IodGV4dCUzRCU1QiUyMmElMjBwaG90byUyMG9mJTIwYSUyMGNhdCUyMiUyQyUyMCUyMmElMjBwaG90byUyMG9mJTIwYSUyMGRvZyUyMiU1RCUyQyUyMGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiUyQyUyMHBhZGRpbmclM0RUcnVlKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsb2dpdHNfcGVyX2ltYWdlJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHNfcGVyX2ltYWdlJTIwJTIwJTIzJTIwdGhpcyUyMGlzJTIwdGhlJTIwaW1hZ2UtdGV4dCUyMHNpbWlsYXJpdHklMjBzY29yZSUwQXByb2JzJTIwJTNEJTIwbG9naXRzX3Blcl9pbWFnZS5zb2Z0bWF4KGRpbSUzRDEpJTIwJTIwJTIzJTIwd2UlMjBjYW4lMjB0YWtlJTIwdGhlJTIwc29mdG1heCUyMHRvJTIwZ2V0JTIwdGhlJTIwbGFiZWwlMjBwcm9iYWJpbGl0aWVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPProcessor, CLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = CLIPModel.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = CLIPProcessor.from_pretrained(<span class="hljs-string">&quot;openai/clip-vit-base-patch32&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),R=new we({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ne=new ts({props:{pipeline:"image-to-text"}}),ke=new we({props:{title:"CLIPConfig",local:"transformers.CLIPConfig",headingTag:"h2"}}),Z=new z({props:{name:"class transformers.CLIPConfig",anchor:"transformers.CLIPConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 512"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTextConfig">CLIPTextConfig</a>.`,name:"text_config"},{anchor:"transformers.CLIPConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPVisionConfig">CLIPVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.CLIPConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.CLIPConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original CLIP implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.CLIPConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/configuration_clip.py#L266"}}),D=new Ge({props:{anchor:"transformers.CLIPConfig.example",$$slots:{default:[os]},$$scope:{ctx:C}}}),A=new z({props:{name:"from_text_vision_configs",anchor:"transformers.CLIPConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": CLIPTextConfig"},{name:"vision_config",val:": CLIPVisionConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/configuration_clip.py#L402",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPConfig"
>CLIPConfig</a></p>
`}}),le=new we({props:{title:"CLIPTextConfig",local:"transformers.CLIPTextConfig",headingTag:"h2"}}),x=new z({props:{name:"class transformers.CLIPTextConfig",anchor:"transformers.CLIPTextConfig",parameters:[{name:"vocab_size",val:" = 49408"},{name:"hidden_size",val:" = 512"},{name:"intermediate_size",val:" = 2048"},{name:"projection_dim",val:" = 512"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 8"},{name:"max_position_embeddings",val:" = 77"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 49406"},{name:"eos_token_id",val:" = 49407"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 49408) &#x2014;
Vocabulary size of the CLIP text model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPModel">CLIPModel</a>.`,name:"vocab_size"},{anchor:"transformers.CLIPTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2048) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPTextConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.CLIPTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 77) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.CLIPTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> <code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.CLIPTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.CLIPTextConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.CLIPTextConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.CLIPTextConfig.bos_token_id",description:`<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49406) &#x2014;
Beginning of stream token id.`,name:"bos_token_id"},{anchor:"transformers.CLIPTextConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 49407) &#x2014;
End of stream token id.`,name:"eos_token_id"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/configuration_clip.py#L39"}}),Je=new Ge({props:{anchor:"transformers.CLIPTextConfig.example",$$slots:{default:[ns]},$$scope:{ctx:C}}}),kt=new we({props:{title:"CLIPVisionConfig",local:"transformers.CLIPVisionConfig",headingTag:"h2"}}),qt=new z({props:{name:"class transformers.CLIPVisionConfig",anchor:"transformers.CLIPVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"projection_dim",val:" = 512"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.CLIPVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.CLIPVisionConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.CLIPVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.CLIPVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.CLIPVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.CLIPVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.CLIPVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.CLIPVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.CLIPVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.CLIPVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.CLIPVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CLIPVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/configuration_clip.py#L157"}}),Rt=new Ge({props:{anchor:"transformers.CLIPVisionConfig.example",$$slots:{default:[ss]},$$scope:{ctx:C}}}),Mt=new we({props:{title:"CLIPTokenizer",local:"transformers.CLIPTokenizer",headingTag:"h2"}}),Zt=new z({props:{name:"class transformers.CLIPTokenizer",anchor:"transformers.CLIPTokenizer",parameters:[{name:"vocab_file",val:""},{name:"merges_file",val:""},{name:"errors",val:" = 'replace'"},{name:"unk_token",val:" = '<|endoftext|>'"},{name:"bos_token",val:" = '<|startoftext|>'"},{name:"eos_token",val:" = '<|endoftext|>'"},{name:"pad_token",val:" = '<|endoftext|>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPTokenizer.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.CLIPTokenizer.merges_file",description:`<strong>merges_file</strong> (<code>str</code>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.CLIPTokenizer.errors",description:`<strong>errors</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;replace&quot;</code>) &#x2014;
Paradigm to follow when decoding bytes to UTF-8. See
<a href="https://docs.python.org/3/library/stdtypes.html#bytes.decode" rel="nofollow">bytes.decode</a> for more information.`,name:"errors"},{anchor:"transformers.CLIPTokenizer.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.CLIPTokenizer.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|startoftext|&gt;&quot;</code>) &#x2014;
The beginning of sequence token.`,name:"bos_token"},{anchor:"transformers.CLIPTokenizer.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.CLIPTokenizer.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/tokenization_clip.py#L272"}}),Jt=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CLIPTokenizer.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPTokenizer.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.CLIPTokenizer.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/tokenization_clip.py#L359",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),Bt=new z({props:{name:"get_special_tokens_mask",anchor:"transformers.CLIPTokenizer.get_special_tokens_mask",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"},{name:"already_has_special_tokens",val:": bool = False"}],parametersDescription:[{anchor:"transformers.CLIPTokenizer.get_special_tokens_mask.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CLIPTokenizer.get_special_tokens_mask.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"},{anchor:"transformers.CLIPTokenizer.get_special_tokens_mask.already_has_special_tokens",description:`<strong>already_has_special_tokens</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not the token list is already formatted with special tokens for the model.`,name:"already_has_special_tokens"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/tokenization_clip.py#L386",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),nt=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.CLIPTokenizer.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPTokenizer.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CLIPTokenizer.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/tokenization_clip.py#L414",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),st=new z({props:{name:"save_vocabulary",anchor:"transformers.CLIPTokenizer.save_vocabulary",parameters:[{name:"save_directory",val:": str"},{name:"filename_prefix",val:": Optional = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/tokenization_clip.py#L509"}}),so=new we({props:{title:"CLIPTokenizerFast",local:"transformers.CLIPTokenizerFast",headingTag:"h2"}}),$o=new z({props:{name:"class transformers.CLIPTokenizerFast",anchor:"transformers.CLIPTokenizerFast",parameters:[{name:"vocab_file",val:" = None"},{name:"merges_file",val:" = None"},{name:"tokenizer_file",val:" = None"},{name:"unk_token",val:" = '<|endoftext|>'"},{name:"bos_token",val:" = '<|startoftext|>'"},{name:"eos_token",val:" = '<|endoftext|>'"},{name:"pad_token",val:" = '<|endoftext|>'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPTokenizerFast.vocab_file",description:`<strong>vocab_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to the vocabulary file.`,name:"vocab_file"},{anchor:"transformers.CLIPTokenizerFast.merges_file",description:`<strong>merges_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
Path to the merges file.`,name:"merges_file"},{anchor:"transformers.CLIPTokenizerFast.tokenizer_file",description:`<strong>tokenizer_file</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The path to a tokenizer file to use instead of the vocab file.`,name:"tokenizer_file"},{anchor:"transformers.CLIPTokenizerFast.unk_token",description:`<strong>unk_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
token instead.`,name:"unk_token"},{anchor:"transformers.CLIPTokenizerFast.bos_token",description:`<strong>bos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|startoftext|&gt;&quot;</code>) &#x2014;
The beginning of sequence token.`,name:"bos_token"},{anchor:"transformers.CLIPTokenizerFast.eos_token",description:`<strong>eos_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The end of sequence token.`,name:"eos_token"},{anchor:"transformers.CLIPTokenizerFast.pad_token",description:`<strong>pad_token</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;&lt;|endoftext|&gt;&quot;</code>) &#x2014;
The token used for padding, for example when batching sequences of different lengths.`,name:"pad_token"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/tokenization_clip_fast.py#L50"}}),wo=new z({props:{name:"build_inputs_with_special_tokens",anchor:"transformers.CLIPTokenizerFast.build_inputs_with_special_tokens",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPTokenizerFast.build_inputs_with_special_tokens.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs to which the special tokens will be added.`,name:"token_ids_0"},{anchor:"transformers.CLIPTokenizerFast.build_inputs_with_special_tokens.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/tokenization_clip_fast.py#L127",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of <a href="../glossary#input-ids">input IDs</a> with the appropriate special tokens.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),Io=new z({props:{name:"create_token_type_ids_from_sequences",anchor:"transformers.CLIPTokenizerFast.create_token_type_ids_from_sequences",parameters:[{name:"token_ids_0",val:": List"},{name:"token_ids_1",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CLIPTokenizerFast.create_token_type_ids_from_sequences.token_ids_0",description:`<strong>token_ids_0</strong> (<code>List[int]</code>) &#x2014;
List of IDs.`,name:"token_ids_0"},{anchor:"transformers.CLIPTokenizerFast.create_token_type_ids_from_sequences.token_ids_1",description:`<strong>token_ids_1</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
Optional second list of IDs for sequence pairs.`,name:"token_ids_1"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/tokenization_clip_fast.py#L154",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>List of zeros.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>List[int]</code></p>
`}}),vo=new we({props:{title:"CLIPImageProcessor",local:"transformers.CLIPImageProcessor",headingTag:"h2"}}),Co=new z({props:{name:"class transformers.CLIPImageProcessor",anchor:"transformers.CLIPImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = <Resampling.BICUBIC: 3>"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": Dict = None"},{name:"do_rescale",val:": bool = True"},{name:"rescale_factor",val:": Union = 0.00392156862745098"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"do_convert_rgb",val:": bool = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by
<code>do_resize</code> in the <code>preprocess</code> method.`,name:"do_resize"},{anchor:"transformers.CLIPImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;shortest_edge&quot; -- 224}</code>):
Size of the image after resizing. The shortest edge of the image is resized to size[&#x201C;shortest_edge&#x201D;], with
the longest edge resized to keep the input aspect ratio. Can be overridden by <code>size</code> in the <code>preprocess</code>
method.`,name:"size"},{anchor:"transformers.CLIPImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>Resampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by <code>resample</code> in the <code>preprocess</code> method.`,name:"resample"},{anchor:"transformers.CLIPImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the image to the specified <code>crop_size</code>. Can be overridden by <code>do_center_crop</code> in the
<code>preprocess</code> method.`,name:"do_center_crop"},{anchor:"transformers.CLIPImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to 224) &#x2014;
Size of the output image after applying <code>center_crop</code>. Can be overridden by <code>crop_size</code> in the <code>preprocess</code>
method.`,name:"crop_size"},{anchor:"transformers.CLIPImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by <code>do_rescale</code> in
the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.CLIPImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by <code>rescale_factor</code> in the <code>preprocess</code>
method.`,name:"rescale_factor"},{anchor:"transformers.CLIPImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by <code>do_normalize</code> in the <code>preprocess</code> method.`,name:"do_normalize"},{anchor:"transformers.CLIPImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>[0.48145466, 0.4578275, 0.40821073]</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.CLIPImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>[0.26862954, 0.26130258, 0.27577711]</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.
Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"},{anchor:"transformers.CLIPImageProcessor.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/image_processing_clip.py#L50"}}),xo=new z({props:{name:"preprocess",anchor:"transformers.CLIPImageProcessor.preprocess",parameters:[{name:"images",val:": Union"},{name:"do_resize",val:": bool = None"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = None"},{name:"do_center_crop",val:": bool = None"},{name:"crop_size",val:": int = None"},{name:"do_rescale",val:": bool = None"},{name:"rescale_factor",val:": float = None"},{name:"do_normalize",val:": bool = None"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"do_convert_rgb",val:": bool = None"},{name:"return_tensors",val:": Union = None"},{name:"data_format",val:": Optional = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.CLIPImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.CLIPImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image after resizing. Shortest edge of the image is resized to size[&#x201C;shortest_edge&#x201D;], with
the longest edge resized to keep the input aspect ratio.`,name:"size"},{anchor:"transformers.CLIPImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>int</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
Resampling filter to use if resizing the image. This can be one of the enum <code>PILImageResampling</code>. Only
has an effect if <code>do_resize</code> is set to <code>True</code>.`,name:"resample"},{anchor:"transformers.CLIPImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.CLIPImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the center crop. Only has an effect if <code>do_center_crop</code> is set to <code>True</code>.`,name:"crop_size"},{anchor:"transformers.CLIPImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image.`,name:"do_rescale"},{anchor:"transformers.CLIPImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.CLIPImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.CLIPImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean to use for normalization. Only has an effect if <code>do_normalize</code> is set to <code>True</code>.`,name:"image_mean"},{anchor:"transformers.CLIPImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation to use for normalization. Only has an effect if <code>do_normalize</code> is set to
<code>True</code>.`,name:"image_std"},{anchor:"transformers.CLIPImageProcessor.preprocess.do_convert_rgb",description:`<strong>do_convert_rgb</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_convert_rgb</code>) &#x2014;
Whether to convert the image to RGB.`,name:"do_convert_rgb"},{anchor:"transformers.CLIPImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li>Unset: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.CLIPImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li>Unset: Use the channel dimension format of the input image.</li>
</ul>`,name:"data_format"},{anchor:"transformers.CLIPImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/image_processing_clip.py#L177"}}),Po=new we({props:{title:"CLIPFeatureExtractor",local:"transformers.CLIPFeatureExtractor",headingTag:"h2"}}),ko=new z({props:{name:"class transformers.CLIPFeatureExtractor",anchor:"transformers.CLIPFeatureExtractor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/feature_extraction_clip.py#L26"}}),Lo=new we({props:{title:"CLIPProcessor",local:"transformers.CLIPProcessor",headingTag:"h2"}}),Jo=new z({props:{name:"class transformers.CLIPProcessor",anchor:"transformers.CLIPProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CLIPProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>, <em>optional</em>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.CLIPProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPTokenizerFast">CLIPTokenizerFast</a>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/processing_clip.py#L25"}}),zo=new z({props:{name:"batch_decode",anchor:"transformers.CLIPProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/processing_clip.py#L114"}}),Uo=new z({props:{name:"decode",anchor:"transformers.CLIPProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/clip/processing_clip.py#L121"}}),_o=new es({props:{pytorch:!0,tensorflow:!0,jax:!0,$$slots:{jax:[Hs],tensorflow:[zs],pytorch:[Ms]},$$scope:{ctx:C}}}),{c(){e=d("meta"),m=i(),o=d("p"),s=i(),u(T.$$.fragment),t=i(),u(w.$$.fragment),Se=i(),N=d("p"),N.innerHTML=Ye,De=i(),F=d("p"),F.textContent=fe,te=i(),k=d("p"),k.innerHTML=ze,Ae=i(),q=d("p"),q.innerHTML=at,ce=i(),u(Ue.$$.fragment),K=i(),Ie=d("p"),Ie.textContent=oe,ve=i(),Ce=d("p"),Ce.innerHTML=ut,he=i(),xe=d("p"),xe.innerHTML=Me,Ze=i(),u(G.$$.fragment),ge=i(),u(R.$$.fragment),Pe=i(),je=d("p"),je.textContent=rt,ye=i(),qe=d("ul"),qe.innerHTML=Q,Be=i(),u(ne.$$.fragment),ae=i(),V=d("ul"),V.innerHTML=Oe,_e=i(),be=d("p"),be.innerHTML=Wt,S=i(),pe=d("ul"),pe.innerHTML=it,Re=i(),H=d("p"),H.innerHTML=Ke,re=i(),me=d("ul"),me.innerHTML=lt,ue=i(),E=d("p"),E.textContent=It,Y=i(),u(ke.$$.fragment),Ee=i(),U=d("div"),u(Z.$$.fragment),$e=i(),Fe=d("p"),Fe.innerHTML=He,vt=i(),ee=d("p"),ee.innerHTML=Ne,et=i(),u(D.$$.fragment),I=i(),j=d("div"),u(A.$$.fragment),X=i(),O=d("p"),O.innerHTML=Le,ie=i(),u(le.$$.fragment),Te=i(),y=d("div"),u(x.$$.fragment),de=i(),We=d("p"),We.innerHTML=dt,_t=i(),B=d("p"),B.innerHTML=ct,tt=i(),u(Je.$$.fragment),ot=i(),u(kt.$$.fragment),Gt=i(),bt=d("div"),u(qt.$$.fragment),Ot=i(),io=d("p"),io.innerHTML=Wo,Tt=i(),Lt=d("p"),Lt.innerHTML=Vo,Kt=i(),u(Rt.$$.fragment),po=i(),u(Mt.$$.fragment),mo=i(),se=d("div"),u(Zt.$$.fragment),Et=i(),Ht=d("p"),Ht.textContent=yt,Xt=i(),eo=d("p"),eo.innerHTML=lo,Zo=i(),$t=d("div"),u(Jt.$$.fragment),Bo=i(),to=d("p"),to.textContent=Ct,Qt=i(),oo=d("ul"),oo.innerHTML=co,No=i(),no=d("p"),no.textContent=St,Mo=i(),ft=d("div"),u(Bt.$$.fragment),M=i(),P=d("p"),P.innerHTML=zt,xt=i(),Xe=d("div"),u(nt.$$.fragment),Pt=i(),Qe=d("p"),Qe.textContent=Ut,ht=i(),pt=d("div"),u(st.$$.fragment),wt=i(),u(so.$$.fragment),yo=i(),mt=d("div"),u($o.$$.fragment),_n=i(),Go=d("p"),Go.innerHTML=Fn,bn=i(),qo=d("p"),qo.innerHTML=Wn,Tn=i(),Nt=d("div"),u(wo.$$.fragment),Mn=i(),Ro=d("p"),Ro.textContent=Vn,yn=i(),Eo=d("ul"),Eo.innerHTML=Zn,$n=i(),Ho=d("p"),Ho.textContent=Bn,wn=i(),uo=d("div"),u(Io.$$.fragment),In=i(),Xo=d("p"),Xo.textContent=Nn,tn=i(),u(vo.$$.fragment),on=i(),Yt=d("div"),u(Co.$$.fragment),vn=i(),Qo=d("p"),Qo.textContent=Gn,Cn=i(),fo=d("div"),u(xo.$$.fragment),xn=i(),So=d("p"),So.textContent=qn,nn=i(),u(Po.$$.fragment),sn=i(),jo=d("div"),u(ko.$$.fragment),an=i(),u(Lo.$$.fragment),rn=i(),jt=d("div"),u(Jo.$$.fragment),Pn=i(),Yo=d("p"),Yo.textContent=Rn,jn=i(),Do=d("p"),Do.innerHTML=En,kn=i(),ho=d("div"),u(zo.$$.fragment),Ln=i(),Ao=d("p"),Ao.innerHTML=Hn,Jn=i(),go=d("div"),u(Uo.$$.fragment),zn=i(),Oo=d("p"),Oo.innerHTML=Xn,ln=i(),u(_o.$$.fragment),dn=i(),en=d("p"),this.h()},l(n){const v=Kn("svelte-u9bgzb",document.head);e=c(v,"META",{name:!0,content:!0}),v.forEach(a),m=l(n),o=c(n,"P",{}),L(o).forEach(a),s=l(n),f(T.$$.fragment,n),t=l(n),f(w.$$.fragment,n),Se=l(n),N=c(n,"P",{"data-svelte-h":!0}),$(N)!=="svelte-19tnle3"&&(N.innerHTML=Ye),De=l(n),F=c(n,"P",{"data-svelte-h":!0}),$(F)!=="svelte-1cv3nri"&&(F.textContent=fe),te=l(n),k=c(n,"P",{"data-svelte-h":!0}),$(k)!=="svelte-1av0egr"&&(k.innerHTML=ze),Ae=l(n),q=c(n,"P",{"data-svelte-h":!0}),$(q)!=="svelte-1j389jl"&&(q.innerHTML=at),ce=l(n),f(Ue.$$.fragment,n),K=l(n),Ie=c(n,"P",{"data-svelte-h":!0}),$(Ie)!=="svelte-1pmim83"&&(Ie.textContent=oe),ve=l(n),Ce=c(n,"P",{"data-svelte-h":!0}),$(Ce)!=="svelte-1rfvsf0"&&(Ce.innerHTML=ut),he=l(n),xe=c(n,"P",{"data-svelte-h":!0}),$(xe)!=="svelte-1ngg8v"&&(xe.innerHTML=Me),Ze=l(n),f(G.$$.fragment,n),ge=l(n),f(R.$$.fragment,n),Pe=l(n),je=c(n,"P",{"data-svelte-h":!0}),$(je)!=="svelte-70cri7"&&(je.textContent=rt),ye=l(n),qe=c(n,"UL",{"data-svelte-h":!0}),$(qe)!=="svelte-1ju1zl5"&&(qe.innerHTML=Q),Be=l(n),f(ne.$$.fragment,n),ae=l(n),V=c(n,"UL",{"data-svelte-h":!0}),$(V)!=="svelte-1io35q"&&(V.innerHTML=Oe),_e=l(n),be=c(n,"P",{"data-svelte-h":!0}),$(be)!=="svelte-4e8459"&&(be.innerHTML=Wt),S=l(n),pe=c(n,"UL",{"data-svelte-h":!0}),$(pe)!=="svelte-1mem73y"&&(pe.innerHTML=it),Re=l(n),H=c(n,"P",{"data-svelte-h":!0}),$(H)!=="svelte-1i0t5za"&&(H.innerHTML=Ke),re=l(n),me=c(n,"UL",{"data-svelte-h":!0}),$(me)!=="svelte-116whqt"&&(me.innerHTML=lt),ue=l(n),E=c(n,"P",{"data-svelte-h":!0}),$(E)!=="svelte-1lzjqri"&&(E.textContent=It),Y=l(n),f(ke.$$.fragment,n),Ee=l(n),U=c(n,"DIV",{class:!0});var Vt=L(U);f(Z.$$.fragment,Vt),$e=l(Vt),Fe=c(Vt,"P",{"data-svelte-h":!0}),$(Fe)!=="svelte-774ynz"&&(Fe.innerHTML=He),vt=l(Vt),ee=c(Vt,"P",{"data-svelte-h":!0}),$(ee)!=="svelte-1s6wgpv"&&(ee.innerHTML=Ne),et=l(Vt),f(D.$$.fragment,Vt),I=l(Vt),j=c(Vt,"DIV",{class:!0});var Fo=L(j);f(A.$$.fragment,Fo),X=l(Fo),O=c(Fo,"P",{"data-svelte-h":!0}),$(O)!=="svelte-a8la44"&&(O.innerHTML=Le),Fo.forEach(a),Vt.forEach(a),ie=l(n),f(le.$$.fragment,n),Te=l(n),y=c(n,"DIV",{class:!0});var Dt=L(y);f(x.$$.fragment,Dt),de=l(Dt),We=c(Dt,"P",{"data-svelte-h":!0}),$(We)!=="svelte-7g6yes"&&(We.innerHTML=dt),_t=l(Dt),B=c(Dt,"P",{"data-svelte-h":!0}),$(B)!=="svelte-1s6wgpv"&&(B.innerHTML=ct),tt=l(Dt),f(Je.$$.fragment,Dt),Dt.forEach(a),ot=l(n),f(kt.$$.fragment,n),Gt=l(n),bt=c(n,"DIV",{class:!0});var At=L(bt);f(qt.$$.fragment,At),Ot=l(At),io=c(At,"P",{"data-svelte-h":!0}),$(io)!=="svelte-1x4phi4"&&(io.innerHTML=Wo),Tt=l(At),Lt=c(At,"P",{"data-svelte-h":!0}),$(Lt)!=="svelte-1s6wgpv"&&(Lt.innerHTML=Vo),Kt=l(At),f(Rt.$$.fragment,At),At.forEach(a),po=l(n),f(Mt.$$.fragment,n),mo=l(n),se=c(n,"DIV",{class:!0});var Ft=L(se);f(Zt.$$.fragment,Ft),Et=l(Ft),Ht=c(Ft,"P",{"data-svelte-h":!0}),$(Ht)!=="svelte-vz3r3l"&&(Ht.textContent=yt),Xt=l(Ft),eo=c(Ft,"P",{"data-svelte-h":!0}),$(eo)!=="svelte-rs9us"&&(eo.innerHTML=lo),Zo=l(Ft),$t=c(Ft,"DIV",{class:!0});var bo=L($t);f(Jt.$$.fragment,bo),Bo=l(bo),to=c(bo,"P",{"data-svelte-h":!0}),$(to)!=="svelte-js0y4v"&&(to.textContent=Ct),Qt=l(bo),oo=c(bo,"UL",{"data-svelte-h":!0}),$(oo)!=="svelte-uzwo54"&&(oo.innerHTML=co),No=l(bo),no=c(bo,"P",{"data-svelte-h":!0}),$(no)!=="svelte-1bzh7dh"&&(no.textContent=St),bo.forEach(a),Mo=l(Ft),ft=c(Ft,"DIV",{class:!0});var pn=L(ft);f(Bt.$$.fragment,pn),M=l(pn),P=c(pn,"P",{"data-svelte-h":!0}),$(P)!=="svelte-1f4f5kp"&&(P.innerHTML=zt),pn.forEach(a),xt=l(Ft),Xe=c(Ft,"DIV",{class:!0});var mn=L(Xe);f(nt.$$.fragment,mn),Pt=l(mn),Qe=c(mn,"P",{"data-svelte-h":!0}),$(Qe)!=="svelte-1ctpnmt"&&(Qe.textContent=Ut),mn.forEach(a),ht=l(Ft),pt=c(Ft,"DIV",{class:!0});var Qn=L(pt);f(st.$$.fragment,Qn),Qn.forEach(a),Ft.forEach(a),wt=l(n),f(so.$$.fragment,n),yo=l(n),mt=c(n,"DIV",{class:!0});var ao=L(mt);f($o.$$.fragment,ao),_n=l(ao),Go=c(ao,"P",{"data-svelte-h":!0}),$(Go)!=="svelte-g8hwlw"&&(Go.innerHTML=Fn),bn=l(ao),qo=c(ao,"P",{"data-svelte-h":!0}),$(qo)!=="svelte-y6yfrk"&&(qo.innerHTML=Wn),Tn=l(ao),Nt=c(ao,"DIV",{class:!0});var To=L(Nt);f(wo.$$.fragment,To),Mn=l(To),Ro=c(To,"P",{"data-svelte-h":!0}),$(Ro)!=="svelte-js0y4v"&&(Ro.textContent=Vn),yn=l(To),Eo=c(To,"UL",{"data-svelte-h":!0}),$(Eo)!=="svelte-uzwo54"&&(Eo.innerHTML=Zn),$n=l(To),Ho=c(To,"P",{"data-svelte-h":!0}),$(Ho)!=="svelte-1bzh7dh"&&(Ho.textContent=Bn),To.forEach(a),wn=l(ao),uo=c(ao,"DIV",{class:!0});var un=L(uo);f(Io.$$.fragment,un),In=l(un),Xo=c(un,"P",{"data-svelte-h":!0}),$(Xo)!=="svelte-1ctpnmt"&&(Xo.textContent=Nn),un.forEach(a),ao.forEach(a),tn=l(n),f(vo.$$.fragment,n),on=l(n),Yt=c(n,"DIV",{class:!0});var Ko=L(Yt);f(Co.$$.fragment,Ko),vn=l(Ko),Qo=c(Ko,"P",{"data-svelte-h":!0}),$(Qo)!=="svelte-x48p92"&&(Qo.textContent=Gn),Cn=l(Ko),fo=c(Ko,"DIV",{class:!0});var fn=L(fo);f(xo.$$.fragment,fn),xn=l(fn),So=c(fn,"P",{"data-svelte-h":!0}),$(So)!=="svelte-1x3yxsa"&&(So.textContent=qn),fn.forEach(a),Ko.forEach(a),nn=l(n),f(Po.$$.fragment,n),sn=l(n),jo=c(n,"DIV",{class:!0});var Sn=L(jo);f(ko.$$.fragment,Sn),Sn.forEach(a),an=l(n),f(Lo.$$.fragment,n),rn=l(n),jt=c(n,"DIV",{class:!0});var ro=L(jt);f(Jo.$$.fragment,ro),Pn=l(ro),Yo=c(ro,"P",{"data-svelte-h":!0}),$(Yo)!=="svelte-ofpncz"&&(Yo.textContent=Rn),jn=l(ro),Do=c(ro,"P",{"data-svelte-h":!0}),$(Do)!=="svelte-82lnfu"&&(Do.innerHTML=En),kn=l(ro),ho=c(ro,"DIV",{class:!0});var hn=L(ho);f(zo.$$.fragment,hn),Ln=l(hn),Ao=c(hn,"P",{"data-svelte-h":!0}),$(Ao)!=="svelte-vld6ul"&&(Ao.innerHTML=Hn),hn.forEach(a),Jn=l(ro),go=c(ro,"DIV",{class:!0});var gn=L(go);f(Uo.$$.fragment,gn),zn=l(gn),Oo=c(gn,"P",{"data-svelte-h":!0}),$(Oo)!=="svelte-1ovp6q3"&&(Oo.innerHTML=Xn),gn.forEach(a),ro.forEach(a),ln=l(n),f(_o.$$.fragment,n),dn=l(n),en=c(n,"P",{}),L(en).forEach(a),this.h()},h(){J(e,"name","hf:doc:metadata"),J(e,"content",Qs),J(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(bt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J($t,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(ft,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Xe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(pt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(se,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Nt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(uo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(mt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(fo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Yt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(jo,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(ho,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(go,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(jt,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(n,v){r(document.head,e),p(n,m,v),p(n,o,v),p(n,s,v),h(T,n,v),p(n,t,v),h(w,n,v),p(n,Se,v),p(n,N,v),p(n,De,v),p(n,F,v),p(n,te,v),p(n,k,v),p(n,Ae,v),p(n,q,v),p(n,ce,v),h(Ue,n,v),p(n,K,v),p(n,Ie,v),p(n,ve,v),p(n,Ce,v),p(n,he,v),p(n,xe,v),p(n,Ze,v),h(G,n,v),p(n,ge,v),h(R,n,v),p(n,Pe,v),p(n,je,v),p(n,ye,v),p(n,qe,v),p(n,Be,v),h(ne,n,v),p(n,ae,v),p(n,V,v),p(n,_e,v),p(n,be,v),p(n,S,v),p(n,pe,v),p(n,Re,v),p(n,H,v),p(n,re,v),p(n,me,v),p(n,ue,v),p(n,E,v),p(n,Y,v),h(ke,n,v),p(n,Ee,v),p(n,U,v),h(Z,U,null),r(U,$e),r(U,Fe),r(U,vt),r(U,ee),r(U,et),h(D,U,null),r(U,I),r(U,j),h(A,j,null),r(j,X),r(j,O),p(n,ie,v),h(le,n,v),p(n,Te,v),p(n,y,v),h(x,y,null),r(y,de),r(y,We),r(y,_t),r(y,B),r(y,tt),h(Je,y,null),p(n,ot,v),h(kt,n,v),p(n,Gt,v),p(n,bt,v),h(qt,bt,null),r(bt,Ot),r(bt,io),r(bt,Tt),r(bt,Lt),r(bt,Kt),h(Rt,bt,null),p(n,po,v),h(Mt,n,v),p(n,mo,v),p(n,se,v),h(Zt,se,null),r(se,Et),r(se,Ht),r(se,Xt),r(se,eo),r(se,Zo),r(se,$t),h(Jt,$t,null),r($t,Bo),r($t,to),r($t,Qt),r($t,oo),r($t,No),r($t,no),r(se,Mo),r(se,ft),h(Bt,ft,null),r(ft,M),r(ft,P),r(se,xt),r(se,Xe),h(nt,Xe,null),r(Xe,Pt),r(Xe,Qe),r(se,ht),r(se,pt),h(st,pt,null),p(n,wt,v),h(so,n,v),p(n,yo,v),p(n,mt,v),h($o,mt,null),r(mt,_n),r(mt,Go),r(mt,bn),r(mt,qo),r(mt,Tn),r(mt,Nt),h(wo,Nt,null),r(Nt,Mn),r(Nt,Ro),r(Nt,yn),r(Nt,Eo),r(Nt,$n),r(Nt,Ho),r(mt,wn),r(mt,uo),h(Io,uo,null),r(uo,In),r(uo,Xo),p(n,tn,v),h(vo,n,v),p(n,on,v),p(n,Yt,v),h(Co,Yt,null),r(Yt,vn),r(Yt,Qo),r(Yt,Cn),r(Yt,fo),h(xo,fo,null),r(fo,xn),r(fo,So),p(n,nn,v),h(Po,n,v),p(n,sn,v),p(n,jo,v),h(ko,jo,null),p(n,an,v),h(Lo,n,v),p(n,rn,v),p(n,jt,v),h(Jo,jt,null),r(jt,Pn),r(jt,Yo),r(jt,jn),r(jt,Do),r(jt,kn),r(jt,ho),h(zo,ho,null),r(ho,Ln),r(ho,Ao),r(jt,Jn),r(jt,go),h(Uo,go,null),r(go,zn),r(go,Oo),p(n,ln,v),h(_o,n,v),p(n,dn,v),p(n,en,v),cn=!0},p(n,[v]){const Vt={};v&2&&(Vt.$$scope={dirty:v,ctx:n}),D.$set(Vt);const Fo={};v&2&&(Fo.$$scope={dirty:v,ctx:n}),Je.$set(Fo);const Dt={};v&2&&(Dt.$$scope={dirty:v,ctx:n}),Rt.$set(Dt);const At={};v&2&&(At.$$scope={dirty:v,ctx:n}),_o.$set(At)},i(n){cn||(g(T.$$.fragment,n),g(w.$$.fragment,n),g(Ue.$$.fragment,n),g(G.$$.fragment,n),g(R.$$.fragment,n),g(ne.$$.fragment,n),g(ke.$$.fragment,n),g(Z.$$.fragment,n),g(D.$$.fragment,n),g(A.$$.fragment,n),g(le.$$.fragment,n),g(x.$$.fragment,n),g(Je.$$.fragment,n),g(kt.$$.fragment,n),g(qt.$$.fragment,n),g(Rt.$$.fragment,n),g(Mt.$$.fragment,n),g(Zt.$$.fragment,n),g(Jt.$$.fragment,n),g(Bt.$$.fragment,n),g(nt.$$.fragment,n),g(st.$$.fragment,n),g(so.$$.fragment,n),g($o.$$.fragment,n),g(wo.$$.fragment,n),g(Io.$$.fragment,n),g(vo.$$.fragment,n),g(Co.$$.fragment,n),g(xo.$$.fragment,n),g(Po.$$.fragment,n),g(ko.$$.fragment,n),g(Lo.$$.fragment,n),g(Jo.$$.fragment,n),g(zo.$$.fragment,n),g(Uo.$$.fragment,n),g(_o.$$.fragment,n),cn=!0)},o(n){_(T.$$.fragment,n),_(w.$$.fragment,n),_(Ue.$$.fragment,n),_(G.$$.fragment,n),_(R.$$.fragment,n),_(ne.$$.fragment,n),_(ke.$$.fragment,n),_(Z.$$.fragment,n),_(D.$$.fragment,n),_(A.$$.fragment,n),_(le.$$.fragment,n),_(x.$$.fragment,n),_(Je.$$.fragment,n),_(kt.$$.fragment,n),_(qt.$$.fragment,n),_(Rt.$$.fragment,n),_(Mt.$$.fragment,n),_(Zt.$$.fragment,n),_(Jt.$$.fragment,n),_(Bt.$$.fragment,n),_(nt.$$.fragment,n),_(st.$$.fragment,n),_(so.$$.fragment,n),_($o.$$.fragment,n),_(wo.$$.fragment,n),_(Io.$$.fragment,n),_(vo.$$.fragment,n),_(Co.$$.fragment,n),_(xo.$$.fragment,n),_(Po.$$.fragment,n),_(ko.$$.fragment,n),_(Lo.$$.fragment,n),_(Jo.$$.fragment,n),_(zo.$$.fragment,n),_(Uo.$$.fragment,n),_(_o.$$.fragment,n),cn=!1},d(n){n&&(a(m),a(o),a(s),a(t),a(Se),a(N),a(De),a(F),a(te),a(k),a(Ae),a(q),a(ce),a(K),a(Ie),a(ve),a(Ce),a(he),a(xe),a(Ze),a(ge),a(Pe),a(je),a(ye),a(qe),a(Be),a(ae),a(V),a(_e),a(be),a(S),a(pe),a(Re),a(H),a(re),a(me),a(ue),a(E),a(Y),a(Ee),a(U),a(ie),a(Te),a(y),a(ot),a(Gt),a(bt),a(po),a(mo),a(se),a(wt),a(yo),a(mt),a(tn),a(on),a(Yt),a(nn),a(sn),a(jo),a(an),a(rn),a(jt),a(ln),a(dn),a(en)),a(e),b(T,n),b(w,n),b(Ue,n),b(G,n),b(R,n),b(ne,n),b(ke,n),b(Z),b(D),b(A),b(le,n),b(x),b(Je),b(kt,n),b(qt),b(Rt),b(Mt,n),b(Zt),b(Jt),b(Bt),b(nt),b(st),b(so,n),b($o),b(wo),b(Io),b(vo,n),b(Co),b(xo),b(Po,n),b(ko),b(Lo,n),b(Jo),b(zo),b(Uo),b(_o,n)}}}const Qs='{"title":"CLIP","local":"clip","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips and example","local":"usage-tips-and-example","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"CLIPConfig","local":"transformers.CLIPConfig","sections":[],"depth":2},{"title":"CLIPTextConfig","local":"transformers.CLIPTextConfig","sections":[],"depth":2},{"title":"CLIPVisionConfig","local":"transformers.CLIPVisionConfig","sections":[],"depth":2},{"title":"CLIPTokenizer","local":"transformers.CLIPTokenizer","sections":[],"depth":2},{"title":"CLIPTokenizerFast","local":"transformers.CLIPTokenizerFast","sections":[],"depth":2},{"title":"CLIPImageProcessor","local":"transformers.CLIPImageProcessor","sections":[],"depth":2},{"title":"CLIPFeatureExtractor","local":"transformers.CLIPFeatureExtractor","sections":[],"depth":2},{"title":"CLIPProcessor","local":"transformers.CLIPProcessor","sections":[],"depth":2},{"title":"CLIPModel","local":"transformers.CLIPModel","sections":[],"depth":2},{"title":"CLIPTextModel","local":"transformers.CLIPTextModel","sections":[],"depth":2},{"title":"CLIPTextModelWithProjection","local":"transformers.CLIPTextModelWithProjection","sections":[],"depth":2},{"title":"CLIPVisionModelWithProjection","local":"transformers.CLIPVisionModelWithProjection","sections":[],"depth":2},{"title":"CLIPVisionModel","local":"transformers.CLIPVisionModel","sections":[],"depth":2},{"title":"TFCLIPModel","local":"transformers.TFCLIPModel","sections":[],"depth":2},{"title":"TFCLIPTextModel","local":"transformers.TFCLIPTextModel","sections":[],"depth":2},{"title":"TFCLIPVisionModel","local":"transformers.TFCLIPVisionModel","sections":[],"depth":2},{"title":"FlaxCLIPModel","local":"transformers.FlaxCLIPModel","sections":[],"depth":2},{"title":"FlaxCLIPTextModel","local":"transformers.FlaxCLIPTextModel","sections":[],"depth":2},{"title":"FlaxCLIPTextModelWithProjection","local":"transformers.FlaxCLIPTextModelWithProjection","sections":[],"depth":2},{"title":"FlaxCLIPVisionModel","local":"transformers.FlaxCLIPVisionModel","sections":[],"depth":2}],"depth":1}';function Ss(C){return Dn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class sa extends An{constructor(e){super(),On(this,e,Ss,Xs,Yn,{})}}export{sa as component};
