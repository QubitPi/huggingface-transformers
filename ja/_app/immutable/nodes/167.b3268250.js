import{s as Pe,o as De,n as He}from"../chunks/scheduler.9bc65507.js";import{S as Ke,i as Oe,g as u,s as r,r as c,A as et,h as J,f as t,c as p,j as Xe,u as h,x as T,k as _e,y as tt,a,v as y,d as g,t as U,w as $}from"../chunks/index.707bf1b6.js";import{T as Se}from"../chunks/Tip.c2ecdbf4.js";import{Y as Ne}from"../chunks/Youtube.e1129c6f.js";import{C as F}from"../chunks/CodeBlock.54a9f38d.js";import{D as st}from"../chunks/DocNotebookDropdown.41f65cb5.js";import{F as qe,M as Qe}from"../chunks/Markdown.8ab98a13.js";import{H as be}from"../chunks/Heading.342b1fa6.js";function at(ge){let i,j=`一部の事前学習済みの重みが使用されず、一部の重みがランダムに初期化された警告が表示されることがあります。心配しないでください、これは完全に正常です！
BERTモデルの事前学習済みのヘッドは破棄され、ランダムに初期化された分類ヘッドで置き換えられます。この新しいモデルヘッドをシーケンス分類タスクでファインチューニングし、事前学習モデルの知識をそれに転送します。`;return{c(){i=u("p"),i.textContent=j},l(m){i=J(m,"P",{"data-svelte-h":!0}),T(i)!=="svelte-1r1rjun"&&(i.textContent=j)},m(m,b){a(m,i,b)},p:He,d(m){m&&t(i)}}}function lt(ge){let i,j,m,b,w,re=`🤗 Transformersは、🤗 Transformersモデルのトレーニングを最適化した<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a>クラスを提供し、独自のトレーニングループを手動で記述せずにトレーニングを開始しやすくしています。
<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a> APIは、ログ記録、勾配累積、混合精度など、さまざまなトレーニングオプションと機能をサポートしています。`,N,pe,H='まず、モデルをロードし、予想されるラベルの数を指定します。Yelp Review <a href="https://huggingface.co/datasets/yelp_review_full#data-fields" rel="nofollow">dataset card</a>から、5つのラベルがあることがわかります：',ie,R,S,Ue,K,Z,le,Q,me=`次に、トレーニングオプションをアクティベートするためのすべてのハイパーパラメータと、調整できるハイパーパラメータを含む<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a>クラスを作成します。
このチュートリアルでは、デフォルトのトレーニング<a href="https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments" rel="nofollow">ハイパーパラメータ</a>を使用して開始できますが、最適な設定を見つけるためにこれらを実験しても構いません。`,z,G,O="トレーニングのチェックポイントを保存する場所を指定します：",we,W,X,A,oe,L,Me=`<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a>はトレーニング中に自動的にモデルのパフォーマンスを評価しません。メトリクスを計算して報告する関数を<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a>に渡す必要があります。
<a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">🤗 Evaluate</a>ライブラリでは、<code>evaluate.load</code>関数を使用して読み込むことができるシンプルな<a href="https://huggingface.co/spaces/evaluate-metric/accuracy" rel="nofollow"><code>accuracy</code></a>関数が提供されています（詳細については<a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">こちらのクイックツアー</a>を参照してください）：`,I,B,ee,C,ue="<code>metric</code>の<code>~evaluate.compute</code>を呼び出して、予測の正確度を計算します。 <code>compute</code>に予測を渡す前に、予測をロジットに変換する必要があります（すべての🤗 Transformersモデルはロジットを返すことを覚えておいてください）：",fe,q,te,k,Je="評価メトリクスをファインチューニング中に監視したい場合、トレーニング引数で <code>evaluation_strategy</code> パラメータを指定して、各エポックの終了時に評価メトリクスを報告します：",$e,E,P,se,V,Te,de='モデル、トレーニング引数、トレーニングおよびテストデータセット、評価関数を使用して<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a>オブジェクトを作成します：',Y,ae,_,ce,D='その後、<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.train">train()</a>を呼び出してモデルを微調整します：',he,v,x;return i=new Ne({props:{id:"nvBXf7s7vTI"}}),m=new be({props:{title:"Train with Pytorch Trainer",local:"train-with-pytorch-trainer",headingTag:"h2"}}),R=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyJTJDJTIwbnVtX2xhYmVscyUzRDUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">5</span>)`,wrap:!1}}),Ue=new Se({props:{$$slots:{default:[at]},$$scope:{ctx:ge}}}),Z=new be({props:{title:"Training Hyperparameters",local:"training-hyperparameters",headingTag:"h3"}}),W=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluaW5nQXJndW1lbnRzJTBBJTBBdHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKG91dHB1dF9kaXIlM0QlMjJ0ZXN0X3RyYWluZXIlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;test_trainer&quot;</span>)`,wrap:!1}}),A=new be({props:{title:"Evaluate",local:"evaluate",headingTag:"h3"}}),B=new F({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBaW1wb3J0JTIwZXZhbHVhdGUlMEElMEFtZXRyaWMlMjAlM0QlMjBldmFsdWF0ZS5sb2FkKCUyMmFjY3VyYWN5JTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`,wrap:!1}}),q=new F({props:{code:"ZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMkMlMjBsYWJlbHMlMjAlM0QlMjBldmFsX3ByZWQlMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUyMCUzRCUyMG5wLmFyZ21heChsb2dpdHMlMkMlMjBheGlzJTNELTEpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwbWV0cmljLmNvbXB1dGUocHJlZGljdGlvbnMlM0RwcmVkaWN0aW9ucyUyQyUyMHJlZmVyZW5jZXMlM0RsYWJlbHMp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    logits, labels = eval_pred
<span class="hljs-meta">... </span>    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)`,wrap:!1}}),E=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRyYWluaW5nQXJndW1lbnRzJTJDJTIwVHJhaW5lciUwQSUwQXRyYWluaW5nX2FyZ3MlMjAlM0QlMjBUcmFpbmluZ0FyZ3VtZW50cyhvdXRwdXRfZGlyJTNEJTIydGVzdF90cmFpbmVyJTIyJTJDJTIwZXZhbHVhdGlvbl9zdHJhdGVneSUzRCUyMmVwb2NoJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;test_trainer&quot;</span>, evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>)`,wrap:!1}}),se=new be({props:{title:"Trainer",local:"trainer",headingTag:"h3"}}),ae=new F({props:{code:"dHJhaW5lciUyMCUzRCUyMFRyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0Rtb2RlbCUyQyUwQSUyMCUyMCUyMCUyMGFyZ3MlM0R0cmFpbmluZ19hcmdzJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRHNtYWxsX3RyYWluX2RhdGFzZXQlMkMlMEElMjAlMjAlMjAlMjBldmFsX2RhdGFzZXQlM0RzbWFsbF9ldmFsX2RhdGFzZXQlMkMlMEElMjAlMjAlMjAlMjBjb21wdXRlX21ldHJpY3MlM0Rjb21wdXRlX21ldHJpY3MlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=small_train_dataset,
<span class="hljs-meta">... </span>    eval_dataset=small_eval_dataset,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)`,wrap:!1}}),v=new F({props:{code:"dHJhaW5lci50cmFpbigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()',wrap:!1}}),{c(){c(i.$$.fragment),j=r(),c(m.$$.fragment),b=r(),w=u("p"),w.innerHTML=re,N=r(),pe=u("p"),pe.innerHTML=H,ie=r(),c(R.$$.fragment),S=r(),c(Ue.$$.fragment),K=r(),c(Z.$$.fragment),le=r(),Q=u("p"),Q.innerHTML=me,z=r(),G=u("p"),G.textContent=O,we=r(),c(W.$$.fragment),X=r(),c(A.$$.fragment),oe=r(),L=u("p"),L.innerHTML=Me,I=r(),c(B.$$.fragment),ee=r(),C=u("p"),C.innerHTML=ue,fe=r(),c(q.$$.fragment),te=r(),k=u("p"),k.innerHTML=Je,$e=r(),c(E.$$.fragment),P=r(),c(se.$$.fragment),V=r(),Te=u("p"),Te.innerHTML=de,Y=r(),c(ae.$$.fragment),_=r(),ce=u("p"),ce.innerHTML=D,he=r(),c(v.$$.fragment)},l(l){h(i.$$.fragment,l),j=p(l),h(m.$$.fragment,l),b=p(l),w=J(l,"P",{"data-svelte-h":!0}),T(w)!=="svelte-xticyv"&&(w.innerHTML=re),N=p(l),pe=J(l,"P",{"data-svelte-h":!0}),T(pe)!=="svelte-idmno9"&&(pe.innerHTML=H),ie=p(l),h(R.$$.fragment,l),S=p(l),h(Ue.$$.fragment,l),K=p(l),h(Z.$$.fragment,l),le=p(l),Q=J(l,"P",{"data-svelte-h":!0}),T(Q)!=="svelte-hahful"&&(Q.innerHTML=me),z=p(l),G=J(l,"P",{"data-svelte-h":!0}),T(G)!=="svelte-yrhe9g"&&(G.textContent=O),we=p(l),h(W.$$.fragment,l),X=p(l),h(A.$$.fragment,l),oe=p(l),L=J(l,"P",{"data-svelte-h":!0}),T(L)!=="svelte-upoddc"&&(L.innerHTML=Me),I=p(l),h(B.$$.fragment,l),ee=p(l),C=J(l,"P",{"data-svelte-h":!0}),T(C)!=="svelte-92ek02"&&(C.innerHTML=ue),fe=p(l),h(q.$$.fragment,l),te=p(l),k=J(l,"P",{"data-svelte-h":!0}),T(k)!=="svelte-7iull8"&&(k.innerHTML=Je),$e=p(l),h(E.$$.fragment,l),P=p(l),h(se.$$.fragment,l),V=p(l),Te=J(l,"P",{"data-svelte-h":!0}),T(Te)!=="svelte-117y1cw"&&(Te.innerHTML=de),Y=p(l),h(ae.$$.fragment,l),_=p(l),ce=J(l,"P",{"data-svelte-h":!0}),T(ce)!=="svelte-brzo73"&&(ce.innerHTML=D),he=p(l),h(v.$$.fragment,l)},m(l,f){y(i,l,f),a(l,j,f),y(m,l,f),a(l,b,f),a(l,w,f),a(l,N,f),a(l,pe,f),a(l,ie,f),y(R,l,f),a(l,S,f),y(Ue,l,f),a(l,K,f),y(Z,l,f),a(l,le,f),a(l,Q,f),a(l,z,f),a(l,G,f),a(l,we,f),y(W,l,f),a(l,X,f),y(A,l,f),a(l,oe,f),a(l,L,f),a(l,I,f),y(B,l,f),a(l,ee,f),a(l,C,f),a(l,fe,f),y(q,l,f),a(l,te,f),a(l,k,f),a(l,$e,f),y(E,l,f),a(l,P,f),y(se,l,f),a(l,V,f),a(l,Te,f),a(l,Y,f),y(ae,l,f),a(l,_,f),a(l,ce,f),a(l,he,f),y(v,l,f),x=!0},p(l,f){const ne={};f&2&&(ne.$$scope={dirty:f,ctx:l}),Ue.$set(ne)},i(l){x||(g(i.$$.fragment,l),g(m.$$.fragment,l),g(R.$$.fragment,l),g(Ue.$$.fragment,l),g(Z.$$.fragment,l),g(W.$$.fragment,l),g(A.$$.fragment,l),g(B.$$.fragment,l),g(q.$$.fragment,l),g(E.$$.fragment,l),g(se.$$.fragment,l),g(ae.$$.fragment,l),g(v.$$.fragment,l),x=!0)},o(l){U(i.$$.fragment,l),U(m.$$.fragment,l),U(R.$$.fragment,l),U(Ue.$$.fragment,l),U(Z.$$.fragment,l),U(W.$$.fragment,l),U(A.$$.fragment,l),U(B.$$.fragment,l),U(q.$$.fragment,l),U(E.$$.fragment,l),U(se.$$.fragment,l),U(ae.$$.fragment,l),U(v.$$.fragment,l),x=!1},d(l){l&&(t(j),t(b),t(w),t(N),t(pe),t(ie),t(S),t(K),t(le),t(Q),t(z),t(G),t(we),t(X),t(oe),t(L),t(I),t(ee),t(C),t(fe),t(te),t(k),t($e),t(P),t(V),t(Te),t(Y),t(_),t(ce),t(he)),$(i,l),$(m,l),$(R,l),$(Ue,l),$(Z,l),$(W,l),$(A,l),$(B,l),$(q,l),$(E,l),$(se,l),$(ae,l),$(v,l)}}}function nt(ge){let i,j;return i=new Qe({props:{$$slots:{default:[lt]},$$scope:{ctx:ge}}}),{c(){c(i.$$.fragment)},l(m){h(i.$$.fragment,m)},m(m,b){y(i,m,b),j=!0},p(m,b){const w={};b&2&&(w.$$scope={dirty:b,ctx:m}),i.$set(w)},i(m){j||(g(i.$$.fragment,m),j=!0)},o(m){U(i.$$.fragment,m),j=!1},d(m){$(i,m)}}}function rt(ge){let i,j=`モデルを<code>compile()</code>する際に<code>loss</code>引数を渡す必要はありません！Hugging Faceモデルは、この引数を空白のままにしておくと、タスクとモデルアーキテクチャに適した損失を自動的に選択します。
必要に応じて自分で損失を指定してオーバーライドすることもできます！`;return{c(){i=u("p"),i.innerHTML=j},l(m){i=J(m,"P",{"data-svelte-h":!0}),T(i)!=="svelte-f8a72j"&&(i.innerHTML=j)},m(m,b){a(m,i,b)},p:He,d(m){m&&t(i)}}}function pt(ge){let i,j,m,b,w,re,N,pe="Keras APIを使用して🤗 TransformersモデルをTensorFlowでトレーニングすることもできます！",H,ie,R,S,Ue=`🤗 TransformersモデルをKeras APIでトレーニングする場合、データセットをKerasが理解できる形式に変換する必要があります。
データセットが小さい場合、データセット全体をNumPy配列に変換してKerasに渡すことができます。
複雑なことをする前に、まずそれを試してみましょう。`,K,Z,le=`まず、データセットを読み込みます。GLUEベンチマークからCoLAデータセットを使用します
(<a href="https://huggingface.co/datasets/glue" rel="nofollow">GLUE Banchmark</a>)、これは単純なバイナリテキスト分類タスクです。今のところトレーニング分割のみを使用します。`,Q,me,z,G,O="次に、トークナイザをロードし、データをNumPy配列としてトークン化します。ラベルは既に<code>0</code>と<code>1</code>のリストであるため、トークン化せずに直接NumPy配列に変換できます！",we,W,X,A,oe=`最後に、モデルをロードし、<a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a> と <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a> メソッドを実行します。
注意点として、Transformersモデルはすべてデフォルトでタスクに関連した損失関数を持っているため、指定しなくても構いません（指定する場合を除く）：`,L,Me,I,B,ee,C,ue=`このアプローチは、小規模なデータセットには適していますが、大規模なデータセットに対しては問題になることがあります。なぜなら、トークナイズされた配列とラベルはメモリに完全に読み込まれる必要があり、またNumPyは「ジャギー」な配列を処理しないため、トークナイズされた各サンプルを全体のデータセット内で最も長いサンプルの長さにパディングする必要があります。
これにより、配列がさらに大きくなり、すべてのパディングトークンがトレーニングを遅くする原因になります！`,fe,q,te,k,Je="トレーニングを遅くせずにデータを読み込むには、データを<code>tf.data.Dataset</code>として読み込むことができます。独自の<code>tf.data</code>パイプラインを作成することもできますが、これを行うための便利な方法が2つあります：",$e,E,P='<li><a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset">prepare_tf_dataset()</a>: これはほとんどの場合で推奨する方法です。モデル上のメソッドなので、モデルを検査してモデル入力として使用可能な列を自動的に把握し、他の列を破棄してより単純で高性能なデータセットを作成できます。</li> <li><code>to_tf_dataset</code>: このメソッドはより低レベルで、データセットがどのように作成されるかを正確に制御する場合に便利です。<code>columns</code>と<code>label_cols</code>を指定して、データセットに含める列を正確に指定できます。</li>',se,V,Te='<a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset">prepare_tf_dataset()</a>を使用する前に、次のコードサンプルに示すように、トークナイザの出力をデータセットに列として追加する必要があります：',de,Y,ae,_,ce=`Hugging Faceのデータセットはデフォルトでディスクに保存されるため、これによりメモリの使用量が増えることはありません！
列が追加されたら、データセットからバッチをストリームし、各バッチにパディングを追加できます。これにより、
データセット全体にパディングを追加する場合と比べて、パディングトークンの数が大幅に削減されます。`,D,he,v,x,l=`上記のコードサンプルでは、トークナイザを<code>prepare_tf_dataset</code>に渡して、バッチを正しく読み込む際に正しくパディングできるようにする必要があります。
データセットのすべてのサンプルが同じ長さであり、パディングが不要な場合は、この引数をスキップできます。
パディング以外の複雑な処理を行う必要がある場合（例：マスク言語モデリングのためのトークンの破損など）、
代わりに<code>collate_fn</code>引数を使用して、サンプルのリストをバッチに変換し、必要な前処理を適用する関数を渡すことができます。
このアプローチを実際に使用した例については、
<a href="https://github.com/huggingface/transformers/tree/main/examples" rel="nofollow">examples</a>や
<a href="https://huggingface.co/docs/transformers/notebooks" rel="nofollow">notebooks</a>をご覧ください。`,f,ne,ke="<code>tf.data.Dataset</code>を作成したら、以前と同様にモデルをコンパイルし、適合させることができます：",ye,Ce,je;return m=new Ne({props:{id:"rnTGBy2ax1c"}}),w=new be({props:{title:"Kerasを使用してTensorFlowモデルをトレーニングする",local:"kerasを使用してtensorflowモデルをトレーニングする",headingTag:"h2"}}),ie=new be({props:{title:"Loading Data from Keras",local:"loading-data-from-keras",headingTag:"h3"}}),me=new F({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJnbHVlJTIyJTJDJTIwJTIyY29sYSUyMiklMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldCU1QiUyMnRyYWluJTIyJTVEJTIwJTIwJTIzJTIwJUU0JUJCJThBJUUzJTgxJUFFJUUzJTgxJUE4JUUzJTgxJTkzJUUzJTgyJThEJUUzJTgzJTg4JUUzJTgzJUFDJUUzJTgzJUJDJUUzJTgzJThCJUUzJTgzJUIzJUUzJTgyJUIwJUU1JTg4JTg2JUU1JTg5JUIyJUUzJTgxJUFFJUUzJTgxJUJGJUUzJTgyJTkyJUU0JUJEJUJGJUU3JTk0JUE4JUUzJTgxJTk3JUUzJTgxJUJFJUUzJTgxJTk5",highlighted:`<span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

dataset = load_dataset(<span class="hljs-string">&quot;glue&quot;</span>, <span class="hljs-string">&quot;cola&quot;</span>)
dataset = dataset[<span class="hljs-string">&quot;train&quot;</span>]  <span class="hljs-comment"># 今のところトレーニング分割のみを使用します</span>`,wrap:!1}}),W=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEF0b2tlbml6ZWRfZGF0YSUyMCUzRCUyMHRva2VuaXplcihkYXRhc2V0JTVCJTIyc2VudGVuY2UlMjIlNUQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMm5wJTIyJTJDJTIwcGFkZGluZyUzRFRydWUpJTBBJTIzJTIwJUUzJTgzJTg4JUUzJTgzJUJDJUUzJTgyJUFGJUUzJTgzJThBJUUzJTgyJUE0JUUzJTgyJUI2JUUzJTgxJUFGQmF0Y2hFbmNvZGluZyVFMyU4MiU5MiVFOCVCRiU5NCVFMyU4MSU5NyVFMyU4MSVCRSVFMyU4MSU5OSVFMyU4MSU4QyVFMyU4MCU4MSVFMyU4MSU5RCVFMyU4MiU4QyVFMyU4MiU5MktlcmFzJUU3JTk0JUE4JUUzJTgxJUFCJUU4JUJFJTlFJUU2JTlCJUI4JUUzJTgxJUFCJUU1JUE0JTg5JUU2JThGJTlCJUUzJTgxJTk3JUUzJTgxJUJFJUUzJTgxJTk5JTBBdG9rZW5pemVkX2RhdGElMjAlM0QlMjBkaWN0KHRva2VuaXplZF9kYXRhKSUwQSUwQWxhYmVscyUyMCUzRCUyMG5wLmFycmF5KGRhdGFzZXQlNUIlMjJsYWJlbCUyMiU1RCklMjAlMjAlMjMlMjAlRTMlODMlQTklRTMlODMlOTklRTMlODMlQUIlRTMlODElQUYlRTMlODElOTklRTMlODElQTclRTMlODElQUIwJUUzJTgxJUE4MSVFMyU4MSVBRSVFOSU4NSU4RCVFNSU4OCU5NyVFMyU4MSVBNyVFMyU4MSU5OQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
tokenized_data = tokenizer(dataset[<span class="hljs-string">&quot;sentence&quot;</span>], return_tensors=<span class="hljs-string">&quot;np&quot;</span>, padding=<span class="hljs-literal">True</span>)
<span class="hljs-comment"># トークナイザはBatchEncodingを返しますが、それをKeras用に辞書に変換します</span>
tokenized_data = <span class="hljs-built_in">dict</span>(tokenized_data)

labels = np.array(dataset[<span class="hljs-string">&quot;label&quot;</span>])  <span class="hljs-comment"># ラベルはすでに0と1の配列です</span>`,wrap:!1}}),Me=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxdWVuY2VDbGFzc2lmaWNhdGlvbiUwQWZyb20lMjB0ZW5zb3JmbG93LmtlcmFzLm9wdGltaXplcnMlMjBpbXBvcnQlMjBBZGFtJTBBJTBBJTIzJTIwJUUzJTgzJUEyJUUzJTgzJTg3JUUzJTgzJUFCJUUzJTgyJTkyJUUzJTgzJUFEJUUzJTgzJUJDJUUzJTgzJTg5JUUzJTgxJTk3JUUzJTgxJUE2JUUzJTgyJUIzJUUzJTgzJUIzJUUzJTgzJTkxJUUzJTgyJUE0JUUzJTgzJUFCJUUzJTgxJTk5JUUzJTgyJThCJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyKSUwQSUyMyUyMCVFMyU4MyU5NSVFMyU4MiVBMSVFMyU4MiVBNCVFMyU4MyVCMyVFMyU4MyU4MSVFMyU4MyVBNSVFMyU4MyVCQyVFMyU4MyU4QiVFMyU4MyVCMyVFMyU4MiVCMCVFMyU4MSVBQiVFMyU4MSVBRiVFOSU4MCU5QSVFNSVCOCVCOCVFMyU4MCU4MSVFNSVBRCVBNiVFNyVCRiU5MiVFNyU4RSU4NyVFMyU4MiU5MiVFNCVCOCU4QiVFMyU4MSU5MiVFMyU4MiU4QiVFMyU4MSVBOCVFOCU4OSVBRiVFMyU4MSU4NCVFMyU4MSVBNyVFMyU4MSU5OSUwQW1vZGVsLmNvbXBpbGUob3B0aW1pemVyJTNEQWRhbSgzZS01KSklMjAlMjAlMjMlMjAlRTYlOTAlOEQlRTUlQTQlQjElRTklOTYlQTIlRTYlOTUlQjAlRTMlODElQUUlRTYlOEMlODclRTUlQUUlOUElRTMlODElQUYlRTQlQjglOEQlRTglQTYlODElRTMlODElQTclRTMlODElOTklRUYlQkMlODElMEElMEFtb2RlbC5maXQodG9rZW5pemVkX2RhdGElMkMlMjBsYWJlbHMp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSequenceClassification
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam

<span class="hljs-comment"># モデルをロードしてコンパイルする</span>
model = TFAutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)
<span class="hljs-comment"># ファインチューニングには通常、学習率を下げると良いです</span>
model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">3e-5</span>))  <span class="hljs-comment"># 損失関数の指定は不要です！</span>

model.fit(tokenized_data, labels)`,wrap:!1}}),B=new Se({props:{$$slots:{default:[rt]},$$scope:{ctx:ge}}}),q=new be({props:{title:"Loading data as a tf.data.Dataset",local:"loading-data-as-a-tfdatadataset",headingTag:"h3"}}),Y=new F({props:{code:"ZGVmJTIwdG9rZW5pemVfZGF0YXNldChkYXRhKSUzQSUwQSUyMCUyMCUyMCUyMCUyMyUyMCVFOCVCRiU5NCVFMyU4MSU5NSVFMyU4MiU4QyVFMyU4MSU5RiVFOCVCRSU5RSVFNiU5QiVCOCVFMyU4MSVBRSVFMyU4MiVBRCVFMyU4MyVCQyVFMyU4MSVBRiVFMyU4MyU4NyVFMyU4MyVCQyVFMyU4MiVCRiVFMyU4MiVCQiVFMyU4MyU4MyVFMyU4MyU4OCVFMyU4MSVBQiVFNSU4OCU5NyVFMyU4MSVBOCVFMyU4MSU5NyVFMyU4MSVBNiVFOCVCRiVCRCVFNSU4QSVBMCVFMyU4MSU5NSVFMyU4MiU4QyVFMyU4MSVCRSVFMyU4MSU5OSUwQSUyMCUyMCUyMCUyMHJldHVybiUyMHRva2VuaXplcihkYXRhJTVCJTIydGV4dCUyMiU1RCklMEElMEElMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5tYXAodG9rZW5pemVfZGF0YXNldCk=",highlighted:`<span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_dataset</span>(<span class="hljs-params">data</span>):
    <span class="hljs-comment"># 返された辞書のキーはデータセットに列として追加されます</span>
    <span class="hljs-keyword">return</span> tokenizer(data[<span class="hljs-string">&quot;text&quot;</span>])


dataset = dataset.<span class="hljs-built_in">map</span>(tokenize_dataset)`,wrap:!1}}),he=new F({props:{code:"dGZfZGF0YXNldCUyMCUzRCUyMG1vZGVsLnByZXBhcmVfdGZfZGF0YXNldChkYXRhc2V0JTVCJTIydHJhaW4lMjIlNUQlMkMlMjBiYXRjaF9zaXplJTNEMTYlMkMlMjBzaHVmZmxlJTNEVHJ1ZSUyQyUyMHRva2VuaXplciUzRHRva2VuaXplcik=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tf_dataset = model.prepare_tf_dataset(dataset[<span class="hljs-string">&quot;train&quot;</span>], batch_size=<span class="hljs-number">16</span>, shuffle=<span class="hljs-literal">True</span>, tokenizer=tokenizer)',wrap:!1}}),Ce=new F({props:{code:"bW9kZWwuY29tcGlsZShvcHRpbWl6ZXIlM0RBZGFtKDNlLTUpKSUyMCUyMCUyMyUyMCVFNiU5MCU4RCVFNSVBNCVCMSVFNSVCQyU5NSVFNiU5NSVCMCVFMyU4MSVBRiVFNCVCOCU4RCVFOCVBNiU4MSVFMyU4MSVBNyVFMyU4MSU5OSVFRiVCQyU4MSUwQSUwQW1vZGVsLmZpdCh0Zl9kYXRhc2V0KQ==",highlighted:`model.<span class="hljs-built_in">compile</span>(optimizer=Adam(<span class="hljs-number">3e-5</span>))  <span class="hljs-comment"># 損失引数は不要です！</span>

model.fit(tf_dataset)`,wrap:!1}}),{c(){i=u("a"),j=r(),c(m.$$.fragment),b=r(),c(w.$$.fragment),re=r(),N=u("p"),N.textContent=pe,H=r(),c(ie.$$.fragment),R=r(),S=u("p"),S.textContent=Ue,K=r(),Z=u("p"),Z.innerHTML=le,Q=r(),c(me.$$.fragment),z=r(),G=u("p"),G.innerHTML=O,we=r(),c(W.$$.fragment),X=r(),A=u("p"),A.innerHTML=oe,L=r(),c(Me.$$.fragment),I=r(),c(B.$$.fragment),ee=r(),C=u("p"),C.textContent=ue,fe=r(),c(q.$$.fragment),te=r(),k=u("p"),k.innerHTML=Je,$e=r(),E=u("ul"),E.innerHTML=P,se=r(),V=u("p"),V.innerHTML=Te,de=r(),c(Y.$$.fragment),ae=r(),_=u("p"),_.textContent=ce,D=r(),c(he.$$.fragment),v=r(),x=u("p"),x.innerHTML=l,f=r(),ne=u("p"),ne.innerHTML=ke,ye=r(),c(Ce.$$.fragment),this.h()},l(n){i=J(n,"A",{id:!0}),Xe(i).forEach(t),j=p(n),h(m.$$.fragment,n),b=p(n),h(w.$$.fragment,n),re=p(n),N=J(n,"P",{"data-svelte-h":!0}),T(N)!=="svelte-hij4u2"&&(N.textContent=pe),H=p(n),h(ie.$$.fragment,n),R=p(n),S=J(n,"P",{"data-svelte-h":!0}),T(S)!=="svelte-bi33d1"&&(S.textContent=Ue),K=p(n),Z=J(n,"P",{"data-svelte-h":!0}),T(Z)!=="svelte-2kwaxv"&&(Z.innerHTML=le),Q=p(n),h(me.$$.fragment,n),z=p(n),G=J(n,"P",{"data-svelte-h":!0}),T(G)!=="svelte-xay811"&&(G.innerHTML=O),we=p(n),h(W.$$.fragment,n),X=p(n),A=J(n,"P",{"data-svelte-h":!0}),T(A)!=="svelte-kxddy9"&&(A.innerHTML=oe),L=p(n),h(Me.$$.fragment,n),I=p(n),h(B.$$.fragment,n),ee=p(n),C=J(n,"P",{"data-svelte-h":!0}),T(C)!=="svelte-1kvpnug"&&(C.textContent=ue),fe=p(n),h(q.$$.fragment,n),te=p(n),k=J(n,"P",{"data-svelte-h":!0}),T(k)!=="svelte-ykzkzj"&&(k.innerHTML=Je),$e=p(n),E=J(n,"UL",{"data-svelte-h":!0}),T(E)!=="svelte-1kei9a2"&&(E.innerHTML=P),se=p(n),V=J(n,"P",{"data-svelte-h":!0}),T(V)!=="svelte-ftmhdo"&&(V.innerHTML=Te),de=p(n),h(Y.$$.fragment,n),ae=p(n),_=J(n,"P",{"data-svelte-h":!0}),T(_)!=="svelte-2syfbr"&&(_.textContent=ce),D=p(n),h(he.$$.fragment,n),v=p(n),x=J(n,"P",{"data-svelte-h":!0}),T(x)!=="svelte-1cfta1t"&&(x.innerHTML=l),f=p(n),ne=J(n,"P",{"data-svelte-h":!0}),T(ne)!=="svelte-xjdq1k"&&(ne.innerHTML=ke),ye=p(n),h(Ce.$$.fragment,n),this.h()},h(){_e(i,"id","keras")},m(n,d){a(n,i,d),a(n,j,d),y(m,n,d),a(n,b,d),y(w,n,d),a(n,re,d),a(n,N,d),a(n,H,d),y(ie,n,d),a(n,R,d),a(n,S,d),a(n,K,d),a(n,Z,d),a(n,Q,d),y(me,n,d),a(n,z,d),a(n,G,d),a(n,we,d),y(W,n,d),a(n,X,d),a(n,A,d),a(n,L,d),y(Me,n,d),a(n,I,d),y(B,n,d),a(n,ee,d),a(n,C,d),a(n,fe,d),y(q,n,d),a(n,te,d),a(n,k,d),a(n,$e,d),a(n,E,d),a(n,se,d),a(n,V,d),a(n,de,d),y(Y,n,d),a(n,ae,d),a(n,_,d),a(n,D,d),y(he,n,d),a(n,v,d),a(n,x,d),a(n,f,d),a(n,ne,d),a(n,ye,d),y(Ce,n,d),je=!0},p(n,d){const Fe={};d&2&&(Fe.$$scope={dirty:d,ctx:n}),B.$set(Fe)},i(n){je||(g(m.$$.fragment,n),g(w.$$.fragment,n),g(ie.$$.fragment,n),g(me.$$.fragment,n),g(W.$$.fragment,n),g(Me.$$.fragment,n),g(B.$$.fragment,n),g(q.$$.fragment,n),g(Y.$$.fragment,n),g(he.$$.fragment,n),g(Ce.$$.fragment,n),je=!0)},o(n){U(m.$$.fragment,n),U(w.$$.fragment,n),U(ie.$$.fragment,n),U(me.$$.fragment,n),U(W.$$.fragment,n),U(Me.$$.fragment,n),U(B.$$.fragment,n),U(q.$$.fragment,n),U(Y.$$.fragment,n),U(he.$$.fragment,n),U(Ce.$$.fragment,n),je=!1},d(n){n&&(t(i),t(j),t(b),t(re),t(N),t(H),t(R),t(S),t(K),t(Z),t(Q),t(z),t(G),t(we),t(X),t(A),t(L),t(I),t(ee),t(C),t(fe),t(te),t(k),t($e),t(E),t(se),t(V),t(de),t(ae),t(_),t(D),t(v),t(x),t(f),t(ne),t(ye)),$(m,n),$(w,n),$(ie,n),$(me,n),$(W,n),$(Me,n),$(B,n),$(q,n),$(Y,n),$(he,n),$(Ce,n)}}}function it(ge){let i,j;return i=new Qe({props:{$$slots:{default:[pt]},$$scope:{ctx:ge}}}),{c(){c(i.$$.fragment)},l(m){h(i.$$.fragment,m)},m(m,b){y(i,m,b),j=!0},p(m,b){const w={};b&2&&(w.$$scope={dirty:b,ctx:m}),i.$set(w)},i(m){j||(g(i.$$.fragment,m),j=!0)},o(m){U(i.$$.fragment,m),j=!1},d(m){$(i,m)}}}function mt(ge){let i,j='クラウドGPUが利用できない場合、<a href="https://colab.research.google.com/" rel="nofollow">Colaboratory</a>や<a href="https://studiolab.sagemaker.aws/" rel="nofollow">SageMaker StudioLab</a>などのホストされたノートブックを使用して無料でGPUにアクセスできます。';return{c(){i=u("p"),i.innerHTML=j},l(m){i=J(m,"P",{"data-svelte-h":!0}),T(i)!=="svelte-jp9818"&&(i.innerHTML=j)},m(m,b){a(m,i,b)},p:He,d(m){m&&t(i)}}}function ot(ge){let i,j,m,b=`<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a>はトレーニングループを処理し、1行のコードでモデルをファインチューニングできるようにします。
トレーニングループを独自に記述したいユーザーのために、🤗 TransformersモデルをネイティブのPyTorchでファインチューニングすることもできます。`,w,re,N="この時点で、ノートブックを再起動するか、以下のコードを実行してメモリを解放する必要があるかもしれません：",pe,H,ie,R,S="<li>モデルは生のテキストを入力として受け取らないため、<code>text</code> 列を削除します：</li>",Ue,K,Z,le,Q="<li><code>label</code>列を<code>labels</code>に名前を変更します。モデルは引数の名前を<code>labels</code>と期待しています：</li>",me,z,G,O,we="<li>データセットの形式をリストではなくPyTorchテンソルを返すように設定します：</li>",W,X,A,oe,L="以前に示したように、ファインチューニングを高速化するためにデータセットの小さなサブセットを作成します：",Me,I,B,ee,C,ue,fe="トレーニングデータセットとテストデータセット用の<code>DataLoader</code>を作成して、データのバッチをイテレートできるようにします：",q,te,k,Je,$e="ロードするモデルと期待されるラベルの数を指定してください：",E,P,se,V,Te,de,Y=`モデルをファインチューニングするためのオプティマイザと学習率スケジューラーを作成しましょう。
PyTorchから<a href="https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html" rel="nofollow"><code>AdamW</code></a>オプティマイザを使用します：`,ae,_,ce,D,he='デフォルトの学習率スケジューラを<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a>から作成する：',v,x,l,f,ne="最後に、GPUを利用できる場合は <code>device</code> を指定してください。それ以外の場合、CPUでのトレーニングは数時間かかる可能性があり、数分で完了することができます。",ke,ye,Ce,je,n,d,Fe="さて、トレーニングの準備が整いました！ 🥳",Ze,Ve,s,o,Ie='トレーニングの進捗を追跡するために、<a href="https://tqdm.github.io/" rel="nofollow">tqdm</a>ライブラリを使用してトレーニングステップの数に対して進行状況バーを追加します：',ve,Re,Ee,ze,xe,Ge,Ae=`<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a>に評価関数を追加したのと同様に、独自のトレーニングループを作成する際にも同様の操作を行う必要があります。
ただし、各エポックの最後にメトリックを計算および報告する代わりに、今回は<code>add_batch</code>を使用してすべてのバッチを蓄積し、最後にメトリックを計算します。`,Be,We,Ye;return i=new Ne({props:{id:"Dh9CL8fyG80"}}),H=new F({props:{code:"ZGVsJTIwbW9kZWwlMEFkZWwlMjB0cmFpbmVyJTBBdG9yY2guY3VkYS5lbXB0eV9jYWNoZSgp",highlighted:`<span class="hljs-keyword">del</span> model
<span class="hljs-keyword">del</span> trainer
torch.cuda.empty_cache()`,wrap:!1}}),K=new F({props:{code:"dG9rZW5pemVkX2RhdGFzZXRzJTIwJTNEJTIwdG9rZW5pemVkX2RhdGFzZXRzLnJlbW92ZV9jb2x1bW5zKCU1QiUyMnRleHQlMjIlNUQp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = tokenized_datasets.remove_columns([<span class="hljs-string">&quot;text&quot;</span>])',wrap:!1}}),z=new F({props:{code:"dG9rZW5pemVkX2RhdGFzZXRzJTIwJTNEJTIwdG9rZW5pemVkX2RhdGFzZXRzLnJlbmFtZV9jb2x1bW4oJTIybGFiZWwlMjIlMkMlMjAlMjJsYWJlbHMlMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = tokenized_datasets.rename_column(<span class="hljs-string">&quot;label&quot;</span>, <span class="hljs-string">&quot;labels&quot;</span>)',wrap:!1}}),X=new F({props:{code:"dG9rZW5pemVkX2RhdGFzZXRzLnNldF9mb3JtYXQoJTIydG9yY2glMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets.set_format(<span class="hljs-string">&quot;torch&quot;</span>)',wrap:!1}}),I=new F({props:{code:"c21hbGxfdHJhaW5fZGF0YXNldCUyMCUzRCUyMHRva2VuaXplZF9kYXRhc2V0cyU1QiUyMnRyYWluJTIyJTVELnNodWZmbGUoc2VlZCUzRDQyKS5zZWxlY3QocmFuZ2UoMTAwMCkpJTBBc21hbGxfZXZhbF9kYXRhc2V0JTIwJTNEJTIwdG9rZW5pemVkX2RhdGFzZXRzJTVCJTIydGVzdCUyMiU1RC5zaHVmZmxlKHNlZWQlM0Q0Mikuc2VsZWN0KHJhbmdlKDEwMDApKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>small_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>small_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))`,wrap:!1}}),ee=new be({props:{title:"DataLoader",local:"dataloader",headingTag:"h3"}}),te=new F({props:{code:"ZnJvbSUyMHRvcmNoLnV0aWxzLmRhdGElMjBpbXBvcnQlMjBEYXRhTG9hZGVyJTBBJTBBdHJhaW5fZGF0YWxvYWRlciUyMCUzRCUyMERhdGFMb2FkZXIoc21hbGxfdHJhaW5fZGF0YXNldCUyQyUyMHNodWZmbGUlM0RUcnVlJTJDJTIwYmF0Y2hfc2l6ZSUzRDgpJTBBZXZhbF9kYXRhbG9hZGVyJTIwJTNEJTIwRGF0YUxvYWRlcihzbWFsbF9ldmFsX2RhdGFzZXQlMkMlMjBiYXRjaF9zaXplJTNEOCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader

<span class="hljs-meta">&gt;&gt;&gt; </span>train_dataloader = DataLoader(small_train_dataset, shuffle=<span class="hljs-literal">True</span>, batch_size=<span class="hljs-number">8</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>eval_dataloader = DataLoader(small_eval_dataset, batch_size=<span class="hljs-number">8</span>)`,wrap:!1}}),P=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvclNlcXVlbmNlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZS1iZXJ0JTJGYmVydC1iYXNlLWNhc2VkJTIyJTJDJTIwbnVtX2xhYmVscyUzRDUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">5</span>)`,wrap:!1}}),V=new be({props:{title:"Optimizer and learning rate scheduler",local:"optimizer-and-learning-rate-scheduler",headingTag:"h3"}}),_=new F({props:{code:"ZnJvbSUyMHRvcmNoLm9wdGltJTIwaW1wb3J0JTIwQWRhbVclMEElMEFvcHRpbWl6ZXIlMjAlM0QlMjBBZGFtVyhtb2RlbC5wYXJhbWV0ZXJzKCklMkMlMjBsciUzRDVlLTUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span> AdamW

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamW(model.parameters(), lr=<span class="hljs-number">5e-5</span>)`,wrap:!1}}),x=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGdldF9zY2hlZHVsZXIlMEElMEFudW1fZXBvY2hzJTIwJTNEJTIwMyUwQW51bV90cmFpbmluZ19zdGVwcyUyMCUzRCUyMG51bV9lcG9jaHMlMjAqJTIwbGVuKHRyYWluX2RhdGFsb2FkZXIpJTBBbHJfc2NoZWR1bGVyJTIwJTNEJTIwZ2V0X3NjaGVkdWxlciglMEElMjAlMjAlMjAlMjBuYW1lJTNEJTIybGluZWFyJTIyJTJDJTIwb3B0aW1pemVyJTNEb3B0aW1pemVyJTJDJTIwbnVtX3dhcm11cF9zdGVwcyUzRDAlMkMlMjBudW1fdHJhaW5pbmdfc3RlcHMlM0RudW1fdHJhaW5pbmdfc3RlcHMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> get_scheduler

<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">3</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_training_steps = num_epochs * <span class="hljs-built_in">len</span>(train_dataloader)
<span class="hljs-meta">&gt;&gt;&gt; </span>lr_scheduler = get_scheduler(
<span class="hljs-meta">... </span>    name=<span class="hljs-string">&quot;linear&quot;</span>, optimizer=optimizer, num_warmup_steps=<span class="hljs-number">0</span>, num_training_steps=num_training_steps
<span class="hljs-meta">... </span>)`,wrap:!1}}),ye=new F({props:{code:"aW1wb3J0JTIwdG9yY2glMEElMEFkZXZpY2UlMjAlM0QlMjB0b3JjaC5kZXZpY2UoJTIyY3VkYSUyMiklMjBpZiUyMHRvcmNoLmN1ZGEuaXNfYXZhaWxhYmxlKCklMjBlbHNlJTIwdG9yY2guZGV2aWNlKCUyMmNwdSUyMiklMEFtb2RlbC50byhkZXZpY2Up",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>device = torch.device(<span class="hljs-string">&quot;cuda&quot;</span>) <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> torch.device(<span class="hljs-string">&quot;cpu&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.to(device)`,wrap:!1}}),je=new Se({props:{$$slots:{default:[mt]},$$scope:{ctx:ge}}}),Ve=new be({props:{title:"トレーニングループ",local:"トレーニングループ",headingTag:"h3"}}),Re=new F({props:{code:"ZnJvbSUyMHRxZG0uYXV0byUyMGltcG9ydCUyMHRxZG0lMEElMEFwcm9ncmVzc19iYXIlMjAlM0QlMjB0cWRtKHJhbmdlKG51bV90cmFpbmluZ19zdGVwcykpJTBBJTBBbW9kZWwudHJhaW4oKSUwQWZvciUyMGVwb2NoJTIwaW4lMjByYW5nZShudW1fZXBvY2hzKSUzQSUwQSUyMCUyMCUyMCUyMGZvciUyMGJhdGNoJTIwaW4lMjB0cmFpbl9kYXRhbG9hZGVyJTNBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYmF0Y2glMjAlM0QlMjAlN0JrJTNBJTIwdi50byhkZXZpY2UpJTIwZm9yJTIwayUyQyUyMHYlMjBpbiUyMGJhdGNoLml0ZW1zKCklN0QlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKipiYXRjaCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBsb3NzJTIwJTNEJTIwb3V0cHV0cy5sb3NzJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbG9zcy5iYWNrd2FyZCgpJTBBJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwb3B0aW1pemVyLnN0ZXAoKSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGxyX3NjaGVkdWxlci5zdGVwKCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvcHRpbWl6ZXIuemVyb19ncmFkKCklMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBwcm9ncmVzc19iYXIudXBkYXRlKDEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tqdm.auto <span class="hljs-keyword">import</span> tqdm

<span class="hljs-meta">&gt;&gt;&gt; </span>progress_bar = tqdm(<span class="hljs-built_in">range</span>(num_training_steps))

<span class="hljs-meta">&gt;&gt;&gt; </span>model.train()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> train_dataloader:
<span class="hljs-meta">... </span>        batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>        outputs = model(**batch)
<span class="hljs-meta">... </span>        loss = outputs.loss
<span class="hljs-meta">... </span>        loss.backward()

<span class="hljs-meta">... </span>        optimizer.step()
<span class="hljs-meta">... </span>        lr_scheduler.step()
<span class="hljs-meta">... </span>        optimizer.zero_grad()
<span class="hljs-meta">... </span>        progress_bar.update(<span class="hljs-number">1</span>)`,wrap:!1}}),ze=new be({props:{title:"Evaluate",local:"evaluate",headingTag:"h3"}}),We=new F({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFtZXRyaWMlMjAlM0QlMjBldmFsdWF0ZS5sb2FkKCUyMmFjY3VyYWN5JTIyKSUwQW1vZGVsLmV2YWwoKSUwQWZvciUyMGJhdGNoJTIwaW4lMjBldmFsX2RhdGFsb2FkZXIlM0ElMEElMjAlMjAlMjAlMjBiYXRjaCUyMCUzRCUyMCU3QmslM0ElMjB2LnRvKGRldmljZSklMjBmb3IlMjBrJTJDJTIwdiUyMGluJTIwYmF0Y2guaXRlbXMoKSU3RCUwQSUyMCUyMCUyMCUyMHdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKipiYXRjaCklMEElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cyUwQSUyMCUyMCUyMCUyMHByZWRpY3Rpb25zJTIwJTNEJTIwdG9yY2guYXJnbWF4KGxvZ2l0cyUyQyUyMGRpbSUzRC0xKSUwQSUyMCUyMCUyMCUyMG1ldHJpYy5hZGRfYmF0Y2gocHJlZGljdGlvbnMlM0RwcmVkaWN0aW9ucyUyQyUyMHJlZmVyZW5jZXMlM0RiYXRjaCU1QiUyMmxhYmVscyUyMiU1RCklMEElMEFtZXRyaWMuY29tcHV0ZSgp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">eval</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> eval_dataloader:
<span class="hljs-meta">... </span>    batch = {k: v.to(device) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> batch.items()}
<span class="hljs-meta">... </span>    <span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>        outputs = model(**batch)

<span class="hljs-meta">... </span>    logits = outputs.logits
<span class="hljs-meta">... </span>    predictions = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    metric.add_batch(predictions=predictions, references=batch[<span class="hljs-string">&quot;labels&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>metric.compute()`,wrap:!1}}),{c(){c(i.$$.fragment),j=r(),m=u("p"),m.innerHTML=b,w=r(),re=u("p"),re.textContent=N,pe=r(),c(H.$$.fragment),ie=r(),R=u("ol"),R.innerHTML=S,Ue=r(),c(K.$$.fragment),Z=r(),le=u("ol"),le.innerHTML=Q,me=r(),c(z.$$.fragment),G=r(),O=u("ol"),O.innerHTML=we,W=r(),c(X.$$.fragment),A=r(),oe=u("p"),oe.textContent=L,Me=r(),c(I.$$.fragment),B=r(),c(ee.$$.fragment),C=r(),ue=u("p"),ue.innerHTML=fe,q=r(),c(te.$$.fragment),k=r(),Je=u("p"),Je.textContent=$e,E=r(),c(P.$$.fragment),se=r(),c(V.$$.fragment),Te=r(),de=u("p"),de.innerHTML=Y,ae=r(),c(_.$$.fragment),ce=r(),D=u("p"),D.innerHTML=he,v=r(),c(x.$$.fragment),l=r(),f=u("p"),f.innerHTML=ne,ke=r(),c(ye.$$.fragment),Ce=r(),c(je.$$.fragment),n=r(),d=u("p"),d.textContent=Fe,Ze=r(),c(Ve.$$.fragment),s=r(),o=u("p"),o.innerHTML=Ie,ve=r(),c(Re.$$.fragment),Ee=r(),c(ze.$$.fragment),xe=r(),Ge=u("p"),Ge.innerHTML=Ae,Be=r(),c(We.$$.fragment),this.h()},l(e){h(i.$$.fragment,e),j=p(e),m=J(e,"P",{"data-svelte-h":!0}),T(m)!=="svelte-18twmdz"&&(m.innerHTML=b),w=p(e),re=J(e,"P",{"data-svelte-h":!0}),T(re)!=="svelte-hmvmwg"&&(re.textContent=N),pe=p(e),h(H.$$.fragment,e),ie=p(e),R=J(e,"OL",{"data-svelte-h":!0}),T(R)!=="svelte-29j9nb"&&(R.innerHTML=S),Ue=p(e),h(K.$$.fragment,e),Z=p(e),le=J(e,"OL",{start:!0,"data-svelte-h":!0}),T(le)!=="svelte-mosrna"&&(le.innerHTML=Q),me=p(e),h(z.$$.fragment,e),G=p(e),O=J(e,"OL",{start:!0,"data-svelte-h":!0}),T(O)!=="svelte-89dtkp"&&(O.innerHTML=we),W=p(e),h(X.$$.fragment,e),A=p(e),oe=J(e,"P",{"data-svelte-h":!0}),T(oe)!=="svelte-1dqgslj"&&(oe.textContent=L),Me=p(e),h(I.$$.fragment,e),B=p(e),h(ee.$$.fragment,e),C=p(e),ue=J(e,"P",{"data-svelte-h":!0}),T(ue)!=="svelte-1iwgiwj"&&(ue.innerHTML=fe),q=p(e),h(te.$$.fragment,e),k=p(e),Je=J(e,"P",{"data-svelte-h":!0}),T(Je)!=="svelte-5rgrad"&&(Je.textContent=$e),E=p(e),h(P.$$.fragment,e),se=p(e),h(V.$$.fragment,e),Te=p(e),de=J(e,"P",{"data-svelte-h":!0}),T(de)!=="svelte-1mj7jqu"&&(de.innerHTML=Y),ae=p(e),h(_.$$.fragment,e),ce=p(e),D=J(e,"P",{"data-svelte-h":!0}),T(D)!=="svelte-ds9kkv"&&(D.innerHTML=he),v=p(e),h(x.$$.fragment,e),l=p(e),f=J(e,"P",{"data-svelte-h":!0}),T(f)!=="svelte-1mxb03k"&&(f.innerHTML=ne),ke=p(e),h(ye.$$.fragment,e),Ce=p(e),h(je.$$.fragment,e),n=p(e),d=J(e,"P",{"data-svelte-h":!0}),T(d)!=="svelte-1sudcwb"&&(d.textContent=Fe),Ze=p(e),h(Ve.$$.fragment,e),s=p(e),o=J(e,"P",{"data-svelte-h":!0}),T(o)!=="svelte-gcp5no"&&(o.innerHTML=Ie),ve=p(e),h(Re.$$.fragment,e),Ee=p(e),h(ze.$$.fragment,e),xe=p(e),Ge=J(e,"P",{"data-svelte-h":!0}),T(Ge)!=="svelte-1bs93jh"&&(Ge.innerHTML=Ae),Be=p(e),h(We.$$.fragment,e),this.h()},h(){_e(le,"start","2"),_e(O,"start","3")},m(e,M){y(i,e,M),a(e,j,M),a(e,m,M),a(e,w,M),a(e,re,M),a(e,pe,M),y(H,e,M),a(e,ie,M),a(e,R,M),a(e,Ue,M),y(K,e,M),a(e,Z,M),a(e,le,M),a(e,me,M),y(z,e,M),a(e,G,M),a(e,O,M),a(e,W,M),y(X,e,M),a(e,A,M),a(e,oe,M),a(e,Me,M),y(I,e,M),a(e,B,M),y(ee,e,M),a(e,C,M),a(e,ue,M),a(e,q,M),y(te,e,M),a(e,k,M),a(e,Je,M),a(e,E,M),y(P,e,M),a(e,se,M),y(V,e,M),a(e,Te,M),a(e,de,M),a(e,ae,M),y(_,e,M),a(e,ce,M),a(e,D,M),a(e,v,M),y(x,e,M),a(e,l,M),a(e,f,M),a(e,ke,M),y(ye,e,M),a(e,Ce,M),y(je,e,M),a(e,n,M),a(e,d,M),a(e,Ze,M),y(Ve,e,M),a(e,s,M),a(e,o,M),a(e,ve,M),y(Re,e,M),a(e,Ee,M),y(ze,e,M),a(e,xe,M),a(e,Ge,M),a(e,Be,M),y(We,e,M),Ye=!0},p(e,M){const Le={};M&2&&(Le.$$scope={dirty:M,ctx:e}),je.$set(Le)},i(e){Ye||(g(i.$$.fragment,e),g(H.$$.fragment,e),g(K.$$.fragment,e),g(z.$$.fragment,e),g(X.$$.fragment,e),g(I.$$.fragment,e),g(ee.$$.fragment,e),g(te.$$.fragment,e),g(P.$$.fragment,e),g(V.$$.fragment,e),g(_.$$.fragment,e),g(x.$$.fragment,e),g(ye.$$.fragment,e),g(je.$$.fragment,e),g(Ve.$$.fragment,e),g(Re.$$.fragment,e),g(ze.$$.fragment,e),g(We.$$.fragment,e),Ye=!0)},o(e){U(i.$$.fragment,e),U(H.$$.fragment,e),U(K.$$.fragment,e),U(z.$$.fragment,e),U(X.$$.fragment,e),U(I.$$.fragment,e),U(ee.$$.fragment,e),U(te.$$.fragment,e),U(P.$$.fragment,e),U(V.$$.fragment,e),U(_.$$.fragment,e),U(x.$$.fragment,e),U(ye.$$.fragment,e),U(je.$$.fragment,e),U(Ve.$$.fragment,e),U(Re.$$.fragment,e),U(ze.$$.fragment,e),U(We.$$.fragment,e),Ye=!1},d(e){e&&(t(j),t(m),t(w),t(re),t(pe),t(ie),t(R),t(Ue),t(Z),t(le),t(me),t(G),t(O),t(W),t(A),t(oe),t(Me),t(B),t(C),t(ue),t(q),t(k),t(Je),t(E),t(se),t(Te),t(de),t(ae),t(ce),t(D),t(v),t(l),t(f),t(ke),t(Ce),t(n),t(d),t(Ze),t(s),t(o),t(ve),t(Ee),t(xe),t(Ge),t(Be)),$(i,e),$(H,e),$(K,e),$(z,e),$(X,e),$(I,e),$(ee,e),$(te,e),$(P,e),$(V,e),$(_,e),$(x,e),$(ye,e),$(je,e),$(Ve,e),$(Re,e),$(ze,e),$(We,e)}}}function Mt(ge){let i,j;return i=new Qe({props:{$$slots:{default:[ot]},$$scope:{ctx:ge}}}),{c(){c(i.$$.fragment)},l(m){h(i.$$.fragment,m)},m(m,b){y(i,m,b),j=!0},p(m,b){const w={};b&2&&(w.$$scope={dirty:b,ctx:m}),i.$set(w)},i(m){j||(g(i.$$.fragment,m),j=!0)},o(m){U(i.$$.fragment,m),j=!1},d(m){$(i,m)}}}function ft(ge){let i,j,m,b,w,re,N,pe,H,ie=`事前学習済みモデルを使用すると、計算コストを削減し、炭素排出量を減少させ、ゼロからモデルをトレーニングする必要なしに最新のモデルを使用できる利点があります。
🤗 Transformersは、さまざまなタスクに対応した数千もの事前学習済みモデルへのアクセスを提供します。
事前学習済みモデルを使用する場合、それを特定のタスクに合わせたデータセットでトレーニングします。これはファインチューニングとして知られ、非常に強力なトレーニング技術です。
このチュートリアルでは、事前学習済みモデルを選択したディープラーニングフレームワークでファインチューニングする方法について説明します：`,R,S,Ue='<li>🤗 Transformersの<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a>を使用して事前学習済みモデルをファインチューニングする。</li> <li>TensorFlowとKerasを使用して事前学習済みモデルをファインチューニングする。</li> <li>ネイティブのPyTorchを使用して事前学習済みモデルをファインチューニングする。</li>',K,Z,le,Q,me,z,G,O,we=`事前学習済みモデルをファインチューニングする前に、データセットをダウンロードしてトレーニング用に準備する必要があります。
前のチュートリアルでは、トレーニングデータの処理方法を説明しましたが、これからはそれらのスキルを活かす機会があります！`,W,X,A='まず、<a href="https://huggingface.co/datasets/yelp_review_full" rel="nofollow">Yelp Reviews</a>データセットを読み込んでみましょう：',oe,L,Me,I,B=`トークナイザがテキストを処理し、可変のシーケンス長を処理するためのパディングと切り捨て戦略を含める必要があることをご存知の通り、
データセットを1つのステップで処理するには、🤗 Datasets の <a href="https://huggingface.co/docs/datasets/process#map" rel="nofollow"><code>map</code></a> メソッドを使用して、
データセット全体に前処理関数を適用します：`,ee,C,ue,fe,q="お好みで、実行時間を短縮するためにフルデータセットの小さなサブセットを作成することができます：",te,k,Je,$e,E,P,se,V,Te=`この時点で、使用したいフレームワークに対応するセクションに従う必要があります。右側のサイドバーのリンクを使用して、ジャンプしたいフレームワークに移動できます。
そして、特定のフレームワークのすべてのコンテンツを非表示にしたい場合は、そのフレームワークのブロック右上にあるボタンを使用してください！`,de,Y,ae,_,ce,D,he,v,x,l,f,ne,ke,ye,Ce="さらなるファインチューニングの例については、以下を参照してください：",je,n,d='<li><p><a href="https://github.com/huggingface/transformers/tree/main/examples" rel="nofollow">🤗 Transformers Examples</a> には、PyTorchとTensorFlowで一般的なNLPタスクをトレーニングするスクリプトが含まれています。</p></li> <li><p><a href="notebooks">🤗 Transformers Notebooks</a> には、特定のタスクにモデルをファインチューニングする方法に関するさまざまなノートブックが含まれています。</p></li>',Fe,Ze,Ve;return w=new be({props:{title:"Fine-tune a pretrained model",local:"fine-tune-a-pretrained-model",headingTag:"h1"}}),N=new st({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/training.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/pytorch/training.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/tensorflow/training.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/training.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/pytorch/training.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/tensorflow/training.ipynb"}]}}),Q=new be({props:{title:"Prepare a dataset",local:"prepare-a-dataset",headingTag:"h2"}}),z=new Ne({props:{id:"_BZearw7f0w"}}),L=new F({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJ5ZWxwX3Jldmlld19mdWxsJTIyKSUwQWRhdGFzZXQlNUIlMjJ0cmFpbiUyMiU1RCU1QjEwMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;yelp_review_full&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">100</span>]
{<span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">0</span>,
 <span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\&#x27;s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\&#x27;s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\&quot;serving off their orders\\\\&quot; when they didn\\&#x27;t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\&#x27;t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\&#x27;ve eaten at various McDonalds restaurants for over 30 years. I\\&#x27;ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!&#x27;</span>}`,wrap:!1}}),C=new F({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUtYmVydCUyRmJlcnQtYmFzZS1jYXNlZCUyMiklMEElMEFkZWYlMjB0b2tlbml6ZV9mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjByZXR1cm4lMjB0b2tlbml6ZXIoZXhhbXBsZXMlNUIlMjJ0ZXh0JTIyJTVEJTJDJTIwcGFkZGluZyUzRCUyMm1heF9sZW5ndGglMjIlMkMlMjB0cnVuY2F0aW9uJTNEVHJ1ZSklMEElMEF0b2tlbml6ZWRfZGF0YXNldHMlMjAlM0QlMjBkYXRhc2V0Lm1hcCh0b2tlbml6ZV9mdW5jdGlvbiUyQyUyMGJhdGNoZWQlM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_datasets = dataset.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)`,wrap:!1}}),k=new F({props:{code:"c21hbGxfdHJhaW5fZGF0YXNldCUyMCUzRCUyMHRva2VuaXplZF9kYXRhc2V0cyU1QiUyMnRyYWluJTIyJTVELnNodWZmbGUoc2VlZCUzRDQyKS5zZWxlY3QocmFuZ2UoMTAwMCkpJTBBc21hbGxfZXZhbF9kYXRhc2V0JTIwJTNEJTIwdG9rZW5pemVkX2RhdGFzZXRzJTVCJTIydGVzdCUyMiU1RC5zaHVmZmxlKHNlZWQlM0Q0Mikuc2VsZWN0KHJhbmdlKDEwMDApKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>small_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>small_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))`,wrap:!1}}),P=new be({props:{title:"Train",local:"train",headingTag:"h2"}}),Y=new qe({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[it],pytorch:[nt]},$$scope:{ctx:ge}}}),D=new be({props:{title:"Train in native Pytorch",local:"train-in-native-pytorch",headingTag:"h2"}}),v=new qe({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Mt]},$$scope:{ctx:ge}}}),ne=new be({props:{title:"追加リソース",local:"追加リソース",headingTag:"h2"}}),{c(){i=u("meta"),j=r(),m=u("p"),b=r(),c(w.$$.fragment),re=r(),c(N.$$.fragment),pe=r(),H=u("p"),H.textContent=ie,R=r(),S=u("ul"),S.innerHTML=Ue,K=r(),Z=u("a"),le=r(),c(Q.$$.fragment),me=r(),c(z.$$.fragment),G=r(),O=u("p"),O.textContent=we,W=r(),X=u("p"),X.innerHTML=A,oe=r(),c(L.$$.fragment),Me=r(),I=u("p"),I.innerHTML=B,ee=r(),c(C.$$.fragment),ue=r(),fe=u("p"),fe.textContent=q,te=r(),c(k.$$.fragment),Je=r(),$e=u("a"),E=r(),c(P.$$.fragment),se=r(),V=u("p"),V.textContent=Te,de=r(),c(Y.$$.fragment),ae=r(),_=u("a"),ce=r(),c(D.$$.fragment),he=r(),c(v.$$.fragment),x=r(),l=u("a"),f=r(),c(ne.$$.fragment),ke=r(),ye=u("p"),ye.textContent=Ce,je=r(),n=u("ul"),n.innerHTML=d,Fe=r(),Ze=u("p"),this.h()},l(s){const o=et("svelte-u9bgzb",document.head);i=J(o,"META",{name:!0,content:!0}),o.forEach(t),j=p(s),m=J(s,"P",{}),Xe(m).forEach(t),b=p(s),h(w.$$.fragment,s),re=p(s),h(N.$$.fragment,s),pe=p(s),H=J(s,"P",{"data-svelte-h":!0}),T(H)!=="svelte-42180y"&&(H.textContent=ie),R=p(s),S=J(s,"UL",{"data-svelte-h":!0}),T(S)!=="svelte-18vtj58"&&(S.innerHTML=Ue),K=p(s),Z=J(s,"A",{id:!0}),Xe(Z).forEach(t),le=p(s),h(Q.$$.fragment,s),me=p(s),h(z.$$.fragment,s),G=p(s),O=J(s,"P",{"data-svelte-h":!0}),T(O)!=="svelte-2srgkq"&&(O.textContent=we),W=p(s),X=J(s,"P",{"data-svelte-h":!0}),T(X)!=="svelte-1jseek3"&&(X.innerHTML=A),oe=p(s),h(L.$$.fragment,s),Me=p(s),I=J(s,"P",{"data-svelte-h":!0}),T(I)!=="svelte-1kc14ha"&&(I.innerHTML=B),ee=p(s),h(C.$$.fragment,s),ue=p(s),fe=J(s,"P",{"data-svelte-h":!0}),T(fe)!=="svelte-1d60wrb"&&(fe.textContent=q),te=p(s),h(k.$$.fragment,s),Je=p(s),$e=J(s,"A",{id:!0}),Xe($e).forEach(t),E=p(s),h(P.$$.fragment,s),se=p(s),V=J(s,"P",{"data-svelte-h":!0}),T(V)!=="svelte-1apofsz"&&(V.textContent=Te),de=p(s),h(Y.$$.fragment,s),ae=p(s),_=J(s,"A",{id:!0}),Xe(_).forEach(t),ce=p(s),h(D.$$.fragment,s),he=p(s),h(v.$$.fragment,s),x=p(s),l=J(s,"A",{id:!0}),Xe(l).forEach(t),f=p(s),h(ne.$$.fragment,s),ke=p(s),ye=J(s,"P",{"data-svelte-h":!0}),T(ye)!=="svelte-d6wo5w"&&(ye.textContent=Ce),je=p(s),n=J(s,"UL",{"data-svelte-h":!0}),T(n)!=="svelte-zjwqqe"&&(n.innerHTML=d),Fe=p(s),Ze=J(s,"P",{}),Xe(Ze).forEach(t),this.h()},h(){_e(i,"name","hf:doc:metadata"),_e(i,"content",dt),_e(Z,"id","data-processing"),_e($e,"id","trainer"),_e(_,"id","pytorch_native"),_e(l,"id","additional-resources")},m(s,o){tt(document.head,i),a(s,j,o),a(s,m,o),a(s,b,o),y(w,s,o),a(s,re,o),y(N,s,o),a(s,pe,o),a(s,H,o),a(s,R,o),a(s,S,o),a(s,K,o),a(s,Z,o),a(s,le,o),y(Q,s,o),a(s,me,o),y(z,s,o),a(s,G,o),a(s,O,o),a(s,W,o),a(s,X,o),a(s,oe,o),y(L,s,o),a(s,Me,o),a(s,I,o),a(s,ee,o),y(C,s,o),a(s,ue,o),a(s,fe,o),a(s,te,o),y(k,s,o),a(s,Je,o),a(s,$e,o),a(s,E,o),y(P,s,o),a(s,se,o),a(s,V,o),a(s,de,o),y(Y,s,o),a(s,ae,o),a(s,_,o),a(s,ce,o),y(D,s,o),a(s,he,o),y(v,s,o),a(s,x,o),a(s,l,o),a(s,f,o),y(ne,s,o),a(s,ke,o),a(s,ye,o),a(s,je,o),a(s,n,o),a(s,Fe,o),a(s,Ze,o),Ve=!0},p(s,[o]){const Ie={};o&2&&(Ie.$$scope={dirty:o,ctx:s}),Y.$set(Ie);const ve={};o&2&&(ve.$$scope={dirty:o,ctx:s}),v.$set(ve)},i(s){Ve||(g(w.$$.fragment,s),g(N.$$.fragment,s),g(Q.$$.fragment,s),g(z.$$.fragment,s),g(L.$$.fragment,s),g(C.$$.fragment,s),g(k.$$.fragment,s),g(P.$$.fragment,s),g(Y.$$.fragment,s),g(D.$$.fragment,s),g(v.$$.fragment,s),g(ne.$$.fragment,s),Ve=!0)},o(s){U(w.$$.fragment,s),U(N.$$.fragment,s),U(Q.$$.fragment,s),U(z.$$.fragment,s),U(L.$$.fragment,s),U(C.$$.fragment,s),U(k.$$.fragment,s),U(P.$$.fragment,s),U(Y.$$.fragment,s),U(D.$$.fragment,s),U(v.$$.fragment,s),U(ne.$$.fragment,s),Ve=!1},d(s){s&&(t(j),t(m),t(b),t(re),t(pe),t(H),t(R),t(S),t(K),t(Z),t(le),t(me),t(G),t(O),t(W),t(X),t(oe),t(Me),t(I),t(ee),t(ue),t(fe),t(te),t(Je),t($e),t(E),t(se),t(V),t(de),t(ae),t(_),t(ce),t(he),t(x),t(l),t(f),t(ke),t(ye),t(je),t(n),t(Fe),t(Ze)),t(i),$(w,s),$(N,s),$(Q,s),$(z,s),$(L,s),$(C,s),$(k,s),$(P,s),$(Y,s),$(D,s),$(v,s),$(ne,s)}}}const dt='{"title":"Fine-tune a pretrained model","local":"fine-tune-a-pretrained-model","sections":[{"title":"Prepare a dataset","local":"prepare-a-dataset","sections":[],"depth":2},{"title":"Train","local":"train","sections":[],"depth":2},{"title":"Train with Pytorch Trainer","local":"train-with-pytorch-trainer","sections":[{"title":"Training Hyperparameters","local":"training-hyperparameters","sections":[],"depth":3},{"title":"Evaluate","local":"evaluate","sections":[],"depth":3},{"title":"Trainer","local":"trainer","sections":[],"depth":3}],"depth":2},{"title":"Kerasを使用してTensorFlowモデルをトレーニングする","local":"kerasを使用してtensorflowモデルをトレーニングする","sections":[{"title":"Loading Data from Keras","local":"loading-data-from-keras","sections":[],"depth":3},{"title":"Loading data as a tf.data.Dataset","local":"loading-data-as-a-tfdatadataset","sections":[],"depth":3}],"depth":2},{"title":"Train in native Pytorch","local":"train-in-native-pytorch","sections":[{"title":"DataLoader","local":"dataloader","sections":[],"depth":3},{"title":"Optimizer and learning rate scheduler","local":"optimizer-and-learning-rate-scheduler","sections":[],"depth":3},{"title":"トレーニングループ","local":"トレーニングループ","sections":[],"depth":3},{"title":"Evaluate","local":"evaluate","sections":[],"depth":3}],"depth":2},{"title":"追加リソース","local":"追加リソース","sections":[],"depth":2}],"depth":1}';function ct(ge){return De(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class jt extends Ke{constructor(i){super(),Oe(this,i,ct,ft,Pe,{})}}export{jt as component};
