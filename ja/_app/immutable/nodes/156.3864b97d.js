import{s as Ys,o as Qs,n as me}from"../chunks/scheduler.9bc65507.js";import{S as qs,i as As,g as j,s as o,r as M,A as Ls,h as g,f as l,c as m,j as Ns,u as y,x as T,k as Fs,y as Ps,a,v as d,d as u,t as h,w as $,m as Es,n as Ss}from"../chunks/index.707bf1b6.js";import{T as js}from"../chunks/Tip.c2ecdbf4.js";import{Y as zs}from"../chunks/Youtube.e1129c6f.js";import{C as R}from"../chunks/CodeBlock.54a9f38d.js";import{D as Ds}from"../chunks/DocNotebookDropdown.41f65cb5.js";import{F as gs,M as ze}from"../chunks/Markdown.8ab98a13.js";import{H as Fe}from"../chunks/Heading.342b1fa6.js";function Ks(k){let t,c,s='<a href="../model_doc/bart">BART</a>, <a href="../model_doc/bigbird_pegasus">BigBird-Pegasus</a>, <a href="../model_doc/blenderbot">Blenderbot</a>, <a href="../model_doc/blenderbot-small">BlenderbotSmall</a>, <a href="../model_doc/encoder-decoder">Encoder decoder</a>, <a href="../model_doc/fsmt">FairSeq Machine-Translation</a>, <a href="../model_doc/gptsan-japanese">GPTSAN-japanese</a>, <a href="../model_doc/led">LED</a>, <a href="../model_doc/longt5">LongT5</a>, <a href="../model_doc/m2m_100">M2M100</a>, <a href="../model_doc/marian">Marian</a>, <a href="../model_doc/mbart">mBART</a>, <a href="../model_doc/mt5">MT5</a>, <a href="../model_doc/mvp">MVP</a>, <a href="../model_doc/nllb">NLLB</a>, <a href="../model_doc/nllb-moe">NLLB-MOE</a>, <a href="../model_doc/pegasus">Pegasus</a>, <a href="../model_doc/pegasus_x">PEGASUS-X</a>, <a href="../model_doc/plbart">PLBart</a>, <a href="../model_doc/prophetnet">ProphetNet</a>, <a href="../model_doc/switch_transformers">SwitchTransformers</a>, <a href="../model_doc/t5">T5</a>, <a href="../model_doc/umt5">UMT5</a>, <a href="../model_doc/xlm-prophetnet">XLM-ProphetNet</a>';return{c(){t=Es(`このチュートリアルで説明するタスクは、次のモデル アーキテクチャでサポートされています。

`),c=j("p"),c.innerHTML=s},l(i){t=Ss(i,`このチュートリアルで説明するタスクは、次のモデル アーキテクチャでサポートされています。

`),c=g(i,"P",{"data-svelte-h":!0}),T(c)!=="svelte-2s8ce4"&&(c.innerHTML=s)},m(i,b){a(i,t,b),a(i,c,b)},p:me,d(i){i&&(l(t),l(c))}}}function Os(k){let t,c;return t=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvclNlcTJTZXElMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGF0YUNvbGxhdG9yRm9yU2VxMlNlcSh0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMjBtb2RlbCUzRGNoZWNrcG9pbnQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)`,wrap:!1}}),{c(){M(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){d(t,s,i),c=!0},p:me,i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function et(k){let t,c;return t=new ze({props:{$$slots:{default:[Os]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){d(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function st(k){let t,c;return t=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERhdGFDb2xsYXRvckZvclNlcTJTZXElMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGF0YUNvbGxhdG9yRm9yU2VxMlNlcSh0b2tlbml6ZXIlM0R0b2tlbml6ZXIlMkMlMjBtb2RlbCUzRGNoZWNrcG9pbnQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DataCollatorForSeq2Seq

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),{c(){M(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){d(t,s,i),c=!0},p:me,i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function tt(k){let t,c;return t=new ze({props:{$$slots:{default:[st]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){d(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function lt(k){let t,c='<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a> を使用したモデルの微調整に慣れていない場合は、<a href="../training#train-with-pytorch-trainer">ここ</a> の基本的なチュートリアルをご覧ください。';return{c(){t=j("p"),t.innerHTML=c},l(s){t=g(s,"P",{"data-svelte-h":!0}),T(t)!=="svelte-1ubngji"&&(t.innerHTML=c)},m(s,i){a(s,t,i)},p:me,d(s){s&&l(t)}}}function at(k){let t,c,s,i='これでモデルのトレーニングを開始する準備が整いました。 <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoModelForSeq2SeqLM">AutoModelForSeq2SeqLM</a> を使用して T5 をロードします。',b,Z,W,X,C="この時点で残っているステップは 3 つだけです。",B,_,I='<li><a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Seq2SeqTrainingArguments">Seq2SeqTrainingArguments</a> でトレーニング ハイパーパラメータを定義します。唯一の必須パラメータは、モデルの保存場所を指定する <code>output_dir</code> です。 <code>push_to_hub=True</code>を設定して、このモデルをハブにプッシュします (モデルをアップロードするには、Hugging Face にサインインする必要があります)。各エポックの終了時に、<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a> は SacreBLEU メトリクスを評価し、トレーニング チェックポイントを保存します。</li> <li>トレーニング引数をモデル、データセット、トークナイザー、データ照合器、および <code>compute_metrics</code> 関数とともに <a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Seq2SeqTrainer">Seq2SeqTrainer</a> に渡します。</li> <li><a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.train">train()</a> を呼び出してモデルを微調整します。</li>',G,U,V,r,J='トレーニングが完了したら、 <a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.push_to_hub">push_to_hub()</a> メソッドを使用してモデルをハブに共有し、誰もがモデルを使用できるようにします。',N,H,x;return t=new js({props:{$$slots:{default:[lt]},$$scope:{ctx:k}}}),Z=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUyQyUyMFNlcTJTZXFUcmFpbmluZ0FyZ3VtZW50cyUyQyUyMFNlcTJTZXFUcmFpbmVyJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JTZXEyU2VxTE0uZnJvbV9wcmV0cmFpbmVkKGNoZWNrcG9pbnQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)`,wrap:!1}}),U=new R({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFNlcTJTZXFUcmFpbmluZ0FyZ3VtZW50cyglMEElMjAlMjAlMjAlMjBvdXRwdXRfZGlyJTNEJTIybXlfYXdlc29tZV9vcHVzX2Jvb2tzX21vZGVsJTIyJTJDJTBBJTIwJTIwJTIwJTIwZXZhbHVhdGlvbl9zdHJhdGVneSUzRCUyMmVwb2NoJTIyJTJDJTBBJTIwJTIwJTIwJTIwbGVhcm5pbmdfcmF0ZSUzRDJlLTUlMkMlMEElMjAlMjAlMjAlMjBwZXJfZGV2aWNlX3RyYWluX2JhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjB3ZWlnaHRfZGVjYXklM0QwLjAxJTJDJTBBJTIwJTIwJTIwJTIwc2F2ZV90b3RhbF9saW1pdCUzRDMlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNEMiUyQyUwQSUyMCUyMCUyMCUyMHByZWRpY3Rfd2l0aF9nZW5lcmF0ZSUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBmcDE2JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMHB1c2hfdG9faHViJTNEVHJ1ZSUyQyUwQSklMEElMEF0cmFpbmVyJTIwJTNEJTIwU2VxMlNlcVRyYWluZXIoJTBBJTIwJTIwJTIwJTIwbW9kZWwlM0Rtb2RlbCUyQyUwQSUyMCUyMCUyMCUyMGFyZ3MlM0R0cmFpbmluZ19hcmdzJTJDJTBBJTIwJTIwJTIwJTIwdHJhaW5fZGF0YXNldCUzRHRva2VuaXplZF9ib29rcyU1QiUyMnRyYWluJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwZXZhbF9kYXRhc2V0JTNEdG9rZW5pemVkX2Jvb2tzJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRHRva2VuaXplciUyQyUwQSUyMCUyMCUyMCUyMGRhdGFfY29sbGF0b3IlM0RkYXRhX2NvbGxhdG9yJTJDJTBBJTIwJTIwJTIwJTIwY29tcHV0ZV9tZXRyaWNzJTNEY29tcHV0ZV9tZXRyaWNzJTJDJTBBKSUwQSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = Seq2SeqTrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">2e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    weight_decay=<span class="hljs-number">0.01</span>,
<span class="hljs-meta">... </span>    save_total_limit=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">2</span>,
<span class="hljs-meta">... </span>    predict_with_generate=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    fp16=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Seq2SeqTrainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=tokenized_books[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=tokenized_books[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),H=new R({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),{c(){M(t.$$.fragment),c=o(),s=j("p"),s.innerHTML=i,b=o(),M(Z.$$.fragment),W=o(),X=j("p"),X.textContent=C,B=o(),_=j("ol"),_.innerHTML=I,G=o(),M(U.$$.fragment),V=o(),r=j("p"),r.innerHTML=J,N=o(),M(H.$$.fragment)},l(f){y(t.$$.fragment,f),c=m(f),s=g(f,"P",{"data-svelte-h":!0}),T(s)!=="svelte-tkyheg"&&(s.innerHTML=i),b=m(f),y(Z.$$.fragment,f),W=m(f),X=g(f,"P",{"data-svelte-h":!0}),T(X)!=="svelte-5p19xw"&&(X.textContent=C),B=m(f),_=g(f,"OL",{"data-svelte-h":!0}),T(_)!=="svelte-1ppvf04"&&(_.innerHTML=I),G=m(f),y(U.$$.fragment,f),V=m(f),r=g(f,"P",{"data-svelte-h":!0}),T(r)!=="svelte-ngexm3"&&(r.innerHTML=J),N=m(f),y(H.$$.fragment,f)},m(f,v){d(t,f,v),a(f,c,v),a(f,s,v),a(f,b,v),d(Z,f,v),a(f,W,v),a(f,X,v),a(f,B,v),a(f,_,v),a(f,G,v),d(U,f,v),a(f,V,v),a(f,r,v),a(f,N,v),d(H,f,v),x=!0},p(f,v){const F={};v&2&&(F.$$scope={dirty:v,ctx:f}),t.$set(F)},i(f){x||(u(t.$$.fragment,f),u(Z.$$.fragment,f),u(U.$$.fragment,f),u(H.$$.fragment,f),x=!0)},o(f){h(t.$$.fragment,f),h(Z.$$.fragment,f),h(U.$$.fragment,f),h(H.$$.fragment,f),x=!1},d(f){f&&(l(c),l(s),l(b),l(W),l(X),l(B),l(_),l(G),l(V),l(r),l(N)),$(t,f),$(Z,f),$(U,f),$(H,f)}}}function nt(k){let t,c;return t=new ze({props:{$$slots:{default:[at]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){d(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function pt(k){let t,c='Keras を使用したモデルの微調整に慣れていない場合は、<a href="../training#train-a-tensorflow-model-with-keras">こちら</a> の基本的なチュートリアルをご覧ください。';return{c(){t=j("p"),t.innerHTML=c},l(s){t=g(s,"P",{"data-svelte-h":!0}),T(t)!=="svelte-1jwo7q8"&&(t.innerHTML=c)},m(s,i){a(s,t,i)},p:me,d(s){s&&l(t)}}}function rt(k){let t,c,s,i,b,Z='次に、<a href="/docs/transformers/main/ja/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM">TFAutoModelForSeq2SeqLM</a> を使用して T5 をロードできます。',W,X,C,B,_='<a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset">prepare_tf_dataset()</a> を使用して、データセットを <code>tf.data.Dataset</code> 形式に変換します。',I,G,U,V,r='<a href="https://keras.io/api/models/model_training_apis/#compile-method" rel="nofollow"><code>compile</code></a> を使用してトレーニング用のモデルを設定します。 Transformers モデルにはすべてデフォルトのタスク関連の損失関数があるため、次の場合を除き、損失関数を指定する必要はないことに注意してください。',J,N,H,x,f='トレーニングを開始する前にセットアップする最後の 2 つのことは、予測から SacreBLEU メトリクスを計算し、モデルをハブにプッシュする方法を提供することです。どちらも <a href="../main_classes/keras_callbacks">Keras コールバック</a> を使用して行われます。',v,F,ce='<code>compute_metrics</code> 関数を <a href="/docs/transformers/main/ja/main_classes/keras_callbacks#transformers.KerasMetricCallback">KerasMetricCallback</a> に渡します。',z,E,S,se,ie='<a href="/docs/transformers/main/ja/main_classes/keras_callbacks#transformers.PushToHubCallback">PushToHubCallback</a> でモデルとトークナイザーをプッシュする場所を指定します。',Y,Q,q,A,te="次に、コールバックをまとめてバンドルします。",fe,L,P,D,le='ついに、モデルのトレーニングを開始する準備が整いました。トレーニングおよび検証データセット、エポック数、コールバックを指定して <a href="https://keras.io/api/models/model_training_apis/#fit-method" rel="nofollow"><code>fit</code></a> を呼び出し、モデルを微調整します。',Me,K,O,ee,ae="トレーニングが完了すると、モデルは自動的にハブにアップロードされ、誰でも使用できるようになります。",ye;return t=new js({props:{$$slots:{default:[pt]},$$scope:{ctx:k}}}),s=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFkYW1XZWlnaHREZWNheSUwQSUwQW9wdGltaXplciUyMCUzRCUyMEFkYW1XZWlnaHREZWNheShsZWFybmluZ19yYXRlJTNEMmUtNSUyQyUyMHdlaWdodF9kZWNheV9yYXRlJTNEMC4wMSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AdamWeightDecay

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer = AdamWeightDecay(learning_rate=<span class="hljs-number">2e-5</span>, weight_decay_rate=<span class="hljs-number">0.01</span>)`,wrap:!1}}),X=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxMlNlcUxNJTBBJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)`,wrap:!1}}),G=new R({props:{code:"dGZfdHJhaW5fc2V0JTIwJTNEJTIwbW9kZWwucHJlcGFyZV90Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMHRva2VuaXplZF9ib29rcyU1QiUyMnRyYWluJTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwc2h1ZmZsZSUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBiYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBjb2xsYXRlX2ZuJTNEZGF0YV9jb2xsYXRvciUyQyUwQSklMEElMEF0Zl90ZXN0X3NldCUyMCUzRCUyMG1vZGVsLnByZXBhcmVfdGZfZGF0YXNldCglMEElMjAlMjAlMjAlMjB0b2tlbml6ZWRfYm9va3MlNUIlMjJ0ZXN0JTIyJTVEJTJDJTBBJTIwJTIwJTIwJTIwc2h1ZmZsZSUzREZhbHNlJTJDJTBBJTIwJTIwJTIwJTIwYmF0Y2hfc2l6ZSUzRDE2JTJDJTBBJTIwJTIwJTIwJTIwY29sbGF0ZV9mbiUzRGRhdGFfY29sbGF0b3IlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_set = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    tokenized_books[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>tf_test_set = model.prepare_tf_dataset(
<span class="hljs-meta">... </span>    tokenized_books[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    shuffle=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    collate_fn=data_collator,
<span class="hljs-meta">... </span>)`,wrap:!1}}),N=new R({props:{code:"aW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEElMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzRG9wdGltaXplciklMjAlMjAlMjMlMjBObyUyMGxvc3MlMjBhcmd1bWVudCE=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf

<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer)  <span class="hljs-comment"># No loss argument!</span>`,wrap:!1}}),E=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBLZXJhc01ldHJpY0NhbGxiYWNrJTBBJTBBbWV0cmljX2NhbGxiYWNrJTIwJTNEJTIwS2VyYXNNZXRyaWNDYWxsYmFjayhtZXRyaWNfZm4lM0Rjb21wdXRlX21ldHJpY3MlMkMlMjBldmFsX2RhdGFzZXQlM0R0Zl92YWxpZGF0aW9uX3NldCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> KerasMetricCallback

<span class="hljs-meta">&gt;&gt;&gt; </span>metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)`,wrap:!1}}),Q=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBQdXNoVG9IdWJDYWxsYmFjayUwQSUwQXB1c2hfdG9faHViX2NhbGxiYWNrJTIwJTNEJTIwUHVzaFRvSHViQ2FsbGJhY2soJTBBJTIwJTIwJTIwJTIwb3V0cHV0X2RpciUzRCUyMm15X2F3ZXNvbWVfb3B1c19ib29rc19tb2RlbCUyMiUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRHRva2VuaXplciUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> PushToHubCallback

<span class="hljs-meta">&gt;&gt;&gt; </span>push_to_hub_callback = PushToHubCallback(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>,
<span class="hljs-meta">... </span>    tokenizer=tokenizer,
<span class="hljs-meta">... </span>)`,wrap:!1}}),L=new R({props:{code:"Y2FsbGJhY2tzJTIwJTNEJTIwJTVCbWV0cmljX2NhbGxiYWNrJTJDJTIwcHVzaF90b19odWJfY2FsbGJhY2slNUQ=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>callbacks = [metric_callback, push_to_hub_callback]',wrap:!1}}),K=new R({props:{code:"bW9kZWwuZml0KHglM0R0Zl90cmFpbl9zZXQlMkMlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl90ZXN0X3NldCUyQyUyMGVwb2NocyUzRDMlMkMlMjBjYWxsYmFja3MlM0RjYWxsYmFja3Mp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=<span class="hljs-number">3</span>, callbacks=callbacks)',wrap:!1}}),{c(){M(t.$$.fragment),c=Es(`
TensorFlow でモデルを微調整するには、オプティマイザー関数、学習率スケジュール、およびいくつかのトレーニング ハイパーパラメーターをセットアップすることから始めます。

	`),M(s.$$.fragment),i=o(),b=j("p"),b.innerHTML=Z,W=o(),M(X.$$.fragment),C=o(),B=j("p"),B.innerHTML=_,I=o(),M(G.$$.fragment),U=o(),V=j("p"),V.innerHTML=r,J=o(),M(N.$$.fragment),H=o(),x=j("p"),x.innerHTML=f,v=o(),F=j("p"),F.innerHTML=ce,z=o(),M(E.$$.fragment),S=o(),se=j("p"),se.innerHTML=ie,Y=o(),M(Q.$$.fragment),q=o(),A=j("p"),A.textContent=te,fe=o(),M(L.$$.fragment),P=o(),D=j("p"),D.innerHTML=le,Me=o(),M(K.$$.fragment),O=o(),ee=j("p"),ee.textContent=ae},l(p){y(t.$$.fragment,p),c=Ss(p,`
TensorFlow でモデルを微調整するには、オプティマイザー関数、学習率スケジュール、およびいくつかのトレーニング ハイパーパラメーターをセットアップすることから始めます。

	`),y(s.$$.fragment,p),i=m(p),b=g(p,"P",{"data-svelte-h":!0}),T(b)!=="svelte-16zhwz"&&(b.innerHTML=Z),W=m(p),y(X.$$.fragment,p),C=m(p),B=g(p,"P",{"data-svelte-h":!0}),T(B)!=="svelte-1mdvspu"&&(B.innerHTML=_),I=m(p),y(G.$$.fragment,p),U=m(p),V=g(p,"P",{"data-svelte-h":!0}),T(V)!=="svelte-1pd5few"&&(V.innerHTML=r),J=m(p),y(N.$$.fragment,p),H=m(p),x=g(p,"P",{"data-svelte-h":!0}),T(x)!=="svelte-nilt2w"&&(x.innerHTML=f),v=m(p),F=g(p,"P",{"data-svelte-h":!0}),T(F)!=="svelte-ht78yi"&&(F.innerHTML=ce),z=m(p),y(E.$$.fragment,p),S=m(p),se=g(p,"P",{"data-svelte-h":!0}),T(se)!=="svelte-1rwfgpb"&&(se.innerHTML=ie),Y=m(p),y(Q.$$.fragment,p),q=m(p),A=g(p,"P",{"data-svelte-h":!0}),T(A)!=="svelte-r16oc5"&&(A.textContent=te),fe=m(p),y(L.$$.fragment,p),P=m(p),D=g(p,"P",{"data-svelte-h":!0}),T(D)!=="svelte-ffgub5"&&(D.innerHTML=le),Me=m(p),y(K.$$.fragment,p),O=m(p),ee=g(p,"P",{"data-svelte-h":!0}),T(ee)!=="svelte-vh7z0v"&&(ee.textContent=ae)},m(p,w){d(t,p,w),a(p,c,w),d(s,p,w),a(p,i,w),a(p,b,w),a(p,W,w),d(X,p,w),a(p,C,w),a(p,B,w),a(p,I,w),d(G,p,w),a(p,U,w),a(p,V,w),a(p,J,w),d(N,p,w),a(p,H,w),a(p,x,w),a(p,v,w),a(p,F,w),a(p,z,w),d(E,p,w),a(p,S,w),a(p,se,w),a(p,Y,w),d(Q,p,w),a(p,q,w),a(p,A,w),a(p,fe,w),d(L,p,w),a(p,P,w),a(p,D,w),a(p,Me,w),d(K,p,w),a(p,O,w),a(p,ee,w),ye=!0},p(p,w){const de={};w&2&&(de.$$scope={dirty:w,ctx:p}),t.$set(de)},i(p){ye||(u(t.$$.fragment,p),u(s.$$.fragment,p),u(X.$$.fragment,p),u(G.$$.fragment,p),u(N.$$.fragment,p),u(E.$$.fragment,p),u(Q.$$.fragment,p),u(L.$$.fragment,p),u(K.$$.fragment,p),ye=!0)},o(p){h(t.$$.fragment,p),h(s.$$.fragment,p),h(X.$$.fragment,p),h(G.$$.fragment,p),h(N.$$.fragment,p),h(E.$$.fragment,p),h(Q.$$.fragment,p),h(L.$$.fragment,p),h(K.$$.fragment,p),ye=!1},d(p){p&&(l(c),l(i),l(b),l(W),l(C),l(B),l(I),l(U),l(V),l(J),l(H),l(x),l(v),l(F),l(z),l(S),l(se),l(Y),l(q),l(A),l(fe),l(P),l(D),l(Me),l(O),l(ee)),$(t,p),$(s,p),$(X,p),$(G,p),$(N,p),$(E,p),$(Q,p),$(L,p),$(K,p)}}}function ot(k){let t,c;return t=new ze({props:{$$slots:{default:[rt]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){d(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function mt(k){let t,c=`翻訳用にモデルを微調整する方法の詳細な例については、対応するドキュメントを参照してください。
<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb" rel="nofollow">PyTorch ノートブック</a>
または <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb" rel="nofollow">TensorFlow ノートブック</a>。`;return{c(){t=j("p"),t.innerHTML=c},l(s){t=g(s,"P",{"data-svelte-h":!0}),T(t)!=="svelte-64mea5"&&(t.innerHTML=c)},m(s,i){a(s,t,i)},p:me,d(s){s&&l(t)}}}function ct(k){let t,c="テキストをトークン化し、<code>input_ids</code> を PyTorch テンソルとして返します。",s,i,b,Z,W='<a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.GenerationMixin.generate">generate()</a> メソッドを使用して翻訳を作成します。さまざまなテキスト生成戦略と生成を制御するためのパラメーターの詳細については、<a href="../main_classes/text_generation">Text Generation</a> API を確認してください。',X,C,B,_,I="生成されたトークン ID をデコードしてテキストに戻します。",G,U,V;return i=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJteV9hd2Vzb21lX29wdXNfYm9va3NfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5pbnB1dF9pZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).input_ids`,wrap:!1}}),C=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvclNlcTJTZXFMTSUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yU2VxMlNlcUxNLmZyb21fcHJldHJhaW5lZCglMjJteV9hd2Vzb21lX29wdXNfYm9va3NfbW9kZWwlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKGlucHV0cyUyQyUyMG1heF9uZXdfdG9rZW5zJTNENDAlMkMlMjBkb19zYW1wbGUlM0RUcnVlJTJDJTIwdG9wX2slM0QzMCUyQyUyMHRvcF9wJTNEMC45NSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(inputs, max_new_tokens=<span class="hljs-number">40</span>, do_sample=<span class="hljs-literal">True</span>, top_k=<span class="hljs-number">30</span>, top_p=<span class="hljs-number">0.95</span>)`,wrap:!1}}),U=new R({props:{code:"dG9rZW5pemVyLmRlY29kZShvdXRwdXRzJTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Les lignées partagent des ressources avec des bactéries enfixant l&#x27;</span>azote.<span class="hljs-string">&#x27;</span>`,wrap:!1}}),{c(){t=j("p"),t.innerHTML=c,s=o(),M(i.$$.fragment),b=o(),Z=j("p"),Z.innerHTML=W,X=o(),M(C.$$.fragment),B=o(),_=j("p"),_.textContent=I,G=o(),M(U.$$.fragment)},l(r){t=g(r,"P",{"data-svelte-h":!0}),T(t)!=="svelte-qm50cf"&&(t.innerHTML=c),s=m(r),y(i.$$.fragment,r),b=m(r),Z=g(r,"P",{"data-svelte-h":!0}),T(Z)!=="svelte-1kdcpie"&&(Z.innerHTML=W),X=m(r),y(C.$$.fragment,r),B=m(r),_=g(r,"P",{"data-svelte-h":!0}),T(_)!=="svelte-izrqac"&&(_.textContent=I),G=m(r),y(U.$$.fragment,r)},m(r,J){a(r,t,J),a(r,s,J),d(i,r,J),a(r,b,J),a(r,Z,J),a(r,X,J),d(C,r,J),a(r,B,J),a(r,_,J),a(r,G,J),d(U,r,J),V=!0},p:me,i(r){V||(u(i.$$.fragment,r),u(C.$$.fragment,r),u(U.$$.fragment,r),V=!0)},o(r){h(i.$$.fragment,r),h(C.$$.fragment,r),h(U.$$.fragment,r),V=!1},d(r){r&&(l(t),l(s),l(b),l(Z),l(X),l(B),l(_),l(G)),$(i,r),$(C,r),$(U,r)}}}function it(k){let t,c;return t=new ze({props:{$$slots:{default:[ct]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){d(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function ft(k){let t,c="<code>input_ids</code>を TensorFlow テンソルとして返します。 tensors:",s,i,b,Z,W='<a href="/docs/transformers/main/ja/main_classes/text_generation#transformers.TFGenerationMixin.generate">generate()</a> メソッドを使用して翻訳を作成します。さまざまなテキスト生成戦略と生成を制御するためのパラメーターの詳細については、<a href="../main_classes/text_generation">Text Generation</a> API を確認してください。',X,C,B,_,I="生成されたトークン ID をデコードしてテキストに戻します。",G,U,V;return i=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJteV9hd2Vzb21lX29wdXNfYm9va3NfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHRleHQlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKS5pbnB1dF9pZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer(text, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).input_ids`,wrap:!1}}),C=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9yU2VxMlNlcUxNJTBBJTBBbW9kZWwlMjAlM0QlMjBURkF1dG9Nb2RlbEZvclNlcTJTZXFMTS5mcm9tX3ByZXRyYWluZWQoJTIybXlfYXdlc29tZV9vcHVzX2Jvb2tzX21vZGVsJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbC5nZW5lcmF0ZShpbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDQwJTJDJTIwZG9fc2FtcGxlJTNEVHJ1ZSUyQyUyMHRvcF9rJTNEMzAlMkMlMjB0b3BfcCUzRDAuOTUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForSeq2SeqLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForSeq2SeqLM.from_pretrained(<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model.generate(inputs, max_new_tokens=<span class="hljs-number">40</span>, do_sample=<span class="hljs-literal">True</span>, top_k=<span class="hljs-number">30</span>, top_p=<span class="hljs-number">0.95</span>)`,wrap:!1}}),U=new R({props:{code:"dG9rZW5pemVyLmRlY29kZShvdXRwdXRzJTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>)
<span class="hljs-string">&#x27;Les lugumes partagent les ressources avec des bactéries fixatrices d&#x27;</span>azote.<span class="hljs-string">&#x27;</span>`,wrap:!1}}),{c(){t=j("p"),t.innerHTML=c,s=o(),M(i.$$.fragment),b=o(),Z=j("p"),Z.innerHTML=W,X=o(),M(C.$$.fragment),B=o(),_=j("p"),_.textContent=I,G=o(),M(U.$$.fragment)},l(r){t=g(r,"P",{"data-svelte-h":!0}),T(t)!=="svelte-258v67"&&(t.innerHTML=c),s=m(r),y(i.$$.fragment,r),b=m(r),Z=g(r,"P",{"data-svelte-h":!0}),T(Z)!=="svelte-1909c9g"&&(Z.innerHTML=W),X=m(r),y(C.$$.fragment,r),B=m(r),_=g(r,"P",{"data-svelte-h":!0}),T(_)!=="svelte-izrqac"&&(_.textContent=I),G=m(r),y(U.$$.fragment,r)},m(r,J){a(r,t,J),a(r,s,J),d(i,r,J),a(r,b,J),a(r,Z,J),a(r,X,J),d(C,r,J),a(r,B,J),a(r,_,J),a(r,G,J),d(U,r,J),V=!0},p:me,i(r){V||(u(i.$$.fragment,r),u(C.$$.fragment,r),u(U.$$.fragment,r),V=!0)},o(r){h(i.$$.fragment,r),h(C.$$.fragment,r),h(U.$$.fragment,r),V=!1},d(r){r&&(l(t),l(s),l(b),l(Z),l(X),l(B),l(_),l(G)),$(i,r),$(C,r),$(U,r)}}}function Mt(k){let t,c;return t=new ze({props:{$$slots:{default:[ft]},$$scope:{ctx:k}}}),{c(){M(t.$$.fragment)},l(s){y(t.$$.fragment,s)},m(s,i){d(t,s,i),c=!0},p(s,i){const b={};i&2&&(b.$$scope={dirty:i,ctx:s}),t.$set(b)},i(s){c||(u(t.$$.fragment,s),c=!0)},o(s){h(t.$$.fragment,s),c=!1},d(s){$(t,s)}}}function yt(k){let t,c,s,i,b,Z,W,X,C,B,_,I="翻訳では、一連のテキストをある言語から別の言語に変換します。これは、シーケンス間問題として定式化できるいくつかのタスクの 1 つであり、翻訳や要約など、入力から何らかの出力を返すための強力なフレームワークです。翻訳システムは通常、異なる言語のテキスト間の翻訳に使用されますが、音声、またはテキストから音声への変換や音声からテキストへの変換など、音声間の組み合わせにも使用できます。",G,U,V="このガイドでは、次の方法を説明します。",r,J,N='<li><a href="https://huggingface.co/datasets/opus_books" rel="nofollow">OPUS Books</a> データセットの英語-フランス語サブセットの <a href="https://huggingface.co/google-t5/t5-small" rel="nofollow">T5</a> を微調整して、英語のテキストを次の形式に翻訳します。フランス語。</li> <li>微調整されたモデルを推論に使用します。</li>',H,x,f,v,F="始める前に、必要なライブラリがすべてインストールされていることを確認してください。",ce,z,E,S,se="モデルをアップロードしてコミュニティと共有できるように、Hugging Face アカウントにログインすることをお勧めします。プロンプトが表示されたら、トークンを入力してログインします。",ie,Y,Q,q,A,te,fe='まず、🤗 データセット ライブラリから <a href="https://huggingface.co/datasets/opus_books" rel="nofollow">OPUS Books</a> データセットの英語とフランス語のサブセットを読み込みます。',L,P,D,le,Me="<code>train_test_split</code> メソッドを使用して、データセットをトレイン セットとテスト セットに分割します。",K,O,ee,ae,ye="次に、例を見てみましょう。",p,w,de,ue,ws="<code>translation</code>: テキストの英語とフランス語の翻訳。",Se,he,Ye,$e,Qe,be,Ts="次のステップでは、T5 トークナイザーをロードして英語とフランス語の言語ペアを処理します。",qe,je,Ae,ge,Js="作成する前処理関数は次のことを行う必要があります。",Le,we,Us="<li>T5 がこれが翻訳タスクであることを認識できるように、入力の前にプロンプ​​トを付けます。複数の NLP タスクが可能な一部のモデルでは、特定のタスクのプロンプトが必要です。</li> <li>英語の語彙で事前トレーニングされたトークナイザーを使用してフランス語のテキストをトークン化することはできないため、入力 (英語) とターゲット (フランス語) を別々にトークン化します。</li> <li><code>max_length</code>パラメータで設定された最大長を超えないようにシーケンスを切り詰めます。</li>",Pe,Te,De,Je,_s="データセット全体に前処理関数を適用するには、🤗 Datasets <code>map</code> メソッドを使用します。 <code>batched=True</code> を設定してデータセットの複数の要素を一度に処理することで、<code>map</code> 関数を高速化できます。",Ke,Ue,Oe,_e,ks="次に、<code>DataCollat​​orForSeq2Seq</code> を使用してサンプルのバッチを作成します。データセット全体を最大長までパディングするのではなく、照合中にバッチ内の最長の長さまで文を <em>動的にパディング</em> する方が効率的です。",es,ne,ss,ke,ts,Ze,Zs='トレーニング中にメトリクスを含めると、多くの場合、モデルのパフォーマンスを評価するのに役立ちます。 🤗 <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> ライブラリを使用して、評価メソッドをすばやくロードできます。このタスクでは、<a href="https://huggingface.co/spaces/evaluate-metric/sacrebleu" rel="nofollow">SacreBLEU</a> メトリクスをロードします (🤗 Evaluate <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">クイック ツアー</a> を参照してください) ) メトリクスの読み込みと計算方法の詳細については、次を参照してください)。',ls,Ce,as,ve,Cs="次に、予測とラベルを <code>compute</code> に渡して SacreBLEU スコアを計算する関数を作成します。",ns,Re,ps,Xe,vs="これで<code>compute_metrics</code>関数の準備が整いました。トレーニングをセットアップするときにこの関数に戻ります。",rs,Be,os,pe,ms,re,cs,Ge,is,Ve,Rs="モデルを微調整したので、それを推論に使用できるようになりました。",fs,We,Xs="別の言語に翻訳したいテキストを考え出します。 T5 の場合、作業中のタスクに応じて入力に接頭辞を付ける必要があります。英語からフランス語に翻訳する場合は、以下に示すように入力に接頭辞を付ける必要があります。",Ms,xe,ys,He,Bs='推論用に微調整されたモデルを試す最も簡単な方法は、それを <a href="/docs/transformers/main/ja/main_classes/pipelines#transformers.pipeline">pipeline()</a> で使用することです。モデルを使用して翻訳用の<code>pipeline</code>をインスタンス化し、テキストをそれに渡します。',ds,Ie,us,Ne,Gs="必要に応じて、<code>pipeline</code>の結果を手動で複製することもできます。",hs,oe,$s,Ee,bs;return b=new Fe({props:{title:"Translation",local:"translation",headingTag:"h1"}}),W=new Ds({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/translation.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/pytorch/translation.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/tensorflow/translation.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/translation.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/pytorch/translation.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/tensorflow/translation.ipynb"}]}}),C=new zs({props:{id:"1JvfrvZgi6c"}}),x=new js({props:{$$slots:{default:[Ks]},$$scope:{ctx:k}}}),z=new R({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGUlMjBzYWNyZWJsZXU=",highlighted:"pip install transformers datasets evaluate sacrebleu",wrap:!1}}),Y=new R({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),q=new Fe({props:{title:"Load OPUS Books dataset",local:"load-opus-books-dataset",headingTag:"h2"}}),P=new R({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBYm9va3MlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyb3B1c19ib29rcyUyMiUyQyUyMCUyMmVuLWZyJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>books = load_dataset(<span class="hljs-string">&quot;opus_books&quot;</span>, <span class="hljs-string">&quot;en-fr&quot;</span>)`,wrap:!1}}),O=new R({props:{code:"Ym9va3MlMjAlM0QlMjBib29rcyU1QiUyMnRyYWluJTIyJTVELnRyYWluX3Rlc3Rfc3BsaXQodGVzdF9zaXplJTNEMC4yKQ==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>books = books[<span class="hljs-string">&quot;train&quot;</span>].train_test_split(test_size=<span class="hljs-number">0.2</span>)',wrap:!1}}),w=new R({props:{code:"Ym9va3MlNUIlMjJ0cmFpbiUyMiU1RCU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>books[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;id&#x27;</span>: <span class="hljs-string">&#x27;90560&#x27;</span>,
 <span class="hljs-string">&#x27;translation&#x27;</span>: {<span class="hljs-string">&#x27;en&#x27;</span>: <span class="hljs-string">&#x27;But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.&#x27;</span>,
  <span class="hljs-string">&#x27;fr&#x27;</span>: <span class="hljs-string">&#x27;Mais ce plateau élevé ne mesurait que quelques toises, et bientôt nous fûmes rentrés dans notre élément.&#x27;</span>}}`,wrap:!1}}),he=new Fe({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),$e=new zs({props:{id:"XAR8jnZZuUs"}}),je=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMEElMEFjaGVja3BvaW50JTIwJTNEJTIwJTIyZ29vZ2xlLXQ1JTJGdDUtc21hbGwlMjIlMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChjaGVja3BvaW50KQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;google-t5/t5-small&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(checkpoint)`,wrap:!1}}),Te=new R({props:{code:"c291cmNlX2xhbmclMjAlM0QlMjAlMjJlbiUyMiUwQXRhcmdldF9sYW5nJTIwJTNEJTIwJTIyZnIlMjIlMEFwcmVmaXglMjAlM0QlMjAlMjJ0cmFuc2xhdGUlMjBFbmdsaXNoJTIwdG8lMjBGcmVuY2glM0ElMjAlMjIlMEElMEElMEFkZWYlMjBwcmVwcm9jZXNzX2Z1bmN0aW9uKGV4YW1wbGVzKSUzQSUwQSUyMCUyMCUyMCUyMGlucHV0cyUyMCUzRCUyMCU1QnByZWZpeCUyMCUyQiUyMGV4YW1wbGUlNUJzb3VyY2VfbGFuZyU1RCUyMGZvciUyMGV4YW1wbGUlMjBpbiUyMGV4YW1wbGVzJTVCJTIydHJhbnNsYXRpb24lMjIlNUQlNUQlMEElMjAlMjAlMjAlMjB0YXJnZXRzJTIwJTNEJTIwJTVCZXhhbXBsZSU1QnRhcmdldF9sYW5nJTVEJTIwZm9yJTIwZXhhbXBsZSUyMGluJTIwZXhhbXBsZXMlNUIlMjJ0cmFuc2xhdGlvbiUyMiU1RCU1RCUwQSUyMCUyMCUyMCUyMG1vZGVsX2lucHV0cyUyMCUzRCUyMHRva2VuaXplcihpbnB1dHMlMkMlMjB0ZXh0X3RhcmdldCUzRHRhcmdldHMlMkMlMjBtYXhfbGVuZ3RoJTNEMTI4JTJDJTIwdHJ1bmNhdGlvbiUzRFRydWUpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwbW9kZWxfaW5wdXRz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>source_lang = <span class="hljs-string">&quot;en&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_lang = <span class="hljs-string">&quot;fr&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prefix = <span class="hljs-string">&quot;translate English to French: &quot;</span>


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    inputs = [prefix + example[source_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    targets = [example[target_lang] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;translation&quot;</span>]]
<span class="hljs-meta">... </span>    model_inputs = tokenizer(inputs, text_target=targets, max_length=<span class="hljs-number">128</span>, truncation=<span class="hljs-literal">True</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> model_inputs`,wrap:!1}}),Ue=new R({props:{code:"dG9rZW5pemVkX2Jvb2tzJTIwJTNEJTIwYm9va3MubWFwKHByZXByb2Nlc3NfZnVuY3Rpb24lMkMlMjBiYXRjaGVkJTNEVHJ1ZSk=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenized_books = books.<span class="hljs-built_in">map</span>(preprocess_function, batched=<span class="hljs-literal">True</span>)',wrap:!1}}),ne=new gs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[tt],pytorch:[et]},$$scope:{ctx:k}}}),ke=new Fe({props:{title:"Evaluate",local:"evaluate",headingTag:"h2"}}),Ce=new R({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFtZXRyaWMlMjAlM0QlMjBldmFsdWF0ZS5sb2FkKCUyMnNhY3JlYmxldSUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>metric = evaluate.load(<span class="hljs-string">&quot;sacrebleu&quot;</span>)`,wrap:!1}}),Re=new R({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwcG9zdHByb2Nlc3NfdGV4dChwcmVkcyUyQyUyMGxhYmVscyklM0ElMEElMjAlMjAlMjAlMjBwcmVkcyUyMCUzRCUyMCU1QnByZWQuc3RyaXAoKSUyMGZvciUyMHByZWQlMjBpbiUyMHByZWRzJTVEJTBBJTIwJTIwJTIwJTIwbGFiZWxzJTIwJTNEJTIwJTVCJTVCbGFiZWwuc3RyaXAoKSU1RCUyMGZvciUyMGxhYmVsJTIwaW4lMjBsYWJlbHMlNUQlMEElMEElMjAlMjAlMjAlMjByZXR1cm4lMjBwcmVkcyUyQyUyMGxhYmVscyUwQSUwQSUwQWRlZiUyMGNvbXB1dGVfbWV0cmljcyhldmFsX3ByZWRzKSUzQSUwQSUyMCUyMCUyMCUyMHByZWRzJTJDJTIwbGFiZWxzJTIwJTNEJTIwZXZhbF9wcmVkcyUwQSUyMCUyMCUyMCUyMGlmJTIwaXNpbnN0YW5jZShwcmVkcyUyQyUyMHR1cGxlKSUzQSUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMHByZWRzJTIwJTNEJTIwcHJlZHMlNUIwJTVEJTBBJTIwJTIwJTIwJTIwZGVjb2RlZF9wcmVkcyUyMCUzRCUyMHRva2VuaXplci5iYXRjaF9kZWNvZGUocHJlZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSklMEElMEElMjAlMjAlMjAlMjBsYWJlbHMlMjAlM0QlMjBucC53aGVyZShsYWJlbHMlMjAhJTNEJTIwLTEwMCUyQyUyMGxhYmVscyUyQyUyMHRva2VuaXplci5wYWRfdG9rZW5faWQpJTBBJTIwJTIwJTIwJTIwZGVjb2RlZF9sYWJlbHMlMjAlM0QlMjB0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGxhYmVscyUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSUwQSUwQSUyMCUyMCUyMCUyMGRlY29kZWRfcHJlZHMlMkMlMjBkZWNvZGVkX2xhYmVscyUyMCUzRCUyMHBvc3Rwcm9jZXNzX3RleHQoZGVjb2RlZF9wcmVkcyUyQyUyMGRlY29kZWRfbGFiZWxzKSUwQSUwQSUyMCUyMCUyMCUyMHJlc3VsdCUyMCUzRCUyMG1ldHJpYy5jb21wdXRlKHByZWRpY3Rpb25zJTNEZGVjb2RlZF9wcmVkcyUyQyUyMHJlZmVyZW5jZXMlM0RkZWNvZGVkX2xhYmVscyklMEElMjAlMjAlMjAlMjByZXN1bHQlMjAlM0QlMjAlN0IlMjJibGV1JTIyJTNBJTIwcmVzdWx0JTVCJTIyc2NvcmUlMjIlNUQlN0QlMEElMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9uX2xlbnMlMjAlM0QlMjAlNUJucC5jb3VudF9ub256ZXJvKHByZWQlMjAhJTNEJTIwdG9rZW5pemVyLnBhZF90b2tlbl9pZCklMjBmb3IlMjBwcmVkJTIwaW4lMjBwcmVkcyU1RCUwQSUyMCUyMCUyMCUyMHJlc3VsdCU1QiUyMmdlbl9sZW4lMjIlNUQlMjAlM0QlMjBucC5tZWFuKHByZWRpY3Rpb25fbGVucyklMEElMjAlMjAlMjAlMjByZXN1bHQlMjAlM0QlMjAlN0JrJTNBJTIwcm91bmQodiUyQyUyMDQpJTIwZm9yJTIwayUyQyUyMHYlMjBpbiUyMHJlc3VsdC5pdGVtcygpJTdEJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwcmVzdWx0",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">postprocess_text</span>(<span class="hljs-params">preds, labels</span>):
<span class="hljs-meta">... </span>    preds = [pred.strip() <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
<span class="hljs-meta">... </span>    labels = [[label.strip()] <span class="hljs-keyword">for</span> label <span class="hljs-keyword">in</span> labels]

<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> preds, labels


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_preds</span>):
<span class="hljs-meta">... </span>    preds, labels = eval_preds
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(preds, <span class="hljs-built_in">tuple</span>):
<span class="hljs-meta">... </span>        preds = preds[<span class="hljs-number">0</span>]
<span class="hljs-meta">... </span>    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    labels = np.where(labels != -<span class="hljs-number">100</span>, labels, tokenizer.pad_token_id)
<span class="hljs-meta">... </span>    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=<span class="hljs-literal">True</span>)

<span class="hljs-meta">... </span>    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

<span class="hljs-meta">... </span>    result = metric.compute(predictions=decoded_preds, references=decoded_labels)
<span class="hljs-meta">... </span>    result = {<span class="hljs-string">&quot;bleu&quot;</span>: result[<span class="hljs-string">&quot;score&quot;</span>]}

<span class="hljs-meta">... </span>    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds]
<span class="hljs-meta">... </span>    result[<span class="hljs-string">&quot;gen_len&quot;</span>] = np.mean(prediction_lens)
<span class="hljs-meta">... </span>    result = {k: <span class="hljs-built_in">round</span>(v, <span class="hljs-number">4</span>) <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> result.items()}
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> result`,wrap:!1}}),Be=new Fe({props:{title:"Train",local:"train",headingTag:"h2"}}),pe=new gs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[ot],pytorch:[nt]},$$scope:{ctx:k}}}),re=new js({props:{$$slots:{default:[mt]},$$scope:{ctx:k}}}),Ge=new Fe({props:{title:"Inference",local:"inference",headingTag:"h2"}}),xe=new R({props:{code:"dGV4dCUyMCUzRCUyMCUyMnRyYW5zbGF0ZSUyMEVuZ2xpc2glMjB0byUyMEZyZW5jaCUzQSUyMExlZ3VtZXMlMjBzaGFyZSUyMHJlc291cmNlcyUyMHdpdGglMjBuaXRyb2dlbi1maXhpbmclMjBiYWN0ZXJpYS4lMjI=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>text = <span class="hljs-string">&quot;translate English to French: Legumes share resources with nitrogen-fixing bacteria.&quot;</span>',wrap:!1}}),Ie=new R({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBdHJhbnNsYXRvciUyMCUzRCUyMHBpcGVsaW5lKCUyMnRyYW5zbGF0aW9uJTIyJTJDJTIwbW9kZWwlM0QlMjJteV9hd2Vzb21lX29wdXNfYm9va3NfbW9kZWwlMjIpJTBBdHJhbnNsYXRvcih0ZXh0KQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>translator = pipeline(<span class="hljs-string">&quot;translation&quot;</span>, model=<span class="hljs-string">&quot;my_awesome_opus_books_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>translator(text)
[{<span class="hljs-string">&#x27;translation_text&#x27;</span>: <span class="hljs-string">&#x27;Legumes partagent des ressources avec des bactéries azotantes.&#x27;</span>}]`,wrap:!1}}),oe=new gs({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Mt],pytorch:[it]},$$scope:{ctx:k}}}),{c(){t=j("meta"),c=o(),s=j("p"),i=o(),M(b.$$.fragment),Z=o(),M(W.$$.fragment),X=o(),M(C.$$.fragment),B=o(),_=j("p"),_.textContent=I,G=o(),U=j("p"),U.textContent=V,r=o(),J=j("ol"),J.innerHTML=N,H=o(),M(x.$$.fragment),f=o(),v=j("p"),v.textContent=F,ce=o(),M(z.$$.fragment),E=o(),S=j("p"),S.textContent=se,ie=o(),M(Y.$$.fragment),Q=o(),M(q.$$.fragment),A=o(),te=j("p"),te.innerHTML=fe,L=o(),M(P.$$.fragment),D=o(),le=j("p"),le.innerHTML=Me,K=o(),M(O.$$.fragment),ee=o(),ae=j("p"),ae.textContent=ye,p=o(),M(w.$$.fragment),de=o(),ue=j("p"),ue.innerHTML=ws,Se=o(),M(he.$$.fragment),Ye=o(),M($e.$$.fragment),Qe=o(),be=j("p"),be.textContent=Ts,qe=o(),M(je.$$.fragment),Ae=o(),ge=j("p"),ge.textContent=Js,Le=o(),we=j("ol"),we.innerHTML=Us,Pe=o(),M(Te.$$.fragment),De=o(),Je=j("p"),Je.innerHTML=_s,Ke=o(),M(Ue.$$.fragment),Oe=o(),_e=j("p"),_e.innerHTML=ks,es=o(),M(ne.$$.fragment),ss=o(),M(ke.$$.fragment),ts=o(),Ze=j("p"),Ze.innerHTML=Zs,ls=o(),M(Ce.$$.fragment),as=o(),ve=j("p"),ve.innerHTML=Cs,ns=o(),M(Re.$$.fragment),ps=o(),Xe=j("p"),Xe.innerHTML=vs,rs=o(),M(Be.$$.fragment),os=o(),M(pe.$$.fragment),ms=o(),M(re.$$.fragment),cs=o(),M(Ge.$$.fragment),is=o(),Ve=j("p"),Ve.textContent=Rs,fs=o(),We=j("p"),We.textContent=Xs,Ms=o(),M(xe.$$.fragment),ys=o(),He=j("p"),He.innerHTML=Bs,ds=o(),M(Ie.$$.fragment),us=o(),Ne=j("p"),Ne.innerHTML=Gs,hs=o(),M(oe.$$.fragment),$s=o(),Ee=j("p"),this.h()},l(e){const n=Ls("svelte-u9bgzb",document.head);t=g(n,"META",{name:!0,content:!0}),n.forEach(l),c=m(e),s=g(e,"P",{}),Ns(s).forEach(l),i=m(e),y(b.$$.fragment,e),Z=m(e),y(W.$$.fragment,e),X=m(e),y(C.$$.fragment,e),B=m(e),_=g(e,"P",{"data-svelte-h":!0}),T(_)!=="svelte-8icxqd"&&(_.textContent=I),G=m(e),U=g(e,"P",{"data-svelte-h":!0}),T(U)!=="svelte-w5jzhi"&&(U.textContent=V),r=m(e),J=g(e,"OL",{"data-svelte-h":!0}),T(J)!=="svelte-1ibteza"&&(J.innerHTML=N),H=m(e),y(x.$$.fragment,e),f=m(e),v=g(e,"P",{"data-svelte-h":!0}),T(v)!=="svelte-1lya3k8"&&(v.textContent=F),ce=m(e),y(z.$$.fragment,e),E=m(e),S=g(e,"P",{"data-svelte-h":!0}),T(S)!=="svelte-193zy02"&&(S.textContent=se),ie=m(e),y(Y.$$.fragment,e),Q=m(e),y(q.$$.fragment,e),A=m(e),te=g(e,"P",{"data-svelte-h":!0}),T(te)!=="svelte-uzg9sm"&&(te.innerHTML=fe),L=m(e),y(P.$$.fragment,e),D=m(e),le=g(e,"P",{"data-svelte-h":!0}),T(le)!=="svelte-1la4z0h"&&(le.innerHTML=Me),K=m(e),y(O.$$.fragment,e),ee=m(e),ae=g(e,"P",{"data-svelte-h":!0}),T(ae)!=="svelte-1r6oj5w"&&(ae.textContent=ye),p=m(e),y(w.$$.fragment,e),de=m(e),ue=g(e,"P",{"data-svelte-h":!0}),T(ue)!=="svelte-nv77ib"&&(ue.innerHTML=ws),Se=m(e),y(he.$$.fragment,e),Ye=m(e),y($e.$$.fragment,e),Qe=m(e),be=g(e,"P",{"data-svelte-h":!0}),T(be)!=="svelte-1u9up01"&&(be.textContent=Ts),qe=m(e),y(je.$$.fragment,e),Ae=m(e),ge=g(e,"P",{"data-svelte-h":!0}),T(ge)!=="svelte-pmbrx1"&&(ge.textContent=Js),Le=m(e),we=g(e,"OL",{"data-svelte-h":!0}),T(we)!=="svelte-1lzk21w"&&(we.innerHTML=Us),Pe=m(e),y(Te.$$.fragment,e),De=m(e),Je=g(e,"P",{"data-svelte-h":!0}),T(Je)!=="svelte-1rdkxip"&&(Je.innerHTML=_s),Ke=m(e),y(Ue.$$.fragment,e),Oe=m(e),_e=g(e,"P",{"data-svelte-h":!0}),T(_e)!=="svelte-1uxhmr6"&&(_e.innerHTML=ks),es=m(e),y(ne.$$.fragment,e),ss=m(e),y(ke.$$.fragment,e),ts=m(e),Ze=g(e,"P",{"data-svelte-h":!0}),T(Ze)!=="svelte-js2qj9"&&(Ze.innerHTML=Zs),ls=m(e),y(Ce.$$.fragment,e),as=m(e),ve=g(e,"P",{"data-svelte-h":!0}),T(ve)!=="svelte-1txbu5a"&&(ve.innerHTML=Cs),ns=m(e),y(Re.$$.fragment,e),ps=m(e),Xe=g(e,"P",{"data-svelte-h":!0}),T(Xe)!=="svelte-18cw5xr"&&(Xe.innerHTML=vs),rs=m(e),y(Be.$$.fragment,e),os=m(e),y(pe.$$.fragment,e),ms=m(e),y(re.$$.fragment,e),cs=m(e),y(Ge.$$.fragment,e),is=m(e),Ve=g(e,"P",{"data-svelte-h":!0}),T(Ve)!=="svelte-cyrfc8"&&(Ve.textContent=Rs),fs=m(e),We=g(e,"P",{"data-svelte-h":!0}),T(We)!=="svelte-s3gdss"&&(We.textContent=Xs),Ms=m(e),y(xe.$$.fragment,e),ys=m(e),He=g(e,"P",{"data-svelte-h":!0}),T(He)!=="svelte-1kjanzo"&&(He.innerHTML=Bs),ds=m(e),y(Ie.$$.fragment,e),us=m(e),Ne=g(e,"P",{"data-svelte-h":!0}),T(Ne)!=="svelte-p649vi"&&(Ne.innerHTML=Gs),hs=m(e),y(oe.$$.fragment,e),$s=m(e),Ee=g(e,"P",{}),Ns(Ee).forEach(l),this.h()},h(){Fs(t,"name","hf:doc:metadata"),Fs(t,"content",dt)},m(e,n){Ps(document.head,t),a(e,c,n),a(e,s,n),a(e,i,n),d(b,e,n),a(e,Z,n),d(W,e,n),a(e,X,n),d(C,e,n),a(e,B,n),a(e,_,n),a(e,G,n),a(e,U,n),a(e,r,n),a(e,J,n),a(e,H,n),d(x,e,n),a(e,f,n),a(e,v,n),a(e,ce,n),d(z,e,n),a(e,E,n),a(e,S,n),a(e,ie,n),d(Y,e,n),a(e,Q,n),d(q,e,n),a(e,A,n),a(e,te,n),a(e,L,n),d(P,e,n),a(e,D,n),a(e,le,n),a(e,K,n),d(O,e,n),a(e,ee,n),a(e,ae,n),a(e,p,n),d(w,e,n),a(e,de,n),a(e,ue,n),a(e,Se,n),d(he,e,n),a(e,Ye,n),d($e,e,n),a(e,Qe,n),a(e,be,n),a(e,qe,n),d(je,e,n),a(e,Ae,n),a(e,ge,n),a(e,Le,n),a(e,we,n),a(e,Pe,n),d(Te,e,n),a(e,De,n),a(e,Je,n),a(e,Ke,n),d(Ue,e,n),a(e,Oe,n),a(e,_e,n),a(e,es,n),d(ne,e,n),a(e,ss,n),d(ke,e,n),a(e,ts,n),a(e,Ze,n),a(e,ls,n),d(Ce,e,n),a(e,as,n),a(e,ve,n),a(e,ns,n),d(Re,e,n),a(e,ps,n),a(e,Xe,n),a(e,rs,n),d(Be,e,n),a(e,os,n),d(pe,e,n),a(e,ms,n),d(re,e,n),a(e,cs,n),d(Ge,e,n),a(e,is,n),a(e,Ve,n),a(e,fs,n),a(e,We,n),a(e,Ms,n),d(xe,e,n),a(e,ys,n),a(e,He,n),a(e,ds,n),d(Ie,e,n),a(e,us,n),a(e,Ne,n),a(e,hs,n),d(oe,e,n),a(e,$s,n),a(e,Ee,n),bs=!0},p(e,[n]){const Vs={};n&2&&(Vs.$$scope={dirty:n,ctx:e}),x.$set(Vs);const Ws={};n&2&&(Ws.$$scope={dirty:n,ctx:e}),ne.$set(Ws);const xs={};n&2&&(xs.$$scope={dirty:n,ctx:e}),pe.$set(xs);const Hs={};n&2&&(Hs.$$scope={dirty:n,ctx:e}),re.$set(Hs);const Is={};n&2&&(Is.$$scope={dirty:n,ctx:e}),oe.$set(Is)},i(e){bs||(u(b.$$.fragment,e),u(W.$$.fragment,e),u(C.$$.fragment,e),u(x.$$.fragment,e),u(z.$$.fragment,e),u(Y.$$.fragment,e),u(q.$$.fragment,e),u(P.$$.fragment,e),u(O.$$.fragment,e),u(w.$$.fragment,e),u(he.$$.fragment,e),u($e.$$.fragment,e),u(je.$$.fragment,e),u(Te.$$.fragment,e),u(Ue.$$.fragment,e),u(ne.$$.fragment,e),u(ke.$$.fragment,e),u(Ce.$$.fragment,e),u(Re.$$.fragment,e),u(Be.$$.fragment,e),u(pe.$$.fragment,e),u(re.$$.fragment,e),u(Ge.$$.fragment,e),u(xe.$$.fragment,e),u(Ie.$$.fragment,e),u(oe.$$.fragment,e),bs=!0)},o(e){h(b.$$.fragment,e),h(W.$$.fragment,e),h(C.$$.fragment,e),h(x.$$.fragment,e),h(z.$$.fragment,e),h(Y.$$.fragment,e),h(q.$$.fragment,e),h(P.$$.fragment,e),h(O.$$.fragment,e),h(w.$$.fragment,e),h(he.$$.fragment,e),h($e.$$.fragment,e),h(je.$$.fragment,e),h(Te.$$.fragment,e),h(Ue.$$.fragment,e),h(ne.$$.fragment,e),h(ke.$$.fragment,e),h(Ce.$$.fragment,e),h(Re.$$.fragment,e),h(Be.$$.fragment,e),h(pe.$$.fragment,e),h(re.$$.fragment,e),h(Ge.$$.fragment,e),h(xe.$$.fragment,e),h(Ie.$$.fragment,e),h(oe.$$.fragment,e),bs=!1},d(e){e&&(l(c),l(s),l(i),l(Z),l(X),l(B),l(_),l(G),l(U),l(r),l(J),l(H),l(f),l(v),l(ce),l(E),l(S),l(ie),l(Q),l(A),l(te),l(L),l(D),l(le),l(K),l(ee),l(ae),l(p),l(de),l(ue),l(Se),l(Ye),l(Qe),l(be),l(qe),l(Ae),l(ge),l(Le),l(we),l(Pe),l(De),l(Je),l(Ke),l(Oe),l(_e),l(es),l(ss),l(ts),l(Ze),l(ls),l(as),l(ve),l(ns),l(ps),l(Xe),l(rs),l(os),l(ms),l(cs),l(is),l(Ve),l(fs),l(We),l(Ms),l(ys),l(He),l(ds),l(us),l(Ne),l(hs),l($s),l(Ee)),l(t),$(b,e),$(W,e),$(C,e),$(x,e),$(z,e),$(Y,e),$(q,e),$(P,e),$(O,e),$(w,e),$(he,e),$($e,e),$(je,e),$(Te,e),$(Ue,e),$(ne,e),$(ke,e),$(Ce,e),$(Re,e),$(Be,e),$(pe,e),$(re,e),$(Ge,e),$(xe,e),$(Ie,e),$(oe,e)}}}const dt='{"title":"Translation","local":"translation","sections":[{"title":"Load OPUS Books dataset","local":"load-opus-books-dataset","sections":[],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[],"depth":2},{"title":"Evaluate","local":"evaluate","sections":[],"depth":2},{"title":"Train","local":"train","sections":[],"depth":2},{"title":"Inference","local":"inference","sections":[],"depth":2}],"depth":1}';function ut(k){return Qs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ut extends qs{constructor(t){super(),As(this,t,ut,yt,Ys,{})}}export{Ut as component};
