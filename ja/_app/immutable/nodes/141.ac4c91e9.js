import{s as La,o as Da,n as fs}from"../chunks/scheduler.9bc65507.js";import{S as Pa,i as Ka,g as w,s as c,r as M,A as Oa,h as T,f as a,c as o,j as qa,u as y,x as _,k as Ua,y as sl,a as l,v as j,d as u,t as b,w as g,m as al,n as ll}from"../chunks/index.707bf1b6.js";import{T as $a}from"../chunks/Tip.c2ecdbf4.js";import{Y as tl}from"../chunks/Youtube.e1129c6f.js";import{C as W}from"../chunks/CodeBlock.54a9f38d.js";import{D as el}from"../chunks/DocNotebookDropdown.41f65cb5.js";import{F as Ys,M as Bs}from"../chunks/Markdown.8ab98a13.js";import{H as xs}from"../chunks/Heading.342b1fa6.js";function nl(X){let t,f,e='<a href="../model_doc/beit">BEiT</a>, <a href="../model_doc/bit">BiT</a>, <a href="../model_doc/convnext">ConvNeXT</a>, <a href="../model_doc/convnextv2">ConvNeXTV2</a>, <a href="../model_doc/cvt">CvT</a>, <a href="../model_doc/data2vec-vision">Data2VecVision</a>, <a href="../model_doc/deit">DeiT</a>, <a href="../model_doc/dinat">DiNAT</a>, <a href="../model_doc/dinov2">DINOv2</a>, <a href="../model_doc/efficientformer">EfficientFormer</a>, <a href="../model_doc/efficientnet">EfficientNet</a>, <a href="../model_doc/focalnet">FocalNet</a>, <a href="../model_doc/imagegpt">ImageGPT</a>, <a href="../model_doc/levit">LeViT</a>, <a href="../model_doc/mobilenet_v1">MobileNetV1</a>, <a href="../model_doc/mobilenet_v2">MobileNetV2</a>, <a href="../model_doc/mobilevit">MobileViT</a>, <a href="../model_doc/mobilevitv2">MobileViTV2</a>, <a href="../model_doc/nat">NAT</a>, <a href="../model_doc/perceiver">Perceiver</a>, <a href="../model_doc/poolformer">PoolFormer</a>, <a href="../model_doc/pvt">PVT</a>, <a href="../model_doc/regnet">RegNet</a>, <a href="../model_doc/resnet">ResNet</a>, <a href="../model_doc/segformer">SegFormer</a>, <a href="../model_doc/swiftformer">SwiftFormer</a>, <a href="../model_doc/swin">Swin Transformer</a>, <a href="../model_doc/swinv2">Swin Transformer V2</a>, <a href="../model_doc/van">VAN</a>, <a href="../model_doc/vit">ViT</a>, <a href="../model_doc/vit_hybrid">ViT Hybrid</a>, <a href="../model_doc/vit_msn">ViTMSN</a>';return{c(){t=al(`このチュートリアルで説明するタスクは、次のモデル アーキテクチャでサポートされています。

`),f=w("p"),f.innerHTML=e},l(i){t=ll(i,`このチュートリアルで説明するタスクは、次のモデル アーキテクチャでサポートされています。

`),f=T(i,"P",{"data-svelte-h":!0}),_(f)!=="svelte-6ce6f7"&&(f.innerHTML=e)},m(i,J){l(i,t,J),l(i,f,J)},p:fs,d(i){i&&(a(t),a(f))}}}function pl(X){let t,f='いくつかの画像変換を画像に適用して、モデルの過学習に対する堅牢性を高めます。ここでは torchvision の <a href="https://pytorch.org/vision/stable/transforms.html" rel="nofollow"><code>transforms</code></a> モジュールを使用しますが、任意の画像ライブラリを使用することもできます。',e,i,J="画像のランダムな部分をトリミングし、サイズを変更し、画像の平均と標準偏差で正規化します。",C,R,k,Z,F="次に、変換を適用し、画像の <code>pixel_values</code> (モデルへの入力) を返す前処理関数を作成します。",I,Y,V,$,x="データセット全体に前処理関数を適用するには、🤗 Datasets <code>with_transform</code> メソッドを使用します。変換は、データセットの要素を読み込むときにオンザフライで適用されます。",r,U,z,G,N="次に、<code>DefaultDataCollat​​or</code> を使用してサンプルのバッチを作成します。 🤗 Transformers の他のデータ照合器とは異なり、<code>DefaultDataCollat​​or</code> はパディングなどの追加の前処理を適用しません。",p,d,E;return R=new W({props:{code:"ZnJvbSUyMHRvcmNodmlzaW9uLnRyYW5zZm9ybXMlMjBpbXBvcnQlMjBSYW5kb21SZXNpemVkQ3JvcCUyQyUyMENvbXBvc2UlMkMlMjBOb3JtYWxpemUlMkMlMjBUb1RlbnNvciUwQSUwQW5vcm1hbGl6ZSUyMCUzRCUyME5vcm1hbGl6ZShtZWFuJTNEaW1hZ2VfcHJvY2Vzc29yLmltYWdlX21lYW4lMkMlMjBzdGQlM0RpbWFnZV9wcm9jZXNzb3IuaW1hZ2Vfc3RkKSUwQXNpemUlMjAlM0QlMjAoJTBBJTIwJTIwJTIwJTIwaW1hZ2VfcHJvY2Vzc29yLnNpemUlNUIlMjJzaG9ydGVzdF9lZGdlJTIyJTVEJTBBJTIwJTIwJTIwJTIwaWYlMjAlMjJzaG9ydGVzdF9lZGdlJTIyJTIwaW4lMjBpbWFnZV9wcm9jZXNzb3Iuc2l6ZSUwQSUyMCUyMCUyMCUyMGVsc2UlMjAoaW1hZ2VfcHJvY2Vzc29yLnNpemUlNUIlMjJoZWlnaHQlMjIlNUQlMkMlMjBpbWFnZV9wcm9jZXNzb3Iuc2l6ZSU1QiUyMndpZHRoJTIyJTVEKSUwQSklMEFfdHJhbnNmb3JtcyUyMCUzRCUyMENvbXBvc2UoJTVCUmFuZG9tUmVzaXplZENyb3Aoc2l6ZSklMkMlMjBUb1RlbnNvcigpJTJDJTIwbm9ybWFsaXplJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> RandomResizedCrop, Compose, Normalize, ToTensor

<span class="hljs-meta">&gt;&gt;&gt; </span>normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)
<span class="hljs-meta">&gt;&gt;&gt; </span>size = (
<span class="hljs-meta">... </span>    image_processor.size[<span class="hljs-string">&quot;shortest_edge&quot;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">if</span> <span class="hljs-string">&quot;shortest_edge&quot;</span> <span class="hljs-keyword">in</span> image_processor.size
<span class="hljs-meta">... </span>    <span class="hljs-keyword">else</span> (image_processor.size[<span class="hljs-string">&quot;height&quot;</span>], image_processor.size[<span class="hljs-string">&quot;width&quot;</span>])
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])`,wrap:!1}}),Y=new W({props:{code:"ZGVmJTIwdHJhbnNmb3JtcyhleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjBleGFtcGxlcyU1QiUyMnBpeGVsX3ZhbHVlcyUyMiU1RCUyMCUzRCUyMCU1Ql90cmFuc2Zvcm1zKGltZy5jb252ZXJ0KCUyMlJHQiUyMikpJTIwZm9yJTIwaW1nJTIwaW4lMjBleGFtcGxlcyU1QiUyMmltYWdlJTIyJTVEJTVEJTBBJTIwJTIwJTIwJTIwZGVsJTIwZXhhbXBsZXMlNUIlMjJpbWFnZSUyMiU1RCUwQSUyMCUyMCUyMCUyMHJldHVybiUyMGV4YW1wbGVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">transforms</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    examples[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [_transforms(img.convert(<span class="hljs-string">&quot;RGB&quot;</span>)) <span class="hljs-keyword">for</span> img <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;image&quot;</span>]]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">del</span> examples[<span class="hljs-string">&quot;image&quot;</span>]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> examples`,wrap:!1}}),U=new W({props:{code:"Zm9vZCUyMCUzRCUyMGZvb2Qud2l0aF90cmFuc2Zvcm0odHJhbnNmb3Jtcyk=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>food = food.with_transform(transforms)',wrap:!1}}),d=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlZmF1bHREYXRhQ29sbGF0b3IlMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGVmYXVsdERhdGFDb2xsYXRvcigp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DefaultDataCollator

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DefaultDataCollator()`,wrap:!1}}),{c(){t=w("p"),t.innerHTML=f,e=c(),i=w("p"),i.textContent=J,C=c(),M(R.$$.fragment),k=c(),Z=w("p"),Z.innerHTML=F,I=c(),M(Y.$$.fragment),V=c(),$=w("p"),$.innerHTML=x,r=c(),M(U.$$.fragment),z=c(),G=w("p"),G.innerHTML=N,p=c(),M(d.$$.fragment)},l(h){t=T(h,"P",{"data-svelte-h":!0}),_(t)!=="svelte-9lrlsl"&&(t.innerHTML=f),e=o(h),i=T(h,"P",{"data-svelte-h":!0}),_(i)!=="svelte-vjeng9"&&(i.textContent=J),C=o(h),y(R.$$.fragment,h),k=o(h),Z=T(h,"P",{"data-svelte-h":!0}),_(Z)!=="svelte-76n8td"&&(Z.innerHTML=F),I=o(h),y(Y.$$.fragment,h),V=o(h),$=T(h,"P",{"data-svelte-h":!0}),_($)!=="svelte-9bsq10"&&($.innerHTML=x),r=o(h),y(U.$$.fragment,h),z=o(h),G=T(h,"P",{"data-svelte-h":!0}),_(G)!=="svelte-me6jz3"&&(G.innerHTML=N),p=o(h),y(d.$$.fragment,h)},m(h,B){l(h,t,B),l(h,e,B),l(h,i,B),l(h,C,B),j(R,h,B),l(h,k,B),l(h,Z,B),l(h,I,B),j(Y,h,B),l(h,V,B),l(h,$,B),l(h,r,B),j(U,h,B),l(h,z,B),l(h,G,B),l(h,p,B),j(d,h,B),E=!0},p:fs,i(h){E||(u(R.$$.fragment,h),u(Y.$$.fragment,h),u(U.$$.fragment,h),u(d.$$.fragment,h),E=!0)},o(h){b(R.$$.fragment,h),b(Y.$$.fragment,h),b(U.$$.fragment,h),b(d.$$.fragment,h),E=!1},d(h){h&&(a(t),a(e),a(i),a(C),a(k),a(Z),a(I),a(V),a($),a(r),a(z),a(G),a(p)),g(R,h),g(Y,h),g(U,h),g(d,h)}}}function rl(X){let t,f;return t=new Bs({props:{$$slots:{default:[pl]},$$scope:{ctx:X}}}),{c(){M(t.$$.fragment)},l(e){y(t.$$.fragment,e)},m(e,i){j(t,e,i),f=!0},p(e,i){const J={};i&2&&(J.$$scope={dirty:i,ctx:e}),t.$set(J)},i(e){f||(u(t.$$.fragment,e),f=!0)},o(e){b(t.$$.fragment,e),f=!1},d(e){g(t,e)}}}function ml(X){let t,f=`過剰適合を回避し、モデルをより堅牢にするために、データセットのトレーニング部分にデータ拡張を追加します。
ここでは、Keras 前処理レイヤーを使用してトレーニング データの変換 (データ拡張を含む) を定義します。
検証データの変換 (中央のトリミング、サイズ変更、正規化のみ)。 <code>tf.image</code> または
他のライブラリでも構いません。`,e,i,J,C,R="次に、一度に 1 つの画像ではなく、画像のバッチに適切な変換を適用する関数を作成します。",k,Z,F,I,Y="🤗 データセット <code>set_transform</code> を使用して、その場で変換を適用します。",V,$,x,r,U=`最後の前処理ステップとして、<code>DefaultDataCollat​​or</code>を使用してサンプルのバッチを作成します。 🤗 Transformers の他のデータ照合機能とは異なり、
<code>DefaultDataCollat​​or</code> は、パディングなどの追加の前処理を適用しません。`,z,G,N;return i=new W({props:{code:"ZnJvbSUyMHRlbnNvcmZsb3clMjBpbXBvcnQlMjBrZXJhcyUwQWZyb20lMjB0ZW5zb3JmbG93LmtlcmFzJTIwaW1wb3J0JTIwbGF5ZXJzJTBBJTBBc2l6ZSUyMCUzRCUyMChpbWFnZV9wcm9jZXNzb3Iuc2l6ZSU1QiUyMmhlaWdodCUyMiU1RCUyQyUyMGltYWdlX3Byb2Nlc3Nvci5zaXplJTVCJTIyd2lkdGglMjIlNUQpJTBBJTBBdHJhaW5fZGF0YV9hdWdtZW50YXRpb24lMjAlM0QlMjBrZXJhcy5TZXF1ZW50aWFsKCUwQSUyMCUyMCUyMCUyMCU1QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGxheWVycy5SYW5kb21Dcm9wKHNpemUlNUIwJTVEJTJDJTIwc2l6ZSU1QjElNUQpJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbGF5ZXJzLlJlc2NhbGluZyhzY2FsZSUzRDEuMCUyMCUyRiUyMDEyNy41JTJDJTIwb2Zmc2V0JTNELTEpJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbGF5ZXJzLlJhbmRvbUZsaXAoJTIyaG9yaXpvbnRhbCUyMiklMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBsYXllcnMuUmFuZG9tUm90YXRpb24oZmFjdG9yJTNEMC4wMiklMkMlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjBsYXllcnMuUmFuZG9tWm9vbShoZWlnaHRfZmFjdG9yJTNEMC4yJTJDJTIwd2lkdGhfZmFjdG9yJTNEMC4yKSUyQyUwQSUyMCUyMCUyMCUyMCU1RCUyQyUwQSUyMCUyMCUyMCUyMG5hbWUlM0QlMjJ0cmFpbl9kYXRhX2F1Z21lbnRhdGlvbiUyMiUyQyUwQSklMEElMEF2YWxfZGF0YV9hdWdtZW50YXRpb24lMjAlM0QlMjBrZXJhcy5TZXF1ZW50aWFsKCUwQSUyMCUyMCUyMCUyMCU1QiUwQSUyMCUyMCUyMCUyMCUyMCUyMCUyMCUyMGxheWVycy5DZW50ZXJDcm9wKHNpemUlNUIwJTVEJTJDJTIwc2l6ZSU1QjElNUQpJTJDJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwbGF5ZXJzLlJlc2NhbGluZyhzY2FsZSUzRDEuMCUyMCUyRiUyMDEyNy41JTJDJTIwb2Zmc2V0JTNELTEpJTJDJTBBJTIwJTIwJTIwJTIwJTVEJTJDJTBBJTIwJTIwJTIwJTIwbmFtZSUzRCUyMnZhbF9kYXRhX2F1Z21lbnRhdGlvbiUyMiUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers

<span class="hljs-meta">&gt;&gt;&gt; </span>size = (image_processor.size[<span class="hljs-string">&quot;height&quot;</span>], image_processor.size[<span class="hljs-string">&quot;width&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>train_data_augmentation = keras.Sequential(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        layers.RandomCrop(size[<span class="hljs-number">0</span>], size[<span class="hljs-number">1</span>]),
<span class="hljs-meta">... </span>        layers.Rescaling(scale=<span class="hljs-number">1.0</span> / <span class="hljs-number">127.5</span>, offset=-<span class="hljs-number">1</span>),
<span class="hljs-meta">... </span>        layers.RandomFlip(<span class="hljs-string">&quot;horizontal&quot;</span>),
<span class="hljs-meta">... </span>        layers.RandomRotation(factor=<span class="hljs-number">0.02</span>),
<span class="hljs-meta">... </span>        layers.RandomZoom(height_factor=<span class="hljs-number">0.2</span>, width_factor=<span class="hljs-number">0.2</span>),
<span class="hljs-meta">... </span>    ],
<span class="hljs-meta">... </span>    name=<span class="hljs-string">&quot;train_data_augmentation&quot;</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>val_data_augmentation = keras.Sequential(
<span class="hljs-meta">... </span>    [
<span class="hljs-meta">... </span>        layers.CenterCrop(size[<span class="hljs-number">0</span>], size[<span class="hljs-number">1</span>]),
<span class="hljs-meta">... </span>        layers.Rescaling(scale=<span class="hljs-number">1.0</span> / <span class="hljs-number">127.5</span>, offset=-<span class="hljs-number">1</span>),
<span class="hljs-meta">... </span>    ],
<span class="hljs-meta">... </span>    name=<span class="hljs-string">&quot;val_data_augmentation&quot;</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),Z=new W({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBaW1wb3J0JTIwdGVuc29yZmxvdyUyMGFzJTIwdGYlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEElMEElMEFkZWYlMjBjb252ZXJ0X3RvX3RmX3RlbnNvcihpbWFnZSUzQSUyMEltYWdlKSUzQSUwQSUyMCUyMCUyMCUyMG5wX2ltYWdlJTIwJTNEJTIwbnAuYXJyYXkoaW1hZ2UpJTBBJTIwJTIwJTIwJTIwdGZfaW1hZ2UlMjAlM0QlMjB0Zi5jb252ZXJ0X3RvX3RlbnNvcihucF9pbWFnZSklMEElMjAlMjAlMjAlMjAlMjMlMjAlNjBleHBhbmRfZGltcygpJTYwJTIwaXMlMjB1c2VkJTIwdG8lMjBhZGQlMjBhJTIwYmF0Y2glMjBkaW1lbnNpb24lMjBzaW5jZSUwQSUyMCUyMCUyMCUyMCUyMyUyMHRoZSUyMFRGJTIwYXVnbWVudGF0aW9uJTIwbGF5ZXJzJTIwb3BlcmF0ZXMlMjBvbiUyMGJhdGNoZWQlMjBpbnB1dHMuJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwdGYuZXhwYW5kX2RpbXModGZfaW1hZ2UlMkMlMjAwKSUwQSUwQSUwQWRlZiUyMHByZXByb2Nlc3NfdHJhaW4oZXhhbXBsZV9iYXRjaCklM0ElMEElMjAlMjAlMjAlMjAlMjIlMjIlMjJBcHBseSUyMHRyYWluX3RyYW5zZm9ybXMlMjBhY3Jvc3MlMjBhJTIwYmF0Y2guJTIyJTIyJTIyJTBBJTIwJTIwJTIwJTIwaW1hZ2VzJTIwJTNEJTIwJTVCJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwdHJhaW5fZGF0YV9hdWdtZW50YXRpb24oY29udmVydF90b190Zl90ZW5zb3IoaW1hZ2UuY29udmVydCglMjJSR0IlMjIpKSklMjBmb3IlMjBpbWFnZSUyMGluJTIwZXhhbXBsZV9iYXRjaCU1QiUyMmltYWdlJTIyJTVEJTBBJTIwJTIwJTIwJTIwJTVEJTBBJTIwJTIwJTIwJTIwZXhhbXBsZV9iYXRjaCU1QiUyMnBpeGVsX3ZhbHVlcyUyMiU1RCUyMCUzRCUyMCU1QnRmLnRyYW5zcG9zZSh0Zi5zcXVlZXplKGltYWdlKSklMjBmb3IlMjBpbWFnZSUyMGluJTIwaW1hZ2VzJTVEJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwZXhhbXBsZV9iYXRjaCUwQSUwQSUwQWRlZiUyMHByZXByb2Nlc3NfdmFsKGV4YW1wbGVfYmF0Y2gpJTNBJTBBJTIwJTIwJTIwJTIwJTIyJTIyJTIyQXBwbHklMjB2YWxfdHJhbnNmb3JtcyUyMGFjcm9zcyUyMGElMjBiYXRjaC4lMjIlMjIlMjIlMEElMjAlMjAlMjAlMjBpbWFnZXMlMjAlM0QlMjAlNUIlMEElMjAlMjAlMjAlMjAlMjAlMjAlMjAlMjB2YWxfZGF0YV9hdWdtZW50YXRpb24oY29udmVydF90b190Zl90ZW5zb3IoaW1hZ2UuY29udmVydCglMjJSR0IlMjIpKSklMjBmb3IlMjBpbWFnZSUyMGluJTIwZXhhbXBsZV9iYXRjaCU1QiUyMmltYWdlJTIyJTVEJTBBJTIwJTIwJTIwJTIwJTVEJTBBJTIwJTIwJTIwJTIwZXhhbXBsZV9iYXRjaCU1QiUyMnBpeGVsX3ZhbHVlcyUyMiU1RCUyMCUzRCUyMCU1QnRmLnRyYW5zcG9zZSh0Zi5zcXVlZXplKGltYWdlKSklMjBmb3IlMjBpbWFnZSUyMGluJTIwaW1hZ2VzJTVEJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwZXhhbXBsZV9iYXRjaA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_to_tf_tensor</span>(<span class="hljs-params">image: Image</span>):
<span class="hljs-meta">... </span>    np_image = np.array(image)
<span class="hljs-meta">... </span>    tf_image = tf.convert_to_tensor(np_image)
<span class="hljs-meta">... </span>    <span class="hljs-comment"># \`expand_dims()\` is used to add a batch dimension since</span>
<span class="hljs-meta">... </span>    <span class="hljs-comment"># the TF augmentation layers operates on batched inputs.</span>
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> tf.expand_dims(tf_image, <span class="hljs-number">0</span>)


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_train</span>(<span class="hljs-params">example_batch</span>):
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;&quot;&quot;Apply train_transforms across a batch.&quot;&quot;&quot;</span>
<span class="hljs-meta">... </span>    images = [
<span class="hljs-meta">... </span>        train_data_augmentation(convert_to_tf_tensor(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>))) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;image&quot;</span>]
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>    example_batch[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [tf.transpose(tf.squeeze(image)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> images]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example_batch


<span class="hljs-meta">... </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_val</span>(<span class="hljs-params">example_batch</span>):
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;&quot;&quot;Apply val_transforms across a batch.&quot;&quot;&quot;</span>
<span class="hljs-meta">... </span>    images = [
<span class="hljs-meta">... </span>        val_data_augmentation(convert_to_tf_tensor(image.convert(<span class="hljs-string">&quot;RGB&quot;</span>))) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> example_batch[<span class="hljs-string">&quot;image&quot;</span>]
<span class="hljs-meta">... </span>    ]
<span class="hljs-meta">... </span>    example_batch[<span class="hljs-string">&quot;pixel_values&quot;</span>] = [tf.transpose(tf.squeeze(image)) <span class="hljs-keyword">for</span> image <span class="hljs-keyword">in</span> images]
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> example_batch`,wrap:!1}}),$=new W({props:{code:"Zm9vZCU1QiUyMnRyYWluJTIyJTVELnNldF90cmFuc2Zvcm0ocHJlcHJvY2Vzc190cmFpbiklMEFmb29kJTVCJTIydGVzdCUyMiU1RC5zZXRfdHJhbnNmb3JtKHByZXByb2Nlc3NfdmFsKQ==",highlighted:`food[<span class="hljs-string">&quot;train&quot;</span>].set_transform(preprocess_train)
food[<span class="hljs-string">&quot;test&quot;</span>].set_transform(preprocess_val)`,wrap:!1}}),G=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlZmF1bHREYXRhQ29sbGF0b3IlMEElMEFkYXRhX2NvbGxhdG9yJTIwJTNEJTIwRGVmYXVsdERhdGFDb2xsYXRvcihyZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DefaultDataCollator

<span class="hljs-meta">&gt;&gt;&gt; </span>data_collator = DefaultDataCollator(return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),{c(){t=w("p"),t.innerHTML=f,e=c(),M(i.$$.fragment),J=c(),C=w("p"),C.textContent=R,k=c(),M(Z.$$.fragment),F=c(),I=w("p"),I.innerHTML=Y,V=c(),M($.$$.fragment),x=c(),r=w("p"),r.innerHTML=U,z=c(),M(G.$$.fragment)},l(p){t=T(p,"P",{"data-svelte-h":!0}),_(t)!=="svelte-1bftz3e"&&(t.innerHTML=f),e=o(p),y(i.$$.fragment,p),J=o(p),C=T(p,"P",{"data-svelte-h":!0}),_(C)!=="svelte-1g4py9p"&&(C.textContent=R),k=o(p),y(Z.$$.fragment,p),F=o(p),I=T(p,"P",{"data-svelte-h":!0}),_(I)!=="svelte-qy5n6r"&&(I.innerHTML=Y),V=o(p),y($.$$.fragment,p),x=o(p),r=T(p,"P",{"data-svelte-h":!0}),_(r)!=="svelte-zm6h83"&&(r.innerHTML=U),z=o(p),y(G.$$.fragment,p)},m(p,d){l(p,t,d),l(p,e,d),j(i,p,d),l(p,J,d),l(p,C,d),l(p,k,d),j(Z,p,d),l(p,F,d),l(p,I,d),l(p,V,d),j($,p,d),l(p,x,d),l(p,r,d),l(p,z,d),j(G,p,d),N=!0},p:fs,i(p){N||(u(i.$$.fragment,p),u(Z.$$.fragment,p),u($.$$.fragment,p),u(G.$$.fragment,p),N=!0)},o(p){b(i.$$.fragment,p),b(Z.$$.fragment,p),b($.$$.fragment,p),b(G.$$.fragment,p),N=!1},d(p){p&&(a(t),a(e),a(J),a(C),a(k),a(F),a(I),a(V),a(x),a(r),a(z)),g(i,p),g(Z,p),g($,p),g(G,p)}}}function cl(X){let t,f;return t=new Bs({props:{$$slots:{default:[ml]},$$scope:{ctx:X}}}),{c(){M(t.$$.fragment)},l(e){y(t.$$.fragment,e)},m(e,i){j(t,e,i),f=!0},p(e,i){const J={};i&2&&(J.$$scope={dirty:i,ctx:e}),t.$set(J)},i(e){f||(u(t.$$.fragment,e),f=!0)},o(e){b(t.$$.fragment,e),f=!1},d(e){g(t,e)}}}function ol(X){let t,f='<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a> を使用したモデルの微調整に慣れていない場合は、<a href="../training#train-with-pytorch-trainer">こちら</a> の基本的なチュートリアルをご覧ください。';return{c(){t=w("p"),t.innerHTML=f},l(e){t=T(e,"P",{"data-svelte-h":!0}),_(t)!=="svelte-u6v1yb"&&(t.innerHTML=f)},m(e,i){l(e,t,i)},p:fs,d(e){e&&a(t)}}}function il(X){let t,f,e,i='これでモデルのトレーニングを開始する準備が整いました。 <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoModelForImageClassification">AutoModelForImageClassification</a> を使用して ViT をロードします。ラベルの数と予想されるラベルの数、およびラベル マッピングを指定します。',J,C,R,k,Z="この時点で残っているステップは 3 つだけです。",F,I,Y='<li><a href="/docs/transformers/main/ja/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> でトレーニング ハイパーパラメータを定義します。 <code>image</code> 列が削除されるため、未使用の列を削除しないことが重要です。 <code>image</code> 列がないと、<code>pixel_values</code> を作成できません。この動作を防ぐには、<code>remove_unused_columns=False</code>を設定してください。他に必要なパラメータは、モデルの保存場所を指定する <code>output_dir</code> だけです。 <code>push_to_hub=True</code>を設定して、このモデルをハブにプッシュします (モデルをアップロードするには、Hugging Face にサインインする必要があります)。各エポックの終了時に、<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a> は精度を評価し、トレーニング チェックポイントを保存します。</li> <li>トレーニング引数を、モデル、データセット、トークナイザー、データ照合器、および <code>compute_metrics</code> 関数とともに <a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a> に渡します。</li> <li><a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.train">train()</a> を呼び出してモデルを微調整します。</li>',V,$,x,r,U='トレーニングが完了したら、 <a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.push_to_hub">push_to_hub()</a> メソッドを使用してモデルをハブに共有し、誰もがモデルを使用できるようにします。',z,G,N;return t=new $a({props:{$$slots:{default:[ol]},$$scope:{ctx:X}}}),C=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24lMkMlMjBUcmFpbmluZ0FyZ3VtZW50cyUyQyUyMFRyYWluZXIlMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMGNoZWNrcG9pbnQlMkMlMEElMjAlMjAlMjAlMjBudW1fbGFiZWxzJTNEbGVuKGxhYmVscyklMkMlMEElMjAlMjAlMjAlMjBpZDJsYWJlbCUzRGlkMmxhYmVsJTJDJTBBJTIwJTIwJTIwJTIwbGFiZWwyaWQlM0RsYWJlbDJpZCUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    checkpoint,
<span class="hljs-meta">... </span>    num_labels=<span class="hljs-built_in">len</span>(labels),
<span class="hljs-meta">... </span>    id2label=id2label,
<span class="hljs-meta">... </span>    label2id=label2id,
<span class="hljs-meta">... </span>)`,wrap:!1}}),$=new W({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJteV9hd2Vzb21lX2Zvb2RfbW9kZWwlMjIlMkMlMEElMjAlMjAlMjAlMjByZW1vdmVfdW51c2VkX2NvbHVtbnMlM0RGYWxzZSUyQyUwQSUyMCUyMCUyMCUyMGV2YWx1YXRpb25fc3RyYXRlZ3klM0QlMjJlcG9jaCUyMiUyQyUwQSUyMCUyMCUyMCUyMHNhdmVfc3RyYXRlZ3klM0QlMjJlcG9jaCUyMiUyQyUwQSUyMCUyMCUyMCUyMGxlYXJuaW5nX3JhdGUlM0Q1ZS01JTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV90cmFpbl9iYXRjaF9zaXplJTNEMTYlMkMlMEElMjAlMjAlMjAlMjBncmFkaWVudF9hY2N1bXVsYXRpb25fc3RlcHMlM0Q0JTJDJTBBJTIwJTIwJTIwJTIwcGVyX2RldmljZV9ldmFsX2JhdGNoX3NpemUlM0QxNiUyQyUwQSUyMCUyMCUyMCUyMG51bV90cmFpbl9lcG9jaHMlM0QzJTJDJTBBJTIwJTIwJTIwJTIwd2FybXVwX3JhdGlvJTNEMC4xJTJDJTBBJTIwJTIwJTIwJTIwbG9nZ2luZ19zdGVwcyUzRDEwJTJDJTBBJTIwJTIwJTIwJTIwbG9hZF9iZXN0X21vZGVsX2F0X2VuZCUzRFRydWUlMkMlMEElMjAlMjAlMjAlMjBtZXRyaWNfZm9yX2Jlc3RfbW9kZWwlM0QlMjJhY2N1cmFjeSUyMiUyQyUwQSUyMCUyMCUyMCUyMHB1c2hfdG9faHViJTNEVHJ1ZSUyQyUwQSklMEElMEF0cmFpbmVyJTIwJTNEJTIwVHJhaW5lciglMEElMjAlMjAlMjAlMjBtb2RlbCUzRG1vZGVsJTJDJTBBJTIwJTIwJTIwJTIwYXJncyUzRHRyYWluaW5nX2FyZ3MlMkMlMEElMjAlMjAlMjAlMjBkYXRhX2NvbGxhdG9yJTNEZGF0YV9jb2xsYXRvciUyQyUwQSUyMCUyMCUyMCUyMHRyYWluX2RhdGFzZXQlM0Rmb29kJTVCJTIydHJhaW4lMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBldmFsX2RhdGFzZXQlM0Rmb29kJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRGltYWdlX3Byb2Nlc3NvciUyQyUwQSUyMCUyMCUyMCUyMGNvbXB1dGVfbWV0cmljcyUzRGNvbXB1dGVfbWV0cmljcyUyQyUwQSklMEElMEF0cmFpbmVyLnRyYWluKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_food_model&quot;</span>,
<span class="hljs-meta">... </span>    remove_unused_columns=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">5e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    gradient_accumulation_steps=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">16</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">3</span>,
<span class="hljs-meta">... </span>    warmup_ratio=<span class="hljs-number">0.1</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">10</span>,
<span class="hljs-meta">... </span>    load_best_model_at_end=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    metric_for_best_model=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    data_collator=data_collator,
<span class="hljs-meta">... </span>    train_dataset=food[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=food[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=image_processor,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),G=new W({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),{c(){M(t.$$.fragment),f=c(),e=w("p"),e.innerHTML=i,J=c(),M(C.$$.fragment),R=c(),k=w("p"),k.textContent=Z,F=c(),I=w("ol"),I.innerHTML=Y,V=c(),M($.$$.fragment),x=c(),r=w("p"),r.innerHTML=U,z=c(),M(G.$$.fragment)},l(p){y(t.$$.fragment,p),f=o(p),e=T(p,"P",{"data-svelte-h":!0}),_(e)!=="svelte-1wsijgh"&&(e.innerHTML=i),J=o(p),y(C.$$.fragment,p),R=o(p),k=T(p,"P",{"data-svelte-h":!0}),_(k)!=="svelte-5p19xw"&&(k.textContent=Z),F=o(p),I=T(p,"OL",{"data-svelte-h":!0}),_(I)!=="svelte-gv3xzt"&&(I.innerHTML=Y),V=o(p),y($.$$.fragment,p),x=o(p),r=T(p,"P",{"data-svelte-h":!0}),_(r)!=="svelte-ngexm3"&&(r.innerHTML=U),z=o(p),y(G.$$.fragment,p)},m(p,d){j(t,p,d),l(p,f,d),l(p,e,d),l(p,J,d),j(C,p,d),l(p,R,d),l(p,k,d),l(p,F,d),l(p,I,d),l(p,V,d),j($,p,d),l(p,x,d),l(p,r,d),l(p,z,d),j(G,p,d),N=!0},p(p,d){const E={};d&2&&(E.$$scope={dirty:d,ctx:p}),t.$set(E)},i(p){N||(u(t.$$.fragment,p),u(C.$$.fragment,p),u($.$$.fragment,p),u(G.$$.fragment,p),N=!0)},o(p){b(t.$$.fragment,p),b(C.$$.fragment,p),b($.$$.fragment,p),b(G.$$.fragment,p),N=!1},d(p){p&&(a(f),a(e),a(J),a(R),a(k),a(F),a(I),a(V),a(x),a(r),a(z)),g(t,p),g(C,p),g($,p),g(G,p)}}}function fl(X){let t,f;return t=new Bs({props:{$$slots:{default:[il]},$$scope:{ctx:X}}}),{c(){M(t.$$.fragment)},l(e){y(t.$$.fragment,e)},m(e,i){j(t,e,i),f=!0},p(e,i){const J={};i&2&&(J.$$scope={dirty:i,ctx:e}),t.$set(J)},i(e){f||(u(t.$$.fragment,e),f=!0)},o(e){b(t.$$.fragment,e),f=!1},d(e){g(t,e)}}}function hl(X){let t,f='Keras を使用したモデルの微調整に慣れていない場合は、まず <a href="./training#train-a-tensorflow-model-with-keras">基本チュートリアル</a> を確認してください。';return{c(){t=w("p"),t.innerHTML=f},l(e){t=T(e,"P",{"data-svelte-h":!0}),_(t)!=="svelte-m0v6mc"&&(t.innerHTML=f)},m(e,i){l(e,t,i)},p:fs,d(e){e&&a(t)}}}function Ml(X){let t,f,e,i="TensorFlow でモデルを微調整するには、次の手順に従います。",J,C,R="<li>トレーニングのハイパーパラメータを定義し、オプティマイザーと学習率スケジュールを設定します。</li> <li>事前トレーニングされたモデルをインスタンス化します。</li> <li>🤗 データセットを <code>tf.data.Dataset</code> に変換します。</li> <li>モデルをコンパイルします。</li> <li>コールバックを追加し、<code>fit()</code> メソッドを使用してトレーニングを実行します。</li> <li>モデルを 🤗 Hub にアップロードしてコミュニティと共有します。</li>",k,Z,F="まず、ハイパーパラメーター、オプティマイザー、学習率スケジュールを定義します。",I,Y,V,$,x='次に、ラベル マッピングとともに <a href="/docs/transformers/main/ja/model_doc/auto#transformers.TFAutoModelForImageClassification">TFAutoModelForImageClassification</a> を使用して ViT を読み込みます。',r,U,z,G,N="Convert your datasets to the <code>tf.data.Dataset</code> format using the <code>to_tf_dataset</code> and your <code>data_collator</code>:",p,d,E,h,B="<code>compile()</code> を使用してトレーニング用にモデルを設定します。",K,Q,hs,L,D=`予測から精度を計算し、モデルを 🤗 ハブにプッシュするには、<a href="../main_classes/keras_callbacks">Keras callbacks</a> を使用します。
<code>compute_metrics</code> 関数を <a href="../main_classes/keras_callbacks#transformers.KerasMetricCallback">KerasMetricCallback</a> に渡します。
<a href="../main_classes/keras_callbacks#transformers.PushToHubCallback">PushToHubCallback</a> を使用してモデルをアップロードします。`,O,H,ss,S,Ns=`ついに、モデルをトレーニングする準備が整いました。トレーニングおよび検証データセット、エポック数、
モデルを微調整するためのコールバック:`,as,A,ls,q,zs="おめでとう！モデルを微調整し、🤗 Hub で共有しました。これで推論に使用できるようになりました。",ts;return t=new $a({props:{$$slots:{default:[hl]},$$scope:{ctx:X}}}),Y=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMGNyZWF0ZV9vcHRpbWl6ZXIlMEElMEFiYXRjaF9zaXplJTIwJTNEJTIwMTYlMEFudW1fZXBvY2hzJTIwJTNEJTIwNSUwQW51bV90cmFpbl9zdGVwcyUyMCUzRCUyMGxlbihmb29kJTVCJTIydHJhaW4lMjIlNUQpJTIwKiUyMG51bV9lcG9jaHMlMEFsZWFybmluZ19yYXRlJTIwJTNEJTIwM2UtNSUwQXdlaWdodF9kZWNheV9yYXRlJTIwJTNEJTIwMC4wMSUwQSUwQW9wdGltaXplciUyQyUyMGxyX3NjaGVkdWxlJTIwJTNEJTIwY3JlYXRlX29wdGltaXplciglMEElMjAlMjAlMjAlMjBpbml0X2xyJTNEbGVhcm5pbmdfcmF0ZSUyQyUwQSUyMCUyMCUyMCUyMG51bV90cmFpbl9zdGVwcyUzRG51bV90cmFpbl9zdGVwcyUyQyUwQSUyMCUyMCUyMCUyMHdlaWdodF9kZWNheV9yYXRlJTNEd2VpZ2h0X2RlY2F5X3JhdGUlMkMlMEElMjAlMjAlMjAlMjBudW1fd2FybXVwX3N0ZXBzJTNEMCUyQyUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> create_optimizer

<span class="hljs-meta">&gt;&gt;&gt; </span>batch_size = <span class="hljs-number">16</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_epochs = <span class="hljs-number">5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>num_train_steps = <span class="hljs-built_in">len</span>(food[<span class="hljs-string">&quot;train&quot;</span>]) * num_epochs
<span class="hljs-meta">&gt;&gt;&gt; </span>learning_rate = <span class="hljs-number">3e-5</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>weight_decay_rate = <span class="hljs-number">0.01</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>optimizer, lr_schedule = create_optimizer(
<span class="hljs-meta">... </span>    init_lr=learning_rate,
<span class="hljs-meta">... </span>    num_train_steps=num_train_steps,
<span class="hljs-meta">... </span>    weight_decay_rate=weight_decay_rate,
<span class="hljs-meta">... </span>    num_warmup_steps=<span class="hljs-number">0</span>,
<span class="hljs-meta">... </span>)`,wrap:!1}}),U=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjBjaGVja3BvaW50JTJDJTBBJTIwJTIwJTIwJTIwaWQybGFiZWwlM0RpZDJsYWJlbCUyQyUwQSUyMCUyMCUyMCUyMGxhYmVsMmlkJTNEbGFiZWwyaWQlMkMlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(
<span class="hljs-meta">... </span>    checkpoint,
<span class="hljs-meta">... </span>    id2label=id2label,
<span class="hljs-meta">... </span>    label2id=label2id,
<span class="hljs-meta">... </span>)`,wrap:!1}}),d=new W({props:{code:"JTIzJTIwY29udmVydGluZyUyMG91ciUyMHRyYWluJTIwZGF0YXNldCUyMHRvJTIwdGYuZGF0YS5EYXRhc2V0JTBBdGZfdHJhaW5fZGF0YXNldCUyMCUzRCUyMGZvb2QlNUIlMjJ0cmFpbiUyMiU1RC50b190Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMGNvbHVtbnMlM0QlMjJwaXhlbF92YWx1ZXMlMjIlMkMlMjBsYWJlbF9jb2xzJTNEJTIybGFiZWwlMjIlMkMlMjBzaHVmZmxlJTNEVHJ1ZSUyQyUyMGJhdGNoX3NpemUlM0RiYXRjaF9zaXplJTJDJTIwY29sbGF0ZV9mbiUzRGRhdGFfY29sbGF0b3IlMEEpJTBBJTBBJTIzJTIwY29udmVydGluZyUyMG91ciUyMHRlc3QlMjBkYXRhc2V0JTIwdG8lMjB0Zi5kYXRhLkRhdGFzZXQlMEF0Zl9ldmFsX2RhdGFzZXQlMjAlM0QlMjBmb29kJTVCJTIydGVzdCUyMiU1RC50b190Zl9kYXRhc2V0KCUwQSUyMCUyMCUyMCUyMGNvbHVtbnMlM0QlMjJwaXhlbF92YWx1ZXMlMjIlMkMlMjBsYWJlbF9jb2xzJTNEJTIybGFiZWwlMjIlMkMlMjBzaHVmZmxlJTNEVHJ1ZSUyQyUyMGJhdGNoX3NpemUlM0RiYXRjaF9zaXplJTJDJTIwY29sbGF0ZV9mbiUzRGRhdGFfY29sbGF0b3IlMEEp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># converting our train dataset to tf.data.Dataset</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_train_dataset = food[<span class="hljs-string">&quot;train&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=<span class="hljs-string">&quot;pixel_values&quot;</span>, label_cols=<span class="hljs-string">&quot;label&quot;</span>, shuffle=<span class="hljs-literal">True</span>, batch_size=batch_size, collate_fn=data_collator
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># converting our test dataset to tf.data.Dataset</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>tf_eval_dataset = food[<span class="hljs-string">&quot;test&quot;</span>].to_tf_dataset(
<span class="hljs-meta">... </span>    columns=<span class="hljs-string">&quot;pixel_values&quot;</span>, label_cols=<span class="hljs-string">&quot;label&quot;</span>, shuffle=<span class="hljs-literal">True</span>, batch_size=batch_size, collate_fn=data_collator
<span class="hljs-meta">... </span>)`,wrap:!1}}),Q=new W({props:{code:"ZnJvbSUyMHRlbnNvcmZsb3cua2VyYXMubG9zc2VzJTIwaW1wb3J0JTIwU3BhcnNlQ2F0ZWdvcmljYWxDcm9zc2VudHJvcHklMEElMEFsb3NzJTIwJTNEJTIwdGYua2VyYXMubG9zc2VzLlNwYXJzZUNhdGVnb3JpY2FsQ3Jvc3NlbnRyb3B5KGZyb21fbG9naXRzJTNEVHJ1ZSklMEFtb2RlbC5jb21waWxlKG9wdGltaXplciUzRG9wdGltaXplciUyQyUyMGxvc3MlM0Rsb3NzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tensorflow.keras.losses <span class="hljs-keyword">import</span> SparseCategoricalCrossentropy

<span class="hljs-meta">&gt;&gt;&gt; </span>loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model.<span class="hljs-built_in">compile</span>(optimizer=optimizer, loss=loss)`,wrap:!1}}),H=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5rZXJhc19jYWxsYmFja3MlMjBpbXBvcnQlMjBLZXJhc01ldHJpY0NhbGxiYWNrJTJDJTIwUHVzaFRvSHViQ2FsbGJhY2slMEElMEFtZXRyaWNfY2FsbGJhY2slMjAlM0QlMjBLZXJhc01ldHJpY0NhbGxiYWNrKG1ldHJpY19mbiUzRGNvbXB1dGVfbWV0cmljcyUyQyUyMGV2YWxfZGF0YXNldCUzRHRmX2V2YWxfZGF0YXNldCklMEFwdXNoX3RvX2h1Yl9jYWxsYmFjayUyMCUzRCUyMFB1c2hUb0h1YkNhbGxiYWNrKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJmb29kX2NsYXNzaWZpZXIlMjIlMkMlMEElMjAlMjAlMjAlMjB0b2tlbml6ZXIlM0RpbWFnZV9wcm9jZXNzb3IlMkMlMEElMjAlMjAlMjAlMjBzYXZlX3N0cmF0ZWd5JTNEJTIybm8lMjIlMkMlMEEpJTBBY2FsbGJhY2tzJTIwJTNEJTIwJTVCbWV0cmljX2NhbGxiYWNrJTJDJTIwcHVzaF90b19odWJfY2FsbGJhY2slNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers.keras_callbacks <span class="hljs-keyword">import</span> KerasMetricCallback, PushToHubCallback

<span class="hljs-meta">&gt;&gt;&gt; </span>metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)
<span class="hljs-meta">&gt;&gt;&gt; </span>push_to_hub_callback = PushToHubCallback(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;food_classifier&quot;</span>,
<span class="hljs-meta">... </span>    tokenizer=image_processor,
<span class="hljs-meta">... </span>    save_strategy=<span class="hljs-string">&quot;no&quot;</span>,
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>callbacks = [metric_callback, push_to_hub_callback]`,wrap:!1}}),A=new W({props:{code:"bW9kZWwuZml0KHRmX3RyYWluX2RhdGFzZXQlMkMlMjB2YWxpZGF0aW9uX2RhdGElM0R0Zl9ldmFsX2RhdGFzZXQlMkMlMjBlcG9jaHMlM0RudW1fZXBvY2hzJTJDJTIwY2FsbGJhY2tzJTNEY2FsbGJhY2tzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)
Epoch <span class="hljs-number">1</span>/<span class="hljs-number">5</span>
<span class="hljs-number">250</span>/<span class="hljs-number">250</span> [==============================] - 313s 1s/step - loss: <span class="hljs-number">2.5623</span> - val_loss: <span class="hljs-number">1.4161</span> - accuracy: <span class="hljs-number">0.9290</span>
Epoch <span class="hljs-number">2</span>/<span class="hljs-number">5</span>
<span class="hljs-number">250</span>/<span class="hljs-number">250</span> [==============================] - 265s 1s/step - loss: <span class="hljs-number">0.9181</span> - val_loss: <span class="hljs-number">0.6808</span> - accuracy: <span class="hljs-number">0.9690</span>
Epoch <span class="hljs-number">3</span>/<span class="hljs-number">5</span>
<span class="hljs-number">250</span>/<span class="hljs-number">250</span> [==============================] - 252s 1s/step - loss: <span class="hljs-number">0.3910</span> - val_loss: <span class="hljs-number">0.4303</span> - accuracy: <span class="hljs-number">0.9820</span>
Epoch <span class="hljs-number">4</span>/<span class="hljs-number">5</span>
<span class="hljs-number">250</span>/<span class="hljs-number">250</span> [==============================] - 251s 1s/step - loss: <span class="hljs-number">0.2028</span> - val_loss: <span class="hljs-number">0.3191</span> - accuracy: <span class="hljs-number">0.9900</span>
Epoch <span class="hljs-number">5</span>/<span class="hljs-number">5</span>
<span class="hljs-number">250</span>/<span class="hljs-number">250</span> [==============================] - 238s 949ms/step - loss: <span class="hljs-number">0.1232</span> - val_loss: <span class="hljs-number">0.3259</span> - accuracy: <span class="hljs-number">0.9890</span>`,wrap:!1}}),{c(){M(t.$$.fragment),f=c(),e=w("p"),e.textContent=i,J=c(),C=w("ol"),C.innerHTML=R,k=c(),Z=w("p"),Z.textContent=F,I=c(),M(Y.$$.fragment),V=c(),$=w("p"),$.innerHTML=x,r=c(),M(U.$$.fragment),z=c(),G=w("p"),G.innerHTML=N,p=c(),M(d.$$.fragment),E=c(),h=w("p"),h.innerHTML=B,K=c(),M(Q.$$.fragment),hs=c(),L=w("p"),L.innerHTML=D,O=c(),M(H.$$.fragment),ss=c(),S=w("p"),S.textContent=Ns,as=c(),M(A.$$.fragment),ls=c(),q=w("p"),q.textContent=zs},l(m){y(t.$$.fragment,m),f=o(m),e=T(m,"P",{"data-svelte-h":!0}),_(e)!=="svelte-a44fb"&&(e.textContent=i),J=o(m),C=T(m,"OL",{"data-svelte-h":!0}),_(C)!=="svelte-k6hrg7"&&(C.innerHTML=R),k=o(m),Z=T(m,"P",{"data-svelte-h":!0}),_(Z)!=="svelte-1u4w84l"&&(Z.textContent=F),I=o(m),y(Y.$$.fragment,m),V=o(m),$=T(m,"P",{"data-svelte-h":!0}),_($)!=="svelte-qyawgk"&&($.innerHTML=x),r=o(m),y(U.$$.fragment,m),z=o(m),G=T(m,"P",{"data-svelte-h":!0}),_(G)!=="svelte-myl0l6"&&(G.innerHTML=N),p=o(m),y(d.$$.fragment,m),E=o(m),h=T(m,"P",{"data-svelte-h":!0}),_(h)!=="svelte-1l4vvgv"&&(h.innerHTML=B),K=o(m),y(Q.$$.fragment,m),hs=o(m),L=T(m,"P",{"data-svelte-h":!0}),_(L)!=="svelte-145ej7o"&&(L.innerHTML=D),O=o(m),y(H.$$.fragment,m),ss=o(m),S=T(m,"P",{"data-svelte-h":!0}),_(S)!=="svelte-bif4s6"&&(S.textContent=Ns),as=o(m),y(A.$$.fragment,m),ls=o(m),q=T(m,"P",{"data-svelte-h":!0}),_(q)!=="svelte-f4xo0m"&&(q.textContent=zs)},m(m,v){j(t,m,v),l(m,f,v),l(m,e,v),l(m,J,v),l(m,C,v),l(m,k,v),l(m,Z,v),l(m,I,v),j(Y,m,v),l(m,V,v),l(m,$,v),l(m,r,v),j(U,m,v),l(m,z,v),l(m,G,v),l(m,p,v),j(d,m,v),l(m,E,v),l(m,h,v),l(m,K,v),j(Q,m,v),l(m,hs,v),l(m,L,v),l(m,O,v),j(H,m,v),l(m,ss,v),l(m,S,v),l(m,as,v),j(A,m,v),l(m,ls,v),l(m,q,v),ts=!0},p(m,v){const P={};v&2&&(P.$$scope={dirty:v,ctx:m}),t.$set(P)},i(m){ts||(u(t.$$.fragment,m),u(Y.$$.fragment,m),u(U.$$.fragment,m),u(d.$$.fragment,m),u(Q.$$.fragment,m),u(H.$$.fragment,m),u(A.$$.fragment,m),ts=!0)},o(m){b(t.$$.fragment,m),b(Y.$$.fragment,m),b(U.$$.fragment,m),b(d.$$.fragment,m),b(Q.$$.fragment,m),b(H.$$.fragment,m),b(A.$$.fragment,m),ts=!1},d(m){m&&(a(f),a(e),a(J),a(C),a(k),a(Z),a(I),a(V),a($),a(r),a(z),a(G),a(p),a(E),a(h),a(K),a(hs),a(L),a(O),a(ss),a(S),a(as),a(ls),a(q)),g(t,m),g(Y,m),g(U,m),g(d,m),g(Q,m),g(H,m),g(A,m)}}}function yl(X){let t,f;return t=new Bs({props:{$$slots:{default:[Ml]},$$scope:{ctx:X}}}),{c(){M(t.$$.fragment)},l(e){y(t.$$.fragment,e)},m(e,i){j(t,e,i),f=!0},p(e,i){const J={};i&2&&(J.$$scope={dirty:i,ctx:e}),t.$set(J)},i(e){f||(u(t.$$.fragment,e),f=!0)},o(e){b(t.$$.fragment,e),f=!1},d(e){g(t,e)}}}function jl(X){let t,f='画像分類用のモデルを微調整する方法の詳細な例については、対応する <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">PyTorch ノートブック</a>';return{c(){t=w("p"),t.innerHTML=f},l(e){t=T(e,"P",{"data-svelte-h":!0}),_(t)!=="svelte-14b518"&&(t.innerHTML=f)},m(e,i){l(e,t,i)},p:fs,d(e){e&&a(t)}}}function ul(X){let t,f="画像プロセッサをロードして画像を前処理し、<code>input</code>を PyTorch テンソルとして返します。",e,i,J,C,R="入力をモデルに渡し、ロジットを返します。",k,Z,F,I,Y="最も高い確率で予測されたラベルを取得し、モデルの <code>id2label</code> マッピングを使用してラベルに変換します。",V,$,x;return i=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQWltcG9ydCUyMHRvcmNoJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJteV9hd2Vzb21lX2Zvb2RfbW9kZWwlMjIpJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;my_awesome_food_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),Z=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm15X2F3ZXNvbWVfZm9vZF9tb2RlbCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;my_awesome_food_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits`,wrap:!1}}),$=new W({props:{code:"cHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_label]
<span class="hljs-string">&#x27;beignets&#x27;</span>`,wrap:!1}}),{c(){t=w("p"),t.innerHTML=f,e=c(),M(i.$$.fragment),J=c(),C=w("p"),C.textContent=R,k=c(),M(Z.$$.fragment),F=c(),I=w("p"),I.innerHTML=Y,V=c(),M($.$$.fragment)},l(r){t=T(r,"P",{"data-svelte-h":!0}),_(t)!=="svelte-cy7wl9"&&(t.innerHTML=f),e=o(r),y(i.$$.fragment,r),J=o(r),C=T(r,"P",{"data-svelte-h":!0}),_(C)!=="svelte-jwtn5w"&&(C.textContent=R),k=o(r),y(Z.$$.fragment,r),F=o(r),I=T(r,"P",{"data-svelte-h":!0}),_(I)!=="svelte-1y0o568"&&(I.innerHTML=Y),V=o(r),y($.$$.fragment,r)},m(r,U){l(r,t,U),l(r,e,U),j(i,r,U),l(r,J,U),l(r,C,U),l(r,k,U),j(Z,r,U),l(r,F,U),l(r,I,U),l(r,V,U),j($,r,U),x=!0},p:fs,i(r){x||(u(i.$$.fragment,r),u(Z.$$.fragment,r),u($.$$.fragment,r),x=!0)},o(r){b(i.$$.fragment,r),b(Z.$$.fragment,r),b($.$$.fragment,r),x=!1},d(r){r&&(a(t),a(e),a(J),a(C),a(k),a(F),a(I),a(V)),g(i,r),g(Z,r),g($,r)}}}function bl(X){let t,f;return t=new Bs({props:{$$slots:{default:[ul]},$$scope:{ctx:X}}}),{c(){M(t.$$.fragment)},l(e){y(t.$$.fragment,e)},m(e,i){j(t,e,i),f=!0},p(e,i){const J={};i&2&&(J.$$scope={dirty:i,ctx:e}),t.$set(J)},i(e){f||(u(t.$$.fragment,e),f=!0)},o(e){b(t.$$.fragment,e),f=!1},d(e){g(t,e)}}}function gl(X){let t,f="画像プロセッサをロードして画像を前処理し、<code>input</code>を TensorFlow テンソルとして返します。",e,i,J,C,R="入力をモデルに渡し、ロジットを返します。",k,Z,F,I,Y="最も高い確率で予測されたラベルを取得し、モデルの <code>id2label</code> マッピングを使用してラベルに変換します。",V,$,x;return i=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyTWFyaWFLJTJGZm9vZF9jbGFzc2lmaWVyJTIyKSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIydGYlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;MariaK/food_classifier&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)`,wrap:!1}}),Z=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFRGQXV0b01vZGVsRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQSUwQW1vZGVsJTIwJTNEJTIwVEZBdXRvTW9kZWxGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJNYXJpYUslMkZmb29kX2NsYXNzaWZpZXIlMjIpJTBBbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFAutoModelForImageClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFAutoModelForImageClassification.from_pretrained(<span class="hljs-string">&quot;MariaK/food_classifier&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits`,wrap:!1}}),$=new W({props:{code:"cHJlZGljdGVkX2NsYXNzX2lkJTIwJTNEJTIwaW50KHRmLm1hdGguYXJnbWF4KGxvZ2l0cyUyQyUyMGF4aXMlM0QtMSklNUIwJTVEKSUwQW1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9jbGFzc19pZCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_id = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>])
<span class="hljs-meta">&gt;&gt;&gt; </span>model.config.id2label[predicted_class_id]
<span class="hljs-string">&#x27;beignets&#x27;</span>`,wrap:!1}}),{c(){t=w("p"),t.innerHTML=f,e=c(),M(i.$$.fragment),J=c(),C=w("p"),C.textContent=R,k=c(),M(Z.$$.fragment),F=c(),I=w("p"),I.innerHTML=Y,V=c(),M($.$$.fragment)},l(r){t=T(r,"P",{"data-svelte-h":!0}),_(t)!=="svelte-e92fpp"&&(t.innerHTML=f),e=o(r),y(i.$$.fragment,r),J=o(r),C=T(r,"P",{"data-svelte-h":!0}),_(C)!=="svelte-jwtn5w"&&(C.textContent=R),k=o(r),y(Z.$$.fragment,r),F=o(r),I=T(r,"P",{"data-svelte-h":!0}),_(I)!=="svelte-1y0o568"&&(I.innerHTML=Y),V=o(r),y($.$$.fragment,r)},m(r,U){l(r,t,U),l(r,e,U),j(i,r,U),l(r,J,U),l(r,C,U),l(r,k,U),j(Z,r,U),l(r,F,U),l(r,I,U),l(r,V,U),j($,r,U),x=!0},p:fs,i(r){x||(u(i.$$.fragment,r),u(Z.$$.fragment,r),u($.$$.fragment,r),x=!0)},o(r){b(i.$$.fragment,r),b(Z.$$.fragment,r),b($.$$.fragment,r),x=!1},d(r){r&&(a(t),a(e),a(J),a(C),a(k),a(F),a(I),a(V)),g(i,r),g(Z,r),g($,r)}}}function dl(X){let t,f;return t=new Bs({props:{$$slots:{default:[gl]},$$scope:{ctx:X}}}),{c(){M(t.$$.fragment)},l(e){y(t.$$.fragment,e)},m(e,i){j(t,e,i),f=!0},p(e,i){const J={};i&2&&(J.$$scope={dirty:i,ctx:e}),t.$set(J)},i(e){f||(u(t.$$.fragment,e),f=!0)},o(e){b(t.$$.fragment,e),f=!1},d(e){g(t,e)}}}function Jl(X){let t,f,e,i,J,C,R,k,Z,F,I,Y=`画像分類では、画像にラベルまたはクラスを割り当てます。テキストや音声の分類とは異なり、入力は
画像を構成するピクセル値。損傷の検出など、画像分類には多くの用途があります
自然災害の後、作物の健康状態を監視したり、病気の兆候がないか医療画像をスクリーニングしたりするのに役立ちます。`,V,$,x="このガイドでは、次の方法を説明します。",r,U,z='<li><a href="https://huggingface.co/datasets/food101" rel="nofollow">Food-101</a> データセットの <a href="model_doc/vit">ViT</a> を微調整して、画像内の食品を分類します。</li> <li>微調整したモデルを推論に使用します。</li>',G,N,p,d,E="始める前に、必要なライブラリがすべてインストールされていることを確認してください。",h,B,K,Q,hs="Hugging Face アカウントにログインして、モデルをアップロードしてコミュニティと共有することをお勧めします。プロンプトが表示されたら、トークンを入力してログインします。",L,D,O,H,ss,S,Ns=`Datasets、🤗 データセット ライブラリから Food-101 データセットの小さいサブセットを読み込みます。これにより、次の機会が得られます
完全なデータセットのトレーニングにさらに時間を費やす前に、実験してすべてが機能することを確認してください。`,as,A,ls,q,zs="<code>train_test_split</code> メソッドを使用して、データセットの <code>train</code> 分割をトレイン セットとテスト セットに分割します。",ts,m,v,P,_a="次に、例を見てみましょう。",Qs,Ms,Hs,ys,Za="データセット内の各例には 2 つのフィールドがあります。",Ss,js,Ca="<li><code>image</code>: 食品の PIL 画像</li> <li><code>label</code>: 食品のラベルクラス</li>",As,us,Ia=`モデルがラベル ID からラベル名を取得しやすくするために、ラベル名をマップする辞書を作成します。
整数への変換、またはその逆:`,qs,bs,Ls,gs,va="これで、ラベル ID をラベル名に変換できるようになりました。",Ds,ds,Ps,Js,Ks,ws,ka="次のステップでは、ViT 画像プロセッサをロードして画像をテンソルに処理します。",Os,Ts,sa,es,aa,ns,la,$s,ta,Us,Ga=`トレーニング中にメトリクスを含めると、多くの場合、モデルのパフォーマンスを評価するのに役立ちます。すぐにロードできます
🤗 <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> ライブラリを使用した評価方法。このタスクでは、ロードします
<a href="https://huggingface.co/spaces/evaluate-metric/accuracy" rel="nofollow">accuracy</a> 指標 (詳細については、🤗 評価 <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">クイック ツアー</a> を参照してくださいメトリクスをロードして計算する方法):`,ea,_s,na,Zs,Wa="次に、予測とラベルを <code>compute</code> に渡して精度を計算する関数を作成します。",pa,Cs,ra,Is,Xa="これで <code>compute_metrics</code>関数の準備が整いました。トレーニングを設定するときにこの関数に戻ります。",ma,vs,ca,ps,oa,rs,ia,ms,fa,ks,ha,Gs,Va="モデルを微調整したので、それを推論に使用できるようになりました。",Ma,Ws,Ra="推論を実行したい画像を読み込みます。",ya,Xs,ja,cs,Fa='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/beignets-task-guide.png" alt="image of beignets"/>',ua,Vs,Ya='推論用に微調整されたモデルを試す最も簡単な方法は、それを <a href="/docs/transformers/main/ja/main_classes/pipelines#transformers.pipeline">pipeline()</a> で使用することです。モデルを使用して画像分類用の<code>pipeline</code>をインスタンス化し、それに画像を渡します。',ba,Rs,ga,Fs,xa="必要に応じて、<code>pipeline</code>の結果を手動で複製することもできます。",da,os,Ja,is,wa,Es,Ta;return J=new xs({props:{title:"Image classification",local:"image-classification",headingTag:"h1"}}),R=new el({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/image_classification.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/pytorch/image_classification.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/tensorflow/image_classification.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/image_classification.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/pytorch/image_classification.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/tensorflow/image_classification.ipynb"}]}}),Z=new tl({props:{id:"tjAIM7BOYhw"}}),N=new $a({props:{$$slots:{default:[nl]},$$scope:{ctx:X}}}),B=new W({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGU=",highlighted:"pip install transformers datasets evaluate",wrap:!1}}),D=new W({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),H=new xs({props:{title:"Load Food-101 dataset",local:"load-food-101-dataset",headingTag:"h2"}}),A=new W({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZm9vZCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJmb29kMTAxJTIyJTJDJTIwc3BsaXQlM0QlMjJ0cmFpbiU1QiUzQTUwMDAlNUQlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>food = load_dataset(<span class="hljs-string">&quot;food101&quot;</span>, split=<span class="hljs-string">&quot;train[:5000]&quot;</span>)`,wrap:!1}}),m=new W({props:{code:"Zm9vZCUyMCUzRCUyMGZvb2QudHJhaW5fdGVzdF9zcGxpdCh0ZXN0X3NpemUlM0QwLjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>food = food.train_test_split(test_size=<span class="hljs-number">0.2</span>)',wrap:!1}}),Ms=new W({props:{code:"Zm9vZCU1QiUyMnRyYWluJTIyJTVEJTVCMCU1RA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>food[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;image&#x27;</span>: &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at <span class="hljs-number">0x7F52AFC8AC50</span>&gt;,
 <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-number">79</span>}`,wrap:!1}}),bs=new W({props:{code:"bGFiZWxzJTIwJTNEJTIwZm9vZCU1QiUyMnRyYWluJTIyJTVELmZlYXR1cmVzJTVCJTIybGFiZWwlMjIlNUQubmFtZXMlMEFsYWJlbDJpZCUyQyUyMGlkMmxhYmVsJTIwJTNEJTIwZGljdCgpJTJDJTIwZGljdCgpJTBBZm9yJTIwaSUyQyUyMGxhYmVsJTIwaW4lMjBlbnVtZXJhdGUobGFiZWxzKSUzQSUwQSUyMCUyMCUyMCUyMGxhYmVsMmlkJTVCbGFiZWwlNUQlMjAlM0QlMjBzdHIoaSklMEElMjAlMjAlMjAlMjBpZDJsYWJlbCU1QnN0cihpKSU1RCUyMCUzRCUyMGxhYmVs",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = food[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">&quot;label&quot;</span>].names
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id, id2label = <span class="hljs-built_in">dict</span>(), <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):
<span class="hljs-meta">... </span>    label2id[label] = <span class="hljs-built_in">str</span>(i)
<span class="hljs-meta">... </span>    id2label[<span class="hljs-built_in">str</span>(i)] = label`,wrap:!1}}),ds=new W({props:{code:"aWQybGFiZWwlNUJzdHIoNzkpJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>id2label[<span class="hljs-built_in">str</span>(<span class="hljs-number">79</span>)]
<span class="hljs-string">&#x27;prime_rib&#x27;</span>`,wrap:!1}}),Js=new xs({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),Ts=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUwQSUwQWNoZWNrcG9pbnQlMjAlM0QlMjAlMjJnb29nbGUlMkZ2aXQtYmFzZS1wYXRjaDE2LTIyNC1pbjIxayUyMiUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;google/vit-base-patch16-224-in21k&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(checkpoint)`,wrap:!1}}),es=new Ys({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[rl]},$$scope:{ctx:X}}}),ns=new Ys({props:{pytorch:!1,tensorflow:!0,jax:!1,$$slots:{tensorflow:[cl]},$$scope:{ctx:X}}}),$s=new xs({props:{title:"Evaluate",local:"evaluate",headingTag:"h2"}}),_s=new W({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFhY2N1cmFjeSUyMCUzRCUyMGV2YWx1YXRlLmxvYWQoJTIyYWNjdXJhY3klMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`,wrap:!1}}),Cs=new W({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUyQyUyMGxhYmVscyUyMCUzRCUyMGV2YWxfcHJlZCUwQSUyMCUyMCUyMCUyMHByZWRpY3Rpb25zJTIwJTNEJTIwbnAuYXJnbWF4KHByZWRpY3Rpb25zJTJDJTIwYXhpcyUzRDEpJTBBJTIwJTIwJTIwJTIwcmV0dXJuJTIwYWNjdXJhY3kuY29tcHV0ZShwcmVkaWN0aW9ucyUzRHByZWRpY3Rpb25zJTJDJTIwcmVmZXJlbmNlcyUzRGxhYmVscyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    predictions, labels = eval_pred
<span class="hljs-meta">... </span>    predictions = np.argmax(predictions, axis=<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> accuracy.compute(predictions=predictions, references=labels)`,wrap:!1}}),vs=new xs({props:{title:"Train",local:"train",headingTag:"h2"}}),ps=new Ys({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[fl]},$$scope:{ctx:X}}}),rs=new Ys({props:{pytorch:!1,tensorflow:!0,jax:!1,$$slots:{tensorflow:[yl]},$$scope:{ctx:X}}}),ms=new $a({props:{$$slots:{default:[jl]},$$scope:{ctx:X}}}),ks=new xs({props:{title:"Inference",local:"inference",headingTag:"h2"}}),Xs=new W({props:{code:"ZHMlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyZm9vZDEwMSUyMiUyQyUyMHNwbGl0JTNEJTIydmFsaWRhdGlvbiU1QiUzQTEwJTVEJTIyKSUwQWltYWdlJTIwJTNEJTIwZHMlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>ds = load_dataset(<span class="hljs-string">&quot;food101&quot;</span>, split=<span class="hljs-string">&quot;validation[:10]&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = ds[<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]`,wrap:!1}}),Rs=new W({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMmltYWdlLWNsYXNzaWZpY2F0aW9uJTIyJTJDJTIwbW9kZWwlM0QlMjJteV9hd2Vzb21lX2Zvb2RfbW9kZWwlMjIpJTBBY2xhc3NpZmllcihpbWFnZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;image-classification&quot;</span>, model=<span class="hljs-string">&quot;my_awesome_food_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(image)
[{<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.31856709718704224</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;beignets&#x27;</span>},
 {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.015232225880026817</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;bruschetta&#x27;</span>},
 {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.01519392803311348</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;chicken_wings&#x27;</span>},
 {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.013022331520915031</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;pork_chop&#x27;</span>},
 {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.012728818692266941</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;prime_rib&#x27;</span>}]`,wrap:!1}}),os=new Ys({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[bl]},$$scope:{ctx:X}}}),is=new Ys({props:{pytorch:!1,tensorflow:!0,jax:!1,$$slots:{tensorflow:[dl]},$$scope:{ctx:X}}}),{c(){t=w("meta"),f=c(),e=w("p"),i=c(),M(J.$$.fragment),C=c(),M(R.$$.fragment),k=c(),M(Z.$$.fragment),F=c(),I=w("p"),I.textContent=Y,V=c(),$=w("p"),$.textContent=x,r=c(),U=w("ol"),U.innerHTML=z,G=c(),M(N.$$.fragment),p=c(),d=w("p"),d.textContent=E,h=c(),M(B.$$.fragment),K=c(),Q=w("p"),Q.textContent=hs,L=c(),M(D.$$.fragment),O=c(),M(H.$$.fragment),ss=c(),S=w("p"),S.textContent=Ns,as=c(),M(A.$$.fragment),ls=c(),q=w("p"),q.innerHTML=zs,ts=c(),M(m.$$.fragment),v=c(),P=w("p"),P.textContent=_a,Qs=c(),M(Ms.$$.fragment),Hs=c(),ys=w("p"),ys.textContent=Za,Ss=c(),js=w("ul"),js.innerHTML=Ca,As=c(),us=w("p"),us.textContent=Ia,qs=c(),M(bs.$$.fragment),Ls=c(),gs=w("p"),gs.textContent=va,Ds=c(),M(ds.$$.fragment),Ps=c(),M(Js.$$.fragment),Ks=c(),ws=w("p"),ws.textContent=ka,Os=c(),M(Ts.$$.fragment),sa=c(),M(es.$$.fragment),aa=c(),M(ns.$$.fragment),la=c(),M($s.$$.fragment),ta=c(),Us=w("p"),Us.innerHTML=Ga,ea=c(),M(_s.$$.fragment),na=c(),Zs=w("p"),Zs.innerHTML=Wa,pa=c(),M(Cs.$$.fragment),ra=c(),Is=w("p"),Is.innerHTML=Xa,ma=c(),M(vs.$$.fragment),ca=c(),M(ps.$$.fragment),oa=c(),M(rs.$$.fragment),ia=c(),M(ms.$$.fragment),fa=c(),M(ks.$$.fragment),ha=c(),Gs=w("p"),Gs.textContent=Va,Ma=c(),Ws=w("p"),Ws.textContent=Ra,ya=c(),M(Xs.$$.fragment),ja=c(),cs=w("div"),cs.innerHTML=Fa,ua=c(),Vs=w("p"),Vs.innerHTML=Ya,ba=c(),M(Rs.$$.fragment),ga=c(),Fs=w("p"),Fs.innerHTML=xa,da=c(),M(os.$$.fragment),Ja=c(),M(is.$$.fragment),wa=c(),Es=w("p"),this.h()},l(s){const n=Oa("svelte-u9bgzb",document.head);t=T(n,"META",{name:!0,content:!0}),n.forEach(a),f=o(s),e=T(s,"P",{}),qa(e).forEach(a),i=o(s),y(J.$$.fragment,s),C=o(s),y(R.$$.fragment,s),k=o(s),y(Z.$$.fragment,s),F=o(s),I=T(s,"P",{"data-svelte-h":!0}),_(I)!=="svelte-33aqnx"&&(I.textContent=Y),V=o(s),$=T(s,"P",{"data-svelte-h":!0}),_($)!=="svelte-w5jzhi"&&($.textContent=x),r=o(s),U=T(s,"OL",{"data-svelte-h":!0}),_(U)!=="svelte-1y90kv1"&&(U.innerHTML=z),G=o(s),y(N.$$.fragment,s),p=o(s),d=T(s,"P",{"data-svelte-h":!0}),_(d)!=="svelte-1lya3k8"&&(d.textContent=E),h=o(s),y(B.$$.fragment,s),K=o(s),Q=T(s,"P",{"data-svelte-h":!0}),_(Q)!=="svelte-1bh3yv"&&(Q.textContent=hs),L=o(s),y(D.$$.fragment,s),O=o(s),y(H.$$.fragment,s),ss=o(s),S=T(s,"P",{"data-svelte-h":!0}),_(S)!=="svelte-1ncb48h"&&(S.textContent=Ns),as=o(s),y(A.$$.fragment,s),ls=o(s),q=T(s,"P",{"data-svelte-h":!0}),_(q)!=="svelte-m4w3yq"&&(q.innerHTML=zs),ts=o(s),y(m.$$.fragment,s),v=o(s),P=T(s,"P",{"data-svelte-h":!0}),_(P)!=="svelte-1r6oj5w"&&(P.textContent=_a),Qs=o(s),y(Ms.$$.fragment,s),Hs=o(s),ys=T(s,"P",{"data-svelte-h":!0}),_(ys)!=="svelte-24qbwx"&&(ys.textContent=Za),Ss=o(s),js=T(s,"UL",{"data-svelte-h":!0}),_(js)!=="svelte-8eiv2s"&&(js.innerHTML=Ca),As=o(s),us=T(s,"P",{"data-svelte-h":!0}),_(us)!=="svelte-ts333s"&&(us.textContent=Ia),qs=o(s),y(bs.$$.fragment,s),Ls=o(s),gs=T(s,"P",{"data-svelte-h":!0}),_(gs)!=="svelte-wrnhuq"&&(gs.textContent=va),Ds=o(s),y(ds.$$.fragment,s),Ps=o(s),y(Js.$$.fragment,s),Ks=o(s),ws=T(s,"P",{"data-svelte-h":!0}),_(ws)!=="svelte-13jkt14"&&(ws.textContent=ka),Os=o(s),y(Ts.$$.fragment,s),sa=o(s),y(es.$$.fragment,s),aa=o(s),y(ns.$$.fragment,s),la=o(s),y($s.$$.fragment,s),ta=o(s),Us=T(s,"P",{"data-svelte-h":!0}),_(Us)!=="svelte-1x40z70"&&(Us.innerHTML=Ga),ea=o(s),y(_s.$$.fragment,s),na=o(s),Zs=T(s,"P",{"data-svelte-h":!0}),_(Zs)!=="svelte-o90xg4"&&(Zs.innerHTML=Wa),pa=o(s),y(Cs.$$.fragment,s),ra=o(s),Is=T(s,"P",{"data-svelte-h":!0}),_(Is)!=="svelte-7bx6io"&&(Is.innerHTML=Xa),ma=o(s),y(vs.$$.fragment,s),ca=o(s),y(ps.$$.fragment,s),oa=o(s),y(rs.$$.fragment,s),ia=o(s),y(ms.$$.fragment,s),fa=o(s),y(ks.$$.fragment,s),ha=o(s),Gs=T(s,"P",{"data-svelte-h":!0}),_(Gs)!=="svelte-cyrfc8"&&(Gs.textContent=Va),Ma=o(s),Ws=T(s,"P",{"data-svelte-h":!0}),_(Ws)!=="svelte-14onr13"&&(Ws.textContent=Ra),ya=o(s),y(Xs.$$.fragment,s),ja=o(s),cs=T(s,"DIV",{class:!0,"data-svelte-h":!0}),_(cs)!=="svelte-pnh0xy"&&(cs.innerHTML=Fa),ua=o(s),Vs=T(s,"P",{"data-svelte-h":!0}),_(Vs)!=="svelte-hu6m5y"&&(Vs.innerHTML=Ya),ba=o(s),y(Rs.$$.fragment,s),ga=o(s),Fs=T(s,"P",{"data-svelte-h":!0}),_(Fs)!=="svelte-p649vi"&&(Fs.innerHTML=xa),da=o(s),y(os.$$.fragment,s),Ja=o(s),y(is.$$.fragment,s),wa=o(s),Es=T(s,"P",{}),qa(Es).forEach(a),this.h()},h(){Ua(t,"name","hf:doc:metadata"),Ua(t,"content",wl),Ua(cs,"class","flex justify-center")},m(s,n){sl(document.head,t),l(s,f,n),l(s,e,n),l(s,i,n),j(J,s,n),l(s,C,n),j(R,s,n),l(s,k,n),j(Z,s,n),l(s,F,n),l(s,I,n),l(s,V,n),l(s,$,n),l(s,r,n),l(s,U,n),l(s,G,n),j(N,s,n),l(s,p,n),l(s,d,n),l(s,h,n),j(B,s,n),l(s,K,n),l(s,Q,n),l(s,L,n),j(D,s,n),l(s,O,n),j(H,s,n),l(s,ss,n),l(s,S,n),l(s,as,n),j(A,s,n),l(s,ls,n),l(s,q,n),l(s,ts,n),j(m,s,n),l(s,v,n),l(s,P,n),l(s,Qs,n),j(Ms,s,n),l(s,Hs,n),l(s,ys,n),l(s,Ss,n),l(s,js,n),l(s,As,n),l(s,us,n),l(s,qs,n),j(bs,s,n),l(s,Ls,n),l(s,gs,n),l(s,Ds,n),j(ds,s,n),l(s,Ps,n),j(Js,s,n),l(s,Ks,n),l(s,ws,n),l(s,Os,n),j(Ts,s,n),l(s,sa,n),j(es,s,n),l(s,aa,n),j(ns,s,n),l(s,la,n),j($s,s,n),l(s,ta,n),l(s,Us,n),l(s,ea,n),j(_s,s,n),l(s,na,n),l(s,Zs,n),l(s,pa,n),j(Cs,s,n),l(s,ra,n),l(s,Is,n),l(s,ma,n),j(vs,s,n),l(s,ca,n),j(ps,s,n),l(s,oa,n),j(rs,s,n),l(s,ia,n),j(ms,s,n),l(s,fa,n),j(ks,s,n),l(s,ha,n),l(s,Gs,n),l(s,Ma,n),l(s,Ws,n),l(s,ya,n),j(Xs,s,n),l(s,ja,n),l(s,cs,n),l(s,ua,n),l(s,Vs,n),l(s,ba,n),j(Rs,s,n),l(s,ga,n),l(s,Fs,n),l(s,da,n),j(os,s,n),l(s,Ja,n),j(is,s,n),l(s,wa,n),l(s,Es,n),Ta=!0},p(s,[n]){const Ba={};n&2&&(Ba.$$scope={dirty:n,ctx:s}),N.$set(Ba);const Na={};n&2&&(Na.$$scope={dirty:n,ctx:s}),es.$set(Na);const za={};n&2&&(za.$$scope={dirty:n,ctx:s}),ns.$set(za);const Ea={};n&2&&(Ea.$$scope={dirty:n,ctx:s}),ps.$set(Ea);const Qa={};n&2&&(Qa.$$scope={dirty:n,ctx:s}),rs.$set(Qa);const Ha={};n&2&&(Ha.$$scope={dirty:n,ctx:s}),ms.$set(Ha);const Sa={};n&2&&(Sa.$$scope={dirty:n,ctx:s}),os.$set(Sa);const Aa={};n&2&&(Aa.$$scope={dirty:n,ctx:s}),is.$set(Aa)},i(s){Ta||(u(J.$$.fragment,s),u(R.$$.fragment,s),u(Z.$$.fragment,s),u(N.$$.fragment,s),u(B.$$.fragment,s),u(D.$$.fragment,s),u(H.$$.fragment,s),u(A.$$.fragment,s),u(m.$$.fragment,s),u(Ms.$$.fragment,s),u(bs.$$.fragment,s),u(ds.$$.fragment,s),u(Js.$$.fragment,s),u(Ts.$$.fragment,s),u(es.$$.fragment,s),u(ns.$$.fragment,s),u($s.$$.fragment,s),u(_s.$$.fragment,s),u(Cs.$$.fragment,s),u(vs.$$.fragment,s),u(ps.$$.fragment,s),u(rs.$$.fragment,s),u(ms.$$.fragment,s),u(ks.$$.fragment,s),u(Xs.$$.fragment,s),u(Rs.$$.fragment,s),u(os.$$.fragment,s),u(is.$$.fragment,s),Ta=!0)},o(s){b(J.$$.fragment,s),b(R.$$.fragment,s),b(Z.$$.fragment,s),b(N.$$.fragment,s),b(B.$$.fragment,s),b(D.$$.fragment,s),b(H.$$.fragment,s),b(A.$$.fragment,s),b(m.$$.fragment,s),b(Ms.$$.fragment,s),b(bs.$$.fragment,s),b(ds.$$.fragment,s),b(Js.$$.fragment,s),b(Ts.$$.fragment,s),b(es.$$.fragment,s),b(ns.$$.fragment,s),b($s.$$.fragment,s),b(_s.$$.fragment,s),b(Cs.$$.fragment,s),b(vs.$$.fragment,s),b(ps.$$.fragment,s),b(rs.$$.fragment,s),b(ms.$$.fragment,s),b(ks.$$.fragment,s),b(Xs.$$.fragment,s),b(Rs.$$.fragment,s),b(os.$$.fragment,s),b(is.$$.fragment,s),Ta=!1},d(s){s&&(a(f),a(e),a(i),a(C),a(k),a(F),a(I),a(V),a($),a(r),a(U),a(G),a(p),a(d),a(h),a(K),a(Q),a(L),a(O),a(ss),a(S),a(as),a(ls),a(q),a(ts),a(v),a(P),a(Qs),a(Hs),a(ys),a(Ss),a(js),a(As),a(us),a(qs),a(Ls),a(gs),a(Ds),a(Ps),a(Ks),a(ws),a(Os),a(sa),a(aa),a(la),a(ta),a(Us),a(ea),a(na),a(Zs),a(pa),a(ra),a(Is),a(ma),a(ca),a(oa),a(ia),a(fa),a(ha),a(Gs),a(Ma),a(Ws),a(ya),a(ja),a(cs),a(ua),a(Vs),a(ba),a(ga),a(Fs),a(da),a(Ja),a(wa),a(Es)),a(t),g(J,s),g(R,s),g(Z,s),g(N,s),g(B,s),g(D,s),g(H,s),g(A,s),g(m,s),g(Ms,s),g(bs,s),g(ds,s),g(Js,s),g(Ts,s),g(es,s),g(ns,s),g($s,s),g(_s,s),g(Cs,s),g(vs,s),g(ps,s),g(rs,s),g(ms,s),g(ks,s),g(Xs,s),g(Rs,s),g(os,s),g(is,s)}}}const wl='{"title":"Image classification","local":"image-classification","sections":[{"title":"Load Food-101 dataset","local":"load-food-101-dataset","sections":[],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[],"depth":2},{"title":"Evaluate","local":"evaluate","sections":[],"depth":2},{"title":"Train","local":"train","sections":[],"depth":2},{"title":"Inference","local":"inference","sections":[],"depth":2}],"depth":1}';function Tl(X){return Da(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Gl extends Pa{constructor(t){super(),Ka(this,t,Tl,Jl,La,{})}}export{Gl as component};
