import{s as _t,o as Tt,n as dt}from"../chunks/scheduler.9bc65507.js";import{S as Pt,i as vt,g as o,s as r,r as M,A as xt,h as s,f as n,c as l,j as ht,u as H,x as m,k as $t,y as gt,a as i,v as q,d as S,t as U,w as X}from"../chunks/index.707bf1b6.js";import{T as Ct}from"../chunks/Tip.c2ecdbf4.js";import{H as J}from"../chunks/Heading.342b1fa6.js";function yt(z){let a,_="PyTorch >= 1.14.0の場合、jitモードはjit.traceでdict入力がサポートされているため、予測と評価に任意のモデルに利益をもたらす可能性があります。",u,p,h="PyTorch < 1.14.0の場合、jitモードはforwardパラメーターの順序がjit.traceのタプル入力の順序と一致するモデルに利益をもたらす可能性があります（質問応答モデルなど）。jit.traceがタプル入力の順序と一致しない場合、テキスト分類モデルなど、jit.traceは失敗し、これをフォールバックさせるために例外でキャッチしています。ログはユーザーに通知するために使用されます。";return{c(){a=o("p"),a.textContent=_,u=r(),p=o("p"),p.textContent=h},l(f){a=s(f,"P",{"data-svelte-h":!0}),m(a)!=="svelte-uj65sm"&&(a.textContent=_),u=l(f),p=s(f,"P",{"data-svelte-h":!0}),m(p)!=="svelte-1mjd070"&&(p.textContent=h)},m(f,c){i(f,a,c),i(f,u,c),i(f,p,c)},p:dt,d(f){f&&(n(a),n(u),n(p))}}}function jt(z){let a,_,u,p,h,f,c,it="このガイドは、CPU上で大規模なモデルの効率的な推論に焦点を当てています。",A,T,B,d,rt='最近、テキスト、画像、および音声モデルのCPU上での高速な推論のために<code>BetterTransformer</code>を統合しました。詳細については、この統合に関するドキュメンテーションを<a href="https://huggingface.co/docs/optimum/bettertransformer/overview" rel="nofollow">こちら</a>で確認してください。',F,P,N,v,lt=`TorchScriptは、PyTorchコードからシリアライズ可能で最適化可能なモデルを作成する方法です。任意のTorchScriptプログラムは、Python依存性のないプロセスで保存およびロードできます。
デフォルトのイーガーモードと比較して、PyTorchのjitモードは通常、オペレーターフュージョンなどの最適化手法によりモデル推論のパフォーマンスが向上します。`,G,x,at='TorchScriptの簡単な紹介については、<a href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#tracing-modules" rel="nofollow">PyTorch TorchScriptチュートリアル</a>を参照してください。',R,g,D,C,ot="Intel® Extension for PyTorchは、Transformersシリーズモデルのjitモードにさらなる最適化を提供します。Intel® Extension for PyTorchをjitモードで使用することを強くお勧めします。Transformersモデルからよく使用されるオペレーターパターンのいくつかは、既にIntel® Extension for PyTorchでjitモードのフュージョンに対応しています。これらのフュージョンパターン（Multi-head-attentionフュージョン、Concat Linear、Linear+Add、Linear+Gelu、Add+LayerNormフュージョンなど）は有効でパフォーマンスが良いです。フュージョンの利点は、ユーザーに透過的に提供されます。分析によれば、最も人気のある質問応答、テキスト分類、トークン分類のNLPタスクの約70％が、これらのフュージョンパターンを使用してFloat32精度とBFloat16混合精度の両方でパフォーマンスの利点を得ることができます。",K,y,st='<a href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/graph_optimization.html" rel="nofollow">IPEXグラフ最適化の詳細情報</a>を確認してください。',O,j,Q,w,ft='IPEXのリリースはPyTorchに従っています。<a href="https://intel.github.io/intel-extension-for-pytorch/" rel="nofollow">IPEXのインストール方法</a>を確認してください。',V,I,W,L,pt="Trainerで評価または予測のためにJITモードを有効にするには、ユーザーはTrainerコマンド引数に<code>jit_mode_eval</code>を追加する必要があります。",Y,$,Z,b,mt='<a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering" rel="nofollow">Transformers質問応答の使用例</a>を参考にしてください。',tt,E,ct=`<li><p>Inference using jit mode on CPU:</p> <pre>python run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
<b>--jit_mode_eval </b></pre></li> <li><p>Inference with IPEX using jit mode on CPU:</p> <pre>python run_qa.py \\
--model_name_or_path csarron/bert-base-uncased-squad-v1 \\
--dataset_name squad \\
--do_eval \\
--max_seq_length 384 \\
--doc_stride 128 \\
--output_dir /tmp/ \\
--no_cuda \\
<b>--use_ipex \\</b>
<b>--jit_mode_eval</b></pre></li>`,et,k,nt;return h=new J({props:{title:"Efficient Inference on CPU",local:"efficient-inference-on-cpu",headingTag:"h1"}}),T=new J({props:{title:"BetterTransformer for faster inference",local:"bettertransformer-for-faster-inference",headingTag:"h2"}}),P=new J({props:{title:"PyTorch JITモード（TorchScript）",local:"pytorch-jitモードtorchscript",headingTag:"h2"}}),g=new J({props:{title:"JITモードでのIPEXグラフ最適化",local:"jitモードでのipexグラフ最適化",headingTag:"h3"}}),j=new J({props:{title:"IPEX installation:",local:"ipex-installation",headingTag:"h4"}}),I=new J({props:{title:"Usage of JIT-mode",local:"usage-of-jit-mode",headingTag:"h3"}}),$=new Ct({props:{warning:!0,$$slots:{default:[yt]},$$scope:{ctx:z}}}),{c(){a=o("meta"),_=r(),u=o("p"),p=r(),M(h.$$.fragment),f=r(),c=o("p"),c.textContent=it,A=r(),M(T.$$.fragment),B=r(),d=o("p"),d.innerHTML=rt,F=r(),M(P.$$.fragment),N=r(),v=o("p"),v.textContent=lt,G=r(),x=o("p"),x.innerHTML=at,R=r(),M(g.$$.fragment),D=r(),C=o("p"),C.textContent=ot,K=r(),y=o("p"),y.innerHTML=st,O=r(),M(j.$$.fragment),Q=r(),w=o("p"),w.innerHTML=ft,V=r(),M(I.$$.fragment),W=r(),L=o("p"),L.innerHTML=pt,Y=r(),M($.$$.fragment),Z=r(),b=o("p"),b.innerHTML=mt,tt=r(),E=o("ul"),E.innerHTML=ct,et=r(),k=o("p"),this.h()},l(t){const e=xt("svelte-u9bgzb",document.head);a=s(e,"META",{name:!0,content:!0}),e.forEach(n),_=l(t),u=s(t,"P",{}),ht(u).forEach(n),p=l(t),H(h.$$.fragment,t),f=l(t),c=s(t,"P",{"data-svelte-h":!0}),m(c)!=="svelte-173ykpk"&&(c.textContent=it),A=l(t),H(T.$$.fragment,t),B=l(t),d=s(t,"P",{"data-svelte-h":!0}),m(d)!=="svelte-1gv96n8"&&(d.innerHTML=rt),F=l(t),H(P.$$.fragment,t),N=l(t),v=s(t,"P",{"data-svelte-h":!0}),m(v)!=="svelte-s6mz7v"&&(v.textContent=lt),G=l(t),x=s(t,"P",{"data-svelte-h":!0}),m(x)!=="svelte-1vr7y5u"&&(x.innerHTML=at),R=l(t),H(g.$$.fragment,t),D=l(t),C=s(t,"P",{"data-svelte-h":!0}),m(C)!=="svelte-gbkgfl"&&(C.textContent=ot),K=l(t),y=s(t,"P",{"data-svelte-h":!0}),m(y)!=="svelte-qo6507"&&(y.innerHTML=st),O=l(t),H(j.$$.fragment,t),Q=l(t),w=s(t,"P",{"data-svelte-h":!0}),m(w)!=="svelte-1u3jvxp"&&(w.innerHTML=ft),V=l(t),H(I.$$.fragment,t),W=l(t),L=s(t,"P",{"data-svelte-h":!0}),m(L)!=="svelte-15m7esz"&&(L.innerHTML=pt),Y=l(t),H($.$$.fragment,t),Z=l(t),b=s(t,"P",{"data-svelte-h":!0}),m(b)!=="svelte-r9hc9d"&&(b.innerHTML=mt),tt=l(t),E=s(t,"UL",{"data-svelte-h":!0}),m(E)!=="svelte-143w2h7"&&(E.innerHTML=ct),et=l(t),k=s(t,"P",{}),ht(k).forEach(n),this.h()},h(){$t(a,"name","hf:doc:metadata"),$t(a,"content",wt)},m(t,e){gt(document.head,a),i(t,_,e),i(t,u,e),i(t,p,e),q(h,t,e),i(t,f,e),i(t,c,e),i(t,A,e),q(T,t,e),i(t,B,e),i(t,d,e),i(t,F,e),q(P,t,e),i(t,N,e),i(t,v,e),i(t,G,e),i(t,x,e),i(t,R,e),q(g,t,e),i(t,D,e),i(t,C,e),i(t,K,e),i(t,y,e),i(t,O,e),q(j,t,e),i(t,Q,e),i(t,w,e),i(t,V,e),q(I,t,e),i(t,W,e),i(t,L,e),i(t,Y,e),q($,t,e),i(t,Z,e),i(t,b,e),i(t,tt,e),i(t,E,e),i(t,et,e),i(t,k,e),nt=!0},p(t,[e]){const ut={};e&2&&(ut.$$scope={dirty:e,ctx:t}),$.$set(ut)},i(t){nt||(S(h.$$.fragment,t),S(T.$$.fragment,t),S(P.$$.fragment,t),S(g.$$.fragment,t),S(j.$$.fragment,t),S(I.$$.fragment,t),S($.$$.fragment,t),nt=!0)},o(t){U(h.$$.fragment,t),U(T.$$.fragment,t),U(P.$$.fragment,t),U(g.$$.fragment,t),U(j.$$.fragment,t),U(I.$$.fragment,t),U($.$$.fragment,t),nt=!1},d(t){t&&(n(_),n(u),n(p),n(f),n(c),n(A),n(B),n(d),n(F),n(N),n(v),n(G),n(x),n(R),n(D),n(C),n(K),n(y),n(O),n(Q),n(w),n(V),n(W),n(L),n(Y),n(Z),n(b),n(tt),n(E),n(et),n(k)),n(a),X(h,t),X(T,t),X(P,t),X(g,t),X(j,t),X(I,t),X($,t)}}}const wt='{"title":"Efficient Inference on CPU","local":"efficient-inference-on-cpu","sections":[{"title":"BetterTransformer for faster inference","local":"bettertransformer-for-faster-inference","sections":[],"depth":2},{"title":"PyTorch JITモード（TorchScript）","local":"pytorch-jitモードtorchscript","sections":[{"title":"JITモードでのIPEXグラフ最適化","local":"jitモードでのipexグラフ最適化","sections":[{"title":"IPEX installation:","local":"ipex-installation","sections":[],"depth":4}],"depth":3},{"title":"Usage of JIT-mode","local":"usage-of-jit-mode","sections":[],"depth":3}],"depth":2}],"depth":1}';function It(z){return Tt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ht extends Pt{constructor(a){super(),vt(this,a,It,jt,_t,{})}}export{Ht as component};
