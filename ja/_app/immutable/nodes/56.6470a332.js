import{s as ar,o as rr,n as fe}from"../chunks/scheduler.9bc65507.js";import{S as sr,i as ir,g as i,s as n,r as m,A as lr,h as l,f as o,c as a,j as w,u as p,x as _,k as $,y as s,a as r,v as f,d as u,t as h,w as g}from"../chunks/index.707bf1b6.js";import{T as pn}from"../chunks/Tip.c2ecdbf4.js";import{D as B}from"../chunks/Docstring.17db21ae.js";import{C as H}from"../chunks/CodeBlock.54a9f38d.js";import{E as fn}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as x}from"../chunks/Heading.342b1fa6.js";function dr(j){let d,y="Example:",b,k,v;return k=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBCYXJrTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzdW5vJTJGYmFyay1zbWFsbCUyMiklMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmstc21hbGwlMjIpJTBBJTBBJTIzJTIwVG8lMjBhZGQlMjBhJTIwdm9pY2UlMjBwcmVzZXQlMkMlMjB5b3UlMjBjYW4lMjBwYXNzJTIwJTYwdm9pY2VfcHJlc2V0JTYwJTIwdG8lMjAlNjBCYXJrUHJvY2Vzc29yLl9fY2FsbF9fKC4uLiklNjAlMEF2b2ljZV9wcmVzZXQlMjAlM0QlMjAlMjJ2MiUyRmVuX3NwZWFrZXJfNiUyMiUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTJDJTIwSSUyMG5lZWQlMjBoaW0lMjBpbiUyMG15JTIwbGlmZSUyMiUyQyUyMHZvaWNlX3ByZXNldCUzRHZvaWNlX3ByZXNldCklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwc2VtYW50aWNfbWF4X25ld190b2tlbnMlM0QxMDApJTBBYXVkaW9fYXJyYXklMjAlM0QlMjBhdWRpb19hcnJheS5jcHUoKS5udW1weSgpLnNxdWVlemUoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, BarkModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To add a voice preset, you can pass \`voice_preset\` to \`BarkProcessor.__call__(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>voice_preset = <span class="hljs-string">&quot;v2/en_speaker_6&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello, my dog is cute, I need him in my life&quot;</span>, voice_preset=voice_preset)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs, semantic_max_new_tokens=<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=n(),m(k.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),_(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(c),p(k.$$.fragment,c)},m(c,M){r(c,d,M),r(c,b,M),f(k,c,M),v=!0},p:fe,i(c){v||(u(k.$$.fragment,c),v=!0)},o(c){h(k.$$.fragment,c),v=!1},d(c){c&&(o(d),o(b)),g(k,c)}}}function cr(j){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),_(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:fe,d(b){b&&o(d)}}}function mr(j){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),_(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:fe,d(b){b&&o(d)}}}function pr(j){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),_(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:fe,d(b){b&&o(d)}}}function fr(j){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),_(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:fe,d(b){b&&o(d)}}}function ur(j){let d,y="Example:",b,k,v;return k=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtDb2Fyc2VDb25maWclMkMlMjBCYXJrQ29hcnNlTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQmFyayUyMHN1Yi1tb2R1bGUlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQmFya0NvYXJzZUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya0NvYXJzZU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkCoarseConfig, BarkCoarseModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkCoarseConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkCoarseModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=n(),m(k.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),_(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(c),p(k.$$.fragment,c)},m(c,M){r(c,d,M),r(c,b,M),f(k,c,M),v=!0},p:fe,i(c){v||(u(k.$$.fragment,c),v=!0)},o(c){h(k.$$.fragment,c),v=!1},d(c){c&&(o(d),o(b)),g(k,c)}}}function hr(j){let d,y="Example:",b,k,v;return k=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtGaW5lQ29uZmlnJTJDJTIwQmFya0ZpbmVNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCYXJrJTIwc3ViLW1vZHVsZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCYXJrRmluZUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya0ZpbmVNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkFineConfig, BarkFineModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkFineConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkFineModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=n(),m(k.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),_(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(c),p(k.$$.fragment,c)},m(c,M){r(c,d,M),r(c,b,M),f(k,c,M),v=!0},p:fe,i(c){v||(u(k.$$.fragment,c),v=!0)},o(c){h(k.$$.fragment,c),v=!1},d(c){c&&(o(d),o(b)),g(k,c)}}}function gr(j){let d,y="Example:",b,k,v;return k=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtTZW1hbnRpY0NvbmZpZyUyQyUyMEJhcmtTZW1hbnRpY01vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJhcmslMjBzdWItbW9kdWxlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEJhcmtTZW1hbnRpY0NvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya1NlbWFudGljTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkSemanticConfig, BarkSemanticModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkSemanticConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkSemanticModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=n(),m(k.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),_(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(c),p(k.$$.fragment,c)},m(c,M){r(c,d,M),r(c,b,M),f(k,c,M),v=!0},p:fe,i(c){v||(u(k.$$.fragment,c),v=!0)},o(c){h(k.$$.fragment,c),v=!1},d(c){c&&(o(d),o(b)),g(k,c)}}}function _r(j){let d,y,b,k,v,c,M,lo,ue,da='Bark は、<a href="https://github.com/suno-ai/bark" rel="nofollow">suno-ai/bark</a> で Suno AI によって提案されたトランスフォーマーベースのテキスト読み上げモデルです。',co,he,ca="Bark は 4 つの主要なモデルで構成されています。",mo,ge,ma='<li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> (‘テキスト’モデルとも呼ばれる): トークン化されたテキストを入力として受け取り、テキストの意味を捉えるセマンティック テキスト トークンを予測する因果的自己回帰変換モデル。</li> <li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a> (‘粗い音響’ モデルとも呼ばれる): <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> モデルの結果を入力として受け取る因果的自己回帰変換器。 EnCodec に必要な最初の 2 つのオーディオ コードブックを予測することを目的としています。</li> <li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> (‘微細音響’ モデル)、今回は非因果的オートエンコーダー トランスフォーマーで、以前のコードブック埋め込みの合計に基づいて最後のコードブックを繰り返し予測します。</li> <li><code>EncodecModel</code> からすべてのコードブック チャネルを予測したので、Bark はそれを使用して出力オーディオ配列をデコードします。</li>',po,_e,pa="最初の 3 つのモジュールはそれぞれ、特定の事前定義された音声に従って出力サウンドを調整するための条件付きスピーカー埋め込みをサポートできることに注意してください。",fo,be,uo,ke,fa="Bark は、コードを数行追加するだけで最適化でき、<strong>メモリ フットプリントが大幅に削減</strong>され、<strong>推論が高速化</strong>されます。",ho,ye,go,ve,ua="モデルを半精度でロードするだけで、推論を高速化し、メモリ使用量を 50% 削減できます。",_o,Me,bo,we,ko,$e,ha="Better Transformer は、内部でカーネル融合を実行する 🤗 最適な機能です。パフォーマンスを低下させることなく、速度を 20% ～ 30% 向上させることができます。モデルを 🤗 Better Transformer にエクスポートするのに必要なコードは 1 行だけです。",yo,Te,vo,Be,ga='この機能を使用する前に 🤗 Optimum をインストールする必要があることに注意してください。 <a href="https://huggingface.co/docs/optimum/installation" rel="nofollow">インストール方法はこちら</a>',Mo,Ce,wo,je,_a="前述したように、Bark は 4 つのサブモデルで構成されており、オーディオ生成中に順番に呼び出されます。言い換えれば、1 つのサブモデルが使用されている間、他のサブモデルはアイドル状態になります。",$o,xe,ba="CUDA デバイスを使用している場合、メモリ フットプリントの 80% 削減による恩恵を受ける簡単な解決策は、アイドル状態の GPU のサブモデルをオフロードすることです。この操作は CPU オフロードと呼ばれます。 1行のコードで使用できます。",To,Je,Bo,ze,ka='この機能を使用する前に、🤗 Accelerate をインストールする必要があることに注意してください。 <a href="https://huggingface.co/docs/accelerate/basic_tutorials/install" rel="nofollow">インストール方法はこちら</a>',Co,Ue,jo,Ie,ya="最適化手法を組み合わせて、CPU オフロード、半精度、🤗 Better Transformer をすべて一度に使用できます。",xo,We,Jo,Ze,va='推論最適化手法の詳細については、<a href="https://huggingface.co/docs/transformers/perf_infer_gpu_one" rel="nofollow">こちら</a> をご覧ください。',zo,Pe,Uo,Ne,Ma=`Suno は、多くの言語で音声プリセットのライブラリを提供しています <a href="https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c" rel="nofollow">こちら</a>。
これらのプリセットは、ハブ <a href="https://huggingface.co/suno/bark-small/tree/main/speaker_embeddings" rel="nofollow">こちら</a> または <a href="https://huggingface.co/suno/bark/tree/main/speaker_embeddings" rel="nofollow">こちら</a>。`,Io,Fe,Wo,Ge,wa="Bark は、非常にリアルな <strong>多言語</strong> 音声だけでなく、音楽、背景ノイズ、単純な効果音などの他の音声も生成できます。",Zo,He,Po,Se,$a="このモデルは、笑う、ため息、泣くなどの<strong>非言語コミュニケーション</strong>を生成することもできます。",No,Ve,Fo,Le,Ta="オーディオを保存するには、モデル設定と scipy ユーティリティからサンプル レートを取得するだけです。",Go,Ye,Ho,Ee,Ba=`このモデルは、<a href="https://huggingface.co/ylacombe" rel="nofollow">Yoach Lacombe (ylacombe)</a> および <a href="https://github.com/sanchit-gandhi" rel="nofollow">Sanchit Gandhi (sanchit-gandhi)</a> によって提供されました。
元のコードは <a href="https://github.com/suno-ai/bark" rel="nofollow">ここ</a> にあります。`,So,Xe,Vo,J,qe,un,Bt,Ca=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkModel">BarkModel</a>. It is used to instantiate a Bark
model according to the specified sub-models configurations, defining the model architecture.`,hn,Ct,ja=`Instantiating a configuration with the defaults will yield a similar configuration to that of the Bark
<a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a> architecture.`,gn,jt,xa=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,_n,R,Re,bn,xt,Ja='Instantiate a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkConfig">BarkConfig</a> (or a derived class) from bark sub-models configuration.',Lo,Qe,Yo,z,De,kn,Jt,za="Constructs a Bark processor which wraps a text tokenizer and optional Bark voice presets into a single processor.",yn,Q,Oe,vn,zt,Ua=`Main method to prepare for the model one or several sequences(s). This method forwards the <code>text</code> and <code>kwargs</code>
arguments to the AutoTokenizer’s <code>__call__()</code> to encode the text. The method also proposes a
voice preset which is a dictionary of arrays that conditions <code>Bark</code>’s output. <code>kwargs</code> arguments are forwarded
to the tokenizer and to <code>cached_file</code> method if <code>voice_preset</code> is a valid filename.`,Mn,D,Ae,wn,Ut,Ia="Instantiate a Bark processor associated with a pretrained model.",$n,O,Ke,Tn,It,Wa=`Saves the attributes of this processor (tokenizer…) in the specified directory so that it can be reloaded
using the <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkProcessor.from_pretrained">from_pretrained()</a> method.`,Eo,et,Xo,T,tt,Bn,Wt,Za="The full Bark model, a text-to-speech model composed of 4 sub-models:",Cn,Zt,Pa=`<li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> (also referred to as the ‘text’ model): a causal auto-regressive transformer model that
takes
as input tokenized text, and predicts semantic text tokens that capture the meaning of the text.</li> <li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a> (also refered to as the ‘coarse acoustics’ model), also a causal autoregressive transformer,
that takes into input the results of the last model. It aims at regressing the first two audio codebooks necessary
to <code>encodec</code>.</li> <li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> (the ‘fine acoustics’ model), this time a non-causal autoencoder transformer, which iteratively
predicts the last codebooks based on the sum of the previous codebooks embeddings.</li> <li>having predicted all the codebook channels from the <code>EncodecModel</code>, Bark uses it to decode the output audio
array.</li>`,jn,Pt,Na=`It should be noted that each of the first three modules can support conditional speaker embeddings to condition the
output sound according to specific predefined voice.`,xn,Nt,Fa=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Jn,Ft,Ga=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,zn,S,ot,Un,Gt,Ha="Generates audio from an input prompt and an additional optional <code>Bark</code> speaker prompt.",In,A,Wn,K,nt,Zn,Ht,Sa=`Offloads all sub-models to CPU using accelerate, reducing memory usage with a low impact on performance. This
method moves one whole sub-model at a time to the GPU when it is used, and the sub-model remains in GPU until
the next sub-model runs.`,qo,at,Ro,U,rt,Pn,St,Va=`Bark semantic (or text) model. It shares the same architecture as the coarse model.
It is a GPT-2 like autoregressive model with a language modeling head on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Nn,Vt,La=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Fn,V,st,Gn,Lt,Ya='The <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',Hn,ee,Qo,it,Do,I,lt,Sn,Yt,Ea=`Bark coarse acoustics model.
It shares the same architecture as the semantic (or text) model. It is a GPT-2 like autoregressive model with a
language modeling head on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Vn,Et,Xa=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ln,L,dt,Yn,Xt,qa='The <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',En,te,Oo,ct,Ao,W,mt,Xn,qt,Ra=`Bark fine acoustics model. It is a non-causal GPT-like model with <code>config.n_codes_total</code> embedding layers and
language modeling heads, one for each codebook.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,qn,Rt,Qa=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Rn,Y,pt,Qn,Qt,Da='The <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> forward method, overrides the <code>__call__</code> special method.',Dn,oe,Ko,ft,en,X,ut,On,E,ht,An,Dt,Oa='The <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',Kn,ne,tn,gt,on,Z,_t,ea,Ot,Aa=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,ta,At,Ka=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,oa,ae,nn,bt,an,P,kt,na,Kt,er=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,aa,eo,tr=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ra,re,rn,yt,sn,N,vt,sa,to,or=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,ia,oo,nr=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,la,se,ln,io,dn;return v=new x({props:{title:"Bark",local:"bark",headingTag:"h1"}}),M=new x({props:{title:"Overview",local:"overview",headingTag:"h2"}}),be=new x({props:{title:"Optimizing Bark",local:"optimizing-bark",headingTag:"h3"}}),ye=new x({props:{title:"Using half-precision",local:"using-half-precision",headingTag:"h4"}}),Me=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMGlmJTIwdG9yY2guY3VkYS5pc19hdmFpbGFibGUoKSUyMGVsc2UlMjAlMjJjcHUlMjIlMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmstc21hbGwlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYpLnRvKGRldmljZSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkModel
<span class="hljs-keyword">import</span> torch

device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>
model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>, torch_dtype=torch.float16).to(device)`,wrap:!1}}),we=new x({props:{title:"Using 🤗 Better Transformer",local:"using--better-transformer",headingTag:"h4"}}),Te=new H({props:{code:"bW9kZWwlMjAlM0QlMjAlMjBtb2RlbC50b19iZXR0ZXJ0cmFuc2Zvcm1lcigp",highlighted:"model =  model.to_bettertransformer()",wrap:!1}}),Ce=new x({props:{title:"Using CPU offload",local:"using-cpu-offload",headingTag:"h4"}}),Je=new H({props:{code:"bW9kZWwuZW5hYmxlX2NwdV9vZmZsb2FkKCk=",highlighted:"model.enable_cpu_offload()",wrap:!1}}),Ue=new x({props:{title:"Combining optimization techniques",local:"combining-optimization-techniques",headingTag:"h4"}}),We=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMGlmJTIwdG9yY2guY3VkYS5pc19hdmFpbGFibGUoKSUyMGVsc2UlMjAlMjJjcHUlMjIlMEElMEElMjMlMjBsb2FkJTIwaW4lMjBmcDE2JTBBbW9kZWwlMjAlM0QlMjBCYXJrTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnN1bm8lMkZiYXJrLXNtYWxsJTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byhkZXZpY2UpJTBBJTBBJTIzJTIwY29udmVydCUyMHRvJTIwYmV0dGVydHJhbnNmb3JtZXIlMEFtb2RlbCUyMCUzRCUyMEJldHRlclRyYW5zZm9ybWVyLnRyYW5zZm9ybShtb2RlbCUyQyUyMGtlZXBfb3JpZ2luYWxfbW9kZWwlM0RGYWxzZSklMEElMEElMjMlMjBlbmFibGUlMjBDUFUlMjBvZmZsb2FkJTBBbW9kZWwuZW5hYmxlX2NwdV9vZmZsb2FkKCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkModel
<span class="hljs-keyword">import</span> torch

device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

<span class="hljs-comment"># load in fp16</span>
model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>, torch_dtype=torch.float16).to(device)

<span class="hljs-comment"># convert to bettertransformer</span>
model = BetterTransformer.transform(model, keep_original_model=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># enable CPU offload</span>
model.enable_cpu_offload()`,wrap:!1}}),Pe=new x({props:{title:"Tips",local:"tips",headingTag:"h3"}}),Fe=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBCYXJrTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzdW5vJTJGYmFyayUyMiklMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmslMjIpJTBBJTBBdm9pY2VfcHJlc2V0JTIwJTNEJTIwJTIydjIlMkZlbl9zcGVha2VyXzYlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoJTIySGVsbG8lMkMlMjBteSUyMGRvZyUyMGlzJTIwY3V0ZSUyMiUyQyUyMHZvaWNlX3ByZXNldCUzRHZvaWNlX3ByZXNldCklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwYXVkaW9fYXJyYXkuY3B1KCkubnVtcHkoKS5zcXVlZXplKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, BarkModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;suno/bark&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>voice_preset = <span class="hljs-string">&quot;v2/en_speaker_6&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, voice_preset=voice_preset)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),He=new H({props:{code:"JTIzJTIwTXVsdGlsaW5ndWFsJTIwc3BlZWNoJTIwLSUyMHNpbXBsaWZpZWQlMjBDaGluZXNlJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKCUyMiVFNiU4MyU4QSVFNCVCQSVCQSVFNyU5QSU4NCVFRiVCQyU4MSVFNiU4OCU5MSVFNCVCQyU5QSVFOCVBRiVCNCVFNCVCOCVBRCVFNiU5NiU4NyUyMiklMEElMEElMjMlMjBNdWx0aWxpbmd1YWwlMjBzcGVlY2glMjAtJTIwRnJlbmNoJTIwLSUyMGxldCdzJTIwdXNlJTIwYSUyMHZvaWNlX3ByZXNldCUyMGFzJTIwd2VsbCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJJbmNyb3lhYmxlISUyMEplJTIwcGV1eCUyMGclQzMlQTluJUMzJUE5cmVyJTIwZHUlMjBzb24uJTIyJTJDJTIwdm9pY2VfcHJlc2V0JTNEJTIyZnJfc3BlYWtlcl81JTIyKSUwQSUwQSUyMyUyMEJhcmslMjBjYW4lMjBhbHNvJTIwZ2VuZXJhdGUlMjBtdXNpYy4lMjBZb3UlMjBjYW4lMjBoZWxwJTIwaXQlMjBvdXQlMjBieSUyMGFkZGluZyUyMG11c2ljJTIwbm90ZXMlMjBhcm91bmQlMjB5b3VyJTIwbHlyaWNzLiUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjIlRTIlOTklQUElMjBIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTIwJUUyJTk5JUFBJTIyKSUwQSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMpJTBBYXVkaW9fYXJyYXklMjAlM0QlMjBhdWRpb19hcnJheS5jcHUoKS5udW1weSgpLnNxdWVlemUoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multilingual speech - simplified Chinese</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;惊人的！我会说中文&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multilingual speech - French - let&#x27;s use a voice_preset as well</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Incroyable! Je peux générer du son.&quot;</span>, voice_preset=<span class="hljs-string">&quot;fr_speaker_5&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Bark can also generate music. You can help it out by adding music notes around your lyrics.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;♪ Hello, my dog is cute ♪&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),Ve=new H({props:{code:"JTIzJTIwQWRkaW5nJTIwbm9uLXNwZWVjaCUyMGN1ZXMlMjB0byUyMHRoZSUyMGlucHV0JTIwdGV4dCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJIZWxsbyUyMHVoJTIwLi4uJTIwJTVCY2xlYXJzJTIwdGhyb2F0JTVEJTJDJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjAlNUJsYXVnaHRlciU1RCUyMiklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwYXVkaW9fYXJyYXkuY3B1KCkubnVtcHkoKS5zcXVlZXplKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Adding non-speech cues to the input text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello uh ... [clears throat], my dog is cute [laughter]&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),Ye=new H({props:{code:"ZnJvbSUyMHNjaXB5LmlvLndhdmZpbGUlMjBpbXBvcnQlMjB3cml0ZSUyMGFzJTIwd3JpdGVfd2F2JTBBJTBBJTIzJTIwc2F2ZSUyMGF1ZGlvJTIwdG8lMjBkaXNrJTJDJTIwYnV0JTIwZmlyc3QlMjB0YWtlJTIwdGhlJTIwc2FtcGxlJTIwcmF0ZSUyMGZyb20lMjB0aGUlMjBtb2RlbCUyMGNvbmZpZyUwQXNhbXBsZV9yYXRlJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGlvbl9jb25maWcuc2FtcGxlX3JhdGUlMEF3cml0ZV93YXYoJTIyYmFya19nZW5lcmF0aW9uLndhdiUyMiUyQyUyMHNhbXBsZV9yYXRlJTJDJTIwYXVkaW9fYXJyYXkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> scipy.io.wavfile <span class="hljs-keyword">import</span> write <span class="hljs-keyword">as</span> write_wav

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save audio to disk, but first take the sample rate from the model config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sample_rate = model.generation_config.sample_rate
<span class="hljs-meta">&gt;&gt;&gt; </span>write_wav(<span class="hljs-string">&quot;bark_generation.wav&quot;</span>, sample_rate, audio_array)`,wrap:!1}}),Xe=new x({props:{title:"BarkConfig",local:"transformers.BarkConfig",headingTag:"h2"}}),qe=new B({props:{name:"class transformers.BarkConfig",anchor:"transformers.BarkConfig",parameters:[{name:"semantic_config",val:": Dict = None"},{name:"coarse_acoustics_config",val:": Dict = None"},{name:"fine_acoustics_config",val:": Dict = None"},{name:"codec_config",val:": Dict = None"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkConfig.semantic_config",description:`<strong>semantic_config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticConfig">BarkSemanticConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying semantic sub-model.`,name:"semantic_config"},{anchor:"transformers.BarkConfig.coarse_acoustics_config",description:`<strong>coarse_acoustics_config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseConfig">BarkCoarseConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying coarse acoustics sub-model.`,name:"coarse_acoustics_config"},{anchor:"transformers.BarkConfig.fine_acoustics_config",description:`<strong>fine_acoustics_config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineConfig">BarkFineConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying fine acoustics sub-model.`,name:"fine_acoustics_config"},{anchor:"transformers.BarkConfig.codec_config",description:`<strong>codec_config</strong> (<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoConfig">AutoConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying codec sub-model.</p>
<p>Example &#x2014;`,name:"codec_config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L219"}}),Re=new B({props:{name:"from_sub_model_configs",anchor:"transformers.BarkConfig.from_sub_model_configs",parameters:[{name:"semantic_config",val:": BarkSemanticConfig"},{name:"coarse_acoustics_config",val:": BarkCoarseConfig"},{name:"fine_acoustics_config",val:": BarkFineConfig"},{name:"codec_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L309",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkConfig"
>BarkConfig</a></p>
`}}),Qe=new x({props:{title:"BarkProcessor",local:"transformers.BarkProcessor",headingTag:"h2"}}),De=new B({props:{name:"class transformers.BarkProcessor",anchor:"transformers.BarkProcessor",parameters:[{name:"tokenizer",val:""},{name:"speaker_embeddings",val:" = None"}],parametersDescription:[{anchor:"transformers.BarkProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>) &#x2014;
An instance of <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>.`,name:"tokenizer"},{anchor:"transformers.BarkProcessor.speaker_embeddings",description:`<strong>speaker_embeddings</strong> (<code>Dict[Dict[str]]</code>, <em>optional</em>) &#x2014;
Optional nested speaker embeddings dictionary. The first level contains voice preset names (e.g
<code>&quot;en_speaker_4&quot;</code>). The second level contains <code>&quot;semantic_prompt&quot;</code>, <code>&quot;coarse_prompt&quot;</code> and <code>&quot;fine_prompt&quot;</code>
embeddings. The values correspond to the path of the corresponding <code>np.ndarray</code>. See
<a href="https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c" rel="nofollow">here</a> for
a list of <code>voice_preset_names</code>.`,name:"speaker_embeddings"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L34"}}),Oe=new B({props:{name:"__call__",anchor:"transformers.BarkProcessor.__call__",parameters:[{name:"text",val:" = None"},{name:"voice_preset",val:" = None"},{name:"return_tensors",val:" = 'pt'"},{name:"max_length",val:" = 256"},{name:"add_special_tokens",val:" = False"},{name:"return_attention_mask",val:" = True"},{name:"return_token_type_ids",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.__call__.text",description:`<strong>text</strong> (<code>str</code>, <code>List[str]</code>, <code>List[List[str]]</code>) &#x2014;
The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).`,name:"text"},{anchor:"transformers.BarkProcessor.__call__.voice_preset",description:`<strong>voice_preset</strong> (<code>str</code>, <code>Dict[np.ndarray]</code>) &#x2014;
The voice preset, i.e the speaker embeddings. It can either be a valid voice_preset name, e.g
<code>&quot;en_speaker_1&quot;</code>, or directly a dictionnary of <code>np.ndarray</code> embeddings for each submodel of <code>Bark</code>. Or
it can be a valid file name of a local <code>.npz</code> single voice preset.`,name:"voice_preset"},{anchor:"transformers.BarkProcessor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/ja/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L218",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A tuple composed of a <a
  href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a>, i.e the output of the
<code>tokenizer</code> and a <a
  href="/docs/transformers/main/ja/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a>, i.e the voice preset with the right tensors type.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Tuple(<a
  href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a>, <a
  href="/docs/transformers/main/ja/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a>)</p>
`}}),Ae=new B({props:{name:"from_pretrained",anchor:"transformers.BarkProcessor.from_pretrained",parameters:[{name:"pretrained_processor_name_or_path",val:""},{name:"speaker_embeddings_dict_path",val:" = 'speaker_embeddings_path.json'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkProcessor">BarkProcessor</a> hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a processor saved using the <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkProcessor.save_pretrained">save_pretrained()</a>
method, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.BarkProcessor.from_pretrained.speaker_embeddings_dict_path",description:`<strong>speaker_embeddings_dict_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings_path.json&quot;</code>) &#x2014;
The name of the <code>.json</code> file containing the speaker_embeddings dictionnary located in
<code>pretrained_model_name_or_path</code>. If <code>None</code>, no speaker_embeddings is loaded.
**kwargs &#x2014;
Additional keyword arguments passed along to both
<code>~tokenization_utils_base.PreTrainedTokenizer.from_pretrained</code>.`,name:"speaker_embeddings_dict_path"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L64"}}),Ke=new B({props:{name:"save_pretrained",anchor:"transformers.BarkProcessor.save_pretrained",parameters:[{name:"save_directory",val:""},{name:"speaker_embeddings_dict_path",val:" = 'speaker_embeddings_path.json'"},{name:"speaker_embeddings_directory",val:" = 'speaker_embeddings'"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the tokenizer files and the speaker embeddings will be saved (directory will be created
if it does not exist).`,name:"save_directory"},{anchor:"transformers.BarkProcessor.save_pretrained.speaker_embeddings_dict_path",description:`<strong>speaker_embeddings_dict_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings_path.json&quot;</code>) &#x2014;
The name of the <code>.json</code> file that will contains the speaker_embeddings nested path dictionnary, if it
exists, and that will be located in <code>pretrained_model_name_or_path/speaker_embeddings_directory</code>.`,name:"speaker_embeddings_dict_path"},{anchor:"transformers.BarkProcessor.save_pretrained.speaker_embeddings_directory",description:`<strong>speaker_embeddings_directory</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings/&quot;</code>) &#x2014;
The name of the folder in which the speaker_embeddings arrays will be saved.`,name:"speaker_embeddings_directory"},{anchor:"transformers.BarkProcessor.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/ja/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L117"}}),et=new x({props:{title:"BarkModel",local:"transformers.BarkModel",headingTag:"h2"}}),tt=new B({props:{name:"class transformers.BarkModel",anchor:"transformers.BarkModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkConfig">BarkConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1629"}}),ot=new B({props:{name:"generate",anchor:"transformers.BarkModel.generate",parameters:[{name:"input_ids",val:": Optional = None"},{name:"history_prompt",val:": Optional = None"},{name:"return_output_lengths",val:": Optional = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkModel.generate.input_ids",description:`<strong>input_ids</strong> (<code>Optional[torch.Tensor]</code> of shape (batch_size, seq_len), <em>optional</em>) &#x2014;
Input ids. Will be truncated up to 256 tokens. Note that the output audios will be as long as the
longest generation among the batch.`,name:"input_ids"},{anchor:"transformers.BarkModel.generate.history_prompt",description:`<strong>history_prompt</strong> (<code>Optional[Dict[str,torch.Tensor]]</code>, <em>optional</em>) &#x2014;
Optional <code>Bark</code> speaker prompt. Note that for now, this model takes only one speaker prompt per batch.`,name:"history_prompt"},{anchor:"transformers.BarkModel.generate.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014; Remaining dictionary of keyword arguments. Keyword arguments are of two types:</p>
<ul>
<li>Without a prefix, they will be entered as <code>**kwargs</code> for the <code>generate</code> method of each sub-model.</li>
<li>With a <em>semantic_</em>, <em>coarse_</em>, <em>fine_</em> prefix, they will be input for the <code>generate</code> method of the
semantic, coarse and fine respectively. It has the priority over the keywords without a prefix.</li>
</ul>
<p>This means you can, for example, specify a generation strategy for all sub-models except one.`,name:"kwargs"},{anchor:"transformers.BarkModel.generate.return_output_lengths",description:`<strong>return_output_lengths</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the waveform lengths. Useful when batching.`,name:"return_output_lengths"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1737",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>audio_waveform</strong> (<code>torch.Tensor</code> of shape (batch_size, seq_len)): Generated audio waveform.
When <code>return_output_lengths=True</code>:
Returns a tuple made of:</li>
<li><strong>audio_waveform</strong> (<code>torch.Tensor</code> of shape (batch_size, seq_len)): Generated audio waveform.</li>
<li><strong>output_lengths</strong> (<code>torch.Tensor</code> of shape (batch_size)): The length of each waveform in the batch</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>By default</p>
`}}),A=new fn({props:{anchor:"transformers.BarkModel.generate.example",$$slots:{default:[dr]},$$scope:{ctx:j}}}),nt=new B({props:{name:"enable_cpu_offload",anchor:"transformers.BarkModel.enable_cpu_offload",parameters:[{name:"gpu_id",val:": Optional = 0"}],parametersDescription:[{anchor:"transformers.BarkModel.enable_cpu_offload.gpu_id",description:`<strong>gpu_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
GPU id on which the sub-models will be loaded and offloaded.`,name:"gpu_id"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1680"}}),at=new x({props:{title:"BarkSemanticModel",local:"transformers.BarkSemanticModel",headingTag:"h2"}}),rt=new B({props:{name:"class transformers.BarkSemanticModel",anchor:"transformers.BarkSemanticModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkSemanticModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticConfig">BarkSemanticConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L913"}}),st=new B({props:{name:"forward",anchor:"transformers.BarkSemanticModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkSemanticModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkSemanticModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkSemanticModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkSemanticModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkSemanticModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkSemanticModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkSemanticModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkSemanticModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkSemanticModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkSemanticModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L749"}}),ee=new pn({props:{$$slots:{default:[cr]},$$scope:{ctx:j}}}),it=new x({props:{title:"BarkCoarseModel",local:"transformers.BarkCoarseModel",headingTag:"h2"}}),lt=new B({props:{name:"class transformers.BarkCoarseModel",anchor:"transformers.BarkCoarseModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkCoarseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseConfig">BarkCoarseConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1022"}}),dt=new B({props:{name:"forward",anchor:"transformers.BarkCoarseModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkCoarseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkCoarseModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkCoarseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkCoarseModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkCoarseModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkCoarseModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkCoarseModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkCoarseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkCoarseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkCoarseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L749"}}),te=new pn({props:{$$slots:{default:[mr]},$$scope:{ctx:j}}}),ct=new x({props:{title:"BarkFineModel",local:"transformers.BarkFineModel",headingTag:"h2"}}),mt=new B({props:{name:"class transformers.BarkFineModel",anchor:"transformers.BarkFineModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkFineModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineConfig">BarkFineConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1243"}}),pt=new B({props:{name:"forward",anchor:"transformers.BarkFineModel.forward",parameters:[{name:"codebook_idx",val:": int"},{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkFineModel.forward.codebook_idx",description:`<strong>codebook_idx</strong> (<code>int</code>) &#x2014;
Index of the codebook that will be predicted.`,name:"codebook_idx"},{anchor:"transformers.BarkFineModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length, number_of_codebooks)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Initially, indices of the first two codebooks are obtained from the <code>coarse</code> sub-model. The rest is
predicted recursively by attending the previously predicted channels. The model predicts on windows of
length 1024.`,name:"input_ids"},{anchor:"transformers.BarkFineModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkFineModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkFineModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkFineModel.forward.labels",description:"<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014; NOT IMPLEMENTED YET.",name:"labels"},{anchor:"transformers.BarkFineModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. If
<code>past_key_values</code> is used, optionally only the last <code>input_embeds</code> have to be input (see
<code>past_key_values</code>). This is useful if you want more control over how to convert <code>input_ids</code> indices into
associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"input_embeds"},{anchor:"transformers.BarkFineModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkFineModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkFineModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1381"}}),oe=new pn({props:{$$slots:{default:[pr]},$$scope:{ctx:j}}}),ft=new x({props:{title:"BarkCausalModel",local:"transformers.BarkCausalModel",headingTag:"h2"}}),ut=new B({props:{name:"class transformers.BarkCausalModel",anchor:"transformers.BarkCausalModel",parameters:[{name:"config",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L658"}}),ht=new B({props:{name:"forward",anchor:"transformers.BarkCausalModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkCausalModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkCausalModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkCausalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkCausalModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkCausalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkCausalModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkCausalModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkCausalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkCausalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkCausalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L749"}}),ne=new pn({props:{$$slots:{default:[fr]},$$scope:{ctx:j}}}),gt=new x({props:{title:"BarkCoarseConfig",local:"transformers.BarkCoarseConfig",headingTag:"h2"}}),_t=new B({props:{name:"class transformers.BarkCoarseConfig",anchor:"transformers.BarkCoarseConfig",parameters:[{name:"block_size",val:" = 1024"},{name:"input_vocab_size",val:" = 10048"},{name:"output_vocab_size",val:" = 10048"},{name:"num_layers",val:" = 12"},{name:"num_heads",val:" = 12"},{name:"hidden_size",val:" = 768"},{name:"dropout",val:" = 0.0"},{name:"bias",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkCoarseConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkCoarseConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkCoarseConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkCoarseConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkCoarseConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkCoarseConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkCoarseConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkCoarseConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkCoarseConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkCoarseConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L164"}}),ae=new fn({props:{anchor:"transformers.BarkCoarseConfig.example",$$slots:{default:[ur]},$$scope:{ctx:j}}}),bt=new x({props:{title:"BarkFineConfig",local:"transformers.BarkFineConfig",headingTag:"h2"}}),kt=new B({props:{name:"class transformers.BarkFineConfig",anchor:"transformers.BarkFineConfig",parameters:[{name:"tie_word_embeddings",val:" = True"},{name:"n_codes_total",val:" = 8"},{name:"n_codes_given",val:" = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkFineConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkFineConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkFineConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkFineConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkFineConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkFineConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkFineConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkFineConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkFineConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkFineConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.BarkFineConfig.n_codes_total",description:`<strong>n_codes_total</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The total number of audio codebooks predicted. Used in the fine acoustics sub-model.`,name:"n_codes_total"},{anchor:"transformers.BarkFineConfig.n_codes_given",description:`<strong>n_codes_given</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of audio codebooks predicted in the coarse acoustics sub-model. Used in the acoustics
sub-models.`,name:"n_codes_given"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L186"}}),re=new fn({props:{anchor:"transformers.BarkFineConfig.example",$$slots:{default:[hr]},$$scope:{ctx:j}}}),yt=new x({props:{title:"BarkSemanticConfig",local:"transformers.BarkSemanticConfig",headingTag:"h2"}}),vt=new B({props:{name:"class transformers.BarkSemanticConfig",anchor:"transformers.BarkSemanticConfig",parameters:[{name:"block_size",val:" = 1024"},{name:"input_vocab_size",val:" = 10048"},{name:"output_vocab_size",val:" = 10048"},{name:"num_layers",val:" = 12"},{name:"num_heads",val:" = 12"},{name:"hidden_size",val:" = 768"},{name:"dropout",val:" = 0.0"},{name:"bias",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkSemanticConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkSemanticConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkSemanticConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkSemanticConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkSemanticConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkSemanticConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkSemanticConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkSemanticConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkSemanticConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkSemanticConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L142"}}),se=new fn({props:{anchor:"transformers.BarkSemanticConfig.example",$$slots:{default:[gr]},$$scope:{ctx:j}}}),{c(){d=i("meta"),y=n(),b=i("p"),k=n(),m(v.$$.fragment),c=n(),m(M.$$.fragment),lo=n(),ue=i("p"),ue.innerHTML=da,co=n(),he=i("p"),he.textContent=ca,mo=n(),ge=i("ul"),ge.innerHTML=ma,po=n(),_e=i("p"),_e.textContent=pa,fo=n(),m(be.$$.fragment),uo=n(),ke=i("p"),ke.innerHTML=fa,ho=n(),m(ye.$$.fragment),go=n(),ve=i("p"),ve.textContent=ua,_o=n(),m(Me.$$.fragment),bo=n(),m(we.$$.fragment),ko=n(),$e=i("p"),$e.textContent=ha,yo=n(),m(Te.$$.fragment),vo=n(),Be=i("p"),Be.innerHTML=ga,Mo=n(),m(Ce.$$.fragment),wo=n(),je=i("p"),je.textContent=_a,$o=n(),xe=i("p"),xe.textContent=ba,To=n(),m(Je.$$.fragment),Bo=n(),ze=i("p"),ze.innerHTML=ka,Co=n(),m(Ue.$$.fragment),jo=n(),Ie=i("p"),Ie.textContent=ya,xo=n(),m(We.$$.fragment),Jo=n(),Ze=i("p"),Ze.innerHTML=va,zo=n(),m(Pe.$$.fragment),Uo=n(),Ne=i("p"),Ne.innerHTML=Ma,Io=n(),m(Fe.$$.fragment),Wo=n(),Ge=i("p"),Ge.innerHTML=wa,Zo=n(),m(He.$$.fragment),Po=n(),Se=i("p"),Se.innerHTML=$a,No=n(),m(Ve.$$.fragment),Fo=n(),Le=i("p"),Le.textContent=Ta,Go=n(),m(Ye.$$.fragment),Ho=n(),Ee=i("p"),Ee.innerHTML=Ba,So=n(),m(Xe.$$.fragment),Vo=n(),J=i("div"),m(qe.$$.fragment),un=n(),Bt=i("p"),Bt.innerHTML=Ca,hn=n(),Ct=i("p"),Ct.innerHTML=ja,gn=n(),jt=i("p"),jt.innerHTML=xa,_n=n(),R=i("div"),m(Re.$$.fragment),bn=n(),xt=i("p"),xt.innerHTML=Ja,Lo=n(),m(Qe.$$.fragment),Yo=n(),z=i("div"),m(De.$$.fragment),kn=n(),Jt=i("p"),Jt.textContent=za,yn=n(),Q=i("div"),m(Oe.$$.fragment),vn=n(),zt=i("p"),zt.innerHTML=Ua,Mn=n(),D=i("div"),m(Ae.$$.fragment),wn=n(),Ut=i("p"),Ut.textContent=Ia,$n=n(),O=i("div"),m(Ke.$$.fragment),Tn=n(),It=i("p"),It.innerHTML=Wa,Eo=n(),m(et.$$.fragment),Xo=n(),T=i("div"),m(tt.$$.fragment),Bn=n(),Wt=i("p"),Wt.textContent=Za,Cn=n(),Zt=i("ul"),Zt.innerHTML=Pa,jn=n(),Pt=i("p"),Pt.textContent=Na,xn=n(),Nt=i("p"),Nt.innerHTML=Fa,Jn=n(),Ft=i("p"),Ft.innerHTML=Ga,zn=n(),S=i("div"),m(ot.$$.fragment),Un=n(),Gt=i("p"),Gt.innerHTML=Ha,In=n(),m(A.$$.fragment),Wn=n(),K=i("div"),m(nt.$$.fragment),Zn=n(),Ht=i("p"),Ht.textContent=Sa,qo=n(),m(at.$$.fragment),Ro=n(),U=i("div"),m(rt.$$.fragment),Pn=n(),St=i("p"),St.innerHTML=Va,Nn=n(),Vt=i("p"),Vt.innerHTML=La,Fn=n(),V=i("div"),m(st.$$.fragment),Gn=n(),Lt=i("p"),Lt.innerHTML=Ya,Hn=n(),m(ee.$$.fragment),Qo=n(),m(it.$$.fragment),Do=n(),I=i("div"),m(lt.$$.fragment),Sn=n(),Yt=i("p"),Yt.innerHTML=Ea,Vn=n(),Et=i("p"),Et.innerHTML=Xa,Ln=n(),L=i("div"),m(dt.$$.fragment),Yn=n(),Xt=i("p"),Xt.innerHTML=qa,En=n(),m(te.$$.fragment),Oo=n(),m(ct.$$.fragment),Ao=n(),W=i("div"),m(mt.$$.fragment),Xn=n(),qt=i("p"),qt.innerHTML=Ra,qn=n(),Rt=i("p"),Rt.innerHTML=Qa,Rn=n(),Y=i("div"),m(pt.$$.fragment),Qn=n(),Qt=i("p"),Qt.innerHTML=Da,Dn=n(),m(oe.$$.fragment),Ko=n(),m(ft.$$.fragment),en=n(),X=i("div"),m(ut.$$.fragment),On=n(),E=i("div"),m(ht.$$.fragment),An=n(),Dt=i("p"),Dt.innerHTML=Oa,Kn=n(),m(ne.$$.fragment),tn=n(),m(gt.$$.fragment),on=n(),Z=i("div"),m(_t.$$.fragment),ea=n(),Ot=i("p"),Ot.innerHTML=Aa,ta=n(),At=i("p"),At.innerHTML=Ka,oa=n(),m(ae.$$.fragment),nn=n(),m(bt.$$.fragment),an=n(),P=i("div"),m(kt.$$.fragment),na=n(),Kt=i("p"),Kt.innerHTML=er,aa=n(),eo=i("p"),eo.innerHTML=tr,ra=n(),m(re.$$.fragment),rn=n(),m(yt.$$.fragment),sn=n(),N=i("div"),m(vt.$$.fragment),sa=n(),to=i("p"),to.innerHTML=or,ia=n(),oo=i("p"),oo.innerHTML=nr,la=n(),m(se.$$.fragment),ln=n(),io=i("p"),this.h()},l(e){const t=lr("svelte-u9bgzb",document.head);d=l(t,"META",{name:!0,content:!0}),t.forEach(o),y=a(e),b=l(e,"P",{}),w(b).forEach(o),k=a(e),p(v.$$.fragment,e),c=a(e),p(M.$$.fragment,e),lo=a(e),ue=l(e,"P",{"data-svelte-h":!0}),_(ue)!=="svelte-hjrtqs"&&(ue.innerHTML=da),co=a(e),he=l(e,"P",{"data-svelte-h":!0}),_(he)!=="svelte-1g9ayez"&&(he.textContent=ca),mo=a(e),ge=l(e,"UL",{"data-svelte-h":!0}),_(ge)!=="svelte-1p97wge"&&(ge.innerHTML=ma),po=a(e),_e=l(e,"P",{"data-svelte-h":!0}),_(_e)!=="svelte-14jfsv4"&&(_e.textContent=pa),fo=a(e),p(be.$$.fragment,e),uo=a(e),ke=l(e,"P",{"data-svelte-h":!0}),_(ke)!=="svelte-snmu9t"&&(ke.innerHTML=fa),ho=a(e),p(ye.$$.fragment,e),go=a(e),ve=l(e,"P",{"data-svelte-h":!0}),_(ve)!=="svelte-1f4itv2"&&(ve.textContent=ua),_o=a(e),p(Me.$$.fragment,e),bo=a(e),p(we.$$.fragment,e),ko=a(e),$e=l(e,"P",{"data-svelte-h":!0}),_($e)!=="svelte-b3n96m"&&($e.textContent=ha),yo=a(e),p(Te.$$.fragment,e),vo=a(e),Be=l(e,"P",{"data-svelte-h":!0}),_(Be)!=="svelte-19sxe7o"&&(Be.innerHTML=ga),Mo=a(e),p(Ce.$$.fragment,e),wo=a(e),je=l(e,"P",{"data-svelte-h":!0}),_(je)!=="svelte-ftbp94"&&(je.textContent=_a),$o=a(e),xe=l(e,"P",{"data-svelte-h":!0}),_(xe)!=="svelte-d00y1u"&&(xe.textContent=ba),To=a(e),p(Je.$$.fragment,e),Bo=a(e),ze=l(e,"P",{"data-svelte-h":!0}),_(ze)!=="svelte-m13j61"&&(ze.innerHTML=ka),Co=a(e),p(Ue.$$.fragment,e),jo=a(e),Ie=l(e,"P",{"data-svelte-h":!0}),_(Ie)!=="svelte-c8agyr"&&(Ie.textContent=ya),xo=a(e),p(We.$$.fragment,e),Jo=a(e),Ze=l(e,"P",{"data-svelte-h":!0}),_(Ze)!=="svelte-12hzxgd"&&(Ze.innerHTML=va),zo=a(e),p(Pe.$$.fragment,e),Uo=a(e),Ne=l(e,"P",{"data-svelte-h":!0}),_(Ne)!=="svelte-1demwxa"&&(Ne.innerHTML=Ma),Io=a(e),p(Fe.$$.fragment,e),Wo=a(e),Ge=l(e,"P",{"data-svelte-h":!0}),_(Ge)!=="svelte-nf94c5"&&(Ge.innerHTML=wa),Zo=a(e),p(He.$$.fragment,e),Po=a(e),Se=l(e,"P",{"data-svelte-h":!0}),_(Se)!=="svelte-1q66j1u"&&(Se.innerHTML=$a),No=a(e),p(Ve.$$.fragment,e),Fo=a(e),Le=l(e,"P",{"data-svelte-h":!0}),_(Le)!=="svelte-sn68fl"&&(Le.textContent=Ta),Go=a(e),p(Ye.$$.fragment,e),Ho=a(e),Ee=l(e,"P",{"data-svelte-h":!0}),_(Ee)!=="svelte-31lx8o"&&(Ee.innerHTML=Ba),So=a(e),p(Xe.$$.fragment,e),Vo=a(e),J=l(e,"DIV",{class:!0});var F=w(J);p(qe.$$.fragment,F),un=a(F),Bt=l(F,"P",{"data-svelte-h":!0}),_(Bt)!=="svelte-13qu8bv"&&(Bt.innerHTML=Ca),hn=a(F),Ct=l(F,"P",{"data-svelte-h":!0}),_(Ct)!=="svelte-c6ui3q"&&(Ct.innerHTML=ja),gn=a(F),jt=l(F,"P",{"data-svelte-h":!0}),_(jt)!=="svelte-1s6wgpv"&&(jt.innerHTML=xa),_n=a(F),R=l(F,"DIV",{class:!0});var Mt=w(R);p(Re.$$.fragment,Mt),bn=a(Mt),xt=l(Mt,"P",{"data-svelte-h":!0}),_(xt)!=="svelte-2lhxaj"&&(xt.innerHTML=Ja),Mt.forEach(o),F.forEach(o),Lo=a(e),p(Qe.$$.fragment,e),Yo=a(e),z=l(e,"DIV",{class:!0});var G=w(z);p(De.$$.fragment,G),kn=a(G),Jt=l(G,"P",{"data-svelte-h":!0}),_(Jt)!=="svelte-1xfrjvw"&&(Jt.textContent=za),yn=a(G),Q=l(G,"DIV",{class:!0});var wt=w(Q);p(Oe.$$.fragment,wt),vn=a(wt),zt=l(wt,"P",{"data-svelte-h":!0}),_(zt)!=="svelte-1hlbbl8"&&(zt.innerHTML=Ua),wt.forEach(o),Mn=a(G),D=l(G,"DIV",{class:!0});var $t=w(D);p(Ae.$$.fragment,$t),wn=a($t),Ut=l($t,"P",{"data-svelte-h":!0}),_(Ut)!=="svelte-1xt1aup"&&(Ut.textContent=Ia),$t.forEach(o),$n=a(G),O=l(G,"DIV",{class:!0});var Tt=w(O);p(Ke.$$.fragment,Tt),Tn=a(Tt),It=l(Tt,"P",{"data-svelte-h":!0}),_(It)!=="svelte-lp2u11"&&(It.innerHTML=Wa),Tt.forEach(o),G.forEach(o),Eo=a(e),p(et.$$.fragment,e),Xo=a(e),T=l(e,"DIV",{class:!0});var C=w(T);p(tt.$$.fragment,C),Bn=a(C),Wt=l(C,"P",{"data-svelte-h":!0}),_(Wt)!=="svelte-xp33tl"&&(Wt.textContent=Za),Cn=a(C),Zt=l(C,"UL",{"data-svelte-h":!0}),_(Zt)!=="svelte-131dx8r"&&(Zt.innerHTML=Pa),jn=a(C),Pt=l(C,"P",{"data-svelte-h":!0}),_(Pt)!=="svelte-beeiv6"&&(Pt.textContent=Na),xn=a(C),Nt=l(C,"P",{"data-svelte-h":!0}),_(Nt)!=="svelte-eisylu"&&(Nt.innerHTML=Fa),Jn=a(C),Ft=l(C,"P",{"data-svelte-h":!0}),_(Ft)!=="svelte-hswkmf"&&(Ft.innerHTML=Ga),zn=a(C),S=l(C,"DIV",{class:!0});var q=w(S);p(ot.$$.fragment,q),Un=a(q),Gt=l(q,"P",{"data-svelte-h":!0}),_(Gt)!=="svelte-4azpa"&&(Gt.innerHTML=Ha),In=a(q),p(A.$$.fragment,q),q.forEach(o),Wn=a(C),K=l(C,"DIV",{class:!0});var cn=w(K);p(nt.$$.fragment,cn),Zn=a(cn),Ht=l(cn,"P",{"data-svelte-h":!0}),_(Ht)!=="svelte-19e6niw"&&(Ht.textContent=Sa),cn.forEach(o),C.forEach(o),qo=a(e),p(at.$$.fragment,e),Ro=a(e),U=l(e,"DIV",{class:!0});var ie=w(U);p(rt.$$.fragment,ie),Pn=a(ie),St=l(ie,"P",{"data-svelte-h":!0}),_(St)!=="svelte-1kz3hoi"&&(St.innerHTML=Va),Nn=a(ie),Vt=l(ie,"P",{"data-svelte-h":!0}),_(Vt)!=="svelte-hswkmf"&&(Vt.innerHTML=La),Fn=a(ie),V=l(ie,"DIV",{class:!0});var no=w(V);p(st.$$.fragment,no),Gn=a(no),Lt=l(no,"P",{"data-svelte-h":!0}),_(Lt)!=="svelte-um9dc7"&&(Lt.innerHTML=Ya),Hn=a(no),p(ee.$$.fragment,no),no.forEach(o),ie.forEach(o),Qo=a(e),p(it.$$.fragment,e),Do=a(e),I=l(e,"DIV",{class:!0});var le=w(I);p(lt.$$.fragment,le),Sn=a(le),Yt=l(le,"P",{"data-svelte-h":!0}),_(Yt)!=="svelte-1r45z6k"&&(Yt.innerHTML=Ea),Vn=a(le),Et=l(le,"P",{"data-svelte-h":!0}),_(Et)!=="svelte-hswkmf"&&(Et.innerHTML=Xa),Ln=a(le),L=l(le,"DIV",{class:!0});var ao=w(L);p(dt.$$.fragment,ao),Yn=a(ao),Xt=l(ao,"P",{"data-svelte-h":!0}),_(Xt)!=="svelte-um9dc7"&&(Xt.innerHTML=qa),En=a(ao),p(te.$$.fragment,ao),ao.forEach(o),le.forEach(o),Oo=a(e),p(ct.$$.fragment,e),Ao=a(e),W=l(e,"DIV",{class:!0});var de=w(W);p(mt.$$.fragment,de),Xn=a(de),qt=l(de,"P",{"data-svelte-h":!0}),_(qt)!=="svelte-1eci89x"&&(qt.innerHTML=Ra),qn=a(de),Rt=l(de,"P",{"data-svelte-h":!0}),_(Rt)!=="svelte-hswkmf"&&(Rt.innerHTML=Qa),Rn=a(de),Y=l(de,"DIV",{class:!0});var ro=w(Y);p(pt.$$.fragment,ro),Qn=a(ro),Qt=l(ro,"P",{"data-svelte-h":!0}),_(Qt)!=="svelte-gqhnvb"&&(Qt.innerHTML=Da),Dn=a(ro),p(oe.$$.fragment,ro),ro.forEach(o),de.forEach(o),Ko=a(e),p(ft.$$.fragment,e),en=a(e),X=l(e,"DIV",{class:!0});var mn=w(X);p(ut.$$.fragment,mn),On=a(mn),E=l(mn,"DIV",{class:!0});var so=w(E);p(ht.$$.fragment,so),An=a(so),Dt=l(so,"P",{"data-svelte-h":!0}),_(Dt)!=="svelte-um9dc7"&&(Dt.innerHTML=Oa),Kn=a(so),p(ne.$$.fragment,so),so.forEach(o),mn.forEach(o),tn=a(e),p(gt.$$.fragment,e),on=a(e),Z=l(e,"DIV",{class:!0});var ce=w(Z);p(_t.$$.fragment,ce),ea=a(ce),Ot=l(ce,"P",{"data-svelte-h":!0}),_(Ot)!=="svelte-12d8l23"&&(Ot.innerHTML=Aa),ta=a(ce),At=l(ce,"P",{"data-svelte-h":!0}),_(At)!=="svelte-1s6wgpv"&&(At.innerHTML=Ka),oa=a(ce),p(ae.$$.fragment,ce),ce.forEach(o),nn=a(e),p(bt.$$.fragment,e),an=a(e),P=l(e,"DIV",{class:!0});var me=w(P);p(kt.$$.fragment,me),na=a(me),Kt=l(me,"P",{"data-svelte-h":!0}),_(Kt)!=="svelte-b4cucb"&&(Kt.innerHTML=er),aa=a(me),eo=l(me,"P",{"data-svelte-h":!0}),_(eo)!=="svelte-1s6wgpv"&&(eo.innerHTML=tr),ra=a(me),p(re.$$.fragment,me),me.forEach(o),rn=a(e),p(yt.$$.fragment,e),sn=a(e),N=l(e,"DIV",{class:!0});var pe=w(N);p(vt.$$.fragment,pe),sa=a(pe),to=l(pe,"P",{"data-svelte-h":!0}),_(to)!=="svelte-1u69emf"&&(to.innerHTML=or),ia=a(pe),oo=l(pe,"P",{"data-svelte-h":!0}),_(oo)!=="svelte-1s6wgpv"&&(oo.innerHTML=nr),la=a(pe),p(se.$$.fragment,pe),pe.forEach(o),ln=a(e),io=l(e,"P",{}),w(io).forEach(o),this.h()},h(){$(d,"name","hf:doc:metadata"),$(d,"content",br),$(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){s(document.head,d),r(e,y,t),r(e,b,t),r(e,k,t),f(v,e,t),r(e,c,t),f(M,e,t),r(e,lo,t),r(e,ue,t),r(e,co,t),r(e,he,t),r(e,mo,t),r(e,ge,t),r(e,po,t),r(e,_e,t),r(e,fo,t),f(be,e,t),r(e,uo,t),r(e,ke,t),r(e,ho,t),f(ye,e,t),r(e,go,t),r(e,ve,t),r(e,_o,t),f(Me,e,t),r(e,bo,t),f(we,e,t),r(e,ko,t),r(e,$e,t),r(e,yo,t),f(Te,e,t),r(e,vo,t),r(e,Be,t),r(e,Mo,t),f(Ce,e,t),r(e,wo,t),r(e,je,t),r(e,$o,t),r(e,xe,t),r(e,To,t),f(Je,e,t),r(e,Bo,t),r(e,ze,t),r(e,Co,t),f(Ue,e,t),r(e,jo,t),r(e,Ie,t),r(e,xo,t),f(We,e,t),r(e,Jo,t),r(e,Ze,t),r(e,zo,t),f(Pe,e,t),r(e,Uo,t),r(e,Ne,t),r(e,Io,t),f(Fe,e,t),r(e,Wo,t),r(e,Ge,t),r(e,Zo,t),f(He,e,t),r(e,Po,t),r(e,Se,t),r(e,No,t),f(Ve,e,t),r(e,Fo,t),r(e,Le,t),r(e,Go,t),f(Ye,e,t),r(e,Ho,t),r(e,Ee,t),r(e,So,t),f(Xe,e,t),r(e,Vo,t),r(e,J,t),f(qe,J,null),s(J,un),s(J,Bt),s(J,hn),s(J,Ct),s(J,gn),s(J,jt),s(J,_n),s(J,R),f(Re,R,null),s(R,bn),s(R,xt),r(e,Lo,t),f(Qe,e,t),r(e,Yo,t),r(e,z,t),f(De,z,null),s(z,kn),s(z,Jt),s(z,yn),s(z,Q),f(Oe,Q,null),s(Q,vn),s(Q,zt),s(z,Mn),s(z,D),f(Ae,D,null),s(D,wn),s(D,Ut),s(z,$n),s(z,O),f(Ke,O,null),s(O,Tn),s(O,It),r(e,Eo,t),f(et,e,t),r(e,Xo,t),r(e,T,t),f(tt,T,null),s(T,Bn),s(T,Wt),s(T,Cn),s(T,Zt),s(T,jn),s(T,Pt),s(T,xn),s(T,Nt),s(T,Jn),s(T,Ft),s(T,zn),s(T,S),f(ot,S,null),s(S,Un),s(S,Gt),s(S,In),f(A,S,null),s(T,Wn),s(T,K),f(nt,K,null),s(K,Zn),s(K,Ht),r(e,qo,t),f(at,e,t),r(e,Ro,t),r(e,U,t),f(rt,U,null),s(U,Pn),s(U,St),s(U,Nn),s(U,Vt),s(U,Fn),s(U,V),f(st,V,null),s(V,Gn),s(V,Lt),s(V,Hn),f(ee,V,null),r(e,Qo,t),f(it,e,t),r(e,Do,t),r(e,I,t),f(lt,I,null),s(I,Sn),s(I,Yt),s(I,Vn),s(I,Et),s(I,Ln),s(I,L),f(dt,L,null),s(L,Yn),s(L,Xt),s(L,En),f(te,L,null),r(e,Oo,t),f(ct,e,t),r(e,Ao,t),r(e,W,t),f(mt,W,null),s(W,Xn),s(W,qt),s(W,qn),s(W,Rt),s(W,Rn),s(W,Y),f(pt,Y,null),s(Y,Qn),s(Y,Qt),s(Y,Dn),f(oe,Y,null),r(e,Ko,t),f(ft,e,t),r(e,en,t),r(e,X,t),f(ut,X,null),s(X,On),s(X,E),f(ht,E,null),s(E,An),s(E,Dt),s(E,Kn),f(ne,E,null),r(e,tn,t),f(gt,e,t),r(e,on,t),r(e,Z,t),f(_t,Z,null),s(Z,ea),s(Z,Ot),s(Z,ta),s(Z,At),s(Z,oa),f(ae,Z,null),r(e,nn,t),f(bt,e,t),r(e,an,t),r(e,P,t),f(kt,P,null),s(P,na),s(P,Kt),s(P,aa),s(P,eo),s(P,ra),f(re,P,null),r(e,rn,t),f(yt,e,t),r(e,sn,t),r(e,N,t),f(vt,N,null),s(N,sa),s(N,to),s(N,ia),s(N,oo),s(N,la),f(se,N,null),r(e,ln,t),r(e,io,t),dn=!0},p(e,[t]){const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),A.$set(F);const Mt={};t&2&&(Mt.$$scope={dirty:t,ctx:e}),ee.$set(Mt);const G={};t&2&&(G.$$scope={dirty:t,ctx:e}),te.$set(G);const wt={};t&2&&(wt.$$scope={dirty:t,ctx:e}),oe.$set(wt);const $t={};t&2&&($t.$$scope={dirty:t,ctx:e}),ne.$set($t);const Tt={};t&2&&(Tt.$$scope={dirty:t,ctx:e}),ae.$set(Tt);const C={};t&2&&(C.$$scope={dirty:t,ctx:e}),re.$set(C);const q={};t&2&&(q.$$scope={dirty:t,ctx:e}),se.$set(q)},i(e){dn||(u(v.$$.fragment,e),u(M.$$.fragment,e),u(be.$$.fragment,e),u(ye.$$.fragment,e),u(Me.$$.fragment,e),u(we.$$.fragment,e),u(Te.$$.fragment,e),u(Ce.$$.fragment,e),u(Je.$$.fragment,e),u(Ue.$$.fragment,e),u(We.$$.fragment,e),u(Pe.$$.fragment,e),u(Fe.$$.fragment,e),u(He.$$.fragment,e),u(Ve.$$.fragment,e),u(Ye.$$.fragment,e),u(Xe.$$.fragment,e),u(qe.$$.fragment,e),u(Re.$$.fragment,e),u(Qe.$$.fragment,e),u(De.$$.fragment,e),u(Oe.$$.fragment,e),u(Ae.$$.fragment,e),u(Ke.$$.fragment,e),u(et.$$.fragment,e),u(tt.$$.fragment,e),u(ot.$$.fragment,e),u(A.$$.fragment,e),u(nt.$$.fragment,e),u(at.$$.fragment,e),u(rt.$$.fragment,e),u(st.$$.fragment,e),u(ee.$$.fragment,e),u(it.$$.fragment,e),u(lt.$$.fragment,e),u(dt.$$.fragment,e),u(te.$$.fragment,e),u(ct.$$.fragment,e),u(mt.$$.fragment,e),u(pt.$$.fragment,e),u(oe.$$.fragment,e),u(ft.$$.fragment,e),u(ut.$$.fragment,e),u(ht.$$.fragment,e),u(ne.$$.fragment,e),u(gt.$$.fragment,e),u(_t.$$.fragment,e),u(ae.$$.fragment,e),u(bt.$$.fragment,e),u(kt.$$.fragment,e),u(re.$$.fragment,e),u(yt.$$.fragment,e),u(vt.$$.fragment,e),u(se.$$.fragment,e),dn=!0)},o(e){h(v.$$.fragment,e),h(M.$$.fragment,e),h(be.$$.fragment,e),h(ye.$$.fragment,e),h(Me.$$.fragment,e),h(we.$$.fragment,e),h(Te.$$.fragment,e),h(Ce.$$.fragment,e),h(Je.$$.fragment,e),h(Ue.$$.fragment,e),h(We.$$.fragment,e),h(Pe.$$.fragment,e),h(Fe.$$.fragment,e),h(He.$$.fragment,e),h(Ve.$$.fragment,e),h(Ye.$$.fragment,e),h(Xe.$$.fragment,e),h(qe.$$.fragment,e),h(Re.$$.fragment,e),h(Qe.$$.fragment,e),h(De.$$.fragment,e),h(Oe.$$.fragment,e),h(Ae.$$.fragment,e),h(Ke.$$.fragment,e),h(et.$$.fragment,e),h(tt.$$.fragment,e),h(ot.$$.fragment,e),h(A.$$.fragment,e),h(nt.$$.fragment,e),h(at.$$.fragment,e),h(rt.$$.fragment,e),h(st.$$.fragment,e),h(ee.$$.fragment,e),h(it.$$.fragment,e),h(lt.$$.fragment,e),h(dt.$$.fragment,e),h(te.$$.fragment,e),h(ct.$$.fragment,e),h(mt.$$.fragment,e),h(pt.$$.fragment,e),h(oe.$$.fragment,e),h(ft.$$.fragment,e),h(ut.$$.fragment,e),h(ht.$$.fragment,e),h(ne.$$.fragment,e),h(gt.$$.fragment,e),h(_t.$$.fragment,e),h(ae.$$.fragment,e),h(bt.$$.fragment,e),h(kt.$$.fragment,e),h(re.$$.fragment,e),h(yt.$$.fragment,e),h(vt.$$.fragment,e),h(se.$$.fragment,e),dn=!1},d(e){e&&(o(y),o(b),o(k),o(c),o(lo),o(ue),o(co),o(he),o(mo),o(ge),o(po),o(_e),o(fo),o(uo),o(ke),o(ho),o(go),o(ve),o(_o),o(bo),o(ko),o($e),o(yo),o(vo),o(Be),o(Mo),o(wo),o(je),o($o),o(xe),o(To),o(Bo),o(ze),o(Co),o(jo),o(Ie),o(xo),o(Jo),o(Ze),o(zo),o(Uo),o(Ne),o(Io),o(Wo),o(Ge),o(Zo),o(Po),o(Se),o(No),o(Fo),o(Le),o(Go),o(Ho),o(Ee),o(So),o(Vo),o(J),o(Lo),o(Yo),o(z),o(Eo),o(Xo),o(T),o(qo),o(Ro),o(U),o(Qo),o(Do),o(I),o(Oo),o(Ao),o(W),o(Ko),o(en),o(X),o(tn),o(on),o(Z),o(nn),o(an),o(P),o(rn),o(sn),o(N),o(ln),o(io)),o(d),g(v,e),g(M,e),g(be,e),g(ye,e),g(Me,e),g(we,e),g(Te,e),g(Ce,e),g(Je,e),g(Ue,e),g(We,e),g(Pe,e),g(Fe,e),g(He,e),g(Ve,e),g(Ye,e),g(Xe,e),g(qe),g(Re),g(Qe,e),g(De),g(Oe),g(Ae),g(Ke),g(et,e),g(tt),g(ot),g(A),g(nt),g(at,e),g(rt),g(st),g(ee),g(it,e),g(lt),g(dt),g(te),g(ct,e),g(mt),g(pt),g(oe),g(ft,e),g(ut),g(ht),g(ne),g(gt,e),g(_t),g(ae),g(bt,e),g(kt),g(re),g(yt,e),g(vt),g(se)}}}const br='{"title":"Bark","local":"bark","sections":[{"title":"Overview","local":"overview","sections":[{"title":"Optimizing Bark","local":"optimizing-bark","sections":[{"title":"Using half-precision","local":"using-half-precision","sections":[],"depth":4},{"title":"Using 🤗 Better Transformer","local":"using--better-transformer","sections":[],"depth":4},{"title":"Using CPU offload","local":"using-cpu-offload","sections":[],"depth":4},{"title":"Combining optimization techniques","local":"combining-optimization-techniques","sections":[],"depth":4}],"depth":3},{"title":"Tips","local":"tips","sections":[],"depth":3}],"depth":2},{"title":"BarkConfig","local":"transformers.BarkConfig","sections":[],"depth":2},{"title":"BarkProcessor","local":"transformers.BarkProcessor","sections":[],"depth":2},{"title":"BarkModel","local":"transformers.BarkModel","sections":[],"depth":2},{"title":"BarkSemanticModel","local":"transformers.BarkSemanticModel","sections":[],"depth":2},{"title":"BarkCoarseModel","local":"transformers.BarkCoarseModel","sections":[],"depth":2},{"title":"BarkFineModel","local":"transformers.BarkFineModel","sections":[],"depth":2},{"title":"BarkCausalModel","local":"transformers.BarkCausalModel","sections":[],"depth":2},{"title":"BarkCoarseConfig","local":"transformers.BarkCoarseConfig","sections":[],"depth":2},{"title":"BarkFineConfig","local":"transformers.BarkFineConfig","sections":[],"depth":2},{"title":"BarkSemanticConfig","local":"transformers.BarkSemanticConfig","sections":[],"depth":2}],"depth":1}';function kr(j){return rr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Cr extends sr{constructor(d){super(),ir(this,d,kr,_r,ar,{})}}export{Cr as component};
