import{s as ar,o as rr,n as fe}from"../chunks/scheduler.9bc65507.js";import{S as sr,i as ir,g as i,s as n,r as m,A as lr,h as l,f as o,c as a,j as w,u as p,x as _,k as $,y as s,a as r,v as f,d as u,t as h,w as g}from"../chunks/index.707bf1b6.js";import{T as pn}from"../chunks/Tip.c2ecdbf4.js";import{D as B}from"../chunks/Docstring.17db21ae.js";import{C as H}from"../chunks/CodeBlock.54a9f38d.js";import{E as fn}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as x}from"../chunks/Heading.342b1fa6.js";function dr(j){let d,y="Example:",b,k,v;return k=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBCYXJrTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzdW5vJTJGYmFyay1zbWFsbCUyMiklMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmstc21hbGwlMjIpJTBBJTBBJTIzJTIwVG8lMjBhZGQlMjBhJTIwdm9pY2UlMjBwcmVzZXQlMkMlMjB5b3UlMjBjYW4lMjBwYXNzJTIwJTYwdm9pY2VfcHJlc2V0JTYwJTIwdG8lMjAlNjBCYXJrUHJvY2Vzc29yLl9fY2FsbF9fKC4uLiklNjAlMEF2b2ljZV9wcmVzZXQlMjAlM0QlMjAlMjJ2MiUyRmVuX3NwZWFrZXJfNiUyMiUwQSUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTJDJTIwSSUyMG5lZWQlMjBoaW0lMjBpbiUyMG15JTIwbGlmZSUyMiUyQyUyMHZvaWNlX3ByZXNldCUzRHZvaWNlX3ByZXNldCklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzJTJDJTIwc2VtYW50aWNfbWF4X25ld190b2tlbnMlM0QxMDApJTBBYXVkaW9fYXJyYXklMjAlM0QlMjBhdWRpb19hcnJheS5jcHUoKS5udW1weSgpLnNxdWVlemUoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, BarkModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># To add a voice preset, you can pass \`voice_preset\` to \`BarkProcessor.__call__(...)\`</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>voice_preset = <span class="hljs-string">&quot;v2/en_speaker_6&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello, my dog is cute, I need him in my life&quot;</span>, voice_preset=voice_preset)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs, semantic_max_new_tokens=<span class="hljs-number">100</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=n(),m(k.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),_(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(c),p(k.$$.fragment,c)},m(c,M){r(c,d,M),r(c,b,M),f(k,c,M),v=!0},p:fe,i(c){v||(u(k.$$.fragment,c),v=!0)},o(c){h(k.$$.fragment,c),v=!1},d(c){c&&(o(d),o(b)),g(k,c)}}}function cr(j){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),_(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:fe,d(b){b&&o(d)}}}function mr(j){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),_(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:fe,d(b){b&&o(d)}}}function pr(j){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),_(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:fe,d(b){b&&o(d)}}}function fr(j){let d,y=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){d=i("p"),d.innerHTML=y},l(b){d=l(b,"P",{"data-svelte-h":!0}),_(d)!=="svelte-fincs2"&&(d.innerHTML=y)},m(b,k){r(b,d,k)},p:fe,d(b){b&&o(d)}}}function ur(j){let d,y="Example:",b,k,v;return k=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtDb2Fyc2VDb25maWclMkMlMjBCYXJrQ29hcnNlTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQmFyayUyMHN1Yi1tb2R1bGUlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQmFya0NvYXJzZUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya0NvYXJzZU1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkCoarseConfig, BarkCoarseModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkCoarseConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkCoarseModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=n(),m(k.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),_(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(c),p(k.$$.fragment,c)},m(c,M){r(c,d,M),r(c,b,M),f(k,c,M),v=!0},p:fe,i(c){v||(u(k.$$.fragment,c),v=!0)},o(c){h(k.$$.fragment,c),v=!1},d(c){c&&(o(d),o(b)),g(k,c)}}}function hr(j){let d,y="Example:",b,k,v;return k=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtGaW5lQ29uZmlnJTJDJTIwQmFya0ZpbmVNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBCYXJrJTIwc3ViLW1vZHVsZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBCYXJrRmluZUNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya0ZpbmVNb2RlbChjb25maWd1cmF0aW9uKSUwQSUwQSUyMyUyMEFjY2Vzc2luZyUyMHRoZSUyMG1vZGVsJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBtb2RlbC5jb25maWc=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkFineConfig, BarkFineModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkFineConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkFineModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=n(),m(k.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),_(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(c),p(k.$$.fragment,c)},m(c,M){r(c,d,M),r(c,b,M),f(k,c,M),v=!0},p:fe,i(c){v||(u(k.$$.fragment,c),v=!0)},o(c){h(k.$$.fragment,c),v=!1},d(c){c&&(o(d),o(b)),g(k,c)}}}function gr(j){let d,y="Example:",b,k,v;return k=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtTZW1hbnRpY0NvbmZpZyUyQyUyMEJhcmtTZW1hbnRpY01vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEJhcmslMjBzdWItbW9kdWxlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEJhcmtTZW1hbnRpY0NvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzdW5vJTJGYmFyayUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQmFya1NlbWFudGljTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkSemanticConfig, BarkSemanticModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Bark sub-module style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = BarkSemanticConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the suno/bark style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkSemanticModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){d=i("p"),d.textContent=y,b=n(),m(k.$$.fragment)},l(c){d=l(c,"P",{"data-svelte-h":!0}),_(d)!=="svelte-11lpom8"&&(d.textContent=y),b=a(c),p(k.$$.fragment,c)},m(c,M){r(c,d,M),r(c,b,M),f(k,c,M),v=!0},p:fe,i(c){v||(u(k.$$.fragment,c),v=!0)},o(c){h(k.$$.fragment,c),v=!1},d(c){c&&(o(d),o(b)),g(k,c)}}}function _r(j){let d,y,b,k,v,c,M,lo,ue,da='Bark ã¯ã€<a href="https://github.com/suno-ai/bark" rel="nofollow">suno-ai/bark</a> ã§ Suno AI ã«ã‚ˆã£ã¦ææ¡ˆã•ã‚ŒãŸãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿ä¸Šã’ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚',co,he,ca="Bark ã¯ 4 ã¤ã®ä¸»è¦ãªãƒ¢ãƒ‡ãƒ«ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚",mo,ge,ma='<li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> (â€˜ãƒ†ã‚­ã‚¹ãƒˆâ€™ãƒ¢ãƒ‡ãƒ«ã¨ã‚‚å‘¼ã°ã‚Œã‚‹): ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã®æ„å‘³ã‚’æ‰ãˆã‚‹ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ãƒ†ã‚­ã‚¹ãƒˆ ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹å› æœçš„è‡ªå·±å›å¸°å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã€‚</li> <li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a> (â€˜ç²—ã„éŸ³éŸ¿â€™ ãƒ¢ãƒ‡ãƒ«ã¨ã‚‚å‘¼ã°ã‚Œã‚‹): <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> ãƒ¢ãƒ‡ãƒ«ã®çµæœã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚‹å› æœçš„è‡ªå·±å›å¸°å¤‰æ›å™¨ã€‚ EnCodec ã«å¿…è¦ãªæœ€åˆã® 2 ã¤ã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ª ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚</li> <li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> (â€˜å¾®ç´°éŸ³éŸ¿â€™ ãƒ¢ãƒ‡ãƒ«)ã€ä»Šå›ã¯éå› æœçš„ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã€ä»¥å‰ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯åŸ‹ã‚è¾¼ã¿ã®åˆè¨ˆã«åŸºã¥ã„ã¦æœ€å¾Œã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ã‚’ç¹°ã‚Šè¿”ã—äºˆæ¸¬ã—ã¾ã™ã€‚</li> <li><code>EncodecModel</code> ã‹ã‚‰ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒƒã‚¯ ãƒãƒ£ãƒãƒ«ã‚’äºˆæ¸¬ã—ãŸã®ã§ã€Bark ã¯ãã‚Œã‚’ä½¿ç”¨ã—ã¦å‡ºåŠ›ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªé…åˆ—ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚</li>',po,_e,pa="æœ€åˆã® 3 ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ãã‚Œãã‚Œã€ç‰¹å®šã®äº‹å‰å®šç¾©ã•ã‚ŒãŸéŸ³å£°ã«å¾“ã£ã¦å‡ºåŠ›ã‚µã‚¦ãƒ³ãƒ‰ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã®æ¡ä»¶ä»˜ãã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼åŸ‹ã‚è¾¼ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã§ãã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚",fo,be,uo,ke,fa="Bark ã¯ã€ã‚³ãƒ¼ãƒ‰ã‚’æ•°è¡Œè¿½åŠ ã™ã‚‹ã ã‘ã§æœ€é©åŒ–ã§ãã€<strong>ãƒ¡ãƒ¢ãƒª ãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆãŒå¤§å¹…ã«å‰Šæ¸›</strong>ã•ã‚Œã€<strong>æ¨è«–ãŒé«˜é€ŸåŒ–</strong>ã•ã‚Œã¾ã™ã€‚",ho,ye,go,ve,ua="ãƒ¢ãƒ‡ãƒ«ã‚’åŠç²¾åº¦ã§ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ã€æ¨è«–ã‚’é«˜é€ŸåŒ–ã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ 50% å‰Šæ¸›ã§ãã¾ã™ã€‚",_o,Me,bo,we,ko,$e,ha="Better Transformer ã¯ã€å†…éƒ¨ã§ã‚«ãƒ¼ãƒãƒ«èåˆã‚’å®Ÿè¡Œã™ã‚‹ ğŸ¤— æœ€é©ãªæ©Ÿèƒ½ã§ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä½ä¸‹ã•ã›ã‚‹ã“ã¨ãªãã€é€Ÿåº¦ã‚’ 20% ï½ 30% å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— Better Transformer ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã®ã«å¿…è¦ãªã‚³ãƒ¼ãƒ‰ã¯ 1 è¡Œã ã‘ã§ã™ã€‚",yo,Te,vo,Be,ga='ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹å‰ã« ğŸ¤— Optimum ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ <a href="https://huggingface.co/docs/optimum/installation" rel="nofollow">ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã¯ã“ã¡ã‚‰</a>',Mo,Ce,wo,je,_a="å‰è¿°ã—ãŸã‚ˆã†ã«ã€Bark ã¯ 4 ã¤ã®ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ã§æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªç”Ÿæˆä¸­ã«é †ç•ªã«å‘¼ã³å‡ºã•ã‚Œã¾ã™ã€‚è¨€ã„æ›ãˆã‚Œã°ã€1 ã¤ã®ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹é–“ã€ä»–ã®ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ã¯ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã«ãªã‚Šã¾ã™ã€‚",$o,xe,ba="CUDA ãƒ‡ãƒã‚¤ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€ãƒ¡ãƒ¢ãƒª ãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã® 80% å‰Šæ¸›ã«ã‚ˆã‚‹æ©æµã‚’å—ã‘ã‚‹ç°¡å˜ãªè§£æ±ºç­–ã¯ã€ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã® GPU ã®ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã®æ“ä½œã¯ CPU ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã¨å‘¼ã°ã‚Œã¾ã™ã€‚ 1è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ä½¿ç”¨ã§ãã¾ã™ã€‚",To,Je,Bo,ze,ka='ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ğŸ¤— Accelerate ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ <a href="https://huggingface.co/docs/accelerate/basic_tutorials/install" rel="nofollow">ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•ã¯ã“ã¡ã‚‰</a>',Co,Ue,jo,Ie,ya="æœ€é©åŒ–æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦ã€CPU ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã€åŠç²¾åº¦ã€ğŸ¤— Better Transformer ã‚’ã™ã¹ã¦ä¸€åº¦ã«ä½¿ç”¨ã§ãã¾ã™ã€‚",xo,We,Jo,Ze,va='æ¨è«–æœ€é©åŒ–æ‰‹æ³•ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€<a href="https://huggingface.co/docs/transformers/perf_infer_gpu_one" rel="nofollow">ã“ã¡ã‚‰</a> ã‚’ã”è¦§ãã ã•ã„ã€‚',zo,Pe,Uo,Ne,Ma=`Suno ã¯ã€å¤šãã®è¨€èªã§éŸ³å£°ãƒ—ãƒªã‚»ãƒƒãƒˆã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æä¾›ã—ã¦ã„ã¾ã™ <a href="https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c" rel="nofollow">ã“ã¡ã‚‰</a>ã€‚
ã“ã‚Œã‚‰ã®ãƒ—ãƒªã‚»ãƒƒãƒˆã¯ã€ãƒãƒ– <a href="https://huggingface.co/suno/bark-small/tree/main/speaker_embeddings" rel="nofollow">ã“ã¡ã‚‰</a> ã¾ãŸã¯ <a href="https://huggingface.co/suno/bark/tree/main/speaker_embeddings" rel="nofollow">ã“ã¡ã‚‰</a>ã€‚`,Io,Fe,Wo,Ge,wa="Bark ã¯ã€éå¸¸ã«ãƒªã‚¢ãƒ«ãª <strong>å¤šè¨€èª</strong> éŸ³å£°ã ã‘ã§ãªãã€éŸ³æ¥½ã€èƒŒæ™¯ãƒã‚¤ã‚ºã€å˜ç´”ãªåŠ¹æœéŸ³ãªã©ã®ä»–ã®éŸ³å£°ã‚‚ç”Ÿæˆã§ãã¾ã™ã€‚",Zo,He,Po,Se,$a="ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ç¬‘ã†ã€ãŸã‚æ¯ã€æ³£ããªã©ã®<strong>éè¨€èªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³</strong>ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚",No,Ve,Fo,Le,Ta="ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚’ä¿å­˜ã™ã‚‹ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«è¨­å®šã¨ scipy ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ« ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—ã™ã‚‹ã ã‘ã§ã™ã€‚",Go,Ye,Ho,Ee,Ba=`ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€<a href="https://huggingface.co/ylacombe" rel="nofollow">Yoach Lacombe (ylacombe)</a> ãŠã‚ˆã³ <a href="https://github.com/sanchit-gandhi" rel="nofollow">Sanchit Gandhi (sanchit-gandhi)</a> ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã¾ã—ãŸã€‚
å…ƒã®ã‚³ãƒ¼ãƒ‰ã¯ <a href="https://github.com/suno-ai/bark" rel="nofollow">ã“ã“</a> ã«ã‚ã‚Šã¾ã™ã€‚`,So,Xe,Vo,J,qe,un,Bt,Ca=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkModel">BarkModel</a>. It is used to instantiate a Bark
model according to the specified sub-models configurations, defining the model architecture.`,hn,Ct,ja=`Instantiating a configuration with the defaults will yield a similar configuration to that of the Bark
<a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a> architecture.`,gn,jt,xa=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,_n,R,Re,bn,xt,Ja='Instantiate a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkConfig">BarkConfig</a> (or a derived class) from bark sub-models configuration.',Lo,Qe,Yo,z,De,kn,Jt,za="Constructs a Bark processor which wraps a text tokenizer and optional Bark voice presets into a single processor.",yn,Q,Oe,vn,zt,Ua=`Main method to prepare for the model one or several sequences(s). This method forwards the <code>text</code> and <code>kwargs</code>
arguments to the AutoTokenizerâ€™s <code>__call__()</code> to encode the text. The method also proposes a
voice preset which is a dictionary of arrays that conditions <code>Bark</code>â€™s output. <code>kwargs</code> arguments are forwarded
to the tokenizer and to <code>cached_file</code> method if <code>voice_preset</code> is a valid filename.`,Mn,D,Ae,wn,Ut,Ia="Instantiate a Bark processor associated with a pretrained model.",$n,O,Ke,Tn,It,Wa=`Saves the attributes of this processor (tokenizerâ€¦) in the specified directory so that it can be reloaded
using the <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkProcessor.from_pretrained">from_pretrained()</a> method.`,Eo,et,Xo,T,tt,Bn,Wt,Za="The full Bark model, a text-to-speech model composed of 4 sub-models:",Cn,Zt,Pa=`<li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a> (also referred to as the â€˜textâ€™ model): a causal auto-regressive transformer model that
takes
as input tokenized text, and predicts semantic text tokens that capture the meaning of the text.</li> <li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a> (also refered to as the â€˜coarse acousticsâ€™ model), also a causal autoregressive transformer,
that takes into input the results of the last model. It aims at regressing the first two audio codebooks necessary
to <code>encodec</code>.</li> <li><a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> (the â€˜fine acousticsâ€™ model), this time a non-causal autoencoder transformer, which iteratively
predicts the last codebooks based on the sum of the previous codebooks embeddings.</li> <li>having predicted all the codebook channels from the <code>EncodecModel</code>, Bark uses it to decode the output audio
array.</li>`,jn,Pt,Na=`It should be noted that each of the first three modules can support conditional speaker embeddings to condition the
output sound according to specific predefined voice.`,xn,Nt,Fa=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Jn,Ft,Ga=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,zn,S,ot,Un,Gt,Ha="Generates audio from an input prompt and an additional optional <code>Bark</code> speaker prompt.",In,A,Wn,K,nt,Zn,Ht,Sa=`Offloads all sub-models to CPU using accelerate, reducing memory usage with a low impact on performance. This
method moves one whole sub-model at a time to the GPU when it is used, and the sub-model remains in GPU until
the next sub-model runs.`,qo,at,Ro,U,rt,Pn,St,Va=`Bark semantic (or text) model. It shares the same architecture as the coarse model.
It is a GPT-2 like autoregressive model with a language modeling head on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Nn,Vt,La=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Fn,V,st,Gn,Lt,Ya='The <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',Hn,ee,Qo,it,Do,I,lt,Sn,Yt,Ea=`Bark coarse acoustics model.
It shares the same architecture as the semantic (or text) model. It is a GPT-2 like autoregressive model with a
language modeling head on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Vn,Et,Xa=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Ln,L,dt,Yn,Xt,qa='The <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',En,te,Oo,ct,Ao,W,mt,Xn,qt,Ra=`Bark fine acoustics model. It is a non-causal GPT-like model with <code>config.n_codes_total</code> embedding layers and
language modeling heads, one for each codebook.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,qn,Rt,Qa=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Rn,Y,pt,Qn,Qt,Da='The <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a> forward method, overrides the <code>__call__</code> special method.',Dn,oe,Ko,ft,en,X,ut,On,E,ht,An,Dt,Oa='The <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCausalModel">BarkCausalModel</a> forward method, overrides the <code>__call__</code> special method.',Kn,ne,tn,gt,on,Z,_t,ea,Ot,Aa=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,ta,At,Ka=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,oa,ae,nn,bt,an,P,kt,na,Kt,er=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,aa,eo,tr=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,ra,re,rn,yt,sn,N,vt,sa,to,or=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. It is used to instantiate the model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Bark <a href="https://huggingface.co/suno/bark" rel="nofollow">suno/bark</a>
architecture.`,ia,oo,nr=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,la,se,ln,io,dn;return v=new x({props:{title:"Bark",local:"bark",headingTag:"h1"}}),M=new x({props:{title:"Overview",local:"overview",headingTag:"h2"}}),be=new x({props:{title:"Optimizing Bark",local:"optimizing-bark",headingTag:"h3"}}),ye=new x({props:{title:"Using half-precision",local:"using-half-precision",headingTag:"h4"}}),Me=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMGlmJTIwdG9yY2guY3VkYS5pc19hdmFpbGFibGUoKSUyMGVsc2UlMjAlMjJjcHUlMjIlMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmstc21hbGwlMjIlMkMlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmZsb2F0MTYpLnRvKGRldmljZSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkModel
<span class="hljs-keyword">import</span> torch

device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>
model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>, torch_dtype=torch.float16).to(device)`,wrap:!1}}),we=new x({props:{title:"Using ğŸ¤— Better Transformer",local:"using--better-transformer",headingTag:"h4"}}),Te=new H({props:{code:"bW9kZWwlMjAlM0QlMjAlMjBtb2RlbC50b19iZXR0ZXJ0cmFuc2Zvcm1lcigp",highlighted:"model =  model.to_bettertransformer()",wrap:!1}}),Ce=new x({props:{title:"Using CPU offload",local:"using-cpu-offload",headingTag:"h4"}}),Je=new H({props:{code:"bW9kZWwuZW5hYmxlX2NwdV9vZmZsb2FkKCk=",highlighted:"model.enable_cpu_offload()",wrap:!1}}),Ue=new x({props:{title:"Combining optimization techniques",local:"combining-optimization-techniques",headingTag:"h4"}}),We=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJhcmtNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGV2aWNlJTIwJTNEJTIwJTIyY3VkYSUyMiUyMGlmJTIwdG9yY2guY3VkYS5pc19hdmFpbGFibGUoKSUyMGVsc2UlMjAlMjJjcHUlMjIlMEElMEElMjMlMjBsb2FkJTIwaW4lMjBmcDE2JTBBbW9kZWwlMjAlM0QlMjBCYXJrTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnN1bm8lMkZiYXJrLXNtYWxsJTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byhkZXZpY2UpJTBBJTBBJTIzJTIwY29udmVydCUyMHRvJTIwYmV0dGVydHJhbnNmb3JtZXIlMEFtb2RlbCUyMCUzRCUyMEJldHRlclRyYW5zZm9ybWVyLnRyYW5zZm9ybShtb2RlbCUyQyUyMGtlZXBfb3JpZ2luYWxfbW9kZWwlM0RGYWxzZSklMEElMEElMjMlMjBlbmFibGUlMjBDUFUlMjBvZmZsb2FkJTBBbW9kZWwuZW5hYmxlX2NwdV9vZmZsb2FkKCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BarkModel
<span class="hljs-keyword">import</span> torch

device = <span class="hljs-string">&quot;cuda&quot;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;cpu&quot;</span>

<span class="hljs-comment"># load in fp16</span>
model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark-small&quot;</span>, torch_dtype=torch.float16).to(device)

<span class="hljs-comment"># convert to bettertransformer</span>
model = BetterTransformer.transform(model, keep_original_model=<span class="hljs-literal">False</span>)

<span class="hljs-comment"># enable CPU offload</span>
model.enable_cpu_offload()`,wrap:!1}}),Pe=new x({props:{title:"Tips",local:"tips",headingTag:"h3"}}),Fe=new H({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBCYXJrTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJzdW5vJTJGYmFyayUyMiklMEFtb2RlbCUyMCUzRCUyMEJhcmtNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyc3VubyUyRmJhcmslMjIpJTBBJTBBdm9pY2VfcHJlc2V0JTIwJTNEJTIwJTIydjIlMkZlbl9zcGVha2VyXzYlMjIlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoJTIySGVsbG8lMkMlMjBteSUyMGRvZyUyMGlzJTIwY3V0ZSUyMiUyQyUyMHZvaWNlX3ByZXNldCUzRHZvaWNlX3ByZXNldCklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwYXVkaW9fYXJyYXkuY3B1KCkubnVtcHkoKS5zcXVlZXplKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, BarkModel

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;suno/bark&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = BarkModel.from_pretrained(<span class="hljs-string">&quot;suno/bark&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>voice_preset = <span class="hljs-string">&quot;v2/en_speaker_6&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello, my dog is cute&quot;</span>, voice_preset=voice_preset)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),He=new H({props:{code:"JTIzJTIwTXVsdGlsaW5ndWFsJTIwc3BlZWNoJTIwLSUyMHNpbXBsaWZpZWQlMjBDaGluZXNlJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKCUyMiVFNiU4MyU4QSVFNCVCQSVCQSVFNyU5QSU4NCVFRiVCQyU4MSVFNiU4OCU5MSVFNCVCQyU5QSVFOCVBRiVCNCVFNCVCOCVBRCVFNiU5NiU4NyUyMiklMEElMEElMjMlMjBNdWx0aWxpbmd1YWwlMjBzcGVlY2glMjAtJTIwRnJlbmNoJTIwLSUyMGxldCdzJTIwdXNlJTIwYSUyMHZvaWNlX3ByZXNldCUyMGFzJTIwd2VsbCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJJbmNyb3lhYmxlISUyMEplJTIwcGV1eCUyMGclQzMlQTluJUMzJUE5cmVyJTIwZHUlMjBzb24uJTIyJTJDJTIwdm9pY2VfcHJlc2V0JTNEJTIyZnJfc3BlYWtlcl81JTIyKSUwQSUwQSUyMyUyMEJhcmslMjBjYW4lMjBhbHNvJTIwZ2VuZXJhdGUlMjBtdXNpYy4lMjBZb3UlMjBjYW4lMjBoZWxwJTIwaXQlMjBvdXQlMjBieSUyMGFkZGluZyUyMG11c2ljJTIwbm90ZXMlMjBhcm91bmQlMjB5b3VyJTIwbHlyaWNzLiUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjIlRTIlOTklQUElMjBIZWxsbyUyQyUyMG15JTIwZG9nJTIwaXMlMjBjdXRlJTIwJUUyJTk5JUFBJTIyKSUwQSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMpJTBBYXVkaW9fYXJyYXklMjAlM0QlMjBhdWRpb19hcnJheS5jcHUoKS5udW1weSgpLnNxdWVlemUoKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multilingual speech - simplified Chinese</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;æƒŠäººçš„ï¼æˆ‘ä¼šè¯´ä¸­æ–‡&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Multilingual speech - French - let&#x27;s use a voice_preset as well</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Incroyable! Je peux gÃ©nÃ©rer du son.&quot;</span>, voice_preset=<span class="hljs-string">&quot;fr_speaker_5&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Bark can also generate music. You can help it out by adding music notes around your lyrics.</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;â™ª Hello, my dog is cute â™ª&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),Ve=new H({props:{code:"JTIzJTIwQWRkaW5nJTIwbm9uLXNwZWVjaCUyMGN1ZXMlMjB0byUyMHRoZSUyMGlucHV0JTIwdGV4dCUwQWlucHV0cyUyMCUzRCUyMHByb2Nlc3NvciglMjJIZWxsbyUyMHVoJTIwLi4uJTIwJTVCY2xlYXJzJTIwdGhyb2F0JTVEJTJDJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjAlNUJsYXVnaHRlciU1RCUyMiklMEElMEFhdWRpb19hcnJheSUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQWF1ZGlvX2FycmF5JTIwJTNEJTIwYXVkaW9fYXJyYXkuY3B1KCkubnVtcHkoKS5zcXVlZXplKCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Adding non-speech cues to the input text</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(<span class="hljs-string">&quot;Hello uh ... [clears throat], my dog is cute [laughter]&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = model.generate(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_array = audio_array.cpu().numpy().squeeze()`,wrap:!1}}),Ye=new H({props:{code:"ZnJvbSUyMHNjaXB5LmlvLndhdmZpbGUlMjBpbXBvcnQlMjB3cml0ZSUyMGFzJTIwd3JpdGVfd2F2JTBBJTBBJTIzJTIwc2F2ZSUyMGF1ZGlvJTIwdG8lMjBkaXNrJTJDJTIwYnV0JTIwZmlyc3QlMjB0YWtlJTIwdGhlJTIwc2FtcGxlJTIwcmF0ZSUyMGZyb20lMjB0aGUlMjBtb2RlbCUyMGNvbmZpZyUwQXNhbXBsZV9yYXRlJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGlvbl9jb25maWcuc2FtcGxlX3JhdGUlMEF3cml0ZV93YXYoJTIyYmFya19nZW5lcmF0aW9uLndhdiUyMiUyQyUyMHNhbXBsZV9yYXRlJTJDJTIwYXVkaW9fYXJyYXkp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> scipy.io.wavfile <span class="hljs-keyword">import</span> write <span class="hljs-keyword">as</span> write_wav

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># save audio to disk, but first take the sample rate from the model config</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>sample_rate = model.generation_config.sample_rate
<span class="hljs-meta">&gt;&gt;&gt; </span>write_wav(<span class="hljs-string">&quot;bark_generation.wav&quot;</span>, sample_rate, audio_array)`,wrap:!1}}),Xe=new x({props:{title:"BarkConfig",local:"transformers.BarkConfig",headingTag:"h2"}}),qe=new B({props:{name:"class transformers.BarkConfig",anchor:"transformers.BarkConfig",parameters:[{name:"semantic_config",val:": Dict = None"},{name:"coarse_acoustics_config",val:": Dict = None"},{name:"fine_acoustics_config",val:": Dict = None"},{name:"codec_config",val:": Dict = None"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkConfig.semantic_config",description:`<strong>semantic_config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticConfig">BarkSemanticConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying semantic sub-model.`,name:"semantic_config"},{anchor:"transformers.BarkConfig.coarse_acoustics_config",description:`<strong>coarse_acoustics_config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseConfig">BarkCoarseConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying coarse acoustics sub-model.`,name:"coarse_acoustics_config"},{anchor:"transformers.BarkConfig.fine_acoustics_config",description:`<strong>fine_acoustics_config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineConfig">BarkFineConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying fine acoustics sub-model.`,name:"fine_acoustics_config"},{anchor:"transformers.BarkConfig.codec_config",description:`<strong>codec_config</strong> (<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoConfig">AutoConfig</a>, <em>optional</em>) &#x2014;
Configuration of the underlying codec sub-model.</p>
<p>Example &#x2014;`,name:"codec_config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L219"}}),Re=new B({props:{name:"from_sub_model_configs",anchor:"transformers.BarkConfig.from_sub_model_configs",parameters:[{name:"semantic_config",val:": BarkSemanticConfig"},{name:"coarse_acoustics_config",val:": BarkCoarseConfig"},{name:"fine_acoustics_config",val:": BarkFineConfig"},{name:"codec_config",val:": PretrainedConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L309",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkConfig"
>BarkConfig</a></p>
`}}),Qe=new x({props:{title:"BarkProcessor",local:"transformers.BarkProcessor",headingTag:"h2"}}),De=new B({props:{name:"class transformers.BarkProcessor",anchor:"transformers.BarkProcessor",parameters:[{name:"tokenizer",val:""},{name:"speaker_embeddings",val:" = None"}],parametersDescription:[{anchor:"transformers.BarkProcessor.tokenizer",description:`<strong>tokenizer</strong> (<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>) &#x2014;
An instance of <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizer">PreTrainedTokenizer</a>.`,name:"tokenizer"},{anchor:"transformers.BarkProcessor.speaker_embeddings",description:`<strong>speaker_embeddings</strong> (<code>Dict[Dict[str]]</code>, <em>optional</em>) &#x2014;
Optional nested speaker embeddings dictionary. The first level contains voice preset names (e.g
<code>&quot;en_speaker_4&quot;</code>). The second level contains <code>&quot;semantic_prompt&quot;</code>, <code>&quot;coarse_prompt&quot;</code> and <code>&quot;fine_prompt&quot;</code>
embeddings. The values correspond to the path of the corresponding <code>np.ndarray</code>. See
<a href="https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c" rel="nofollow">here</a> for
a list of <code>voice_preset_names</code>.`,name:"speaker_embeddings"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L34"}}),Oe=new B({props:{name:"__call__",anchor:"transformers.BarkProcessor.__call__",parameters:[{name:"text",val:" = None"},{name:"voice_preset",val:" = None"},{name:"return_tensors",val:" = 'pt'"},{name:"max_length",val:" = 256"},{name:"add_special_tokens",val:" = False"},{name:"return_attention_mask",val:" = True"},{name:"return_token_type_ids",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.__call__.text",description:`<strong>text</strong> (<code>str</code>, <code>List[str]</code>, <code>List[List[str]]</code>) &#x2014;
The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings
(pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set
<code>is_split_into_words=True</code> (to lift the ambiguity with a batch of sequences).`,name:"text"},{anchor:"transformers.BarkProcessor.__call__.voice_preset",description:`<strong>voice_preset</strong> (<code>str</code>, <code>Dict[np.ndarray]</code>) &#x2014;
The voice preset, i.e the speaker embeddings. It can either be a valid voice_preset name, e.g
<code>&quot;en_speaker_1&quot;</code>, or directly a dictionnary of <code>np.ndarray</code> embeddings for each submodel of <code>Bark</code>. Or
it can be a valid file name of a local <code>.npz</code> single voice preset.`,name:"voice_preset"},{anchor:"transformers.BarkProcessor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/ja/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors of a particular framework. Acceptable values are:</p>
<ul>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return NumPy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L218",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A tuple composed of a <a
  href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a>, i.e the output of the
<code>tokenizer</code> and a <a
  href="/docs/transformers/main/ja/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a>, i.e the voice preset with the right tensors type.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Tuple(<a
  href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.BatchEncoding"
>BatchEncoding</a>, <a
  href="/docs/transformers/main/ja/main_classes/image_processor#transformers.BatchFeature"
>BatchFeature</a>)</p>
`}}),Ae=new B({props:{name:"from_pretrained",anchor:"transformers.BarkProcessor.from_pretrained",parameters:[{name:"pretrained_processor_name_or_path",val:""},{name:"speaker_embeddings_dict_path",val:" = 'speaker_embeddings_path.json'"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.from_pretrained.pretrained_model_name_or_path",description:`<strong>pretrained_model_name_or_path</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
This can be either:</p>
<ul>
<li>a string, the <em>model id</em> of a pretrained <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkProcessor">BarkProcessor</a> hosted inside a model repo on
huggingface.co.</li>
<li>a path to a <em>directory</em> containing a processor saved using the <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkProcessor.save_pretrained">save_pretrained()</a>
method, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"pretrained_model_name_or_path"},{anchor:"transformers.BarkProcessor.from_pretrained.speaker_embeddings_dict_path",description:`<strong>speaker_embeddings_dict_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings_path.json&quot;</code>) &#x2014;
The name of the <code>.json</code> file containing the speaker_embeddings dictionnary located in
<code>pretrained_model_name_or_path</code>. If <code>None</code>, no speaker_embeddings is loaded.
**kwargs &#x2014;
Additional keyword arguments passed along to both
<code>~tokenization_utils_base.PreTrainedTokenizer.from_pretrained</code>.`,name:"speaker_embeddings_dict_path"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L64"}}),Ke=new B({props:{name:"save_pretrained",anchor:"transformers.BarkProcessor.save_pretrained",parameters:[{name:"save_directory",val:""},{name:"speaker_embeddings_dict_path",val:" = 'speaker_embeddings_path.json'"},{name:"speaker_embeddings_directory",val:" = 'speaker_embeddings'"},{name:"push_to_hub",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkProcessor.save_pretrained.save_directory",description:`<strong>save_directory</strong> (<code>str</code> or <code>os.PathLike</code>) &#x2014;
Directory where the tokenizer files and the speaker embeddings will be saved (directory will be created
if it does not exist).`,name:"save_directory"},{anchor:"transformers.BarkProcessor.save_pretrained.speaker_embeddings_dict_path",description:`<strong>speaker_embeddings_dict_path</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings_path.json&quot;</code>) &#x2014;
The name of the <code>.json</code> file that will contains the speaker_embeddings nested path dictionnary, if it
exists, and that will be located in <code>pretrained_model_name_or_path/speaker_embeddings_directory</code>.`,name:"speaker_embeddings_dict_path"},{anchor:"transformers.BarkProcessor.save_pretrained.speaker_embeddings_directory",description:`<strong>speaker_embeddings_directory</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;speaker_embeddings/&quot;</code>) &#x2014;
The name of the folder in which the speaker_embeddings arrays will be saved.`,name:"speaker_embeddings_directory"},{anchor:"transformers.BarkProcessor.save_pretrained.push_to_hub",description:`<strong>push_to_hub</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
repository you want to push to with <code>repo_id</code> (will default to the name of <code>save_directory</code> in your
namespace).
kwargs &#x2014;
Additional key word arguments passed along to the <a href="/docs/transformers/main/ja/main_classes/model#transformers.utils.PushToHubMixin.push_to_hub">push_to_hub()</a> method.`,name:"push_to_hub"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/processing_bark.py#L117"}}),et=new x({props:{title:"BarkModel",local:"transformers.BarkModel",headingTag:"h2"}}),tt=new B({props:{name:"class transformers.BarkModel",anchor:"transformers.BarkModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkConfig">BarkConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1629"}}),ot=new B({props:{name:"generate",anchor:"transformers.BarkModel.generate",parameters:[{name:"input_ids",val:": Optional = None"},{name:"history_prompt",val:": Optional = None"},{name:"return_output_lengths",val:": Optional = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkModel.generate.input_ids",description:`<strong>input_ids</strong> (<code>Optional[torch.Tensor]</code> of shape (batch_size, seq_len), <em>optional</em>) &#x2014;
Input ids. Will be truncated up to 256 tokens. Note that the output audios will be as long as the
longest generation among the batch.`,name:"input_ids"},{anchor:"transformers.BarkModel.generate.history_prompt",description:`<strong>history_prompt</strong> (<code>Optional[Dict[str,torch.Tensor]]</code>, <em>optional</em>) &#x2014;
Optional <code>Bark</code> speaker prompt. Note that for now, this model takes only one speaker prompt per batch.`,name:"history_prompt"},{anchor:"transformers.BarkModel.generate.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014; Remaining dictionary of keyword arguments. Keyword arguments are of two types:</p>
<ul>
<li>Without a prefix, they will be entered as <code>**kwargs</code> for the <code>generate</code> method of each sub-model.</li>
<li>With a <em>semantic_</em>, <em>coarse_</em>, <em>fine_</em> prefix, they will be input for the <code>generate</code> method of the
semantic, coarse and fine respectively. It has the priority over the keywords without a prefix.</li>
</ul>
<p>This means you can, for example, specify a generation strategy for all sub-models except one.`,name:"kwargs"},{anchor:"transformers.BarkModel.generate.return_output_lengths",description:`<strong>return_output_lengths</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the waveform lengths. Useful when batching.`,name:"return_output_lengths"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1737",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<ul>
<li><strong>audio_waveform</strong> (<code>torch.Tensor</code> of shape (batch_size, seq_len)): Generated audio waveform.
When <code>return_output_lengths=True</code>:
Returns a tuple made of:</li>
<li><strong>audio_waveform</strong> (<code>torch.Tensor</code> of shape (batch_size, seq_len)): Generated audio waveform.</li>
<li><strong>output_lengths</strong> (<code>torch.Tensor</code> of shape (batch_size)): The length of each waveform in the batch</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>By default</p>
`}}),A=new fn({props:{anchor:"transformers.BarkModel.generate.example",$$slots:{default:[dr]},$$scope:{ctx:j}}}),nt=new B({props:{name:"enable_cpu_offload",anchor:"transformers.BarkModel.enable_cpu_offload",parameters:[{name:"gpu_id",val:": Optional = 0"}],parametersDescription:[{anchor:"transformers.BarkModel.enable_cpu_offload.gpu_id",description:`<strong>gpu_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
GPU id on which the sub-models will be loaded and offloaded.`,name:"gpu_id"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1680"}}),at=new x({props:{title:"BarkSemanticModel",local:"transformers.BarkSemanticModel",headingTag:"h2"}}),rt=new B({props:{name:"class transformers.BarkSemanticModel",anchor:"transformers.BarkSemanticModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkSemanticModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticConfig">BarkSemanticConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L913"}}),st=new B({props:{name:"forward",anchor:"transformers.BarkSemanticModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkSemanticModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkSemanticModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkSemanticModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkSemanticModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkSemanticModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkSemanticModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkSemanticModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkSemanticModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkSemanticModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkSemanticModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L749"}}),ee=new pn({props:{$$slots:{default:[cr]},$$scope:{ctx:j}}}),it=new x({props:{title:"BarkCoarseModel",local:"transformers.BarkCoarseModel",headingTag:"h2"}}),lt=new B({props:{name:"class transformers.BarkCoarseModel",anchor:"transformers.BarkCoarseModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkCoarseModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseConfig">BarkCoarseConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1022"}}),dt=new B({props:{name:"forward",anchor:"transformers.BarkCoarseModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkCoarseModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkCoarseModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkCoarseModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkCoarseModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkCoarseModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkCoarseModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkCoarseModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkCoarseModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkCoarseModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkCoarseModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L749"}}),te=new pn({props:{$$slots:{default:[mr]},$$scope:{ctx:j}}}),ct=new x({props:{title:"BarkFineModel",local:"transformers.BarkFineModel",headingTag:"h2"}}),mt=new B({props:{name:"class transformers.BarkFineModel",anchor:"transformers.BarkFineModel",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.BarkFineModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineConfig">BarkFineConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1243"}}),pt=new B({props:{name:"forward",anchor:"transformers.BarkFineModel.forward",parameters:[{name:"codebook_idx",val:": int"},{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkFineModel.forward.codebook_idx",description:`<strong>codebook_idx</strong> (<code>int</code>) &#x2014;
Index of the codebook that will be predicted.`,name:"codebook_idx"},{anchor:"transformers.BarkFineModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length, number_of_codebooks)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Initially, indices of the first two codebooks are obtained from the <code>coarse</code> sub-model. The rest is
predicted recursively by attending the previously predicted channels. The model predicts on windows of
length 1024.`,name:"input_ids"},{anchor:"transformers.BarkFineModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkFineModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkFineModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkFineModel.forward.labels",description:"<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014; NOT IMPLEMENTED YET.",name:"labels"},{anchor:"transformers.BarkFineModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. If
<code>past_key_values</code> is used, optionally only the last <code>input_embeds</code> have to be input (see
<code>past_key_values</code>). This is useful if you want more control over how to convert <code>input_ids</code> indices into
associated vectors than the model&#x2019;s internal embedding lookup matrix.`,name:"input_embeds"},{anchor:"transformers.BarkFineModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkFineModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkFineModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L1381"}}),oe=new pn({props:{$$slots:{default:[pr]},$$scope:{ctx:j}}}),ft=new x({props:{title:"BarkCausalModel",local:"transformers.BarkCausalModel",headingTag:"h2"}}),ut=new B({props:{name:"class transformers.BarkCausalModel",anchor:"transformers.BarkCausalModel",parameters:[{name:"config",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L658"}}),ht=new B({props:{name:"forward",anchor:"transformers.BarkCausalModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"past_key_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"input_embeds",val:": Optional = None"},{name:"use_cache",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.BarkCausalModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it. Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details. <a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.BarkCausalModel.forward.past_key_values",description:`<strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache</code> is passed or when <code>config.use_cache=True</code>) &#x2014;
Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
<code>past_key_values</code> input) to speed up sequential decoding.</p>
<p>If <code>past_key_values</code> are used, the user can optionally input only the last <code>decoder_input_ids</code> (those that
don&#x2019;t have their past key value states given to this model) of shape <code>(batch_size, 1)</code> instead of all
<code>input_ids</code> of shape <code>(batch_size, sequence_length)</code>.`,name:"past_key_values"},{anchor:"transformers.BarkCausalModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.BarkCausalModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.BarkCausalModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.Tensor</code> of shape <code>(encoder_layers, encoder_attention_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.BarkCausalModel.forward.input_embeds",description:`<strong>input_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, input_sequence_length, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation.
Here, due to <code>Bark</code> particularities, if <code>past_key_values</code> is used, <code>input_embeds</code> will be ignored and you
have to use <code>input_ids</code>. If <code>past_key_values</code> is not used and <code>use_cache</code> is set to <code>True</code>, <code>input_embeds</code>
is used in priority instead of <code>input_ids</code>.`,name:"input_embeds"},{anchor:"transformers.BarkCausalModel.forward.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
If set to <code>True</code>, <code>past_key_values</code> key value states are returned and can be used to speed up decoding (see
<code>past_key_values</code>).`,name:"use_cache"},{anchor:"transformers.BarkCausalModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.BarkCausalModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.BarkCausalModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/modeling_bark.py#L749"}}),ne=new pn({props:{$$slots:{default:[fr]},$$scope:{ctx:j}}}),gt=new x({props:{title:"BarkCoarseConfig",local:"transformers.BarkCoarseConfig",headingTag:"h2"}}),_t=new B({props:{name:"class transformers.BarkCoarseConfig",anchor:"transformers.BarkCoarseConfig",parameters:[{name:"block_size",val:" = 1024"},{name:"input_vocab_size",val:" = 10048"},{name:"output_vocab_size",val:" = 10048"},{name:"num_layers",val:" = 12"},{name:"num_heads",val:" = 12"},{name:"hidden_size",val:" = 768"},{name:"dropout",val:" = 0.0"},{name:"bias",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkCoarseConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkCoarseConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkCoarseConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkCoarseModel">BarkCoarseModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkCoarseConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkCoarseConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkCoarseConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkCoarseConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkCoarseConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkCoarseConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkCoarseConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L164"}}),ae=new fn({props:{anchor:"transformers.BarkCoarseConfig.example",$$slots:{default:[ur]},$$scope:{ctx:j}}}),bt=new x({props:{title:"BarkFineConfig",local:"transformers.BarkFineConfig",headingTag:"h2"}}),kt=new B({props:{name:"class transformers.BarkFineConfig",anchor:"transformers.BarkFineConfig",parameters:[{name:"tie_word_embeddings",val:" = True"},{name:"n_codes_total",val:" = 8"},{name:"n_codes_given",val:" = 1"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkFineConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkFineConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkFineConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkFineModel">BarkFineModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkFineConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkFineConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkFineConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkFineConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkFineConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkFineConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkFineConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"},{anchor:"transformers.BarkFineConfig.n_codes_total",description:`<strong>n_codes_total</strong> (<code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
The total number of audio codebooks predicted. Used in the fine acoustics sub-model.`,name:"n_codes_total"},{anchor:"transformers.BarkFineConfig.n_codes_given",description:`<strong>n_codes_given</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The number of audio codebooks predicted in the coarse acoustics sub-model. Used in the acoustics
sub-models.`,name:"n_codes_given"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L186"}}),re=new fn({props:{anchor:"transformers.BarkFineConfig.example",$$slots:{default:[hr]},$$scope:{ctx:j}}}),yt=new x({props:{title:"BarkSemanticConfig",local:"transformers.BarkSemanticConfig",headingTag:"h2"}}),vt=new B({props:{name:"class transformers.BarkSemanticConfig",anchor:"transformers.BarkSemanticConfig",parameters:[{name:"block_size",val:" = 1024"},{name:"input_vocab_size",val:" = 10048"},{name:"output_vocab_size",val:" = 10048"},{name:"num_layers",val:" = 12"},{name:"num_heads",val:" = 12"},{name:"hidden_size",val:" = 768"},{name:"dropout",val:" = 0.0"},{name:"bias",val:" = True"},{name:"initializer_range",val:" = 0.02"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BarkSemanticConfig.block_size",description:`<strong>block_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"block_size"},{anchor:"transformers.BarkSemanticConfig.input_vocab_size",description:`<strong>input_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. Defaults to 10_048 but should be carefully thought with
regards to the chosen sub-model.`,name:"input_vocab_size"},{anchor:"transformers.BarkSemanticConfig.output_vocab_size",description:`<strong>output_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 10_048) &#x2014;
Output vocabulary size of a Bark sub-model. Defines the number of different tokens that can be represented
by the: <code>output_ids</code> when passing forward a <a href="/docs/transformers/main/ja/model_doc/bark#transformers.BarkSemanticModel">BarkSemanticModel</a>. Defaults to 10_048 but should be carefully thought
with regards to the chosen sub-model.`,name:"output_vocab_size"},{anchor:"transformers.BarkSemanticConfig.num_layers",description:`<strong>num_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the given sub-model.`,name:"num_layers"},{anchor:"transformers.BarkSemanticConfig.num_heads",description:`<strong>num_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer architecture.`,name:"num_heads"},{anchor:"transformers.BarkSemanticConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the architecture.`,name:"hidden_size"},{anchor:"transformers.BarkSemanticConfig.dropout",description:`<strong>dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"dropout"},{anchor:"transformers.BarkSemanticConfig.bias",description:`<strong>bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to use bias in the linear layers and layer norm layers.`,name:"bias"},{anchor:"transformers.BarkSemanticConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.BarkSemanticConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models).`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/bark/configuration_bark.py#L142"}}),se=new fn({props:{anchor:"transformers.BarkSemanticConfig.example",$$slots:{default:[gr]},$$scope:{ctx:j}}}),{c(){d=i("meta"),y=n(),b=i("p"),k=n(),m(v.$$.fragment),c=n(),m(M.$$.fragment),lo=n(),ue=i("p"),ue.innerHTML=da,co=n(),he=i("p"),he.textContent=ca,mo=n(),ge=i("ul"),ge.innerHTML=ma,po=n(),_e=i("p"),_e.textContent=pa,fo=n(),m(be.$$.fragment),uo=n(),ke=i("p"),ke.innerHTML=fa,ho=n(),m(ye.$$.fragment),go=n(),ve=i("p"),ve.textContent=ua,_o=n(),m(Me.$$.fragment),bo=n(),m(we.$$.fragment),ko=n(),$e=i("p"),$e.textContent=ha,yo=n(),m(Te.$$.fragment),vo=n(),Be=i("p"),Be.innerHTML=ga,Mo=n(),m(Ce.$$.fragment),wo=n(),je=i("p"),je.textContent=_a,$o=n(),xe=i("p"),xe.textContent=ba,To=n(),m(Je.$$.fragment),Bo=n(),ze=i("p"),ze.innerHTML=ka,Co=n(),m(Ue.$$.fragment),jo=n(),Ie=i("p"),Ie.textContent=ya,xo=n(),m(We.$$.fragment),Jo=n(),Ze=i("p"),Ze.innerHTML=va,zo=n(),m(Pe.$$.fragment),Uo=n(),Ne=i("p"),Ne.innerHTML=Ma,Io=n(),m(Fe.$$.fragment),Wo=n(),Ge=i("p"),Ge.innerHTML=wa,Zo=n(),m(He.$$.fragment),Po=n(),Se=i("p"),Se.innerHTML=$a,No=n(),m(Ve.$$.fragment),Fo=n(),Le=i("p"),Le.textContent=Ta,Go=n(),m(Ye.$$.fragment),Ho=n(),Ee=i("p"),Ee.innerHTML=Ba,So=n(),m(Xe.$$.fragment),Vo=n(),J=i("div"),m(qe.$$.fragment),un=n(),Bt=i("p"),Bt.innerHTML=Ca,hn=n(),Ct=i("p"),Ct.innerHTML=ja,gn=n(),jt=i("p"),jt.innerHTML=xa,_n=n(),R=i("div"),m(Re.$$.fragment),bn=n(),xt=i("p"),xt.innerHTML=Ja,Lo=n(),m(Qe.$$.fragment),Yo=n(),z=i("div"),m(De.$$.fragment),kn=n(),Jt=i("p"),Jt.textContent=za,yn=n(),Q=i("div"),m(Oe.$$.fragment),vn=n(),zt=i("p"),zt.innerHTML=Ua,Mn=n(),D=i("div"),m(Ae.$$.fragment),wn=n(),Ut=i("p"),Ut.textContent=Ia,$n=n(),O=i("div"),m(Ke.$$.fragment),Tn=n(),It=i("p"),It.innerHTML=Wa,Eo=n(),m(et.$$.fragment),Xo=n(),T=i("div"),m(tt.$$.fragment),Bn=n(),Wt=i("p"),Wt.textContent=Za,Cn=n(),Zt=i("ul"),Zt.innerHTML=Pa,jn=n(),Pt=i("p"),Pt.textContent=Na,xn=n(),Nt=i("p"),Nt.innerHTML=Fa,Jn=n(),Ft=i("p"),Ft.innerHTML=Ga,zn=n(),S=i("div"),m(ot.$$.fragment),Un=n(),Gt=i("p"),Gt.innerHTML=Ha,In=n(),m(A.$$.fragment),Wn=n(),K=i("div"),m(nt.$$.fragment),Zn=n(),Ht=i("p"),Ht.textContent=Sa,qo=n(),m(at.$$.fragment),Ro=n(),U=i("div"),m(rt.$$.fragment),Pn=n(),St=i("p"),St.innerHTML=Va,Nn=n(),Vt=i("p"),Vt.innerHTML=La,Fn=n(),V=i("div"),m(st.$$.fragment),Gn=n(),Lt=i("p"),Lt.innerHTML=Ya,Hn=n(),m(ee.$$.fragment),Qo=n(),m(it.$$.fragment),Do=n(),I=i("div"),m(lt.$$.fragment),Sn=n(),Yt=i("p"),Yt.innerHTML=Ea,Vn=n(),Et=i("p"),Et.innerHTML=Xa,Ln=n(),L=i("div"),m(dt.$$.fragment),Yn=n(),Xt=i("p"),Xt.innerHTML=qa,En=n(),m(te.$$.fragment),Oo=n(),m(ct.$$.fragment),Ao=n(),W=i("div"),m(mt.$$.fragment),Xn=n(),qt=i("p"),qt.innerHTML=Ra,qn=n(),Rt=i("p"),Rt.innerHTML=Qa,Rn=n(),Y=i("div"),m(pt.$$.fragment),Qn=n(),Qt=i("p"),Qt.innerHTML=Da,Dn=n(),m(oe.$$.fragment),Ko=n(),m(ft.$$.fragment),en=n(),X=i("div"),m(ut.$$.fragment),On=n(),E=i("div"),m(ht.$$.fragment),An=n(),Dt=i("p"),Dt.innerHTML=Oa,Kn=n(),m(ne.$$.fragment),tn=n(),m(gt.$$.fragment),on=n(),Z=i("div"),m(_t.$$.fragment),ea=n(),Ot=i("p"),Ot.innerHTML=Aa,ta=n(),At=i("p"),At.innerHTML=Ka,oa=n(),m(ae.$$.fragment),nn=n(),m(bt.$$.fragment),an=n(),P=i("div"),m(kt.$$.fragment),na=n(),Kt=i("p"),Kt.innerHTML=er,aa=n(),eo=i("p"),eo.innerHTML=tr,ra=n(),m(re.$$.fragment),rn=n(),m(yt.$$.fragment),sn=n(),N=i("div"),m(vt.$$.fragment),sa=n(),to=i("p"),to.innerHTML=or,ia=n(),oo=i("p"),oo.innerHTML=nr,la=n(),m(se.$$.fragment),ln=n(),io=i("p"),this.h()},l(e){const t=lr("svelte-u9bgzb",document.head);d=l(t,"META",{name:!0,content:!0}),t.forEach(o),y=a(e),b=l(e,"P",{}),w(b).forEach(o),k=a(e),p(v.$$.fragment,e),c=a(e),p(M.$$.fragment,e),lo=a(e),ue=l(e,"P",{"data-svelte-h":!0}),_(ue)!=="svelte-hjrtqs"&&(ue.innerHTML=da),co=a(e),he=l(e,"P",{"data-svelte-h":!0}),_(he)!=="svelte-1g9ayez"&&(he.textContent=ca),mo=a(e),ge=l(e,"UL",{"data-svelte-h":!0}),_(ge)!=="svelte-1p97wge"&&(ge.innerHTML=ma),po=a(e),_e=l(e,"P",{"data-svelte-h":!0}),_(_e)!=="svelte-14jfsv4"&&(_e.textContent=pa),fo=a(e),p(be.$$.fragment,e),uo=a(e),ke=l(e,"P",{"data-svelte-h":!0}),_(ke)!=="svelte-snmu9t"&&(ke.innerHTML=fa),ho=a(e),p(ye.$$.fragment,e),go=a(e),ve=l(e,"P",{"data-svelte-h":!0}),_(ve)!=="svelte-1f4itv2"&&(ve.textContent=ua),_o=a(e),p(Me.$$.fragment,e),bo=a(e),p(we.$$.fragment,e),ko=a(e),$e=l(e,"P",{"data-svelte-h":!0}),_($e)!=="svelte-b3n96m"&&($e.textContent=ha),yo=a(e),p(Te.$$.fragment,e),vo=a(e),Be=l(e,"P",{"data-svelte-h":!0}),_(Be)!=="svelte-19sxe7o"&&(Be.innerHTML=ga),Mo=a(e),p(Ce.$$.fragment,e),wo=a(e),je=l(e,"P",{"data-svelte-h":!0}),_(je)!=="svelte-ftbp94"&&(je.textContent=_a),$o=a(e),xe=l(e,"P",{"data-svelte-h":!0}),_(xe)!=="svelte-d00y1u"&&(xe.textContent=ba),To=a(e),p(Je.$$.fragment,e),Bo=a(e),ze=l(e,"P",{"data-svelte-h":!0}),_(ze)!=="svelte-m13j61"&&(ze.innerHTML=ka),Co=a(e),p(Ue.$$.fragment,e),jo=a(e),Ie=l(e,"P",{"data-svelte-h":!0}),_(Ie)!=="svelte-c8agyr"&&(Ie.textContent=ya),xo=a(e),p(We.$$.fragment,e),Jo=a(e),Ze=l(e,"P",{"data-svelte-h":!0}),_(Ze)!=="svelte-12hzxgd"&&(Ze.innerHTML=va),zo=a(e),p(Pe.$$.fragment,e),Uo=a(e),Ne=l(e,"P",{"data-svelte-h":!0}),_(Ne)!=="svelte-1demwxa"&&(Ne.innerHTML=Ma),Io=a(e),p(Fe.$$.fragment,e),Wo=a(e),Ge=l(e,"P",{"data-svelte-h":!0}),_(Ge)!=="svelte-nf94c5"&&(Ge.innerHTML=wa),Zo=a(e),p(He.$$.fragment,e),Po=a(e),Se=l(e,"P",{"data-svelte-h":!0}),_(Se)!=="svelte-1q66j1u"&&(Se.innerHTML=$a),No=a(e),p(Ve.$$.fragment,e),Fo=a(e),Le=l(e,"P",{"data-svelte-h":!0}),_(Le)!=="svelte-sn68fl"&&(Le.textContent=Ta),Go=a(e),p(Ye.$$.fragment,e),Ho=a(e),Ee=l(e,"P",{"data-svelte-h":!0}),_(Ee)!=="svelte-31lx8o"&&(Ee.innerHTML=Ba),So=a(e),p(Xe.$$.fragment,e),Vo=a(e),J=l(e,"DIV",{class:!0});var F=w(J);p(qe.$$.fragment,F),un=a(F),Bt=l(F,"P",{"data-svelte-h":!0}),_(Bt)!=="svelte-13qu8bv"&&(Bt.innerHTML=Ca),hn=a(F),Ct=l(F,"P",{"data-svelte-h":!0}),_(Ct)!=="svelte-c6ui3q"&&(Ct.innerHTML=ja),gn=a(F),jt=l(F,"P",{"data-svelte-h":!0}),_(jt)!=="svelte-1s6wgpv"&&(jt.innerHTML=xa),_n=a(F),R=l(F,"DIV",{class:!0});var Mt=w(R);p(Re.$$.fragment,Mt),bn=a(Mt),xt=l(Mt,"P",{"data-svelte-h":!0}),_(xt)!=="svelte-2lhxaj"&&(xt.innerHTML=Ja),Mt.forEach(o),F.forEach(o),Lo=a(e),p(Qe.$$.fragment,e),Yo=a(e),z=l(e,"DIV",{class:!0});var G=w(z);p(De.$$.fragment,G),kn=a(G),Jt=l(G,"P",{"data-svelte-h":!0}),_(Jt)!=="svelte-1xfrjvw"&&(Jt.textContent=za),yn=a(G),Q=l(G,"DIV",{class:!0});var wt=w(Q);p(Oe.$$.fragment,wt),vn=a(wt),zt=l(wt,"P",{"data-svelte-h":!0}),_(zt)!=="svelte-1hlbbl8"&&(zt.innerHTML=Ua),wt.forEach(o),Mn=a(G),D=l(G,"DIV",{class:!0});var $t=w(D);p(Ae.$$.fragment,$t),wn=a($t),Ut=l($t,"P",{"data-svelte-h":!0}),_(Ut)!=="svelte-1xt1aup"&&(Ut.textContent=Ia),$t.forEach(o),$n=a(G),O=l(G,"DIV",{class:!0});var Tt=w(O);p(Ke.$$.fragment,Tt),Tn=a(Tt),It=l(Tt,"P",{"data-svelte-h":!0}),_(It)!=="svelte-lp2u11"&&(It.innerHTML=Wa),Tt.forEach(o),G.forEach(o),Eo=a(e),p(et.$$.fragment,e),Xo=a(e),T=l(e,"DIV",{class:!0});var C=w(T);p(tt.$$.fragment,C),Bn=a(C),Wt=l(C,"P",{"data-svelte-h":!0}),_(Wt)!=="svelte-xp33tl"&&(Wt.textContent=Za),Cn=a(C),Zt=l(C,"UL",{"data-svelte-h":!0}),_(Zt)!=="svelte-131dx8r"&&(Zt.innerHTML=Pa),jn=a(C),Pt=l(C,"P",{"data-svelte-h":!0}),_(Pt)!=="svelte-beeiv6"&&(Pt.textContent=Na),xn=a(C),Nt=l(C,"P",{"data-svelte-h":!0}),_(Nt)!=="svelte-eisylu"&&(Nt.innerHTML=Fa),Jn=a(C),Ft=l(C,"P",{"data-svelte-h":!0}),_(Ft)!=="svelte-hswkmf"&&(Ft.innerHTML=Ga),zn=a(C),S=l(C,"DIV",{class:!0});var q=w(S);p(ot.$$.fragment,q),Un=a(q),Gt=l(q,"P",{"data-svelte-h":!0}),_(Gt)!=="svelte-4azpa"&&(Gt.innerHTML=Ha),In=a(q),p(A.$$.fragment,q),q.forEach(o),Wn=a(C),K=l(C,"DIV",{class:!0});var cn=w(K);p(nt.$$.fragment,cn),Zn=a(cn),Ht=l(cn,"P",{"data-svelte-h":!0}),_(Ht)!=="svelte-19e6niw"&&(Ht.textContent=Sa),cn.forEach(o),C.forEach(o),qo=a(e),p(at.$$.fragment,e),Ro=a(e),U=l(e,"DIV",{class:!0});var ie=w(U);p(rt.$$.fragment,ie),Pn=a(ie),St=l(ie,"P",{"data-svelte-h":!0}),_(St)!=="svelte-1kz3hoi"&&(St.innerHTML=Va),Nn=a(ie),Vt=l(ie,"P",{"data-svelte-h":!0}),_(Vt)!=="svelte-hswkmf"&&(Vt.innerHTML=La),Fn=a(ie),V=l(ie,"DIV",{class:!0});var no=w(V);p(st.$$.fragment,no),Gn=a(no),Lt=l(no,"P",{"data-svelte-h":!0}),_(Lt)!=="svelte-um9dc7"&&(Lt.innerHTML=Ya),Hn=a(no),p(ee.$$.fragment,no),no.forEach(o),ie.forEach(o),Qo=a(e),p(it.$$.fragment,e),Do=a(e),I=l(e,"DIV",{class:!0});var le=w(I);p(lt.$$.fragment,le),Sn=a(le),Yt=l(le,"P",{"data-svelte-h":!0}),_(Yt)!=="svelte-1r45z6k"&&(Yt.innerHTML=Ea),Vn=a(le),Et=l(le,"P",{"data-svelte-h":!0}),_(Et)!=="svelte-hswkmf"&&(Et.innerHTML=Xa),Ln=a(le),L=l(le,"DIV",{class:!0});var ao=w(L);p(dt.$$.fragment,ao),Yn=a(ao),Xt=l(ao,"P",{"data-svelte-h":!0}),_(Xt)!=="svelte-um9dc7"&&(Xt.innerHTML=qa),En=a(ao),p(te.$$.fragment,ao),ao.forEach(o),le.forEach(o),Oo=a(e),p(ct.$$.fragment,e),Ao=a(e),W=l(e,"DIV",{class:!0});var de=w(W);p(mt.$$.fragment,de),Xn=a(de),qt=l(de,"P",{"data-svelte-h":!0}),_(qt)!=="svelte-1eci89x"&&(qt.innerHTML=Ra),qn=a(de),Rt=l(de,"P",{"data-svelte-h":!0}),_(Rt)!=="svelte-hswkmf"&&(Rt.innerHTML=Qa),Rn=a(de),Y=l(de,"DIV",{class:!0});var ro=w(Y);p(pt.$$.fragment,ro),Qn=a(ro),Qt=l(ro,"P",{"data-svelte-h":!0}),_(Qt)!=="svelte-gqhnvb"&&(Qt.innerHTML=Da),Dn=a(ro),p(oe.$$.fragment,ro),ro.forEach(o),de.forEach(o),Ko=a(e),p(ft.$$.fragment,e),en=a(e),X=l(e,"DIV",{class:!0});var mn=w(X);p(ut.$$.fragment,mn),On=a(mn),E=l(mn,"DIV",{class:!0});var so=w(E);p(ht.$$.fragment,so),An=a(so),Dt=l(so,"P",{"data-svelte-h":!0}),_(Dt)!=="svelte-um9dc7"&&(Dt.innerHTML=Oa),Kn=a(so),p(ne.$$.fragment,so),so.forEach(o),mn.forEach(o),tn=a(e),p(gt.$$.fragment,e),on=a(e),Z=l(e,"DIV",{class:!0});var ce=w(Z);p(_t.$$.fragment,ce),ea=a(ce),Ot=l(ce,"P",{"data-svelte-h":!0}),_(Ot)!=="svelte-12d8l23"&&(Ot.innerHTML=Aa),ta=a(ce),At=l(ce,"P",{"data-svelte-h":!0}),_(At)!=="svelte-1s6wgpv"&&(At.innerHTML=Ka),oa=a(ce),p(ae.$$.fragment,ce),ce.forEach(o),nn=a(e),p(bt.$$.fragment,e),an=a(e),P=l(e,"DIV",{class:!0});var me=w(P);p(kt.$$.fragment,me),na=a(me),Kt=l(me,"P",{"data-svelte-h":!0}),_(Kt)!=="svelte-b4cucb"&&(Kt.innerHTML=er),aa=a(me),eo=l(me,"P",{"data-svelte-h":!0}),_(eo)!=="svelte-1s6wgpv"&&(eo.innerHTML=tr),ra=a(me),p(re.$$.fragment,me),me.forEach(o),rn=a(e),p(yt.$$.fragment,e),sn=a(e),N=l(e,"DIV",{class:!0});var pe=w(N);p(vt.$$.fragment,pe),sa=a(pe),to=l(pe,"P",{"data-svelte-h":!0}),_(to)!=="svelte-1u69emf"&&(to.innerHTML=or),ia=a(pe),oo=l(pe,"P",{"data-svelte-h":!0}),_(oo)!=="svelte-1s6wgpv"&&(oo.innerHTML=nr),la=a(pe),p(se.$$.fragment,pe),pe.forEach(o),ln=a(e),io=l(e,"P",{}),w(io).forEach(o),this.h()},h(){$(d,"name","hf:doc:metadata"),$(d,"content",br),$(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(O,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){s(document.head,d),r(e,y,t),r(e,b,t),r(e,k,t),f(v,e,t),r(e,c,t),f(M,e,t),r(e,lo,t),r(e,ue,t),r(e,co,t),r(e,he,t),r(e,mo,t),r(e,ge,t),r(e,po,t),r(e,_e,t),r(e,fo,t),f(be,e,t),r(e,uo,t),r(e,ke,t),r(e,ho,t),f(ye,e,t),r(e,go,t),r(e,ve,t),r(e,_o,t),f(Me,e,t),r(e,bo,t),f(we,e,t),r(e,ko,t),r(e,$e,t),r(e,yo,t),f(Te,e,t),r(e,vo,t),r(e,Be,t),r(e,Mo,t),f(Ce,e,t),r(e,wo,t),r(e,je,t),r(e,$o,t),r(e,xe,t),r(e,To,t),f(Je,e,t),r(e,Bo,t),r(e,ze,t),r(e,Co,t),f(Ue,e,t),r(e,jo,t),r(e,Ie,t),r(e,xo,t),f(We,e,t),r(e,Jo,t),r(e,Ze,t),r(e,zo,t),f(Pe,e,t),r(e,Uo,t),r(e,Ne,t),r(e,Io,t),f(Fe,e,t),r(e,Wo,t),r(e,Ge,t),r(e,Zo,t),f(He,e,t),r(e,Po,t),r(e,Se,t),r(e,No,t),f(Ve,e,t),r(e,Fo,t),r(e,Le,t),r(e,Go,t),f(Ye,e,t),r(e,Ho,t),r(e,Ee,t),r(e,So,t),f(Xe,e,t),r(e,Vo,t),r(e,J,t),f(qe,J,null),s(J,un),s(J,Bt),s(J,hn),s(J,Ct),s(J,gn),s(J,jt),s(J,_n),s(J,R),f(Re,R,null),s(R,bn),s(R,xt),r(e,Lo,t),f(Qe,e,t),r(e,Yo,t),r(e,z,t),f(De,z,null),s(z,kn),s(z,Jt),s(z,yn),s(z,Q),f(Oe,Q,null),s(Q,vn),s(Q,zt),s(z,Mn),s(z,D),f(Ae,D,null),s(D,wn),s(D,Ut),s(z,$n),s(z,O),f(Ke,O,null),s(O,Tn),s(O,It),r(e,Eo,t),f(et,e,t),r(e,Xo,t),r(e,T,t),f(tt,T,null),s(T,Bn),s(T,Wt),s(T,Cn),s(T,Zt),s(T,jn),s(T,Pt),s(T,xn),s(T,Nt),s(T,Jn),s(T,Ft),s(T,zn),s(T,S),f(ot,S,null),s(S,Un),s(S,Gt),s(S,In),f(A,S,null),s(T,Wn),s(T,K),f(nt,K,null),s(K,Zn),s(K,Ht),r(e,qo,t),f(at,e,t),r(e,Ro,t),r(e,U,t),f(rt,U,null),s(U,Pn),s(U,St),s(U,Nn),s(U,Vt),s(U,Fn),s(U,V),f(st,V,null),s(V,Gn),s(V,Lt),s(V,Hn),f(ee,V,null),r(e,Qo,t),f(it,e,t),r(e,Do,t),r(e,I,t),f(lt,I,null),s(I,Sn),s(I,Yt),s(I,Vn),s(I,Et),s(I,Ln),s(I,L),f(dt,L,null),s(L,Yn),s(L,Xt),s(L,En),f(te,L,null),r(e,Oo,t),f(ct,e,t),r(e,Ao,t),r(e,W,t),f(mt,W,null),s(W,Xn),s(W,qt),s(W,qn),s(W,Rt),s(W,Rn),s(W,Y),f(pt,Y,null),s(Y,Qn),s(Y,Qt),s(Y,Dn),f(oe,Y,null),r(e,Ko,t),f(ft,e,t),r(e,en,t),r(e,X,t),f(ut,X,null),s(X,On),s(X,E),f(ht,E,null),s(E,An),s(E,Dt),s(E,Kn),f(ne,E,null),r(e,tn,t),f(gt,e,t),r(e,on,t),r(e,Z,t),f(_t,Z,null),s(Z,ea),s(Z,Ot),s(Z,ta),s(Z,At),s(Z,oa),f(ae,Z,null),r(e,nn,t),f(bt,e,t),r(e,an,t),r(e,P,t),f(kt,P,null),s(P,na),s(P,Kt),s(P,aa),s(P,eo),s(P,ra),f(re,P,null),r(e,rn,t),f(yt,e,t),r(e,sn,t),r(e,N,t),f(vt,N,null),s(N,sa),s(N,to),s(N,ia),s(N,oo),s(N,la),f(se,N,null),r(e,ln,t),r(e,io,t),dn=!0},p(e,[t]){const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),A.$set(F);const Mt={};t&2&&(Mt.$$scope={dirty:t,ctx:e}),ee.$set(Mt);const G={};t&2&&(G.$$scope={dirty:t,ctx:e}),te.$set(G);const wt={};t&2&&(wt.$$scope={dirty:t,ctx:e}),oe.$set(wt);const $t={};t&2&&($t.$$scope={dirty:t,ctx:e}),ne.$set($t);const Tt={};t&2&&(Tt.$$scope={dirty:t,ctx:e}),ae.$set(Tt);const C={};t&2&&(C.$$scope={dirty:t,ctx:e}),re.$set(C);const q={};t&2&&(q.$$scope={dirty:t,ctx:e}),se.$set(q)},i(e){dn||(u(v.$$.fragment,e),u(M.$$.fragment,e),u(be.$$.fragment,e),u(ye.$$.fragment,e),u(Me.$$.fragment,e),u(we.$$.fragment,e),u(Te.$$.fragment,e),u(Ce.$$.fragment,e),u(Je.$$.fragment,e),u(Ue.$$.fragment,e),u(We.$$.fragment,e),u(Pe.$$.fragment,e),u(Fe.$$.fragment,e),u(He.$$.fragment,e),u(Ve.$$.fragment,e),u(Ye.$$.fragment,e),u(Xe.$$.fragment,e),u(qe.$$.fragment,e),u(Re.$$.fragment,e),u(Qe.$$.fragment,e),u(De.$$.fragment,e),u(Oe.$$.fragment,e),u(Ae.$$.fragment,e),u(Ke.$$.fragment,e),u(et.$$.fragment,e),u(tt.$$.fragment,e),u(ot.$$.fragment,e),u(A.$$.fragment,e),u(nt.$$.fragment,e),u(at.$$.fragment,e),u(rt.$$.fragment,e),u(st.$$.fragment,e),u(ee.$$.fragment,e),u(it.$$.fragment,e),u(lt.$$.fragment,e),u(dt.$$.fragment,e),u(te.$$.fragment,e),u(ct.$$.fragment,e),u(mt.$$.fragment,e),u(pt.$$.fragment,e),u(oe.$$.fragment,e),u(ft.$$.fragment,e),u(ut.$$.fragment,e),u(ht.$$.fragment,e),u(ne.$$.fragment,e),u(gt.$$.fragment,e),u(_t.$$.fragment,e),u(ae.$$.fragment,e),u(bt.$$.fragment,e),u(kt.$$.fragment,e),u(re.$$.fragment,e),u(yt.$$.fragment,e),u(vt.$$.fragment,e),u(se.$$.fragment,e),dn=!0)},o(e){h(v.$$.fragment,e),h(M.$$.fragment,e),h(be.$$.fragment,e),h(ye.$$.fragment,e),h(Me.$$.fragment,e),h(we.$$.fragment,e),h(Te.$$.fragment,e),h(Ce.$$.fragment,e),h(Je.$$.fragment,e),h(Ue.$$.fragment,e),h(We.$$.fragment,e),h(Pe.$$.fragment,e),h(Fe.$$.fragment,e),h(He.$$.fragment,e),h(Ve.$$.fragment,e),h(Ye.$$.fragment,e),h(Xe.$$.fragment,e),h(qe.$$.fragment,e),h(Re.$$.fragment,e),h(Qe.$$.fragment,e),h(De.$$.fragment,e),h(Oe.$$.fragment,e),h(Ae.$$.fragment,e),h(Ke.$$.fragment,e),h(et.$$.fragment,e),h(tt.$$.fragment,e),h(ot.$$.fragment,e),h(A.$$.fragment,e),h(nt.$$.fragment,e),h(at.$$.fragment,e),h(rt.$$.fragment,e),h(st.$$.fragment,e),h(ee.$$.fragment,e),h(it.$$.fragment,e),h(lt.$$.fragment,e),h(dt.$$.fragment,e),h(te.$$.fragment,e),h(ct.$$.fragment,e),h(mt.$$.fragment,e),h(pt.$$.fragment,e),h(oe.$$.fragment,e),h(ft.$$.fragment,e),h(ut.$$.fragment,e),h(ht.$$.fragment,e),h(ne.$$.fragment,e),h(gt.$$.fragment,e),h(_t.$$.fragment,e),h(ae.$$.fragment,e),h(bt.$$.fragment,e),h(kt.$$.fragment,e),h(re.$$.fragment,e),h(yt.$$.fragment,e),h(vt.$$.fragment,e),h(se.$$.fragment,e),dn=!1},d(e){e&&(o(y),o(b),o(k),o(c),o(lo),o(ue),o(co),o(he),o(mo),o(ge),o(po),o(_e),o(fo),o(uo),o(ke),o(ho),o(go),o(ve),o(_o),o(bo),o(ko),o($e),o(yo),o(vo),o(Be),o(Mo),o(wo),o(je),o($o),o(xe),o(To),o(Bo),o(ze),o(Co),o(jo),o(Ie),o(xo),o(Jo),o(Ze),o(zo),o(Uo),o(Ne),o(Io),o(Wo),o(Ge),o(Zo),o(Po),o(Se),o(No),o(Fo),o(Le),o(Go),o(Ho),o(Ee),o(So),o(Vo),o(J),o(Lo),o(Yo),o(z),o(Eo),o(Xo),o(T),o(qo),o(Ro),o(U),o(Qo),o(Do),o(I),o(Oo),o(Ao),o(W),o(Ko),o(en),o(X),o(tn),o(on),o(Z),o(nn),o(an),o(P),o(rn),o(sn),o(N),o(ln),o(io)),o(d),g(v,e),g(M,e),g(be,e),g(ye,e),g(Me,e),g(we,e),g(Te,e),g(Ce,e),g(Je,e),g(Ue,e),g(We,e),g(Pe,e),g(Fe,e),g(He,e),g(Ve,e),g(Ye,e),g(Xe,e),g(qe),g(Re),g(Qe,e),g(De),g(Oe),g(Ae),g(Ke),g(et,e),g(tt),g(ot),g(A),g(nt),g(at,e),g(rt),g(st),g(ee),g(it,e),g(lt),g(dt),g(te),g(ct,e),g(mt),g(pt),g(oe),g(ft,e),g(ut),g(ht),g(ne),g(gt,e),g(_t),g(ae),g(bt,e),g(kt),g(re),g(yt,e),g(vt),g(se)}}}const br='{"title":"Bark","local":"bark","sections":[{"title":"Overview","local":"overview","sections":[{"title":"Optimizing Bark","local":"optimizing-bark","sections":[{"title":"Using half-precision","local":"using-half-precision","sections":[],"depth":4},{"title":"Using ğŸ¤— Better Transformer","local":"using--better-transformer","sections":[],"depth":4},{"title":"Using CPU offload","local":"using-cpu-offload","sections":[],"depth":4},{"title":"Combining optimization techniques","local":"combining-optimization-techniques","sections":[],"depth":4}],"depth":3},{"title":"Tips","local":"tips","sections":[],"depth":3}],"depth":2},{"title":"BarkConfig","local":"transformers.BarkConfig","sections":[],"depth":2},{"title":"BarkProcessor","local":"transformers.BarkProcessor","sections":[],"depth":2},{"title":"BarkModel","local":"transformers.BarkModel","sections":[],"depth":2},{"title":"BarkSemanticModel","local":"transformers.BarkSemanticModel","sections":[],"depth":2},{"title":"BarkCoarseModel","local":"transformers.BarkCoarseModel","sections":[],"depth":2},{"title":"BarkFineModel","local":"transformers.BarkFineModel","sections":[],"depth":2},{"title":"BarkCausalModel","local":"transformers.BarkCausalModel","sections":[],"depth":2},{"title":"BarkCoarseConfig","local":"transformers.BarkCoarseConfig","sections":[],"depth":2},{"title":"BarkFineConfig","local":"transformers.BarkFineConfig","sections":[],"depth":2},{"title":"BarkSemanticConfig","local":"transformers.BarkSemanticConfig","sections":[],"depth":2}],"depth":1}';function kr(j){return rr(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Cr extends sr{constructor(d){super(),ir(this,d,kr,_r,ar,{})}}export{Cr as component};
