import{s as ho,o as uo,n as N}from"../chunks/scheduler.9bc65507.js";import{S as _o,i as bo,g as m,s,r as g,A as Mo,h as p,f as a,c as r,j as k,u as f,x as y,k as x,y as i,a as c,v as h,d as u,t as _,w as b}from"../chunks/index.707bf1b6.js";import{T as Tt}from"../chunks/Tip.c2ecdbf4.js";import{D as C}from"../chunks/Docstring.17db21ae.js";import{C as fe}from"../chunks/CodeBlock.54a9f38d.js";import{E as Oe}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as X}from"../chunks/Heading.342b1fa6.js";function yo(w){let n,T="Example:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsaWduQ29uZmlnJTJDJTIwQWxpZ25Nb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBBbGlnbkNvbmZpZyUyMHdpdGglMjBrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbGlnbkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEFsaWduTW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBBbGlnbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZyUwQSUwQSUyMyUyMFdlJTIwY2FuJTIwYWxzbyUyMGluaXRpYWxpemUlMjBhJTIwQWxpZ25Db25maWclMjBmcm9tJTIwYSUyMEFsaWduVGV4dENvbmZpZyUyMGFuZCUyMGElMjBBbGlnblZpc2lvbkNvbmZpZyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBbGlnblRleHRDb25maWclMkMlMjBBbGlnblZpc2lvbkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMEFMSUdOJTIwVGV4dCUyMGFuZCUyMFZpc2lvbiUyMGNvbmZpZ3VyYXRpb25zJTBBY29uZmlnX3RleHQlMjAlM0QlMjBBbGlnblRleHRDb25maWcoKSUwQWNvbmZpZ192aXNpb24lMjAlM0QlMjBBbGlnblZpc2lvbkNvbmZpZygpJTBBJTBBY29uZmlnJTIwJTNEJTIwQWxpZ25Db25maWcuZnJvbV90ZXh0X3Zpc2lvbl9jb25maWdzKGNvbmZpZ190ZXh0JTJDJTIwY29uZmlnX3Zpc2lvbik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AlignConfig, AlignModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignConfig with kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AlignConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignModel (with random weights) from the kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a AlignConfig from a AlignTextConfig and a AlignVisionConfig</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AlignTextConfig, AlignVisionConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing ALIGN Text and Vision configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = AlignTextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = AlignVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = AlignConfig.from_text_vision_configs(config_text, config_vision)`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function To(w){let n,T="Example:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsaWduVGV4dENvbmZpZyUyQyUyMEFsaWduVGV4dE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEFsaWduVGV4dENvbmZpZyUyMHdpdGglMjBrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbGlnblRleHRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBBbGlnblRleHRNb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwa2FrYW9icmFpbiUyRmFsaWduLWJhc2UlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEFsaWduVGV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AlignTextConfig, AlignTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignTextConfig with kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AlignTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignTextModel (with random weights) from the kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function vo(w){let n,T="Example:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsaWduVmlzaW9uQ29uZmlnJTJDJTIwQWxpZ25WaXNpb25Nb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBBbGlnblZpc2lvbkNvbmZpZyUyMHdpdGglMjBrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbGlnblZpc2lvbkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEFsaWduVmlzaW9uTW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBBbGlnblZpc2lvbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AlignVisionConfig, AlignVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignVisionConfig with kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AlignVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignVisionModel (with random weights) from the kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function wo(w){let n,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=T},l(l){n=p(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=T)},m(l,d){c(l,n,d)},p:N,d(l){l&&a(n)}}}function $o(w){let n,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=T},l(l){n=p(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=T)},m(l,d){c(l,n,d)},p:N,d(l){l&&a(n)}}}function ko(w){let n,T="Examples:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBbGlnbk1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBBbGlnbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXRleHRfZmVhdHVyZXMlMjAlM0QlMjBtb2RlbC5nZXRfdGV4dF9mZWF0dXJlcygqKmlucHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AlignModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignModel.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function xo(w){let n,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=T},l(l){n=p(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=T)},m(l,d){c(l,n,d)},p:N,d(l){l&&a(n)}}}function jo(w){let n,T="Examples:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEFsaWduTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEFsaWduTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQWltYWdlX2ZlYXR1cmVzJTIwJTNEJTIwbW9kZWwuZ2V0X2ltYWdlX2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AlignModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignModel.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function Ao(w){let n,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=T},l(l){n=p(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=T)},m(l,d){c(l,n,d)},p:N,d(l){l&&a(n)}}}function Co(w){let n,T="Examples:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBbGlnblRleHRNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQWxpZ25UZXh0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQXBvb2xlZF9vdXRwdXQlMjAlM0QlMjBvdXRwdXRzLnBvb2xlcl9vdXRwdXQlMjAlMjAlMjMlMjBwb29sZWQlMjAoRU9TJTIwdG9rZW4pJTIwc3RhdGVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AlignTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignTextModel.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function Jo(w){let n,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=T},l(l){n=p(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=T)},m(l,d){c(l,n,d)},p:N,d(l){l&&a(n)}}}function zo(w){let n,T="Examples:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEFsaWduVmlzaW9uTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEFsaWduVmlzaW9uTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZSUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFwb29sZWRfb3V0cHV0JTIwJTNEJTIwb3V0cHV0cy5wb29sZXJfb3V0cHV0JTIwJTIwJTIzJTIwcG9vbGVkJTIwQ0xTJTIwc3RhdGVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AlignVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignVisionModel.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function Zo(w){let n,T,l,d,M,t,v,vt,he,Gn='ALIGNãƒ¢ãƒ‡ãƒ«ã¯ã€ã€Œ<a href="https://arxiv.org/abs/2102.05918" rel="nofollow">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</a>ã€ã¨ã„ã†è«–æ–‡ã§Chao Jiaã€Yinfei Yangã€Ye Xiaã€Yi-Ting Chenã€Zarana Parekhã€Hieu Phamã€Quoc V. Leã€Yunhsuan Sungã€Zhen Liã€Tom Duerigã«ã‚ˆã£ã¦ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚ALIGNã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã“ã‚Œã¯ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼åº¦ã‚„ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒåˆ†é¡ã«ä½¿ç”¨ã§ãã¾ã™ã€‚ALIGNã¯<a href="efficientnet">EfficientNet</a>ã‚’è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨ã—ã¦ã€<a href="bert">BERT</a>ã‚’ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨ã—ã¦æ­è¼‰ã—ãŸãƒ‡ãƒ¥ã‚¢ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼æ§‹é€ ã‚’ç‰¹å¾´ã¨ã—ã€å¯¾ç…§å­¦ç¿’ã«ã‚ˆã£ã¦è¦–è¦šã¨ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨ç¾ã‚’æ•´åˆã•ã›ã‚‹ã“ã¨ã‚’å­¦ã³ã¾ã™ã€‚ãã‚Œã¾ã§ã®ç ”ç©¶ã¨ã¯ç•°ãªã‚Šã€ALIGNã¯å·¨å¤§ã§ãƒã‚¤ã‚¸ãƒ¼ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ´»ç”¨ã—ã€ã‚³ãƒ¼ãƒ‘ã‚¹ã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã—ã¦å˜ç´”ãªæ–¹æ³•ãªãŒã‚‰æœ€å…ˆç«¯ã®è¡¨ç¾ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚',wt,ue,Pn="è«–æ–‡ã®è¦æ—¨ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š",$t,_e,Nn="<em>äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¡¨ç¾ã¯ã€å¤šãã®è‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ãŠã‚ˆã³çŸ¥è¦šã‚¿ã‚¹ã‚¯ã«ã¨ã£ã¦é‡è¦ã«ãªã£ã¦ã„ã¾ã™ã€‚NLPã«ãŠã‘ã‚‹è¡¨ç¾å­¦ç¿’ã¯ã€äººé–“ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ãªã„ç”Ÿã®ãƒ†ã‚­ã‚¹ãƒˆã§ã®å­¦ç¿’ã¸ã¨ç§»è¡Œã—ã¦ã„ã¾ã™ãŒã€è¦–è¦šãŠã‚ˆã³è¦–è¦šè¨€èªã®è¡¨ç¾ã¯ä¾ç„¶ã¨ã—ã¦ç²¾å·§ãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¤§ããä¾å­˜ã—ã¦ãŠã‚Šã€ã“ã‚Œã¯é«˜ä¾¡ã§ã‚ã£ãŸã‚Šå°‚é–€çŸ¥è­˜ã‚’å¿…è¦ã¨ã—ãŸã‚Šã—ã¾ã™ã€‚è¦–è¦šã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å ´åˆã€ImageNetã‚„OpenImagesã®ã‚ˆã†ãªæ˜ç¤ºçš„ãªã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã•ã‚Œã‚‹ã“ã¨ãŒã»ã¨ã‚“ã©ã§ã™ã€‚è¦–è¦šè¨€èªã®å ´åˆã€Conceptual Captionsã€MSCOCOã€CLIPãªã©ã®äººæ°—ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã™ã¹ã¦ã€ãã‚Œãã‚Œç„¡è¦–ã§ããªã„ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆãŠã‚ˆã³ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰ãƒ—ãƒ­ã‚»ã‚¹ã‚’å«ã¿ã¾ã™ã€‚ã“ã®ã‚³ã‚¹ãƒˆã®ã‹ã‹ã‚‹ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã‚’åˆ¶é™ã—ã€è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å¦¨ã’ã¾ã™ã€‚æœ¬è«–æ–‡ã§ã¯ã€Conceptual Captionsãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é«˜ä¾¡ãªãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚„å¾Œå‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ãªã—ã§å¾—ã‚‰ã‚ŒãŸã€10å„„ã‚’è¶…ãˆã‚‹ç”»åƒalt-textãƒšã‚¢ã®ãƒã‚¤ã‚ºã®å¤šã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ´»ç”¨ã—ã¾ã™ã€‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¥ã‚¢ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€å¯¾ç…§æå¤±ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã®è¦–è¦šçš„ãŠã‚ˆã³è¨€èªçš„è¡¨ç¾ã‚’æ•´åˆã•ã›ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã¾ã™ã€‚æˆ‘ã€…ã¯ã€ã‚³ãƒ¼ãƒ‘ã‚¹ã®è¦æ¨¡ãŒãã®ãƒã‚¤ã‚ºã‚’è£œã„ã€ã“ã®ã‚ˆã†ãªå˜ç´”ãªå­¦ç¿’ã‚¹ã‚­ãƒ¼ãƒ ã§ã‚‚æœ€å…ˆç«¯ã®è¡¨ç¾ã«ã¤ãªãŒã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚æˆ‘ã€…ã®è¦–è¦šè¡¨ç¾ã¯ã€ImageNetã‚„VTABãªã©ã®åˆ†é¡ã‚¿ã‚¹ã‚¯ã¸ã®è»¢ç§»ã«ãŠã„ã¦å¼·åŠ›ãªæ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚æ•´åˆã—ãŸè¦–è¦šçš„ãŠã‚ˆã³è¨€èªçš„è¡¨ç¾ã¯ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒåˆ†é¡ã‚’å¯èƒ½ã«ã—ã€ã¾ãŸã€ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã‚‚ã€Flickr30KãŠã‚ˆã³MSCOCOç”»åƒãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ãŠã„ã¦æ–°ãŸãªæœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã¾ã™ã€‚ã¾ãŸã€ã“ã‚Œã‚‰ã®è¡¨ç¾ã¯ã€è¤‡é›‘ãªãƒ†ã‚­ã‚¹ãƒˆãŠã‚ˆã³ãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒã®ã‚¯ã‚¨ãƒªã‚’ç”¨ã„ãŸã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«æ¤œç´¢ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚</em>",kt,be,Ln=`ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯<a href="https://huggingface.co/adirik" rel="nofollow">Alara Dirik</a>ã«ã‚ˆã‚Šæä¾›ã•ã‚Œã¾ã—ãŸã€‚
ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ãŠã‚‰ãšã€ã“ã®å®Ÿè£…ã¯å…ƒè«–æ–‡ã«åŸºã¥ã„ãŸKakao Brainã®å®Ÿè£…ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ã€‚`,xt,Me,jt,ye,Fn="ALIGNã¯EfficientNetã‚’ä½¿ç”¨ã—ã¦è¦–è¦šçš„ç‰¹å¾´ã‚’ã€BERTã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´ã‚’å–å¾—ã—ã¾ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã¨è¦–è¦šã®ä¸¡æ–¹ã®ç‰¹å¾´ã¯ã€åŒä¸€ã®æ¬¡å…ƒã‚’æŒã¤æ½œåœ¨ç©ºé–“ã«å°„å½±ã•ã‚Œã¾ã™ã€‚å°„å½±ã•ã‚ŒãŸç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é–“ã®ãƒ‰ãƒƒãƒˆç©ãŒé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚",At,Te,Hn='<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignProcessor">AlignProcessor</a>ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã¨ç”»åƒã®å‰å‡¦ç†ã‚’ä¸¡æ–¹è¡Œã†ãŸã‚ã«ã€<code>EfficientNetImageProcessor</code>ã¨<a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>ã‚’å˜ä¸€ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ãƒ©ãƒƒãƒ—ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ä¾‹ã¯ã€<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignProcessor">AlignProcessor</a>ã¨<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a>ã‚’ä½¿ç”¨ã—ã¦ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’å–å¾—ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚',Ct,ve,Jt,we,zt,$e,qn="ALIGNã®ä½¿ç”¨ã‚’é–‹å§‹ã™ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ã®Hugging Faceã¨ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼ˆğŸŒã§ç¤ºã•ã‚Œã¦ã„ã‚‹ï¼‰ã®å‚è€ƒè³‡æ–™ã®ä¸€è¦§ã§ã™ã€‚",Zt,ke,En='<li><a href="https://huggingface.co/blog/vit-align" rel="nofollow">ALIGNã¨COYO-700Mãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</a>ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿ã€‚</li> <li>ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒåˆ†é¡<a href="https://huggingface.co/spaces/adirik/ALIGN-zero-shot-image-classification" rel="nofollow">ãƒ‡ãƒ¢</a>ã€‚</li> <li><code>kakaobrain/align-base</code> ãƒ¢ãƒ‡ãƒ«ã®<a href="https://huggingface.co/kakaobrain/align-base" rel="nofollow">ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰</a>ã€‚</li>',Wt,xe,Rn="ã“ã“ã«å‚è€ƒè³‡æ–™ã‚’æå‡ºã—ãŸã„å ´åˆã¯ã€æ°—å…¼ã­ãªãPull Requestã‚’é–‹ã„ã¦ãã ã•ã„ã€‚ç§ãŸã¡ã¯ãã‚Œã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã„ãŸã—ã¾ã™ï¼å‚è€ƒè³‡æ–™ã¯ã€æ—¢å­˜ã®ã‚‚ã®ã‚’è¤‡è£½ã™ã‚‹ã®ã§ã¯ãªãã€ä½•ã‹æ–°ã—ã„ã“ã¨ã‚’ç¤ºã™ã“ã¨ãŒç†æƒ³çš„ã§ã™ã€‚",Ut,je,It,j,Ae,St,Ke,Xn=`<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a> is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a>. It is used to
instantiate a ALIGN model according to the specified arguments, defining the text model and vision model configs.
Instantiating a configuration with the defaults will yield a similar configuration to that of the ALIGN
<a href="https://huggingface.co/kakaobrain/align-base" rel="nofollow">kakaobrain/align-base</a> architecture.`,Dt,et,Yn=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ot,S,Kt,D,Ce,en,tt,Qn=`Instantiate a <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a> (or a derived class) from align text model configuration and align vision model
configuration.`,Bt,Je,Vt,J,ze,tn,nt,Sn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextModel">AlignTextModel</a>. It is used to instantiate a
ALIGN text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the text encoder of the ALIGN
<a href="https://huggingface.co/kakaobrain/align-base" rel="nofollow">kakaobrain/align-base</a> architecture. The default values here are
copied from BERT.`,nn,ot,Dn=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,on,O,Gt,Ze,Pt,z,We,an,at,On=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignVisionModel">AlignVisionModel</a>. It is used to instantiate a
ALIGN vision encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the vision encoder of the ALIGN
<a href="https://huggingface.co/kakaobrain/align-base" rel="nofollow">kakaobrain/align-base</a> architecture. The default values are copied
from EfficientNet (efficientnet-b7)`,sn,st,Kn=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,rn,K,Nt,Ue,Lt,Z,Ie,ln,rt,eo=`Constructs an ALIGN processor which wraps <code>EfficientNetImageProcessor</code> and
<a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>/<a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> into a single processor that interits both the image processor and
tokenizer functionalities. See the <code>__call__()</code> and <code>decode()</code> for more
information.`,dn,ee,Be,cn,it,to=`This method forwards all its arguments to BertTokenizerFastâ€™s <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.batch_decode">batch_decode()</a>. Please
refer to the docstring of this method for more information.`,mn,te,Ve,pn,lt,no=`This method forwards all its arguments to BertTokenizerFastâ€™s <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,Ft,Ge,Ht,$,Pe,gn,dt,oo=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,fn,ct,ao=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,hn,Y,Ne,un,mt,so='The <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a> forward method, overrides the <code>__call__</code> special method.',_n,ne,bn,B,Le,Mn,pt,ro='The <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a> forward method, overrides the <code>__call__</code> special method.',yn,oe,Tn,ae,vn,V,Fe,wn,gt,io='The <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a> forward method, overrides the <code>__call__</code> special method.',$n,se,kn,re,qt,He,Et,W,qe,xn,ft,lo=`The text model from ALIGN without any head or projection on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,jn,ht,co=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,An,G,Ee,Cn,ut,mo='The <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextModel">AlignTextModel</a> forward method, overrides the <code>__call__</code> special method.',Jn,ie,zn,le,Rt,Re,Xt,U,Xe,Zn,_t,po=`The vision model from ALIGN without any head or projection on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Wn,bt,go=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Un,P,Ye,In,Mt,fo='The <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignVisionModel">AlignVisionModel</a> forward method, overrides the <code>__call__</code> special method.',Bn,de,Vn,ce,Yt,yt,Qt;return M=new X({props:{title:"ALIGN",local:"align",headingTag:"h1"}}),v=new X({props:{title:"æ¦‚è¦",local:"æ¦‚è¦",headingTag:"h2"}}),Me=new X({props:{title:"ä½¿ç”¨ä¾‹",local:"ä½¿ç”¨ä¾‹",headingTag:"h2"}}),ve=new fe({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBbGlnblByb2Nlc3NvciUyQyUyMEFsaWduTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBbGlnblByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIya2FrYW9icmFpbiUyRmFsaWduLWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBBbGlnbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBY2FuZGlkYXRlX2xhYmVscyUyMCUzRCUyMCU1QiUyMmFuJTIwaW1hZ2UlMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhbiUyMGltYWdlJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0RjYW5kaWRhdGVfbGFiZWxzJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwdGhpcyUyMGlzJTIwdGhlJTIwaW1hZ2UtdGV4dCUyMHNpbWlsYXJpdHklMjBzY29yZSUwQWxvZ2l0c19wZXJfaW1hZ2UlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0c19wZXJfaW1hZ2UlMEElMEElMjMlMjB3ZSUyMGNhbiUyMHRha2UlMjB0aGUlMjBzb2Z0bWF4JTIwdG8lMjBnZXQlMjB0aGUlMjBsYWJlbCUyMHByb2JhYmlsaXRpZXMlMEFwcm9icyUyMCUzRCUyMGxvZ2l0c19wZXJfaW1hZ2Uuc29mdG1heChkaW0lM0QxKSUwQXByaW50KHByb2JzKQ==",highlighted:`<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AlignProcessor, AlignModel

processor = AlignProcessor.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)
model = AlignModel.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)

url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
candidate_labels = [<span class="hljs-string">&quot;an image of a cat&quot;</span>, <span class="hljs-string">&quot;an image of a dog&quot;</span>]

inputs = processor(text=candidate_labels, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

<span class="hljs-comment"># this is the image-text similarity score</span>
logits_per_image = outputs.logits_per_image

<span class="hljs-comment"># we can take the softmax to get the label probabilities</span>
probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(probs)`,wrap:!1}}),we=new X({props:{title:"å‚è€ƒè³‡æ–™",local:"å‚è€ƒè³‡æ–™",headingTag:"h2"}}),je=new X({props:{title:"AlignConfig",local:"transformers.AlignConfig",headingTag:"h2"}}),Ae=new C({props:{name:"class transformers.AlignConfig",anchor:"transformers.AlignConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 640"},{name:"temperature_init_value",val:" = 1.0"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AlignConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextConfig">AlignTextConfig</a>.`,name:"text_config"},{anchor:"transformers.AlignConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignVisionConfig">AlignVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.AlignConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 640) &#x2014;
Dimentionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.AlignConfig.temperature_init_value",description:`<strong>temperature_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The inital value of the <em>temperature</em> paramter. Default is used as per the original ALIGN implementation.`,name:"temperature_init_value"},{anchor:"transformers.AlignConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.AlignConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/configuration_align.py#L298"}}),S=new Oe({props:{anchor:"transformers.AlignConfig.example",$$slots:{default:[yo]},$$scope:{ctx:w}}}),Ce=new C({props:{name:"from_text_vision_configs",anchor:"transformers.AlignConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": AlignTextConfig"},{name:"vision_config",val:": AlignVisionConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/configuration_align.py#L374",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig"
>AlignConfig</a></p>
`}}),Je=new X({props:{title:"AlignTextConfig",local:"transformers.AlignTextConfig",headingTag:"h2"}}),ze=new C({props:{name:"class transformers.AlignTextConfig",anchor:"transformers.AlignTextConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AlignTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Align Text model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextModel">AlignTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.AlignTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.AlignTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.AlignTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.AlignTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.AlignTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.AlignTextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.AlignTextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.AlignTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.AlignTextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextModel">AlignTextModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.AlignTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.AlignTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.AlignTextConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.AlignTextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.AlignTextConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/configuration_align.py#L35"}}),O=new Oe({props:{anchor:"transformers.AlignTextConfig.example",$$slots:{default:[To]},$$scope:{ctx:w}}}),Ze=new X({props:{title:"AlignVisionConfig",local:"transformers.AlignVisionConfig",headingTag:"h2"}}),We=new C({props:{name:"class transformers.AlignVisionConfig",anchor:"transformers.AlignVisionConfig",parameters:[{name:"num_channels",val:": int = 3"},{name:"image_size",val:": int = 600"},{name:"width_coefficient",val:": float = 2.0"},{name:"depth_coefficient",val:": float = 3.1"},{name:"depth_divisor",val:": int = 8"},{name:"kernel_sizes",val:": List = [3, 3, 5, 3, 5, 5, 3]"},{name:"in_channels",val:": List = [32, 16, 24, 40, 80, 112, 192]"},{name:"out_channels",val:": List = [16, 24, 40, 80, 112, 192, 320]"},{name:"depthwise_padding",val:": List = []"},{name:"strides",val:": List = [1, 2, 2, 2, 1, 2, 1]"},{name:"num_block_repeats",val:": List = [1, 2, 2, 3, 3, 4, 1]"},{name:"expand_ratios",val:": List = [1, 6, 6, 6, 6, 6, 6]"},{name:"squeeze_expansion_ratio",val:": float = 0.25"},{name:"hidden_act",val:": str = 'swish'"},{name:"hidden_dim",val:": int = 2560"},{name:"pooling_type",val:": str = 'mean'"},{name:"initializer_range",val:": float = 0.02"},{name:"batch_norm_eps",val:": float = 0.001"},{name:"batch_norm_momentum",val:": float = 0.99"},{name:"drop_connect_rate",val:": float = 0.2"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AlignVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.AlignVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 600) &#x2014;
The input image size.`,name:"image_size"},{anchor:"transformers.AlignVisionConfig.width_coefficient",description:`<strong>width_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 2.0) &#x2014;
Scaling coefficient for network width at each stage.`,name:"width_coefficient"},{anchor:"transformers.AlignVisionConfig.depth_coefficient",description:`<strong>depth_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 3.1) &#x2014;
Scaling coefficient for network depth at each stage.`,name:"depth_coefficient"},{anchor:"transformers.AlignVisionConfig.depth_divisor",description:`<strong>depth_divisor</strong> <code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
A unit of network width.`,name:"depth_divisor"},{anchor:"transformers.AlignVisionConfig.kernel_sizes",description:`<strong>kernel_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 5, 3, 5, 5, 3]</code>) &#x2014;
List of kernel sizes to be used in each block.`,name:"kernel_sizes"},{anchor:"transformers.AlignVisionConfig.in_channels",description:`<strong>in_channels</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[32, 16, 24, 40, 80, 112, 192]</code>) &#x2014;
List of input channel sizes to be used in each block for convolutional layers.`,name:"in_channels"},{anchor:"transformers.AlignVisionConfig.out_channels",description:`<strong>out_channels</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[16, 24, 40, 80, 112, 192, 320]</code>) &#x2014;
List of output channel sizes to be used in each block for convolutional layers.`,name:"out_channels"},{anchor:"transformers.AlignVisionConfig.depthwise_padding",description:`<strong>depthwise_padding</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[]</code>) &#x2014;
List of block indices with square padding.`,name:"depthwise_padding"},{anchor:"transformers.AlignVisionConfig.strides",description:`<strong>strides</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 2, 2, 1, 2, 1]</code>) &#x2014;
List of stride sizes to be used in each block for convolutional layers.`,name:"strides"},{anchor:"transformers.AlignVisionConfig.num_block_repeats",description:`<strong>num_block_repeats</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 2, 3, 3, 4, 1]</code>) &#x2014;
List of the number of times each block is to repeated.`,name:"num_block_repeats"},{anchor:"transformers.AlignVisionConfig.expand_ratios",description:`<strong>expand_ratios</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 6, 6, 6, 6, 6, 6]</code>) &#x2014;
List of scaling coefficient of each block.`,name:"expand_ratios"},{anchor:"transformers.AlignVisionConfig.squeeze_expansion_ratio",description:`<strong>squeeze_expansion_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0.25) &#x2014;
Squeeze expansion ratio.`,name:"squeeze_expansion_ratio"},{anchor:"transformers.AlignVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each block. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;, </code>&#x201C;gelu_new&#x201D;<code>, </code>&#x201C;silu&#x201D;<code>and</code>&#x201C;mish&#x201D;\` are supported.`,name:"hidden_act"},{anchor:"transformers.AlignVisionConfig.hiddem_dim",description:`<strong>hiddem_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1280) &#x2014;
The hidden dimension of the layer before the classification head.`,name:"hiddem_dim"},{anchor:"transformers.AlignVisionConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Type of final pooling to be applied before the dense classification head. Available options are [<code>&quot;mean&quot;</code>,
<code>&quot;max&quot;</code>]`,name:"pooling_type"},{anchor:"transformers.AlignVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.AlignVisionConfig.batch_norm_eps",description:`<strong>batch_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The epsilon used by the batch normalization layers.`,name:"batch_norm_eps"},{anchor:"transformers.AlignVisionConfig.batch_norm_momentum",description:`<strong>batch_norm_momentum</strong> (<code>float</code>, <em>optional</em>, defaults to 0.99) &#x2014;
The momentum used by the batch normalization layers.`,name:"batch_norm_momentum"},{anchor:"transformers.AlignVisionConfig.drop_connect_rate",description:`<strong>drop_connect_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The drop rate for skip connections.`,name:"drop_connect_rate"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/configuration_align.py#L159"}}),K=new Oe({props:{anchor:"transformers.AlignVisionConfig.example",$$slots:{default:[vo]},$$scope:{ctx:w}}}),Ue=new X({props:{title:"AlignProcessor",local:"transformers.AlignProcessor",headingTag:"h2"}}),Ie=new C({props:{name:"class transformers.AlignProcessor",anchor:"transformers.AlignProcessor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.AlignProcessor.image_processor",description:`<strong>image_processor</strong> (<code>EfficientNetImageProcessor</code>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.AlignProcessor.tokenizer",description:`<strong>tokenizer</strong> ([<code>BertTokenizer</code>, <code>BertTokenizerFast</code>]) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/processing_align.py#L24"}}),Be=new C({props:{name:"batch_decode",anchor:"transformers.AlignProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/processing_align.py#L104"}}),Ve=new C({props:{name:"decode",anchor:"transformers.AlignProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/processing_align.py#L111"}}),Ge=new X({props:{title:"AlignModel",local:"transformers.AlignModel",headingTag:"h2"}}),Pe=new C({props:{name:"class transformers.AlignModel",anchor:"transformers.AlignModel",parameters:[{name:"config",val:": AlignConfig"}],parametersDescription:[{anchor:"transformers.AlignModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1411"}}),Ne=new C({props:{name:"forward",anchor:"transformers.AlignModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"return_loss",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AlignModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>
attention_mask (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>
position_ids (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>
token_type_ids (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>):
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>
head_mask (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>):
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>
<p>inputs_embeds (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>):
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.
pixel_values (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>):
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">EfficientNetImageProcessor.<strong>call</strong>()</a> for details.
return_loss (<code>bool</code>, <em>optional</em>):
Whether or not to return the contrastive loss.
output_attentions (<code>bool</code>, <em>optional</em>):
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.
output_hidden_states (<code>bool</code>, <em>optional</em>):
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.
return_dict (<code>bool</code>, <em>optional</em>):
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"input_ids"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1543"}}),ne=new Tt({props:{$$slots:{default:[wo]},$$scope:{ctx:w}}}),Le=new C({props:{name:"get_text_features",anchor:"transformers.AlignModel.get_text_features",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AlignModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.AlignModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.AlignModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.AlignModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.AlignModel.get_text_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.AlignModel.get_text_features.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.AlignModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AlignModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AlignModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1445",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextModel"
>AlignTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),oe=new Tt({props:{$$slots:{default:[$o]},$$scope:{ctx:w}}}),ae=new Oe({props:{anchor:"transformers.AlignModel.get_text_features.example",$$slots:{default:[ko]},$$scope:{ctx:w}}}),Fe=new C({props:{name:"get_image_features",anchor:"transformers.AlignModel.get_image_features",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AlignModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">EfficientNetImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.AlignModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AlignModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1498",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/align#transformers.AlignVisionModel"
>AlignVisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),se=new Tt({props:{$$slots:{default:[xo]},$$scope:{ctx:w}}}),re=new Oe({props:{anchor:"transformers.AlignModel.get_image_features.example",$$slots:{default:[jo]},$$scope:{ctx:w}}}),He=new X({props:{title:"AlignTextModel",local:"transformers.AlignTextModel",headingTag:"h2"}}),qe=new C({props:{name:"class transformers.AlignTextModel",anchor:"transformers.AlignTextModel",parameters:[{name:"config",val:": AlignTextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.AlignTextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1196"}}),Ee=new C({props:{name:"forward",anchor:"transformers.AlignTextModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AlignTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.AlignTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.AlignTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.AlignTextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.AlignTextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.AlignTextModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.AlignTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AlignTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AlignTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1221",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.align.configuration_align.AlignTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoderâ€™s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) â€” Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ie=new Tt({props:{$$slots:{default:[Ao]},$$scope:{ctx:w}}}),le=new Oe({props:{anchor:"transformers.AlignTextModel.forward.example",$$slots:{default:[Co]},$$scope:{ctx:w}}}),Re=new X({props:{title:"AlignVisionModel",local:"transformers.AlignVisionModel",headingTag:"h2"}}),Xe=new C({props:{name:"class transformers.AlignVisionModel",anchor:"transformers.AlignVisionModel",parameters:[{name:"config",val:": AlignVisionConfig"}],parametersDescription:[{anchor:"transformers.AlignVisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1322"}}),Ye=new C({props:{name:"forward",anchor:"transformers.AlignVisionModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AlignVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">EfficientNetImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.AlignVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AlignVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1351",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.align.configuration_align.AlignVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),de=new Tt({props:{$$slots:{default:[Jo]},$$scope:{ctx:w}}}),ce=new Oe({props:{anchor:"transformers.AlignVisionModel.forward.example",$$slots:{default:[zo]},$$scope:{ctx:w}}}),{c(){n=m("meta"),T=s(),l=m("p"),d=s(),g(M.$$.fragment),t=s(),g(v.$$.fragment),vt=s(),he=m("p"),he.innerHTML=Gn,wt=s(),ue=m("p"),ue.textContent=Pn,$t=s(),_e=m("p"),_e.innerHTML=Nn,kt=s(),be=m("p"),be.innerHTML=Ln,xt=s(),g(Me.$$.fragment),jt=s(),ye=m("p"),ye.textContent=Fn,At=s(),Te=m("p"),Te.innerHTML=Hn,Ct=s(),g(ve.$$.fragment),Jt=s(),g(we.$$.fragment),zt=s(),$e=m("p"),$e.textContent=qn,Zt=s(),ke=m("ul"),ke.innerHTML=En,Wt=s(),xe=m("p"),xe.textContent=Rn,Ut=s(),g(je.$$.fragment),It=s(),j=m("div"),g(Ae.$$.fragment),St=s(),Ke=m("p"),Ke.innerHTML=Xn,Dt=s(),et=m("p"),et.innerHTML=Yn,Ot=s(),g(S.$$.fragment),Kt=s(),D=m("div"),g(Ce.$$.fragment),en=s(),tt=m("p"),tt.innerHTML=Qn,Bt=s(),g(Je.$$.fragment),Vt=s(),J=m("div"),g(ze.$$.fragment),tn=s(),nt=m("p"),nt.innerHTML=Sn,nn=s(),ot=m("p"),ot.innerHTML=Dn,on=s(),g(O.$$.fragment),Gt=s(),g(Ze.$$.fragment),Pt=s(),z=m("div"),g(We.$$.fragment),an=s(),at=m("p"),at.innerHTML=On,sn=s(),st=m("p"),st.innerHTML=Kn,rn=s(),g(K.$$.fragment),Nt=s(),g(Ue.$$.fragment),Lt=s(),Z=m("div"),g(Ie.$$.fragment),ln=s(),rt=m("p"),rt.innerHTML=eo,dn=s(),ee=m("div"),g(Be.$$.fragment),cn=s(),it=m("p"),it.innerHTML=to,mn=s(),te=m("div"),g(Ve.$$.fragment),pn=s(),lt=m("p"),lt.innerHTML=no,Ft=s(),g(Ge.$$.fragment),Ht=s(),$=m("div"),g(Pe.$$.fragment),gn=s(),dt=m("p"),dt.innerHTML=oo,fn=s(),ct=m("p"),ct.innerHTML=ao,hn=s(),Y=m("div"),g(Ne.$$.fragment),un=s(),mt=m("p"),mt.innerHTML=so,_n=s(),g(ne.$$.fragment),bn=s(),B=m("div"),g(Le.$$.fragment),Mn=s(),pt=m("p"),pt.innerHTML=ro,yn=s(),g(oe.$$.fragment),Tn=s(),g(ae.$$.fragment),vn=s(),V=m("div"),g(Fe.$$.fragment),wn=s(),gt=m("p"),gt.innerHTML=io,$n=s(),g(se.$$.fragment),kn=s(),g(re.$$.fragment),qt=s(),g(He.$$.fragment),Et=s(),W=m("div"),g(qe.$$.fragment),xn=s(),ft=m("p"),ft.innerHTML=lo,jn=s(),ht=m("p"),ht.innerHTML=co,An=s(),G=m("div"),g(Ee.$$.fragment),Cn=s(),ut=m("p"),ut.innerHTML=mo,Jn=s(),g(ie.$$.fragment),zn=s(),g(le.$$.fragment),Rt=s(),g(Re.$$.fragment),Xt=s(),U=m("div"),g(Xe.$$.fragment),Zn=s(),_t=m("p"),_t.innerHTML=po,Wn=s(),bt=m("p"),bt.innerHTML=go,Un=s(),P=m("div"),g(Ye.$$.fragment),In=s(),Mt=m("p"),Mt.innerHTML=fo,Bn=s(),g(de.$$.fragment),Vn=s(),g(ce.$$.fragment),Yt=s(),yt=m("p"),this.h()},l(e){const o=Mo("svelte-u9bgzb",document.head);n=p(o,"META",{name:!0,content:!0}),o.forEach(a),T=r(e),l=p(e,"P",{}),k(l).forEach(a),d=r(e),f(M.$$.fragment,e),t=r(e),f(v.$$.fragment,e),vt=r(e),he=p(e,"P",{"data-svelte-h":!0}),y(he)!=="svelte-13yjjdl"&&(he.innerHTML=Gn),wt=r(e),ue=p(e,"P",{"data-svelte-h":!0}),y(ue)!=="svelte-1pvwld5"&&(ue.textContent=Pn),$t=r(e),_e=p(e,"P",{"data-svelte-h":!0}),y(_e)!=="svelte-xlfmk0"&&(_e.innerHTML=Nn),kt=r(e),be=p(e,"P",{"data-svelte-h":!0}),y(be)!=="svelte-196ockx"&&(be.innerHTML=Ln),xt=r(e),f(Me.$$.fragment,e),jt=r(e),ye=p(e,"P",{"data-svelte-h":!0}),y(ye)!=="svelte-p64s9e"&&(ye.textContent=Fn),At=r(e),Te=p(e,"P",{"data-svelte-h":!0}),y(Te)!=="svelte-6nlwzo"&&(Te.innerHTML=Hn),Ct=r(e),f(ve.$$.fragment,e),Jt=r(e),f(we.$$.fragment,e),zt=r(e),$e=p(e,"P",{"data-svelte-h":!0}),y($e)!=="svelte-yvd8pp"&&($e.textContent=qn),Zt=r(e),ke=p(e,"UL",{"data-svelte-h":!0}),y(ke)!=="svelte-1rvc113"&&(ke.innerHTML=En),Wt=r(e),xe=p(e,"P",{"data-svelte-h":!0}),y(xe)!=="svelte-1ca3035"&&(xe.textContent=Rn),Ut=r(e),f(je.$$.fragment,e),It=r(e),j=p(e,"DIV",{class:!0});var I=k(j);f(Ae.$$.fragment,I),St=r(I),Ke=p(I,"P",{"data-svelte-h":!0}),y(Ke)!=="svelte-180fck9"&&(Ke.innerHTML=Xn),Dt=r(I),et=p(I,"P",{"data-svelte-h":!0}),y(et)!=="svelte-1s6wgpv"&&(et.innerHTML=Yn),Ot=r(I),f(S.$$.fragment,I),Kt=r(I),D=p(I,"DIV",{class:!0});var Qe=k(D);f(Ce.$$.fragment,Qe),en=r(Qe),tt=p(Qe,"P",{"data-svelte-h":!0}),y(tt)!=="svelte-18bz0s1"&&(tt.innerHTML=Qn),Qe.forEach(a),I.forEach(a),Bt=r(e),f(Je.$$.fragment,e),Vt=r(e),J=p(e,"DIV",{class:!0});var L=k(J);f(ze.$$.fragment,L),tn=r(L),nt=p(L,"P",{"data-svelte-h":!0}),y(nt)!=="svelte-3azg6e"&&(nt.innerHTML=Sn),nn=r(L),ot=p(L,"P",{"data-svelte-h":!0}),y(ot)!=="svelte-1s6wgpv"&&(ot.innerHTML=Dn),on=r(L),f(O.$$.fragment,L),L.forEach(a),Gt=r(e),f(Ze.$$.fragment,e),Pt=r(e),z=p(e,"DIV",{class:!0});var F=k(z);f(We.$$.fragment,F),an=r(F),at=p(F,"P",{"data-svelte-h":!0}),y(at)!=="svelte-1g4akfy"&&(at.innerHTML=On),sn=r(F),st=p(F,"P",{"data-svelte-h":!0}),y(st)!=="svelte-1s6wgpv"&&(st.innerHTML=Kn),rn=r(F),f(K.$$.fragment,F),F.forEach(a),Nt=r(e),f(Ue.$$.fragment,e),Lt=r(e),Z=p(e,"DIV",{class:!0});var H=k(Z);f(Ie.$$.fragment,H),ln=r(H),rt=p(H,"P",{"data-svelte-h":!0}),y(rt)!=="svelte-vcfia9"&&(rt.innerHTML=eo),dn=r(H),ee=p(H,"DIV",{class:!0});var Se=k(ee);f(Be.$$.fragment,Se),cn=r(Se),it=p(Se,"P",{"data-svelte-h":!0}),y(it)!=="svelte-1ew01iy"&&(it.innerHTML=to),Se.forEach(a),mn=r(H),te=p(H,"DIV",{class:!0});var De=k(te);f(Ve.$$.fragment,De),pn=r(De),lt=p(De,"P",{"data-svelte-h":!0}),y(lt)!=="svelte-2o9eik"&&(lt.innerHTML=no),De.forEach(a),H.forEach(a),Ft=r(e),f(Ge.$$.fragment,e),Ht=r(e),$=p(e,"DIV",{class:!0});var A=k($);f(Pe.$$.fragment,A),gn=r(A),dt=p(A,"P",{"data-svelte-h":!0}),y(dt)!=="svelte-eisylu"&&(dt.innerHTML=oo),fn=r(A),ct=p(A,"P",{"data-svelte-h":!0}),y(ct)!=="svelte-hswkmf"&&(ct.innerHTML=ao),hn=r(A),Y=p(A,"DIV",{class:!0});var Q=k(Y);f(Ne.$$.fragment,Q),un=r(Q),mt=p(Q,"P",{"data-svelte-h":!0}),y(mt)!=="svelte-74bsgo"&&(mt.innerHTML=so),_n=r(Q),f(ne.$$.fragment,Q),Q.forEach(a),bn=r(A),B=p(A,"DIV",{class:!0});var q=k(B);f(Le.$$.fragment,q),Mn=r(q),pt=p(q,"P",{"data-svelte-h":!0}),y(pt)!=="svelte-74bsgo"&&(pt.innerHTML=ro),yn=r(q),f(oe.$$.fragment,q),Tn=r(q),f(ae.$$.fragment,q),q.forEach(a),vn=r(A),V=p(A,"DIV",{class:!0});var E=k(V);f(Fe.$$.fragment,E),wn=r(E),gt=p(E,"P",{"data-svelte-h":!0}),y(gt)!=="svelte-74bsgo"&&(gt.innerHTML=io),$n=r(E),f(se.$$.fragment,E),kn=r(E),f(re.$$.fragment,E),E.forEach(a),A.forEach(a),qt=r(e),f(He.$$.fragment,e),Et=r(e),W=p(e,"DIV",{class:!0});var R=k(W);f(qe.$$.fragment,R),xn=r(R),ft=p(R,"P",{"data-svelte-h":!0}),y(ft)!=="svelte-17paowa"&&(ft.innerHTML=lo),jn=r(R),ht=p(R,"P",{"data-svelte-h":!0}),y(ht)!=="svelte-hswkmf"&&(ht.innerHTML=co),An=r(R),G=p(R,"DIV",{class:!0});var me=k(G);f(Ee.$$.fragment,me),Cn=r(me),ut=p(me,"P",{"data-svelte-h":!0}),y(ut)!=="svelte-17ttuoi"&&(ut.innerHTML=mo),Jn=r(me),f(ie.$$.fragment,me),zn=r(me),f(le.$$.fragment,me),me.forEach(a),R.forEach(a),Rt=r(e),f(Re.$$.fragment,e),Xt=r(e),U=p(e,"DIV",{class:!0});var pe=k(U);f(Xe.$$.fragment,pe),Zn=r(pe),_t=p(pe,"P",{"data-svelte-h":!0}),y(_t)!=="svelte-1on7bqb"&&(_t.innerHTML=po),Wn=r(pe),bt=p(pe,"P",{"data-svelte-h":!0}),y(bt)!=="svelte-hswkmf"&&(bt.innerHTML=go),Un=r(pe),P=p(pe,"DIV",{class:!0});var ge=k(P);f(Ye.$$.fragment,ge),In=r(ge),Mt=p(ge,"P",{"data-svelte-h":!0}),y(Mt)!=="svelte-dqqgg8"&&(Mt.innerHTML=fo),Bn=r(ge),f(de.$$.fragment,ge),Vn=r(ge),f(ce.$$.fragment,ge),ge.forEach(a),pe.forEach(a),Yt=r(e),yt=p(e,"P",{}),k(yt).forEach(a),this.h()},h(){x(n,"name","hf:doc:metadata"),x(n,"content",Wo),x(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){i(document.head,n),c(e,T,o),c(e,l,o),c(e,d,o),h(M,e,o),c(e,t,o),h(v,e,o),c(e,vt,o),c(e,he,o),c(e,wt,o),c(e,ue,o),c(e,$t,o),c(e,_e,o),c(e,kt,o),c(e,be,o),c(e,xt,o),h(Me,e,o),c(e,jt,o),c(e,ye,o),c(e,At,o),c(e,Te,o),c(e,Ct,o),h(ve,e,o),c(e,Jt,o),h(we,e,o),c(e,zt,o),c(e,$e,o),c(e,Zt,o),c(e,ke,o),c(e,Wt,o),c(e,xe,o),c(e,Ut,o),h(je,e,o),c(e,It,o),c(e,j,o),h(Ae,j,null),i(j,St),i(j,Ke),i(j,Dt),i(j,et),i(j,Ot),h(S,j,null),i(j,Kt),i(j,D),h(Ce,D,null),i(D,en),i(D,tt),c(e,Bt,o),h(Je,e,o),c(e,Vt,o),c(e,J,o),h(ze,J,null),i(J,tn),i(J,nt),i(J,nn),i(J,ot),i(J,on),h(O,J,null),c(e,Gt,o),h(Ze,e,o),c(e,Pt,o),c(e,z,o),h(We,z,null),i(z,an),i(z,at),i(z,sn),i(z,st),i(z,rn),h(K,z,null),c(e,Nt,o),h(Ue,e,o),c(e,Lt,o),c(e,Z,o),h(Ie,Z,null),i(Z,ln),i(Z,rt),i(Z,dn),i(Z,ee),h(Be,ee,null),i(ee,cn),i(ee,it),i(Z,mn),i(Z,te),h(Ve,te,null),i(te,pn),i(te,lt),c(e,Ft,o),h(Ge,e,o),c(e,Ht,o),c(e,$,o),h(Pe,$,null),i($,gn),i($,dt),i($,fn),i($,ct),i($,hn),i($,Y),h(Ne,Y,null),i(Y,un),i(Y,mt),i(Y,_n),h(ne,Y,null),i($,bn),i($,B),h(Le,B,null),i(B,Mn),i(B,pt),i(B,yn),h(oe,B,null),i(B,Tn),h(ae,B,null),i($,vn),i($,V),h(Fe,V,null),i(V,wn),i(V,gt),i(V,$n),h(se,V,null),i(V,kn),h(re,V,null),c(e,qt,o),h(He,e,o),c(e,Et,o),c(e,W,o),h(qe,W,null),i(W,xn),i(W,ft),i(W,jn),i(W,ht),i(W,An),i(W,G),h(Ee,G,null),i(G,Cn),i(G,ut),i(G,Jn),h(ie,G,null),i(G,zn),h(le,G,null),c(e,Rt,o),h(Re,e,o),c(e,Xt,o),c(e,U,o),h(Xe,U,null),i(U,Zn),i(U,_t),i(U,Wn),i(U,bt),i(U,Un),i(U,P),h(Ye,P,null),i(P,In),i(P,Mt),i(P,Bn),h(de,P,null),i(P,Vn),h(ce,P,null),c(e,Yt,o),c(e,yt,o),Qt=!0},p(e,[o]){const I={};o&2&&(I.$$scope={dirty:o,ctx:e}),S.$set(I);const Qe={};o&2&&(Qe.$$scope={dirty:o,ctx:e}),O.$set(Qe);const L={};o&2&&(L.$$scope={dirty:o,ctx:e}),K.$set(L);const F={};o&2&&(F.$$scope={dirty:o,ctx:e}),ne.$set(F);const H={};o&2&&(H.$$scope={dirty:o,ctx:e}),oe.$set(H);const Se={};o&2&&(Se.$$scope={dirty:o,ctx:e}),ae.$set(Se);const De={};o&2&&(De.$$scope={dirty:o,ctx:e}),se.$set(De);const A={};o&2&&(A.$$scope={dirty:o,ctx:e}),re.$set(A);const Q={};o&2&&(Q.$$scope={dirty:o,ctx:e}),ie.$set(Q);const q={};o&2&&(q.$$scope={dirty:o,ctx:e}),le.$set(q);const E={};o&2&&(E.$$scope={dirty:o,ctx:e}),de.$set(E);const R={};o&2&&(R.$$scope={dirty:o,ctx:e}),ce.$set(R)},i(e){Qt||(u(M.$$.fragment,e),u(v.$$.fragment,e),u(Me.$$.fragment,e),u(ve.$$.fragment,e),u(we.$$.fragment,e),u(je.$$.fragment,e),u(Ae.$$.fragment,e),u(S.$$.fragment,e),u(Ce.$$.fragment,e),u(Je.$$.fragment,e),u(ze.$$.fragment,e),u(O.$$.fragment,e),u(Ze.$$.fragment,e),u(We.$$.fragment,e),u(K.$$.fragment,e),u(Ue.$$.fragment,e),u(Ie.$$.fragment,e),u(Be.$$.fragment,e),u(Ve.$$.fragment,e),u(Ge.$$.fragment,e),u(Pe.$$.fragment,e),u(Ne.$$.fragment,e),u(ne.$$.fragment,e),u(Le.$$.fragment,e),u(oe.$$.fragment,e),u(ae.$$.fragment,e),u(Fe.$$.fragment,e),u(se.$$.fragment,e),u(re.$$.fragment,e),u(He.$$.fragment,e),u(qe.$$.fragment,e),u(Ee.$$.fragment,e),u(ie.$$.fragment,e),u(le.$$.fragment,e),u(Re.$$.fragment,e),u(Xe.$$.fragment,e),u(Ye.$$.fragment,e),u(de.$$.fragment,e),u(ce.$$.fragment,e),Qt=!0)},o(e){_(M.$$.fragment,e),_(v.$$.fragment,e),_(Me.$$.fragment,e),_(ve.$$.fragment,e),_(we.$$.fragment,e),_(je.$$.fragment,e),_(Ae.$$.fragment,e),_(S.$$.fragment,e),_(Ce.$$.fragment,e),_(Je.$$.fragment,e),_(ze.$$.fragment,e),_(O.$$.fragment,e),_(Ze.$$.fragment,e),_(We.$$.fragment,e),_(K.$$.fragment,e),_(Ue.$$.fragment,e),_(Ie.$$.fragment,e),_(Be.$$.fragment,e),_(Ve.$$.fragment,e),_(Ge.$$.fragment,e),_(Pe.$$.fragment,e),_(Ne.$$.fragment,e),_(ne.$$.fragment,e),_(Le.$$.fragment,e),_(oe.$$.fragment,e),_(ae.$$.fragment,e),_(Fe.$$.fragment,e),_(se.$$.fragment,e),_(re.$$.fragment,e),_(He.$$.fragment,e),_(qe.$$.fragment,e),_(Ee.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),_(Re.$$.fragment,e),_(Xe.$$.fragment,e),_(Ye.$$.fragment,e),_(de.$$.fragment,e),_(ce.$$.fragment,e),Qt=!1},d(e){e&&(a(T),a(l),a(d),a(t),a(vt),a(he),a(wt),a(ue),a($t),a(_e),a(kt),a(be),a(xt),a(jt),a(ye),a(At),a(Te),a(Ct),a(Jt),a(zt),a($e),a(Zt),a(ke),a(Wt),a(xe),a(Ut),a(It),a(j),a(Bt),a(Vt),a(J),a(Gt),a(Pt),a(z),a(Nt),a(Lt),a(Z),a(Ft),a(Ht),a($),a(qt),a(Et),a(W),a(Rt),a(Xt),a(U),a(Yt),a(yt)),a(n),b(M,e),b(v,e),b(Me,e),b(ve,e),b(we,e),b(je,e),b(Ae),b(S),b(Ce),b(Je,e),b(ze),b(O),b(Ze,e),b(We),b(K),b(Ue,e),b(Ie),b(Be),b(Ve),b(Ge,e),b(Pe),b(Ne),b(ne),b(Le),b(oe),b(ae),b(Fe),b(se),b(re),b(He,e),b(qe),b(Ee),b(ie),b(le),b(Re,e),b(Xe),b(Ye),b(de),b(ce)}}}const Wo='{"title":"ALIGN","local":"align","sections":[{"title":"æ¦‚è¦","local":"æ¦‚è¦","sections":[],"depth":2},{"title":"ä½¿ç”¨ä¾‹","local":"ä½¿ç”¨ä¾‹","sections":[],"depth":2},{"title":"å‚è€ƒè³‡æ–™","local":"å‚è€ƒè³‡æ–™","sections":[],"depth":2},{"title":"AlignConfig","local":"transformers.AlignConfig","sections":[],"depth":2},{"title":"AlignTextConfig","local":"transformers.AlignTextConfig","sections":[],"depth":2},{"title":"AlignVisionConfig","local":"transformers.AlignVisionConfig","sections":[],"depth":2},{"title":"AlignProcessor","local":"transformers.AlignProcessor","sections":[],"depth":2},{"title":"AlignModel","local":"transformers.AlignModel","sections":[],"depth":2},{"title":"AlignTextModel","local":"transformers.AlignTextModel","sections":[],"depth":2},{"title":"AlignVisionModel","local":"transformers.AlignVisionModel","sections":[],"depth":2}],"depth":1}';function Uo(w){return uo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fo extends _o{constructor(n){super(),bo(this,n,Uo,Zo,ho,{})}}export{Fo as component};
