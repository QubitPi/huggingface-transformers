import{s as ho,o as uo,n as N}from"../chunks/scheduler.9bc65507.js";import{S as _o,i as bo,g as m,s,r as g,A as Mo,h as p,f as a,c as r,j as k,u as f,x as y,k as x,y as i,a as c,v as h,d as u,t as _,w as b}from"../chunks/index.707bf1b6.js";import{T as Tt}from"../chunks/Tip.c2ecdbf4.js";import{D as C}from"../chunks/Docstring.17db21ae.js";import{C as fe}from"../chunks/CodeBlock.54a9f38d.js";import{E as Oe}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as X}from"../chunks/Heading.342b1fa6.js";function yo(w){let n,T="Example:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsaWduQ29uZmlnJTJDJTIwQWxpZ25Nb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBBbGlnbkNvbmZpZyUyMHdpdGglMjBrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbGlnbkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEFsaWduTW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBBbGlnbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZyUwQSUwQSUyMyUyMFdlJTIwY2FuJTIwYWxzbyUyMGluaXRpYWxpemUlMjBhJTIwQWxpZ25Db25maWclMjBmcm9tJTIwYSUyMEFsaWduVGV4dENvbmZpZyUyMGFuZCUyMGElMjBBbGlnblZpc2lvbkNvbmZpZyUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBbGlnblRleHRDb25maWclMkMlMjBBbGlnblZpc2lvbkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMEFMSUdOJTIwVGV4dCUyMGFuZCUyMFZpc2lvbiUyMGNvbmZpZ3VyYXRpb25zJTBBY29uZmlnX3RleHQlMjAlM0QlMjBBbGlnblRleHRDb25maWcoKSUwQWNvbmZpZ192aXNpb24lMjAlM0QlMjBBbGlnblZpc2lvbkNvbmZpZygpJTBBJTBBY29uZmlnJTIwJTNEJTIwQWxpZ25Db25maWcuZnJvbV90ZXh0X3Zpc2lvbl9jb25maWdzKGNvbmZpZ190ZXh0JTJDJTIwY29uZmlnX3Zpc2lvbik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AlignConfig, AlignModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignConfig with kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AlignConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignModel (with random weights) from the kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a AlignConfig from a AlignTextConfig and a AlignVisionConfig</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AlignTextConfig, AlignVisionConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing ALIGN Text and Vision configurations</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = AlignTextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = AlignVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = AlignConfig.from_text_vision_configs(config_text, config_vision)`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function To(w){let n,T="Example:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsaWduVGV4dENvbmZpZyUyQyUyMEFsaWduVGV4dE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEFsaWduVGV4dENvbmZpZyUyMHdpdGglMjBrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbGlnblRleHRDb25maWcoKSUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBBbGlnblRleHRNb2RlbCUyMCh3aXRoJTIwcmFuZG9tJTIwd2VpZ2h0cyklMjBmcm9tJTIwdGhlJTIwa2FrYW9icmFpbiUyRmFsaWduLWJhc2UlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEFsaWduVGV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AlignTextConfig, AlignTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignTextConfig with kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AlignTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignTextModel (with random weights) from the kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function vo(w){let n,T="Example:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsaWduVmlzaW9uQ29uZmlnJTJDJTIwQWxpZ25WaXNpb25Nb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBBbGlnblZpc2lvbkNvbmZpZyUyMHdpdGglMjBrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbGlnblZpc2lvbkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEFsaWduVmlzaW9uTW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMGtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBBbGlnblZpc2lvbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AlignVisionConfig, AlignVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignVisionConfig with kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AlignVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AlignVisionModel (with random weights) from the kakaobrain/align-base style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-11lpom8"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function wo(w){let n,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=T},l(l){n=p(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=T)},m(l,d){c(l,n,d)},p:N,d(l){l&&a(n)}}}function $o(w){let n,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=T},l(l){n=p(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=T)},m(l,d){c(l,n,d)},p:N,d(l){l&&a(n)}}}function ko(w){let n,T="Examples:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBbGlnbk1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBBbGlnbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMiklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZCglMjJrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoJTVCJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwY2F0JTIyJTJDJTIwJTIyYSUyMHBob3RvJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXRleHRfZmVhdHVyZXMlMjAlM0QlMjBtb2RlbC5nZXRfdGV4dF9mZWF0dXJlcygqKmlucHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AlignModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignModel.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function xo(w){let n,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=T},l(l){n=p(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=T)},m(l,d){c(l,n,d)},p:N,d(l){l&&a(n)}}}function jo(w){let n,T="Examples:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEFsaWduTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEFsaWduTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQWltYWdlX2ZlYXR1cmVzJTIwJTNEJTIwbW9kZWwuZ2V0X2ltYWdlX2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AlignModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignModel.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function Ao(w){let n,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=T},l(l){n=p(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=T)},m(l,d){c(l,n,d)},p:N,d(l){l&&a(n)}}}function Co(w){let n,T="Examples:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Ub2tlbml6ZXIlMkMlMjBBbGlnblRleHRNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQWxpZ25UZXh0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMHRva2VuaXplciglNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxhc3RfaGlkZGVuX3N0YXRlJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQXBvb2xlZF9vdXRwdXQlMjAlM0QlMjBvdXRwdXRzLnBvb2xlcl9vdXRwdXQlMjAlMjAlMjMlMjBwb29sZWQlMjAoRU9TJTIwdG9rZW4pJTIwc3RhdGVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AlignTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignTextModel.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = tokenizer([<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled (EOS token) states</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function Jo(w){let n,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){n=m("p"),n.innerHTML=T},l(l){n=p(l,"P",{"data-svelte-h":!0}),y(n)!=="svelte-fincs2"&&(n.innerHTML=T)},m(l,d){c(l,n,d)},p:N,d(l){l&&a(n)}}}function zo(w){let n,T="Examples:",l,d,M;return d=new fe({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEFsaWduVmlzaW9uTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEFsaWduVmlzaW9uTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmtha2FvYnJhaW4lMkZhbGlnbi1iYXNlJTIyKSUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZSUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFwb29sZWRfb3V0cHV0JTIwJTNEJTIwb3V0cHV0cy5wb29sZXJfb3V0cHV0JTIwJTIwJTIzJTIwcG9vbGVkJTIwQ0xTJTIwc3RhdGVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AlignVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AlignVisionModel.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){n=m("p"),n.textContent=T,l=s(),g(d.$$.fragment)},l(t){n=p(t,"P",{"data-svelte-h":!0}),y(n)!=="svelte-kvfsh7"&&(n.textContent=T),l=r(t),f(d.$$.fragment,t)},m(t,v){c(t,n,v),c(t,l,v),h(d,t,v),M=!0},p:N,i(t){M||(u(d.$$.fragment,t),M=!0)},o(t){_(d.$$.fragment,t),M=!1},d(t){t&&(a(n),a(l)),b(d,t)}}}function Zo(w){let n,T,l,d,M,t,v,vt,he,Gn='ALIGNモデルは、「<a href="https://arxiv.org/abs/2102.05918" rel="nofollow">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision</a>」という論文でChao Jia、Yinfei Yang、Ye Xia、Yi-Ting Chen、Zarana Parekh、Hieu Pham、Quoc V. Le、Yunhsuan Sung、Zhen Li、Tom Duerigによって提案されました。ALIGNはマルチモーダルな視覚言語モデルです。これは画像とテキストの類似度や、ゼロショット画像分類に使用できます。ALIGNは<a href="efficientnet">EfficientNet</a>を視覚エンコーダーとして、<a href="bert">BERT</a>をテキストエンコーダーとして搭載したデュアルエンコーダー構造を特徴とし、対照学習によって視覚とテキストの表現を整合させることを学びます。それまでの研究とは異なり、ALIGNは巨大でノイジーなデータセットを活用し、コーパスのスケールを利用して単純な方法ながら最先端の表現を達成できることを示しています。',wt,ue,Pn="論文の要旨は以下の通りです：",$t,_e,Nn="<em>事前学習された表現は、多くの自然言語処理（NLP）および知覚タスクにとって重要になっています。NLPにおける表現学習は、人間のアノテーションのない生のテキストでの学習へと移行していますが、視覚および視覚言語の表現は依然として精巧な学習データセットに大きく依存しており、これは高価であったり専門知識を必要としたりします。視覚アプリケーションの場合、ImageNetやOpenImagesのような明示的なクラスラベルを持つデータセットを使用して学習されることがほとんどです。視覚言語の場合、Conceptual Captions、MSCOCO、CLIPなどの人気のあるデータセットはすべて、それぞれ無視できないデータ収集（およびクリーニング）プロセスを含みます。このコストのかかるキュレーションプロセスはデータセットのサイズを制限し、訓練されたモデルのスケーリングを妨げます。本論文では、Conceptual Captionsデータセットの高価なフィルタリングや後処理ステップなしで得られた、10億を超える画像alt-textペアのノイズの多いデータセットを活用します。シンプルなデュアルエンコーダーアーキテクチャは、対照損失を使用して画像とテキストペアの視覚的および言語的表現を整合させることを学習します。我々は、コーパスの規模がそのノイズを補い、このような単純な学習スキームでも最先端の表現につながることを示します。我々の視覚表現は、ImageNetやVTABなどの分類タスクへの転移において強力な性能を発揮します。整合した視覚的および言語的表現は、ゼロショット画像分類を可能にし、また、より洗練されたクロスアテンションモデルと比較しても、Flickr30KおよびMSCOCO画像テキスト検索ベンチマークにおいて新たな最先端の結果を達成します。また、これらの表現は、複雑なテキストおよびテキスト+画像のクエリを用いたクロスモーダル検索を可能にします。</em>",kt,be,Ln=`このモデルは<a href="https://huggingface.co/adirik" rel="nofollow">Alara Dirik</a>により提供されました。
オリジナルのコードは公開されておらず、この実装は元論文に基づいたKakao Brainの実装をベースにしています。`,xt,Me,jt,ye,Fn="ALIGNはEfficientNetを使用して視覚的特徴を、BERTを使用してテキスト特徴を取得します。テキストと視覚の両方の特徴は、同一の次元を持つ潜在空間に射影されます。射影された画像とテキスト特徴間のドット積が類似度スコアとして使用されます。",At,Te,Hn='<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignProcessor">AlignProcessor</a>は、テキストのエンコードと画像の前処理を両方行うために、<code>EfficientNetImageProcessor</code>と<a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>を単一のインスタンスにラップします。以下の例は、<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignProcessor">AlignProcessor</a>と<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a>を使用して画像-テキスト類似度スコアを取得する方法を示しています。',Ct,ve,Jt,we,zt,$e,qn="ALIGNの使用を開始するのに役立つ公式のHugging Faceとコミュニティ（🌎で示されている）の参考資料の一覧です。",Zt,ke,En='<li><a href="https://huggingface.co/blog/vit-align" rel="nofollow">ALIGNとCOYO-700Mデータセット</a>に関するブログ投稿。</li> <li>ゼロショット画像分類<a href="https://huggingface.co/spaces/adirik/ALIGN-zero-shot-image-classification" rel="nofollow">デモ</a>。</li> <li><code>kakaobrain/align-base</code> モデルの<a href="https://huggingface.co/kakaobrain/align-base" rel="nofollow">モデルカード</a>。</li>',Wt,xe,Rn="ここに参考資料を提出したい場合は、気兼ねなくPull Requestを開いてください。私たちはそれをレビューいたします！参考資料は、既存のものを複製するのではなく、何か新しいことを示すことが理想的です。",Ut,je,It,j,Ae,St,Ke,Xn=`<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a> is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a>. It is used to
instantiate a ALIGN model according to the specified arguments, defining the text model and vision model configs.
Instantiating a configuration with the defaults will yield a similar configuration to that of the ALIGN
<a href="https://huggingface.co/kakaobrain/align-base" rel="nofollow">kakaobrain/align-base</a> architecture.`,Dt,et,Yn=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ot,S,Kt,D,Ce,en,tt,Qn=`Instantiate a <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a> (or a derived class) from align text model configuration and align vision model
configuration.`,Bt,Je,Vt,J,ze,tn,nt,Sn=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextModel">AlignTextModel</a>. It is used to instantiate a
ALIGN text encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the text encoder of the ALIGN
<a href="https://huggingface.co/kakaobrain/align-base" rel="nofollow">kakaobrain/align-base</a> architecture. The default values here are
copied from BERT.`,nn,ot,Dn=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,on,O,Gt,Ze,Pt,z,We,an,at,On=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignVisionModel">AlignVisionModel</a>. It is used to instantiate a
ALIGN vision encoder according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the vision encoder of the ALIGN
<a href="https://huggingface.co/kakaobrain/align-base" rel="nofollow">kakaobrain/align-base</a> architecture. The default values are copied
from EfficientNet (efficientnet-b7)`,sn,st,Kn=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,rn,K,Nt,Ue,Lt,Z,Ie,ln,rt,eo=`Constructs an ALIGN processor which wraps <code>EfficientNetImageProcessor</code> and
<a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizer">BertTokenizer</a>/<a href="/docs/transformers/main/ja/model_doc/bert#transformers.BertTokenizerFast">BertTokenizerFast</a> into a single processor that interits both the image processor and
tokenizer functionalities. See the <code>__call__()</code> and <code>decode()</code> for more
information.`,dn,ee,Be,cn,it,to=`This method forwards all its arguments to BertTokenizerFast’s <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.batch_decode">batch_decode()</a>. Please
refer to the docstring of this method for more information.`,mn,te,Ve,pn,lt,no=`This method forwards all its arguments to BertTokenizerFast’s <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.decode">decode()</a>. Please refer to
the docstring of this method for more information.`,Ft,Ge,Ht,$,Pe,gn,dt,oo=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,fn,ct,ao=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,hn,Y,Ne,un,mt,so='The <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a> forward method, overrides the <code>__call__</code> special method.',_n,ne,bn,B,Le,Mn,pt,ro='The <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a> forward method, overrides the <code>__call__</code> special method.',yn,oe,Tn,ae,vn,V,Fe,wn,gt,io='The <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignModel">AlignModel</a> forward method, overrides the <code>__call__</code> special method.',$n,se,kn,re,qt,He,Et,W,qe,xn,ft,lo=`The text model from ALIGN without any head or projection on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,jn,ht,co=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,An,G,Ee,Cn,ut,mo='The <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextModel">AlignTextModel</a> forward method, overrides the <code>__call__</code> special method.',Jn,ie,zn,le,Rt,Re,Xt,U,Xe,Zn,_t,po=`The vision model from ALIGN without any head or projection on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Wn,bt,go=`This model is also a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass.
Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage
and behavior.`,Un,P,Ye,In,Mt,fo='The <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignVisionModel">AlignVisionModel</a> forward method, overrides the <code>__call__</code> special method.',Bn,de,Vn,ce,Yt,yt,Qt;return M=new X({props:{title:"ALIGN",local:"align",headingTag:"h1"}}),v=new X({props:{title:"概要",local:"概要",headingTag:"h2"}}),Me=new X({props:{title:"使用例",local:"使用例",headingTag:"h2"}}),ve=new fe({props:{code:"aW1wb3J0JTIwcmVxdWVzdHMlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWZyb20lMjB0cmFuc2Zvcm1lcnMlMjBpbXBvcnQlMjBBbGlnblByb2Nlc3NvciUyQyUyMEFsaWduTW9kZWwlMEElMEFwcm9jZXNzb3IlMjAlM0QlMjBBbGlnblByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIya2FrYW9icmFpbiUyRmFsaWduLWJhc2UlMjIpJTBBbW9kZWwlMjAlM0QlMjBBbGlnbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJrYWthb2JyYWluJTJGYWxpZ24tYmFzZSUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBY2FuZGlkYXRlX2xhYmVscyUyMCUzRCUyMCU1QiUyMmFuJTIwaW1hZ2UlMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhbiUyMGltYWdlJTIwb2YlMjBhJTIwZG9nJTIyJTVEJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0RjYW5kaWRhdGVfbGFiZWxzJTJDJTIwaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBJTIzJTIwdGhpcyUyMGlzJTIwdGhlJTIwaW1hZ2UtdGV4dCUyMHNpbWlsYXJpdHklMjBzY29yZSUwQWxvZ2l0c19wZXJfaW1hZ2UlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0c19wZXJfaW1hZ2UlMEElMEElMjMlMjB3ZSUyMGNhbiUyMHRha2UlMjB0aGUlMjBzb2Z0bWF4JTIwdG8lMjBnZXQlMjB0aGUlMjBsYWJlbCUyMHByb2JhYmlsaXRpZXMlMEFwcm9icyUyMCUzRCUyMGxvZ2l0c19wZXJfaW1hZ2Uuc29mdG1heChkaW0lM0QxKSUwQXByaW50KHByb2JzKQ==",highlighted:`<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AlignProcessor, AlignModel

processor = AlignProcessor.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)
model = AlignModel.from_pretrained(<span class="hljs-string">&quot;kakaobrain/align-base&quot;</span>)

url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
candidate_labels = [<span class="hljs-string">&quot;an image of a cat&quot;</span>, <span class="hljs-string">&quot;an image of a dog&quot;</span>]

inputs = processor(text=candidate_labels, images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-keyword">with</span> torch.no_grad():
    outputs = model(**inputs)

<span class="hljs-comment"># this is the image-text similarity score</span>
logits_per_image = outputs.logits_per_image

<span class="hljs-comment"># we can take the softmax to get the label probabilities</span>
probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(probs)`,wrap:!1}}),we=new X({props:{title:"参考資料",local:"参考資料",headingTag:"h2"}}),je=new X({props:{title:"AlignConfig",local:"transformers.AlignConfig",headingTag:"h2"}}),Ae=new C({props:{name:"class transformers.AlignConfig",anchor:"transformers.AlignConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 640"},{name:"temperature_init_value",val:" = 1.0"},{name:"initializer_range",val:" = 0.02"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AlignConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextConfig">AlignTextConfig</a>.`,name:"text_config"},{anchor:"transformers.AlignConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignVisionConfig">AlignVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.AlignConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 640) &#x2014;
Dimentionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.AlignConfig.temperature_init_value",description:`<strong>temperature_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
The inital value of the <em>temperature</em> paramter. Default is used as per the original ALIGN implementation.`,name:"temperature_init_value"},{anchor:"transformers.AlignConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.AlignConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/configuration_align.py#L298"}}),S=new Oe({props:{anchor:"transformers.AlignConfig.example",$$slots:{default:[yo]},$$scope:{ctx:w}}}),Ce=new C({props:{name:"from_text_vision_configs",anchor:"transformers.AlignConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": AlignTextConfig"},{name:"vision_config",val:": AlignVisionConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/configuration_align.py#L374",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig"
>AlignConfig</a></p>
`}}),Je=new X({props:{title:"AlignTextConfig",local:"transformers.AlignTextConfig",headingTag:"h2"}}),ze=new C({props:{name:"class transformers.AlignTextConfig",anchor:"transformers.AlignTextConfig",parameters:[{name:"vocab_size",val:" = 30522"},{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 512"},{name:"type_vocab_size",val:" = 2"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"pad_token_id",val:" = 0"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"use_cache",val:" = True"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AlignTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 30522) &#x2014;
Vocabulary size of the Align Text model. Defines the number of different tokens that can be represented by
the <code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextModel">AlignTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.AlignTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.AlignTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.AlignTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.AlignTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.AlignTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.AlignTextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.AlignTextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.AlignTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.AlignTextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 2) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextModel">AlignTextModel</a>.`,name:"type_vocab_size"},{anchor:"transformers.AlignTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.AlignTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.AlignTextConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014;
Padding token id.`,name:"pad_token_id"},{anchor:"transformers.AlignTextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.AlignTextConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/configuration_align.py#L35"}}),O=new Oe({props:{anchor:"transformers.AlignTextConfig.example",$$slots:{default:[To]},$$scope:{ctx:w}}}),Ze=new X({props:{title:"AlignVisionConfig",local:"transformers.AlignVisionConfig",headingTag:"h2"}}),We=new C({props:{name:"class transformers.AlignVisionConfig",anchor:"transformers.AlignVisionConfig",parameters:[{name:"num_channels",val:": int = 3"},{name:"image_size",val:": int = 600"},{name:"width_coefficient",val:": float = 2.0"},{name:"depth_coefficient",val:": float = 3.1"},{name:"depth_divisor",val:": int = 8"},{name:"kernel_sizes",val:": List = [3, 3, 5, 3, 5, 5, 3]"},{name:"in_channels",val:": List = [32, 16, 24, 40, 80, 112, 192]"},{name:"out_channels",val:": List = [16, 24, 40, 80, 112, 192, 320]"},{name:"depthwise_padding",val:": List = []"},{name:"strides",val:": List = [1, 2, 2, 2, 1, 2, 1]"},{name:"num_block_repeats",val:": List = [1, 2, 2, 3, 3, 4, 1]"},{name:"expand_ratios",val:": List = [1, 6, 6, 6, 6, 6, 6]"},{name:"squeeze_expansion_ratio",val:": float = 0.25"},{name:"hidden_act",val:": str = 'swish'"},{name:"hidden_dim",val:": int = 2560"},{name:"pooling_type",val:": str = 'mean'"},{name:"initializer_range",val:": float = 0.02"},{name:"batch_norm_eps",val:": float = 0.001"},{name:"batch_norm_momentum",val:": float = 0.99"},{name:"drop_connect_rate",val:": float = 0.2"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AlignVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.AlignVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 600) &#x2014;
The input image size.`,name:"image_size"},{anchor:"transformers.AlignVisionConfig.width_coefficient",description:`<strong>width_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 2.0) &#x2014;
Scaling coefficient for network width at each stage.`,name:"width_coefficient"},{anchor:"transformers.AlignVisionConfig.depth_coefficient",description:`<strong>depth_coefficient</strong> (<code>float</code>, <em>optional</em>, defaults to 3.1) &#x2014;
Scaling coefficient for network depth at each stage.`,name:"depth_coefficient"},{anchor:"transformers.AlignVisionConfig.depth_divisor",description:`<strong>depth_divisor</strong> <code>int</code>, <em>optional</em>, defaults to 8) &#x2014;
A unit of network width.`,name:"depth_divisor"},{anchor:"transformers.AlignVisionConfig.kernel_sizes",description:`<strong>kernel_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 5, 3, 5, 5, 3]</code>) &#x2014;
List of kernel sizes to be used in each block.`,name:"kernel_sizes"},{anchor:"transformers.AlignVisionConfig.in_channels",description:`<strong>in_channels</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[32, 16, 24, 40, 80, 112, 192]</code>) &#x2014;
List of input channel sizes to be used in each block for convolutional layers.`,name:"in_channels"},{anchor:"transformers.AlignVisionConfig.out_channels",description:`<strong>out_channels</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[16, 24, 40, 80, 112, 192, 320]</code>) &#x2014;
List of output channel sizes to be used in each block for convolutional layers.`,name:"out_channels"},{anchor:"transformers.AlignVisionConfig.depthwise_padding",description:`<strong>depthwise_padding</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[]</code>) &#x2014;
List of block indices with square padding.`,name:"depthwise_padding"},{anchor:"transformers.AlignVisionConfig.strides",description:`<strong>strides</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 2, 2, 1, 2, 1]</code>) &#x2014;
List of stride sizes to be used in each block for convolutional layers.`,name:"strides"},{anchor:"transformers.AlignVisionConfig.num_block_repeats",description:`<strong>num_block_repeats</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 2, 3, 3, 4, 1]</code>) &#x2014;
List of the number of times each block is to repeated.`,name:"num_block_repeats"},{anchor:"transformers.AlignVisionConfig.expand_ratios",description:`<strong>expand_ratios</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 6, 6, 6, 6, 6, 6]</code>) &#x2014;
List of scaling coefficient of each block.`,name:"expand_ratios"},{anchor:"transformers.AlignVisionConfig.squeeze_expansion_ratio",description:`<strong>squeeze_expansion_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 0.25) &#x2014;
Squeeze expansion ratio.`,name:"squeeze_expansion_ratio"},{anchor:"transformers.AlignVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;silu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each block. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;, </code>&#x201C;gelu_new&#x201D;<code>, </code>&#x201C;silu&#x201D;<code>and</code>&#x201C;mish&#x201D;\` are supported.`,name:"hidden_act"},{anchor:"transformers.AlignVisionConfig.hiddem_dim",description:`<strong>hiddem_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 1280) &#x2014;
The hidden dimension of the layer before the classification head.`,name:"hiddem_dim"},{anchor:"transformers.AlignVisionConfig.pooling_type",description:`<strong>pooling_type</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;mean&quot;</code>) &#x2014;
Type of final pooling to be applied before the dense classification head. Available options are [<code>&quot;mean&quot;</code>,
<code>&quot;max&quot;</code>]`,name:"pooling_type"},{anchor:"transformers.AlignVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.AlignVisionConfig.batch_norm_eps",description:`<strong>batch_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-3) &#x2014;
The epsilon used by the batch normalization layers.`,name:"batch_norm_eps"},{anchor:"transformers.AlignVisionConfig.batch_norm_momentum",description:`<strong>batch_norm_momentum</strong> (<code>float</code>, <em>optional</em>, defaults to 0.99) &#x2014;
The momentum used by the batch normalization layers.`,name:"batch_norm_momentum"},{anchor:"transformers.AlignVisionConfig.drop_connect_rate",description:`<strong>drop_connect_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.2) &#x2014;
The drop rate for skip connections.`,name:"drop_connect_rate"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/configuration_align.py#L159"}}),K=new Oe({props:{anchor:"transformers.AlignVisionConfig.example",$$slots:{default:[vo]},$$scope:{ctx:w}}}),Ue=new X({props:{title:"AlignProcessor",local:"transformers.AlignProcessor",headingTag:"h2"}}),Ie=new C({props:{name:"class transformers.AlignProcessor",anchor:"transformers.AlignProcessor",parameters:[{name:"image_processor",val:""},{name:"tokenizer",val:""}],parametersDescription:[{anchor:"transformers.AlignProcessor.image_processor",description:`<strong>image_processor</strong> (<code>EfficientNetImageProcessor</code>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.AlignProcessor.tokenizer",description:`<strong>tokenizer</strong> ([<code>BertTokenizer</code>, <code>BertTokenizerFast</code>]) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/processing_align.py#L24"}}),Be=new C({props:{name:"batch_decode",anchor:"transformers.AlignProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/processing_align.py#L104"}}),Ve=new C({props:{name:"decode",anchor:"transformers.AlignProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/processing_align.py#L111"}}),Ge=new X({props:{title:"AlignModel",local:"transformers.AlignModel",headingTag:"h2"}}),Pe=new C({props:{name:"class transformers.AlignModel",anchor:"transformers.AlignModel",parameters:[{name:"config",val:": AlignConfig"}],parametersDescription:[{anchor:"transformers.AlignModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1411"}}),Ne=new C({props:{name:"forward",anchor:"transformers.AlignModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"return_loss",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AlignModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>
attention_mask (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>
position_ids (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>):
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>
token_type_ids (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>):
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>
head_mask (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>):
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>
<p>inputs_embeds (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>):
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.
pixel_values (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>):
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">EfficientNetImageProcessor.<strong>call</strong>()</a> for details.
return_loss (<code>bool</code>, <em>optional</em>):
Whether or not to return the contrastive loss.
output_attentions (<code>bool</code>, <em>optional</em>):
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.
output_hidden_states (<code>bool</code>, <em>optional</em>):
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.
return_dict (<code>bool</code>, <em>optional</em>):
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"input_ids"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1543"}}),ne=new Tt({props:{$$slots:{default:[wo]},$$scope:{ctx:w}}}),Le=new C({props:{name:"get_text_features",anchor:"transformers.AlignModel.get_text_features",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AlignModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.AlignModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.AlignModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.AlignModel.get_text_features.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.AlignModel.get_text_features.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.AlignModel.get_text_features.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.AlignModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AlignModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AlignModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1445",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/align#transformers.AlignTextModel"
>AlignTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),oe=new Tt({props:{$$slots:{default:[$o]},$$scope:{ctx:w}}}),ae=new Oe({props:{anchor:"transformers.AlignModel.get_text_features.example",$$slots:{default:[ko]},$$scope:{ctx:w}}}),Fe=new C({props:{name:"get_image_features",anchor:"transformers.AlignModel.get_image_features",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AlignModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">EfficientNetImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.AlignModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AlignModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1498",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/align#transformers.AlignVisionModel"
>AlignVisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),se=new Tt({props:{$$slots:{default:[xo]},$$scope:{ctx:w}}}),re=new Oe({props:{anchor:"transformers.AlignModel.get_image_features.example",$$slots:{default:[jo]},$$scope:{ctx:w}}}),He=new X({props:{title:"AlignTextModel",local:"transformers.AlignTextModel",headingTag:"h2"}}),qe=new C({props:{name:"class transformers.AlignTextModel",anchor:"transformers.AlignTextModel",parameters:[{name:"config",val:": AlignTextConfig"},{name:"add_pooling_layer",val:": bool = True"}],parametersDescription:[{anchor:"transformers.AlignTextModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1196"}}),Ee=new C({props:{name:"forward",anchor:"transformers.AlignTextModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AlignTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.AlignTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.AlignTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.AlignTextModel.forward.token_type_ids",description:`<strong>token_type_ids</strong> (<code>torch.LongTensor</code> of shape <code>({0})</code>, <em>optional</em>) &#x2014;
Segment token indices to indicate first and second portions of the inputs. Indices are selected in <code>[0, 1]</code>:</p>
<ul>
<li>0 corresponds to a <em>sentence A</em> token,</li>
<li>1 corresponds to a <em>sentence B</em> token.</li>
</ul>
<p><a href="../glossary#token-type-ids">What are token type IDs?</a>`,name:"token_type_ids"},{anchor:"transformers.AlignTextModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.AlignTextModel.forward.inputs_embeds",description:`<strong>inputs_embeds</strong> (<code>torch.FloatTensor</code> of shape <code>({0}, hidden_size)</code>, <em>optional</em>) &#x2014;
Optionally, instead of passing <code>input_ids</code> you can choose to directly pass an embedded representation. This
is useful if you want more control over how to convert <code>input_ids</code> indices into associated vectors than the
model&#x2019;s internal embedding lookup matrix.`,name:"inputs_embeds"},{anchor:"transformers.AlignTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AlignTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AlignTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1221",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.align.configuration_align.AlignTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>cross_attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> and <code>config.add_cross_attention=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights of the decoder’s cross-attention layer, after the attention softmax, used to compute the
weighted average in the cross-attention heads.</p>
</li>
<li>
<p><strong>past_key_values</strong> (<code>tuple(tuple(torch.FloatTensor))</code>, <em>optional</em>, returned when <code>use_cache=True</code> is passed or when <code>config.use_cache=True</code>) — Tuple of <code>tuple(torch.FloatTensor)</code> of length <code>config.n_layers</code>, with each tuple having 2 tensors of shape
<code>(batch_size, num_heads, sequence_length, embed_size_per_head)</code>) and optionally if
<code>config.is_encoder_decoder=True</code> 2 additional tensors of shape <code>(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)</code>.</p>
<p>Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
<code>config.is_encoder_decoder=True</code> in the cross-attention blocks) that can be used (see <code>past_key_values</code>
input) to speed up sequential decoding.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions"
>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ie=new Tt({props:{$$slots:{default:[Ao]},$$scope:{ctx:w}}}),le=new Oe({props:{anchor:"transformers.AlignTextModel.forward.example",$$slots:{default:[Co]},$$scope:{ctx:w}}}),Re=new X({props:{title:"AlignVisionModel",local:"transformers.AlignVisionModel",headingTag:"h2"}}),Xe=new C({props:{name:"class transformers.AlignVisionModel",anchor:"transformers.AlignVisionModel",parameters:[{name:"config",val:": AlignVisionConfig"}],parametersDescription:[{anchor:"transformers.AlignVisionModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/align#transformers.AlignConfig">AlignConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1322"}}),Ye=new C({props:{name:"forward",anchor:"transformers.AlignVisionModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AlignVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">EfficientNetImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.AlignVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AlignVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/align/modeling_align.py#L1351",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.align.configuration_align.AlignVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),de=new Tt({props:{$$slots:{default:[Jo]},$$scope:{ctx:w}}}),ce=new Oe({props:{anchor:"transformers.AlignVisionModel.forward.example",$$slots:{default:[zo]},$$scope:{ctx:w}}}),{c(){n=m("meta"),T=s(),l=m("p"),d=s(),g(M.$$.fragment),t=s(),g(v.$$.fragment),vt=s(),he=m("p"),he.innerHTML=Gn,wt=s(),ue=m("p"),ue.textContent=Pn,$t=s(),_e=m("p"),_e.innerHTML=Nn,kt=s(),be=m("p"),be.innerHTML=Ln,xt=s(),g(Me.$$.fragment),jt=s(),ye=m("p"),ye.textContent=Fn,At=s(),Te=m("p"),Te.innerHTML=Hn,Ct=s(),g(ve.$$.fragment),Jt=s(),g(we.$$.fragment),zt=s(),$e=m("p"),$e.textContent=qn,Zt=s(),ke=m("ul"),ke.innerHTML=En,Wt=s(),xe=m("p"),xe.textContent=Rn,Ut=s(),g(je.$$.fragment),It=s(),j=m("div"),g(Ae.$$.fragment),St=s(),Ke=m("p"),Ke.innerHTML=Xn,Dt=s(),et=m("p"),et.innerHTML=Yn,Ot=s(),g(S.$$.fragment),Kt=s(),D=m("div"),g(Ce.$$.fragment),en=s(),tt=m("p"),tt.innerHTML=Qn,Bt=s(),g(Je.$$.fragment),Vt=s(),J=m("div"),g(ze.$$.fragment),tn=s(),nt=m("p"),nt.innerHTML=Sn,nn=s(),ot=m("p"),ot.innerHTML=Dn,on=s(),g(O.$$.fragment),Gt=s(),g(Ze.$$.fragment),Pt=s(),z=m("div"),g(We.$$.fragment),an=s(),at=m("p"),at.innerHTML=On,sn=s(),st=m("p"),st.innerHTML=Kn,rn=s(),g(K.$$.fragment),Nt=s(),g(Ue.$$.fragment),Lt=s(),Z=m("div"),g(Ie.$$.fragment),ln=s(),rt=m("p"),rt.innerHTML=eo,dn=s(),ee=m("div"),g(Be.$$.fragment),cn=s(),it=m("p"),it.innerHTML=to,mn=s(),te=m("div"),g(Ve.$$.fragment),pn=s(),lt=m("p"),lt.innerHTML=no,Ft=s(),g(Ge.$$.fragment),Ht=s(),$=m("div"),g(Pe.$$.fragment),gn=s(),dt=m("p"),dt.innerHTML=oo,fn=s(),ct=m("p"),ct.innerHTML=ao,hn=s(),Y=m("div"),g(Ne.$$.fragment),un=s(),mt=m("p"),mt.innerHTML=so,_n=s(),g(ne.$$.fragment),bn=s(),B=m("div"),g(Le.$$.fragment),Mn=s(),pt=m("p"),pt.innerHTML=ro,yn=s(),g(oe.$$.fragment),Tn=s(),g(ae.$$.fragment),vn=s(),V=m("div"),g(Fe.$$.fragment),wn=s(),gt=m("p"),gt.innerHTML=io,$n=s(),g(se.$$.fragment),kn=s(),g(re.$$.fragment),qt=s(),g(He.$$.fragment),Et=s(),W=m("div"),g(qe.$$.fragment),xn=s(),ft=m("p"),ft.innerHTML=lo,jn=s(),ht=m("p"),ht.innerHTML=co,An=s(),G=m("div"),g(Ee.$$.fragment),Cn=s(),ut=m("p"),ut.innerHTML=mo,Jn=s(),g(ie.$$.fragment),zn=s(),g(le.$$.fragment),Rt=s(),g(Re.$$.fragment),Xt=s(),U=m("div"),g(Xe.$$.fragment),Zn=s(),_t=m("p"),_t.innerHTML=po,Wn=s(),bt=m("p"),bt.innerHTML=go,Un=s(),P=m("div"),g(Ye.$$.fragment),In=s(),Mt=m("p"),Mt.innerHTML=fo,Bn=s(),g(de.$$.fragment),Vn=s(),g(ce.$$.fragment),Yt=s(),yt=m("p"),this.h()},l(e){const o=Mo("svelte-u9bgzb",document.head);n=p(o,"META",{name:!0,content:!0}),o.forEach(a),T=r(e),l=p(e,"P",{}),k(l).forEach(a),d=r(e),f(M.$$.fragment,e),t=r(e),f(v.$$.fragment,e),vt=r(e),he=p(e,"P",{"data-svelte-h":!0}),y(he)!=="svelte-13yjjdl"&&(he.innerHTML=Gn),wt=r(e),ue=p(e,"P",{"data-svelte-h":!0}),y(ue)!=="svelte-1pvwld5"&&(ue.textContent=Pn),$t=r(e),_e=p(e,"P",{"data-svelte-h":!0}),y(_e)!=="svelte-xlfmk0"&&(_e.innerHTML=Nn),kt=r(e),be=p(e,"P",{"data-svelte-h":!0}),y(be)!=="svelte-196ockx"&&(be.innerHTML=Ln),xt=r(e),f(Me.$$.fragment,e),jt=r(e),ye=p(e,"P",{"data-svelte-h":!0}),y(ye)!=="svelte-p64s9e"&&(ye.textContent=Fn),At=r(e),Te=p(e,"P",{"data-svelte-h":!0}),y(Te)!=="svelte-6nlwzo"&&(Te.innerHTML=Hn),Ct=r(e),f(ve.$$.fragment,e),Jt=r(e),f(we.$$.fragment,e),zt=r(e),$e=p(e,"P",{"data-svelte-h":!0}),y($e)!=="svelte-yvd8pp"&&($e.textContent=qn),Zt=r(e),ke=p(e,"UL",{"data-svelte-h":!0}),y(ke)!=="svelte-1rvc113"&&(ke.innerHTML=En),Wt=r(e),xe=p(e,"P",{"data-svelte-h":!0}),y(xe)!=="svelte-1ca3035"&&(xe.textContent=Rn),Ut=r(e),f(je.$$.fragment,e),It=r(e),j=p(e,"DIV",{class:!0});var I=k(j);f(Ae.$$.fragment,I),St=r(I),Ke=p(I,"P",{"data-svelte-h":!0}),y(Ke)!=="svelte-180fck9"&&(Ke.innerHTML=Xn),Dt=r(I),et=p(I,"P",{"data-svelte-h":!0}),y(et)!=="svelte-1s6wgpv"&&(et.innerHTML=Yn),Ot=r(I),f(S.$$.fragment,I),Kt=r(I),D=p(I,"DIV",{class:!0});var Qe=k(D);f(Ce.$$.fragment,Qe),en=r(Qe),tt=p(Qe,"P",{"data-svelte-h":!0}),y(tt)!=="svelte-18bz0s1"&&(tt.innerHTML=Qn),Qe.forEach(a),I.forEach(a),Bt=r(e),f(Je.$$.fragment,e),Vt=r(e),J=p(e,"DIV",{class:!0});var L=k(J);f(ze.$$.fragment,L),tn=r(L),nt=p(L,"P",{"data-svelte-h":!0}),y(nt)!=="svelte-3azg6e"&&(nt.innerHTML=Sn),nn=r(L),ot=p(L,"P",{"data-svelte-h":!0}),y(ot)!=="svelte-1s6wgpv"&&(ot.innerHTML=Dn),on=r(L),f(O.$$.fragment,L),L.forEach(a),Gt=r(e),f(Ze.$$.fragment,e),Pt=r(e),z=p(e,"DIV",{class:!0});var F=k(z);f(We.$$.fragment,F),an=r(F),at=p(F,"P",{"data-svelte-h":!0}),y(at)!=="svelte-1g4akfy"&&(at.innerHTML=On),sn=r(F),st=p(F,"P",{"data-svelte-h":!0}),y(st)!=="svelte-1s6wgpv"&&(st.innerHTML=Kn),rn=r(F),f(K.$$.fragment,F),F.forEach(a),Nt=r(e),f(Ue.$$.fragment,e),Lt=r(e),Z=p(e,"DIV",{class:!0});var H=k(Z);f(Ie.$$.fragment,H),ln=r(H),rt=p(H,"P",{"data-svelte-h":!0}),y(rt)!=="svelte-vcfia9"&&(rt.innerHTML=eo),dn=r(H),ee=p(H,"DIV",{class:!0});var Se=k(ee);f(Be.$$.fragment,Se),cn=r(Se),it=p(Se,"P",{"data-svelte-h":!0}),y(it)!=="svelte-1ew01iy"&&(it.innerHTML=to),Se.forEach(a),mn=r(H),te=p(H,"DIV",{class:!0});var De=k(te);f(Ve.$$.fragment,De),pn=r(De),lt=p(De,"P",{"data-svelte-h":!0}),y(lt)!=="svelte-2o9eik"&&(lt.innerHTML=no),De.forEach(a),H.forEach(a),Ft=r(e),f(Ge.$$.fragment,e),Ht=r(e),$=p(e,"DIV",{class:!0});var A=k($);f(Pe.$$.fragment,A),gn=r(A),dt=p(A,"P",{"data-svelte-h":!0}),y(dt)!=="svelte-eisylu"&&(dt.innerHTML=oo),fn=r(A),ct=p(A,"P",{"data-svelte-h":!0}),y(ct)!=="svelte-hswkmf"&&(ct.innerHTML=ao),hn=r(A),Y=p(A,"DIV",{class:!0});var Q=k(Y);f(Ne.$$.fragment,Q),un=r(Q),mt=p(Q,"P",{"data-svelte-h":!0}),y(mt)!=="svelte-74bsgo"&&(mt.innerHTML=so),_n=r(Q),f(ne.$$.fragment,Q),Q.forEach(a),bn=r(A),B=p(A,"DIV",{class:!0});var q=k(B);f(Le.$$.fragment,q),Mn=r(q),pt=p(q,"P",{"data-svelte-h":!0}),y(pt)!=="svelte-74bsgo"&&(pt.innerHTML=ro),yn=r(q),f(oe.$$.fragment,q),Tn=r(q),f(ae.$$.fragment,q),q.forEach(a),vn=r(A),V=p(A,"DIV",{class:!0});var E=k(V);f(Fe.$$.fragment,E),wn=r(E),gt=p(E,"P",{"data-svelte-h":!0}),y(gt)!=="svelte-74bsgo"&&(gt.innerHTML=io),$n=r(E),f(se.$$.fragment,E),kn=r(E),f(re.$$.fragment,E),E.forEach(a),A.forEach(a),qt=r(e),f(He.$$.fragment,e),Et=r(e),W=p(e,"DIV",{class:!0});var R=k(W);f(qe.$$.fragment,R),xn=r(R),ft=p(R,"P",{"data-svelte-h":!0}),y(ft)!=="svelte-17paowa"&&(ft.innerHTML=lo),jn=r(R),ht=p(R,"P",{"data-svelte-h":!0}),y(ht)!=="svelte-hswkmf"&&(ht.innerHTML=co),An=r(R),G=p(R,"DIV",{class:!0});var me=k(G);f(Ee.$$.fragment,me),Cn=r(me),ut=p(me,"P",{"data-svelte-h":!0}),y(ut)!=="svelte-17ttuoi"&&(ut.innerHTML=mo),Jn=r(me),f(ie.$$.fragment,me),zn=r(me),f(le.$$.fragment,me),me.forEach(a),R.forEach(a),Rt=r(e),f(Re.$$.fragment,e),Xt=r(e),U=p(e,"DIV",{class:!0});var pe=k(U);f(Xe.$$.fragment,pe),Zn=r(pe),_t=p(pe,"P",{"data-svelte-h":!0}),y(_t)!=="svelte-1on7bqb"&&(_t.innerHTML=po),Wn=r(pe),bt=p(pe,"P",{"data-svelte-h":!0}),y(bt)!=="svelte-hswkmf"&&(bt.innerHTML=go),Un=r(pe),P=p(pe,"DIV",{class:!0});var ge=k(P);f(Ye.$$.fragment,ge),In=r(ge),Mt=p(ge,"P",{"data-svelte-h":!0}),y(Mt)!=="svelte-dqqgg8"&&(Mt.innerHTML=fo),Bn=r(ge),f(de.$$.fragment,ge),Vn=r(ge),f(ce.$$.fragment,ge),ge.forEach(a),pe.forEach(a),Yt=r(e),yt=p(e,"P",{}),k(yt).forEach(a),this.h()},h(){x(n,"name","hf:doc:metadata"),x(n,"content",Wo),x(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(ee,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),x(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,o){i(document.head,n),c(e,T,o),c(e,l,o),c(e,d,o),h(M,e,o),c(e,t,o),h(v,e,o),c(e,vt,o),c(e,he,o),c(e,wt,o),c(e,ue,o),c(e,$t,o),c(e,_e,o),c(e,kt,o),c(e,be,o),c(e,xt,o),h(Me,e,o),c(e,jt,o),c(e,ye,o),c(e,At,o),c(e,Te,o),c(e,Ct,o),h(ve,e,o),c(e,Jt,o),h(we,e,o),c(e,zt,o),c(e,$e,o),c(e,Zt,o),c(e,ke,o),c(e,Wt,o),c(e,xe,o),c(e,Ut,o),h(je,e,o),c(e,It,o),c(e,j,o),h(Ae,j,null),i(j,St),i(j,Ke),i(j,Dt),i(j,et),i(j,Ot),h(S,j,null),i(j,Kt),i(j,D),h(Ce,D,null),i(D,en),i(D,tt),c(e,Bt,o),h(Je,e,o),c(e,Vt,o),c(e,J,o),h(ze,J,null),i(J,tn),i(J,nt),i(J,nn),i(J,ot),i(J,on),h(O,J,null),c(e,Gt,o),h(Ze,e,o),c(e,Pt,o),c(e,z,o),h(We,z,null),i(z,an),i(z,at),i(z,sn),i(z,st),i(z,rn),h(K,z,null),c(e,Nt,o),h(Ue,e,o),c(e,Lt,o),c(e,Z,o),h(Ie,Z,null),i(Z,ln),i(Z,rt),i(Z,dn),i(Z,ee),h(Be,ee,null),i(ee,cn),i(ee,it),i(Z,mn),i(Z,te),h(Ve,te,null),i(te,pn),i(te,lt),c(e,Ft,o),h(Ge,e,o),c(e,Ht,o),c(e,$,o),h(Pe,$,null),i($,gn),i($,dt),i($,fn),i($,ct),i($,hn),i($,Y),h(Ne,Y,null),i(Y,un),i(Y,mt),i(Y,_n),h(ne,Y,null),i($,bn),i($,B),h(Le,B,null),i(B,Mn),i(B,pt),i(B,yn),h(oe,B,null),i(B,Tn),h(ae,B,null),i($,vn),i($,V),h(Fe,V,null),i(V,wn),i(V,gt),i(V,$n),h(se,V,null),i(V,kn),h(re,V,null),c(e,qt,o),h(He,e,o),c(e,Et,o),c(e,W,o),h(qe,W,null),i(W,xn),i(W,ft),i(W,jn),i(W,ht),i(W,An),i(W,G),h(Ee,G,null),i(G,Cn),i(G,ut),i(G,Jn),h(ie,G,null),i(G,zn),h(le,G,null),c(e,Rt,o),h(Re,e,o),c(e,Xt,o),c(e,U,o),h(Xe,U,null),i(U,Zn),i(U,_t),i(U,Wn),i(U,bt),i(U,Un),i(U,P),h(Ye,P,null),i(P,In),i(P,Mt),i(P,Bn),h(de,P,null),i(P,Vn),h(ce,P,null),c(e,Yt,o),c(e,yt,o),Qt=!0},p(e,[o]){const I={};o&2&&(I.$$scope={dirty:o,ctx:e}),S.$set(I);const Qe={};o&2&&(Qe.$$scope={dirty:o,ctx:e}),O.$set(Qe);const L={};o&2&&(L.$$scope={dirty:o,ctx:e}),K.$set(L);const F={};o&2&&(F.$$scope={dirty:o,ctx:e}),ne.$set(F);const H={};o&2&&(H.$$scope={dirty:o,ctx:e}),oe.$set(H);const Se={};o&2&&(Se.$$scope={dirty:o,ctx:e}),ae.$set(Se);const De={};o&2&&(De.$$scope={dirty:o,ctx:e}),se.$set(De);const A={};o&2&&(A.$$scope={dirty:o,ctx:e}),re.$set(A);const Q={};o&2&&(Q.$$scope={dirty:o,ctx:e}),ie.$set(Q);const q={};o&2&&(q.$$scope={dirty:o,ctx:e}),le.$set(q);const E={};o&2&&(E.$$scope={dirty:o,ctx:e}),de.$set(E);const R={};o&2&&(R.$$scope={dirty:o,ctx:e}),ce.$set(R)},i(e){Qt||(u(M.$$.fragment,e),u(v.$$.fragment,e),u(Me.$$.fragment,e),u(ve.$$.fragment,e),u(we.$$.fragment,e),u(je.$$.fragment,e),u(Ae.$$.fragment,e),u(S.$$.fragment,e),u(Ce.$$.fragment,e),u(Je.$$.fragment,e),u(ze.$$.fragment,e),u(O.$$.fragment,e),u(Ze.$$.fragment,e),u(We.$$.fragment,e),u(K.$$.fragment,e),u(Ue.$$.fragment,e),u(Ie.$$.fragment,e),u(Be.$$.fragment,e),u(Ve.$$.fragment,e),u(Ge.$$.fragment,e),u(Pe.$$.fragment,e),u(Ne.$$.fragment,e),u(ne.$$.fragment,e),u(Le.$$.fragment,e),u(oe.$$.fragment,e),u(ae.$$.fragment,e),u(Fe.$$.fragment,e),u(se.$$.fragment,e),u(re.$$.fragment,e),u(He.$$.fragment,e),u(qe.$$.fragment,e),u(Ee.$$.fragment,e),u(ie.$$.fragment,e),u(le.$$.fragment,e),u(Re.$$.fragment,e),u(Xe.$$.fragment,e),u(Ye.$$.fragment,e),u(de.$$.fragment,e),u(ce.$$.fragment,e),Qt=!0)},o(e){_(M.$$.fragment,e),_(v.$$.fragment,e),_(Me.$$.fragment,e),_(ve.$$.fragment,e),_(we.$$.fragment,e),_(je.$$.fragment,e),_(Ae.$$.fragment,e),_(S.$$.fragment,e),_(Ce.$$.fragment,e),_(Je.$$.fragment,e),_(ze.$$.fragment,e),_(O.$$.fragment,e),_(Ze.$$.fragment,e),_(We.$$.fragment,e),_(K.$$.fragment,e),_(Ue.$$.fragment,e),_(Ie.$$.fragment,e),_(Be.$$.fragment,e),_(Ve.$$.fragment,e),_(Ge.$$.fragment,e),_(Pe.$$.fragment,e),_(Ne.$$.fragment,e),_(ne.$$.fragment,e),_(Le.$$.fragment,e),_(oe.$$.fragment,e),_(ae.$$.fragment,e),_(Fe.$$.fragment,e),_(se.$$.fragment,e),_(re.$$.fragment,e),_(He.$$.fragment,e),_(qe.$$.fragment,e),_(Ee.$$.fragment,e),_(ie.$$.fragment,e),_(le.$$.fragment,e),_(Re.$$.fragment,e),_(Xe.$$.fragment,e),_(Ye.$$.fragment,e),_(de.$$.fragment,e),_(ce.$$.fragment,e),Qt=!1},d(e){e&&(a(T),a(l),a(d),a(t),a(vt),a(he),a(wt),a(ue),a($t),a(_e),a(kt),a(be),a(xt),a(jt),a(ye),a(At),a(Te),a(Ct),a(Jt),a(zt),a($e),a(Zt),a(ke),a(Wt),a(xe),a(Ut),a(It),a(j),a(Bt),a(Vt),a(J),a(Gt),a(Pt),a(z),a(Nt),a(Lt),a(Z),a(Ft),a(Ht),a($),a(qt),a(Et),a(W),a(Rt),a(Xt),a(U),a(Yt),a(yt)),a(n),b(M,e),b(v,e),b(Me,e),b(ve,e),b(we,e),b(je,e),b(Ae),b(S),b(Ce),b(Je,e),b(ze),b(O),b(Ze,e),b(We),b(K),b(Ue,e),b(Ie),b(Be),b(Ve),b(Ge,e),b(Pe),b(Ne),b(ne),b(Le),b(oe),b(ae),b(Fe),b(se),b(re),b(He,e),b(qe),b(Ee),b(ie),b(le),b(Re,e),b(Xe),b(Ye),b(de),b(ce)}}}const Wo='{"title":"ALIGN","local":"align","sections":[{"title":"概要","local":"概要","sections":[],"depth":2},{"title":"使用例","local":"使用例","sections":[],"depth":2},{"title":"参考資料","local":"参考資料","sections":[],"depth":2},{"title":"AlignConfig","local":"transformers.AlignConfig","sections":[],"depth":2},{"title":"AlignTextConfig","local":"transformers.AlignTextConfig","sections":[],"depth":2},{"title":"AlignVisionConfig","local":"transformers.AlignVisionConfig","sections":[],"depth":2},{"title":"AlignProcessor","local":"transformers.AlignProcessor","sections":[],"depth":2},{"title":"AlignModel","local":"transformers.AlignModel","sections":[],"depth":2},{"title":"AlignTextModel","local":"transformers.AlignTextModel","sections":[],"depth":2},{"title":"AlignVisionModel","local":"transformers.AlignVisionModel","sections":[],"depth":2}],"depth":1}';function Uo(w){return uo(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Fo extends _o{constructor(n){super(),bo(this,n,Uo,Zo,ho,{})}}export{Fo as component};
