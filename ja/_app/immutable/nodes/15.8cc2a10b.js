import{s as pe,n as fe,o as ce}from"../chunks/scheduler.9bc65507.js";import{S as ke,i as Me,g as r,s as n,r as U,A as de,h as i,f as s,c as a,j as oe,u as Z,x as m,k as me,y as je,a as l,v as w,d as J,t as V,w as _}from"../chunks/index.707bf1b6.js";import{C as K}from"../chunks/CodeBlock.54a9f38d.js";import{H as O}from"../chunks/Heading.342b1fa6.js";function ye(D){let o,v,B,C,p,I,f,ee='<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>は<a href="https://huggingface.co/docs/tokenizers" rel="nofollow">🤗 Tokenizers</a>ライブラリに依存しています。🤗 Tokenizersライブラリから取得したトークナイザーは、非常に簡単に🤗 Transformersにロードできます。',P,c,te="具体的な内容に入る前に、まずはいくつかの行でダミーのトークナイザーを作成することから始めましょう：",N,k,Q,M,se=`私たちは今、定義したファイルにトレーニングされたトークナイザーを持っています。これをランタイムで引き続き使用するか、
将来の再利用のためにJSONファイルに保存することができます。`,W,d,X,j,le=`🤗 Transformersライブラリでこのトークナイザーオブジェクトをどのように活用できるかを見てみましょう。<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>クラスは、
<em>tokenizer</em>オブジェクトを引数として受け入れ、簡単にインスタンス化できるようにします。`,G,y,q,g,ne='このオブジェクトは、🤗 Transformers トークナイザーが共有するすべてのメソッドと一緒に使用できます！詳細については、<a href="main_classes/tokenizer">トークナイザーページ</a>をご覧ください。',E,h,R,T,ae="JSONファイルからトークナイザーを読み込むには、まずトークナイザーを保存することから始めましょう：",L,u,x,z,re="このファイルを保存したパスは、<code>PreTrainedTokenizerFast</code> の初期化メソッドに <code>tokenizer_file</code> パラメータを使用して渡すことができます：",H,$,S,b,ie='このオブジェクトは、🤗 Transformers トークナイザーが共有するすべてのメソッドと一緒に使用できるようになりました！詳細については、<a href="main_classes/tokenizer">トークナイザーページ</a>をご覧ください。',A,F,Y;return p=new O({props:{title:"Use tokenizers from 🤗 Tokenizers",local:"use-tokenizers-from--tokenizers",headingTag:"h1"}}),k=new K({props:{code:"ZnJvbSUyMHRva2VuaXplcnMlMjBpbXBvcnQlMjBUb2tlbml6ZXIlMEFmcm9tJTIwdG9rZW5pemVycy5tb2RlbHMlMjBpbXBvcnQlMjBCUEUlMEFmcm9tJTIwdG9rZW5pemVycy50cmFpbmVycyUyMGltcG9ydCUyMEJwZVRyYWluZXIlMEFmcm9tJTIwdG9rZW5pemVycy5wcmVfdG9rZW5pemVycyUyMGltcG9ydCUyMFdoaXRlc3BhY2UlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBUb2tlbml6ZXIoQlBFKHVua190b2tlbiUzRCUyMiU1QlVOSyU1RCUyMikpJTBBdHJhaW5lciUyMCUzRCUyMEJwZVRyYWluZXIoc3BlY2lhbF90b2tlbnMlM0QlNUIlMjIlNUJVTkslNUQlMjIlMkMlMjAlMjIlNUJDTFMlNUQlMjIlMkMlMjAlMjIlNUJTRVAlNUQlMjIlMkMlMjAlMjIlNUJQQUQlNUQlMjIlMkMlMjAlMjIlNUJNQVNLJTVEJTIyJTVEKSUwQSUwQXRva2VuaXplci5wcmVfdG9rZW5pemVyJTIwJTNEJTIwV2hpdGVzcGFjZSgpJTBBZmlsZXMlMjAlM0QlMjAlNUIuLi4lNUQlMEF0b2tlbml6ZXIudHJhaW4oZmlsZXMlMkMlMjB0cmFpbmVyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pre_tokenizer = Whitespace()
<span class="hljs-meta">&gt;&gt;&gt; </span>files = [...]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.train(files, trainer)`,wrap:!1}}),d=new O({props:{title:"Loading directly from the tokenizer object",local:"loading-directly-from-the-tokenizer-object",headingTag:"h2"}}),y=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfb2JqZWN0JTNEdG9rZW5pemVyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`,wrap:!1}}),h=new O({props:{title:"Loading from a JSON file",local:"loading-from-a-json-file",headingTag:"h2"}}),u=new K({props:{code:"dG9rZW5pemVyLnNhdmUoJTIydG9rZW5pemVyLmpzb24lMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)',wrap:!1}}),$=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfZmlsZSUzRCUyMnRva2VuaXplci5qc29uJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="hljs-string">&quot;tokenizer.json&quot;</span>)`,wrap:!1}}),{c(){o=r("meta"),v=n(),B=r("p"),C=n(),U(p.$$.fragment),I=n(),f=r("p"),f.innerHTML=ee,P=n(),c=r("p"),c.textContent=te,N=n(),U(k.$$.fragment),Q=n(),M=r("p"),M.textContent=se,W=n(),U(d.$$.fragment),X=n(),j=r("p"),j.innerHTML=le,G=n(),U(y.$$.fragment),q=n(),g=r("p"),g.innerHTML=ne,E=n(),U(h.$$.fragment),R=n(),T=r("p"),T.textContent=ae,L=n(),U(u.$$.fragment),x=n(),z=r("p"),z.innerHTML=re,H=n(),U($.$$.fragment),S=n(),b=r("p"),b.innerHTML=ie,A=n(),F=r("p"),this.h()},l(e){const t=de("svelte-u9bgzb",document.head);o=i(t,"META",{name:!0,content:!0}),t.forEach(s),v=a(e),B=i(e,"P",{}),oe(B).forEach(s),C=a(e),Z(p.$$.fragment,e),I=a(e),f=i(e,"P",{"data-svelte-h":!0}),m(f)!=="svelte-19yjkfk"&&(f.innerHTML=ee),P=a(e),c=i(e,"P",{"data-svelte-h":!0}),m(c)!=="svelte-ut5c9x"&&(c.textContent=te),N=a(e),Z(k.$$.fragment,e),Q=a(e),M=i(e,"P",{"data-svelte-h":!0}),m(M)!=="svelte-invdsu"&&(M.textContent=se),W=a(e),Z(d.$$.fragment,e),X=a(e),j=i(e,"P",{"data-svelte-h":!0}),m(j)!=="svelte-myekfd"&&(j.innerHTML=le),G=a(e),Z(y.$$.fragment,e),q=a(e),g=i(e,"P",{"data-svelte-h":!0}),m(g)!=="svelte-3q8kj6"&&(g.innerHTML=ne),E=a(e),Z(h.$$.fragment,e),R=a(e),T=i(e,"P",{"data-svelte-h":!0}),m(T)!=="svelte-132sdzq"&&(T.textContent=ae),L=a(e),Z(u.$$.fragment,e),x=a(e),z=i(e,"P",{"data-svelte-h":!0}),m(z)!=="svelte-1djco1p"&&(z.innerHTML=re),H=a(e),Z($.$$.fragment,e),S=a(e),b=i(e,"P",{"data-svelte-h":!0}),m(b)!=="svelte-1ipwbgd"&&(b.innerHTML=ie),A=a(e),F=i(e,"P",{}),oe(F).forEach(s),this.h()},h(){me(o,"name","hf:doc:metadata"),me(o,"content",ge)},m(e,t){je(document.head,o),l(e,v,t),l(e,B,t),l(e,C,t),w(p,e,t),l(e,I,t),l(e,f,t),l(e,P,t),l(e,c,t),l(e,N,t),w(k,e,t),l(e,Q,t),l(e,M,t),l(e,W,t),w(d,e,t),l(e,X,t),l(e,j,t),l(e,G,t),w(y,e,t),l(e,q,t),l(e,g,t),l(e,E,t),w(h,e,t),l(e,R,t),l(e,T,t),l(e,L,t),w(u,e,t),l(e,x,t),l(e,z,t),l(e,H,t),w($,e,t),l(e,S,t),l(e,b,t),l(e,A,t),l(e,F,t),Y=!0},p:fe,i(e){Y||(J(p.$$.fragment,e),J(k.$$.fragment,e),J(d.$$.fragment,e),J(y.$$.fragment,e),J(h.$$.fragment,e),J(u.$$.fragment,e),J($.$$.fragment,e),Y=!0)},o(e){V(p.$$.fragment,e),V(k.$$.fragment,e),V(d.$$.fragment,e),V(y.$$.fragment,e),V(h.$$.fragment,e),V(u.$$.fragment,e),V($.$$.fragment,e),Y=!1},d(e){e&&(s(v),s(B),s(C),s(I),s(f),s(P),s(c),s(N),s(Q),s(M),s(W),s(X),s(j),s(G),s(q),s(g),s(E),s(R),s(T),s(L),s(x),s(z),s(H),s(S),s(b),s(A),s(F)),s(o),_(p,e),_(k,e),_(d,e),_(y,e),_(h,e),_(u,e),_($,e)}}}const ge='{"title":"Use tokenizers from 🤗 Tokenizers","local":"use-tokenizers-from--tokenizers","sections":[{"title":"Loading directly from the tokenizer object","local":"loading-directly-from-the-tokenizer-object","sections":[],"depth":2},{"title":"Loading from a JSON file","local":"loading-from-a-json-file","sections":[],"depth":2}],"depth":1}';function he(D){return ce(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class be extends ke{constructor(o){super(),Me(this,o,he,ye,pe,{})}}export{be as component};
