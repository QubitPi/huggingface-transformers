import{s as pe,n as fe,o as ce}from"../chunks/scheduler.9bc65507.js";import{S as ke,i as Me,g as r,s as n,r as U,A as de,h as i,f as s,c as a,j as oe,u as Z,x as m,k as me,y as je,a as l,v as w,d as J,t as V,w as _}from"../chunks/index.707bf1b6.js";import{C as K}from"../chunks/CodeBlock.54a9f38d.js";import{H as O}from"../chunks/Heading.342b1fa6.js";function ye(D){let o,v,B,C,p,I,f,ee='<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>ã¯<a href="https://huggingface.co/docs/tokenizers" rel="nofollow">ğŸ¤— Tokenizers</a>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ä¾å­˜ã—ã¦ã„ã¾ã™ã€‚ğŸ¤— Tokenizersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰å–å¾—ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯ã€éå¸¸ã«ç°¡å˜ã«ğŸ¤— Transformersã«ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚',P,c,te="å…·ä½“çš„ãªå†…å®¹ã«å…¥ã‚‹å‰ã«ã€ã¾ãšã¯ã„ãã¤ã‹ã®è¡Œã§ãƒ€ãƒŸãƒ¼ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ï¼š",N,k,Q,M,se=`ç§ãŸã¡ã¯ä»Šã€å®šç¾©ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã‚Œã‚’ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã§å¼•ãç¶šãä½¿ç”¨ã™ã‚‹ã‹ã€
å°†æ¥ã®å†åˆ©ç”¨ã®ãŸã‚ã«JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚`,W,d,X,j,le=`ğŸ¤— Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã“ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã©ã®ã‚ˆã†ã«æ´»ç”¨ã§ãã‚‹ã‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast">PreTrainedTokenizerFast</a>ã‚¯ãƒ©ã‚¹ã¯ã€
<em>tokenizer</em>ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å¼•æ•°ã¨ã—ã¦å—ã‘å…¥ã‚Œã€ç°¡å˜ã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚`,G,y,q,g,ne='ã“ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ğŸ¤— Transformers ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒå…±æœ‰ã™ã‚‹ã™ã¹ã¦ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¨ä¸€ç·’ã«ä½¿ç”¨ã§ãã¾ã™ï¼è©³ç´°ã«ã¤ã„ã¦ã¯ã€<a href="main_classes/tokenizer">ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒšãƒ¼ã‚¸</a>ã‚’ã”è¦§ãã ã•ã„ã€‚',E,h,R,T,ae="JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã‚€ã«ã¯ã€ã¾ãšãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ï¼š",L,u,x,z,re="ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ãŸãƒ‘ã‚¹ã¯ã€<code>PreTrainedTokenizerFast</code> ã®åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰ã« <code>tokenizer_file</code> ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ï¼š",H,$,S,b,ie='ã“ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ğŸ¤— Transformers ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãŒå…±æœ‰ã™ã‚‹ã™ã¹ã¦ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¨ä¸€ç·’ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸï¼è©³ç´°ã«ã¤ã„ã¦ã¯ã€<a href="main_classes/tokenizer">ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒšãƒ¼ã‚¸</a>ã‚’ã”è¦§ãã ã•ã„ã€‚',A,F,Y;return p=new O({props:{title:"Use tokenizers from ğŸ¤— Tokenizers",local:"use-tokenizers-from--tokenizers",headingTag:"h1"}}),k=new K({props:{code:"ZnJvbSUyMHRva2VuaXplcnMlMjBpbXBvcnQlMjBUb2tlbml6ZXIlMEFmcm9tJTIwdG9rZW5pemVycy5tb2RlbHMlMjBpbXBvcnQlMjBCUEUlMEFmcm9tJTIwdG9rZW5pemVycy50cmFpbmVycyUyMGltcG9ydCUyMEJwZVRyYWluZXIlMEFmcm9tJTIwdG9rZW5pemVycy5wcmVfdG9rZW5pemVycyUyMGltcG9ydCUyMFdoaXRlc3BhY2UlMEElMEF0b2tlbml6ZXIlMjAlM0QlMjBUb2tlbml6ZXIoQlBFKHVua190b2tlbiUzRCUyMiU1QlVOSyU1RCUyMikpJTBBdHJhaW5lciUyMCUzRCUyMEJwZVRyYWluZXIoc3BlY2lhbF90b2tlbnMlM0QlNUIlMjIlNUJVTkslNUQlMjIlMkMlMjAlMjIlNUJDTFMlNUQlMjIlMkMlMjAlMjIlNUJTRVAlNUQlMjIlMkMlMjAlMjIlNUJQQUQlNUQlMjIlMkMlMjAlMjIlNUJNQVNLJTVEJTIyJTVEKSUwQSUwQXRva2VuaXplci5wcmVfdG9rZW5pemVyJTIwJTNEJTIwV2hpdGVzcGFjZSgpJTBBZmlsZXMlMjAlM0QlMjAlNUIuLi4lNUQlMEF0b2tlbml6ZXIudHJhaW4oZmlsZXMlMkMlMjB0cmFpbmVyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers <span class="hljs-keyword">import</span> Tokenizer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.models <span class="hljs-keyword">import</span> BPE
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.trainers <span class="hljs-keyword">import</span> BpeTrainer
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> tokenizers.pre_tokenizers <span class="hljs-keyword">import</span> Whitespace

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = Tokenizer(BPE(unk_token=<span class="hljs-string">&quot;[UNK]&quot;</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = BpeTrainer(special_tokens=[<span class="hljs-string">&quot;[UNK]&quot;</span>, <span class="hljs-string">&quot;[CLS]&quot;</span>, <span class="hljs-string">&quot;[SEP]&quot;</span>, <span class="hljs-string">&quot;[PAD]&quot;</span>, <span class="hljs-string">&quot;[MASK]&quot;</span>])

<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.pre_tokenizer = Whitespace()
<span class="hljs-meta">&gt;&gt;&gt; </span>files = [...]
<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.train(files, trainer)`,wrap:!1}}),d=new O({props:{title:"Loading directly from the tokenizer object",local:"loading-directly-from-the-tokenizer-object",headingTag:"h2"}}),y=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfb2JqZWN0JTNEdG9rZW5pemVyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)`,wrap:!1}}),h=new O({props:{title:"Loading from a JSON file",local:"loading-from-a-json-file",headingTag:"h2"}}),u=new K({props:{code:"dG9rZW5pemVyLnNhdmUoJTIydG9rZW5pemVyLmpzb24lMjIp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer.save(<span class="hljs-string">&quot;tokenizer.json&quot;</span>)',wrap:!1}}),$=new K({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMFByZVRyYWluZWRUb2tlbml6ZXJGYXN0JTBBJTBBZmFzdF90b2tlbml6ZXIlMjAlM0QlMjBQcmVUcmFpbmVkVG9rZW5pemVyRmFzdCh0b2tlbml6ZXJfZmlsZSUzRCUyMnRva2VuaXplci5qc29uJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> PreTrainedTokenizerFast

<span class="hljs-meta">&gt;&gt;&gt; </span>fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=<span class="hljs-string">&quot;tokenizer.json&quot;</span>)`,wrap:!1}}),{c(){o=r("meta"),v=n(),B=r("p"),C=n(),U(p.$$.fragment),I=n(),f=r("p"),f.innerHTML=ee,P=n(),c=r("p"),c.textContent=te,N=n(),U(k.$$.fragment),Q=n(),M=r("p"),M.textContent=se,W=n(),U(d.$$.fragment),X=n(),j=r("p"),j.innerHTML=le,G=n(),U(y.$$.fragment),q=n(),g=r("p"),g.innerHTML=ne,E=n(),U(h.$$.fragment),R=n(),T=r("p"),T.textContent=ae,L=n(),U(u.$$.fragment),x=n(),z=r("p"),z.innerHTML=re,H=n(),U($.$$.fragment),S=n(),b=r("p"),b.innerHTML=ie,A=n(),F=r("p"),this.h()},l(e){const t=de("svelte-u9bgzb",document.head);o=i(t,"META",{name:!0,content:!0}),t.forEach(s),v=a(e),B=i(e,"P",{}),oe(B).forEach(s),C=a(e),Z(p.$$.fragment,e),I=a(e),f=i(e,"P",{"data-svelte-h":!0}),m(f)!=="svelte-19yjkfk"&&(f.innerHTML=ee),P=a(e),c=i(e,"P",{"data-svelte-h":!0}),m(c)!=="svelte-ut5c9x"&&(c.textContent=te),N=a(e),Z(k.$$.fragment,e),Q=a(e),M=i(e,"P",{"data-svelte-h":!0}),m(M)!=="svelte-invdsu"&&(M.textContent=se),W=a(e),Z(d.$$.fragment,e),X=a(e),j=i(e,"P",{"data-svelte-h":!0}),m(j)!=="svelte-myekfd"&&(j.innerHTML=le),G=a(e),Z(y.$$.fragment,e),q=a(e),g=i(e,"P",{"data-svelte-h":!0}),m(g)!=="svelte-3q8kj6"&&(g.innerHTML=ne),E=a(e),Z(h.$$.fragment,e),R=a(e),T=i(e,"P",{"data-svelte-h":!0}),m(T)!=="svelte-132sdzq"&&(T.textContent=ae),L=a(e),Z(u.$$.fragment,e),x=a(e),z=i(e,"P",{"data-svelte-h":!0}),m(z)!=="svelte-1djco1p"&&(z.innerHTML=re),H=a(e),Z($.$$.fragment,e),S=a(e),b=i(e,"P",{"data-svelte-h":!0}),m(b)!=="svelte-1ipwbgd"&&(b.innerHTML=ie),A=a(e),F=i(e,"P",{}),oe(F).forEach(s),this.h()},h(){me(o,"name","hf:doc:metadata"),me(o,"content",ge)},m(e,t){je(document.head,o),l(e,v,t),l(e,B,t),l(e,C,t),w(p,e,t),l(e,I,t),l(e,f,t),l(e,P,t),l(e,c,t),l(e,N,t),w(k,e,t),l(e,Q,t),l(e,M,t),l(e,W,t),w(d,e,t),l(e,X,t),l(e,j,t),l(e,G,t),w(y,e,t),l(e,q,t),l(e,g,t),l(e,E,t),w(h,e,t),l(e,R,t),l(e,T,t),l(e,L,t),w(u,e,t),l(e,x,t),l(e,z,t),l(e,H,t),w($,e,t),l(e,S,t),l(e,b,t),l(e,A,t),l(e,F,t),Y=!0},p:fe,i(e){Y||(J(p.$$.fragment,e),J(k.$$.fragment,e),J(d.$$.fragment,e),J(y.$$.fragment,e),J(h.$$.fragment,e),J(u.$$.fragment,e),J($.$$.fragment,e),Y=!0)},o(e){V(p.$$.fragment,e),V(k.$$.fragment,e),V(d.$$.fragment,e),V(y.$$.fragment,e),V(h.$$.fragment,e),V(u.$$.fragment,e),V($.$$.fragment,e),Y=!1},d(e){e&&(s(v),s(B),s(C),s(I),s(f),s(P),s(c),s(N),s(Q),s(M),s(W),s(X),s(j),s(G),s(q),s(g),s(E),s(R),s(T),s(L),s(x),s(z),s(H),s(S),s(b),s(A),s(F)),s(o),_(p,e),_(k,e),_(d,e),_(y,e),_(h,e),_(u,e),_($,e)}}}const ge='{"title":"Use tokenizers from ğŸ¤— Tokenizers","local":"use-tokenizers-from--tokenizers","sections":[{"title":"Loading directly from the tokenizer object","local":"loading-directly-from-the-tokenizer-object","sections":[],"depth":2},{"title":"Loading from a JSON file","local":"loading-from-a-json-file","sections":[],"depth":2}],"depth":1}';function he(D){return ce(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class be extends ke{constructor(o){super(),Me(this,o,he,ye,pe,{})}}export{be as component};
