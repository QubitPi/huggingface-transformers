import{s as qt,f as Rt,o as Xt,n as Se}from"../chunks/scheduler.9bc65507.js";import{S as Bt,i as Yt,g as i,s as r,r as f,A as Qt,h as d,f as a,c as l,j as W,u as h,x as u,k as J,y as c,a as s,v as g,d as _,t as T,w as b}from"../chunks/index.707bf1b6.js";import{T as Et}from"../chunks/Tip.c2ecdbf4.js";import{D as ge}from"../chunks/Docstring.17db21ae.js";import{C as bt}from"../chunks/CodeBlock.54a9f38d.js";import{E as Tt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as Ht}from"../chunks/PipelineTag.44585822.js";import{H as B}from"../chunks/Heading.342b1fa6.js";function Lt(j){let o,M="Example:",m,p,y;return p=new bt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFTVENvbmZpZyUyQyUyMEFTVE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEFTVCUyME1JVCUyRmFzdC1maW5ldHVuZWQtYXVkaW9zZXQtMTAtMTAtMC40NTkzJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEFTVENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBNSVQlMkZhc3QtZmluZXR1bmVkLWF1ZGlvc2V0LTEwLTEwLTAuNDU5MyUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQVNUTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ASTConfig, ASTModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AST MIT/ast-finetuned-audioset-10-10-0.4593 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ASTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the MIT/ast-finetuned-audioset-10-10-0.4593 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ASTModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=i("p"),o.textContent=M,m=r(),f(p.$$.fragment)},l(n){o=d(n,"P",{"data-svelte-h":!0}),u(o)!=="svelte-11lpom8"&&(o.textContent=M),m=l(n),h(p.$$.fragment,n)},m(n,w){s(n,o,w),s(n,m,w),g(p,n,w),y=!0},p:Se,i(n){y||(_(p.$$.fragment,n),y=!0)},o(n){T(p.$$.fragment,n),y=!1},d(n){n&&(a(o),a(m)),b(p,n)}}}function Pt(j){let o,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=i("p"),o.innerHTML=M},l(m){o=d(m,"P",{"data-svelte-h":!0}),u(o)!=="svelte-fincs2"&&(o.innerHTML=M)},m(m,p){s(m,o,p)},p:Se,d(m){m&&a(o)}}}function Dt(j){let o,M="Example:",m,p,y;return p=new bt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBU1RNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJoZi1pbnRlcm5hbC10ZXN0aW5nJTJGbGlicmlzcGVlY2hfYXNyX2RlbW8lMjIlMkMlMjAlMjJjbGVhbiUyMiUyQyUyMHNwbGl0JTNEJTIydmFsaWRhdGlvbiUyMiklMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5zb3J0KCUyMmlkJTIyKSUwQXNhbXBsaW5nX3JhdGUlMjAlM0QlMjBkYXRhc2V0LmZlYXR1cmVzJTVCJTIyYXVkaW8lMjIlNUQuc2FtcGxpbmdfcmF0ZSUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMk1JVCUyRmFzdC1maW5ldHVuZWQtYXVkaW9zZXQtMTAtMTAtMC40NTkzJTIyKSUwQW1vZGVsJTIwJTNEJTIwQVNUTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMk1JVCUyRmFzdC1maW5ldHVuZWQtYXVkaW9zZXQtMTAtMTAtMC40NTkzJTIyKSUwQSUwQSUyMyUyMGF1ZGlvJTIwZmlsZSUyMGlzJTIwZGVjb2RlZCUyMG9uJTIwdGhlJTIwZmx5JTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGRhdGFzZXQlNUIwJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUIlMjJhcnJheSUyMiU1RCUyQyUyMHNhbXBsaW5nX3JhdGUlM0RzYW1wbGluZ19yYXRlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, ASTModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;MIT/ast-finetuned-audioset-10-10-0.4593&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ASTModel.from_pretrained(<span class="hljs-string">&quot;MIT/ast-finetuned-audioset-10-10-0.4593&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">1214</span>, <span class="hljs-number">768</span>]`,wrap:!1}}),{c(){o=i("p"),o.textContent=M,m=r(),f(p.$$.fragment)},l(n){o=d(n,"P",{"data-svelte-h":!0}),u(o)!=="svelte-11lpom8"&&(o.textContent=M),m=l(n),h(p.$$.fragment,n)},m(n,w){s(n,o,w),s(n,m,w),g(p,n,w),y=!0},p:Se,i(n){y||(_(p.$$.fragment,n),y=!0)},o(n){T(p.$$.fragment,n),y=!1},d(n){n&&(a(o),a(m)),b(p,n)}}}function Kt(j){let o,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=i("p"),o.innerHTML=M},l(m){o=d(m,"P",{"data-svelte-h":!0}),u(o)!=="svelte-fincs2"&&(o.innerHTML=M)},m(m,p){s(m,o,p)},p:Se,d(m){m&&a(o)}}}function Ot(j){let o,M="Example:",m,p,y;return p=new bt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTJDJTIwQVNURm9yQXVkaW9DbGFzc2lmaWNhdGlvbiUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJoZi1pbnRlcm5hbC10ZXN0aW5nJTJGbGlicmlzcGVlY2hfYXNyX2RlbW8lMjIlMkMlMjAlMjJjbGVhbiUyMiUyQyUyMHNwbGl0JTNEJTIydmFsaWRhdGlvbiUyMiklMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5zb3J0KCUyMmlkJTIyKSUwQXNhbXBsaW5nX3JhdGUlMjAlM0QlMjBkYXRhc2V0LmZlYXR1cmVzJTVCJTIyYXVkaW8lMjIlNUQuc2FtcGxpbmdfcmF0ZSUwQSUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwQXV0b0ZlYXR1cmVFeHRyYWN0b3IuZnJvbV9wcmV0cmFpbmVkKCUyMk1JVCUyRmFzdC1maW5ldHVuZWQtYXVkaW9zZXQtMTAtMTAtMC40NTkzJTIyKSUwQW1vZGVsJTIwJTNEJTIwQVNURm9yQXVkaW9DbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyTUlUJTJGYXN0LWZpbmV0dW5lZC1hdWRpb3NldC0xMC0xMC0wLjQ1OTMlMjIpJTBBJTBBJTIzJTIwYXVkaW8lMjBmaWxlJTIwaXMlMjBkZWNvZGVkJTIwb24lMjB0aGUlMjBmbHklMEFpbnB1dHMlMjAlM0QlMjBmZWF0dXJlX2V4dHJhY3RvcihkYXRhc2V0JTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyYXJyYXklMjIlNUQlMkMlMjBzYW1wbGluZ19yYXRlJTNEc2FtcGxpbmdfcmF0ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEFwcmVkaWN0ZWRfY2xhc3NfaWRzJTIwJTNEJTIwdG9yY2guYXJnbWF4KGxvZ2l0cyUyQyUyMGRpbSUzRC0xKS5pdGVtKCklMEFwcmVkaWN0ZWRfbGFiZWwlMjAlM0QlMjBtb2RlbC5jb25maWcuaWQybGFiZWwlNUJwcmVkaWN0ZWRfY2xhc3NfaWRzJTVEJTBBcHJlZGljdGVkX2xhYmVsJTBBJTBBJTIzJTIwY29tcHV0ZSUyMGxvc3MlMjAtJTIwdGFyZ2V0X2xhYmVsJTIwaXMlMjBlLmcuJTIwJTIyZG93biUyMiUwQXRhcmdldF9sYWJlbCUyMCUzRCUyMG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QjAlNUQlMEFpbnB1dHMlNUIlMjJsYWJlbHMlMjIlNUQlMjAlM0QlMjB0b3JjaC50ZW5zb3IoJTVCbW9kZWwuY29uZmlnLmxhYmVsMmlkJTVCdGFyZ2V0X2xhYmVsJTVEJTVEKSUwQWxvc3MlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9zcyUwQXJvdW5kKGxvc3MuaXRlbSgpJTJDJTIwMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, ASTForAudioClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;MIT/ast-finetuned-audioset-10-10-0.4593&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ASTForAudioClassification.from_pretrained(<span class="hljs-string">&quot;MIT/ast-finetuned-audioset-10-10-0.4593&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = model.config.id2label[predicted_class_ids]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label
<span class="hljs-string">&#x27;Speech&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss - target_label is e.g. &quot;down&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_label = model.config.id2label[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([model.config.label2id[target_label]])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">0.17</span>`,wrap:!1}}),{c(){o=i("p"),o.textContent=M,m=r(),f(p.$$.fragment)},l(n){o=d(n,"P",{"data-svelte-h":!0}),u(o)!=="svelte-11lpom8"&&(o.textContent=M),m=l(n),h(p.$$.fragment,n)},m(n,w){s(n,o,w),s(n,m,w),g(p,n,w),y=!0},p:Se,i(n){y||(_(p.$$.fragment,n),y=!0)},o(n){T(p.$$.fragment,n),y=!1},d(n){n&&(a(o),a(m)),b(p,n)}}}function ea(j){let o,M,m,p,y,n,w,Je,Y,yt='Audio Spectrogram Transformerモデルは、<a href="https://arxiv.org/abs/2104.01778" rel="nofollow">AST: Audio Spectrogram Transformer</a>という論文でYuan Gong、Yu-An Chung、James Glassによって提案されました。これは、音声を画像（スペクトログラム）に変換することで、音声に<a href="vit">Vision Transformer</a>を適用します。このモデルは音声分類において最先端の結果を得ています。',Fe,Q,Mt="論文の要旨は以下の通りです：",Ue,H,wt="<em>過去10年間で、畳み込みニューラルネットワーク（CNN）は、音声スペクトログラムから対応するラベルへの直接的なマッピングを学習することを目指す、エンドツーエンドの音声分類モデルの主要な構成要素として広く採用されてきました。長距離のグローバルなコンテキストをより良く捉えるため、最近の傾向として、CNNの上にセルフアテンション機構を追加し、CNN-アテンションハイブリッドモデルを形成することがあります。しかし、CNNへの依存が必要かどうか、そして純粋にアテンションに基づくニューラルネットワークだけで音声分類において良いパフォーマンスを得ることができるかどうかは明らかではありません。本論文では、これらの問いに答えるため、音声分類用では最初の畳み込みなしで純粋にアテンションベースのモデルであるAudio Spectrogram Transformer（AST）を紹介します。我々はASTを様々なオーディオ分類ベンチマークで評価し、AudioSetで0.485 mAP、ESC-50で95.6%の正解率、Speech Commands V2で98.1%の正解率という新たな最先端の結果を達成しました。</em>",ke,z,vt,Ge,L,$t='Audio Spectrogram Transformerのアーキテクチャ。<a href="https://arxiv.org/abs/2104.01778">元論文</a>より抜粋。',We,P,xt=`このモデルは<a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>より提供されました。
オリジナルのコードは<a href="https://github.com/YuanGongND/ast" rel="nofollow">こちら</a>で見ることができます。`,ze,D,Ze,K,Ct='<li>独自のデータセットでAudio Spectrogram Transformer（AST）をファインチューニングする場合、入力の正規化（入力の平均を0、標準偏差を0.5にすること）処理することが推奨されます。<a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor">ASTFeatureExtractor</a>はこれを処理します。デフォルトではAudioSetの平均と標準偏差を使用していることに注意してください。著者が下流のデータセットの統計をどのように計算しているかは、<a href="https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py" rel="nofollow"><code>ast/src/get_norm_stats.py</code></a>で確認することができます。</li> <li>ASTは低い学習率が必要であり 著者は<a href="https://arxiv.org/abs/2102.01243" rel="nofollow">PSLA論文</a>で提案されたCNNモデルに比べて10倍小さい学習率を使用しています）、素早く収束するため、タスクに適した学習率と学習率スケジューラーを探すことをお勧めします。</li>',Ve,O,Ne,ee,jt="Audio Spectrogram Transformerの使用を開始するのに役立つ公式のHugging Faceおよびコミュニティ（🌎で示されている）の参考資料の一覧です。",Ie,te,Ee,ae,At='<li>ASTを用いた音声分類の推論を説明するノートブックは<a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/AST" rel="nofollow">こちら</a>で見ることができます。</li> <li><a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification">ASTForAudioClassification</a>は、この<a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification" rel="nofollow">例示スクリプト</a>と<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb" rel="nofollow">ノートブック</a>によってサポートされています。</li> <li>こちらも参照：<a href="../tasks/audio_classification">音声分類タスク</a>。</li>',qe,oe,St="ここに参考資料を提出したい場合は、気兼ねなくPull Requestを開いてください。私たちはそれをレビューいたします！参考資料は、既存のものを複製するのではなく、何か新しいことを示すことが理想的です。",Re,ne,Xe,$,se,Oe,_e,Jt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTModel">ASTModel</a>. It is used to instantiate an AST
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the AST
<a href="https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593" rel="nofollow">MIT/ast-finetuned-audioset-10-10-0.4593</a>
architecture.`,et,Te,Ft=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,tt,Z,Be,re,Ye,v,le,at,be,Ut="Constructs a Audio Spectrogram Transformer (AST) feature extractor.",ot,ye,kt=`This feature extractor inherits from <a href="/docs/transformers/main/ja/main_classes/feature_extractor#transformers.SequenceFeatureExtractor">SequenceFeatureExtractor</a> which contains
most of the main methods. Users should refer to this superclass for more information regarding those methods.`,nt,Me,Gt=`This class extracts mel-filter bank features from raw speech using TorchAudio if installed or using numpy
otherwise, pads/truncates them to a fixed length and normalizes them using a mean and standard deviation.`,st,V,ie,rt,we,Wt="Main method to featurize and prepare for the model one or several sequence(s).",Qe,de,He,F,ce,lt,ve,zt=`The bare AST Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,it,A,me,dt,$e,Zt='The <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTModel">ASTModel</a> forward method, overrides the <code>__call__</code> special method.',ct,N,mt,I,Le,pe,Pe,x,ue,pt,xe,Vt=`Audio Spectrogram Transformer model with an audio classification head on top (a linear layer on top of the pooled
output) e.g. for datasets like AudioSet, Speech Commands v2.`,ut,Ce,Nt=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,ft,S,fe,ht,je,It='The <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification">ASTForAudioClassification</a> forward method, overrides the <code>__call__</code> special method.',gt,E,_t,q,De,Ae,Ke;return y=new B({props:{title:"Audio Spectrogram Transformer",local:"audio-spectrogram-transformer",headingTag:"h1"}}),w=new B({props:{title:"概要",local:"概要",headingTag:"h2"}}),D=new B({props:{title:"使用上のヒント",local:"使用上のヒント",headingTag:"h2"}}),O=new B({props:{title:"参考資料",local:"参考資料",headingTag:"h2"}}),te=new Ht({props:{pipeline:"audio-classification"}}),ne=new B({props:{title:"ASTConfig",local:"transformers.ASTConfig",headingTag:"h2"}}),se=new ge({props:{name:"class transformers.ASTConfig",anchor:"transformers.ASTConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"patch_size",val:" = 16"},{name:"qkv_bias",val:" = True"},{name:"frequency_stride",val:" = 10"},{name:"time_stride",val:" = 10"},{name:"max_length",val:" = 1024"},{name:"num_mel_bins",val:" = 128"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ASTConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.ASTConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.ASTConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.ASTConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.ASTConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.ASTConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.ASTConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.ASTConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ASTConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.ASTConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.ASTConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.ASTConfig.frequency_stride",description:`<strong>frequency_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Frequency stride to use when patchifying the spectrograms.`,name:"frequency_stride"},{anchor:"transformers.ASTConfig.time_stride",description:`<strong>time_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Temporal stride to use when patchifying the spectrograms.`,name:"time_stride"},{anchor:"transformers.ASTConfig.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Temporal dimension of the spectrograms.`,name:"max_length"},{anchor:"transformers.ASTConfig.num_mel_bins",description:`<strong>num_mel_bins</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Frequency dimension of the spectrograms (number of Mel-frequency bins).`,name:"num_mel_bins"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py#L31"}}),Z=new Tt({props:{anchor:"transformers.ASTConfig.example",$$slots:{default:[Lt]},$$scope:{ctx:j}}}),re=new B({props:{title:"ASTFeatureExtractor",local:"transformers.ASTFeatureExtractor",headingTag:"h2"}}),le=new ge({props:{name:"class transformers.ASTFeatureExtractor",anchor:"transformers.ASTFeatureExtractor",parameters:[{name:"feature_size",val:" = 1"},{name:"sampling_rate",val:" = 16000"},{name:"num_mel_bins",val:" = 128"},{name:"max_length",val:" = 1024"},{name:"padding_value",val:" = 0.0"},{name:"do_normalize",val:" = True"},{name:"mean",val:" = -4.2677393"},{name:"std",val:" = 4.5689974"},{name:"return_attention_mask",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ASTFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.ASTFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 16000) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).`,name:"sampling_rate"},{anchor:"transformers.ASTFeatureExtractor.num_mel_bins",description:`<strong>num_mel_bins</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Number of Mel-frequency bins.`,name:"num_mel_bins"},{anchor:"transformers.ASTFeatureExtractor.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Maximum length to which to pad/truncate the extracted features.`,name:"max_length"},{anchor:"transformers.ASTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the log-Mel features using <code>mean</code> and <code>std</code>.`,name:"do_normalize"},{anchor:"transformers.ASTFeatureExtractor.mean",description:`<strong>mean</strong> (<code>float</code>, <em>optional</em>, defaults to -4.2677393) &#x2014;
The mean value used to normalize the log-Mel features. Uses the AudioSet mean by default.`,name:"mean"},{anchor:"transformers.ASTFeatureExtractor.std",description:`<strong>std</strong> (<code>float</code>, <em>optional</em>, defaults to 4.5689974) &#x2014;
The standard deviation value used to normalize the log-Mel features. Uses the AudioSet standard deviation
by default.`,name:"std"},{anchor:"transformers.ASTFeatureExtractor.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__"><strong>call</strong>()</a> should return <code>attention_mask</code>.`,name:"return_attention_mask"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L39"}}),ie=new ge({props:{name:"__call__",anchor:"transformers.ASTFeatureExtractor.__call__",parameters:[{name:"raw_speech",val:": Union"},{name:"sampling_rate",val:": Optional = None"},{name:"return_tensors",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ASTFeatureExtractor.__call__.raw_speech",description:`<strong>raw_speech</strong> (<code>np.ndarray</code>, <code>List[float]</code>, <code>List[np.ndarray]</code>, <code>List[List[float]]</code>) &#x2014;
The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float
values, a list of numpy arrays or a list of list of float values. Must be mono channel audio, not
stereo, i.e. single float per timestep.`,name:"raw_speech"},{anchor:"transformers.ASTFeatureExtractor.__call__.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The sampling rate at which the <code>raw_speech</code> input was sampled. It is strongly recommended to pass
<code>sampling_rate</code> at the forward call to prevent silent errors.`,name:"sampling_rate"},{anchor:"transformers.ASTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/ja/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L161"}}),de=new B({props:{title:"ASTModel",local:"transformers.ASTModel",headingTag:"h2"}}),ce=new ge({props:{name:"class transformers.ASTModel",anchor:"transformers.ASTModel",parameters:[{name:"config",val:": ASTConfig"}],parametersDescription:[{anchor:"transformers.ASTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L430"}}),me=new ge({props:{name:"forward",anchor:"transformers.ASTModel.forward",parameters:[{name:"input_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ASTModel.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, max_length, num_mel_bins)</code>) &#x2014;
Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by
loading a <code>.flac</code> or <code>.wav</code> audio file into an array of type <code>List[float]</code> or a <code>numpy.ndarray</code>, <em>e.g.</em> via
the soundfile library (<code>pip install soundfile</code>). To prepare the array into <code>input_features</code>, the
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a> should be used for extracting the mel features, padding and conversion into a
tensor of type <code>torch.FloatTensor</code>. See <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__"><strong>call</strong>()</a>`,name:"input_values"},{anchor:"transformers.ASTModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ASTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ASTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ASTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L458",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig"
>ASTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),N=new Et({props:{$$slots:{default:[Pt]},$$scope:{ctx:j}}}),I=new Tt({props:{anchor:"transformers.ASTModel.forward.example",$$slots:{default:[Dt]},$$scope:{ctx:j}}}),pe=new B({props:{title:"ASTForAudioClassification",local:"transformers.ASTForAudioClassification",headingTag:"h2"}}),ue=new ge({props:{name:"class transformers.ASTForAudioClassification",anchor:"transformers.ASTForAudioClassification",parameters:[{name:"config",val:": ASTConfig"}],parametersDescription:[{anchor:"transformers.ASTForAudioClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L527"}}),fe=new ge({props:{name:"forward",anchor:"transformers.ASTForAudioClassification.forward",parameters:[{name:"input_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ASTForAudioClassification.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, max_length, num_mel_bins)</code>) &#x2014;
Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by
loading a <code>.flac</code> or <code>.wav</code> audio file into an array of type <code>List[float]</code> or a <code>numpy.ndarray</code>, <em>e.g.</em> via
the soundfile library (<code>pip install soundfile</code>). To prepare the array into <code>input_features</code>, the
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a> should be used for extracting the mel features, padding and conversion into a
tensor of type <code>torch.FloatTensor</code>. See <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__"><strong>call</strong>()</a>`,name:"input_values"},{anchor:"transformers.ASTForAudioClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ASTForAudioClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ASTForAudioClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ASTForAudioClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ASTForAudioClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the audio classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L547",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig"
>ASTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),E=new Et({props:{$$slots:{default:[Kt]},$$scope:{ctx:j}}}),q=new Tt({props:{anchor:"transformers.ASTForAudioClassification.forward.example",$$slots:{default:[Ot]},$$scope:{ctx:j}}}),{c(){o=i("meta"),M=r(),m=i("p"),p=r(),f(y.$$.fragment),n=r(),f(w.$$.fragment),Je=r(),Y=i("p"),Y.innerHTML=yt,Fe=r(),Q=i("p"),Q.textContent=Mt,Ue=r(),H=i("p"),H.innerHTML=wt,ke=r(),z=i("img"),Ge=r(),L=i("small"),L.innerHTML=$t,We=r(),P=i("p"),P.innerHTML=xt,ze=r(),f(D.$$.fragment),Ze=r(),K=i("ul"),K.innerHTML=Ct,Ve=r(),f(O.$$.fragment),Ne=r(),ee=i("p"),ee.textContent=jt,Ie=r(),f(te.$$.fragment),Ee=r(),ae=i("ul"),ae.innerHTML=At,qe=r(),oe=i("p"),oe.textContent=St,Re=r(),f(ne.$$.fragment),Xe=r(),$=i("div"),f(se.$$.fragment),Oe=r(),_e=i("p"),_e.innerHTML=Jt,et=r(),Te=i("p"),Te.innerHTML=Ft,tt=r(),f(Z.$$.fragment),Be=r(),f(re.$$.fragment),Ye=r(),v=i("div"),f(le.$$.fragment),at=r(),be=i("p"),be.textContent=Ut,ot=r(),ye=i("p"),ye.innerHTML=kt,nt=r(),Me=i("p"),Me.textContent=Gt,st=r(),V=i("div"),f(ie.$$.fragment),rt=r(),we=i("p"),we.textContent=Wt,Qe=r(),f(de.$$.fragment),He=r(),F=i("div"),f(ce.$$.fragment),lt=r(),ve=i("p"),ve.innerHTML=zt,it=r(),A=i("div"),f(me.$$.fragment),dt=r(),$e=i("p"),$e.innerHTML=Zt,ct=r(),f(N.$$.fragment),mt=r(),f(I.$$.fragment),Le=r(),f(pe.$$.fragment),Pe=r(),x=i("div"),f(ue.$$.fragment),pt=r(),xe=i("p"),xe.textContent=Vt,ut=r(),Ce=i("p"),Ce.innerHTML=Nt,ft=r(),S=i("div"),f(fe.$$.fragment),ht=r(),je=i("p"),je.innerHTML=It,gt=r(),f(E.$$.fragment),_t=r(),f(q.$$.fragment),De=r(),Ae=i("p"),this.h()},l(e){const t=Qt("svelte-u9bgzb",document.head);o=d(t,"META",{name:!0,content:!0}),t.forEach(a),M=l(e),m=d(e,"P",{}),W(m).forEach(a),p=l(e),h(y.$$.fragment,e),n=l(e),h(w.$$.fragment,e),Je=l(e),Y=d(e,"P",{"data-svelte-h":!0}),u(Y)!=="svelte-1ez9eok"&&(Y.innerHTML=yt),Fe=l(e),Q=d(e,"P",{"data-svelte-h":!0}),u(Q)!=="svelte-1pvwld5"&&(Q.textContent=Mt),Ue=l(e),H=d(e,"P",{"data-svelte-h":!0}),u(H)!=="svelte-579vxj"&&(H.innerHTML=wt),ke=l(e),z=d(e,"IMG",{src:!0,alt:!0,width:!0}),Ge=l(e),L=d(e,"SMALL",{"data-svelte-h":!0}),u(L)!=="svelte-1r69l05"&&(L.innerHTML=$t),We=l(e),P=d(e,"P",{"data-svelte-h":!0}),u(P)!=="svelte-12otjfn"&&(P.innerHTML=xt),ze=l(e),h(D.$$.fragment,e),Ze=l(e),K=d(e,"UL",{"data-svelte-h":!0}),u(K)!=="svelte-u6m1v2"&&(K.innerHTML=Ct),Ve=l(e),h(O.$$.fragment,e),Ne=l(e),ee=d(e,"P",{"data-svelte-h":!0}),u(ee)!=="svelte-lnqsp9"&&(ee.textContent=jt),Ie=l(e),h(te.$$.fragment,e),Ee=l(e),ae=d(e,"UL",{"data-svelte-h":!0}),u(ae)!=="svelte-1y1nfy1"&&(ae.innerHTML=At),qe=l(e),oe=d(e,"P",{"data-svelte-h":!0}),u(oe)!=="svelte-1ca3035"&&(oe.textContent=St),Re=l(e),h(ne.$$.fragment,e),Xe=l(e),$=d(e,"DIV",{class:!0});var U=W($);h(se.$$.fragment,U),Oe=l(U),_e=d(U,"P",{"data-svelte-h":!0}),u(_e)!=="svelte-1ppuncx"&&(_e.innerHTML=Jt),et=l(U),Te=d(U,"P",{"data-svelte-h":!0}),u(Te)!=="svelte-1s6wgpv"&&(Te.innerHTML=Ft),tt=l(U),h(Z.$$.fragment,U),U.forEach(a),Be=l(e),h(re.$$.fragment,e),Ye=l(e),v=d(e,"DIV",{class:!0});var C=W(v);h(le.$$.fragment,C),at=l(C),be=d(C,"P",{"data-svelte-h":!0}),u(be)!=="svelte-1vcyh3y"&&(be.textContent=Ut),ot=l(C),ye=d(C,"P",{"data-svelte-h":!0}),u(ye)!=="svelte-fxahc3"&&(ye.innerHTML=kt),nt=l(C),Me=d(C,"P",{"data-svelte-h":!0}),u(Me)!=="svelte-47ck0t"&&(Me.textContent=Gt),st=l(C),V=d(C,"DIV",{class:!0});var he=W(V);h(ie.$$.fragment,he),rt=l(he),we=d(he,"P",{"data-svelte-h":!0}),u(we)!=="svelte-1a6wgfx"&&(we.textContent=Wt),he.forEach(a),C.forEach(a),Qe=l(e),h(de.$$.fragment,e),He=l(e),F=d(e,"DIV",{class:!0});var G=W(F);h(ce.$$.fragment,G),lt=l(G),ve=d(G,"P",{"data-svelte-h":!0}),u(ve)!=="svelte-1eae68l"&&(ve.innerHTML=zt),it=l(G),A=d(G,"DIV",{class:!0});var k=W(A);h(me.$$.fragment,k),dt=l(k),$e=d(k,"P",{"data-svelte-h":!0}),u($e)!=="svelte-1dqlmj3"&&($e.innerHTML=Zt),ct=l(k),h(N.$$.fragment,k),mt=l(k),h(I.$$.fragment,k),k.forEach(a),G.forEach(a),Le=l(e),h(pe.$$.fragment,e),Pe=l(e),x=d(e,"DIV",{class:!0});var R=W(x);h(ue.$$.fragment,R),pt=l(R),xe=d(R,"P",{"data-svelte-h":!0}),u(xe)!=="svelte-1b8t75l"&&(xe.textContent=Vt),ut=l(R),Ce=d(R,"P",{"data-svelte-h":!0}),u(Ce)!=="svelte-1gjh92c"&&(Ce.innerHTML=Nt),ft=l(R),S=d(R,"DIV",{class:!0});var X=W(S);h(fe.$$.fragment,X),ht=l(X),je=d(X,"P",{"data-svelte-h":!0}),u(je)!=="svelte-hqd4qx"&&(je.innerHTML=It),gt=l(X),h(E.$$.fragment,X),_t=l(X),h(q.$$.fragment,X),X.forEach(a),R.forEach(a),De=l(e),Ae=d(e,"P",{}),W(Ae).forEach(a),this.h()},h(){J(o,"name","hf:doc:metadata"),J(o,"content",ta),Rt(z.src,vt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/audio_spectogram_transformer_architecture.png")||J(z,"src",vt),J(z,"alt","drawing"),J(z,"width","600"),J($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){c(document.head,o),s(e,M,t),s(e,m,t),s(e,p,t),g(y,e,t),s(e,n,t),g(w,e,t),s(e,Je,t),s(e,Y,t),s(e,Fe,t),s(e,Q,t),s(e,Ue,t),s(e,H,t),s(e,ke,t),s(e,z,t),s(e,Ge,t),s(e,L,t),s(e,We,t),s(e,P,t),s(e,ze,t),g(D,e,t),s(e,Ze,t),s(e,K,t),s(e,Ve,t),g(O,e,t),s(e,Ne,t),s(e,ee,t),s(e,Ie,t),g(te,e,t),s(e,Ee,t),s(e,ae,t),s(e,qe,t),s(e,oe,t),s(e,Re,t),g(ne,e,t),s(e,Xe,t),s(e,$,t),g(se,$,null),c($,Oe),c($,_e),c($,et),c($,Te),c($,tt),g(Z,$,null),s(e,Be,t),g(re,e,t),s(e,Ye,t),s(e,v,t),g(le,v,null),c(v,at),c(v,be),c(v,ot),c(v,ye),c(v,nt),c(v,Me),c(v,st),c(v,V),g(ie,V,null),c(V,rt),c(V,we),s(e,Qe,t),g(de,e,t),s(e,He,t),s(e,F,t),g(ce,F,null),c(F,lt),c(F,ve),c(F,it),c(F,A),g(me,A,null),c(A,dt),c(A,$e),c(A,ct),g(N,A,null),c(A,mt),g(I,A,null),s(e,Le,t),g(pe,e,t),s(e,Pe,t),s(e,x,t),g(ue,x,null),c(x,pt),c(x,xe),c(x,ut),c(x,Ce),c(x,ft),c(x,S),g(fe,S,null),c(S,ht),c(S,je),c(S,gt),g(E,S,null),c(S,_t),g(q,S,null),s(e,De,t),s(e,Ae,t),Ke=!0},p(e,[t]){const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),Z.$set(U);const C={};t&2&&(C.$$scope={dirty:t,ctx:e}),N.$set(C);const he={};t&2&&(he.$$scope={dirty:t,ctx:e}),I.$set(he);const G={};t&2&&(G.$$scope={dirty:t,ctx:e}),E.$set(G);const k={};t&2&&(k.$$scope={dirty:t,ctx:e}),q.$set(k)},i(e){Ke||(_(y.$$.fragment,e),_(w.$$.fragment,e),_(D.$$.fragment,e),_(O.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(Z.$$.fragment,e),_(re.$$.fragment,e),_(le.$$.fragment,e),_(ie.$$.fragment,e),_(de.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(N.$$.fragment,e),_(I.$$.fragment,e),_(pe.$$.fragment,e),_(ue.$$.fragment,e),_(fe.$$.fragment,e),_(E.$$.fragment,e),_(q.$$.fragment,e),Ke=!0)},o(e){T(y.$$.fragment,e),T(w.$$.fragment,e),T(D.$$.fragment,e),T(O.$$.fragment,e),T(te.$$.fragment,e),T(ne.$$.fragment,e),T(se.$$.fragment,e),T(Z.$$.fragment,e),T(re.$$.fragment,e),T(le.$$.fragment,e),T(ie.$$.fragment,e),T(de.$$.fragment,e),T(ce.$$.fragment,e),T(me.$$.fragment,e),T(N.$$.fragment,e),T(I.$$.fragment,e),T(pe.$$.fragment,e),T(ue.$$.fragment,e),T(fe.$$.fragment,e),T(E.$$.fragment,e),T(q.$$.fragment,e),Ke=!1},d(e){e&&(a(M),a(m),a(p),a(n),a(Je),a(Y),a(Fe),a(Q),a(Ue),a(H),a(ke),a(z),a(Ge),a(L),a(We),a(P),a(ze),a(Ze),a(K),a(Ve),a(Ne),a(ee),a(Ie),a(Ee),a(ae),a(qe),a(oe),a(Re),a(Xe),a($),a(Be),a(Ye),a(v),a(Qe),a(He),a(F),a(Le),a(Pe),a(x),a(De),a(Ae)),a(o),b(y,e),b(w,e),b(D,e),b(O,e),b(te,e),b(ne,e),b(se),b(Z),b(re,e),b(le),b(ie),b(de,e),b(ce),b(me),b(N),b(I),b(pe,e),b(ue),b(fe),b(E),b(q)}}}const ta='{"title":"Audio Spectrogram Transformer","local":"audio-spectrogram-transformer","sections":[{"title":"概要","local":"概要","sections":[],"depth":2},{"title":"使用上のヒント","local":"使用上のヒント","sections":[],"depth":2},{"title":"参考資料","local":"参考資料","sections":[],"depth":2},{"title":"ASTConfig","local":"transformers.ASTConfig","sections":[],"depth":2},{"title":"ASTFeatureExtractor","local":"transformers.ASTFeatureExtractor","sections":[],"depth":2},{"title":"ASTModel","local":"transformers.ASTModel","sections":[],"depth":2},{"title":"ASTForAudioClassification","local":"transformers.ASTForAudioClassification","sections":[],"depth":2}],"depth":1}';function aa(j){return Xt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ma extends Bt{constructor(o){super(),Yt(this,o,aa,ea,qt,{})}}export{ma as component};
