import{s as qt,f as Rt,o as Xt,n as Se}from"../chunks/scheduler.9bc65507.js";import{S as Bt,i as Yt,g as i,s as r,r as f,A as Qt,h as d,f as a,c as l,j as W,u as h,x as u,k as J,y as c,a as s,v as g,d as _,t as T,w as b}from"../chunks/index.707bf1b6.js";import{T as Et}from"../chunks/Tip.c2ecdbf4.js";import{D as ge}from"../chunks/Docstring.17db21ae.js";import{C as bt}from"../chunks/CodeBlock.54a9f38d.js";import{E as Tt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as Ht}from"../chunks/PipelineTag.44585822.js";import{H as B}from"../chunks/Heading.342b1fa6.js";function Lt(j){let o,M="Example:",m,p,y;return p=new bt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFTVENvbmZpZyUyQyUyMEFTVE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEFTVCUyME1JVCUyRmFzdC1maW5ldHVuZWQtYXVkaW9zZXQtMTAtMTAtMC40NTkzJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMEFTVENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBNSVQlMkZhc3QtZmluZXR1bmVkLWF1ZGlvc2V0LTEwLTEwLTAuNDU5MyUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQVNUTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ASTConfig, ASTModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AST MIT/ast-finetuned-audioset-10-10-0.4593 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ASTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the MIT/ast-finetuned-audioset-10-10-0.4593 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ASTModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=i("p"),o.textContent=M,m=r(),f(p.$$.fragment)},l(n){o=d(n,"P",{"data-svelte-h":!0}),u(o)!=="svelte-11lpom8"&&(o.textContent=M),m=l(n),h(p.$$.fragment,n)},m(n,w){s(n,o,w),s(n,m,w),g(p,n,w),y=!0},p:Se,i(n){y||(_(p.$$.fragment,n),y=!0)},o(n){T(p.$$.fragment,n),y=!1},d(n){n&&(a(o),a(m)),b(p,n)}}}function Pt(j){let o,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=i("p"),o.innerHTML=M},l(m){o=d(m,"P",{"data-svelte-h":!0}),u(o)!=="svelte-fincs2"&&(o.innerHTML=M)},m(m,p){s(m,o,p)},p:Se,d(m){m&&a(o)}}}function Dt(j){let o,M="Example:",m,p,y;return p=new bt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBU1RNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJoZi1pbnRlcm5hbC10ZXN0aW5nJTJGbGlicmlzcGVlY2hfYXNyX2RlbW8lMjIlMkMlMjAlMjJjbGVhbiUyMiUyQyUyMHNwbGl0JTNEJTIydmFsaWRhdGlvbiUyMiklMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5zb3J0KCUyMmlkJTIyKSUwQXNhbXBsaW5nX3JhdGUlMjAlM0QlMjBkYXRhc2V0LmZlYXR1cmVzJTVCJTIyYXVkaW8lMjIlNUQuc2FtcGxpbmdfcmF0ZSUwQSUwQXByb2Nlc3NvciUyMCUzRCUyMEF1dG9Qcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMk1JVCUyRmFzdC1maW5ldHVuZWQtYXVkaW9zZXQtMTAtMTAtMC40NTkzJTIyKSUwQW1vZGVsJTIwJTNEJTIwQVNUTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMk1JVCUyRmFzdC1maW5ldHVuZWQtYXVkaW9zZXQtMTAtMTAtMC40NTkzJTIyKSUwQSUwQSUyMyUyMGF1ZGlvJTIwZmlsZSUyMGlzJTIwZGVjb2RlZCUyMG9uJTIwdGhlJTIwZmx5JTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGRhdGFzZXQlNUIwJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUIlMjJhcnJheSUyMiU1RCUyQyUyMHNhbXBsaW5nX3JhdGUlM0RzYW1wbGluZ19yYXRlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, ASTModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;MIT/ast-finetuned-audioset-10-10-0.4593&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ASTModel.from_pretrained(<span class="hljs-string">&quot;MIT/ast-finetuned-audioset-10-10-0.4593&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">1214</span>, <span class="hljs-number">768</span>]`,wrap:!1}}),{c(){o=i("p"),o.textContent=M,m=r(),f(p.$$.fragment)},l(n){o=d(n,"P",{"data-svelte-h":!0}),u(o)!=="svelte-11lpom8"&&(o.textContent=M),m=l(n),h(p.$$.fragment,n)},m(n,w){s(n,o,w),s(n,m,w),g(p,n,w),y=!0},p:Se,i(n){y||(_(p.$$.fragment,n),y=!0)},o(n){T(p.$$.fragment,n),y=!1},d(n){n&&(a(o),a(m)),b(p,n)}}}function Kt(j){let o,M=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=i("p"),o.innerHTML=M},l(m){o=d(m,"P",{"data-svelte-h":!0}),u(o)!=="svelte-fincs2"&&(o.innerHTML=M)},m(m,p){s(m,o,p)},p:Se,d(m){m&&a(o)}}}function Ot(j){let o,M="Example:",m,p,y;return p=new bt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTJDJTIwQVNURm9yQXVkaW9DbGFzc2lmaWNhdGlvbiUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQWltcG9ydCUyMHRvcmNoJTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJoZi1pbnRlcm5hbC10ZXN0aW5nJTJGbGlicmlzcGVlY2hfYXNyX2RlbW8lMjIlMkMlMjAlMjJjbGVhbiUyMiUyQyUyMHNwbGl0JTNEJTIydmFsaWRhdGlvbiUyMiklMEFkYXRhc2V0JTIwJTNEJTIwZGF0YXNldC5zb3J0KCUyMmlkJTIyKSUwQXNhbXBsaW5nX3JhdGUlMjAlM0QlMjBkYXRhc2V0LmZlYXR1cmVzJTVCJTIyYXVkaW8lMjIlNUQuc2FtcGxpbmdfcmF0ZSUwQSUwQWZlYXR1cmVfZXh0cmFjdG9yJTIwJTNEJTIwQXV0b0ZlYXR1cmVFeHRyYWN0b3IuZnJvbV9wcmV0cmFpbmVkKCUyMk1JVCUyRmFzdC1maW5ldHVuZWQtYXVkaW9zZXQtMTAtMTAtMC40NTkzJTIyKSUwQW1vZGVsJTIwJTNEJTIwQVNURm9yQXVkaW9DbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyTUlUJTJGYXN0LWZpbmV0dW5lZC1hdWRpb3NldC0xMC0xMC0wLjQ1OTMlMjIpJTBBJTBBJTIzJTIwYXVkaW8lMjBmaWxlJTIwaXMlMjBkZWNvZGVkJTIwb24lMjB0aGUlMjBmbHklMEFpbnB1dHMlMjAlM0QlMjBmZWF0dXJlX2V4dHJhY3RvcihkYXRhc2V0JTVCMCU1RCU1QiUyMmF1ZGlvJTIyJTVEJTVCJTIyYXJyYXklMjIlNUQlMkMlMjBzYW1wbGluZ19yYXRlJTNEc2FtcGxpbmdfcmF0ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEFwcmVkaWN0ZWRfY2xhc3NfaWRzJTIwJTNEJTIwdG9yY2guYXJnbWF4KGxvZ2l0cyUyQyUyMGRpbSUzRC0xKS5pdGVtKCklMEFwcmVkaWN0ZWRfbGFiZWwlMjAlM0QlMjBtb2RlbC5jb25maWcuaWQybGFiZWwlNUJwcmVkaWN0ZWRfY2xhc3NfaWRzJTVEJTBBcHJlZGljdGVkX2xhYmVsJTBBJTBBJTIzJTIwY29tcHV0ZSUyMGxvc3MlMjAtJTIwdGFyZ2V0X2xhYmVsJTIwaXMlMjBlLmcuJTIwJTIyZG93biUyMiUwQXRhcmdldF9sYWJlbCUyMCUzRCUyMG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QjAlNUQlMEFpbnB1dHMlNUIlMjJsYWJlbHMlMjIlNUQlMjAlM0QlMjB0b3JjaC50ZW5zb3IoJTVCbW9kZWwuY29uZmlnLmxhYmVsMmlkJTVCdGFyZ2V0X2xhYmVsJTVEJTVEKSUwQWxvc3MlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9zcyUwQXJvdW5kKGxvc3MuaXRlbSgpJTJDJTIwMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor, ASTForAudioClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;hf-internal-testing/librispeech_asr_demo&quot;</span>, <span class="hljs-string">&quot;clean&quot;</span>, split=<span class="hljs-string">&quot;validation&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.sort(<span class="hljs-string">&quot;id&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;MIT/ast-finetuned-audioset-10-10-0.4593&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ASTForAudioClassification.from_pretrained(<span class="hljs-string">&quot;MIT/ast-finetuned-audioset-10-10-0.4593&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># audio file is decoded on the fly</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_ids = torch.argmax(logits, dim=-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = model.config.id2label[predicted_class_ids]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label
<span class="hljs-string">&#x27;Speech&#x27;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># compute loss - target_label is e.g. &quot;down&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>target_label = model.config.id2label[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs[<span class="hljs-string">&quot;labels&quot;</span>] = torch.tensor([model.config.label2id[target_label]])
<span class="hljs-meta">&gt;&gt;&gt; </span>loss = model(**inputs).loss
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">round</span>(loss.item(), <span class="hljs-number">2</span>)
<span class="hljs-number">0.17</span>`,wrap:!1}}),{c(){o=i("p"),o.textContent=M,m=r(),f(p.$$.fragment)},l(n){o=d(n,"P",{"data-svelte-h":!0}),u(o)!=="svelte-11lpom8"&&(o.textContent=M),m=l(n),h(p.$$.fragment,n)},m(n,w){s(n,o,w),s(n,m,w),g(p,n,w),y=!0},p:Se,i(n){y||(_(p.$$.fragment,n),y=!0)},o(n){T(p.$$.fragment,n),y=!1},d(n){n&&(a(o),a(m)),b(p,n)}}}function ea(j){let o,M,m,p,y,n,w,Je,Y,yt='Audio Spectrogram Transformerãƒ¢ãƒ‡ãƒ«ã¯ã€<a href="https://arxiv.org/abs/2104.01778" rel="nofollow">AST: Audio Spectrogram Transformer</a>ã¨ã„ã†è«–æ–‡ã§Yuan Gongã€Yu-An Chungã€James Glassã«ã‚ˆã£ã¦ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã¯ã€éŸ³å£°ã‚’ç”»åƒï¼ˆã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ï¼‰ã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã€éŸ³å£°ã«<a href="vit">Vision Transformer</a>ã‚’é©ç”¨ã—ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯éŸ³å£°åˆ†é¡ã«ãŠã„ã¦æœ€å…ˆç«¯ã®çµæœã‚’å¾—ã¦ã„ã¾ã™ã€‚',Fe,Q,Mt="è«–æ–‡ã®è¦æ—¨ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š",Ue,H,wt="<em>éå»10å¹´é–“ã§ã€ç•³ã¿è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆCNNï¼‰ã¯ã€éŸ³å£°ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã‹ã‚‰å¯¾å¿œã™ã‚‹ãƒ©ãƒ™ãƒ«ã¸ã®ç›´æ¥çš„ãªãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®éŸ³å£°åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®ä¸»è¦ãªæ§‹æˆè¦ç´ ã¨ã—ã¦åºƒãæ¡ç”¨ã•ã‚Œã¦ãã¾ã—ãŸã€‚é•·è·é›¢ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚ˆã‚Šè‰¯ãæ‰ãˆã‚‹ãŸã‚ã€æœ€è¿‘ã®å‚¾å‘ã¨ã—ã¦ã€CNNã®ä¸Šã«ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹ã‚’è¿½åŠ ã—ã€CNN-ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã‚’å½¢æˆã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€CNNã¸ã®ä¾å­˜ãŒå¿…è¦ã‹ã©ã†ã‹ã€ãã—ã¦ç´”ç²‹ã«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«åŸºã¥ããƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã ã‘ã§éŸ³å£°åˆ†é¡ã«ãŠã„ã¦è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã‚‹ã‹ã©ã†ã‹ã¯æ˜ã‚‰ã‹ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æœ¬è«–æ–‡ã§ã¯ã€ã“ã‚Œã‚‰ã®å•ã„ã«ç­”ãˆã‚‹ãŸã‚ã€éŸ³å£°åˆ†é¡ç”¨ã§ã¯æœ€åˆã®ç•³ã¿è¾¼ã¿ãªã—ã§ç´”ç²‹ã«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Audio Spectrogram Transformerï¼ˆASTï¼‰ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚æˆ‘ã€…ã¯ASTã‚’æ§˜ã€…ãªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªåˆ†é¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§è©•ä¾¡ã—ã€AudioSetã§0.485 mAPã€ESC-50ã§95.6%ã®æ­£è§£ç‡ã€Speech Commands V2ã§98.1%ã®æ­£è§£ç‡ã¨ã„ã†æ–°ãŸãªæœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã¾ã—ãŸã€‚</em>",ke,z,vt,Ge,L,$t='Audio Spectrogram Transformerã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚<a href="https://arxiv.org/abs/2104.01778">å…ƒè«–æ–‡</a>ã‚ˆã‚ŠæŠœç²‹ã€‚',We,P,xt=`ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯<a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a>ã‚ˆã‚Šæä¾›ã•ã‚Œã¾ã—ãŸã€‚
ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ã‚³ãƒ¼ãƒ‰ã¯<a href="https://github.com/YuanGongND/ast" rel="nofollow">ã“ã¡ã‚‰</a>ã§è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚`,ze,D,Ze,K,Ct='<li>ç‹¬è‡ªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Audio Spectrogram Transformerï¼ˆASTï¼‰ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã€å…¥åŠ›ã®æ­£è¦åŒ–ï¼ˆå…¥åŠ›ã®å¹³å‡ã‚’0ã€æ¨™æº–åå·®ã‚’0.5ã«ã™ã‚‹ã“ã¨ï¼‰å‡¦ç†ã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚<a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor">ASTFeatureExtractor</a>ã¯ã“ã‚Œã‚’å‡¦ç†ã—ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯AudioSetã®å¹³å‡ã¨æ¨™æº–åå·®ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚è‘—è€…ãŒä¸‹æµã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆã‚’ã©ã®ã‚ˆã†ã«è¨ˆç®—ã—ã¦ã„ã‚‹ã‹ã¯ã€<a href="https://github.com/YuanGongND/ast/blob/master/src/get_norm_stats.py" rel="nofollow"><code>ast/src/get_norm_stats.py</code></a>ã§ç¢ºèªã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚</li> <li>ASTã¯ä½ã„å­¦ç¿’ç‡ãŒå¿…è¦ã§ã‚ã‚Š è‘—è€…ã¯<a href="https://arxiv.org/abs/2102.01243" rel="nofollow">PSLAè«–æ–‡</a>ã§ææ¡ˆã•ã‚ŒãŸCNNãƒ¢ãƒ‡ãƒ«ã«æ¯”ã¹ã¦10å€å°ã•ã„å­¦ç¿’ç‡ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ï¼‰ã€ç´ æ—©ãåæŸã™ã‚‹ãŸã‚ã€ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸå­¦ç¿’ç‡ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’æ¢ã™ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚</li>',Ve,O,Ne,ee,jt="Audio Spectrogram Transformerã®ä½¿ç”¨ã‚’é–‹å§‹ã™ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ã®Hugging FaceãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ï¼ˆğŸŒã§ç¤ºã•ã‚Œã¦ã„ã‚‹ï¼‰ã®å‚è€ƒè³‡æ–™ã®ä¸€è¦§ã§ã™ã€‚",Ie,te,Ee,ae,At='<li>ASTã‚’ç”¨ã„ãŸéŸ³å£°åˆ†é¡ã®æ¨è«–ã‚’èª¬æ˜ã™ã‚‹ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯<a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/AST" rel="nofollow">ã“ã¡ã‚‰</a>ã§è¦‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚</li> <li><a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification">ASTForAudioClassification</a>ã¯ã€ã“ã®<a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/audio-classification" rel="nofollow">ä¾‹ç¤ºã‚¹ã‚¯ãƒªãƒ—ãƒˆ</a>ã¨<a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb" rel="nofollow">ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯</a>ã«ã‚ˆã£ã¦ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚</li> <li>ã“ã¡ã‚‰ã‚‚å‚ç…§ï¼š<a href="../tasks/audio_classification">éŸ³å£°åˆ†é¡ã‚¿ã‚¹ã‚¯</a>ã€‚</li>',qe,oe,St="ã“ã“ã«å‚è€ƒè³‡æ–™ã‚’æå‡ºã—ãŸã„å ´åˆã¯ã€æ°—å…¼ã­ãªãPull Requestã‚’é–‹ã„ã¦ãã ã•ã„ã€‚ç§ãŸã¡ã¯ãã‚Œã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã„ãŸã—ã¾ã™ï¼å‚è€ƒè³‡æ–™ã¯ã€æ—¢å­˜ã®ã‚‚ã®ã‚’è¤‡è£½ã™ã‚‹ã®ã§ã¯ãªãã€ä½•ã‹æ–°ã—ã„ã“ã¨ã‚’ç¤ºã™ã“ã¨ãŒç†æƒ³çš„ã§ã™ã€‚",Re,ne,Xe,$,se,Oe,_e,Jt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTModel">ASTModel</a>. It is used to instantiate an AST
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the AST
<a href="https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593" rel="nofollow">MIT/ast-finetuned-audioset-10-10-0.4593</a>
architecture.`,et,Te,Ft=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,tt,Z,Be,re,Ye,v,le,at,be,Ut="Constructs a Audio Spectrogram Transformer (AST) feature extractor.",ot,ye,kt=`This feature extractor inherits from <a href="/docs/transformers/main/ja/main_classes/feature_extractor#transformers.SequenceFeatureExtractor">SequenceFeatureExtractor</a> which contains
most of the main methods. Users should refer to this superclass for more information regarding those methods.`,nt,Me,Gt=`This class extracts mel-filter bank features from raw speech using TorchAudio if installed or using numpy
otherwise, pads/truncates them to a fixed length and normalizes them using a mean and standard deviation.`,st,V,ie,rt,we,Wt="Main method to featurize and prepare for the model one or several sequence(s).",Qe,de,He,F,ce,lt,ve,zt=`The bare AST Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,it,A,me,dt,$e,Zt='The <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTModel">ASTModel</a> forward method, overrides the <code>__call__</code> special method.',ct,N,mt,I,Le,pe,Pe,x,ue,pt,xe,Vt=`Audio Spectrogram Transformer model with an audio classification head on top (a linear layer on top of the pooled
output) e.g. for datasets like AudioSet, Speech Commands v2.`,ut,Ce,Nt=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,ft,S,fe,ht,je,It='The <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTForAudioClassification">ASTForAudioClassification</a> forward method, overrides the <code>__call__</code> special method.',gt,E,_t,q,De,Ae,Ke;return y=new B({props:{title:"Audio Spectrogram Transformer",local:"audio-spectrogram-transformer",headingTag:"h1"}}),w=new B({props:{title:"æ¦‚è¦",local:"æ¦‚è¦",headingTag:"h2"}}),D=new B({props:{title:"ä½¿ç”¨ä¸Šã®ãƒ’ãƒ³ãƒˆ",local:"ä½¿ç”¨ä¸Šã®ãƒ’ãƒ³ãƒˆ",headingTag:"h2"}}),O=new B({props:{title:"å‚è€ƒè³‡æ–™",local:"å‚è€ƒè³‡æ–™",headingTag:"h2"}}),te=new Ht({props:{pipeline:"audio-classification"}}),ne=new B({props:{title:"ASTConfig",local:"transformers.ASTConfig",headingTag:"h2"}}),se=new ge({props:{name:"class transformers.ASTConfig",anchor:"transformers.ASTConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"patch_size",val:" = 16"},{name:"qkv_bias",val:" = True"},{name:"frequency_stride",val:" = 10"},{name:"time_stride",val:" = 10"},{name:"max_length",val:" = 1024"},{name:"num_mel_bins",val:" = 128"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ASTConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.ASTConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.ASTConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.ASTConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.ASTConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.ASTConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.ASTConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.ASTConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ASTConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.ASTConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.ASTConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.ASTConfig.frequency_stride",description:`<strong>frequency_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Frequency stride to use when patchifying the spectrograms.`,name:"frequency_stride"},{anchor:"transformers.ASTConfig.time_stride",description:`<strong>time_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 10) &#x2014;
Temporal stride to use when patchifying the spectrograms.`,name:"time_stride"},{anchor:"transformers.ASTConfig.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Temporal dimension of the spectrograms.`,name:"max_length"},{anchor:"transformers.ASTConfig.num_mel_bins",description:`<strong>num_mel_bins</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Frequency dimension of the spectrograms (number of Mel-frequency bins).`,name:"num_mel_bins"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/configuration_audio_spectrogram_transformer.py#L31"}}),Z=new Tt({props:{anchor:"transformers.ASTConfig.example",$$slots:{default:[Lt]},$$scope:{ctx:j}}}),re=new B({props:{title:"ASTFeatureExtractor",local:"transformers.ASTFeatureExtractor",headingTag:"h2"}}),le=new ge({props:{name:"class transformers.ASTFeatureExtractor",anchor:"transformers.ASTFeatureExtractor",parameters:[{name:"feature_size",val:" = 1"},{name:"sampling_rate",val:" = 16000"},{name:"num_mel_bins",val:" = 128"},{name:"max_length",val:" = 1024"},{name:"padding_value",val:" = 0.0"},{name:"do_normalize",val:" = True"},{name:"mean",val:" = -4.2677393"},{name:"std",val:" = 4.5689974"},{name:"return_attention_mask",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ASTFeatureExtractor.feature_size",description:`<strong>feature_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The feature dimension of the extracted features.`,name:"feature_size"},{anchor:"transformers.ASTFeatureExtractor.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>, defaults to 16000) &#x2014;
The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).`,name:"sampling_rate"},{anchor:"transformers.ASTFeatureExtractor.num_mel_bins",description:`<strong>num_mel_bins</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
Number of Mel-frequency bins.`,name:"num_mel_bins"},{anchor:"transformers.ASTFeatureExtractor.max_length",description:`<strong>max_length</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Maximum length to which to pad/truncate the extracted features.`,name:"max_length"},{anchor:"transformers.ASTFeatureExtractor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not to normalize the log-Mel features using <code>mean</code> and <code>std</code>.`,name:"do_normalize"},{anchor:"transformers.ASTFeatureExtractor.mean",description:`<strong>mean</strong> (<code>float</code>, <em>optional</em>, defaults to -4.2677393) &#x2014;
The mean value used to normalize the log-Mel features. Uses the AudioSet mean by default.`,name:"mean"},{anchor:"transformers.ASTFeatureExtractor.std",description:`<strong>std</strong> (<code>float</code>, <em>optional</em>, defaults to 4.5689974) &#x2014;
The standard deviation value used to normalize the log-Mel features. Uses the AudioSet standard deviation
by default.`,name:"std"},{anchor:"transformers.ASTFeatureExtractor.return_attention_mask",description:`<strong>return_attention_mask</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__"><strong>call</strong>()</a> should return <code>attention_mask</code>.`,name:"return_attention_mask"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L39"}}),ie=new ge({props:{name:"__call__",anchor:"transformers.ASTFeatureExtractor.__call__",parameters:[{name:"raw_speech",val:": Union"},{name:"sampling_rate",val:": Optional = None"},{name:"return_tensors",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ASTFeatureExtractor.__call__.raw_speech",description:`<strong>raw_speech</strong> (<code>np.ndarray</code>, <code>List[float]</code>, <code>List[np.ndarray]</code>, <code>List[List[float]]</code>) &#x2014;
The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float
values, a list of numpy arrays or a list of list of float values. Must be mono channel audio, not
stereo, i.e. single float per timestep.`,name:"raw_speech"},{anchor:"transformers.ASTFeatureExtractor.__call__.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The sampling rate at which the <code>raw_speech</code> input was sampled. It is strongly recommended to pass
<code>sampling_rate</code> at the forward call to prevent silent errors.`,name:"sampling_rate"},{anchor:"transformers.ASTFeatureExtractor.__call__.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <a href="/docs/transformers/main/ja/internal/file_utils#transformers.TensorType">TensorType</a>, <em>optional</em>) &#x2014;
If set, will return tensors instead of list of python integers. Acceptable values are:</p>
<ul>
<li><code>&apos;tf&apos;</code>: Return TensorFlow <code>tf.constant</code> objects.</li>
<li><code>&apos;pt&apos;</code>: Return PyTorch <code>torch.Tensor</code> objects.</li>
<li><code>&apos;np&apos;</code>: Return Numpy <code>np.ndarray</code> objects.</li>
</ul>`,name:"return_tensors"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/feature_extraction_audio_spectrogram_transformer.py#L161"}}),de=new B({props:{title:"ASTModel",local:"transformers.ASTModel",headingTag:"h2"}}),ce=new ge({props:{name:"class transformers.ASTModel",anchor:"transformers.ASTModel",parameters:[{name:"config",val:": ASTConfig"}],parametersDescription:[{anchor:"transformers.ASTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L430"}}),me=new ge({props:{name:"forward",anchor:"transformers.ASTModel.forward",parameters:[{name:"input_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ASTModel.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, max_length, num_mel_bins)</code>) &#x2014;
Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by
loading a <code>.flac</code> or <code>.wav</code> audio file into an array of type <code>List[float]</code> or a <code>numpy.ndarray</code>, <em>e.g.</em> via
the soundfile library (<code>pip install soundfile</code>). To prepare the array into <code>input_features</code>, the
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a> should be used for extracting the mel features, padding and conversion into a
tensor of type <code>torch.FloatTensor</code>. See <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__"><strong>call</strong>()</a>`,name:"input_values"},{anchor:"transformers.ASTModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ASTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ASTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ASTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L458",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig"
>ASTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) â€” Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),N=new Et({props:{$$slots:{default:[Pt]},$$scope:{ctx:j}}}),I=new Tt({props:{anchor:"transformers.ASTModel.forward.example",$$slots:{default:[Dt]},$$scope:{ctx:j}}}),pe=new B({props:{title:"ASTForAudioClassification",local:"transformers.ASTForAudioClassification",headingTag:"h2"}}),ue=new ge({props:{name:"class transformers.ASTForAudioClassification",anchor:"transformers.ASTForAudioClassification",parameters:[{name:"config",val:": ASTConfig"}],parametersDescription:[{anchor:"transformers.ASTForAudioClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig">ASTConfig</a>) &#x2014;
Model configuration class with all the parameters of the model. Initializing with a config file does not
load the weights associated with the model, only the configuration. Check out the
<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L527"}}),fe=new ge({props:{name:"forward",anchor:"transformers.ASTForAudioClassification.forward",parameters:[{name:"input_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ASTForAudioClassification.forward.input_values",description:`<strong>input_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, max_length, num_mel_bins)</code>) &#x2014;
Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by
loading a <code>.flac</code> or <code>.wav</code> audio file into an array of type <code>List[float]</code> or a <code>numpy.ndarray</code>, <em>e.g.</em> via
the soundfile library (<code>pip install soundfile</code>). To prepare the array into <code>input_features</code>, the
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoFeatureExtractor">AutoFeatureExtractor</a> should be used for extracting the mel features, padding and conversion into a
tensor of type <code>torch.FloatTensor</code>. See <a href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTFeatureExtractor.__call__"><strong>call</strong>()</a>`,name:"input_values"},{anchor:"transformers.ASTForAudioClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.ASTForAudioClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.ASTForAudioClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ASTForAudioClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ASTForAudioClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the audio classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py#L547",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/audio-spectrogram-transformer#transformers.ASTConfig"
>ASTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput"
>transformers.modeling_outputs.SequenceClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),E=new Et({props:{$$slots:{default:[Kt]},$$scope:{ctx:j}}}),q=new Tt({props:{anchor:"transformers.ASTForAudioClassification.forward.example",$$slots:{default:[Ot]},$$scope:{ctx:j}}}),{c(){o=i("meta"),M=r(),m=i("p"),p=r(),f(y.$$.fragment),n=r(),f(w.$$.fragment),Je=r(),Y=i("p"),Y.innerHTML=yt,Fe=r(),Q=i("p"),Q.textContent=Mt,Ue=r(),H=i("p"),H.innerHTML=wt,ke=r(),z=i("img"),Ge=r(),L=i("small"),L.innerHTML=$t,We=r(),P=i("p"),P.innerHTML=xt,ze=r(),f(D.$$.fragment),Ze=r(),K=i("ul"),K.innerHTML=Ct,Ve=r(),f(O.$$.fragment),Ne=r(),ee=i("p"),ee.textContent=jt,Ie=r(),f(te.$$.fragment),Ee=r(),ae=i("ul"),ae.innerHTML=At,qe=r(),oe=i("p"),oe.textContent=St,Re=r(),f(ne.$$.fragment),Xe=r(),$=i("div"),f(se.$$.fragment),Oe=r(),_e=i("p"),_e.innerHTML=Jt,et=r(),Te=i("p"),Te.innerHTML=Ft,tt=r(),f(Z.$$.fragment),Be=r(),f(re.$$.fragment),Ye=r(),v=i("div"),f(le.$$.fragment),at=r(),be=i("p"),be.textContent=Ut,ot=r(),ye=i("p"),ye.innerHTML=kt,nt=r(),Me=i("p"),Me.textContent=Gt,st=r(),V=i("div"),f(ie.$$.fragment),rt=r(),we=i("p"),we.textContent=Wt,Qe=r(),f(de.$$.fragment),He=r(),F=i("div"),f(ce.$$.fragment),lt=r(),ve=i("p"),ve.innerHTML=zt,it=r(),A=i("div"),f(me.$$.fragment),dt=r(),$e=i("p"),$e.innerHTML=Zt,ct=r(),f(N.$$.fragment),mt=r(),f(I.$$.fragment),Le=r(),f(pe.$$.fragment),Pe=r(),x=i("div"),f(ue.$$.fragment),pt=r(),xe=i("p"),xe.textContent=Vt,ut=r(),Ce=i("p"),Ce.innerHTML=Nt,ft=r(),S=i("div"),f(fe.$$.fragment),ht=r(),je=i("p"),je.innerHTML=It,gt=r(),f(E.$$.fragment),_t=r(),f(q.$$.fragment),De=r(),Ae=i("p"),this.h()},l(e){const t=Qt("svelte-u9bgzb",document.head);o=d(t,"META",{name:!0,content:!0}),t.forEach(a),M=l(e),m=d(e,"P",{}),W(m).forEach(a),p=l(e),h(y.$$.fragment,e),n=l(e),h(w.$$.fragment,e),Je=l(e),Y=d(e,"P",{"data-svelte-h":!0}),u(Y)!=="svelte-1ez9eok"&&(Y.innerHTML=yt),Fe=l(e),Q=d(e,"P",{"data-svelte-h":!0}),u(Q)!=="svelte-1pvwld5"&&(Q.textContent=Mt),Ue=l(e),H=d(e,"P",{"data-svelte-h":!0}),u(H)!=="svelte-579vxj"&&(H.innerHTML=wt),ke=l(e),z=d(e,"IMG",{src:!0,alt:!0,width:!0}),Ge=l(e),L=d(e,"SMALL",{"data-svelte-h":!0}),u(L)!=="svelte-1r69l05"&&(L.innerHTML=$t),We=l(e),P=d(e,"P",{"data-svelte-h":!0}),u(P)!=="svelte-12otjfn"&&(P.innerHTML=xt),ze=l(e),h(D.$$.fragment,e),Ze=l(e),K=d(e,"UL",{"data-svelte-h":!0}),u(K)!=="svelte-u6m1v2"&&(K.innerHTML=Ct),Ve=l(e),h(O.$$.fragment,e),Ne=l(e),ee=d(e,"P",{"data-svelte-h":!0}),u(ee)!=="svelte-lnqsp9"&&(ee.textContent=jt),Ie=l(e),h(te.$$.fragment,e),Ee=l(e),ae=d(e,"UL",{"data-svelte-h":!0}),u(ae)!=="svelte-1y1nfy1"&&(ae.innerHTML=At),qe=l(e),oe=d(e,"P",{"data-svelte-h":!0}),u(oe)!=="svelte-1ca3035"&&(oe.textContent=St),Re=l(e),h(ne.$$.fragment,e),Xe=l(e),$=d(e,"DIV",{class:!0});var U=W($);h(se.$$.fragment,U),Oe=l(U),_e=d(U,"P",{"data-svelte-h":!0}),u(_e)!=="svelte-1ppuncx"&&(_e.innerHTML=Jt),et=l(U),Te=d(U,"P",{"data-svelte-h":!0}),u(Te)!=="svelte-1s6wgpv"&&(Te.innerHTML=Ft),tt=l(U),h(Z.$$.fragment,U),U.forEach(a),Be=l(e),h(re.$$.fragment,e),Ye=l(e),v=d(e,"DIV",{class:!0});var C=W(v);h(le.$$.fragment,C),at=l(C),be=d(C,"P",{"data-svelte-h":!0}),u(be)!=="svelte-1vcyh3y"&&(be.textContent=Ut),ot=l(C),ye=d(C,"P",{"data-svelte-h":!0}),u(ye)!=="svelte-fxahc3"&&(ye.innerHTML=kt),nt=l(C),Me=d(C,"P",{"data-svelte-h":!0}),u(Me)!=="svelte-47ck0t"&&(Me.textContent=Gt),st=l(C),V=d(C,"DIV",{class:!0});var he=W(V);h(ie.$$.fragment,he),rt=l(he),we=d(he,"P",{"data-svelte-h":!0}),u(we)!=="svelte-1a6wgfx"&&(we.textContent=Wt),he.forEach(a),C.forEach(a),Qe=l(e),h(de.$$.fragment,e),He=l(e),F=d(e,"DIV",{class:!0});var G=W(F);h(ce.$$.fragment,G),lt=l(G),ve=d(G,"P",{"data-svelte-h":!0}),u(ve)!=="svelte-1eae68l"&&(ve.innerHTML=zt),it=l(G),A=d(G,"DIV",{class:!0});var k=W(A);h(me.$$.fragment,k),dt=l(k),$e=d(k,"P",{"data-svelte-h":!0}),u($e)!=="svelte-1dqlmj3"&&($e.innerHTML=Zt),ct=l(k),h(N.$$.fragment,k),mt=l(k),h(I.$$.fragment,k),k.forEach(a),G.forEach(a),Le=l(e),h(pe.$$.fragment,e),Pe=l(e),x=d(e,"DIV",{class:!0});var R=W(x);h(ue.$$.fragment,R),pt=l(R),xe=d(R,"P",{"data-svelte-h":!0}),u(xe)!=="svelte-1b8t75l"&&(xe.textContent=Vt),ut=l(R),Ce=d(R,"P",{"data-svelte-h":!0}),u(Ce)!=="svelte-1gjh92c"&&(Ce.innerHTML=Nt),ft=l(R),S=d(R,"DIV",{class:!0});var X=W(S);h(fe.$$.fragment,X),ht=l(X),je=d(X,"P",{"data-svelte-h":!0}),u(je)!=="svelte-hqd4qx"&&(je.innerHTML=It),gt=l(X),h(E.$$.fragment,X),_t=l(X),h(q.$$.fragment,X),X.forEach(a),R.forEach(a),De=l(e),Ae=d(e,"P",{}),W(Ae).forEach(a),this.h()},h(){J(o,"name","hf:doc:metadata"),J(o,"content",ta),Rt(z.src,vt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/audio_spectogram_transformer_architecture.png")||J(z,"src",vt),J(z,"alt","drawing"),J(z,"width","600"),J($,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){c(document.head,o),s(e,M,t),s(e,m,t),s(e,p,t),g(y,e,t),s(e,n,t),g(w,e,t),s(e,Je,t),s(e,Y,t),s(e,Fe,t),s(e,Q,t),s(e,Ue,t),s(e,H,t),s(e,ke,t),s(e,z,t),s(e,Ge,t),s(e,L,t),s(e,We,t),s(e,P,t),s(e,ze,t),g(D,e,t),s(e,Ze,t),s(e,K,t),s(e,Ve,t),g(O,e,t),s(e,Ne,t),s(e,ee,t),s(e,Ie,t),g(te,e,t),s(e,Ee,t),s(e,ae,t),s(e,qe,t),s(e,oe,t),s(e,Re,t),g(ne,e,t),s(e,Xe,t),s(e,$,t),g(se,$,null),c($,Oe),c($,_e),c($,et),c($,Te),c($,tt),g(Z,$,null),s(e,Be,t),g(re,e,t),s(e,Ye,t),s(e,v,t),g(le,v,null),c(v,at),c(v,be),c(v,ot),c(v,ye),c(v,nt),c(v,Me),c(v,st),c(v,V),g(ie,V,null),c(V,rt),c(V,we),s(e,Qe,t),g(de,e,t),s(e,He,t),s(e,F,t),g(ce,F,null),c(F,lt),c(F,ve),c(F,it),c(F,A),g(me,A,null),c(A,dt),c(A,$e),c(A,ct),g(N,A,null),c(A,mt),g(I,A,null),s(e,Le,t),g(pe,e,t),s(e,Pe,t),s(e,x,t),g(ue,x,null),c(x,pt),c(x,xe),c(x,ut),c(x,Ce),c(x,ft),c(x,S),g(fe,S,null),c(S,ht),c(S,je),c(S,gt),g(E,S,null),c(S,_t),g(q,S,null),s(e,De,t),s(e,Ae,t),Ke=!0},p(e,[t]){const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),Z.$set(U);const C={};t&2&&(C.$$scope={dirty:t,ctx:e}),N.$set(C);const he={};t&2&&(he.$$scope={dirty:t,ctx:e}),I.$set(he);const G={};t&2&&(G.$$scope={dirty:t,ctx:e}),E.$set(G);const k={};t&2&&(k.$$scope={dirty:t,ctx:e}),q.$set(k)},i(e){Ke||(_(y.$$.fragment,e),_(w.$$.fragment,e),_(D.$$.fragment,e),_(O.$$.fragment,e),_(te.$$.fragment,e),_(ne.$$.fragment,e),_(se.$$.fragment,e),_(Z.$$.fragment,e),_(re.$$.fragment,e),_(le.$$.fragment,e),_(ie.$$.fragment,e),_(de.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(N.$$.fragment,e),_(I.$$.fragment,e),_(pe.$$.fragment,e),_(ue.$$.fragment,e),_(fe.$$.fragment,e),_(E.$$.fragment,e),_(q.$$.fragment,e),Ke=!0)},o(e){T(y.$$.fragment,e),T(w.$$.fragment,e),T(D.$$.fragment,e),T(O.$$.fragment,e),T(te.$$.fragment,e),T(ne.$$.fragment,e),T(se.$$.fragment,e),T(Z.$$.fragment,e),T(re.$$.fragment,e),T(le.$$.fragment,e),T(ie.$$.fragment,e),T(de.$$.fragment,e),T(ce.$$.fragment,e),T(me.$$.fragment,e),T(N.$$.fragment,e),T(I.$$.fragment,e),T(pe.$$.fragment,e),T(ue.$$.fragment,e),T(fe.$$.fragment,e),T(E.$$.fragment,e),T(q.$$.fragment,e),Ke=!1},d(e){e&&(a(M),a(m),a(p),a(n),a(Je),a(Y),a(Fe),a(Q),a(Ue),a(H),a(ke),a(z),a(Ge),a(L),a(We),a(P),a(ze),a(Ze),a(K),a(Ve),a(Ne),a(ee),a(Ie),a(Ee),a(ae),a(qe),a(oe),a(Re),a(Xe),a($),a(Be),a(Ye),a(v),a(Qe),a(He),a(F),a(Le),a(Pe),a(x),a(De),a(Ae)),a(o),b(y,e),b(w,e),b(D,e),b(O,e),b(te,e),b(ne,e),b(se),b(Z),b(re,e),b(le),b(ie),b(de,e),b(ce),b(me),b(N),b(I),b(pe,e),b(ue),b(fe),b(E),b(q)}}}const ta='{"title":"Audio Spectrogram Transformer","local":"audio-spectrogram-transformer","sections":[{"title":"æ¦‚è¦","local":"æ¦‚è¦","sections":[],"depth":2},{"title":"ä½¿ç”¨ä¸Šã®ãƒ’ãƒ³ãƒˆ","local":"ä½¿ç”¨ä¸Šã®ãƒ’ãƒ³ãƒˆ","sections":[],"depth":2},{"title":"å‚è€ƒè³‡æ–™","local":"å‚è€ƒè³‡æ–™","sections":[],"depth":2},{"title":"ASTConfig","local":"transformers.ASTConfig","sections":[],"depth":2},{"title":"ASTFeatureExtractor","local":"transformers.ASTFeatureExtractor","sections":[],"depth":2},{"title":"ASTModel","local":"transformers.ASTModel","sections":[],"depth":2},{"title":"ASTForAudioClassification","local":"transformers.ASTForAudioClassification","sections":[],"depth":2}],"depth":1}';function aa(j){return Xt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ma extends Bt{constructor(o){super(),Yt(this,o,aa,ea,qt,{})}}export{ma as component};
