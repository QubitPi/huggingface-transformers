import{s as fn,n as pn,o as cn}from"../chunks/scheduler.9bc65507.js";import{S as gn,i as un,g as a,s as r,r as l,A as hn,h as s,f as n,c as o,j as _,u as d,x as u,k as $,y as t,a as m,v as f,d as p,t as c,w as g}from"../chunks/index.707bf1b6.js";import{D as v}from"../chunks/Docstring.17db21ae.js";import{H as ue}from"../chunks/Heading.342b1fa6.js";function xn(zt){let O,Ee,Fe,Ie,Q,Se,V,Ht=`🤗 Transformers は <code>transformers.onnx</code> パッケージを提供します。
設定オブジェクトを利用することで、モデルのチェックポイントをONNXグラフに変換することができます。`,We,z,Xt=`詳細は<a href="../serialization">ガイド</a> を参照してください。
を参照してください。`,Qe,H,Ve,X,jt=`以下の3つの抽象クラスを提供しています。
エクスポートしたいモデルアーキテクチャのタイプに応じて、継承すべき3つの抽象クラスを提供します：`,ze,j,At='<li>エンコーダーベースのモデルは <a href="/docs/transformers/main/ja/main_classes/onnx#transformers.onnx.OnnxConfig">OnnxConfig</a> を継承します。</li> <li>デコーダーベースのモデルは <a href="/docs/transformers/main/ja/main_classes/onnx#transformers.onnx.OnnxConfigWithPast">OnnxConfigWithPast</a> を継承します。</li> <li>エンコーダー・デコーダーモデルは <a href="/docs/transformers/main/ja/main_classes/onnx#transformers.onnx.OnnxSeq2SeqConfigWithPast">OnnxSeq2SeqConfigWithPast</a> を継承しています。</li>',He,A,Xe,h,U,pt,he,Ut="Base class for ONNX exportable model describing metadata on how to export the model through the ONNX format.",ct,T,G,gt,xe,Gt=`Flatten any potential nested structure expanding the name of the field with the index of the element within the
structure.`,ut,P,R,ht,_e,Rt="Instantiate a OnnxConfig for a specific model",xt,k,B,_t,$e,Bt="Generate inputs to provide to the ONNX exporter for the specific framework",$t,M,J,vt,ve,Jt=`Generate inputs for ONNX Runtime using the reference model inputs. Override this to run inference with seq2seq
models which have the encoder and decoder exported as separate ONNX files.`,bt,q,K,Ct,be,Kt="Flag indicating if the model requires using external data format",je,Y,Ae,w,Z,yt,D,ee,wt,Ce,Yt="Fill the input_or_outputs mapping with past_key_values dynamic axes considering.",Ot,N,te,Tt,ye,Zt="Instantiate a OnnxConfig with <code>use_past</code> attribute set to True",Ue,ne,Ge,re,oe,Re,ae,Be,se,en=`各 ONNX 構成は、次のことを可能にする一連の <em>機能</em> に関連付けられています。
さまざまなタイプのトポロジまたはタスクのモデルをエクスポートします。`,Je,ie,Ke,x,me,Pt,F,le,kt,we,tn="Check whether or not the model has the requested features.",Mt,y,de,qt,Oe,nn="Determines the framework to use for the export.",Dt,Te,rn="The priority is in the following order:",Nt,Pe,on="<li>User input via <code>framework</code>.</li> <li>If local checkpoint is provided, use the same framework as the checkpoint.</li> <li>Available framework in environment, with priority given to PyTorch</li>",Ft,L,fe,Lt,ke,an="Gets the OnnxConfig for a model_type and feature combination.",Et,E,pe,It,Me,sn="Attempts to retrieve an AutoModel class from a feature name.",St,I,ce,Wt,qe,mn="Attempts to retrieve a model from a model’s name and the feature to be enabled.",Qt,S,ge,Vt,De,ln="Tries to retrieve the feature -> OnnxConfig constructor map from the model type.",Ye,Le,Ze;return Q=new ue({props:{title:"Exporting 🤗 Transformers models to ONNX",local:"exporting--transformers-models-to-onnx",headingTag:"h1"}}),H=new ue({props:{title:"ONNX Configurations",local:"onnx-configurations",headingTag:"h2"}}),A=new ue({props:{title:"OnnxConfig",local:"transformers.onnx.OnnxConfig",headingTag:"h3"}}),U=new v({props:{name:"class transformers.onnx.OnnxConfig",anchor:"transformers.onnx.OnnxConfig",parameters:[{name:"config",val:": PretrainedConfig"},{name:"task",val:": str = 'default'"},{name:"patching_specs",val:": List = None"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/config.py#L68"}}),G=new v({props:{name:"flatten_output_collection_property",anchor:"transformers.onnx.OnnxConfig.flatten_output_collection_property",parameters:[{name:"name",val:": str"},{name:"field",val:": Iterable"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/config.py#L424",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Outputs with flattened structure and key mapping this new structure.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>(Dict[str, Any])</p>
`}}),R=new v({props:{name:"from_model_config",anchor:"transformers.onnx.OnnxConfig.from_model_config",parameters:[{name:"config",val:": PretrainedConfig"},{name:"task",val:": str = 'default'"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/config.py#L127",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>OnnxConfig for this model</p>
`}}),B=new v({props:{name:"generate_dummy_inputs",anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs",parameters:[{name:"preprocessor",val:": Union"},{name:"batch_size",val:": int = -1"},{name:"seq_length",val:": int = -1"},{name:"num_choices",val:": int = -1"},{name:"is_pair",val:": bool = False"},{name:"framework",val:": Optional = None"},{name:"num_channels",val:": int = 3"},{name:"image_width",val:": int = 40"},{name:"image_height",val:": int = 40"},{name:"sampling_rate",val:": int = 22050"},{name:"time_duration",val:": float = 5.0"},{name:"frequency",val:": int = 220"},{name:"tokenizer",val:": PreTrainedTokenizerBase = None"}],parametersDescription:[{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The batch size to export the model for (-1 means dynamic axis).`,name:"batch_size"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.num_choices",description:`<strong>num_choices</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The number of candidate answers provided for multiple choice task (-1 means dynamic axis).`,name:"num_choices"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.seq_length",description:`<strong>seq_length</strong> (<code>int</code>, <em>optional</em>, defaults to -1) &#x2014;
The sequence length to export the model for (-1 means dynamic axis).`,name:"seq_length"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.is_pair",description:`<strong>is_pair</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Indicate if the input is a pair (sentence 1, sentence 2)`,name:"is_pair"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.framework",description:`<strong>framework</strong> (<code>TensorType</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The framework (PyTorch or TensorFlow) that the tokenizer will generate tensors for.`,name:"framework"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of channels of the generated images.`,name:"num_channels"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.image_width",description:`<strong>image_width</strong> (<code>int</code>, <em>optional</em>, defaults to 40) &#x2014;
The width of the generated images.`,name:"image_width"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.image_height",description:`<strong>image_height</strong> (<code>int</code>, <em>optional</em>, defaults to 40) &#x2014;
The height of the generated images.`,name:"image_height"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.sampling_rate",description:`<strong>sampling_rate</strong> (<code>int</code>, <em>optional</em> defaults to 22050) &#x2014;
The sampling rate for audio data generation.`,name:"sampling_rate"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.time_duration",description:`<strong>time_duration</strong> (<code>float</code>, <em>optional</em> defaults to 5.0) &#x2014;
Total seconds of sampling for audio data generation.`,name:"time_duration"},{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs.frequency",description:`<strong>frequency</strong> (<code>int</code>, <em>optional</em> defaults to 220) &#x2014;
The desired natural frequency of generated audio.`,name:"frequency"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/config.py#L280",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Mapping[str, Tensor] holding the kwargs to provide to the model’s forward function</p>
`}}),J=new v({props:{name:"generate_dummy_inputs_onnxruntime",anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs_onnxruntime",parameters:[{name:"reference_model_inputs",val:": Mapping"}],parametersDescription:[{anchor:"transformers.onnx.OnnxConfig.generate_dummy_inputs_onnxruntime.reference_model_inputs",description:`<strong>reference_model_inputs</strong> ([<code>Mapping[str, Tensor]</code>) &#x2014;
Reference inputs for the model.`,name:"reference_model_inputs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/config.py#L400",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The mapping holding the kwargs to provide to the model’s forward function</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Mapping[str, Tensor]</code></p>
`}}),K=new v({props:{name:"use_external_data_format",anchor:"transformers.onnx.OnnxConfig.use_external_data_format",parameters:[{name:"num_parameters",val:": int"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/config.py#L241",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>True if model.num_parameters() * size_of(float32) >= 2Gb False otherwise</p>
`}}),Y=new ue({props:{title:"OnnxConfigWithPast",local:"transformers.onnx.OnnxConfigWithPast",headingTag:"h3"}}),Z=new v({props:{name:"class transformers.onnx.OnnxConfigWithPast",anchor:"transformers.onnx.OnnxConfigWithPast",parameters:[{name:"config",val:": PretrainedConfig"},{name:"task",val:": str = 'default'"},{name:"patching_specs",val:": List = None"},{name:"use_past",val:": bool = False"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/config.py#L443"}}),ee=new v({props:{name:"fill_with_past_key_values_",anchor:"transformers.onnx.OnnxConfigWithPast.fill_with_past_key_values_",parameters:[{name:"inputs_or_outputs",val:": Mapping"},{name:"direction",val:": str"},{name:"inverted_values_shape",val:": bool = False"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/config.py#L550"}}),te=new v({props:{name:"with_past",anchor:"transformers.onnx.OnnxConfigWithPast.with_past",parameters:[{name:"config",val:": PretrainedConfig"},{name:"task",val:": str = 'default'"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/config.py#L454",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>OnnxConfig with <code>.use_past = True</code></p>
`}}),ne=new ue({props:{title:"OnnxSeq2SeqConfigWithPast",local:"transformers.onnx.OnnxSeq2SeqConfigWithPast",headingTag:"h3"}}),oe=new v({props:{name:"class transformers.onnx.OnnxSeq2SeqConfigWithPast",anchor:"transformers.onnx.OnnxSeq2SeqConfigWithPast",parameters:[{name:"config",val:": PretrainedConfig"},{name:"task",val:": str = 'default'"},{name:"patching_specs",val:": List = None"},{name:"use_past",val:": bool = False"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/config.py#L590"}}),ae=new ue({props:{title:"ONNX Features",local:"onnx-features",headingTag:"h2"}}),ie=new ue({props:{title:"FeaturesManager",local:"transformers.onnx.FeaturesManager",headingTag:"h3"}}),me=new v({props:{name:"class transformers.onnx.FeaturesManager",anchor:"transformers.onnx.FeaturesManager",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/features.py#L85"}}),le=new v({props:{name:"check_supported_model_or_raise",anchor:"transformers.onnx.FeaturesManager.check_supported_model_or_raise",parameters:[{name:"model",val:": Union"},{name:"feature",val:": str = 'default'"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/features.py#L711",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>(str) The type of the model (OnnxConfig) The OnnxConfig instance holding the model export properties.</p>
`}}),de=new v({props:{name:"determine_framework",anchor:"transformers.onnx.FeaturesManager.determine_framework",parameters:[{name:"model",val:": str"},{name:"framework",val:": str = None"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.determine_framework.model",description:`<strong>model</strong> (<code>str</code>) &#x2014;
The name of the model to export.`,name:"model"},{anchor:"transformers.onnx.FeaturesManager.determine_framework.framework",description:`<strong>framework</strong> (<code>str</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The framework to use for the export. See above for priority if none provided.`,name:"framework"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/features.py#L628",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The framework to use for the export.</p>
`}}),fe=new v({props:{name:"get_config",anchor:"transformers.onnx.FeaturesManager.get_config",parameters:[{name:"model_type",val:": str"},{name:"feature",val:": str"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.get_config.model_type",description:`<strong>model_type</strong> (<code>str</code>) &#x2014;
The model type to retrieve the config for.`,name:"model_type"},{anchor:"transformers.onnx.FeaturesManager.get_config.feature",description:`<strong>feature</strong> (<code>str</code>) &#x2014;
The feature to retrieve the config for.`,name:"feature"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/features.py#L736",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>config for the combination</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>OnnxConfig</code></p>
`}}),pe=new v({props:{name:"get_model_class_for_feature",anchor:"transformers.onnx.FeaturesManager.get_model_class_for_feature",parameters:[{name:"feature",val:": str"},{name:"framework",val:": str = 'pt'"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.get_model_class_for_feature.feature",description:`<strong>feature</strong> (<code>str</code>) &#x2014;
The feature required.`,name:"feature"},{anchor:"transformers.onnx.FeaturesManager.get_model_class_for_feature.framework",description:`<strong>framework</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;pt&quot;</code>) &#x2014;
The framework to use for the export.`,name:"framework"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/features.py#L601",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The AutoModel class corresponding to the feature.</p>
`}}),ce=new v({props:{name:"get_model_from_feature",anchor:"transformers.onnx.FeaturesManager.get_model_from_feature",parameters:[{name:"feature",val:": str"},{name:"model",val:": str"},{name:"framework",val:": str = None"},{name:"cache_dir",val:": str = None"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.get_model_from_feature.feature",description:`<strong>feature</strong> (<code>str</code>) &#x2014;
The feature required.`,name:"feature"},{anchor:"transformers.onnx.FeaturesManager.get_model_from_feature.model",description:`<strong>model</strong> (<code>str</code>) &#x2014;
The name of the model to export.`,name:"model"},{anchor:"transformers.onnx.FeaturesManager.get_model_from_feature.framework",description:`<strong>framework</strong> (<code>str</code>, <em>optional</em>, defaults to <code>None</code>) &#x2014;
The framework to use for the export. See <code>FeaturesManager.determine_framework</code> for the priority should
none be provided.`,name:"framework"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/features.py#L678",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The instance of the model.</p>
`}}),ge=new v({props:{name:"get_supported_features_for_model_type",anchor:"transformers.onnx.FeaturesManager.get_supported_features_for_model_type",parameters:[{name:"model_type",val:": str"},{name:"model_name",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.onnx.FeaturesManager.get_supported_features_for_model_type.model_type",description:`<strong>model_type</strong> (<code>str</code>) &#x2014;
The model type to retrieve the supported features for.`,name:"model_type"},{anchor:"transformers.onnx.FeaturesManager.get_supported_features_for_model_type.model_name",description:`<strong>model_name</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The name attribute of the model object, only used for the exception message.`,name:"model_name"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/onnx/features.py#L556",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The dictionary mapping each feature to a corresponding OnnxConfig constructor.</p>
`}}),{c(){O=a("meta"),Ee=r(),Fe=a("p"),Ie=r(),l(Q.$$.fragment),Se=r(),V=a("p"),V.innerHTML=Ht,We=r(),z=a("p"),z.innerHTML=Xt,Qe=r(),l(H.$$.fragment),Ve=r(),X=a("p"),X.textContent=jt,ze=r(),j=a("ul"),j.innerHTML=At,He=r(),l(A.$$.fragment),Xe=r(),h=a("div"),l(U.$$.fragment),pt=r(),he=a("p"),he.textContent=Ut,ct=r(),T=a("div"),l(G.$$.fragment),gt=r(),xe=a("p"),xe.textContent=Gt,ut=r(),P=a("div"),l(R.$$.fragment),ht=r(),_e=a("p"),_e.textContent=Rt,xt=r(),k=a("div"),l(B.$$.fragment),_t=r(),$e=a("p"),$e.textContent=Bt,$t=r(),M=a("div"),l(J.$$.fragment),vt=r(),ve=a("p"),ve.textContent=Jt,bt=r(),q=a("div"),l(K.$$.fragment),Ct=r(),be=a("p"),be.textContent=Kt,je=r(),l(Y.$$.fragment),Ae=r(),w=a("div"),l(Z.$$.fragment),yt=r(),D=a("div"),l(ee.$$.fragment),wt=r(),Ce=a("p"),Ce.textContent=Yt,Ot=r(),N=a("div"),l(te.$$.fragment),Tt=r(),ye=a("p"),ye.innerHTML=Zt,Ue=r(),l(ne.$$.fragment),Ge=r(),re=a("div"),l(oe.$$.fragment),Re=r(),l(ae.$$.fragment),Be=r(),se=a("p"),se.innerHTML=en,Je=r(),l(ie.$$.fragment),Ke=r(),x=a("div"),l(me.$$.fragment),Pt=r(),F=a("div"),l(le.$$.fragment),kt=r(),we=a("p"),we.textContent=tn,Mt=r(),y=a("div"),l(de.$$.fragment),qt=r(),Oe=a("p"),Oe.textContent=nn,Dt=r(),Te=a("p"),Te.textContent=rn,Nt=r(),Pe=a("ol"),Pe.innerHTML=on,Ft=r(),L=a("div"),l(fe.$$.fragment),Lt=r(),ke=a("p"),ke.textContent=an,Et=r(),E=a("div"),l(pe.$$.fragment),It=r(),Me=a("p"),Me.textContent=sn,St=r(),I=a("div"),l(ce.$$.fragment),Wt=r(),qe=a("p"),qe.textContent=mn,Qt=r(),S=a("div"),l(ge.$$.fragment),Vt=r(),De=a("p"),De.textContent=ln,Ye=r(),Le=a("p"),this.h()},l(e){const i=hn("svelte-u9bgzb",document.head);O=s(i,"META",{name:!0,content:!0}),i.forEach(n),Ee=o(e),Fe=s(e,"P",{}),_(Fe).forEach(n),Ie=o(e),d(Q.$$.fragment,e),Se=o(e),V=s(e,"P",{"data-svelte-h":!0}),u(V)!=="svelte-5rvatf"&&(V.innerHTML=Ht),We=o(e),z=s(e,"P",{"data-svelte-h":!0}),u(z)!=="svelte-mil2iv"&&(z.innerHTML=Xt),Qe=o(e),d(H.$$.fragment,e),Ve=o(e),X=s(e,"P",{"data-svelte-h":!0}),u(X)!=="svelte-r4ilu2"&&(X.textContent=jt),ze=o(e),j=s(e,"UL",{"data-svelte-h":!0}),u(j)!=="svelte-x1o0ya"&&(j.innerHTML=At),He=o(e),d(A.$$.fragment,e),Xe=o(e),h=s(e,"DIV",{class:!0});var b=_(h);d(U.$$.fragment,b),pt=o(b),he=s(b,"P",{"data-svelte-h":!0}),u(he)!=="svelte-1gqzpaz"&&(he.textContent=Ut),ct=o(b),T=s(b,"DIV",{class:!0});var et=_(T);d(G.$$.fragment,et),gt=o(et),xe=s(et,"P",{"data-svelte-h":!0}),u(xe)!=="svelte-1lfqihc"&&(xe.textContent=Gt),et.forEach(n),ut=o(b),P=s(b,"DIV",{class:!0});var tt=_(P);d(R.$$.fragment,tt),ht=o(tt),_e=s(tt,"P",{"data-svelte-h":!0}),u(_e)!=="svelte-1u54gj1"&&(_e.textContent=Rt),tt.forEach(n),xt=o(b),k=s(b,"DIV",{class:!0});var nt=_(k);d(B.$$.fragment,nt),_t=o(nt),$e=s(nt,"P",{"data-svelte-h":!0}),u($e)!=="svelte-1oyyynq"&&($e.textContent=Bt),nt.forEach(n),$t=o(b),M=s(b,"DIV",{class:!0});var rt=_(M);d(J.$$.fragment,rt),vt=o(rt),ve=s(rt,"P",{"data-svelte-h":!0}),u(ve)!=="svelte-tenqiw"&&(ve.textContent=Jt),rt.forEach(n),bt=o(b),q=s(b,"DIV",{class:!0});var ot=_(q);d(K.$$.fragment,ot),Ct=o(ot),be=s(ot,"P",{"data-svelte-h":!0}),u(be)!=="svelte-coevd"&&(be.textContent=Kt),ot.forEach(n),b.forEach(n),je=o(e),d(Y.$$.fragment,e),Ae=o(e),w=s(e,"DIV",{class:!0});var Ne=_(w);d(Z.$$.fragment,Ne),yt=o(Ne),D=s(Ne,"DIV",{class:!0});var at=_(D);d(ee.$$.fragment,at),wt=o(at),Ce=s(at,"P",{"data-svelte-h":!0}),u(Ce)!=="svelte-1bh80il"&&(Ce.textContent=Yt),at.forEach(n),Ot=o(Ne),N=s(Ne,"DIV",{class:!0});var st=_(N);d(te.$$.fragment,st),Tt=o(st),ye=s(st,"P",{"data-svelte-h":!0}),u(ye)!=="svelte-1kg2yu2"&&(ye.innerHTML=Zt),st.forEach(n),Ne.forEach(n),Ue=o(e),d(ne.$$.fragment,e),Ge=o(e),re=s(e,"DIV",{class:!0});var dn=_(re);d(oe.$$.fragment,dn),dn.forEach(n),Re=o(e),d(ae.$$.fragment,e),Be=o(e),se=s(e,"P",{"data-svelte-h":!0}),u(se)!=="svelte-518qzv"&&(se.innerHTML=en),Je=o(e),d(ie.$$.fragment,e),Ke=o(e),x=s(e,"DIV",{class:!0});var C=_(x);d(me.$$.fragment,C),Pt=o(C),F=s(C,"DIV",{class:!0});var it=_(F);d(le.$$.fragment,it),kt=o(it),we=s(it,"P",{"data-svelte-h":!0}),u(we)!=="svelte-rlyyl3"&&(we.textContent=tn),it.forEach(n),Mt=o(C),y=s(C,"DIV",{class:!0});var W=_(y);d(de.$$.fragment,W),qt=o(W),Oe=s(W,"P",{"data-svelte-h":!0}),u(Oe)!=="svelte-rshgf5"&&(Oe.textContent=nn),Dt=o(W),Te=s(W,"P",{"data-svelte-h":!0}),u(Te)!=="svelte-1wbth9c"&&(Te.textContent=rn),Nt=o(W),Pe=s(W,"OL",{"data-svelte-h":!0}),u(Pe)!=="svelte-qby6wj"&&(Pe.innerHTML=on),W.forEach(n),Ft=o(C),L=s(C,"DIV",{class:!0});var mt=_(L);d(fe.$$.fragment,mt),Lt=o(mt),ke=s(mt,"P",{"data-svelte-h":!0}),u(ke)!=="svelte-il0adz"&&(ke.textContent=an),mt.forEach(n),Et=o(C),E=s(C,"DIV",{class:!0});var lt=_(E);d(pe.$$.fragment,lt),It=o(lt),Me=s(lt,"P",{"data-svelte-h":!0}),u(Me)!=="svelte-k5ftuy"&&(Me.textContent=sn),lt.forEach(n),St=o(C),I=s(C,"DIV",{class:!0});var dt=_(I);d(ce.$$.fragment,dt),Wt=o(dt),qe=s(dt,"P",{"data-svelte-h":!0}),u(qe)!=="svelte-auvjm5"&&(qe.textContent=mn),dt.forEach(n),Qt=o(C),S=s(C,"DIV",{class:!0});var ft=_(S);d(ge.$$.fragment,ft),Vt=o(ft),De=s(ft,"P",{"data-svelte-h":!0}),u(De)!=="svelte-tbalh5"&&(De.textContent=ln),ft.forEach(n),C.forEach(n),Ye=o(e),Le=s(e,"P",{}),_(Le).forEach(n),this.h()},h(){$(O,"name","hf:doc:metadata"),$(O,"content",_n),$(T,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(q,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(h,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(re,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),$(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,i){t(document.head,O),m(e,Ee,i),m(e,Fe,i),m(e,Ie,i),f(Q,e,i),m(e,Se,i),m(e,V,i),m(e,We,i),m(e,z,i),m(e,Qe,i),f(H,e,i),m(e,Ve,i),m(e,X,i),m(e,ze,i),m(e,j,i),m(e,He,i),f(A,e,i),m(e,Xe,i),m(e,h,i),f(U,h,null),t(h,pt),t(h,he),t(h,ct),t(h,T),f(G,T,null),t(T,gt),t(T,xe),t(h,ut),t(h,P),f(R,P,null),t(P,ht),t(P,_e),t(h,xt),t(h,k),f(B,k,null),t(k,_t),t(k,$e),t(h,$t),t(h,M),f(J,M,null),t(M,vt),t(M,ve),t(h,bt),t(h,q),f(K,q,null),t(q,Ct),t(q,be),m(e,je,i),f(Y,e,i),m(e,Ae,i),m(e,w,i),f(Z,w,null),t(w,yt),t(w,D),f(ee,D,null),t(D,wt),t(D,Ce),t(w,Ot),t(w,N),f(te,N,null),t(N,Tt),t(N,ye),m(e,Ue,i),f(ne,e,i),m(e,Ge,i),m(e,re,i),f(oe,re,null),m(e,Re,i),f(ae,e,i),m(e,Be,i),m(e,se,i),m(e,Je,i),f(ie,e,i),m(e,Ke,i),m(e,x,i),f(me,x,null),t(x,Pt),t(x,F),f(le,F,null),t(F,kt),t(F,we),t(x,Mt),t(x,y),f(de,y,null),t(y,qt),t(y,Oe),t(y,Dt),t(y,Te),t(y,Nt),t(y,Pe),t(x,Ft),t(x,L),f(fe,L,null),t(L,Lt),t(L,ke),t(x,Et),t(x,E),f(pe,E,null),t(E,It),t(E,Me),t(x,St),t(x,I),f(ce,I,null),t(I,Wt),t(I,qe),t(x,Qt),t(x,S),f(ge,S,null),t(S,Vt),t(S,De),m(e,Ye,i),m(e,Le,i),Ze=!0},p:pn,i(e){Ze||(p(Q.$$.fragment,e),p(H.$$.fragment,e),p(A.$$.fragment,e),p(U.$$.fragment,e),p(G.$$.fragment,e),p(R.$$.fragment,e),p(B.$$.fragment,e),p(J.$$.fragment,e),p(K.$$.fragment,e),p(Y.$$.fragment,e),p(Z.$$.fragment,e),p(ee.$$.fragment,e),p(te.$$.fragment,e),p(ne.$$.fragment,e),p(oe.$$.fragment,e),p(ae.$$.fragment,e),p(ie.$$.fragment,e),p(me.$$.fragment,e),p(le.$$.fragment,e),p(de.$$.fragment,e),p(fe.$$.fragment,e),p(pe.$$.fragment,e),p(ce.$$.fragment,e),p(ge.$$.fragment,e),Ze=!0)},o(e){c(Q.$$.fragment,e),c(H.$$.fragment,e),c(A.$$.fragment,e),c(U.$$.fragment,e),c(G.$$.fragment,e),c(R.$$.fragment,e),c(B.$$.fragment,e),c(J.$$.fragment,e),c(K.$$.fragment,e),c(Y.$$.fragment,e),c(Z.$$.fragment,e),c(ee.$$.fragment,e),c(te.$$.fragment,e),c(ne.$$.fragment,e),c(oe.$$.fragment,e),c(ae.$$.fragment,e),c(ie.$$.fragment,e),c(me.$$.fragment,e),c(le.$$.fragment,e),c(de.$$.fragment,e),c(fe.$$.fragment,e),c(pe.$$.fragment,e),c(ce.$$.fragment,e),c(ge.$$.fragment,e),Ze=!1},d(e){e&&(n(Ee),n(Fe),n(Ie),n(Se),n(V),n(We),n(z),n(Qe),n(Ve),n(X),n(ze),n(j),n(He),n(Xe),n(h),n(je),n(Ae),n(w),n(Ue),n(Ge),n(re),n(Re),n(Be),n(se),n(Je),n(Ke),n(x),n(Ye),n(Le)),n(O),g(Q,e),g(H,e),g(A,e),g(U),g(G),g(R),g(B),g(J),g(K),g(Y,e),g(Z),g(ee),g(te),g(ne,e),g(oe),g(ae,e),g(ie,e),g(me),g(le),g(de),g(fe),g(pe),g(ce),g(ge)}}}const _n='{"title":"Exporting 🤗 Transformers models to ONNX","local":"exporting--transformers-models-to-onnx","sections":[{"title":"ONNX Configurations","local":"onnx-configurations","sections":[{"title":"OnnxConfig","local":"transformers.onnx.OnnxConfig","sections":[],"depth":3},{"title":"OnnxConfigWithPast","local":"transformers.onnx.OnnxConfigWithPast","sections":[],"depth":3},{"title":"OnnxSeq2SeqConfigWithPast","local":"transformers.onnx.OnnxSeq2SeqConfigWithPast","sections":[],"depth":3}],"depth":2},{"title":"ONNX Features","local":"onnx-features","sections":[{"title":"FeaturesManager","local":"transformers.onnx.FeaturesManager","sections":[],"depth":3}],"depth":2}],"depth":1}';function $n(zt){return cn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class wn extends gn{constructor(O){super(),un(this,O,$n,xn,fn,{})}}export{wn as component};
