import{s as Ot,o as te,n as ee}from"../chunks/scheduler.9bc65507.js";import{S as se,i as le,g as p,s as a,r as c,A as ae,h as i,f as s,c as n,j as Kt,u as r,x as m,k as K,y as ne,a as l,v as g,d as f,t as d,w as h,m as pe,n as ie}from"../chunks/index.707bf1b6.js";import{T as me}from"../chunks/Tip.c2ecdbf4.js";import{C as w}from"../chunks/CodeBlock.54a9f38d.js";import{H as Rt}from"../chunks/Heading.342b1fa6.js";function oe(O){let o,u,$='<a href="../model_doc/dpt">DPT</a>, <a href="../model_doc/glpn">GLPN</a>';return{c(){o=pe(`このチュートリアルで説明するタスクは、次のモデル アーキテクチャでサポートされています。

`),u=p("p"),u.innerHTML=$},l(M){o=ie(M,`このチュートリアルで説明するタスクは、次のモデル アーキテクチャでサポートされています。

`),u=i(M,"P",{"data-svelte-h":!0}),m(u)!=="svelte-940txo"&&(u.innerHTML=$)},m(M,y){l(M,o,y),l(M,u,y)},p:ee,d(M){M&&(s(o),s(u))}}}function ce(O){let o,u,$,M,y,tt,U,_t=`単眼奥行き推定は、シーンの奥行き情報を画像から予測することを含むコンピューター ビジョン タスクです。
単一の画像。言い換えれば、シーン内のオブジェクトの距離を距離から推定するプロセスです。
単一カメラの視点。`,et,v,Gt=`単眼奥行き推定には、3D 再構築、拡張現実、自動運転、
そしてロボット工学。モデルがオブジェクト間の複雑な関係を理解する必要があるため、これは困難な作業です。
シーンとそれに対応する深度情報（照明条件などの要因の影響を受ける可能性があります）
オクルージョンとテクスチャ。`,st,j,lt,C,Bt="このガイドでは、次の方法を学びます。",at,Z,Ht="<li>深度推定パイプラインを作成する</li> <li>手動で深度推定推論を実行します</li>",nt,x,It="始める前に、必要なライブラリがすべてインストールされていることを確認してください。",pt,W,it,k,mt,R,Et=`深度推定をサポートするモデルで推論を試す最も簡単な方法は、対応する <a href="/docs/transformers/main/ja/main_classes/pipelines#transformers.pipeline">pipeline()</a> を使用することです。
<a href="https://huggingface.co/models?pipeline_tag=Depth-estimation&amp;sort=downloads" rel="nofollow">Hugging Face Hub のチェックポイント</a> からパイプラインをインスタンス化します。`,ot,_,ct,G,Vt="次に、分析する画像を選択します。",rt,B,gt,b,zt='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-estimation-example.jpg" alt="Photo of a busy street"/>',ft,H,Nt="画像をパイプラインに渡します。",dt,I,ht,E,Xt=`パイプラインは 2 つのエントリを含む辞書を返します。最初のものは<code>predicted_ Depth</code>と呼ばれ、次の値を持つテンソルです。
深さは各ピクセルのメートル単位で表されます。
2 番目の<code>depth</code>は、深度推定結果を視覚化する PIL 画像です。`,ut,V,Qt="視覚化された結果を見てみましょう。",Mt,z,yt,J,Ft='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-visualization.png" alt="Depth estimation visualization"/>',wt,N,$t,X,Yt="深度推定パイプラインの使用方法を理解したので、同じ結果を手動で複製する方法を見てみましょう。",jt,Q,St=`まず、<a href="https://huggingface.co/models?pipeline_tag=Depth-estimation&amp;sort=downloads" rel="nofollow">Hugging Face Hub のチェックポイント</a> からモデルと関連プロセッサをロードします。
ここでは、前と同じチェックポイントを使用します。`,bt,F,Jt,Y,Lt=`必要な画像変換を処理する<code>image_processor</code>を使用して、モデルの画像入力を準備します。
サイズ変更や正規化など:`,Tt,S,Ut,L,Pt="準備された入力をモデルに渡します。",vt,P,Ct,A,At="結果を視覚化します。",Zt,q,xt,T,qt='<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-visualization.png" alt="Depth estimation visualization"/>',Wt,D,kt;return y=new Rt({props:{title:"Monocular depth estimation",local:"monocular-depth-estimation",headingTag:"h1"}}),j=new me({props:{$$slots:{default:[oe]},$$scope:{ctx:O}}}),W=new w({props:{code:"cGlwJTIwaW5zdGFsbCUyMC1xJTIwdHJhbnNmb3JtZXJz",highlighted:"pip install -q transformers",wrap:!1}}),k=new Rt({props:{title:"Depth estimation pipeline",local:"depth-estimation-pipeline",headingTag:"h2"}}),_=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2hlY2twb2ludCUyMCUzRCUyMCUyMnZpbnZpbm8wMiUyRmdscG4tbnl1JTIyJTBBZGVwdGhfZXN0aW1hdG9yJTIwJTNEJTIwcGlwZWxpbmUoJTIyZGVwdGgtZXN0aW1hdGlvbiUyMiUyQyUyMG1vZGVsJTNEY2hlY2twb2ludCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;vinvino02/glpn-nyu&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>depth_estimator = pipeline(<span class="hljs-string">&quot;depth-estimation&quot;</span>, model=checkpoint)`,wrap:!1}}),B=new w({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwcyUzQSUyRiUyRnVuc3BsYXNoLmNvbSUyRnBob3RvcyUyRkh3QkFzU2JQQkRVJTJGZG93bmxvYWQlM0ZpeGlkJTNETW53eE1qQTNmREI4TVh4elpXRnlZMmg4TXpSOGZHTmhjaVV5TUdsdUpUSXdkR2hsSlRJd2MzUnlaV1YwZkdWdWZEQjhNSHg4ZkRFMk56ZzVNREV3T0RnJTI2Zm9yY2UlM0R0cnVlJTI2dyUzRDY0MCUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQWltYWdl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;https://unsplash.com/photos/HwBAsSbPBDU/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MzR8fGNhciUyMGluJTIwdGhlJTIwc3RyZWV0fGVufDB8MHx8fDE2Nzg5MDEwODg&amp;force=true&amp;w=640&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>image`,wrap:!1}}),I=new w({props:{code:"cHJlZGljdGlvbnMlMjAlM0QlMjBkZXB0aF9lc3RpbWF0b3IoaW1hZ2Up",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>predictions = depth_estimator(image)',wrap:!1}}),z=new w({props:{code:"cHJlZGljdGlvbnMlNUIlMjJkZXB0aCUyMiU1RA==",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>predictions[<span class="hljs-string">&quot;depth&quot;</span>]',wrap:!1}}),N=new Rt({props:{title:"Depth estimation inference by hand",local:"depth-estimation-inference-by-hand",headingTag:"h2"}}),F=new w({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbiUwQSUwQWNoZWNrcG9pbnQlMjAlM0QlMjAlMjJ2aW52aW5vMDIlMkZnbHBuLW55dSUyMiUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckRlcHRoRXN0aW1hdGlvbi5mcm9tX3ByZXRyYWluZWQoY2hlY2twb2ludCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, AutoModelForDepthEstimation

<span class="hljs-meta">&gt;&gt;&gt; </span>checkpoint = <span class="hljs-string">&quot;vinvino02/glpn-nyu&quot;</span>

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(checkpoint)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForDepthEstimation.from_pretrained(checkpoint)`,wrap:!1}}),S=new w({props:{code:"cGl4ZWxfdmFsdWVzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMikucGl4ZWxfdmFsdWVz",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values',wrap:!1}}),P=new w({props:{code:"aW1wb3J0JTIwdG9yY2glMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKHBpeGVsX3ZhbHVlcyklMEElMjAlMjAlMjAlMjBwcmVkaWN0ZWRfZGVwdGglMjAlM0QlMjBvdXRwdXRzLnByZWRpY3RlZF9kZXB0aA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(pixel_values)
<span class="hljs-meta">... </span>    predicted_depth = outputs.predicted_depth`,wrap:!1}}),q=new w({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTIzJTIwaW50ZXJwb2xhdGUlMjB0byUyMG9yaWdpbmFsJTIwc2l6ZSUwQXByZWRpY3Rpb24lMjAlM0QlMjB0b3JjaC5ubi5mdW5jdGlvbmFsLmludGVycG9sYXRlKCUwQSUyMCUyMCUyMCUyMHByZWRpY3RlZF9kZXB0aC51bnNxdWVlemUoMSklMkMlMEElMjAlMjAlMjAlMjBzaXplJTNEaW1hZ2Uuc2l6ZSU1QiUzQSUzQS0xJTVEJTJDJTBBJTIwJTIwJTIwJTIwbW9kZSUzRCUyMmJpY3ViaWMlMjIlMkMlMEElMjAlMjAlMjAlMjBhbGlnbl9jb3JuZXJzJTNERmFsc2UlMkMlMEEpLnNxdWVlemUoKSUwQW91dHB1dCUyMCUzRCUyMHByZWRpY3Rpb24ubnVtcHkoKSUwQSUwQWZvcm1hdHRlZCUyMCUzRCUyMChvdXRwdXQlMjAqJTIwMjU1JTIwJTJGJTIwbnAubWF4KG91dHB1dCkpLmFzdHlwZSglMjJ1aW50OCUyMiklMEFkZXB0aCUyMCUzRCUyMEltYWdlLmZyb21hcnJheShmb3JtYXR0ZWQpJTBBZGVwdGg=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># interpolate to original size</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>prediction = torch.nn.functional.interpolate(
<span class="hljs-meta">... </span>    predicted_depth.unsqueeze(<span class="hljs-number">1</span>),
<span class="hljs-meta">... </span>    size=image.size[::-<span class="hljs-number">1</span>],
<span class="hljs-meta">... </span>    mode=<span class="hljs-string">&quot;bicubic&quot;</span>,
<span class="hljs-meta">... </span>    align_corners=<span class="hljs-literal">False</span>,
<span class="hljs-meta">... </span>).squeeze()
<span class="hljs-meta">&gt;&gt;&gt; </span>output = prediction.numpy()

<span class="hljs-meta">&gt;&gt;&gt; </span>formatted = (output * <span class="hljs-number">255</span> / np.<span class="hljs-built_in">max</span>(output)).astype(<span class="hljs-string">&quot;uint8&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>depth = Image.fromarray(formatted)
<span class="hljs-meta">&gt;&gt;&gt; </span>depth`,wrap:!1}}),{c(){o=p("meta"),u=a(),$=p("p"),M=a(),c(y.$$.fragment),tt=a(),U=p("p"),U.textContent=_t,et=a(),v=p("p"),v.textContent=Gt,st=a(),c(j.$$.fragment),lt=a(),C=p("p"),C.textContent=Bt,at=a(),Z=p("ul"),Z.innerHTML=Ht,nt=a(),x=p("p"),x.textContent=It,pt=a(),c(W.$$.fragment),it=a(),c(k.$$.fragment),mt=a(),R=p("p"),R.innerHTML=Et,ot=a(),c(_.$$.fragment),ct=a(),G=p("p"),G.textContent=Vt,rt=a(),c(B.$$.fragment),gt=a(),b=p("div"),b.innerHTML=zt,ft=a(),H=p("p"),H.textContent=Nt,dt=a(),c(I.$$.fragment),ht=a(),E=p("p"),E.innerHTML=Xt,ut=a(),V=p("p"),V.textContent=Qt,Mt=a(),c(z.$$.fragment),yt=a(),J=p("div"),J.innerHTML=Ft,wt=a(),c(N.$$.fragment),$t=a(),X=p("p"),X.textContent=Yt,jt=a(),Q=p("p"),Q.innerHTML=St,bt=a(),c(F.$$.fragment),Jt=a(),Y=p("p"),Y.innerHTML=Lt,Tt=a(),c(S.$$.fragment),Ut=a(),L=p("p"),L.textContent=Pt,vt=a(),c(P.$$.fragment),Ct=a(),A=p("p"),A.textContent=At,Zt=a(),c(q.$$.fragment),xt=a(),T=p("div"),T.innerHTML=qt,Wt=a(),D=p("p"),this.h()},l(t){const e=ae("svelte-u9bgzb",document.head);o=i(e,"META",{name:!0,content:!0}),e.forEach(s),u=n(t),$=i(t,"P",{}),Kt($).forEach(s),M=n(t),r(y.$$.fragment,t),tt=n(t),U=i(t,"P",{"data-svelte-h":!0}),m(U)!=="svelte-9fs1p5"&&(U.textContent=_t),et=n(t),v=i(t,"P",{"data-svelte-h":!0}),m(v)!=="svelte-1tgyug5"&&(v.textContent=Gt),st=n(t),r(j.$$.fragment,t),lt=n(t),C=i(t,"P",{"data-svelte-h":!0}),m(C)!=="svelte-tlz6vm"&&(C.textContent=Bt),at=n(t),Z=i(t,"UL",{"data-svelte-h":!0}),m(Z)!=="svelte-1gu43po"&&(Z.innerHTML=Ht),nt=n(t),x=i(t,"P",{"data-svelte-h":!0}),m(x)!=="svelte-1lya3k8"&&(x.textContent=It),pt=n(t),r(W.$$.fragment,t),it=n(t),r(k.$$.fragment,t),mt=n(t),R=i(t,"P",{"data-svelte-h":!0}),m(R)!=="svelte-msjy62"&&(R.innerHTML=Et),ot=n(t),r(_.$$.fragment,t),ct=n(t),G=i(t,"P",{"data-svelte-h":!0}),m(G)!=="svelte-1n3gtfx"&&(G.textContent=Vt),rt=n(t),r(B.$$.fragment,t),gt=n(t),b=i(t,"DIV",{class:!0,"data-svelte-h":!0}),m(b)!=="svelte-10bakl"&&(b.innerHTML=zt),ft=n(t),H=i(t,"P",{"data-svelte-h":!0}),m(H)!=="svelte-vb0w5q"&&(H.textContent=Nt),dt=n(t),r(I.$$.fragment,t),ht=n(t),E=i(t,"P",{"data-svelte-h":!0}),m(E)!=="svelte-1s2udfg"&&(E.innerHTML=Xt),ut=n(t),V=i(t,"P",{"data-svelte-h":!0}),m(V)!=="svelte-x30tw4"&&(V.textContent=Qt),Mt=n(t),r(z.$$.fragment,t),yt=n(t),J=i(t,"DIV",{class:!0,"data-svelte-h":!0}),m(J)!=="svelte-43wxxb"&&(J.innerHTML=Ft),wt=n(t),r(N.$$.fragment,t),$t=n(t),X=i(t,"P",{"data-svelte-h":!0}),m(X)!=="svelte-1nhrkgu"&&(X.textContent=Yt),jt=n(t),Q=i(t,"P",{"data-svelte-h":!0}),m(Q)!=="svelte-1q9460k"&&(Q.innerHTML=St),bt=n(t),r(F.$$.fragment,t),Jt=n(t),Y=i(t,"P",{"data-svelte-h":!0}),m(Y)!=="svelte-povovq"&&(Y.innerHTML=Lt),Tt=n(t),r(S.$$.fragment,t),Ut=n(t),L=i(t,"P",{"data-svelte-h":!0}),m(L)!=="svelte-1u18nel"&&(L.textContent=Pt),vt=n(t),r(P.$$.fragment,t),Ct=n(t),A=i(t,"P",{"data-svelte-h":!0}),m(A)!=="svelte-8h8zfm"&&(A.textContent=At),Zt=n(t),r(q.$$.fragment,t),xt=n(t),T=i(t,"DIV",{class:!0,"data-svelte-h":!0}),m(T)!=="svelte-43wxxb"&&(T.innerHTML=qt),Wt=n(t),D=i(t,"P",{}),Kt(D).forEach(s),this.h()},h(){K(o,"name","hf:doc:metadata"),K(o,"content",re),K(b,"class","flex justify-center"),K(J,"class","flex justify-center"),K(T,"class","flex justify-center")},m(t,e){ne(document.head,o),l(t,u,e),l(t,$,e),l(t,M,e),g(y,t,e),l(t,tt,e),l(t,U,e),l(t,et,e),l(t,v,e),l(t,st,e),g(j,t,e),l(t,lt,e),l(t,C,e),l(t,at,e),l(t,Z,e),l(t,nt,e),l(t,x,e),l(t,pt,e),g(W,t,e),l(t,it,e),g(k,t,e),l(t,mt,e),l(t,R,e),l(t,ot,e),g(_,t,e),l(t,ct,e),l(t,G,e),l(t,rt,e),g(B,t,e),l(t,gt,e),l(t,b,e),l(t,ft,e),l(t,H,e),l(t,dt,e),g(I,t,e),l(t,ht,e),l(t,E,e),l(t,ut,e),l(t,V,e),l(t,Mt,e),g(z,t,e),l(t,yt,e),l(t,J,e),l(t,wt,e),g(N,t,e),l(t,$t,e),l(t,X,e),l(t,jt,e),l(t,Q,e),l(t,bt,e),g(F,t,e),l(t,Jt,e),l(t,Y,e),l(t,Tt,e),g(S,t,e),l(t,Ut,e),l(t,L,e),l(t,vt,e),g(P,t,e),l(t,Ct,e),l(t,A,e),l(t,Zt,e),g(q,t,e),l(t,xt,e),l(t,T,e),l(t,Wt,e),l(t,D,e),kt=!0},p(t,[e]){const Dt={};e&2&&(Dt.$$scope={dirty:e,ctx:t}),j.$set(Dt)},i(t){kt||(f(y.$$.fragment,t),f(j.$$.fragment,t),f(W.$$.fragment,t),f(k.$$.fragment,t),f(_.$$.fragment,t),f(B.$$.fragment,t),f(I.$$.fragment,t),f(z.$$.fragment,t),f(N.$$.fragment,t),f(F.$$.fragment,t),f(S.$$.fragment,t),f(P.$$.fragment,t),f(q.$$.fragment,t),kt=!0)},o(t){d(y.$$.fragment,t),d(j.$$.fragment,t),d(W.$$.fragment,t),d(k.$$.fragment,t),d(_.$$.fragment,t),d(B.$$.fragment,t),d(I.$$.fragment,t),d(z.$$.fragment,t),d(N.$$.fragment,t),d(F.$$.fragment,t),d(S.$$.fragment,t),d(P.$$.fragment,t),d(q.$$.fragment,t),kt=!1},d(t){t&&(s(u),s($),s(M),s(tt),s(U),s(et),s(v),s(st),s(lt),s(C),s(at),s(Z),s(nt),s(x),s(pt),s(it),s(mt),s(R),s(ot),s(ct),s(G),s(rt),s(gt),s(b),s(ft),s(H),s(dt),s(ht),s(E),s(ut),s(V),s(Mt),s(yt),s(J),s(wt),s($t),s(X),s(jt),s(Q),s(bt),s(Jt),s(Y),s(Tt),s(Ut),s(L),s(vt),s(Ct),s(A),s(Zt),s(xt),s(T),s(Wt),s(D)),s(o),h(y,t),h(j,t),h(W,t),h(k,t),h(_,t),h(B,t),h(I,t),h(z,t),h(N,t),h(F,t),h(S,t),h(P,t),h(q,t)}}}const re='{"title":"Monocular depth estimation","local":"monocular-depth-estimation","sections":[{"title":"Depth estimation pipeline","local":"depth-estimation-pipeline","sections":[],"depth":2},{"title":"Depth estimation inference by hand","local":"depth-estimation-inference-by-hand","sections":[],"depth":2}],"depth":1}';function ge(O){return te(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ye extends se{constructor(o){super(),le(this,o,ge,ce,Ot,{})}}export{ye as component};
