import{s as ra,o as ma,n as vt}from"../chunks/scheduler.9bc65507.js";import{S as ca,i as oa,g as c,s as l,r as u,A as ua,h as o,f as a,c as n,j as la,u as f,x as y,k as na,y as fa,a as e,v as h,d,t as g,w as j,m as ha,n as da}from"../chunks/index.707bf1b6.js";import{T as kt}from"../chunks/Tip.c2ecdbf4.js";import{Y as ga}from"../chunks/Youtube.e1129c6f.js";import{C as _}from"../chunks/CodeBlock.54a9f38d.js";import{D as ja}from"../chunks/DocNotebookDropdown.41f65cb5.js";import{F as pa,M as ia}from"../chunks/Markdown.8ab98a13.js";import{H as Cs}from"../chunks/Heading.342b1fa6.js";function Ma(v){let p,b,i='<a href="../model_doc/audio-spectrogram-transformer">Audio Spectrogram Transformer</a>, <a href="../model_doc/data2vec-audio">Data2VecAudio</a>, <a href="../model_doc/hubert">Hubert</a>, <a href="../model_doc/sew">SEW</a>, <a href="../model_doc/sew-d">SEW-D</a>, <a href="../model_doc/unispeech">UniSpeech</a>, <a href="../model_doc/unispeech-sat">UniSpeechSat</a>, <a href="../model_doc/wav2vec2">Wav2Vec2</a>, <a href="../model_doc/wav2vec2-conformer">Wav2Vec2-Conformer</a>, <a href="../model_doc/wavlm">WavLM</a>, <a href="../model_doc/whisper">Whisper</a>';return{c(){p=ha(`このチュートリアルで説明するタスクは、次のモデル アーキテクチャでサポートされています。

`),b=c("p"),b.innerHTML=i},l(M){p=da(M,`このチュートリアルで説明するタスクは、次のモデル アーキテクチャでサポートされています。

`),b=o(M,"P",{"data-svelte-h":!0}),y(b)!=="svelte-1x7pfyj"&&(b.innerHTML=i)},m(M,$){e(M,p,$),e(M,b,$)},p:vt,d(M){M&&(a(p),a(b))}}}function ba(v){let p,b='<a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a> を使用したモデルの微調整に慣れていない場合は、<a href="../training#train-with-pytorch-trainer">こちら</a> の基本的なチュートリアルをご覧ください。';return{c(){p=c("p"),p.innerHTML=b},l(i){p=o(i,"P",{"data-svelte-h":!0}),y(p)!=="svelte-u6v1yb"&&(p.innerHTML=b)},m(i,M){e(i,p,M)},p:vt,d(i){i&&a(p)}}}function ya(v){let p,b,i,M='これでモデルのトレーニングを開始する準備が整いました。 <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoModelForAudioClassification">AutoModelForAudioClassification</a> を使用して、予期されるラベルの数とラベル マッピングを使用して Wav2Vec2 を読み込みます。',$,U,k,Z,W="この時点で残っている手順は次の 3 つだけです。",C,J,V='<li><a href="/docs/transformers/main/ja/main_classes/trainer#transformers.TrainingArguments">TrainingArguments</a> でトレーニング ハイパーパラメータを定義します。唯一の必須パラメータは、モデルの保存場所を指定する <code>output_dir</code> です。 <code>push_to_hub=True</code>を設定して、このモデルをハブにプッシュします (モデルをアップロードするには、Hugging Face にサインインする必要があります)。各エポックの終了時に、<code>トレーナー</code> は精度を評価し、トレーニング チェックポイントを保存します。</li> <li>トレーニング引数を、モデル、データセット、トークナイザー、データ照合器、および <code>compute_metrics</code> 関数とともに <a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer">Trainer</a> に渡します。</li> <li><a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.train">train()</a> を呼び出してモデルを微調整します。</li>',I,T,R,m,x='トレーニングが完了したら、 <a href="/docs/transformers/main/ja/main_classes/trainer#transformers.Trainer.push_to_hub">push_to_hub()</a> メソッドを使用してモデルをハブに共有し、誰もがモデルを使用できるようにします。',H,G,X;return p=new kt({props:{$$slots:{default:[ba]},$$scope:{ctx:v}}}),U=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24lMkMlMjBUcmFpbmluZ0FyZ3VtZW50cyUyQyUyMFRyYWluZXIlMEElMEFudW1fbGFiZWxzJTIwJTNEJTIwbGVuKGlkMmxhYmVsKSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQXVkaW9DbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlJTIyJTJDJTIwbnVtX2xhYmVscyUzRG51bV9sYWJlbHMlMkMlMjBsYWJlbDJpZCUzRGxhYmVsMmlkJTJDJTIwaWQybGFiZWwlM0RpZDJsYWJlbCUwQSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForAudioClassification, TrainingArguments, Trainer

<span class="hljs-meta">&gt;&gt;&gt; </span>num_labels = <span class="hljs-built_in">len</span>(id2label)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(
<span class="hljs-meta">... </span>    <span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>, num_labels=num_labels, label2id=label2id, id2label=id2label
<span class="hljs-meta">... </span>)`,wrap:!1}}),T=new _({props:{code:"dHJhaW5pbmdfYXJncyUyMCUzRCUyMFRyYWluaW5nQXJndW1lbnRzKCUwQSUyMCUyMCUyMCUyMG91dHB1dF9kaXIlM0QlMjJteV9hd2Vzb21lX21pbmRfbW9kZWwlMjIlMkMlMEElMjAlMjAlMjAlMjBldmFsdWF0aW9uX3N0cmF0ZWd5JTNEJTIyZXBvY2glMjIlMkMlMEElMjAlMjAlMjAlMjBzYXZlX3N0cmF0ZWd5JTNEJTIyZXBvY2glMjIlMkMlMEElMjAlMjAlMjAlMjBsZWFybmluZ19yYXRlJTNEM2UtNSUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfdHJhaW5fYmF0Y2hfc2l6ZSUzRDMyJTJDJTBBJTIwJTIwJTIwJTIwZ3JhZGllbnRfYWNjdW11bGF0aW9uX3N0ZXBzJTNENCUyQyUwQSUyMCUyMCUyMCUyMHBlcl9kZXZpY2VfZXZhbF9iYXRjaF9zaXplJTNEMzIlMkMlMEElMjAlMjAlMjAlMjBudW1fdHJhaW5fZXBvY2hzJTNEMTAlMkMlMEElMjAlMjAlMjAlMjB3YXJtdXBfcmF0aW8lM0QwLjElMkMlMEElMjAlMjAlMjAlMjBsb2dnaW5nX3N0ZXBzJTNEMTAlMkMlMEElMjAlMjAlMjAlMjBsb2FkX2Jlc3RfbW9kZWxfYXRfZW5kJTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMG1ldHJpY19mb3JfYmVzdF9tb2RlbCUzRCUyMmFjY3VyYWN5JTIyJTJDJTBBJTIwJTIwJTIwJTIwcHVzaF90b19odWIlM0RUcnVlJTJDJTBBKSUwQSUwQXRyYWluZXIlMjAlM0QlMjBUcmFpbmVyKCUwQSUyMCUyMCUyMCUyMG1vZGVsJTNEbW9kZWwlMkMlMEElMjAlMjAlMjAlMjBhcmdzJTNEdHJhaW5pbmdfYXJncyUyQyUwQSUyMCUyMCUyMCUyMHRyYWluX2RhdGFzZXQlM0RlbmNvZGVkX21pbmRzJTVCJTIydHJhaW4lMjIlNUQlMkMlMEElMjAlMjAlMjAlMjBldmFsX2RhdGFzZXQlM0RlbmNvZGVkX21pbmRzJTVCJTIydGVzdCUyMiU1RCUyQyUwQSUyMCUyMCUyMCUyMHRva2VuaXplciUzRGZlYXR1cmVfZXh0cmFjdG9yJTJDJTBBJTIwJTIwJTIwJTIwY29tcHV0ZV9tZXRyaWNzJTNEY29tcHV0ZV9tZXRyaWNzJTJDJTBBKSUwQSUwQXRyYWluZXIudHJhaW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>training_args = TrainingArguments(
<span class="hljs-meta">... </span>    output_dir=<span class="hljs-string">&quot;my_awesome_mind_model&quot;</span>,
<span class="hljs-meta">... </span>    evaluation_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    save_strategy=<span class="hljs-string">&quot;epoch&quot;</span>,
<span class="hljs-meta">... </span>    learning_rate=<span class="hljs-number">3e-5</span>,
<span class="hljs-meta">... </span>    per_device_train_batch_size=<span class="hljs-number">32</span>,
<span class="hljs-meta">... </span>    gradient_accumulation_steps=<span class="hljs-number">4</span>,
<span class="hljs-meta">... </span>    per_device_eval_batch_size=<span class="hljs-number">32</span>,
<span class="hljs-meta">... </span>    num_train_epochs=<span class="hljs-number">10</span>,
<span class="hljs-meta">... </span>    warmup_ratio=<span class="hljs-number">0.1</span>,
<span class="hljs-meta">... </span>    logging_steps=<span class="hljs-number">10</span>,
<span class="hljs-meta">... </span>    load_best_model_at_end=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>    metric_for_best_model=<span class="hljs-string">&quot;accuracy&quot;</span>,
<span class="hljs-meta">... </span>    push_to_hub=<span class="hljs-literal">True</span>,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer = Trainer(
<span class="hljs-meta">... </span>    model=model,
<span class="hljs-meta">... </span>    args=training_args,
<span class="hljs-meta">... </span>    train_dataset=encoded_minds[<span class="hljs-string">&quot;train&quot;</span>],
<span class="hljs-meta">... </span>    eval_dataset=encoded_minds[<span class="hljs-string">&quot;test&quot;</span>],
<span class="hljs-meta">... </span>    tokenizer=feature_extractor,
<span class="hljs-meta">... </span>    compute_metrics=compute_metrics,
<span class="hljs-meta">... </span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.train()`,wrap:!1}}),G=new _({props:{code:"dHJhaW5lci5wdXNoX3RvX2h1Yigp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>trainer.push_to_hub()',wrap:!1}}),{c(){u(p.$$.fragment),b=l(),i=c("p"),i.innerHTML=M,$=l(),u(U.$$.fragment),k=l(),Z=c("p"),Z.textContent=W,C=l(),J=c("ol"),J.innerHTML=V,I=l(),u(T.$$.fragment),R=l(),m=c("p"),m.innerHTML=x,H=l(),u(G.$$.fragment)},l(r){f(p.$$.fragment,r),b=n(r),i=o(r,"P",{"data-svelte-h":!0}),y(i)!=="svelte-1ygen11"&&(i.innerHTML=M),$=n(r),f(U.$$.fragment,r),k=n(r),Z=o(r,"P",{"data-svelte-h":!0}),y(Z)!=="svelte-1j8bgyv"&&(Z.textContent=W),C=n(r),J=o(r,"OL",{"data-svelte-h":!0}),y(J)!=="svelte-1w5x017"&&(J.innerHTML=V),I=n(r),f(T.$$.fragment,r),R=n(r),m=o(r,"P",{"data-svelte-h":!0}),y(m)!=="svelte-ngexm3"&&(m.innerHTML=x),H=n(r),f(G.$$.fragment,r)},m(r,w){h(p,r,w),e(r,b,w),e(r,i,w),e(r,$,w),h(U,r,w),e(r,k,w),e(r,Z,w),e(r,C,w),e(r,J,w),e(r,I,w),h(T,r,w),e(r,R,w),e(r,m,w),e(r,H,w),h(G,r,w),X=!0},p(r,w){const Is={};w&2&&(Is.$$scope={dirty:w,ctx:r}),p.$set(Is)},i(r){X||(d(p.$$.fragment,r),d(U.$$.fragment,r),d(T.$$.fragment,r),d(G.$$.fragment,r),X=!0)},o(r){g(p.$$.fragment,r),g(U.$$.fragment,r),g(T.$$.fragment,r),g(G.$$.fragment,r),X=!1},d(r){r&&(a(b),a(i),a($),a(k),a(Z),a(C),a(J),a(I),a(R),a(m),a(H)),j(p,r),j(U,r),j(T,r),j(G,r)}}}function $a(v){let p,b;return p=new ia({props:{$$slots:{default:[ya]},$$scope:{ctx:v}}}),{c(){u(p.$$.fragment)},l(i){f(p.$$.fragment,i)},m(i,M){h(p,i,M),b=!0},p(i,M){const $={};M&2&&($.$$scope={dirty:M,ctx:i}),p.$set($)},i(i){b||(d(p.$$.fragment,i),b=!0)},o(i){g(p.$$.fragment,i),b=!1},d(i){j(p,i)}}}function wa(v){let p,b='音声分類用のモデルを微調整する方法の詳細な例については、対応する <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/audio_classification.ipynb" rel="nofollow">PyTorch notebook</a>.';return{c(){p=c("p"),p.innerHTML=b},l(i){p=o(i,"P",{"data-svelte-h":!0}),y(p)!=="svelte-nc5jhf"&&(p.innerHTML=b)},m(i,M){e(i,p,M)},p:vt,d(i){i&&a(p)}}}function Ja(v){let p,b="特徴抽出器をロードしてオーディオ ファイルを前処理し、<code>input</code>を PyTorch テンソルとして返します。",i,M,$,U,k="入力をモデルに渡し、ロジットを返します。",Z,W,C,J,V="最も高い確率でクラスを取得し、モデルの <code>id2label</code> マッピングを使用してそれをラベルに変換します。",I,T,R;return M=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyc3RldmhsaXUlMkZteV9hd2Vzb21lX21pbmRzX21vZGVsJTIyKSUwQWlucHV0cyUyMCUzRCUyMGZlYXR1cmVfZXh0cmFjdG9yKGRhdGFzZXQlNUIwJTVEJTVCJTIyYXVkaW8lMjIlNUQlNUIlMjJhcnJheSUyMiU1RCUyQyUyMHNhbXBsaW5nX3JhdGUlM0RzYW1wbGluZ19yYXRlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_minds_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = feature_extractor(dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;array&quot;</span>], sampling_rate=sampling_rate, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)`,wrap:!1}}),W=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24lMEElMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckF1ZGlvQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMnN0ZXZobGl1JTJGbXlfYXdlc29tZV9taW5kc19tb2RlbCUyMiklMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForAudioClassification

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModelForAudioClassification.from_pretrained(<span class="hljs-string">&quot;stevhliu/my_awesome_minds_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits`,wrap:!1}}),T=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEElMEFwcmVkaWN0ZWRfY2xhc3NfaWRzJTIwJTNEJTIwdG9yY2guYXJnbWF4KGxvZ2l0cykuaXRlbSgpJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2NsYXNzX2lkcyU1RCUwQXByZWRpY3RlZF9sYWJlbA==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch

<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_ids = torch.argmax(logits).item()
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = model.config.id2label[predicted_class_ids]
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label
<span class="hljs-string">&#x27;cash_deposit&#x27;</span>`,wrap:!1}}),{c(){p=c("p"),p.innerHTML=b,i=l(),u(M.$$.fragment),$=l(),U=c("p"),U.textContent=k,Z=l(),u(W.$$.fragment),C=l(),J=c("p"),J.innerHTML=V,I=l(),u(T.$$.fragment)},l(m){p=o(m,"P",{"data-svelte-h":!0}),y(p)!=="svelte-1lbojgb"&&(p.innerHTML=b),i=n(m),f(M.$$.fragment,m),$=n(m),U=o(m,"P",{"data-svelte-h":!0}),y(U)!=="svelte-jwtn5w"&&(U.textContent=k),Z=n(m),f(W.$$.fragment,m),C=n(m),J=o(m,"P",{"data-svelte-h":!0}),y(J)!=="svelte-x53zvr"&&(J.innerHTML=V),I=n(m),f(T.$$.fragment,m)},m(m,x){e(m,p,x),e(m,i,x),h(M,m,x),e(m,$,x),e(m,U,x),e(m,Z,x),h(W,m,x),e(m,C,x),e(m,J,x),e(m,I,x),h(T,m,x),R=!0},p:vt,i(m){R||(d(M.$$.fragment,m),d(W.$$.fragment,m),d(T.$$.fragment,m),R=!0)},o(m){g(M.$$.fragment,m),g(W.$$.fragment,m),g(T.$$.fragment,m),R=!1},d(m){m&&(a(p),a(i),a($),a(U),a(Z),a(C),a(J),a(I)),j(M,m),j(W,m),j(T,m)}}}function Ta(v){let p,b;return p=new ia({props:{$$slots:{default:[Ja]},$$scope:{ctx:v}}}),{c(){u(p.$$.fragment)},l(i){f(p.$$.fragment,i)},m(i,M){h(p,i,M),b=!0},p(i,M){const $={};M&2&&($.$$scope={dirty:M,ctx:i}),p.$set($)},i(i){b||(d(p.$$.fragment,i),b=!0)},o(i){g(p.$$.fragment,i),b=!1},d(i){j(p,i)}}}function _a(v){let p,b,i,M,$,U,k,Z,W,C,J,V="音声分類では、テキストと同様に、入力データから出力されたクラス ラベルを割り当てます。唯一の違いは、テキスト入力の代わりに生のオーディオ波形があることです。音声分類の実際的な応用例には、話者の意図、言語分類、さらには音による動物の種類の識別などがあります。",I,T,R="このガイドでは、次の方法を説明します。",m,x,H='<li><a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">MInDS-14</a> データセットで <a href="https://huggingface.co/facebook/wav2vec2-base" rel="nofollow">Wav2Vec2</a> を微調整して話者の意図を分類します。</li> <li>微調整したモデルを推論に使用します。</li>',G,X,r,w,Is="始める前に、必要なライブラリがすべてインストールされていることを確認してください。",Rs,B,Gs,E,Ct="モデルをアップロードしてコミュニティと共有できるように、Hugging Face アカウントにログインすることをお勧めします。プロンプトが表示されたら、トークンを入力してログインします。",Vs,Q,Ys,A,Fs,z,It="まず、🤗 データセット ライブラリから MInDS-14 データセットをロードします。",Ns,q,Hs,S,Xt="<code>train_test_split</code> メソッドを使用して、データセットの <code>train</code> をより小さなトレインとテスト セットに分割します。これにより、完全なデータセットにさらに時間を費やす前に、実験してすべてが機能することを確認する機会が得られます。",Bs,L,Es,P,Rt="次に、データセットを見てみましょう。",Qs,D,As,K,Gt="データセットには<code>lang_id</code>や<code>english_transcription</code>などの多くの有用な情報が含まれていますが、このガイドでは<code>audio</code>と<code>intent_class</code>に焦点を当てます。 <code>remove_columns</code> メソッドを使用して他の列を削除します。",zs,O,qs,ss,Vt="ここで例を見てみましょう。",Ss,ts,Ls,as,Yt="次の 2 つのフィールドがあります。",Ps,es,Ft="<li><code>audio</code>: 音声ファイルをロードしてリサンプリングするために呼び出す必要がある音声信号の 1 次元の <code>array</code>。</li> <li><code>intent_class</code>: スピーカーのインテントのクラス ID を表します。</li>",Ds,ls,Nt="モデルがラベル ID からラベル名を取得しやすくするために、ラベル名を整数に、またはその逆にマップする辞書を作成します。",Ks,ns,Os,ps,Ht="これで、ラベル ID をラベル名に変換できるようになりました。",st,is,tt,rs,at,ms,Bt="次のステップでは、Wav2Vec2 特徴抽出プログラムをロードしてオーディオ信号を処理します。",et,cs,lt,os,Et='MInDS-14 データセットのサンプリング レートは 8000khz です (この情報は <a href="https://huggingface.co/datasets/PolyAI/minds14" rel="nofollow">データセット カード</a> で確認できます)。つまり、データセットを再サンプリングする必要があります。事前トレーニングされた Wav2Vec2 モデルを使用するには、16000kHz に設定します。',nt,us,pt,fs,Qt="次に、次の前処理関数を作成します。",it,hs,At='<li><code>audio</code>列を呼び出してロードし、必要に応じてオーディオ ファイルをリサンプリングします。</li> <li>オーディオ ファイルのサンプリング レートが、モデルが事前トレーニングされたオーディオ データのサンプリング レートと一致するかどうかを確認します。この情報は、Wav2Vec2 <a href="https://huggingface.co/facebook/wav2vec2-base" rel="nofollow">モデル カード</a> で見つけることができます。</li> <li>入力の最大長を設定して、長い入力を切り捨てずにバッチ処理します。</li>',rt,ds,mt,gs,zt="データセット全体に前処理関数を適用するには、🤗 Datasets <code>map</code> 関数を使用します。 <code>batched=True</code> を設定してデータセットの複数の要素を一度に処理することで、<code>map</code> を高速化できます。不要な列を削除し、<code>intent_class</code> の名前を <code>label</code> に変更します。これはモデルが期待する名前であるためです。",ct,js,ot,Ms,ut,bs,qt='トレーニング中にメトリクスを含めると、多くの場合、モデルのパフォーマンスを評価するのに役立ちます。 🤗 <a href="https://huggingface.co/docs/evaluate/index" rel="nofollow">Evaluate</a> ライブラリを使用して、評価メソッドをすばやくロードできます。このタスクでは、<a href="https://huggingface.co/spaces/evaluate-metric/accuracy" rel="nofollow">accuracy</a> メトリクスを読み込みます (🤗 Evaluate <a href="https://huggingface.co/docs/evaluate/a_quick_tour" rel="nofollow">クイック ツアー</a> を参照してください) メトリクスの読み込みと計算方法の詳細については、次を参照してください。',ft,ys,ht,$s,St="次に、予測とラベルを <code>compute</code> に渡して精度を計算する関数を作成します。",dt,ws,gt,Js,Lt="これで<code>compute_metrics</code>関数の準備が整いました。トレーニングをセットアップするときにこの関数に戻ります。",jt,Ts,Mt,Y,bt,F,yt,_s,$t,xs,Pt="モデルを微調整したので、それを推論に使用できるようになりました。",wt,Us,Dt="推論を実行したい音声ファイルをロードします。必要に応じて、オーディオ ファイルのサンプリング レートをモデルのサンプリング レートと一致するようにリサンプリングすることを忘れないでください。",Jt,Ws,Tt,Zs,Kt='推論用に微調整されたモデルを試す最も簡単な方法は、それを <a href="/docs/transformers/main/ja/main_classes/pipelines#transformers.pipeline">pipeline()</a> で使用することです。モデルを使用して音声分類用の<code>pipeline</code>をインスタンス化し、それに音声ファイルを渡します。',_t,vs,xt,ks,Ot="必要に応じて、<code>pipeline</code> の結果を手動で複製することもできます。",Ut,N,Wt,Xs,Zt;return $=new Cs({props:{title:"Audio classification",local:"audio-classification",headingTag:"h1"}}),k=new ja({props:{classNames:"absolute z-10 right-0 top-0",options:[{label:"Mixed",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/audio_classification.ipynb"},{label:"PyTorch",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/pytorch/audio_classification.ipynb"},{label:"TensorFlow",value:"https://colab.research.google.com/github/huggingface/notebooks/blob/main/transformers_doc/ja/tensorflow/audio_classification.ipynb"},{label:"Mixed",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/audio_classification.ipynb"},{label:"PyTorch",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/pytorch/audio_classification.ipynb"},{label:"TensorFlow",value:"https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/main/transformers_doc/ja/tensorflow/audio_classification.ipynb"}]}}),W=new ga({props:{id:"KWwzcmG98Ds"}}),X=new kt({props:{$$slots:{default:[Ma]},$$scope:{ctx:v}}}),B=new _({props:{code:"cGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGRhdGFzZXRzJTIwZXZhbHVhdGU=",highlighted:"pip install transformers datasets evaluate",wrap:!1}}),Q=new _({props:{code:"ZnJvbSUyMGh1Z2dpbmdmYWNlX2h1YiUyMGltcG9ydCUyMG5vdGVib29rX2xvZ2luJTBBJTBBbm90ZWJvb2tfbG9naW4oKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> huggingface_hub <span class="hljs-keyword">import</span> notebook_login

<span class="hljs-meta">&gt;&gt;&gt; </span>notebook_login()`,wrap:!1}}),A=new Cs({props:{title:"Load MInDS-14 dataset",local:"load-minds-14-dataset",headingTag:"h2"}}),q=new _({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFtaW5kcyUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJQb2x5QUklMkZtaW5kczE0JTIyJTJDJTIwbmFtZSUzRCUyMmVuLVVTJTIyJTJDJTIwc3BsaXQlM0QlMjJ0cmFpbiUyMik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>minds = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)`,wrap:!1}}),L=new _({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy50cmFpbl90ZXN0X3NwbGl0KHRlc3Rfc2l6ZSUzRDAuMik=",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.train_test_split(test_size=<span class="hljs-number">0.2</span>)',wrap:!1}}),D=new _({props:{code:"bWluZHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds
DatasetDict({
    train: Dataset({
        features: [<span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;transcription&#x27;</span>, <span class="hljs-string">&#x27;english_transcription&#x27;</span>, <span class="hljs-string">&#x27;intent_class&#x27;</span>, <span class="hljs-string">&#x27;lang_id&#x27;</span>],
        num_rows: <span class="hljs-number">450</span>
    })
    test: Dataset({
        features: [<span class="hljs-string">&#x27;path&#x27;</span>, <span class="hljs-string">&#x27;audio&#x27;</span>, <span class="hljs-string">&#x27;transcription&#x27;</span>, <span class="hljs-string">&#x27;english_transcription&#x27;</span>, <span class="hljs-string">&#x27;intent_class&#x27;</span>, <span class="hljs-string">&#x27;lang_id&#x27;</span>],
        num_rows: <span class="hljs-number">113</span>
    })
})`,wrap:!1}}),O=new _({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy5yZW1vdmVfY29sdW1ucyglNUIlMjJwYXRoJTIyJTJDJTIwJTIydHJhbnNjcmlwdGlvbiUyMiUyQyUyMCUyMmVuZ2xpc2hfdHJhbnNjcmlwdGlvbiUyMiUyQyUyMCUyMmxhbmdfaWQlMjIlNUQp",highlighted:'<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.remove_columns([<span class="hljs-string">&quot;path&quot;</span>, <span class="hljs-string">&quot;transcription&quot;</span>, <span class="hljs-string">&quot;english_transcription&quot;</span>, <span class="hljs-string">&quot;lang_id&quot;</span>])',wrap:!1}}),ts=new _({props:{code:"bWluZHMlNUIlMjJ0cmFpbiUyMiU1RCU1QjAlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        ,  <span class="hljs-number">0.</span>        , ..., -<span class="hljs-number">0.00048828</span>,
         -<span class="hljs-number">0.00024414</span>, -<span class="hljs-number">0.00024414</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">8000</span>},
 <span class="hljs-string">&#x27;intent_class&#x27;</span>: <span class="hljs-number">2</span>}`,wrap:!1}}),ns=new _({props:{code:"bGFiZWxzJTIwJTNEJTIwbWluZHMlNUIlMjJ0cmFpbiUyMiU1RC5mZWF0dXJlcyU1QiUyMmludGVudF9jbGFzcyUyMiU1RC5uYW1lcyUwQWxhYmVsMmlkJTJDJTIwaWQybGFiZWwlMjAlM0QlMjBkaWN0KCklMkMlMjBkaWN0KCklMEFmb3IlMjBpJTJDJTIwbGFiZWwlMjBpbiUyMGVudW1lcmF0ZShsYWJlbHMpJTNBJTBBJTIwJTIwJTIwJTIwbGFiZWwyaWQlNUJsYWJlbCU1RCUyMCUzRCUyMHN0cihpKSUwQSUyMCUyMCUyMCUyMGlkMmxhYmVsJTVCc3RyKGkpJTVEJTIwJTNEJTIwbGFiZWw=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>labels = minds[<span class="hljs-string">&quot;train&quot;</span>].features[<span class="hljs-string">&quot;intent_class&quot;</span>].names
<span class="hljs-meta">&gt;&gt;&gt; </span>label2id, id2label = <span class="hljs-built_in">dict</span>(), <span class="hljs-built_in">dict</span>()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">for</span> i, label <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(labels):
<span class="hljs-meta">... </span>    label2id[label] = <span class="hljs-built_in">str</span>(i)
<span class="hljs-meta">... </span>    id2label[<span class="hljs-built_in">str</span>(i)] = label`,wrap:!1}}),is=new _({props:{code:"aWQybGFiZWwlNUJzdHIoMiklNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>id2label[<span class="hljs-built_in">str</span>(<span class="hljs-number">2</span>)]
<span class="hljs-string">&#x27;app_error&#x27;</span>`,wrap:!1}}),rs=new Cs({props:{title:"Preprocess",local:"preprocess",headingTag:"h2"}}),cs=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9GZWF0dXJlRXh0cmFjdG9yJTBBJTBBZmVhdHVyZV9leHRyYWN0b3IlMjAlM0QlMjBBdXRvRmVhdHVyZUV4dHJhY3Rvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZ3YXYydmVjMi1iYXNlJTIyKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoFeatureExtractor

<span class="hljs-meta">&gt;&gt;&gt; </span>feature_extractor = AutoFeatureExtractor.from_pretrained(<span class="hljs-string">&quot;facebook/wav2vec2-base&quot;</span>)`,wrap:!1}}),us=new _({props:{code:"bWluZHMlMjAlM0QlMjBtaW5kcy5jYXN0X2NvbHVtbiglMjJhdWRpbyUyMiUyQyUyMEF1ZGlvKHNhbXBsaW5nX3JhdGUlM0QxNl8wMDApKSUwQW1pbmRzJTVCJTIydHJhaW4lMjIlNUQlNUIwJTVE",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>minds = minds.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16_000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>minds[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">0</span>]
{<span class="hljs-string">&#x27;audio&#x27;</span>: {<span class="hljs-string">&#x27;array&#x27;</span>: array([ <span class="hljs-number">2.2098757e-05</span>,  <span class="hljs-number">4.6582241e-05</span>, -<span class="hljs-number">2.2803260e-05</span>, ...,
         -<span class="hljs-number">2.8419291e-04</span>, -<span class="hljs-number">2.3305941e-04</span>, -<span class="hljs-number">1.1425107e-04</span>], dtype=float32),
  <span class="hljs-string">&#x27;path&#x27;</span>: <span class="hljs-string">&#x27;/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~APP_ERROR/602b9a5fbb1e6d0fbce91f52.wav&#x27;</span>,
  <span class="hljs-string">&#x27;sampling_rate&#x27;</span>: <span class="hljs-number">16000</span>},
 <span class="hljs-string">&#x27;intent_class&#x27;</span>: <span class="hljs-number">2</span>}`,wrap:!1}}),ds=new _({props:{code:"ZGVmJTIwcHJlcHJvY2Vzc19mdW5jdGlvbihleGFtcGxlcyklM0ElMEElMjAlMjAlMjAlMjBhdWRpb19hcnJheXMlMjAlM0QlMjAlNUJ4JTVCJTIyYXJyYXklMjIlNUQlMjBmb3IlMjB4JTIwaW4lMjBleGFtcGxlcyU1QiUyMmF1ZGlvJTIyJTVEJTVEJTBBJTIwJTIwJTIwJTIwaW5wdXRzJTIwJTNEJTIwZmVhdHVyZV9leHRyYWN0b3IoJTBBJTIwJTIwJTIwJTIwJTIwJTIwJTIwJTIwYXVkaW9fYXJyYXlzJTJDJTIwc2FtcGxpbmdfcmF0ZSUzRGZlYXR1cmVfZXh0cmFjdG9yLnNhbXBsaW5nX3JhdGUlMkMlMjBtYXhfbGVuZ3RoJTNEMTYwMDAlMkMlMjB0cnVuY2F0aW9uJTNEVHJ1ZSUwQSUyMCUyMCUyMCUyMCklMEElMjAlMjAlMjAlMjByZXR1cm4lMjBpbnB1dHM=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_function</span>(<span class="hljs-params">examples</span>):
<span class="hljs-meta">... </span>    audio_arrays = [x[<span class="hljs-string">&quot;array&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> examples[<span class="hljs-string">&quot;audio&quot;</span>]]
<span class="hljs-meta">... </span>    inputs = feature_extractor(
<span class="hljs-meta">... </span>        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=<span class="hljs-number">16000</span>, truncation=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>    )
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> inputs`,wrap:!1}}),js=new _({props:{code:"ZW5jb2RlZF9taW5kcyUyMCUzRCUyMG1pbmRzLm1hcChwcmVwcm9jZXNzX2Z1bmN0aW9uJTJDJTIwcmVtb3ZlX2NvbHVtbnMlM0QlMjJhdWRpbyUyMiUyQyUyMGJhdGNoZWQlM0RUcnVlKSUwQWVuY29kZWRfbWluZHMlMjAlM0QlMjBlbmNvZGVkX21pbmRzLnJlbmFtZV9jb2x1bW4oJTIyaW50ZW50X2NsYXNzJTIyJTJDJTIwJTIybGFiZWwlMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_minds = minds.<span class="hljs-built_in">map</span>(preprocess_function, remove_columns=<span class="hljs-string">&quot;audio&quot;</span>, batched=<span class="hljs-literal">True</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>encoded_minds = encoded_minds.rename_column(<span class="hljs-string">&quot;intent_class&quot;</span>, <span class="hljs-string">&quot;label&quot;</span>)`,wrap:!1}}),Ms=new Cs({props:{title:"Evaluate",local:"evaluate",headingTag:"h2"}}),ys=new _({props:{code:"aW1wb3J0JTIwZXZhbHVhdGUlMEElMEFhY2N1cmFjeSUyMCUzRCUyMGV2YWx1YXRlLmxvYWQoJTIyYWNjdXJhY3klMjIp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> evaluate

<span class="hljs-meta">&gt;&gt;&gt; </span>accuracy = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)`,wrap:!1}}),ws=new _({props:{code:"aW1wb3J0JTIwbnVtcHklMjBhcyUyMG5wJTBBJTBBJTBBZGVmJTIwY29tcHV0ZV9tZXRyaWNzKGV2YWxfcHJlZCklM0ElMEElMjAlMjAlMjAlMjBwcmVkaWN0aW9ucyUyMCUzRCUyMG5wLmFyZ21heChldmFsX3ByZWQucHJlZGljdGlvbnMlMkMlMjBheGlzJTNEMSklMEElMjAlMjAlMjAlMjByZXR1cm4lMjBhY2N1cmFjeS5jb21wdXRlKHByZWRpY3Rpb25zJTNEcHJlZGljdGlvbnMlMkMlMjByZWZlcmVuY2VzJTNEZXZhbF9wcmVkLmxhYmVsX2lkcyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):
<span class="hljs-meta">... </span>    predictions = np.argmax(eval_pred.predictions, axis=<span class="hljs-number">1</span>)
<span class="hljs-meta">... </span>    <span class="hljs-keyword">return</span> accuracy.compute(predictions=predictions, references=eval_pred.label_ids)`,wrap:!1}}),Ts=new Cs({props:{title:"Train",local:"train",headingTag:"h2"}}),Y=new pa({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[$a]},$$scope:{ctx:v}}}),F=new kt({props:{$$slots:{default:[wa]},$$scope:{ctx:v}}}),_s=new Cs({props:{title:"Inference",local:"inference",headingTag:"h2"}}),Ws=new _({props:{code:"ZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTJDJTIwQXVkaW8lMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMlBvbHlBSSUyRm1pbmRzMTQlMjIlMkMlMjBuYW1lJTNEJTIyZW4tVVMlMjIlMkMlMjBzcGxpdCUzRCUyMnRyYWluJTIyKSUwQWRhdGFzZXQlMjAlM0QlMjBkYXRhc2V0LmNhc3RfY29sdW1uKCUyMmF1ZGlvJTIyJTJDJTIwQXVkaW8oc2FtcGxpbmdfcmF0ZSUzRDE2MDAwKSklMEFzYW1wbGluZ19yYXRlJTIwJTNEJTIwZGF0YXNldC5mZWF0dXJlcyU1QiUyMmF1ZGlvJTIyJTVELnNhbXBsaW5nX3JhdGUlMEFhdWRpb19maWxlJTIwJTNEJTIwZGF0YXNldCU1QjAlNUQlNUIlMjJhdWRpbyUyMiU1RCU1QiUyMnBhdGglMjIlNUQ=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset, Audio

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;PolyAI/minds14&quot;</span>, name=<span class="hljs-string">&quot;en-US&quot;</span>, split=<span class="hljs-string">&quot;train&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = dataset.cast_column(<span class="hljs-string">&quot;audio&quot;</span>, Audio(sampling_rate=<span class="hljs-number">16000</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span>sampling_rate = dataset.features[<span class="hljs-string">&quot;audio&quot;</span>].sampling_rate
<span class="hljs-meta">&gt;&gt;&gt; </span>audio_file = dataset[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;audio&quot;</span>][<span class="hljs-string">&quot;path&quot;</span>]`,wrap:!1}}),vs=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMHBpcGVsaW5lJTBBJTBBY2xhc3NpZmllciUyMCUzRCUyMHBpcGVsaW5lKCUyMmF1ZGlvLWNsYXNzaWZpY2F0aW9uJTIyJTJDJTIwbW9kZWwlM0QlMjJzdGV2aGxpdSUyRm15X2F3ZXNvbWVfbWluZHNfbW9kZWwlMjIpJTBBY2xhc3NpZmllcihhdWRpb19maWxlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline

<span class="hljs-meta">&gt;&gt;&gt; </span>classifier = pipeline(<span class="hljs-string">&quot;audio-classification&quot;</span>, model=<span class="hljs-string">&quot;stevhliu/my_awesome_minds_model&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>classifier(audio_file)
[
    {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.09766869246959686</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;cash_deposit&#x27;</span>},
    {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.07998877018690109</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;app_error&#x27;</span>},
    {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.0781070664525032</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;joint_account&#x27;</span>},
    {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.07667109370231628</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;pay_bill&#x27;</span>},
    {<span class="hljs-string">&#x27;score&#x27;</span>: <span class="hljs-number">0.0755252093076706</span>, <span class="hljs-string">&#x27;label&#x27;</span>: <span class="hljs-string">&#x27;balance&#x27;</span>}
]`,wrap:!1}}),N=new pa({props:{pytorch:!0,tensorflow:!1,jax:!1,$$slots:{pytorch:[Ta]},$$scope:{ctx:v}}}),{c(){p=c("meta"),b=l(),i=c("p"),M=l(),u($.$$.fragment),U=l(),u(k.$$.fragment),Z=l(),u(W.$$.fragment),C=l(),J=c("p"),J.textContent=V,I=l(),T=c("p"),T.textContent=R,m=l(),x=c("ol"),x.innerHTML=H,G=l(),u(X.$$.fragment),r=l(),w=c("p"),w.textContent=Is,Rs=l(),u(B.$$.fragment),Gs=l(),E=c("p"),E.textContent=Ct,Vs=l(),u(Q.$$.fragment),Ys=l(),u(A.$$.fragment),Fs=l(),z=c("p"),z.textContent=It,Ns=l(),u(q.$$.fragment),Hs=l(),S=c("p"),S.innerHTML=Xt,Bs=l(),u(L.$$.fragment),Es=l(),P=c("p"),P.textContent=Rt,Qs=l(),u(D.$$.fragment),As=l(),K=c("p"),K.innerHTML=Gt,zs=l(),u(O.$$.fragment),qs=l(),ss=c("p"),ss.textContent=Vt,Ss=l(),u(ts.$$.fragment),Ls=l(),as=c("p"),as.textContent=Yt,Ps=l(),es=c("ul"),es.innerHTML=Ft,Ds=l(),ls=c("p"),ls.textContent=Nt,Ks=l(),u(ns.$$.fragment),Os=l(),ps=c("p"),ps.textContent=Ht,st=l(),u(is.$$.fragment),tt=l(),u(rs.$$.fragment),at=l(),ms=c("p"),ms.textContent=Bt,et=l(),u(cs.$$.fragment),lt=l(),os=c("p"),os.innerHTML=Et,nt=l(),u(us.$$.fragment),pt=l(),fs=c("p"),fs.textContent=Qt,it=l(),hs=c("ol"),hs.innerHTML=At,rt=l(),u(ds.$$.fragment),mt=l(),gs=c("p"),gs.innerHTML=zt,ct=l(),u(js.$$.fragment),ot=l(),u(Ms.$$.fragment),ut=l(),bs=c("p"),bs.innerHTML=qt,ft=l(),u(ys.$$.fragment),ht=l(),$s=c("p"),$s.innerHTML=St,dt=l(),u(ws.$$.fragment),gt=l(),Js=c("p"),Js.innerHTML=Lt,jt=l(),u(Ts.$$.fragment),Mt=l(),u(Y.$$.fragment),bt=l(),u(F.$$.fragment),yt=l(),u(_s.$$.fragment),$t=l(),xs=c("p"),xs.textContent=Pt,wt=l(),Us=c("p"),Us.textContent=Dt,Jt=l(),u(Ws.$$.fragment),Tt=l(),Zs=c("p"),Zs.innerHTML=Kt,_t=l(),u(vs.$$.fragment),xt=l(),ks=c("p"),ks.innerHTML=Ot,Ut=l(),u(N.$$.fragment),Wt=l(),Xs=c("p"),this.h()},l(s){const t=ua("svelte-u9bgzb",document.head);p=o(t,"META",{name:!0,content:!0}),t.forEach(a),b=n(s),i=o(s,"P",{}),la(i).forEach(a),M=n(s),f($.$$.fragment,s),U=n(s),f(k.$$.fragment,s),Z=n(s),f(W.$$.fragment,s),C=n(s),J=o(s,"P",{"data-svelte-h":!0}),y(J)!=="svelte-8wnnrc"&&(J.textContent=V),I=n(s),T=o(s,"P",{"data-svelte-h":!0}),y(T)!=="svelte-w5jzhi"&&(T.textContent=R),m=n(s),x=o(s,"OL",{"data-svelte-h":!0}),y(x)!=="svelte-1ask5b9"&&(x.innerHTML=H),G=n(s),f(X.$$.fragment,s),r=n(s),w=o(s,"P",{"data-svelte-h":!0}),y(w)!=="svelte-1lya3k8"&&(w.textContent=Is),Rs=n(s),f(B.$$.fragment,s),Gs=n(s),E=o(s,"P",{"data-svelte-h":!0}),y(E)!=="svelte-193zy02"&&(E.textContent=Ct),Vs=n(s),f(Q.$$.fragment,s),Ys=n(s),f(A.$$.fragment,s),Fs=n(s),z=o(s,"P",{"data-svelte-h":!0}),y(z)!=="svelte-175h6bn"&&(z.textContent=It),Ns=n(s),f(q.$$.fragment,s),Hs=n(s),S=o(s,"P",{"data-svelte-h":!0}),y(S)!=="svelte-4rd872"&&(S.innerHTML=Xt),Bs=n(s),f(L.$$.fragment,s),Es=n(s),P=o(s,"P",{"data-svelte-h":!0}),y(P)!=="svelte-1px2i3t"&&(P.textContent=Rt),Qs=n(s),f(D.$$.fragment,s),As=n(s),K=o(s,"P",{"data-svelte-h":!0}),y(K)!=="svelte-13u82xc"&&(K.innerHTML=Gt),zs=n(s),f(O.$$.fragment,s),qs=n(s),ss=o(s,"P",{"data-svelte-h":!0}),y(ss)!=="svelte-1tlby18"&&(ss.textContent=Vt),Ss=n(s),f(ts.$$.fragment,s),Ls=n(s),as=o(s,"P",{"data-svelte-h":!0}),y(as)!=="svelte-gnvpca"&&(as.textContent=Yt),Ps=n(s),es=o(s,"UL",{"data-svelte-h":!0}),y(es)!=="svelte-1b9n6bc"&&(es.innerHTML=Ft),Ds=n(s),ls=o(s,"P",{"data-svelte-h":!0}),y(ls)!=="svelte-1xwss2e"&&(ls.textContent=Nt),Ks=n(s),f(ns.$$.fragment,s),Os=n(s),ps=o(s,"P",{"data-svelte-h":!0}),y(ps)!=="svelte-wrnhuq"&&(ps.textContent=Ht),st=n(s),f(is.$$.fragment,s),tt=n(s),f(rs.$$.fragment,s),at=n(s),ms=o(s,"P",{"data-svelte-h":!0}),y(ms)!=="svelte-8a7fkd"&&(ms.textContent=Bt),et=n(s),f(cs.$$.fragment,s),lt=n(s),os=o(s,"P",{"data-svelte-h":!0}),y(os)!=="svelte-83kdwu"&&(os.innerHTML=Et),nt=n(s),f(us.$$.fragment,s),pt=n(s),fs=o(s,"P",{"data-svelte-h":!0}),y(fs)!=="svelte-16wi3kv"&&(fs.textContent=Qt),it=n(s),hs=o(s,"OL",{"data-svelte-h":!0}),y(hs)!=="svelte-3k203q"&&(hs.innerHTML=At),rt=n(s),f(ds.$$.fragment,s),mt=n(s),gs=o(s,"P",{"data-svelte-h":!0}),y(gs)!=="svelte-1o2k96z"&&(gs.innerHTML=zt),ct=n(s),f(js.$$.fragment,s),ot=n(s),f(Ms.$$.fragment,s),ut=n(s),bs=o(s,"P",{"data-svelte-h":!0}),y(bs)!=="svelte-1xd1ltl"&&(bs.innerHTML=qt),ft=n(s),f(ys.$$.fragment,s),ht=n(s),$s=o(s,"P",{"data-svelte-h":!0}),y($s)!=="svelte-o90xg4"&&($s.innerHTML=St),dt=n(s),f(ws.$$.fragment,s),gt=n(s),Js=o(s,"P",{"data-svelte-h":!0}),y(Js)!=="svelte-18cw5xr"&&(Js.innerHTML=Lt),jt=n(s),f(Ts.$$.fragment,s),Mt=n(s),f(Y.$$.fragment,s),bt=n(s),f(F.$$.fragment,s),yt=n(s),f(_s.$$.fragment,s),$t=n(s),xs=o(s,"P",{"data-svelte-h":!0}),y(xs)!=="svelte-cyrfc8"&&(xs.textContent=Pt),wt=n(s),Us=o(s,"P",{"data-svelte-h":!0}),y(Us)!=="svelte-frlt8d"&&(Us.textContent=Dt),Jt=n(s),f(Ws.$$.fragment,s),Tt=n(s),Zs=o(s,"P",{"data-svelte-h":!0}),y(Zs)!=="svelte-1soovt5"&&(Zs.innerHTML=Kt),_t=n(s),f(vs.$$.fragment,s),xt=n(s),ks=o(s,"P",{"data-svelte-h":!0}),y(ks)!=="svelte-1lwhma4"&&(ks.innerHTML=Ot),Ut=n(s),f(N.$$.fragment,s),Wt=n(s),Xs=o(s,"P",{}),la(Xs).forEach(a),this.h()},h(){na(p,"name","hf:doc:metadata"),na(p,"content",xa)},m(s,t){fa(document.head,p),e(s,b,t),e(s,i,t),e(s,M,t),h($,s,t),e(s,U,t),h(k,s,t),e(s,Z,t),h(W,s,t),e(s,C,t),e(s,J,t),e(s,I,t),e(s,T,t),e(s,m,t),e(s,x,t),e(s,G,t),h(X,s,t),e(s,r,t),e(s,w,t),e(s,Rs,t),h(B,s,t),e(s,Gs,t),e(s,E,t),e(s,Vs,t),h(Q,s,t),e(s,Ys,t),h(A,s,t),e(s,Fs,t),e(s,z,t),e(s,Ns,t),h(q,s,t),e(s,Hs,t),e(s,S,t),e(s,Bs,t),h(L,s,t),e(s,Es,t),e(s,P,t),e(s,Qs,t),h(D,s,t),e(s,As,t),e(s,K,t),e(s,zs,t),h(O,s,t),e(s,qs,t),e(s,ss,t),e(s,Ss,t),h(ts,s,t),e(s,Ls,t),e(s,as,t),e(s,Ps,t),e(s,es,t),e(s,Ds,t),e(s,ls,t),e(s,Ks,t),h(ns,s,t),e(s,Os,t),e(s,ps,t),e(s,st,t),h(is,s,t),e(s,tt,t),h(rs,s,t),e(s,at,t),e(s,ms,t),e(s,et,t),h(cs,s,t),e(s,lt,t),e(s,os,t),e(s,nt,t),h(us,s,t),e(s,pt,t),e(s,fs,t),e(s,it,t),e(s,hs,t),e(s,rt,t),h(ds,s,t),e(s,mt,t),e(s,gs,t),e(s,ct,t),h(js,s,t),e(s,ot,t),h(Ms,s,t),e(s,ut,t),e(s,bs,t),e(s,ft,t),h(ys,s,t),e(s,ht,t),e(s,$s,t),e(s,dt,t),h(ws,s,t),e(s,gt,t),e(s,Js,t),e(s,jt,t),h(Ts,s,t),e(s,Mt,t),h(Y,s,t),e(s,bt,t),h(F,s,t),e(s,yt,t),h(_s,s,t),e(s,$t,t),e(s,xs,t),e(s,wt,t),e(s,Us,t),e(s,Jt,t),h(Ws,s,t),e(s,Tt,t),e(s,Zs,t),e(s,_t,t),h(vs,s,t),e(s,xt,t),e(s,ks,t),e(s,Ut,t),h(N,s,t),e(s,Wt,t),e(s,Xs,t),Zt=!0},p(s,[t]){const sa={};t&2&&(sa.$$scope={dirty:t,ctx:s}),X.$set(sa);const ta={};t&2&&(ta.$$scope={dirty:t,ctx:s}),Y.$set(ta);const aa={};t&2&&(aa.$$scope={dirty:t,ctx:s}),F.$set(aa);const ea={};t&2&&(ea.$$scope={dirty:t,ctx:s}),N.$set(ea)},i(s){Zt||(d($.$$.fragment,s),d(k.$$.fragment,s),d(W.$$.fragment,s),d(X.$$.fragment,s),d(B.$$.fragment,s),d(Q.$$.fragment,s),d(A.$$.fragment,s),d(q.$$.fragment,s),d(L.$$.fragment,s),d(D.$$.fragment,s),d(O.$$.fragment,s),d(ts.$$.fragment,s),d(ns.$$.fragment,s),d(is.$$.fragment,s),d(rs.$$.fragment,s),d(cs.$$.fragment,s),d(us.$$.fragment,s),d(ds.$$.fragment,s),d(js.$$.fragment,s),d(Ms.$$.fragment,s),d(ys.$$.fragment,s),d(ws.$$.fragment,s),d(Ts.$$.fragment,s),d(Y.$$.fragment,s),d(F.$$.fragment,s),d(_s.$$.fragment,s),d(Ws.$$.fragment,s),d(vs.$$.fragment,s),d(N.$$.fragment,s),Zt=!0)},o(s){g($.$$.fragment,s),g(k.$$.fragment,s),g(W.$$.fragment,s),g(X.$$.fragment,s),g(B.$$.fragment,s),g(Q.$$.fragment,s),g(A.$$.fragment,s),g(q.$$.fragment,s),g(L.$$.fragment,s),g(D.$$.fragment,s),g(O.$$.fragment,s),g(ts.$$.fragment,s),g(ns.$$.fragment,s),g(is.$$.fragment,s),g(rs.$$.fragment,s),g(cs.$$.fragment,s),g(us.$$.fragment,s),g(ds.$$.fragment,s),g(js.$$.fragment,s),g(Ms.$$.fragment,s),g(ys.$$.fragment,s),g(ws.$$.fragment,s),g(Ts.$$.fragment,s),g(Y.$$.fragment,s),g(F.$$.fragment,s),g(_s.$$.fragment,s),g(Ws.$$.fragment,s),g(vs.$$.fragment,s),g(N.$$.fragment,s),Zt=!1},d(s){s&&(a(b),a(i),a(M),a(U),a(Z),a(C),a(J),a(I),a(T),a(m),a(x),a(G),a(r),a(w),a(Rs),a(Gs),a(E),a(Vs),a(Ys),a(Fs),a(z),a(Ns),a(Hs),a(S),a(Bs),a(Es),a(P),a(Qs),a(As),a(K),a(zs),a(qs),a(ss),a(Ss),a(Ls),a(as),a(Ps),a(es),a(Ds),a(ls),a(Ks),a(Os),a(ps),a(st),a(tt),a(at),a(ms),a(et),a(lt),a(os),a(nt),a(pt),a(fs),a(it),a(hs),a(rt),a(mt),a(gs),a(ct),a(ot),a(ut),a(bs),a(ft),a(ht),a($s),a(dt),a(gt),a(Js),a(jt),a(Mt),a(bt),a(yt),a($t),a(xs),a(wt),a(Us),a(Jt),a(Tt),a(Zs),a(_t),a(xt),a(ks),a(Ut),a(Wt),a(Xs)),a(p),j($,s),j(k,s),j(W,s),j(X,s),j(B,s),j(Q,s),j(A,s),j(q,s),j(L,s),j(D,s),j(O,s),j(ts,s),j(ns,s),j(is,s),j(rs,s),j(cs,s),j(us,s),j(ds,s),j(js,s),j(Ms,s),j(ys,s),j(ws,s),j(Ts,s),j(Y,s),j(F,s),j(_s,s),j(Ws,s),j(vs,s),j(N,s)}}}const xa='{"title":"Audio classification","local":"audio-classification","sections":[{"title":"Load MInDS-14 dataset","local":"load-minds-14-dataset","sections":[],"depth":2},{"title":"Preprocess","local":"preprocess","sections":[],"depth":2},{"title":"Evaluate","local":"evaluate","sections":[],"depth":2},{"title":"Train","local":"train","sections":[],"depth":2},{"title":"Inference","local":"inference","sections":[],"depth":2}],"depth":1}';function Ua(v){return ma(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ga extends ca{constructor(p){super(),oa(this,p,Ua,_a,ra,{})}}export{Ga as component};
