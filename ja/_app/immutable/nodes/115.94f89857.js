import{s as Os,o as ea,n as Ut}from"../chunks/scheduler.9bc65507.js";import{S as ta,i as la,g as i,s,r,A as na,h as o,f as l,c as a,j as Ds,u as f,x as p,k as Ks,l as kn,y as sa,a as n,v as d,d as u,t as b,w as T}from"../chunks/index.707bf1b6.js";import{T as xt}from"../chunks/Tip.c2ecdbf4.js";import{C as $}from"../chunks/CodeBlock.54a9f38d.js";import{H as c}from"../chunks/Heading.342b1fa6.js";function aa(w){let m,y="この機能は実験的であり、将来のバージョンで大幅に変更される可能性があります。たとえば、Flash Attention 2 APIは近い将来<code>BetterTransformer</code> APIに移行するかもしれません。";return{c(){m=i("p"),m.innerHTML=y},l(M){m=o(M,"P",{"data-svelte-h":!0}),p(m)!=="svelte-7uacjn"&&(m.innerHTML=y)},m(M,J){n(M,m,J)},p:Ut,d(M){M&&l(m)}}}function ia(w){let m,y="Flash Attention 2は、モデルのdtypeが<code>fp16</code>または<code>bf16</code>の場合にのみ使用でき、NVIDIA-GPUデバイスでのみ実行されます。この機能を使用する前に、モデルを適切なdtypeにキャストし、サポートされているデバイスにロードしてください。";return{c(){m=i("p"),m.innerHTML=y},l(M){m=o(M,"P",{"data-svelte-h":!0}),p(m)!=="svelte-1q0hw1w"&&(m.innerHTML=y)},m(M,J){n(M,m,J)},p:Ut,d(M){M&&l(m)}}}function oa(w){let m,y="Flash Attentionは、fp16またはbf16のdtypeを使用するモデルにのみ使用できます。BetterTransformerを使用する前に、モデルを適切なdtypeにキャストしてください。";return{c(){m=i("p"),m.textContent=y},l(M){m=o(M,"P",{"data-svelte-h":!0}),p(m)!=="svelte-8snhtv"&&(m.textContent=y)},m(M,J){n(M,m,J)},p:Ut,d(M){M&&l(m)}}}function pa(w){let m,y="Note that this feature can also be used in a multi GPU setup.",M,J,v="この機能は、マルチGPUセットアップでも使用できることに注意してください。";return{c(){m=i("p"),m.textContent=y,M=s(),J=i("p"),J.textContent=v},l(h){m=o(h,"P",{"data-svelte-h":!0}),p(m)!=="svelte-8elnl8"&&(m.textContent=y),M=a(h),J=o(h,"P",{"data-svelte-h":!0}),p(J)!=="svelte-1lcfhr2"&&(J.textContent=v)},m(h,g){n(h,m,g),n(h,M,g),n(h,J,g)},p:Ut,d(h){h&&(l(m),l(M),l(J))}}}function ma(w){let m,y="この機能は、マルチGPU環境でも使用できます。";return{c(){m=i("p"),m.textContent=y},l(M){m=o(M,"P",{"data-svelte-h":!0}),p(m)!=="svelte-dptqzr"&&(m.textContent=y)},m(M,J){n(M,m,J)},p:Ut,d(M){M&&l(m)}}}function ra(w){let m,y,M,J,v,h,g,Gn='このガイドに加えて、<a href="perf_train_gpu_one">1つのGPUでのトレーニングガイド</a>と<a href="perf_infer_cpu">CPUでの推論ガイド</a>に関連する情報があります。',Wt,G,Zt,_,jt,I,In='Flash Attention 2は、トランスフォーマーベースのモデルのトレーニングと推論速度を大幅に高速化できます。Flash Attention 2は、Tri Dao氏によって<a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">公式のFlash Attentionリポジトリ</a>で導入されました。Flash Attentionに関する科学論文は<a href="https://arxiv.org/abs/2205.14135" rel="nofollow">こちら</a>で見ることができます。',Bt,F,Fn="Flash Attention 2を正しくインストールするには、上記のリポジトリに記載されているインストールガイドに従ってください。",kt,R,Rn="以下のモデルに対してFlash Attention 2をネイティブサポートしています：",Gt,X,Xn="<li>Llama</li> <li>Falcon</li>",It,V,Vn="さらに多くのモデルにFlash Attention 2のサポートを追加することをGitHubで提案することもでき、変更を統合するためにプルリクエストを開くこともできます。サポートされているモデルは、パディングトークンを使用してトレーニングを含む、推論とトレーニングに使用できます（現在の<code>BetterTransformer</code> APIではサポートされていない）。",Ft,C,Rt,H,Xt,L,Hn="モデルでFlash Attention 2を有効にするには、<code>from_pretrained</code>の引数に<code>attn_implementation=&quot;flash_attention_2&quot;</code>を追加します。",Vt,P,Ht,A,Ln="こちらは、生成または微調整のために使用するテキストです。",Lt,N,Pt,z,Pn="特に長いシーケンスに対して、微調整と推論の際には、かなりの高速化が期待できます。ただし、Flash Attentionはパディングトークンを使用してアテンションスコアを計算しないため、シーケンスにパディングトークンが含まれる場合、バッチ推論においてアテンションスコアを手動でパッド/アンパッドする必要があり、パディングトークンを含むバッチ生成の大幅な遅延が発生します。",At,q,An='これを克服するために、トレーニング中にシーケンスにパディングトークンを使用せずにFlash Attentionを使用する必要があります（たとえば、データセットをパックすることにより、シーケンスを最大シーケンス長に達するまで連結することなど）。ここに<a href="https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516" rel="nofollow">例</a>が提供されています。',Nt,E,Nn='以下は、パディングトークンのない場合に、シーケンス長が4096の<a href="https://hf.co/tiiuae/falcon-7b" rel="nofollow">tiiuae/falcon-7b</a>に対する単純なフォワードパスの予想される高速化です。さまざまなバッチサイズが示されています：',zt,x,zn='<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/falcon-7b-inference-large-seqlen.png"/>',qt,Q,qn='以下は、パディングトークンのない場合に、シーケンス長が4096の<a href="https://hf.co/meta-llama/Llama-7b-hf" rel="nofollow"><code>meta-llama/Llama-7b-hf</code></a>に対する単純なフォワードパスの予想される高速化です。さまざまなバッチサイズが示されています：',Et,U,En='<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-7b-inference-large-seqlen.png"/>',Qt,Y,Qn="パディングトークンを含むシーケンス（パディングトークンを使用してトレーニングまたは生成する）の場合、アテンションスコアを正しく計算するために入力シーケンスをアンパッド/パッドする必要があります。比較的小さいシーケンス長の場合、純粋なフォワードパスではパディングトークンが30%未満しか埋められていないため、これはわずかな高速化をもたらします。",Yt,W,Yn='<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-small-seqlen-padding.png"/>',St,S,Sn="しかし、大きなシーケンス長の場合、純粋な推論（トレーニングも含む）には興味深い高速化が得られます。",Dt,D,Dn='Flash Attentionは、アテンション計算をよりメモリ効率の良いものにし、大きなシーケンス長でのCUDA OOMの問題を回避できるようにします。大きなシーケンス長に対して最大20のメモリ削減をもたらすことがあります。詳細については、<a href="https://github.com/Dao-AILab/flash-attention" rel="nofollow">公式のFlash Attentionリポジトリ</a>をご覧ください。',Kt,Z,Kn='<img src="https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-large-seqlen-padding.png"/>',Ot,K,el,O,On="この機能をモデルの最適化に多くの既存の機能と組み合わせることができます。以下にいくつかの例を示します：",tl,ee,ll,te,es="この機能を8ビットの量子化と組み合わせることができます：",nl,le,sl,ne,al,se,ts="この機能を 4 ビットの量子化と組み合わせることができます：",il,ae,ol,ie,pl,oe,ls="この機能を使用して、Flash Attention 2をベースにアダプターをトレーニングする際にPEFTを組み合わせることができます。",ml,pe,rl,me,fl,re,ns='<a href="https://huggingface.co/docs/optimum/bettertransformer/overview" rel="nofollow">BetterTransformer</a>は、🤗 TransformersモデルをPyTorchネイティブの高速パス実行に変換します。これにより、Flash Attentionなどの最適化されたカーネルが内部で呼び出されます。',dl,fe,ss="BetterTransformerは、テキスト、画像、およびオーディオモデルの単一およびマルチGPUでの高速な推論をサポートしています。",ul,j,bl,de,Tl,ue,as='PyTorchネイティブの<a href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/" rel="nofollow"><code>nn.MultiHeadAttention</code></a>アテンション高速パス、BetterTransformerと呼ばれるものは、<a href="https://huggingface.co/docs/optimum/bettertransformer/overview" rel="nofollow">🤗 Optimumライブラリ</a>の統合を通じてTransformersと一緒に使用できます。',Ml,be,is='PyTorchのアテンション高速パスを使用すると、カーネルフュージョンと<a href="https://pytorch.org/docs/stable/nested.html" rel="nofollow">ネストされたテンソル</a>の使用により、推論を高速化できます。詳細なベンチマーク情報は<a href="https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2" rel="nofollow">このブログ記事</a>にあります。',cl,Te,os='<a href="https://github.com/huggingface/optimum" rel="nofollow"><code>optimum</code></a>パッケージをインストールした後、推論中にBetter Transformerを使用するには、関連する内部モジュールを呼び出すことで置き換える必要があります<a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.to_bettertransformer">to_bettertransformer()</a>:',Jl,Me,yl,ce,ps='メソッド <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.reverse_bettertransformer">reverse_bettertransformer()</a> は、モデルを保存する前に使用すべきで、標準のトランスフォーマーモデリングを使用するためのものです：',$l,Je,hl,ye,ms='BetterTransformer APIを使ったエンコーダーモデルの可能性について詳しく知るには、<a href="https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2" rel="nofollow">このブログポスト</a>をご覧ください。',wl,$e,gl,he,rs='テキストモデル、特にデコーダーベースのモデル（GPT、T5、Llamaなど）にとって、BetterTransformer APIはすべての注意操作を<a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention" rel="nofollow"><code>torch.nn.functional.scaled_dot_product_attention</code>オペレーター</a>（SDPA）を使用するように変換します。このオペレーターはPyTorch 2.0以降でのみ利用可能です。',vl,we,fs="モデルをBetterTransformerに変換するには、以下の手順を実行してください：",_l,ge,Cl,ve,ds='SDPAは、ハードウェアや問題のサイズに応じて<a href="https://arxiv.org/abs/2205.14135" rel="nofollow">Flash Attention</a>カーネルを使用することもできます。Flash Attentionを有効にするか、特定の設定（ハードウェア、問題サイズ）で使用可能かどうかを確認するには、<a href="https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel" rel="nofollow"><code>torch.backends.cuda.sdp_kernel</code></a>をコンテキストマネージャとして使用します。',xl,_e,Ul,Ce,us="もしトレースバックにバグが表示された場合",Wl,xe,Zl,Ue,bs="Flash Attention の広範なカバレッジを持つかもしれない PyTorch のナイトリーバージョンを試してみることをお勧めします。",jl,We,Bl,Ze,Ts="Or make sure your model is correctly casted in float16 or bfloat16",kl,je,Ms="モデルが正しくfloat16またはbfloat16にキャストされていることを確認してください。",Gl,Be,cs='Have a look at <a href="https://pytorch.org/blog/out-of-the-box-acceleration/" rel="nofollow">this detailed blogpost</a> to read more about what is possible to do with <code>BetterTransformer</code> + SDPA API.',Il,ke,Js='<code>BetterTransformer</code> + SDPA APIを使用して何が可能かについて詳しく読むには、<a href="https://pytorch.org/blog/out-of-the-box-acceleration/" rel="nofollow">この詳細なブログポスト</a>をご覧ください。',Fl,Ge,Rl,Ie,ys="FP4混合精度推論のための<code>bitsandbytes</code>統合",Xl,Fe,$s="You can install <code>bitsandbytes</code> and benefit from easy model compression on GPUs. Using FP4 quantization you can expect to reduce up to 8x the model size compared to its native full precision version. Check out below how to get started.",Vl,Re,hs="<code>bitsandbytes</code>をインストールし、GPUで簡単なモデルの圧縮を利用できます。FP4量子化を使用すると、ネイティブのフルプレシジョンバージョンと比較してモデルサイズを最大8倍削減できることが期待できます。以下を確認して、どのように始めるかをご覧ください。",Hl,B,Ll,Xe,Pl,Ve,ws=`<li><p>Latest <code>bitsandbytes</code> library
<code>pip install bitsandbytes&gt;=0.39.0</code></p></li> <li><p>Install latest <code>accelerate</code> from source
<code>pip install git+https://github.com/huggingface/accelerate.git</code></p></li> <li><p>Install latest <code>transformers</code> from source
<code>pip install git+https://github.com/huggingface/transformers.git</code></p></li>`,Al,He,Nl,Le,gs="以下のコードを実行することで、簡単に単一のGPUでFP4モデルを実行できます:",zl,Pe,ql,Ae,vs="注意: <code>device_map</code>はオプションですが、推論時に <code>device_map = &#39;auto&#39;</code> を設定することが推奨されています。これにより、利用可能なリソースに効率的にモデルがディスパッチされます。",El,Ne,Ql,ze,_s="混合4ビットモデルを複数のGPUにロードする方法は、単一GPUセットアップと同じです（単一GPUセットアップと同じコマンドです）：",Yl,qe,Sl,Ee,Cs="しかし、<code>accelerate</code>を使用して、各GPUに割り当てるGPU RAMを制御することができます。以下のように、<code>max_memory</code>引数を使用します：",Dl,Qe,Kl,Ye,xs="この例では、最初のGPUは600MBのメモリを使用し、2番目のGPUは1GBを使用します。",Ol,Se,en,De,Us='このメソッドのさらなる高度な使用法については、<a href="main_classes/quantization">量子化</a>のドキュメンテーションページをご覧ください。',tn,Ke,ln,k,nn,Oe,Ws='論文<a href="https://arxiv.org/abs/2208.07339" rel="nofollow"><code>LLM.int8()：スケーラブルなTransformer向けの8ビット行列乗算</code></a>によれば、Hugging Face統合がHub内のすべてのモデルでわずか数行のコードでサポートされています。このメソッドは、半精度（<code>float16</code>および<code>bfloat16</code>）の重みの場合に<code>nn.Linear</code>サイズを2倍、単精度（<code>float32</code>）の重みの場合は4倍に縮小し、外れ値に対してほとんど影響を与えません。',sn,et,Zs='<img src="https://cdn-uploads.huggingface.co/production/uploads/1659861207959-62441d1d9fdefb55a0b7d12c.png" alt="HFxbitsandbytes.png"/>',an,tt,js=`Int8混合精度行列分解は、行列乗算を2つのストリームに分割することによって動作します：(1) システマティックな特徴外れ値ストリームがfp16で行列乗算（0.01%）、(2) int8行列乗算の通常のストリーム（99.9%）。この方法を使用すると、非常に大きなモデルに対して予測の劣化なしにint8推論が可能です。
このメソッドの詳細については、<a href="https://arxiv.org/abs/2208.07339" rel="nofollow">論文</a>または<a href="https://huggingface.co/blog/hf-bitsandbytes-integration" rel="nofollow">この統合に関するブログ記事</a>をご確認ください。`,on,lt,Bs='<img src="https://cdn-uploads.huggingface.co/production/uploads/1660567469965-62441d1d9fdefb55a0b7d12c.gif" alt="MixedInt8.gif"/>',pn,nt,ks=`なお、この機能を使用するにはGPUが必要であり、カーネルはGPU専用にコンパイルされている必要があります。この機能を使用する前に、モデルの1/4（またはハーフ精度の重みの場合は1/2）を保存するのに十分なGPUメモリがあることを確認してください。
このモジュールを使用する際のヘルプに関する詳細は、以下のノートをご覧いただくか、<a href="#colab-demos">Google Colabのデモ</a>をご覧ください。`,mn,st,rn,at,Gs=`<li><code>bitsandbytes&lt;0.37.0</code>を使用する場合、NVIDIA GPUを使用していることを確認し、8ビットテンソルコアをサポートしていることを確認してください（Turing、Ampere、またはそれ以降のアーキテクチャー、例：T4、RTX20s RTX30s、A40-A100など）。<code>bitsandbytes&gt;=0.37.0</code>の場合、すべてのGPUがサポートされるはずです。</li> <li>正しいバージョンの<code>bitsandbytes</code>をインストールするには、次のコマンドを実行してください：
<code>pip install bitsandbytes&gt;=0.31.5</code></li> <li><code>accelerate</code>をインストールします：
<code>pip install accelerate&gt;=0.12.0</code></li>`,fn,it,dn,ot,Is="必要なライブラリをインストールした後、ミックス 8 ビットモデルを読み込む方法は次の通りです：",un,pt,bn,mt,Fs="以下はシンプルな例です：",Tn,rt,Rs="<li><code>pipeline()</code> 関数の代わりに、モデルの <code>generate()</code> メソッドを使用することをお勧めします。<code>pipeline()</code> 関数を使用して推論することは可能ですが、混合8ビットモデルに最適化されておらず、<code>generate()</code> メソッドを使用するよりも遅くなります。また、一部のサンプリング戦略（例：ヌクレウスサンプリング）は、<code>pipeline()</code> 関数では混合8ビットモデルではサポートされていません。</li> <li>すべての入力をモデルと同じデバイスに配置してください。</li>",Mn,ft,cn,dt,Jn,ut,Xs="複数のGPUに混合8ビットモデルをロードする方法は、次の通りです（シングルGPUセットアップと同じコマンドです）：",yn,bt,$n,Tt,Vs="<code>accelerate</code>を使用して各GPUに割り当てるGPU RAMを制御する際には、以下のように<code>max_memory</code>引数を使用します：",hn,Mt,wn,ct,Hs="In this example, the first GPU will use 1GB of memory and the second 2GB.",gn,Jt,vn,yt,Ls="この方法を使用すると、以前のGoogle Colabでは推論できなかったモデルに対して推論を行うことができます。以下は、Google Colabで8ビット量子化を使用してT5-11b（fp32で42GB）を実行するデモのリンクです：",_n,$t,Ps='<a href="https://colab.research.google.com/drive/1YORPWx4okIHXnjW7MSAidXN29mPVNT7F?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab: T5-11b demo"/></a>',Cn,ht,As="また、BLOOM-3Bのデモもご覧いただけます：",xn,wt,Ns='<a href="https://colab.research.google.com/drive/1qOjXfQIAULfKvZqwCen8-MoWKGdSatZ4?usp=sharing" rel="nofollow"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab: BLOOM-3b demo"/></a>',Un,gt,Wn,vt,zs="異なる方法を組み合わせて、モデルの最適なパフォーマンスを得ることができます。例えば、BetterTransformerを使用してFP4ミックスプレシジョン推論とフラッシュアテンションを組み合わせることができます。",Zn,_t,jn,Ct,Bn;return v=new c({props:{title:"Efficient Inference on a Single GPU",local:"efficient-inference-on-a-single-gpu",headingTag:"h1"}}),G=new c({props:{title:"Flash Attention 2",local:"flash-attention-2",headingTag:"h2"}}),_=new xt({props:{$$slots:{default:[aa]},$$scope:{ctx:w}}}),C=new xt({props:{$$slots:{default:[ia]},$$scope:{ctx:w}}}),H=new c({props:{title:"Quick usage",local:"quick-usage",headingTag:"h3"}}),P=new $({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwTGxhbWFGb3JDYXVzYWxMTSUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIydGlpdWFlJTJGZmFsY29uLTdiJTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwbW9kZWxfaWQlMkMlMjAlMEElMjAlMjAlMjAlMjB0b3JjaF9kdHlwZSUzRHRvcmNoLmJmbG9hdDE2JTJDJTIwJTBBJTIwJTIwJTIwJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMmZsYXNoX2F0dGVudGlvbl8yJTIyJTJDJTBBKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = <span class="hljs-string">&quot;tiiuae/falcon-7b&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    torch_dtype=torch.bfloat16, 
    attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>,
)`,wrap:!1}}),N=new c({props:{title:"Expected speedups",local:"expected-speedups",headingTag:"h3"}}),K=new c({props:{title:"Advanced usage",local:"advanced-usage",headingTag:"h3"}}),ee=new c({props:{title:"Combining Flash Attention 2 and 8-bit models",local:"combining-flash-attention-2-and-8-bit-models",headingTag:"h3"}}),le=new $({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwTGxhbWFGb3JDYXVzYWxMTSUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIydGlpdWFlJTJGZmFsY29uLTdiJTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwbW9kZWxfaWQlMkMlMjAlMEElMjAlMjAlMjAlMjBsb2FkX2luXzhiaXQlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMmZsYXNoX2F0dGVudGlvbl8yJTIyJTJDJTBBKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = <span class="hljs-string">&quot;tiiuae/falcon-7b&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_8bit=<span class="hljs-literal">True</span>,
    attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>,
)`,wrap:!1}}),ne=new c({props:{title:"Combining Flash Attention 2 and 4-bit models",local:"combining-flash-attention-2-and-4-bit-models",headingTag:"h3"}}),ae=new $({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwTGxhbWFGb3JDYXVzYWxMTSUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIydGlpdWFlJTJGZmFsY29uLTdiJTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTBBJTIwJTIwJTIwJTIwbW9kZWxfaWQlMkMlMjAlMEElMjAlMjAlMjAlMjBsb2FkX2luXzRiaXQlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYXR0bl9pbXBsZW1lbnRhdGlvbiUzRCUyMmZsYXNoX2F0dGVudGlvbl8yJTIyJTJDJTBBKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM

model_id = <span class="hljs-string">&quot;tiiuae/falcon-7b&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_4bit=<span class="hljs-literal">True</span>,
    attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>,
)`,wrap:!1}}),ie=new c({props:{title:"Combining Flash Attention 2 and PEFT",local:"combining-flash-attention-2-and-peft",headingTag:"h3"}}),pe=new $({props:{code:"JTBBJTBBJTBBJTBBJTBBJTIzJTIwdHJhaW4lMjB5b3VyJTIwbW9kZWw=",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, LlamaForCausalLM
<span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig

model_id = <span class="hljs-string">&quot;tiiuae/falcon-7b&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_id)

model = AutoModelForCausalLM.from_pretrained(
    model_id, 
    load_in_4bit=<span class="hljs-literal">True</span>,
    attn_implementation=<span class="hljs-string">&quot;flash_attention_2&quot;</span>,
)

lora_config = LoraConfig(
    r=<span class="hljs-number">8</span>,
    task_type=<span class="hljs-string">&quot;CAUSAL_LM&quot;</span>
)

model.add_adapter(lora_config)

<span class="hljs-meta">... </span><span class="hljs-comment"># train your model</span>`,wrap:!1}}),me=new c({props:{title:"BetterTransformer",local:"bettertransformer",headingTag:"h2"}}),j=new xt({props:{$$slots:{default:[oa]},$$scope:{ctx:w}}}),de=new c({props:{title:"Encoder models",local:"encoder-models",headingTag:"h3"}}),Me=new $({props:{code:"bW9kZWwlMjAlM0QlMjBtb2RlbC50b19iZXR0ZXJ0cmFuc2Zvcm1lcigp",highlighted:"model = model.to_bettertransformer()",wrap:!1}}),Je=new $({props:{code:"bW9kZWwlMjAlM0QlMjBtb2RlbC5yZXZlcnNlX2JldHRlcnRyYW5zZm9ybWVyKCklMEFtb2RlbC5zYXZlX3ByZXRyYWluZWQoJTIyc2F2ZWRfbW9kZWwlMjIp",highlighted:`model = model.reverse_bettertransformer()
model.save_pretrained(<span class="hljs-string">&quot;saved_model&quot;</span>)`,wrap:!1}}),$e=new c({props:{title:"Decoder models",local:"decoder-models",headingTag:"h3"}}),ge=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZvcHQtMzUwbSUyMiklMEElMjMlMjBjb252ZXJ0JTIwdGhlJTIwbW9kZWwlMjB0byUyMEJldHRlclRyYW5zZm9ybWVyJTBBbW9kZWwudG9fYmV0dGVydHJhbnNmb3JtZXIoKSUwQSUwQSUyMyUyMFVzZSUyMGl0JTIwZm9yJTIwdHJhaW5pbmclMjBvciUyMGluZmVyZW5jZQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>)
<span class="hljs-comment"># convert the model to BetterTransformer</span>
model.to_bettertransformer()

<span class="hljs-comment"># Use it for training or inference</span>`,wrap:!1}}),_e=new $({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZvcHQtMzUwbSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm9wdC0zNTBtJTIyJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDE2KS50byglMjJjdWRhJTIyKSUwQSUyMyUyMGNvbnZlcnQlMjB0aGUlMjBtb2RlbCUyMHRvJTIwQmV0dGVyVHJhbnNmb3JtZXIlMEFtb2RlbC50b19iZXR0ZXJ0cmFuc2Zvcm1lcigpJTBBJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMkhlbGxvJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjBhbmQlMjIlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoaW5wdXRfdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKCUyMmN1ZGElMjIpJTBBJTBBJTJCJTIwd2l0aCUyMHRvcmNoLmJhY2tlbmRzLmN1ZGEuc2RwX2tlcm5lbChlbmFibGVfZmxhc2glM0RUcnVlJTJDJTIwZW5hYmxlX21hdGglM0RGYWxzZSUyQyUyMGVuYWJsZV9tZW1fZWZmaWNpZW50JTNERmFsc2UpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0cyU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/opt-350m&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;facebook/opt-350m&quot;, torch_dtype=torch.float16).to(&quot;cuda&quot;)
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = &quot;Hello my dog is cute and&quot;
inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)

<span class="hljs-addition">+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):</span>
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))`,wrap:!1}}),xe=new $({props:{code:"UnVudGltZUVycm9yJTNBJTIwTm8lMjBhdmFpbGFibGUlMjBrZXJuZWwuJTIwJTIwQWJvcnRpbmclMjBleGVjdXRpb24u",highlighted:"RuntimeError: No available kernel.  Aborting execution.",wrap:!1}}),We=new $({props:{code:"cGlwMyUyMGluc3RhbGwlMjAtVSUyMC0tcHJlJTIwdG9yY2glMjB0b3JjaHZpc2lvbiUyMHRvcmNoYXVkaW8lMjAtLWluZGV4LXVybCUyMGh0dHBzJTNBJTJGJTJGZG93bmxvYWQucHl0b3JjaC5vcmclMkZ3aGwlMkZuaWdodGx5JTJGY3UxMTg=",highlighted:"pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118",wrap:!1}}),Ge=new c({props:{title:"bitsandbytes integration for FP4 mixed-precision inference",local:"bitsandbytes-integration-for-fp4-mixed-precision-inference",headingTag:"h2"}}),B=new xt({props:{$$slots:{default:[pa]},$$scope:{ctx:w}}}),Xe=new c({props:{title:"Requirements",local:"requirements-for-fp4-mixedprecision-inference",headingTag:"h3"}}),He=new c({props:{title:"Running FP4 models - single GPU setup - Quickstart",local:"running-fp4-models---single-gpu-setup---quickstart",headingTag:"h3"}}),Pe=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfbmFtZSUyMCUzRCUyMCUyMmJpZ3NjaWVuY2UlMkZibG9vbS0yYjUlMjIlMEFtb2RlbF80Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fNGJpdCUzRFRydWUp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),Ne=new c({props:{title:"Running FP4 models - multi GPU setup",local:"running-fp4-models---multi-gpu-setup",headingTag:"h3"}}),qe=new $({props:{code:"bW9kZWxfbmFtZSUyMCUzRCUyMCUyMmJpZ3NjaWVuY2UlMkZibG9vbS0yYjUlMjIlMEFtb2RlbF80Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fNGJpdCUzRFRydWUp",highlighted:`model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),Qe=new $({props:{code:"bWF4X21lbW9yeV9tYXBwaW5nJTIwJTNEJTIwJTdCMCUzQSUyMCUyMjYwME1CJTIyJTJDJTIwMSUzQSUyMCUyMjFHQiUyMiU3RCUwQW1vZGVsX25hbWUlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tM2IlMjIlMEFtb2RlbF80Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fNGJpdCUzRFRydWUlMkMlMjBtYXhfbWVtb3J5JTNEbWF4X21lbW9yeV9tYXBwaW5nJTBBKQ==",highlighted:`max_memory_mapping = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;600MB&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;1GB&quot;</span>}
model_name = <span class="hljs-string">&quot;bigscience/bloom-3b&quot;</span>
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>, max_memory=max_memory_mapping
)`,wrap:!1}}),Se=new c({props:{title:"Advanced usage",local:"advanced-usage",headingTag:"h3"}}),Ke=new c({props:{title:"bitsandbytes integration for Int8 mixed-precision matrix decomposition",local:"bitsandbytes-integration-for-int8-mixed-precision-matrix-decomposition",headingTag:"h2"}}),k=new xt({props:{$$slots:{default:[ma]},$$scope:{ctx:w}}}),st=new c({props:{title:"Requirements",local:"requirements-for-int8-mixedprecision-matrix-decomposition",headingTag:"h3"}}),it=new c({props:{title:"Running mixed-Int8 models - single GPU setup",local:"running-mixed-int8-models---single-gpu-setup",headingTag:"h3"}}),pt=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfbmFtZSUyMCUzRCUyMCUyMmJpZ3NjaWVuY2UlMkZibG9vbS0yYjUlMjIlMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),ft=new $({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsX25hbWUlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMmI1JTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfbmFtZSklMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUpJTBBJTBBcHJvbXB0JTIwJTNEJTIwJTIySGVsbG8lMkMlMjBteSUyMGxsYW1hJTIwaXMlMjBjdXRlJTIyJTBBaW5wdXRzJTIwJTNEJTIwdG9rZW5pemVyKHByb21wdCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKCUyMmN1ZGElMjIpJTBBZ2VuZXJhdGVkX2lkcyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQW91dHB1dHMlMjAlM0QlMjB0b2tlbml6ZXIuYmF0Y2hfZGVjb2RlKGdlbmVyYXRlZF9pZHMlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_name)
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)

prompt = <span class="hljs-string">&quot;Hello, my llama is cute&quot;</span>
inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)
generated_ids = model.generate(**inputs)
outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=<span class="hljs-literal">True</span>)`,wrap:!1}}),dt=new c({props:{title:"Running mixed-int8 models - multi GPU setup",local:"running-mixed-int8-models---multi-gpu-setup",headingTag:"h3"}}),bt=new $({props:{code:"bW9kZWxfbmFtZSUyMCUzRCUyMCUyMmJpZ3NjaWVuY2UlMkZibG9vbS0yYjUlMjIlMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX25hbWUlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUp",highlighted:`model_name = <span class="hljs-string">&quot;bigscience/bloom-2b5&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),Mt=new $({props:{code:"bWF4X21lbW9yeV9tYXBwaW5nJTIwJTNEJTIwJTdCMCUzQSUyMCUyMjFHQiUyMiUyQyUyMDElM0ElMjAlMjIyR0IlMjIlN0QlMEFtb2RlbF9uYW1lJTIwJTNEJTIwJTIyYmlnc2NpZW5jZSUyRmJsb29tLTNiJTIyJTBBbW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjBtb2RlbF9uYW1lJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlJTJDJTIwbWF4X21lbW9yeSUzRG1heF9tZW1vcnlfbWFwcGluZyUwQSk=",highlighted:`max_memory_mapping = {<span class="hljs-number">0</span>: <span class="hljs-string">&quot;1GB&quot;</span>, <span class="hljs-number">1</span>: <span class="hljs-string">&quot;2GB&quot;</span>}
model_name = <span class="hljs-string">&quot;bigscience/bloom-3b&quot;</span>
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, max_memory=max_memory_mapping
)`,wrap:!1}}),Jt=new c({props:{title:"Colab demos",local:"colab-demos",headingTag:"h3"}}),gt=new c({props:{title:"Advanced usage: mixing FP4 (or Int8) and BetterTransformer",local:"advanced-usage-mixing-fp4-or-int8-and-bettertransformer",headingTag:"h2"}}),_t=new $({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyglMEElMjAlMjAlMjAlMjBsb2FkX2luXzRiaXQlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYm5iXzRiaXRfY29tcHV0ZV9kdHlwZSUzRHRvcmNoLmZsb2F0MTYlMEEpJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZvcHQtMzUwbSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm9wdC0zNTBtJTIyJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWcpJTBBJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMkhlbGxvJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjBhbmQlMjIlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoaW5wdXRfdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKCUyMmN1ZGElMjIpJTBBJTBBd2l0aCUyMHRvcmNoLmJhY2tlbmRzLmN1ZGEuc2RwX2tlcm5lbChlbmFibGVfZmxhc2glM0RUcnVlJTJDJTIwZW5hYmxlX21hdGglM0RGYWxzZSUyQyUyMGVuYWJsZV9tZW1fZWZmaWNpZW50JTNERmFsc2UpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0cyU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, quantization_config=quantization_config)

input_text = <span class="hljs-string">&quot;Hello my dog is cute and&quot;</span>
inputs = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-keyword">with</span> torch.backends.cuda.sdp_kernel(enable_flash=<span class="hljs-literal">True</span>, enable_math=<span class="hljs-literal">False</span>, enable_mem_efficient=<span class="hljs-literal">False</span>):
    outputs = model.generate(**inputs)

<span class="hljs-built_in">print</span>(tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){m=i("meta"),y=s(),M=i("p"),J=s(),r(v.$$.fragment),h=s(),g=i("p"),g.innerHTML=Gn,Wt=s(),r(G.$$.fragment),Zt=s(),r(_.$$.fragment),jt=s(),I=i("p"),I.innerHTML=In,Bt=s(),F=i("p"),F.textContent=Fn,kt=s(),R=i("p"),R.textContent=Rn,Gt=s(),X=i("ul"),X.innerHTML=Xn,It=s(),V=i("p"),V.innerHTML=Vn,Ft=s(),r(C.$$.fragment),Rt=s(),r(H.$$.fragment),Xt=s(),L=i("p"),L.innerHTML=Hn,Vt=s(),r(P.$$.fragment),Ht=s(),A=i("p"),A.textContent=Ln,Lt=s(),r(N.$$.fragment),Pt=s(),z=i("p"),z.textContent=Pn,At=s(),q=i("p"),q.innerHTML=An,Nt=s(),E=i("p"),E.innerHTML=Nn,zt=s(),x=i("div"),x.innerHTML=zn,qt=s(),Q=i("p"),Q.innerHTML=qn,Et=s(),U=i("div"),U.innerHTML=En,Qt=s(),Y=i("p"),Y.textContent=Qn,Yt=s(),W=i("div"),W.innerHTML=Yn,St=s(),S=i("p"),S.textContent=Sn,Dt=s(),D=i("p"),D.innerHTML=Dn,Kt=s(),Z=i("div"),Z.innerHTML=Kn,Ot=s(),r(K.$$.fragment),el=s(),O=i("p"),O.textContent=On,tl=s(),r(ee.$$.fragment),ll=s(),te=i("p"),te.textContent=es,nl=s(),r(le.$$.fragment),sl=s(),r(ne.$$.fragment),al=s(),se=i("p"),se.textContent=ts,il=s(),r(ae.$$.fragment),ol=s(),r(ie.$$.fragment),pl=s(),oe=i("p"),oe.textContent=ls,ml=s(),r(pe.$$.fragment),rl=s(),r(me.$$.fragment),fl=s(),re=i("p"),re.innerHTML=ns,dl=s(),fe=i("p"),fe.textContent=ss,ul=s(),r(j.$$.fragment),bl=s(),r(de.$$.fragment),Tl=s(),ue=i("p"),ue.innerHTML=as,Ml=s(),be=i("p"),be.innerHTML=is,cl=s(),Te=i("p"),Te.innerHTML=os,Jl=s(),r(Me.$$.fragment),yl=s(),ce=i("p"),ce.innerHTML=ps,$l=s(),r(Je.$$.fragment),hl=s(),ye=i("p"),ye.innerHTML=ms,wl=s(),r($e.$$.fragment),gl=s(),he=i("p"),he.innerHTML=rs,vl=s(),we=i("p"),we.textContent=fs,_l=s(),r(ge.$$.fragment),Cl=s(),ve=i("p"),ve.innerHTML=ds,xl=s(),r(_e.$$.fragment),Ul=s(),Ce=i("p"),Ce.textContent=us,Wl=s(),r(xe.$$.fragment),Zl=s(),Ue=i("p"),Ue.textContent=bs,jl=s(),r(We.$$.fragment),Bl=s(),Ze=i("p"),Ze.textContent=Ts,kl=s(),je=i("p"),je.textContent=Ms,Gl=s(),Be=i("p"),Be.innerHTML=cs,Il=s(),ke=i("p"),ke.innerHTML=Js,Fl=s(),r(Ge.$$.fragment),Rl=s(),Ie=i("p"),Ie.innerHTML=ys,Xl=s(),Fe=i("p"),Fe.innerHTML=$s,Vl=s(),Re=i("p"),Re.innerHTML=hs,Hl=s(),r(B.$$.fragment),Ll=s(),r(Xe.$$.fragment),Pl=s(),Ve=i("ul"),Ve.innerHTML=ws,Al=s(),r(He.$$.fragment),Nl=s(),Le=i("p"),Le.textContent=gs,zl=s(),r(Pe.$$.fragment),ql=s(),Ae=i("p"),Ae.innerHTML=vs,El=s(),r(Ne.$$.fragment),Ql=s(),ze=i("p"),ze.textContent=_s,Yl=s(),r(qe.$$.fragment),Sl=s(),Ee=i("p"),Ee.innerHTML=Cs,Dl=s(),r(Qe.$$.fragment),Kl=s(),Ye=i("p"),Ye.textContent=xs,Ol=s(),r(Se.$$.fragment),en=s(),De=i("p"),De.innerHTML=Us,tn=s(),r(Ke.$$.fragment),ln=s(),r(k.$$.fragment),nn=s(),Oe=i("p"),Oe.innerHTML=Ws,sn=s(),et=i("p"),et.innerHTML=Zs,an=s(),tt=i("p"),tt.innerHTML=js,on=s(),lt=i("p"),lt.innerHTML=Bs,pn=s(),nt=i("p"),nt.innerHTML=ks,mn=s(),r(st.$$.fragment),rn=s(),at=i("ul"),at.innerHTML=Gs,fn=s(),r(it.$$.fragment),dn=s(),ot=i("p"),ot.textContent=Is,un=s(),r(pt.$$.fragment),bn=s(),mt=i("p"),mt.textContent=Fs,Tn=s(),rt=i("ul"),rt.innerHTML=Rs,Mn=s(),r(ft.$$.fragment),cn=s(),r(dt.$$.fragment),Jn=s(),ut=i("p"),ut.textContent=Xs,yn=s(),r(bt.$$.fragment),$n=s(),Tt=i("p"),Tt.innerHTML=Vs,hn=s(),r(Mt.$$.fragment),wn=s(),ct=i("p"),ct.textContent=Hs,gn=s(),r(Jt.$$.fragment),vn=s(),yt=i("p"),yt.textContent=Ls,_n=s(),$t=i("p"),$t.innerHTML=Ps,Cn=s(),ht=i("p"),ht.textContent=As,xn=s(),wt=i("p"),wt.innerHTML=Ns,Un=s(),r(gt.$$.fragment),Wn=s(),vt=i("p"),vt.textContent=zs,Zn=s(),r(_t.$$.fragment),jn=s(),Ct=i("p"),this.h()},l(e){const t=na("svelte-u9bgzb",document.head);m=o(t,"META",{name:!0,content:!0}),t.forEach(l),y=a(e),M=o(e,"P",{}),Ds(M).forEach(l),J=a(e),f(v.$$.fragment,e),h=a(e),g=o(e,"P",{"data-svelte-h":!0}),p(g)!=="svelte-fzdu4j"&&(g.innerHTML=Gn),Wt=a(e),f(G.$$.fragment,e),Zt=a(e),f(_.$$.fragment,e),jt=a(e),I=o(e,"P",{"data-svelte-h":!0}),p(I)!=="svelte-1hue4gy"&&(I.innerHTML=In),Bt=a(e),F=o(e,"P",{"data-svelte-h":!0}),p(F)!=="svelte-uc6qdz"&&(F.textContent=Fn),kt=a(e),R=o(e,"P",{"data-svelte-h":!0}),p(R)!=="svelte-10j7r84"&&(R.textContent=Rn),Gt=a(e),X=o(e,"UL",{"data-svelte-h":!0}),p(X)!=="svelte-7xk7rk"&&(X.innerHTML=Xn),It=a(e),V=o(e,"P",{"data-svelte-h":!0}),p(V)!=="svelte-1xg4p19"&&(V.innerHTML=Vn),Ft=a(e),f(C.$$.fragment,e),Rt=a(e),f(H.$$.fragment,e),Xt=a(e),L=o(e,"P",{"data-svelte-h":!0}),p(L)!=="svelte-qebdou"&&(L.innerHTML=Hn),Vt=a(e),f(P.$$.fragment,e),Ht=a(e),A=o(e,"P",{"data-svelte-h":!0}),p(A)!=="svelte-8hn31v"&&(A.textContent=Ln),Lt=a(e),f(N.$$.fragment,e),Pt=a(e),z=o(e,"P",{"data-svelte-h":!0}),p(z)!=="svelte-d598nm"&&(z.textContent=Pn),At=a(e),q=o(e,"P",{"data-svelte-h":!0}),p(q)!=="svelte-95y2nm"&&(q.innerHTML=An),Nt=a(e),E=o(e,"P",{"data-svelte-h":!0}),p(E)!=="svelte-g8drlk"&&(E.innerHTML=Nn),zt=a(e),x=o(e,"DIV",{style:!0,"data-svelte-h":!0}),p(x)!=="svelte-u3wzwi"&&(x.innerHTML=zn),qt=a(e),Q=o(e,"P",{"data-svelte-h":!0}),p(Q)!=="svelte-1v69pah"&&(Q.innerHTML=qn),Et=a(e),U=o(e,"DIV",{style:!0,"data-svelte-h":!0}),p(U)!=="svelte-1yuov6e"&&(U.innerHTML=En),Qt=a(e),Y=o(e,"P",{"data-svelte-h":!0}),p(Y)!=="svelte-1v37jvv"&&(Y.textContent=Qn),Yt=a(e),W=o(e,"DIV",{style:!0,"data-svelte-h":!0}),p(W)!=="svelte-cixhj1"&&(W.innerHTML=Yn),St=a(e),S=o(e,"P",{"data-svelte-h":!0}),p(S)!=="svelte-1nalxxh"&&(S.textContent=Sn),Dt=a(e),D=o(e,"P",{"data-svelte-h":!0}),p(D)!=="svelte-1u8lofd"&&(D.innerHTML=Dn),Kt=a(e),Z=o(e,"DIV",{style:!0,"data-svelte-h":!0}),p(Z)!=="svelte-13f0ql9"&&(Z.innerHTML=Kn),Ot=a(e),f(K.$$.fragment,e),el=a(e),O=o(e,"P",{"data-svelte-h":!0}),p(O)!=="svelte-153ejfa"&&(O.textContent=On),tl=a(e),f(ee.$$.fragment,e),ll=a(e),te=o(e,"P",{"data-svelte-h":!0}),p(te)!=="svelte-rj8paa"&&(te.textContent=es),nl=a(e),f(le.$$.fragment,e),sl=a(e),f(ne.$$.fragment,e),al=a(e),se=o(e,"P",{"data-svelte-h":!0}),p(se)!=="svelte-eee45y"&&(se.textContent=ts),il=a(e),f(ae.$$.fragment,e),ol=a(e),f(ie.$$.fragment,e),pl=a(e),oe=o(e,"P",{"data-svelte-h":!0}),p(oe)!=="svelte-1nd12n2"&&(oe.textContent=ls),ml=a(e),f(pe.$$.fragment,e),rl=a(e),f(me.$$.fragment,e),fl=a(e),re=o(e,"P",{"data-svelte-h":!0}),p(re)!=="svelte-1mxj58t"&&(re.innerHTML=ns),dl=a(e),fe=o(e,"P",{"data-svelte-h":!0}),p(fe)!=="svelte-1ncf36d"&&(fe.textContent=ss),ul=a(e),f(j.$$.fragment,e),bl=a(e),f(de.$$.fragment,e),Tl=a(e),ue=o(e,"P",{"data-svelte-h":!0}),p(ue)!=="svelte-1460ksx"&&(ue.innerHTML=as),Ml=a(e),be=o(e,"P",{"data-svelte-h":!0}),p(be)!=="svelte-obqyae"&&(be.innerHTML=is),cl=a(e),Te=o(e,"P",{"data-svelte-h":!0}),p(Te)!=="svelte-304wy3"&&(Te.innerHTML=os),Jl=a(e),f(Me.$$.fragment,e),yl=a(e),ce=o(e,"P",{"data-svelte-h":!0}),p(ce)!=="svelte-jcksqc"&&(ce.innerHTML=ps),$l=a(e),f(Je.$$.fragment,e),hl=a(e),ye=o(e,"P",{"data-svelte-h":!0}),p(ye)!=="svelte-bx0bhv"&&(ye.innerHTML=ms),wl=a(e),f($e.$$.fragment,e),gl=a(e),he=o(e,"P",{"data-svelte-h":!0}),p(he)!=="svelte-t1pi44"&&(he.innerHTML=rs),vl=a(e),we=o(e,"P",{"data-svelte-h":!0}),p(we)!=="svelte-1k16omg"&&(we.textContent=fs),_l=a(e),f(ge.$$.fragment,e),Cl=a(e),ve=o(e,"P",{"data-svelte-h":!0}),p(ve)!=="svelte-ov9ztb"&&(ve.innerHTML=ds),xl=a(e),f(_e.$$.fragment,e),Ul=a(e),Ce=o(e,"P",{"data-svelte-h":!0}),p(Ce)!=="svelte-1rokqgh"&&(Ce.textContent=us),Wl=a(e),f(xe.$$.fragment,e),Zl=a(e),Ue=o(e,"P",{"data-svelte-h":!0}),p(Ue)!=="svelte-ptizo3"&&(Ue.textContent=bs),jl=a(e),f(We.$$.fragment,e),Bl=a(e),Ze=o(e,"P",{"data-svelte-h":!0}),p(Ze)!=="svelte-1chszbv"&&(Ze.textContent=Ts),kl=a(e),je=o(e,"P",{"data-svelte-h":!0}),p(je)!=="svelte-2wjwue"&&(je.textContent=Ms),Gl=a(e),Be=o(e,"P",{"data-svelte-h":!0}),p(Be)!=="svelte-1c2vyan"&&(Be.innerHTML=cs),Il=a(e),ke=o(e,"P",{"data-svelte-h":!0}),p(ke)!=="svelte-1u8mhyg"&&(ke.innerHTML=Js),Fl=a(e),f(Ge.$$.fragment,e),Rl=a(e),Ie=o(e,"P",{"data-svelte-h":!0}),p(Ie)!=="svelte-15spw9f"&&(Ie.innerHTML=ys),Xl=a(e),Fe=o(e,"P",{"data-svelte-h":!0}),p(Fe)!=="svelte-1f2adro"&&(Fe.innerHTML=$s),Vl=a(e),Re=o(e,"P",{"data-svelte-h":!0}),p(Re)!=="svelte-1y9rpsh"&&(Re.innerHTML=hs),Hl=a(e),f(B.$$.fragment,e),Ll=a(e),f(Xe.$$.fragment,e),Pl=a(e),Ve=o(e,"UL",{"data-svelte-h":!0}),p(Ve)!=="svelte-1aope0v"&&(Ve.innerHTML=ws),Al=a(e),f(He.$$.fragment,e),Nl=a(e),Le=o(e,"P",{"data-svelte-h":!0}),p(Le)!=="svelte-1j88lhl"&&(Le.textContent=gs),zl=a(e),f(Pe.$$.fragment,e),ql=a(e),Ae=o(e,"P",{"data-svelte-h":!0}),p(Ae)!=="svelte-1k9z2lk"&&(Ae.innerHTML=vs),El=a(e),f(Ne.$$.fragment,e),Ql=a(e),ze=o(e,"P",{"data-svelte-h":!0}),p(ze)!=="svelte-16qk9l"&&(ze.textContent=_s),Yl=a(e),f(qe.$$.fragment,e),Sl=a(e),Ee=o(e,"P",{"data-svelte-h":!0}),p(Ee)!=="svelte-1gi1mku"&&(Ee.innerHTML=Cs),Dl=a(e),f(Qe.$$.fragment,e),Kl=a(e),Ye=o(e,"P",{"data-svelte-h":!0}),p(Ye)!=="svelte-1x3md4a"&&(Ye.textContent=xs),Ol=a(e),f(Se.$$.fragment,e),en=a(e),De=o(e,"P",{"data-svelte-h":!0}),p(De)!=="svelte-1r2xmm8"&&(De.innerHTML=Us),tn=a(e),f(Ke.$$.fragment,e),ln=a(e),f(k.$$.fragment,e),nn=a(e),Oe=o(e,"P",{"data-svelte-h":!0}),p(Oe)!=="svelte-1egajcb"&&(Oe.innerHTML=Ws),sn=a(e),et=o(e,"P",{"data-svelte-h":!0}),p(et)!=="svelte-1tsrdqi"&&(et.innerHTML=Zs),an=a(e),tt=o(e,"P",{"data-svelte-h":!0}),p(tt)!=="svelte-q484rg"&&(tt.innerHTML=js),on=a(e),lt=o(e,"P",{"data-svelte-h":!0}),p(lt)!=="svelte-y2mgdg"&&(lt.innerHTML=Bs),pn=a(e),nt=o(e,"P",{"data-svelte-h":!0}),p(nt)!=="svelte-12wp0pi"&&(nt.innerHTML=ks),mn=a(e),f(st.$$.fragment,e),rn=a(e),at=o(e,"UL",{"data-svelte-h":!0}),p(at)!=="svelte-1xtnufy"&&(at.innerHTML=Gs),fn=a(e),f(it.$$.fragment,e),dn=a(e),ot=o(e,"P",{"data-svelte-h":!0}),p(ot)!=="svelte-6ppaxs"&&(ot.textContent=Is),un=a(e),f(pt.$$.fragment,e),bn=a(e),mt=o(e,"P",{"data-svelte-h":!0}),p(mt)!=="svelte-fbzfro"&&(mt.textContent=Fs),Tn=a(e),rt=o(e,"UL",{"data-svelte-h":!0}),p(rt)!=="svelte-aixuph"&&(rt.innerHTML=Rs),Mn=a(e),f(ft.$$.fragment,e),cn=a(e),f(dt.$$.fragment,e),Jn=a(e),ut=o(e,"P",{"data-svelte-h":!0}),p(ut)!=="svelte-8ubh6h"&&(ut.textContent=Xs),yn=a(e),f(bt.$$.fragment,e),$n=a(e),Tt=o(e,"P",{"data-svelte-h":!0}),p(Tt)!=="svelte-bn1lgg"&&(Tt.innerHTML=Vs),hn=a(e),f(Mt.$$.fragment,e),wn=a(e),ct=o(e,"P",{"data-svelte-h":!0}),p(ct)!=="svelte-1fnofha"&&(ct.textContent=Hs),gn=a(e),f(Jt.$$.fragment,e),vn=a(e),yt=o(e,"P",{"data-svelte-h":!0}),p(yt)!=="svelte-xttkwq"&&(yt.textContent=Ls),_n=a(e),$t=o(e,"P",{"data-svelte-h":!0}),p($t)!=="svelte-1yb5ek4"&&($t.innerHTML=Ps),Cn=a(e),ht=o(e,"P",{"data-svelte-h":!0}),p(ht)!=="svelte-310aim"&&(ht.textContent=As),xn=a(e),wt=o(e,"P",{"data-svelte-h":!0}),p(wt)!=="svelte-6z7881"&&(wt.innerHTML=Ns),Un=a(e),f(gt.$$.fragment,e),Wn=a(e),vt=o(e,"P",{"data-svelte-h":!0}),p(vt)!=="svelte-1xkwmw9"&&(vt.textContent=zs),Zn=a(e),f(_t.$$.fragment,e),jn=a(e),Ct=o(e,"P",{}),Ds(Ct).forEach(l),this.h()},h(){Ks(m,"name","hf:doc:metadata"),Ks(m,"content",fa),kn(x,"text-align","center"),kn(U,"text-align","center"),kn(W,"text-align","center"),kn(Z,"text-align","center")},m(e,t){sa(document.head,m),n(e,y,t),n(e,M,t),n(e,J,t),d(v,e,t),n(e,h,t),n(e,g,t),n(e,Wt,t),d(G,e,t),n(e,Zt,t),d(_,e,t),n(e,jt,t),n(e,I,t),n(e,Bt,t),n(e,F,t),n(e,kt,t),n(e,R,t),n(e,Gt,t),n(e,X,t),n(e,It,t),n(e,V,t),n(e,Ft,t),d(C,e,t),n(e,Rt,t),d(H,e,t),n(e,Xt,t),n(e,L,t),n(e,Vt,t),d(P,e,t),n(e,Ht,t),n(e,A,t),n(e,Lt,t),d(N,e,t),n(e,Pt,t),n(e,z,t),n(e,At,t),n(e,q,t),n(e,Nt,t),n(e,E,t),n(e,zt,t),n(e,x,t),n(e,qt,t),n(e,Q,t),n(e,Et,t),n(e,U,t),n(e,Qt,t),n(e,Y,t),n(e,Yt,t),n(e,W,t),n(e,St,t),n(e,S,t),n(e,Dt,t),n(e,D,t),n(e,Kt,t),n(e,Z,t),n(e,Ot,t),d(K,e,t),n(e,el,t),n(e,O,t),n(e,tl,t),d(ee,e,t),n(e,ll,t),n(e,te,t),n(e,nl,t),d(le,e,t),n(e,sl,t),d(ne,e,t),n(e,al,t),n(e,se,t),n(e,il,t),d(ae,e,t),n(e,ol,t),d(ie,e,t),n(e,pl,t),n(e,oe,t),n(e,ml,t),d(pe,e,t),n(e,rl,t),d(me,e,t),n(e,fl,t),n(e,re,t),n(e,dl,t),n(e,fe,t),n(e,ul,t),d(j,e,t),n(e,bl,t),d(de,e,t),n(e,Tl,t),n(e,ue,t),n(e,Ml,t),n(e,be,t),n(e,cl,t),n(e,Te,t),n(e,Jl,t),d(Me,e,t),n(e,yl,t),n(e,ce,t),n(e,$l,t),d(Je,e,t),n(e,hl,t),n(e,ye,t),n(e,wl,t),d($e,e,t),n(e,gl,t),n(e,he,t),n(e,vl,t),n(e,we,t),n(e,_l,t),d(ge,e,t),n(e,Cl,t),n(e,ve,t),n(e,xl,t),d(_e,e,t),n(e,Ul,t),n(e,Ce,t),n(e,Wl,t),d(xe,e,t),n(e,Zl,t),n(e,Ue,t),n(e,jl,t),d(We,e,t),n(e,Bl,t),n(e,Ze,t),n(e,kl,t),n(e,je,t),n(e,Gl,t),n(e,Be,t),n(e,Il,t),n(e,ke,t),n(e,Fl,t),d(Ge,e,t),n(e,Rl,t),n(e,Ie,t),n(e,Xl,t),n(e,Fe,t),n(e,Vl,t),n(e,Re,t),n(e,Hl,t),d(B,e,t),n(e,Ll,t),d(Xe,e,t),n(e,Pl,t),n(e,Ve,t),n(e,Al,t),d(He,e,t),n(e,Nl,t),n(e,Le,t),n(e,zl,t),d(Pe,e,t),n(e,ql,t),n(e,Ae,t),n(e,El,t),d(Ne,e,t),n(e,Ql,t),n(e,ze,t),n(e,Yl,t),d(qe,e,t),n(e,Sl,t),n(e,Ee,t),n(e,Dl,t),d(Qe,e,t),n(e,Kl,t),n(e,Ye,t),n(e,Ol,t),d(Se,e,t),n(e,en,t),n(e,De,t),n(e,tn,t),d(Ke,e,t),n(e,ln,t),d(k,e,t),n(e,nn,t),n(e,Oe,t),n(e,sn,t),n(e,et,t),n(e,an,t),n(e,tt,t),n(e,on,t),n(e,lt,t),n(e,pn,t),n(e,nt,t),n(e,mn,t),d(st,e,t),n(e,rn,t),n(e,at,t),n(e,fn,t),d(it,e,t),n(e,dn,t),n(e,ot,t),n(e,un,t),d(pt,e,t),n(e,bn,t),n(e,mt,t),n(e,Tn,t),n(e,rt,t),n(e,Mn,t),d(ft,e,t),n(e,cn,t),d(dt,e,t),n(e,Jn,t),n(e,ut,t),n(e,yn,t),d(bt,e,t),n(e,$n,t),n(e,Tt,t),n(e,hn,t),d(Mt,e,t),n(e,wn,t),n(e,ct,t),n(e,gn,t),d(Jt,e,t),n(e,vn,t),n(e,yt,t),n(e,_n,t),n(e,$t,t),n(e,Cn,t),n(e,ht,t),n(e,xn,t),n(e,wt,t),n(e,Un,t),d(gt,e,t),n(e,Wn,t),n(e,vt,t),n(e,Zn,t),d(_t,e,t),n(e,jn,t),n(e,Ct,t),Bn=!0},p(e,[t]){const qs={};t&2&&(qs.$$scope={dirty:t,ctx:e}),_.$set(qs);const Es={};t&2&&(Es.$$scope={dirty:t,ctx:e}),C.$set(Es);const Qs={};t&2&&(Qs.$$scope={dirty:t,ctx:e}),j.$set(Qs);const Ys={};t&2&&(Ys.$$scope={dirty:t,ctx:e}),B.$set(Ys);const Ss={};t&2&&(Ss.$$scope={dirty:t,ctx:e}),k.$set(Ss)},i(e){Bn||(u(v.$$.fragment,e),u(G.$$.fragment,e),u(_.$$.fragment,e),u(C.$$.fragment,e),u(H.$$.fragment,e),u(P.$$.fragment,e),u(N.$$.fragment,e),u(K.$$.fragment,e),u(ee.$$.fragment,e),u(le.$$.fragment,e),u(ne.$$.fragment,e),u(ae.$$.fragment,e),u(ie.$$.fragment,e),u(pe.$$.fragment,e),u(me.$$.fragment,e),u(j.$$.fragment,e),u(de.$$.fragment,e),u(Me.$$.fragment,e),u(Je.$$.fragment,e),u($e.$$.fragment,e),u(ge.$$.fragment,e),u(_e.$$.fragment,e),u(xe.$$.fragment,e),u(We.$$.fragment,e),u(Ge.$$.fragment,e),u(B.$$.fragment,e),u(Xe.$$.fragment,e),u(He.$$.fragment,e),u(Pe.$$.fragment,e),u(Ne.$$.fragment,e),u(qe.$$.fragment,e),u(Qe.$$.fragment,e),u(Se.$$.fragment,e),u(Ke.$$.fragment,e),u(k.$$.fragment,e),u(st.$$.fragment,e),u(it.$$.fragment,e),u(pt.$$.fragment,e),u(ft.$$.fragment,e),u(dt.$$.fragment,e),u(bt.$$.fragment,e),u(Mt.$$.fragment,e),u(Jt.$$.fragment,e),u(gt.$$.fragment,e),u(_t.$$.fragment,e),Bn=!0)},o(e){b(v.$$.fragment,e),b(G.$$.fragment,e),b(_.$$.fragment,e),b(C.$$.fragment,e),b(H.$$.fragment,e),b(P.$$.fragment,e),b(N.$$.fragment,e),b(K.$$.fragment,e),b(ee.$$.fragment,e),b(le.$$.fragment,e),b(ne.$$.fragment,e),b(ae.$$.fragment,e),b(ie.$$.fragment,e),b(pe.$$.fragment,e),b(me.$$.fragment,e),b(j.$$.fragment,e),b(de.$$.fragment,e),b(Me.$$.fragment,e),b(Je.$$.fragment,e),b($e.$$.fragment,e),b(ge.$$.fragment,e),b(_e.$$.fragment,e),b(xe.$$.fragment,e),b(We.$$.fragment,e),b(Ge.$$.fragment,e),b(B.$$.fragment,e),b(Xe.$$.fragment,e),b(He.$$.fragment,e),b(Pe.$$.fragment,e),b(Ne.$$.fragment,e),b(qe.$$.fragment,e),b(Qe.$$.fragment,e),b(Se.$$.fragment,e),b(Ke.$$.fragment,e),b(k.$$.fragment,e),b(st.$$.fragment,e),b(it.$$.fragment,e),b(pt.$$.fragment,e),b(ft.$$.fragment,e),b(dt.$$.fragment,e),b(bt.$$.fragment,e),b(Mt.$$.fragment,e),b(Jt.$$.fragment,e),b(gt.$$.fragment,e),b(_t.$$.fragment,e),Bn=!1},d(e){e&&(l(y),l(M),l(J),l(h),l(g),l(Wt),l(Zt),l(jt),l(I),l(Bt),l(F),l(kt),l(R),l(Gt),l(X),l(It),l(V),l(Ft),l(Rt),l(Xt),l(L),l(Vt),l(Ht),l(A),l(Lt),l(Pt),l(z),l(At),l(q),l(Nt),l(E),l(zt),l(x),l(qt),l(Q),l(Et),l(U),l(Qt),l(Y),l(Yt),l(W),l(St),l(S),l(Dt),l(D),l(Kt),l(Z),l(Ot),l(el),l(O),l(tl),l(ll),l(te),l(nl),l(sl),l(al),l(se),l(il),l(ol),l(pl),l(oe),l(ml),l(rl),l(fl),l(re),l(dl),l(fe),l(ul),l(bl),l(Tl),l(ue),l(Ml),l(be),l(cl),l(Te),l(Jl),l(yl),l(ce),l($l),l(hl),l(ye),l(wl),l(gl),l(he),l(vl),l(we),l(_l),l(Cl),l(ve),l(xl),l(Ul),l(Ce),l(Wl),l(Zl),l(Ue),l(jl),l(Bl),l(Ze),l(kl),l(je),l(Gl),l(Be),l(Il),l(ke),l(Fl),l(Rl),l(Ie),l(Xl),l(Fe),l(Vl),l(Re),l(Hl),l(Ll),l(Pl),l(Ve),l(Al),l(Nl),l(Le),l(zl),l(ql),l(Ae),l(El),l(Ql),l(ze),l(Yl),l(Sl),l(Ee),l(Dl),l(Kl),l(Ye),l(Ol),l(en),l(De),l(tn),l(ln),l(nn),l(Oe),l(sn),l(et),l(an),l(tt),l(on),l(lt),l(pn),l(nt),l(mn),l(rn),l(at),l(fn),l(dn),l(ot),l(un),l(bn),l(mt),l(Tn),l(rt),l(Mn),l(cn),l(Jn),l(ut),l(yn),l($n),l(Tt),l(hn),l(wn),l(ct),l(gn),l(vn),l(yt),l(_n),l($t),l(Cn),l(ht),l(xn),l(wt),l(Un),l(Wn),l(vt),l(Zn),l(jn),l(Ct)),l(m),T(v,e),T(G,e),T(_,e),T(C,e),T(H,e),T(P,e),T(N,e),T(K,e),T(ee,e),T(le,e),T(ne,e),T(ae,e),T(ie,e),T(pe,e),T(me,e),T(j,e),T(de,e),T(Me,e),T(Je,e),T($e,e),T(ge,e),T(_e,e),T(xe,e),T(We,e),T(Ge,e),T(B,e),T(Xe,e),T(He,e),T(Pe,e),T(Ne,e),T(qe,e),T(Qe,e),T(Se,e),T(Ke,e),T(k,e),T(st,e),T(it,e),T(pt,e),T(ft,e),T(dt,e),T(bt,e),T(Mt,e),T(Jt,e),T(gt,e),T(_t,e)}}}const fa='{"title":"Efficient Inference on a Single GPU","local":"efficient-inference-on-a-single-gpu","sections":[{"title":"Flash Attention 2","local":"flash-attention-2","sections":[{"title":"Quick usage","local":"quick-usage","sections":[],"depth":3},{"title":"Expected speedups","local":"expected-speedups","sections":[],"depth":3},{"title":"Advanced usage","local":"advanced-usage","sections":[],"depth":3},{"title":"Combining Flash Attention 2 and 8-bit models","local":"combining-flash-attention-2-and-8-bit-models","sections":[],"depth":3},{"title":"Combining Flash Attention 2 and 4-bit models","local":"combining-flash-attention-2-and-4-bit-models","sections":[],"depth":3},{"title":"Combining Flash Attention 2 and PEFT","local":"combining-flash-attention-2-and-peft","sections":[],"depth":3}],"depth":2},{"title":"BetterTransformer","local":"bettertransformer","sections":[{"title":"Encoder models","local":"encoder-models","sections":[],"depth":3},{"title":"Decoder models","local":"decoder-models","sections":[],"depth":3}],"depth":2},{"title":"bitsandbytes integration for FP4 mixed-precision inference","local":"bitsandbytes-integration-for-fp4-mixed-precision-inference","sections":[{"title":"Requirements","local":"requirements-for-fp4-mixedprecision-inference","sections":[],"depth":3},{"title":"Running FP4 models - single GPU setup - Quickstart","local":"running-fp4-models---single-gpu-setup---quickstart","sections":[],"depth":3},{"title":"Running FP4 models - multi GPU setup","local":"running-fp4-models---multi-gpu-setup","sections":[],"depth":3},{"title":"Advanced usage","local":"advanced-usage","sections":[],"depth":3}],"depth":2},{"title":"bitsandbytes integration for Int8 mixed-precision matrix decomposition","local":"bitsandbytes-integration-for-int8-mixed-precision-matrix-decomposition","sections":[{"title":"Requirements","local":"requirements-for-int8-mixedprecision-matrix-decomposition","sections":[],"depth":3},{"title":"Running mixed-Int8 models - single GPU setup","local":"running-mixed-int8-models---single-gpu-setup","sections":[],"depth":3},{"title":"Running mixed-int8 models - multi GPU setup","local":"running-mixed-int8-models---multi-gpu-setup","sections":[],"depth":3},{"title":"Colab demos","local":"colab-demos","sections":[],"depth":3}],"depth":2},{"title":"Advanced usage: mixing FP4 (or Int8) and BetterTransformer","local":"advanced-usage-mixing-fp4-or-int8-and-bettertransformer","sections":[],"depth":2}],"depth":1}';function da(w){return ea(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ja extends ta{constructor(m){super(),la(this,m,da,ra,Os,{})}}export{Ja as component};
