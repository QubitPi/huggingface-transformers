import{s as It,f as Ft,o as Wt,n as Te}from"../chunks/scheduler.9bc65507.js";import{S as Ut,i as Jt,g as l,s as r,r as u,A as Nt,h as d,f as n,c as i,j as ce,u as g,x as f,k as J,y as p,a as o,v as _,d as b,t as w,w as M}from"../chunks/index.707bf1b6.js";import{T as Dt}from"../chunks/Tip.c2ecdbf4.js";import{D as Me}from"../chunks/Docstring.17db21ae.js";import{C as it}from"../chunks/CodeBlock.54a9f38d.js";import{E as rt}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as Zt}from"../chunks/PipelineTag.44585822.js";import{H as me}from"../chunks/Heading.342b1fa6.js";function kt(C){let a,T="Example:",c,m,h;return m=new it({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERpbmF0Q29uZmlnJTJDJTIwRGluYXRNb2RlbCUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBEaW5hdCUyMHNoaS1sYWJzJTJGZGluYXQtbWluaS1pbjFrLTIyNCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBEaW5hdENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBzaGktbGFicyUyRmRpbmF0LW1pbmktaW4xay0yMjQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMERpbmF0TW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DinatConfig, DinatModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Dinat shi-labs/dinat-mini-in1k-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DinatConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the shi-labs/dinat-mini-in1k-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DinatModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){a=l("p"),a.textContent=T,c=r(),u(m.$$.fragment)},l(s){a=d(s,"P",{"data-svelte-h":!0}),f(a)!=="svelte-11lpom8"&&(a.textContent=T),c=i(s),g(m.$$.fragment,s)},m(s,$){o(s,a,$),o(s,c,$),_(m,s,$),h=!0},p:Te,i(s){h||(b(m.$$.fragment,s),h=!0)},o(s){w(m.$$.fragment,s),h=!1},d(s){s&&(n(a),n(c)),M(m,s)}}}function zt(C){let a,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){a=l("p"),a.innerHTML=T},l(c){a=d(c,"P",{"data-svelte-h":!0}),f(a)!=="svelte-fincs2"&&(a.innerHTML=T)},m(c,m){o(c,a,m)},p:Te,d(c){c&&n(a)}}}function Gt(C){let a,T="Example:",c,m,h;return m=new it({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMERpbmF0TW9kZWwlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMnNoaS1sYWJzJTJGZGluYXQtbWluaS1pbjFrLTIyNCUyMiklMEFtb2RlbCUyMCUzRCUyMERpbmF0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMnNoaS1sYWJzJTJGZGluYXQtbWluaS1pbjFrLTIyNCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBJTBBbGFzdF9oaWRkZW5fc3RhdGVzJTIwJTNEJTIwb3V0cHV0cy5sYXN0X2hpZGRlbl9zdGF0ZSUwQWxpc3QobGFzdF9oaWRkZW5fc3RhdGVzLnNoYXBlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, DinatModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;shi-labs/dinat-mini-in1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DinatModel.from_pretrained(<span class="hljs-string">&quot;shi-labs/dinat-mini-in1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">512</span>]`,wrap:!1}}),{c(){a=l("p"),a.textContent=T,c=r(),u(m.$$.fragment)},l(s){a=d(s,"P",{"data-svelte-h":!0}),f(a)!=="svelte-11lpom8"&&(a.textContent=T),c=i(s),g(m.$$.fragment,s)},m(s,$){o(s,a,$),o(s,c,$),_(m,s,$),h=!0},p:Te,i(s){h||(b(m.$$.fragment,s),h=!0)},o(s){w(m.$$.fragment,s),h=!1},d(s){s&&(n(a),n(c)),M(m,s)}}}function Rt(C){let a,T=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){a=l("p"),a.innerHTML=T},l(c){a=d(c,"P",{"data-svelte-h":!0}),f(a)!=="svelte-fincs2"&&(a.innerHTML=T)},m(c,m){o(c,a,m)},p:Te,d(c){c&&n(a)}}}function Et(C){let a,T="Example:",c,m,h;return m=new it({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMERpbmF0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyc2hpLWxhYnMlMkZkaW5hdC1taW5pLWluMWstMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwRGluYXRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJzaGktbGFicyUyRmRpbmF0LW1pbmktaW4xay0yMjQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwbG9naXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpLmxvZ2l0cyUwQSUwQSUyMyUyMG1vZGVsJTIwcHJlZGljdHMlMjBvbmUlMjBvZiUyMHRoZSUyMDEwMDAlMjBJbWFnZU5ldCUyMGNsYXNzZXMlMEFwcmVkaWN0ZWRfbGFiZWwlMjAlM0QlMjBsb2dpdHMuYXJnbWF4KC0xKS5pdGVtKCklMEFwcmludChtb2RlbC5jb25maWcuaWQybGFiZWwlNUJwcmVkaWN0ZWRfbGFiZWwlNUQp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, DinatForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;shi-labs/dinat-mini-in1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DinatForImageClassification.from_pretrained(<span class="hljs-string">&quot;shi-labs/dinat-mini-in1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){a=l("p"),a.textContent=T,c=r(),u(m.$$.fragment)},l(s){a=d(s,"P",{"data-svelte-h":!0}),f(a)!=="svelte-11lpom8"&&(a.textContent=T),c=i(s),g(m.$$.fragment,s)},m(s,$){o(s,a,$),o(s,c,$),_(m,s,$),h=!0},p:Te,i(s){h||(b(m.$$.fragment,s),h=!0)},o(s){w(m.$$.fragment,s),h=!1},d(s){s&&(n(a),n(c)),M(m,s)}}}function Pt(C){let a,T,c,m,h,s,$,$e,P,lt=`DiNAT ã¯ <a href="https://arxiv.org/abs/2209.15001" rel="nofollow">Dilated Neighborhood Attender Transformer</a> ã§ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚
Ali Hassani and Humphrey Shi.`,ye,H,dt=`<a href="nat">NAT</a> ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«ã€æ‹¡å¼µè¿‘éš£ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è¿½åŠ ã—ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ« ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã¾ã™ã€‚
ãã—ã¦ãã‚Œã¨æ¯”è¼ƒã—ã¦å¤§å¹…ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å‘ä¸ŠãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚`,ve,L,ct="è«–æ–‡ã®è¦ç´„ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚",Ce,A,mt=`<em>ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯æ€¥é€Ÿã«ã€ã•ã¾ã–ã¾ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ã‚ãŸã£ã¦æœ€ã‚‚é »ç¹ã«é©ç”¨ã•ã‚Œã‚‹æ·±å±¤å­¦ç¿’ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã® 1 ã¤ã«ãªã‚Šã¤ã¤ã‚ã‚Šã¾ã™ã€‚
ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨ã‚¿ã‚¹ã‚¯ã€‚ãƒ“ã‚¸ãƒ§ãƒ³ã§ã¯ã€å˜ç´”ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¸ã®ç¶™ç¶šçš„ãªå–ã‚Šçµ„ã¿ã«åŠ ãˆã¦ã€éšå±¤å‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒ
ã¾ãŸã€ãã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨æ—¢å­˜ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¸ã®ç°¡å˜ãªçµ±åˆã®ãŠã‹ã’ã§ã€å¤§ããªæ³¨ç›®ã‚’é›†ã‚ã¾ã—ãŸã€‚
ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯é€šå¸¸ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚° ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®è¿‘éš£ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ (NA) ãªã©ã®å±€æ‰€çš„ãªæ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚
ã¾ãŸã¯ Swin Transformer ã®ã‚·ãƒ•ãƒˆ ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ ã‚»ãƒ«ãƒ• ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã€‚è‡ªå·±æ³¨æ„ã®äºŒæ¬¡è¤‡é›‘ã•ã‚’è»½æ¸›ã™ã‚‹ã®ã«åŠ¹æœçš„ã§ã™ãŒã€
å±€æ‰€çš„ãªæ³¨æ„ã¯ã€è‡ªå·±æ³¨æ„ã®æœ€ã‚‚æœ›ã¾ã—ã„ 2 ã¤ã®ç‰¹æ€§ã‚’å¼±ã‚ã¾ã™ã€‚ãã‚Œã¯ã€é•·è·é›¢ã®ç›¸äº’ä¾å­˜æ€§ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§ã™ã€‚
ãã—ã¦å…¨ä½“çš„ãªå—å®¹é‡ã€‚ã“ã®ãƒšãƒ¼ãƒ‘ãƒ¼ã§ã¯ã€è‡ªç„¶ã§æŸ”è»Ÿã§ã€
NA ã¸ã®åŠ¹ç‡çš„ãªæ‹¡å¼µã«ã‚ˆã‚Šã€ã‚ˆã‚Šã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•æ‰ã—ã€å—å®¹é‡ã‚’ã‚¼ãƒ­ã‹ã‚‰æŒ‡æ•°é–¢æ•°çš„ã«æ‹¡å¼µã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
è¿½åŠ è²»ç”¨ã€‚ NA ã®ãƒ­ãƒ¼ã‚«ãƒ«ãªæ³¨ç›®ã¨ DiNA ã®ã¾ã°ã‚‰ãªã‚°ãƒ­ãƒ¼ãƒãƒ«ãªæ³¨ç›®ã¯ç›¸äº’ã«è£œå®Œã—åˆã†ãŸã‚ã€ç§ãŸã¡ã¯
ä¸¡æ–¹ã«åŸºã¥ã„ã¦æ§‹ç¯‰ã•ã‚ŒãŸæ–°ã—ã„éšå±¤å‹ãƒ“ã‚¸ãƒ§ãƒ³ ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã‚ã‚‹ Dilated Neighborhood Attendant Transformer (DiNAT) ã‚’å°å…¥ã—ã¾ã™ã€‚
DiNAT ã®ãƒãƒªã‚¢ãƒ³ãƒˆã¯ã€NATã€Swinã€ConvNeXt ãªã©ã®å¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«æ¯”ã¹ã¦å¤§å¹…ã«æ”¹å–„ã•ã‚Œã¦ã„ã¾ã™ã€‚
ç§ãŸã¡ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¯ã€COCO ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã«ãŠã„ã¦ Swin ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚é«˜é€Ÿã§ã€ãƒœãƒƒã‚¯ã‚¹ AP ãŒ 1.5% å„ªã‚Œã¦ã„ã¾ã™ã€‚
COCO ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ 1.3% ã®ãƒã‚¹ã‚¯ APã€ADE20K ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ 1.1% ã® mIoUã€‚
æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨çµ„ã¿åˆã‚ã›ãŸå½“ç¤¾ã®å¤§è¦æ¨¡ãƒãƒªã‚¢ãƒ³ãƒˆã¯ã€COCO (58.2 PQ) ä¸Šã®æ–°ã—ã„æœ€å…ˆç«¯ã®ãƒ‘ãƒãƒ—ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚
ãŠã‚ˆã³ ADE20K (48.5 PQ)ã€ãŠã‚ˆã³ Cityscapes (44.5 AP) ãŠã‚ˆã³ ADE20K (35.4 AP) ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ« (è¿½åŠ ãƒ‡ãƒ¼ã‚¿ãªã—)ã€‚
ã¾ãŸã€ADE20K (58.2 mIoU) ä¸Šã®æœ€å…ˆç«¯ã®ç‰¹æ®Šãªã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ«ã¨ã‚‚ä¸€è‡´ã—ã¾ã™ã€‚
éƒ½å¸‚æ™¯è¦³ (84.5 mIoU) ã§ã¯ 2 ä½ã«ãƒ©ãƒ³ã‚¯ã•ã‚Œã¦ã„ã¾ã™ (è¿½åŠ ãƒ‡ãƒ¼ã‚¿ãªã—)ã€‚</em>`,je,Z,pt,xe,B,ft=`ç•°ãªã‚‹æ‹¡å¼µå€¤ã‚’ä½¿ç”¨ã—ãŸè¿‘éš£ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã€‚
<a href="https://arxiv.org/abs/2209.15001">å…ƒã®è«–æ–‡</a>ã‹ã‚‰æŠœç²‹ã€‚`,De,q,ht=`ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ <a href="https://huggingface.co/alihassanijr" rel="nofollow">Ali Hassani</a> ã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã¾ã—ãŸã€‚
å…ƒã®ã‚³ãƒ¼ãƒ‰ã¯ <a href="https://github.com/SHI-Labs/Neighborhood-Attendance-Transformer" rel="nofollow">ã“ã“</a> ã«ã‚ã‚Šã¾ã™ã€‚`,Ie,Q,Fe,S,ut=`DiNAT ã¯ <em>ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³</em> ã¨ã—ã¦ä½¿ç”¨ã§ãã¾ã™ã€‚ ã€Œoutput<em>hidden</em>â€‹â€‹states = Trueã€ã®å ´åˆã€
<code>hidden_â€‹â€‹states</code> ã¨ <code>reshaped_hidden_â€‹â€‹states</code> ã®ä¸¡æ–¹ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚ <code>reshape_hidden_â€‹â€‹states</code> ã¯ã€<code>(batch_size, height, width, num_channels)</code> ã§ã¯ãªãã€<code>(batch, num_channels, height, width)</code> ã®å½¢çŠ¶ã‚’æŒã£ã¦ã„ã¾ã™ã€‚`,We,V,gt="ãƒãƒ¼ãƒˆï¼š",Ue,X,_t=`<li>DiNAT ã¯ã€<a href="https://github.com/SHI-Labs/NATTEN/" rel="nofollow">NATTEN</a> ã«ã‚ˆã‚‹è¿‘éš£ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨æ‹¡å¼µè¿‘éš£ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè£…ã«ä¾å­˜ã—ã¦ã„ã¾ã™ã€‚
<a href="https://shi-labs.com/natten" rel="nofollow">shi-labs.com/natten</a> ã‚’å‚ç…§ã—ã¦ã€Linux ç”¨ã®ãƒ“ãƒ«ãƒ‰æ¸ˆã¿ãƒ›ã‚¤ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã‹ã€<code>pip install natten</code> ã‚’å®Ÿè¡Œã—ã¦ã‚·ã‚¹ãƒ†ãƒ ä¸Šã«æ§‹ç¯‰ã§ãã¾ã™ã€‚
å¾Œè€…ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«æ™‚é–“ãŒã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ NATTEN ã¯ã¾ã  Windows ãƒ‡ãƒã‚¤ã‚¹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚</li> <li>ç¾æ™‚ç‚¹ã§ã¯ãƒ‘ãƒƒãƒ ã‚µã‚¤ã‚º 4 ã®ã¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚</li>`,Je,Y,Ne,O,bt="DiNAT ã®ä½¿ç”¨ã‚’é–‹å§‹ã™ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ (ğŸŒ ã§ç¤ºã•ã‚Œã¦ã„ã‚‹) ãƒªã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆã€‚",Ze,K,ke,ee,wt='<li><a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatForImageClassification">DinatForImageClassification</a> ã¯ã€ã“ã® <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ</a> ãŠã‚ˆã³ <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯</a>ã€‚</li> <li>å‚ç…§: <a href="../tasks/image_classification">ç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰</a></li>',ze,te,Mt="ã“ã“ã«å«ã‚ã‚‹ãƒªã‚½ãƒ¼ã‚¹ã®é€ä¿¡ã«èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯ã€ãŠæ°—è»½ã«ãƒ—ãƒ« ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é–‹ã„ã¦ãã ã•ã„ã€‚å¯©æŸ»ã•ã›ã¦ã„ãŸã ãã¾ã™ã€‚ãƒªã‚½ãƒ¼ã‚¹ã¯ã€æ—¢å­˜ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’è¤‡è£½ã™ã‚‹ã®ã§ã¯ãªãã€ä½•ã‹æ–°ã—ã„ã‚‚ã®ã‚’ç¤ºã™ã“ã¨ãŒç†æƒ³çš„ã§ã™ã€‚",Ge,ne,Re,y,oe,qe,pe,Tt=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatModel">DinatModel</a>. It is used to instantiate a Dinat
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the Dinat
<a href="https://huggingface.co/shi-labs/dinat-mini-in1k-224" rel="nofollow">shi-labs/dinat-mini-in1k-224</a> architecture.`,Qe,fe,$t=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Se,k,Ee,ae,Pe,D,se,Ve,he,yt=`The bare Dinat Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Xe,j,re,Ye,ue,vt='The <a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatModel">DinatModel</a> forward method, overrides the <code>__call__</code> special method.',Oe,z,Ke,G,He,ie,Le,v,le,et,ge,Ct=`Dinat Model transformer with an image classification head on top (a linear layer on top of the final hidden state
of the [CLS] token) e.g. for ImageNet.`,tt,_e,jt=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> sub-class. Use
it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,nt,x,de,ot,be,xt='The <a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatForImageClassification">DinatForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',at,R,st,E,Ae,we,Be;return h=new me({props:{title:"Dilated Neighborhood Attention Transformer",local:"dilated-neighborhood-attention-transformer",headingTag:"h1"}}),$=new me({props:{title:"Overview",local:"overview",headingTag:"h2"}}),Q=new me({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),Y=new me({props:{title:"Resources",local:"resources",headingTag:"h2"}}),K=new Zt({props:{pipeline:"image-classification"}}),ne=new me({props:{title:"DinatConfig",local:"transformers.DinatConfig",headingTag:"h2"}}),oe=new Me({props:{name:"class transformers.DinatConfig",anchor:"transformers.DinatConfig",parameters:[{name:"patch_size",val:" = 4"},{name:"num_channels",val:" = 3"},{name:"embed_dim",val:" = 64"},{name:"depths",val:" = [3, 4, 6, 5]"},{name:"num_heads",val:" = [2, 4, 8, 16]"},{name:"kernel_size",val:" = 7"},{name:"dilations",val:" = [[1, 8, 1], [1, 4, 1, 4], [1, 2, 1, 2, 1, 2], [1, 1, 1, 1, 1]]"},{name:"mlp_ratio",val:" = 3.0"},{name:"qkv_bias",val:" = True"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"drop_path_rate",val:" = 0.1"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"layer_scale_init_value",val:" = 0.0"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DinatConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4) &#x2014;
The size (resolution) of each patch. NOTE: Only patch size of 4 is supported at the moment.`,name:"patch_size"},{anchor:"transformers.DinatConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.DinatConfig.embed_dim",description:`<strong>embed_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 64) &#x2014;
Dimensionality of patch embedding.`,name:"embed_dim"},{anchor:"transformers.DinatConfig.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 4, 6, 5]</code>) &#x2014;
Number of layers in each level of the encoder.`,name:"depths"},{anchor:"transformers.DinatConfig.num_heads",description:`<strong>num_heads</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[2, 4, 8, 16]</code>) &#x2014;
Number of attention heads in each layer of the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.DinatConfig.kernel_size",description:`<strong>kernel_size</strong> (<code>int</code>, <em>optional</em>, defaults to 7) &#x2014;
Neighborhood Attention kernel size.`,name:"kernel_size"},{anchor:"transformers.DinatConfig.dilations",description:`<strong>dilations</strong> (<code>List[List[int]]</code>, <em>optional</em>, defaults to <code>[[1, 8, 1], [1, 4, 1, 4], [1, 2, 1, 2, 1, 2], [1, 1, 1, 1, 1]]</code>) &#x2014;
Dilation value of each NA layer in the Transformer encoder.`,name:"dilations"},{anchor:"transformers.DinatConfig.mlp_ratio",description:`<strong>mlp_ratio</strong> (<code>float</code>, <em>optional</em>, defaults to 3.0) &#x2014;
Ratio of MLP hidden dimensionality to embedding dimensionality.`,name:"mlp_ratio"},{anchor:"transformers.DinatConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not a learnable bias should be added to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.DinatConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings and encoder.`,name:"hidden_dropout_prob"},{anchor:"transformers.DinatConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DinatConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
Stochastic depth rate.`,name:"drop_path_rate"},{anchor:"transformers.DinatConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DinatConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DinatConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DinatConfig.layer_scale_init_value",description:`<strong>layer_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The initial value for the layer scale. Disabled if &lt;=0.`,name:"layer_scale_init_value"},{anchor:"transformers.DinatConfig.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.DinatConfig.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dinat/configuration_dinat.py#L30"}}),k=new rt({props:{anchor:"transformers.DinatConfig.example",$$slots:{default:[kt]},$$scope:{ctx:C}}}),ae=new me({props:{title:"DinatModel",local:"transformers.DinatModel",headingTag:"h2"}}),se=new Me({props:{name:"class transformers.DinatModel",anchor:"transformers.DinatModel",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.DinatModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatConfig">DinatConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dinat/modeling_dinat.py#L692"}}),re=new Me({props:{name:"forward",anchor:"transformers.DinatModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.DinatModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.DinatModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DinatModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DinatModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dinat/modeling_dinat.py#L727",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.dinat.modeling_dinat.DinatModelOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatConfig"
>DinatConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) â€” Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>, <em>optional</em>, returned when <code>add_pooling_layer=True</code> is passed) â€” Average pooling of the last layer hidden-state.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each stage) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.dinat.modeling_dinat.DinatModelOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),z=new Dt({props:{$$slots:{default:[zt]},$$scope:{ctx:C}}}),G=new rt({props:{anchor:"transformers.DinatModel.forward.example",$$slots:{default:[Gt]},$$scope:{ctx:C}}}),ie=new me({props:{title:"DinatForImageClassification",local:"transformers.DinatForImageClassification",headingTag:"h2"}}),le=new Me({props:{name:"class transformers.DinatForImageClassification",anchor:"transformers.DinatForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.DinatForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatConfig">DinatConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dinat/modeling_dinat.py#L782"}}),de=new Me({props:{name:"forward",anchor:"transformers.DinatForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.DinatForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ViTImageProcessor.<strong>call</strong>()</a>
for details.`,name:"pixel_values"},{anchor:"transformers.DinatForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DinatForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DinatForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DinatForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/dinat/modeling_dinat.py#L806",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.dinat.modeling_dinat.DinatImageClassifierOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/dinat#transformers.DinatConfig"
>DinatConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) â€” Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) â€” Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for each stage) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>reshaped_hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) â€” Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each stage) of
shape <code>(batch_size, hidden_size, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to
include the spatial dimensions.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.dinat.modeling_dinat.DinatImageClassifierOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),R=new Dt({props:{$$slots:{default:[Rt]},$$scope:{ctx:C}}}),E=new rt({props:{anchor:"transformers.DinatForImageClassification.forward.example",$$slots:{default:[Et]},$$scope:{ctx:C}}}),{c(){a=l("meta"),T=r(),c=l("p"),m=r(),u(h.$$.fragment),s=r(),u($.$$.fragment),$e=r(),P=l("p"),P.innerHTML=lt,ye=r(),H=l("p"),H.innerHTML=dt,ve=r(),L=l("p"),L.textContent=ct,Ce=r(),A=l("p"),A.innerHTML=mt,je=r(),Z=l("img"),xe=r(),B=l("small"),B.innerHTML=ft,De=r(),q=l("p"),q.innerHTML=ht,Ie=r(),u(Q.$$.fragment),Fe=r(),S=l("p"),S.innerHTML=ut,We=r(),V=l("p"),V.textContent=gt,Ue=r(),X=l("ul"),X.innerHTML=_t,Je=r(),u(Y.$$.fragment),Ne=r(),O=l("p"),O.textContent=bt,Ze=r(),u(K.$$.fragment),ke=r(),ee=l("ul"),ee.innerHTML=wt,ze=r(),te=l("p"),te.textContent=Mt,Ge=r(),u(ne.$$.fragment),Re=r(),y=l("div"),u(oe.$$.fragment),qe=r(),pe=l("p"),pe.innerHTML=Tt,Qe=r(),fe=l("p"),fe.innerHTML=$t,Se=r(),u(k.$$.fragment),Ee=r(),u(ae.$$.fragment),Pe=r(),D=l("div"),u(se.$$.fragment),Ve=r(),he=l("p"),he.innerHTML=yt,Xe=r(),j=l("div"),u(re.$$.fragment),Ye=r(),ue=l("p"),ue.innerHTML=vt,Oe=r(),u(z.$$.fragment),Ke=r(),u(G.$$.fragment),He=r(),u(ie.$$.fragment),Le=r(),v=l("div"),u(le.$$.fragment),et=r(),ge=l("p"),ge.textContent=Ct,tt=r(),_e=l("p"),_e.innerHTML=jt,nt=r(),x=l("div"),u(de.$$.fragment),ot=r(),be=l("p"),be.innerHTML=xt,at=r(),u(R.$$.fragment),st=r(),u(E.$$.fragment),Ae=r(),we=l("p"),this.h()},l(e){const t=Nt("svelte-u9bgzb",document.head);a=d(t,"META",{name:!0,content:!0}),t.forEach(n),T=i(e),c=d(e,"P",{}),ce(c).forEach(n),m=i(e),g(h.$$.fragment,e),s=i(e),g($.$$.fragment,e),$e=i(e),P=d(e,"P",{"data-svelte-h":!0}),f(P)!=="svelte-4vh80g"&&(P.innerHTML=lt),ye=i(e),H=d(e,"P",{"data-svelte-h":!0}),f(H)!=="svelte-5x2367"&&(H.innerHTML=dt),ve=i(e),L=d(e,"P",{"data-svelte-h":!0}),f(L)!=="svelte-1cv3nri"&&(L.textContent=ct),Ce=i(e),A=d(e,"P",{"data-svelte-h":!0}),f(A)!=="svelte-yhhr0s"&&(A.innerHTML=mt),je=i(e),Z=d(e,"IMG",{src:!0,alt:!0,width:!0}),xe=i(e),B=d(e,"SMALL",{"data-svelte-h":!0}),f(B)!=="svelte-1i39043"&&(B.innerHTML=ft),De=i(e),q=d(e,"P",{"data-svelte-h":!0}),f(q)!=="svelte-8ihpwm"&&(q.innerHTML=ht),Ie=i(e),g(Q.$$.fragment,e),Fe=i(e),S=d(e,"P",{"data-svelte-h":!0}),f(S)!=="svelte-hbgrnn"&&(S.innerHTML=ut),We=i(e),V=d(e,"P",{"data-svelte-h":!0}),f(V)!=="svelte-adlvo8"&&(V.textContent=gt),Ue=i(e),X=d(e,"UL",{"data-svelte-h":!0}),f(X)!=="svelte-bu636x"&&(X.innerHTML=_t),Je=i(e),g(Y.$$.fragment,e),Ne=i(e),O=d(e,"P",{"data-svelte-h":!0}),f(O)!=="svelte-15ra2ii"&&(O.textContent=bt),Ze=i(e),g(K.$$.fragment,e),ke=i(e),ee=d(e,"UL",{"data-svelte-h":!0}),f(ee)!=="svelte-17nwvol"&&(ee.innerHTML=wt),ze=i(e),te=d(e,"P",{"data-svelte-h":!0}),f(te)!=="svelte-17ytafw"&&(te.textContent=Mt),Ge=i(e),g(ne.$$.fragment,e),Re=i(e),y=d(e,"DIV",{class:!0});var I=ce(y);g(oe.$$.fragment,I),qe=i(I),pe=d(I,"P",{"data-svelte-h":!0}),f(pe)!=="svelte-c19fd9"&&(pe.innerHTML=Tt),Qe=i(I),fe=d(I,"P",{"data-svelte-h":!0}),f(fe)!=="svelte-1s6wgpv"&&(fe.innerHTML=$t),Se=i(I),g(k.$$.fragment,I),I.forEach(n),Ee=i(e),g(ae.$$.fragment,e),Pe=i(e),D=d(e,"DIV",{class:!0});var N=ce(D);g(se.$$.fragment,N),Ve=i(N),he=d(N,"P",{"data-svelte-h":!0}),f(he)!=="svelte-bq276k"&&(he.innerHTML=yt),Xe=i(N),j=d(N,"DIV",{class:!0});var F=ce(j);g(re.$$.fragment,F),Ye=i(F),ue=d(F,"P",{"data-svelte-h":!0}),f(ue)!=="svelte-1d2ruq1"&&(ue.innerHTML=vt),Oe=i(F),g(z.$$.fragment,F),Ke=i(F),g(G.$$.fragment,F),F.forEach(n),N.forEach(n),He=i(e),g(ie.$$.fragment,e),Le=i(e),v=d(e,"DIV",{class:!0});var W=ce(v);g(le.$$.fragment,W),et=i(W),ge=d(W,"P",{"data-svelte-h":!0}),f(ge)!=="svelte-17tpduv"&&(ge.textContent=Ct),tt=i(W),_e=d(W,"P",{"data-svelte-h":!0}),f(_e)!=="svelte-68lg8f"&&(_e.innerHTML=jt),nt=i(W),x=d(W,"DIV",{class:!0});var U=ce(x);g(de.$$.fragment,U),ot=i(U),be=d(U,"P",{"data-svelte-h":!0}),f(be)!=="svelte-ngm5gr"&&(be.innerHTML=xt),at=i(U),g(R.$$.fragment,U),st=i(U),g(E.$$.fragment,U),U.forEach(n),W.forEach(n),Ae=i(e),we=d(e,"P",{}),ce(we).forEach(n),this.h()},h(){J(a,"name","hf:doc:metadata"),J(a,"content",Ht),Ft(Z.src,pt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/dilated-neighborhood-attention-pattern.jpg")||J(Z,"src",pt),J(Z,"alt","drawing"),J(Z,"width","600"),J(y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(D,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(v,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){p(document.head,a),o(e,T,t),o(e,c,t),o(e,m,t),_(h,e,t),o(e,s,t),_($,e,t),o(e,$e,t),o(e,P,t),o(e,ye,t),o(e,H,t),o(e,ve,t),o(e,L,t),o(e,Ce,t),o(e,A,t),o(e,je,t),o(e,Z,t),o(e,xe,t),o(e,B,t),o(e,De,t),o(e,q,t),o(e,Ie,t),_(Q,e,t),o(e,Fe,t),o(e,S,t),o(e,We,t),o(e,V,t),o(e,Ue,t),o(e,X,t),o(e,Je,t),_(Y,e,t),o(e,Ne,t),o(e,O,t),o(e,Ze,t),_(K,e,t),o(e,ke,t),o(e,ee,t),o(e,ze,t),o(e,te,t),o(e,Ge,t),_(ne,e,t),o(e,Re,t),o(e,y,t),_(oe,y,null),p(y,qe),p(y,pe),p(y,Qe),p(y,fe),p(y,Se),_(k,y,null),o(e,Ee,t),_(ae,e,t),o(e,Pe,t),o(e,D,t),_(se,D,null),p(D,Ve),p(D,he),p(D,Xe),p(D,j),_(re,j,null),p(j,Ye),p(j,ue),p(j,Oe),_(z,j,null),p(j,Ke),_(G,j,null),o(e,He,t),_(ie,e,t),o(e,Le,t),o(e,v,t),_(le,v,null),p(v,et),p(v,ge),p(v,tt),p(v,_e),p(v,nt),p(v,x),_(de,x,null),p(x,ot),p(x,be),p(x,at),_(R,x,null),p(x,st),_(E,x,null),o(e,Ae,t),o(e,we,t),Be=!0},p(e,[t]){const I={};t&2&&(I.$$scope={dirty:t,ctx:e}),k.$set(I);const N={};t&2&&(N.$$scope={dirty:t,ctx:e}),z.$set(N);const F={};t&2&&(F.$$scope={dirty:t,ctx:e}),G.$set(F);const W={};t&2&&(W.$$scope={dirty:t,ctx:e}),R.$set(W);const U={};t&2&&(U.$$scope={dirty:t,ctx:e}),E.$set(U)},i(e){Be||(b(h.$$.fragment,e),b($.$$.fragment,e),b(Q.$$.fragment,e),b(Y.$$.fragment,e),b(K.$$.fragment,e),b(ne.$$.fragment,e),b(oe.$$.fragment,e),b(k.$$.fragment,e),b(ae.$$.fragment,e),b(se.$$.fragment,e),b(re.$$.fragment,e),b(z.$$.fragment,e),b(G.$$.fragment,e),b(ie.$$.fragment,e),b(le.$$.fragment,e),b(de.$$.fragment,e),b(R.$$.fragment,e),b(E.$$.fragment,e),Be=!0)},o(e){w(h.$$.fragment,e),w($.$$.fragment,e),w(Q.$$.fragment,e),w(Y.$$.fragment,e),w(K.$$.fragment,e),w(ne.$$.fragment,e),w(oe.$$.fragment,e),w(k.$$.fragment,e),w(ae.$$.fragment,e),w(se.$$.fragment,e),w(re.$$.fragment,e),w(z.$$.fragment,e),w(G.$$.fragment,e),w(ie.$$.fragment,e),w(le.$$.fragment,e),w(de.$$.fragment,e),w(R.$$.fragment,e),w(E.$$.fragment,e),Be=!1},d(e){e&&(n(T),n(c),n(m),n(s),n($e),n(P),n(ye),n(H),n(ve),n(L),n(Ce),n(A),n(je),n(Z),n(xe),n(B),n(De),n(q),n(Ie),n(Fe),n(S),n(We),n(V),n(Ue),n(X),n(Je),n(Ne),n(O),n(Ze),n(ke),n(ee),n(ze),n(te),n(Ge),n(Re),n(y),n(Ee),n(Pe),n(D),n(He),n(Le),n(v),n(Ae),n(we)),n(a),M(h,e),M($,e),M(Q,e),M(Y,e),M(K,e),M(ne,e),M(oe),M(k),M(ae,e),M(se),M(re),M(z),M(G),M(ie,e),M(le),M(de),M(R),M(E)}}}const Ht='{"title":"Dilated Neighborhood Attention Transformer","local":"dilated-neighborhood-attention-transformer","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"DinatConfig","local":"transformers.DinatConfig","sections":[],"depth":2},{"title":"DinatModel","local":"transformers.DinatModel","sections":[],"depth":2},{"title":"DinatForImageClassification","local":"transformers.DinatForImageClassification","sections":[],"depth":2}],"depth":1}';function Lt(C){return Wt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ot extends Ut{constructor(a){super(),Jt(this,a,Lt,Pt,It,{})}}export{Ot as component};
