import{s as bs,o as gs,n as fs}from"../chunks/scheduler.9bc65507.js";import{S as hs,i as _s,g as s,s as a,r,A as ys,h as i,f as n,c as o,j as C,u as m,x as u,k as J,y as b,a as l,v as d,d as p,t as f,w as c,m as cs,n as us}from"../chunks/index.707bf1b6.js";import{T as Wa}from"../chunks/Tip.c2ecdbf4.js";import{D as x}from"../chunks/Docstring.17db21ae.js";import{C as _}from"../chunks/CodeBlock.54a9f38d.js";import{H as h}from"../chunks/Heading.342b1fa6.js";function $s(k){let g;return{c(){g=cs("GPTQ 量子化は、現時点ではテキスト モデルでのみ機能します。さらに、量子化プロセスはハードウェアによっては長時間かかる場合があります (NVIDIA A100 を使用した場合、175B モデル = 4 gpu 時間)。モデルの GPTQ 量子化バージョンが存在しない場合は、ハブで確認してください。そうでない場合は、github で要求を送信できます。")},l(y){g=us(y,"GPTQ 量子化は、現時点ではテキスト モデルでのみ機能します。さらに、量子化プロセスはハードウェアによっては長時間かかる場合があります (NVIDIA A100 を使用した場合、175B モデル = 4 gpu 時間)。モデルの GPTQ 量子化バージョンが存在しない場合は、ハブで確認してください。そうでない場合は、github で要求を送信できます。")},m(y,$){l(y,g,$)},d(y){y&&n(g)}}}function Ms(k){let g,y="モデルが 4 ビットでロードされると、現時点では量子化された重みをハブにプッシュすることはできないことに注意してください。 4 ビットの重みはまだサポートされていないため、トレーニングできないことにも注意してください。ただし、4 ビット モデルを使用して追加のパラメーターをトレーニングすることもできます。これについては次のセクションで説明します。";return{c(){g=s("p"),g.textContent=y},l($){g=i($,"P",{"data-svelte-h":!0}),u(g)!=="svelte-1p6yucr"&&(g.textContent=y)},m($,q){l($,g,q)},p:fs,d($){$&&n(g)}}}function Ts(k){let g;return{c(){g=cs("モデルが 8 ビットでロードされると、最新の `transformers`と`bitsandbytes`を使用する場合を除き、量子化された重みをハブにプッシュすることは現在不可能であることに注意してください。 8 ビットの重みはまだサポートされていないため、トレーニングできないことにも注意してください。ただし、8 ビット モデルを使用して追加のパラメーターをトレーニングすることもできます。これについては次のセクションで説明します。\nまた、`device_map` はオプションですが、利用可能なリソース上でモデルを効率的にディスパッチするため、推論には `device_map = 'auto'` を設定することが推奨されます。")},l(y){g=us(y,"モデルが 8 ビットでロードされると、最新の `transformers`と`bitsandbytes`を使用する場合を除き、量子化された重みをハブにプッシュすることは現在不可能であることに注意してください。 8 ビットの重みはまだサポートされていないため、トレーニングできないことにも注意してください。ただし、8 ビット モデルを使用して追加のパラメーターをトレーニングすることもできます。これについては次のセクションで説明します。\nまた、`device_map` はオプションですが、利用可能なリソース上でモデルを効率的にディスパッチするため、推論には `device_map = 'auto'` を設定することが推奨されます。")},m(y,$){l(y,g,$)},d(y){y&&n(g)}}}function ws(k){let g,y="大規模なモデルでは、ハブ上で 8 ビット モデルをプッシュすることが強く推奨されます。これにより、コミュニティはメモリ フットプリントの削減と、たとえば Google Colab での大規模なモデルの読み込みによる恩恵を受けることができます。";return{c(){g=s("p"),g.textContent=y},l($){g=i($,"P",{"data-svelte-h":!0}),u(g)!=="svelte-1bkhcu"&&(g.textContent=y)},m($,q){l($,g,q)},p:fs,d($){$&&n(g)}}}function vs(k){let g,y,$,q,R,hn,F,_n,N,Ka="🤗 Transformers には、言語モデルで GPTQ 量子化を実行するための <code>optimum</code> API が統合されています。パフォーマンスを大幅に低下させることなく、推論速度を高速化することなく、モデルを 8、4、3、さらには 2 ビットでロードおよび量子化できます。これは、ほとんどの GPU ハードウェアでサポートされています。",yn,H,Oa="量子化モデルの詳細については、以下を確認してください。",$n,P,eo='<li><a href="https://arxiv.org/pdf/2210.17323.pdf" rel="nofollow">GPTQ</a> 論文</li> <li>GPTQ 量子化に関する <code>optimum</code> <a href="https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization" rel="nofollow">ガイド</a></li> <li>バックエンドとして使用される <a href="https://github.com/PanQiWei/AutoGPTQ" rel="nofollow"><code>AutoGPTQ</code></a> ライブラリ</li>',Mn,I,Tn,A,to="以下のコードを実行するには、以下の要件がインストールされている必要があります：",wn,E,no=`<li><p>最新の <code>AutoGPTQ</code> ライブラリをインストールする。
<code>pip install auto-gptq</code> をインストールする。</p></li> <li><p>最新の <code>optimum</code> をソースからインストールする。
<code>git+https://github.com/huggingface/optimum.git</code> をインストールする。</p></li> <li><p>最新の <code>transformers</code> をソースからインストールする。
最新の <code>transformers</code> をソースからインストールする <code>pip install git+https://github.com/huggingface/transformers.git</code></p></li> <li><p>最新の <code>accelerate</code> ライブラリをインストールする。
<code>pip install --upgrade accelerate</code> を実行する。</p></li>`,vn,Y,lo="GPTQ統合は今のところテキストモデルのみをサポートしているので、視覚、音声、マルチモーダルモデルでは予期せぬ挙動に遭遇するかもしれないことに注意してください。",Cn,S,Jn,D,ao="GPTQ は、量子化モデルを使用する前に重みのキャリブレーションを必要とする量子化方法です。トランスフォーマー モデルを最初から量子化する場合は、量子化モデルを作成するまでに時間がかかることがあります (<code>facebook/opt-350m</code>モデルの Google colab では約 5 分)。",kn,K,oo="したがって、GPTQ 量子化モデルを使用するシナリオは 2 つあります。最初の使用例は、ハブで利用可能な他のユーザーによってすでに量子化されたモデルをロードすることです。2 番目の使用例は、モデルを最初から量子化し、保存するかハブにプッシュして、他のユーザーが使用できるようにすることです。それも使ってください。",xn,O,qn,ee,so='モデルをロードして量子化するには、<a href="/docs/transformers/main/ja/main_classes/quantization#transformers.GPTQConfig">GPTQConfig</a> を作成する必要があります。データセットを準備するには、<code>bits</code>の数、量子化を調整するための<code>dataset</code>、およびモデルの<code>Tokenizer</code>を渡す必要があります。',Un,te,zn,ne,io="独自のデータセットを文字列のリストとして渡すことができることに注意してください。ただし、GPTQ 論文のデータセットを使用することを強くお勧めします。",Zn,le,Wn,ae,jn,oe,ro="<code>from_pretrained</code> を使用し、<code>quantization_config</code> を設定することでモデルを量子化できます。",Gn,se,Qn,ie,mo="モデルを量子化するには GPU が必要であることに注意してください。モデルを CPU に配置し、量子化するためにモジュールを GPU に前後に移動させます。",Bn,re,po="CPU オフロードの使用中に GPU の使用量を最大化したい場合は、<code>device_map = &quot;auto&quot;</code> を設定できます。",Xn,me,Vn,de,fo='ディスク オフロードはサポートされていないことに注意してください。さらに、データセットが原因でメモリが不足している場合は、<code>from_pretained</code> で <code>max_memory</code> を渡す必要がある場合があります。 <code>device_map</code>と<code>max_memory</code>の詳細については、この <a href="https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map" rel="nofollow">ガイド</a> を参照してください。',Ln,U,Rn,pe,Fn,fe,co="他の 🤗 モデルと同様に、<code>push_to_hub</code> を使用して量子化モデルをハブにプッシュできます。量子化構成は保存され、モデルに沿ってプッシュされます。",Nn,ce,Hn,ue,uo="量子化されたモデルをローカル マシンに保存したい場合は、<code>save_pretrained</code> を使用して行うこともできます。",Pn,be,In,ge,bo="<code>device_map</code> を使用してモデルを量子化した場合は、保存する前にモデル全体を GPU または <code>cpu</code> のいずれかに移動してください。",An,he,En,_e,Yn,ye,go=`<code>from_pretrained</code>を使用して、量子化されたモデルをハブからロードできます。
属性 <code>quantization_config</code> がモデル設定オブジェクトに存在することを確認して、プッシュされた重みが量子化されていることを確認します。`,Sn,$e,Dn,Me,ho="必要以上のメモリを割り当てずにモデルをより速くロードしたい場合は、<code>device_map</code> 引数は量子化モデルでも機能します。 <code>accelerate</code>ライブラリがインストールされていることを確認してください。",Kn,Te,On,we,el,ve,_o='4 ビット モデルの場合、推論速度を高めるために exllama カーネルを使用できます。デフォルトで有効になっています。 <a href="/docs/transformers/main/ja/main_classes/quantization#transformers.GPTQConfig">GPTQConfig</a> で <code>disable_exllama</code> を渡すことで、その動作を変更できます。これにより、設定に保存されている量子化設定が上書きされます。カーネルに関連する属性のみを上書きできることに注意してください。さらに、exllama カーネルを使用したい場合は、モデル全体を GPU 上に置く必要があります。',tl,Ce,nl,Je,yo="現時点では 4 ビット モデルのみがサポートされていることに注意してください。さらに、peft を使用して量子化モデルを微調整している場合は、exllama カーネルを非アクティブ化することをお勧めします。",ll,ke,al,xe,$o=`Hugging Face エコシステムのアダプターの公式サポートにより、GPTQ で量子化されたモデルを微調整できます。
詳細については、<a href="https://github.com/huggingface/peft" rel="nofollow"><code>peft</code></a> ライブラリをご覧ください。`,ol,qe,sl,Ue,Mo='GPTQ を使用してモデルを量子化する方法と、peft を使用して量子化されたモデルを微調整する方法については、Google Colab <a href="https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing" rel="nofollow">ノートブック</a> を参照してください。',il,ze,rl,w,Ze,ja,an,To=`This is a wrapper class about all possible attributes and features that you can play with a model that has been
loaded using <code>optimum</code> api for gptq quantization relying on auto_gptq backend.`,Ga,z,We,Qa,on,wo="Get compatible class with optimum gptq config dict",Ba,Z,je,Xa,sn,vo="Safety checker that arguments are correct",Va,W,Ge,La,rn,Co="Get compatible dict for optimum gptq config",ml,Qe,dl,Be,Jo=`🤗 Transformers は、<code>bitsandbytes</code> で最もよく使用されるモジュールと緊密に統合されています。数行のコードでモデルを 8 ビット精度でロードできます。
これは、<code>bitsandbytes</code>の <code>0.37.0</code>リリース以降、ほとんどの GPU ハードウェアでサポートされています。`,pl,Xe,ko='量子化方法の詳細については、<a href="https://arxiv.org/abs/2208.07339" rel="nofollow">LLM.int8()</a> 論文、または <a href="https://huggingface.co/blog/hf-bitsandbytes-" rel="nofollow">ブログ投稿</a> をご覧ください。統合）コラボレーションについて。',fl,Ve,xo="<code>0.39.0</code>リリース以降、FP4 データ型を活用し、4 ビット量子化を使用して<code>device_map</code>をサポートする任意のモデルをロードできます。",cl,Le,qo='独自の pytorch モデルを量子化したい場合は、🤗 Accelerate ライブラリの <a href="https://huggingface.co/docs/accelerate/main/en/usage_guides/quantization" rel="nofollow">ドキュメント</a> をチェックしてください。',ul,Re,Uo="<code>bitsandbytes</code>統合を使用してできることは次のとおりです",bl,Fe,gl,Ne,zo='モデルが 🤗 Accelerate による読み込みをサポートし、<code>torch.nn.Linear</code> レイヤーが含まれている限り、 <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> メソッドを呼び出すときに <code>load_in_8bit</code> または <code>load_in_4bit</code> 引数を使用してモデルを量子化できます。これはどのようなモダリティでも同様に機能するはずです。',hl,He,_l,Pe,Zo="デフォルトでは、他のすべてのモジュール (例: <code>torch.nn.LayerNorm</code>) は <code>torch.float16</code> に変換されますが、その <code>dtype</code> を変更したい場合は、<code>torch_dtype</code> 引数を上書きできます。",yl,Ie,$l,Ae,Ml,Ee,Tl,Ye,Wo="以下のコード スニペットを実行する前に、以下の要件がインストールされていることを確認してください。",wl,Se,jo=`<li><p>最新の<code>bitsandbytes</code>ライブラリ
<code>pip install bitsandbytes&gt;=0.39.0</code></p></li> <li><p>最新の<code>accelerate</code>をインストールする
<code>pip install --upgrade accelerate</code></p></li> <li><p>最新の <code>transformers</code> をインストールする
<code>pip install --upgrade transformers</code></p></li>`,vl,De,Cl,Ke,Go='<li><p><strong>高度な使用法:</strong> 可能なすべてのオプションを使用した 4 ビット量子化の高度な使用法については、<a href="https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf" rel="nofollow">この Google Colab ノートブック</a> を参照してください。</p></li> <li><p><strong><code>batch_size=1</code> による高速推論 :</strong> bitsandbytes の <code>0.40.0</code> リリース以降、<code>batch_size=1</code> では高速推論の恩恵を受けることができます。 <a href="https://github.com/TimDettmers/bitsandbytes/releases/tag/0.40.0" rel="nofollow">これらのリリース ノート</a> を確認し、この機能を活用するには<code>0.40.0</code>以降のバージョンを使用していることを確認してください。箱の。</p></li> <li><p><strong>トレーニング:</strong> <a href="https://arxiv.org/abs/2305.14314" rel="nofollow">QLoRA 論文</a> によると、4 ビット基本モデルをトレーニングする場合 (例: LoRA アダプターを使用)、<code>bnb_4bit_quant_type=&#39;nf4&#39;</code> を使用する必要があります。 。</p></li> <li><p><strong>推論:</strong> 推論の場合、<code>bnb_4bit_quant_type</code> はパフォーマンスに大きな影響を与えません。ただし、モデルの重みとの一貫性を保つために、必ず同じ <code>bnb_4bit_compute_dtype</code> および <code>torch_dtype</code> 引数を使用してください。</p></li>',Jl,Oe,kl,et,Qo="<code>.from_pretrained</code> メソッドを呼び出すときに <code>load_in_4bit=True</code> を使用すると、メモリ使用量を (おおよそ) 4 で割ることができます。",xl,tt,ql,j,Ul,nt,zl,lt,Bo="<code>.from_pretrained</code> メソッドを呼び出すときに <code>load_in_8bit=True</code> 引数を使用すると、メモリ要件をおよそ半分にしてモデルをロードできます。",Zl,at,Wl,ot,Xo='次に、通常 <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a> を使用するのと同じようにモデルを使用します。',jl,st,Vo="<code>get_memory_footprint</code> メソッドを使用して、モデルのメモリ フットプリントを確認できます。",Gl,it,Ql,rt,Lo="この統合により、大きなモデルを小さなデバイスにロードし、問題なく実行できるようになりました。",Bl,G,Xl,mt,Vl,dt,Ro="ここでは、FP4 量子化を使用して実行できるいくつかの高度な使用例について説明します。",Ll,pt,Rl,ft,Fo="compute dtype は、計算中に使用される dtype を変更するために使用されます。たとえば、隠し状態は<code>float32</code>にありますが、高速化のために計算を bf16 に設定できます。デフォルトでは、compute dtype は <code>float32</code> に設定されます。",Fl,ct,Nl,ut,Hl,bt,No="NF4 データ型を使用することもできます。これは、正規分布を使用して初期化された重みに適合した新しい 4 ビット データ型です。その実行のために:",Pl,gt,Il,ht,Al,_t,Ho="また、ネストされた量子化手法を使用することをお勧めします。これにより、パフォーマンスを追加することなく、より多くのメモリが節約されます。経験的な観察から、これにより、NVIDIA-T4 16GB 上でシーケンス長 1024、バッチ サイズ 1、勾配累積ステップ 4 の llama-13b モデルを微調整することが可能になります。",El,yt,Yl,$t,Sl,Mt,Po=`<code>push_to_hub</code>メソッドを単純に使用することで、量子化されたモデルをハブにプッシュできます。これにより、最初に量子化構成ファイルがプッシュされ、次に量子化されたモデルの重みがプッシュされます。
この機能を使用できるようにするには、必ず <code>bitsandbytes&gt;0.37.2</code> を使用してください (この記事の執筆時点では、<code>bitsandbytes==0.38.0.post1</code> でテストしました)。`,Dl,Tt,Kl,Q,Ol,wt,ea,vt,Io="<code>from_pretrained</code>メソッドを使用して、ハブから量子化モデルをロードできます。属性 <code>quantization_config</code> がモデル設定オブジェクトに存在することを確認して、プッシュされた重みが量子化されていることを確認します。",ta,Ct,na,Jt,Ao=`この場合、引数 <code>load_in_8bit=True</code> を指定する必要はありませんが、<code>bitsandbytes</code> と <code>accelerate</code> がインストールされていることを確認する必要があることに注意してください。
また、<code>device_map</code> はオプションですが、利用可能なリソース上でモデルを効率的にディスパッチするため、推論には <code>device_map = &#39;auto&#39;</code> を設定することが推奨されます。`,la,kt,aa,xt,Eo="このセクションは、8 ビット モデルのロードと実行以外に何ができるかを探求したい上級ユーザーを対象としています。",oa,qt,sa,Ut,Yo="この高度な使用例の 1 つは、モデルをロードし、<code>CPU</code>と<code>GPU</code>の間で重みをディスパッチできることです。 CPU 上でディスパッチされる重みは <strong>8 ビットに変換されない</strong>ため、<code>float32</code>に保持されることに注意してください。この機能は、非常に大規模なモデルを適合させ、そのモデルを GPU と CPU の間でディスパッチしたいユーザーを対象としています。",ia,zt,So='まず、<code>transformers</code> から <a href="/docs/transformers/main/ja/main_classes/quantization#transformers.BitsAndBytesConfig">BitsAndBytesConfig</a> をロードし、属性 <code>llm_int8_enable_fp32_cpu_offload</code> を <code>True</code> に設定します。',ra,Zt,ma,Wt,Do="<code>bigscience/bloom-1b7</code>モデルをロードする必要があり、<code>lm_head</code>を除くモデル全体に​​適合するのに十分な GPU RAM があるとします。したがって、次のようにカスタム device_map を作成します。",da,jt,pa,Gt,Ko="そして、次のようにモデルをロードします。",fa,Qt,ca,Bt,Oo="以上です！モデルを楽しんでください！",ua,Xt,ba,Vt,es=`<code>llm_int8_threshold</code> 引数を操作して、外れ値のしきい値を変更できます。 外れ値 とは、特定のしきい値より大きい隠れた状態の値です。
これは、<code>LLM.int8()</code>論文で説明されている外れ値検出の外れ値しきい値に対応します。このしきい値を超える隠し状態の値は外れ値とみなされ、それらの値に対する操作は fp16 で実行されます。通常、値は正規分布します。つまり、ほとんどの値は [-3.5, 3.5] の範囲内にありますが、大規模なモデルでは大きく異なる分布を示す例外的な系統的外れ値がいくつかあります。これらの外れ値は、多くの場合 [-60, -6] または [6, 60] の範囲内にあります。 Int8 量子化は、大きさが 5 程度までの値ではうまく機能しますが、それを超えると、パフォーマンスが大幅に低下します。適切なデフォルトのしきい値は 6 ですが、より不安定なモデル (小規模なモデル、微調整) では、より低いしきい値が必要になる場合があります。
この引数は、モデルの推論速度に影響を与える可能性があります。このパラメータを試してみて、ユースケースに最適なパラメータを見つけることをお勧めします。`,ga,Lt,ha,Rt,_a,Ft,ts="一部のモデルには、安定性を確保するために 8 ビットに変換する必要がないモジュールがいくつかあります。たとえば、ジュークボックス モデルには、スキップする必要があるいくつかの <code>lm_head</code> モジュールがあります。 <code>llm_int8_skip_modules</code> で遊んでみる",ya,Nt,$a,Ht,Ma,Pt,ns=`Hugging Face エコシステムのアダプターの公式サポートにより、8 ビットでロードされたモデルを微調整できます。
これにより、単一の Google Colab で<code>flan-t5-large</code>や<code>facebook/opt-6.7b</code>などの大規模モデルを微調整することができます。詳細については、<a href="https://github.com/huggingface/peft" rel="nofollow"><code>peft</code></a> ライブラリをご覧ください。`,Ta,It,ls="トレーニング用のモデルをロードするときに <code>device_map</code> を渡す必要がないことに注意してください。モデルが GPU に自動的にロードされます。必要に応じて、デバイス マップを特定のデバイスに設定することもできます (例: <code>cuda:0</code>、<code>0</code>、<code>torch.device(&#39;cuda:0&#39;)</code>)。 <code>device_map=auto</code>は推論のみに使用する必要があることに注意してください。",wa,At,va,M,Et,Ra,mn,as=`This is a wrapper class about all possible attributes and features that you can play with a model that has been
loaded using <code>bitsandbytes</code>.`,Fa,dn,os="This replaces <code>load_in_8bit</code> or <code>load_in_4bit</code>therefore both options are mutually exclusive.",Na,pn,ss=`Currently only supports <code>LLM.int8()</code>, <code>FP4</code>, and <code>NF4</code> quantization. If more methods are added to <code>bitsandbytes</code>,
then more arguments will be added to this class.`,Ha,B,Yt,Pa,fn,is="Returns <code>True</code> if the model is quantizable, <code>False</code> otherwise.",Ia,X,St,Aa,cn,rs="Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.",Ea,V,Dt,Ya,un,ms=`This method returns the quantization method used for the model. If the model is not quantizable, it returns
<code>None</code>.`,Sa,L,Kt,Da,bn,ds=`Removes all attributes from config which correspond to the default config attributes for better readability and
serializes to a Python dictionary.`,Ca,Ot,Ja,en,ps='<code>optimum</code>でサポートされている量子化方法の詳細については、<a href="https://huggingface.co/docs/optimum/index" rel="nofollow">Optimum ドキュメント</a> を参照し、これらが自分のユースケースに適用できるかどうかを確認してください。',ka,gn,xa;return R=new h({props:{title:"Quantize 🤗 Transformers models",local:"quantize--transformers-models",headingTag:"h1"}}),F=new h({props:{title:"AutoGPTQ Integration",local:"autogptq-integration",headingTag:"h2"}}),I=new h({props:{title:"Requirements",local:"requirements",headingTag:"h3"}}),S=new h({props:{title:"Load and quantize a model",local:"load-and-quantize-a-model",headingTag:"h3"}}),O=new h({props:{title:"GPTQ Configuration",local:"gptq-configuration",headingTag:"h4"}}),te=new _({props:{code:"bW9kZWxfaWQlMjAlM0QlMjAlMjJmYWNlYm9vayUyRm9wdC0xMjVtJTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBZ3B0cV9jb25maWclMjAlM0QlMjBHUFRRQ29uZmlnKGJpdHMlM0Q0JTJDJTIwZGF0YXNldCUyMCUzRCUyMCUyMmM0JTIyJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKQ==",highlighted:`model_id = <span class="hljs-string">&quot;facebook/opt-125m&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>, dataset = <span class="hljs-string">&quot;c4&quot;</span>, tokenizer=tokenizer)`,wrap:!1}}),le=new _({props:{code:"ZGF0YXNldCUyMCUzRCUyMCU1QiUyMmF1dG8tZ3B0cSUyMGlzJTIwYW4lMjBlYXN5LXRvLXVzZSUyMG1vZGVsJTIwcXVhbnRpemF0aW9uJTIwbGlicmFyeSUyMHdpdGglMjB1c2VyLWZyaWVuZGx5JTIwYXBpcyUyQyUyMGJhc2VkJTIwb24lMjBHUFRRJTIwYWxnb3JpdGhtLiUyMiU1RCUwQXF1YW50aXphdGlvbiUyMCUzRCUyMEdQVFFDb25maWcoYml0cyUzRDQlMkMlMjBkYXRhc2V0JTIwJTNEJTIwZGF0YXNldCUyQyUyMHRva2VuaXplciUzRHRva2VuaXplcik=",highlighted:`dataset = [<span class="hljs-string">&quot;auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.&quot;</span>]
quantization = GPTQConfig(bits=<span class="hljs-number">4</span>, dataset = dataset, tokenizer=tokenizer)`,wrap:!1}}),ae=new h({props:{title:"Quantization",local:"quantization",headingTag:"h4"}}),se=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEZ3B0cV9jb25maWcp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=gptq_config)`,wrap:!1}}),me=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMHF1YW50aXphdGlvbl9jb25maWclM0RncHRxX2NvbmZpZyk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config=gptq_config)`,wrap:!1}}),U=new Wa({props:{warning:!0,$$slots:{default:[$s]},$$scope:{ctx:k}}}),pe=new h({props:{title:"Push quantized model to 🤗 Hub",local:"push-quantized-model-to--hub",headingTag:"h3"}}),ce=new _({props:{code:"cXVhbnRpemVkX21vZGVsLnB1c2hfdG9faHViKCUyMm9wdC0xMjVtLWdwdHElMjIpJTBBdG9rZW5pemVyLnB1c2hfdG9faHViKCUyMm9wdC0xMjVtLWdwdHElMjIp",highlighted:`quantized_model.push_to_hub(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)
tokenizer.push_to_hub(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),be=new _({props:{code:"cXVhbnRpemVkX21vZGVsLnNhdmVfcHJldHJhaW5lZCglMjJvcHQtMTI1bS1ncHRxJTIyKSUwQXRva2VuaXplci5zYXZlX3ByZXRyYWluZWQoJTIyb3B0LTEyNW0tZ3B0cSUyMik=",highlighted:`quantized_model.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)
tokenizer.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),he=new _({props:{code:"cXVhbnRpemVkX21vZGVsLnRvKCUyMmNwdSUyMiklMEFxdWFudGl6ZWRfbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKCUyMm9wdC0xMjVtLWdwdHElMjIp",highlighted:`quantized_model.to(<span class="hljs-string">&quot;cpu&quot;</span>)
quantized_model.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),_e=new h({props:{title:"Load a quantized model from the 🤗 Hub",local:"load-a-quantized-model-from-the--hub",headingTag:"h3"}}),$e=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyJTdCeW91cl91c2VybmFtZSU3RCUyRm9wdC0xMjVtLWdwdHElMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>)`,wrap:!1}}),Te=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyJTdCeW91cl91c2VybmFtZSU3RCUyRm9wdC0xMjVtLWdwdHElMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),we=new h({props:{title:"Exllama kernels for faster inference",local:"exllama-kernels-for-faster-inference",headingTag:"h3"}}),Ce=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFncHRxX2NvbmZpZyUyMCUzRCUyMEdQVFFDb25maWcoYml0cyUzRDQlMkMlMjBkaXNhYmxlX2V4bGxhbWElM0RGYWxzZSklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjIlN0J5b3VyX3VzZXJuYW1lJTdEJTJGb3B0LTEyNW0tZ3B0cSUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMGdwdHFfY29uZmlnKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>, disable_exllama=<span class="hljs-literal">False</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config = gptq_config)`,wrap:!1}}),ke=new h({props:{title:"Fine-tune a quantized model",local:"fine-tune-a-quantized-model",headingTag:"h4"}}),qe=new h({props:{title:"Example demo",local:"example-demo",headingTag:"h3"}}),ze=new h({props:{title:"GPTQConfig",local:"transformers.GPTQConfig",headingTag:"h3"}}),Ze=new x({props:{name:"class transformers.GPTQConfig",anchor:"transformers.GPTQConfig",parameters:[{name:"bits",val:": int"},{name:"tokenizer",val:": Any = None"},{name:"dataset",val:": Union = None"},{name:"group_size",val:": int = 128"},{name:"damp_percent",val:": float = 0.1"},{name:"desc_act",val:": bool = False"},{name:"sym",val:": bool = True"},{name:"true_sequential",val:": bool = True"},{name:"use_cuda_fp16",val:": bool = False"},{name:"model_seqlen",val:": Optional = None"},{name:"block_name_to_quantize",val:": Optional = None"},{name:"module_name_preceding_first_block",val:": Optional = None"},{name:"batch_size",val:": int = 1"},{name:"pad_token_id",val:": Optional = None"},{name:"use_exllama",val:": Optional = None"},{name:"max_input_length",val:": Optional = None"},{name:"exllama_config",val:": Optional = None"},{name:"cache_block_outputs",val:": bool = True"},{name:"modules_in_block_to_quantize",val:": Optional = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTQConfig.bits",description:`<strong>bits</strong> (<code>int</code>) &#x2014;
The number of bits to quantize to, supported numbers are (2, 3, 4, 8).`,name:"bits"},{anchor:"transformers.GPTQConfig.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizerBase</code>, <em>optional</em>) &#x2014;
The tokenizer used to process the dataset. You can pass either:<ul>
<li>A custom tokenizer object.</li>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/main/ja/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"tokenizer"},{anchor:"transformers.GPTQConfig.dataset",description:`<strong>dataset</strong> (<code>Union[List[str]]</code>, <em>optional</em>) &#x2014;
The dataset used for quantization. You can provide your own dataset in a list of string or just use the
original datasets used in GPTQ paper [&#x2018;wikitext2&#x2019;,&#x2018;c4&#x2019;,&#x2018;c4-new&#x2019;,&#x2018;ptb&#x2019;,&#x2018;ptb-new&#x2019;]`,name:"dataset"},{anchor:"transformers.GPTQConfig.group_size",description:`<strong>group_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The group size to use for quantization. Recommended value is 128 and -1 uses per-column quantization.`,name:"group_size"},{anchor:"transformers.GPTQConfig.damp_percent",description:`<strong>damp_percent</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The percent of the average Hessian diagonal to use for dampening. Recommended value is 0.1.`,name:"damp_percent"},{anchor:"transformers.GPTQConfig.desc_act",description:`<strong>desc_act</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to quantize columns in order of decreasing activation size. Setting it to False can significantly
speed up inference but the perplexity may become slightly worse. Also known as act-order.`,name:"desc_act"},{anchor:"transformers.GPTQConfig.sym",description:`<strong>sym</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use symetric quantization.`,name:"sym"},{anchor:"transformers.GPTQConfig.true_sequential",description:`<strong>true_sequential</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to perform sequential quantization even within a single Transformer block. Instead of quantizing
the entire block at once, we perform layer-wise quantization. As a result, each layer undergoes
quantization using inputs that have passed through the previously quantized layers.`,name:"true_sequential"},{anchor:"transformers.GPTQConfig.use_cuda_fp16",description:`<strong>use_cuda_fp16</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use optimized cuda kernel for fp16 model. Need to have model in fp16.`,name:"use_cuda_fp16"},{anchor:"transformers.GPTQConfig.model_seqlen",description:`<strong>model_seqlen</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum sequence length that the model can take.`,name:"model_seqlen"},{anchor:"transformers.GPTQConfig.block_name_to_quantize",description:`<strong>block_name_to_quantize</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The transformers block name to quantize. If None, we will infer the block name using common patterns (e.g. model.layers)`,name:"block_name_to_quantize"},{anchor:"transformers.GPTQConfig.module_name_preceding_first_block",description:`<strong>module_name_preceding_first_block</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
The layers that are preceding the first Transformer block.`,name:"module_name_preceding_first_block"},{anchor:"transformers.GPTQConfig.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The batch size used when processing the dataset`,name:"batch_size"},{anchor:"transformers.GPTQConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The pad token id. Needed to prepare the dataset when <code>batch_size</code> &gt; 1.`,name:"pad_token_id"},{anchor:"transformers.GPTQConfig.use_exllama",description:`<strong>use_exllama</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to use exllama backend. Defaults to <code>True</code> if unset. Only works with <code>bits</code> = 4.`,name:"use_exllama"},{anchor:"transformers.GPTQConfig.max_input_length",description:`<strong>max_input_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum input length. This is needed to initialize a buffer that depends on the maximum expected input
length. It is specific to the exllama backend with act-order.`,name:"max_input_length"},{anchor:"transformers.GPTQConfig.exllama_config",description:`<strong>exllama_config</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The exllama config. You can specify the version of the exllama kernel through the <code>version</code> key. Defaults
to <code>{&quot;version&quot;: 1}</code> if unset.`,name:"exllama_config"},{anchor:"transformers.GPTQConfig.cache_block_outputs",description:`<strong>cache_block_outputs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to cache block outputs to reuse as inputs for the succeeding block.`,name:"cache_block_outputs"},{anchor:"transformers.GPTQConfig.modules_in_block_to_quantize",description:`<strong>modules_in_block_to_quantize</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
List of list of module names to quantize in the specified block. This argument is useful to exclude certain linear modules from being quantized.
The block to quantize can be specified by setting <code>block_name_to_quantize</code>. We will quantize each list sequentially. If not set, we will quantize all linear layers.
Example: <code>modules_in_block_to_quantize =[[&quot;self_attn.k_proj&quot;, &quot;self_attn.v_proj&quot;, &quot;self_attn.q_proj&quot;], [&quot;self_attn.o_proj&quot;]]</code>.
In this example, we will first quantize the q,k,v layers simultaneously since they are independent.
Then, we will quantize <code>self_attn.o_proj</code> layer with the q,k,v layers quantized. This way, we will get
better results since it reflects the real input <code>self_attn.o_proj</code> will get when the model is quantized.`,name:"modules_in_block_to_quantize"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L383"}}),We=new x({props:{name:"from_dict_optimum",anchor:"transformers.GPTQConfig.from_dict_optimum",parameters:[{name:"config_dict",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L580"}}),je=new x({props:{name:"post_init",anchor:"transformers.GPTQConfig.post_init",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L497"}}),Ge=new x({props:{name:"to_dict_optimum",anchor:"transformers.GPTQConfig.to_dict_optimum",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L571"}}),Qe=new h({props:{title:"bitsandbytes Integration",local:"bitsandbytes-integration",headingTag:"h2"}}),Fe=new h({props:{title:"General usage",local:"general-usage",headingTag:"h3"}}),He=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm9wdC0zNTBtJTIyJTJDJTIwbG9hZF9pbl84Yml0JTNEVHJ1ZSklMEFtb2RlbF80Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGb3B0LTM1MG0lMjIlMkMlMjBsb2FkX2luXzRiaXQlM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)
model_4bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),Ie=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGb3B0LTM1MG0lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDMyKSUwQW1vZGVsXzhiaXQubW9kZWwuZGVjb2Rlci5sYXllcnMlNUItMSU1RC5maW5hbF9sYXllcl9ub3JtLndlaWdodC5kdHlwZQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model_8bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, torch_dtype=torch.float32)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_8bit.model.decoder.layers[-<span class="hljs-number">1</span>].final_layer_norm.weight.dtype
torch.float32`,wrap:!1}}),Ae=new h({props:{title:"FP4 quantization",local:"fp4-quantization",headingTag:"h3"}}),Ee=new h({props:{title:"Requirements",local:"requirements",headingTag:"h4"}}),De=new h({props:{title:"Tips and best practices",local:"tips-and-best-practices",headingTag:"h4"}}),Oe=new h({props:{title:"Load a large model in 4bit",local:"load-a-large-model-in-4bit",headingTag:"h4"}}),tt=new _({props:{code:"JTIzJTIwcGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGFjY2VsZXJhdGUlMjBiaXRzYW5kYnl0ZXMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fNGJpdCUzRFRydWUp",highlighted:`<span class="hljs-comment"># pip install transformers accelerate bitsandbytes</span>
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),j=new Wa({props:{warning:!0,$$slots:{default:[Ms]},$$scope:{ctx:k}}}),nt=new h({props:{title:"Load a large model in 8bit",local:"load-a-large-model-in-8bit",headingTag:"h3"}}),at=new _({props:{code:"JTIzJTIwcGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGFjY2VsZXJhdGUlMjBiaXRzYW5kYnl0ZXMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUp",highlighted:`<span class="hljs-comment"># pip install transformers accelerate bitsandbytes</span>
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),it=new _({props:{code:"cHJpbnQobW9kZWwuZ2V0X21lbW9yeV9mb290cHJpbnQoKSk=",highlighted:'<span class="hljs-built_in">print</span>(model.get_memory_footprint())',wrap:!1}}),G=new Wa({props:{warning:!0,$$slots:{default:[Ts]},$$scope:{ctx:k}}}),mt=new h({props:{title:"Advanced use cases",local:"advanced-use-cases",headingTag:"h4"}}),pt=new h({props:{title:"Change the compute dtype",local:"change-the-compute-dtype",headingTag:"h5"}}),ct=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyhsb2FkX2luXzRiaXQlM0RUcnVlJTJDJTIwYm5iXzRiaXRfY29tcHV0ZV9kdHlwZSUzRHRvcmNoLmJmbG9hdDE2KQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=<span class="hljs-literal">True</span>, bnb_4bit_compute_dtype=torch.bfloat16)`,wrap:!1}}),ut=new h({props:{title:"Using NF4 (Normal Float 4) data type",local:"using-nf4-normal-float-4-data-type",headingTag:"h5"}}),gt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW5mNF9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbG9hZF9pbl80Yml0JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGJuYl80Yml0X3F1YW50X3R5cGUlM0QlMjJuZjQlMjIlMkMlMEEpJTBBJTBBbW9kZWxfbmY0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRG5mNF9jb25maWcp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_quant_type=<span class="hljs-string">&quot;nf4&quot;</span>,
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)`,wrap:!1}}),ht=new h({props:{title:"Use nested quantization for more memory efficient inference",local:"use-nested-quantization-for-more-memory-efficient-inference",headingTag:"h5"}}),yt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQWRvdWJsZV9xdWFudF9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbG9hZF9pbl80Yml0JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGJuYl80Yml0X3VzZV9kb3VibGVfcXVhbnQlM0RUcnVlJTJDJTBBKSUwQSUwQW1vZGVsX2RvdWJsZV9xdWFudCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCUyQyUyMHF1YW50aXphdGlvbl9jb25maWclM0Rkb3VibGVfcXVhbnRfY29uZmlnKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_use_double_quant=<span class="hljs-literal">True</span>,
)

model_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config)`,wrap:!1}}),$t=new h({props:{title:"Push quantized models on the 🤗 Hub",local:"push-quantized-models-on-the--hub",headingTag:"h3"}}),Tt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmJpZ3NjaWVuY2UlMkZibG9vbS01NjBtJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmJpZ3NjaWVuY2UlMkZibG9vbS01NjBtJTIyKSUwQSUwQW1vZGVsLnB1c2hfdG9faHViKCUyMmJsb29tLTU2MG0tOGJpdCUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom-560m&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom-560m&quot;</span>)

model.push_to_hub(<span class="hljs-string">&quot;bloom-560m-8bit&quot;</span>)`,wrap:!1}}),Q=new Wa({props:{warning:!0,$$slots:{default:[ws]},$$scope:{ctx:k}}}),wt=new h({props:{title:"Load a quantized model from the 🤗 Hub",local:"load-a-quantized-model-from-the--hub",headingTag:"h3"}}),Ct=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMiU3QnlvdXJfdXNlcm5hbWUlN0QlMkZibG9vbS01NjBtLThiaXQlMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/bloom-560m-8bit&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),kt=new h({props:{title:"Advanced use cases",local:"advanced-use-cases",headingTag:"h3"}}),qt=new h({props:{title:"Offload between cpu and gpu",local:"offload-between-cpu-and-gpu",headingTag:"h4"}}),Zt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcobGxtX2ludDhfZW5hYmxlX2ZwMzJfY3B1X29mZmxvYWQlM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=<span class="hljs-literal">True</span>)`,wrap:!1}}),jt=new _({props:{code:"ZGV2aWNlX21hcCUyMCUzRCUyMCU3QiUwQSUyMCUyMCUyMCUyMCUyMnRyYW5zZm9ybWVyLndvcmRfZW1iZWRkaW5ncyUyMiUzQSUyMDAlMkMlMEElMjAlMjAlMjAlMjAlMjJ0cmFuc2Zvcm1lci53b3JkX2VtYmVkZGluZ3NfbGF5ZXJub3JtJTIyJTNBJTIwMCUyQyUwQSUyMCUyMCUyMCUyMCUyMmxtX2hlYWQlMjIlM0ElMjAlMjJjcHUlMjIlMkMlMEElMjAlMjAlMjAlMjAlMjJ0cmFuc2Zvcm1lci5oJTIyJTNBJTIwMCUyQyUwQSUyMCUyMCUyMCUyMCUyMnRyYW5zZm9ybWVyLmxuX2YlMjIlM0ElMjAwJTJDJTBBJTdE",highlighted:`device_map = {
    <span class="hljs-string">&quot;transformer.word_embeddings&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;transformer.word_embeddings_layernorm&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;lm_head&quot;</span>: <span class="hljs-string">&quot;cpu&quot;</span>,
    <span class="hljs-string">&quot;transformer.h&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;transformer.ln_f&quot;</span>: <span class="hljs-number">0</span>,
}`,wrap:!1}}),Qt=new _({props:{code:"bW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRGRldmljZV9tYXAlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUyQyUwQSk=",highlighted:`model_8bit = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>,
    device_map=device_map,
    quantization_config=quantization_config,
)`,wrap:!1}}),Xt=new h({props:{title:"Play with llm_int8_threshold",local:"play-with-llmint8threshold",headingTag:"h4"}}),Lt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyYmlnc2NpZW5jZSUyRmJsb29tLTFiNyUyMiUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbGxtX2ludDhfdGhyZXNob2xkJTNEMTAlMkMlMEEpJTBBJTBBbW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjBtb2RlbF9pZCUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0RkZXZpY2VfbWFwJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMkMlMEEpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=<span class="hljs-number">10</span>,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)`,wrap:!1}}),Rt=new h({props:{title:"Skip the conversion of some modules",local:"skip-the-conversion-of-some-modules",headingTag:"h4"}}),Nt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyYmlnc2NpZW5jZSUyRmJsb29tLTFiNyUyMiUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbGxtX2ludDhfc2tpcF9tb2R1bGVzJTNEJTVCJTIybG1faGVhZCUyMiU1RCUyQyUwQSklMEElMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMG1vZGVsX2lkJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRGRldmljZV9tYXAlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUyQyUwQSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=[<span class="hljs-string">&quot;lm_head&quot;</span>],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)`,wrap:!1}}),Ht=new h({props:{title:"Fine-tune a model that has been loaded in 8-bit",local:"fine-tune-a-model-that-has-been-loaded-in-8-bit",headingTag:"h4"}}),At=new h({props:{title:"BitsAndBytesConfig",local:"transformers.BitsAndBytesConfig",headingTag:"h3"}}),Et=new x({props:{name:"class transformers.BitsAndBytesConfig",anchor:"transformers.BitsAndBytesConfig",parameters:[{name:"load_in_8bit",val:" = False"},{name:"load_in_4bit",val:" = False"},{name:"llm_int8_threshold",val:" = 6.0"},{name:"llm_int8_skip_modules",val:" = None"},{name:"llm_int8_enable_fp32_cpu_offload",val:" = False"},{name:"llm_int8_has_fp16_weight",val:" = False"},{name:"bnb_4bit_compute_dtype",val:" = None"},{name:"bnb_4bit_quant_type",val:" = 'fp4'"},{name:"bnb_4bit_use_double_quant",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BitsAndBytesConfig.load_in_8bit",description:`<strong>load_in_8bit</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used to enable 8-bit quantization with LLM.int8().`,name:"load_in_8bit"},{anchor:"transformers.BitsAndBytesConfig.load_in_4bit",description:`<strong>load_in_4bit</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from
<code>bitsandbytes</code>.`,name:"load_in_4bit"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_threshold",description:`<strong>llm_int8_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 6.0) &#x2014;
This corresponds to the outlier threshold for outlier detection as described in <code>LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale</code> paper: <a href="https://arxiv.org/abs/2208.07339" rel="nofollow">https://arxiv.org/abs/2208.07339</a> Any hidden states value
that is above this threshold will be considered an outlier and the operation on those values will be done
in fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], but
there are some exceptional systematic outliers that are very differently distributed for large models.
These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values of
magnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6,
but a lower threshold might be needed for more unstable models (small models, fine-tuning).`,name:"llm_int8_threshold"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_skip_modules",description:`<strong>llm_int8_skip_modules</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such as
Jukebox that has several heads in different places and not necessarily at the last position. For example
for <code>CausalLM</code> models, the last <code>lm_head</code> is kept in its original <code>dtype</code>.`,name:"llm_int8_skip_modules"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_enable_fp32_cpu_offload",description:`<strong>llm_int8_enable_fp32_cpu_offload</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used for advanced use cases and users that are aware of this feature. If you want to split
your model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can use
this flag. This is useful for offloading large models such as <code>google/flan-t5-xxl</code>. Note that the int8
operations will not be run on CPU.`,name:"llm_int8_enable_fp32_cpu_offload"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_has_fp16_weight",description:`<strong>llm_int8_has_fp16_weight</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do not
have to be converted back and forth for the backward pass.`,name:"llm_int8_has_fp16_weight"},{anchor:"transformers.BitsAndBytesConfig.bnb_4bit_compute_dtype",description:`<strong>bnb_4bit_compute_dtype</strong> (<code>torch.dtype</code> or str, <em>optional</em>, defaults to <code>torch.float32</code>) &#x2014;
This sets the computational type which might be different than the input time. For example, inputs might be
fp32, but computation can be set to bf16 for speedups.`,name:"bnb_4bit_compute_dtype"},{anchor:"transformers.BitsAndBytesConfig.bnb_4bit_quant_type",description:`<strong>bnb_4bit_quant_type</strong> (<code>str</code>,  <em>optional</em>, defaults to <code>&quot;fp4&quot;</code>) &#x2014;
This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types
which are specified by <code>fp4</code> or <code>nf4</code>.`,name:"bnb_4bit_quant_type"},{anchor:"transformers.BitsAndBytesConfig.bnb_4bit_use_double_quant",description:`<strong>bnb_4bit_use_double_quant</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used for nested quantization where the quantization constants from the first quantization are
quantized again.`,name:"bnb_4bit_use_double_quant"},{anchor:"transformers.BitsAndBytesConfig.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional parameters from which to initialize the configuration object.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L179"}}),Yt=new x({props:{name:"is_quantizable",anchor:"transformers.BitsAndBytesConfig.is_quantizable",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L319"}}),St=new x({props:{name:"post_init",anchor:"transformers.BitsAndBytesConfig.post_init",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L288"}}),Dt=new x({props:{name:"quantization_method",anchor:"transformers.BitsAndBytesConfig.quantization_method",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L325"}}),Kt=new x({props:{name:"to_diff_dict",anchor:"transformers.BitsAndBytesConfig.to_diff_dict",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L355",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Dict[str, Any]</code></p>
`}}),Ot=new h({props:{title:"Quantization with 🤗 optimum",local:"quantization-with--optimum",headingTag:"h2"}}),{c(){g=s("meta"),y=a(),$=s("p"),q=a(),r(R.$$.fragment),hn=a(),r(F.$$.fragment),_n=a(),N=s("p"),N.innerHTML=Ka,yn=a(),H=s("p"),H.textContent=Oa,$n=a(),P=s("ul"),P.innerHTML=eo,Mn=a(),r(I.$$.fragment),Tn=a(),A=s("p"),A.textContent=to,wn=a(),E=s("ul"),E.innerHTML=no,vn=a(),Y=s("p"),Y.textContent=lo,Cn=a(),r(S.$$.fragment),Jn=a(),D=s("p"),D.innerHTML=ao,kn=a(),K=s("p"),K.textContent=oo,xn=a(),r(O.$$.fragment),qn=a(),ee=s("p"),ee.innerHTML=so,Un=a(),r(te.$$.fragment),zn=a(),ne=s("p"),ne.textContent=io,Zn=a(),r(le.$$.fragment),Wn=a(),r(ae.$$.fragment),jn=a(),oe=s("p"),oe.innerHTML=ro,Gn=a(),r(se.$$.fragment),Qn=a(),ie=s("p"),ie.textContent=mo,Bn=a(),re=s("p"),re.innerHTML=po,Xn=a(),r(me.$$.fragment),Vn=a(),de=s("p"),de.innerHTML=fo,Ln=a(),r(U.$$.fragment),Rn=a(),r(pe.$$.fragment),Fn=a(),fe=s("p"),fe.innerHTML=co,Nn=a(),r(ce.$$.fragment),Hn=a(),ue=s("p"),ue.innerHTML=uo,Pn=a(),r(be.$$.fragment),In=a(),ge=s("p"),ge.innerHTML=bo,An=a(),r(he.$$.fragment),En=a(),r(_e.$$.fragment),Yn=a(),ye=s("p"),ye.innerHTML=go,Sn=a(),r($e.$$.fragment),Dn=a(),Me=s("p"),Me.innerHTML=ho,Kn=a(),r(Te.$$.fragment),On=a(),r(we.$$.fragment),el=a(),ve=s("p"),ve.innerHTML=_o,tl=a(),r(Ce.$$.fragment),nl=a(),Je=s("p"),Je.textContent=yo,ll=a(),r(ke.$$.fragment),al=a(),xe=s("p"),xe.innerHTML=$o,ol=a(),r(qe.$$.fragment),sl=a(),Ue=s("p"),Ue.innerHTML=Mo,il=a(),r(ze.$$.fragment),rl=a(),w=s("div"),r(Ze.$$.fragment),ja=a(),an=s("p"),an.innerHTML=To,Ga=a(),z=s("div"),r(We.$$.fragment),Qa=a(),on=s("p"),on.textContent=wo,Ba=a(),Z=s("div"),r(je.$$.fragment),Xa=a(),sn=s("p"),sn.textContent=vo,Va=a(),W=s("div"),r(Ge.$$.fragment),La=a(),rn=s("p"),rn.textContent=Co,ml=a(),r(Qe.$$.fragment),dl=a(),Be=s("p"),Be.innerHTML=Jo,pl=a(),Xe=s("p"),Xe.innerHTML=ko,fl=a(),Ve=s("p"),Ve.innerHTML=xo,cl=a(),Le=s("p"),Le.innerHTML=qo,ul=a(),Re=s("p"),Re.innerHTML=Uo,bl=a(),r(Fe.$$.fragment),gl=a(),Ne=s("p"),Ne.innerHTML=zo,hl=a(),r(He.$$.fragment),_l=a(),Pe=s("p"),Pe.innerHTML=Zo,yl=a(),r(Ie.$$.fragment),$l=a(),r(Ae.$$.fragment),Ml=a(),r(Ee.$$.fragment),Tl=a(),Ye=s("p"),Ye.textContent=Wo,wl=a(),Se=s("ul"),Se.innerHTML=jo,vl=a(),r(De.$$.fragment),Cl=a(),Ke=s("ul"),Ke.innerHTML=Go,Jl=a(),r(Oe.$$.fragment),kl=a(),et=s("p"),et.innerHTML=Qo,xl=a(),r(tt.$$.fragment),ql=a(),r(j.$$.fragment),Ul=a(),r(nt.$$.fragment),zl=a(),lt=s("p"),lt.innerHTML=Bo,Zl=a(),r(at.$$.fragment),Wl=a(),ot=s("p"),ot.innerHTML=Xo,jl=a(),st=s("p"),st.innerHTML=Vo,Gl=a(),r(it.$$.fragment),Ql=a(),rt=s("p"),rt.textContent=Lo,Bl=a(),r(G.$$.fragment),Xl=a(),r(mt.$$.fragment),Vl=a(),dt=s("p"),dt.textContent=Ro,Ll=a(),r(pt.$$.fragment),Rl=a(),ft=s("p"),ft.innerHTML=Fo,Fl=a(),r(ct.$$.fragment),Nl=a(),r(ut.$$.fragment),Hl=a(),bt=s("p"),bt.textContent=No,Pl=a(),r(gt.$$.fragment),Il=a(),r(ht.$$.fragment),Al=a(),_t=s("p"),_t.textContent=Ho,El=a(),r(yt.$$.fragment),Yl=a(),r($t.$$.fragment),Sl=a(),Mt=s("p"),Mt.innerHTML=Po,Dl=a(),r(Tt.$$.fragment),Kl=a(),r(Q.$$.fragment),Ol=a(),r(wt.$$.fragment),ea=a(),vt=s("p"),vt.innerHTML=Io,ta=a(),r(Ct.$$.fragment),na=a(),Jt=s("p"),Jt.innerHTML=Ao,la=a(),r(kt.$$.fragment),aa=a(),xt=s("p"),xt.textContent=Eo,oa=a(),r(qt.$$.fragment),sa=a(),Ut=s("p"),Ut.innerHTML=Yo,ia=a(),zt=s("p"),zt.innerHTML=So,ra=a(),r(Zt.$$.fragment),ma=a(),Wt=s("p"),Wt.innerHTML=Do,da=a(),r(jt.$$.fragment),pa=a(),Gt=s("p"),Gt.textContent=Ko,fa=a(),r(Qt.$$.fragment),ca=a(),Bt=s("p"),Bt.textContent=Oo,ua=a(),r(Xt.$$.fragment),ba=a(),Vt=s("p"),Vt.innerHTML=es,ga=a(),r(Lt.$$.fragment),ha=a(),r(Rt.$$.fragment),_a=a(),Ft=s("p"),Ft.innerHTML=ts,ya=a(),r(Nt.$$.fragment),$a=a(),r(Ht.$$.fragment),Ma=a(),Pt=s("p"),Pt.innerHTML=ns,Ta=a(),It=s("p"),It.innerHTML=ls,wa=a(),r(At.$$.fragment),va=a(),M=s("div"),r(Et.$$.fragment),Ra=a(),mn=s("p"),mn.innerHTML=as,Fa=a(),dn=s("p"),dn.innerHTML=os,Na=a(),pn=s("p"),pn.innerHTML=ss,Ha=a(),B=s("div"),r(Yt.$$.fragment),Pa=a(),fn=s("p"),fn.innerHTML=is,Ia=a(),X=s("div"),r(St.$$.fragment),Aa=a(),cn=s("p"),cn.textContent=rs,Ea=a(),V=s("div"),r(Dt.$$.fragment),Ya=a(),un=s("p"),un.innerHTML=ms,Sa=a(),L=s("div"),r(Kt.$$.fragment),Da=a(),bn=s("p"),bn.textContent=ds,Ca=a(),r(Ot.$$.fragment),Ja=a(),en=s("p"),en.innerHTML=ps,ka=a(),gn=s("p"),this.h()},l(e){const t=ys("svelte-u9bgzb",document.head);g=i(t,"META",{name:!0,content:!0}),t.forEach(n),y=o(e),$=i(e,"P",{}),C($).forEach(n),q=o(e),m(R.$$.fragment,e),hn=o(e),m(F.$$.fragment,e),_n=o(e),N=i(e,"P",{"data-svelte-h":!0}),u(N)!=="svelte-1ag92u2"&&(N.innerHTML=Ka),yn=o(e),H=i(e,"P",{"data-svelte-h":!0}),u(H)!=="svelte-1gbdty9"&&(H.textContent=Oa),$n=o(e),P=i(e,"UL",{"data-svelte-h":!0}),u(P)!=="svelte-z5jal9"&&(P.innerHTML=eo),Mn=o(e),m(I.$$.fragment,e),Tn=o(e),A=i(e,"P",{"data-svelte-h":!0}),u(A)!=="svelte-1l7cenj"&&(A.textContent=to),wn=o(e),E=i(e,"UL",{"data-svelte-h":!0}),u(E)!=="svelte-hwx8is"&&(E.innerHTML=no),vn=o(e),Y=i(e,"P",{"data-svelte-h":!0}),u(Y)!=="svelte-mtpiza"&&(Y.textContent=lo),Cn=o(e),m(S.$$.fragment,e),Jn=o(e),D=i(e,"P",{"data-svelte-h":!0}),u(D)!=="svelte-1cu78c7"&&(D.innerHTML=ao),kn=o(e),K=i(e,"P",{"data-svelte-h":!0}),u(K)!=="svelte-173mkyz"&&(K.textContent=oo),xn=o(e),m(O.$$.fragment,e),qn=o(e),ee=i(e,"P",{"data-svelte-h":!0}),u(ee)!=="svelte-8sbga0"&&(ee.innerHTML=so),Un=o(e),m(te.$$.fragment,e),zn=o(e),ne=i(e,"P",{"data-svelte-h":!0}),u(ne)!=="svelte-o6s3th"&&(ne.textContent=io),Zn=o(e),m(le.$$.fragment,e),Wn=o(e),m(ae.$$.fragment,e),jn=o(e),oe=i(e,"P",{"data-svelte-h":!0}),u(oe)!=="svelte-1h9rpz3"&&(oe.innerHTML=ro),Gn=o(e),m(se.$$.fragment,e),Qn=o(e),ie=i(e,"P",{"data-svelte-h":!0}),u(ie)!=="svelte-1wifo6l"&&(ie.textContent=mo),Bn=o(e),re=i(e,"P",{"data-svelte-h":!0}),u(re)!=="svelte-1dkzvn5"&&(re.innerHTML=po),Xn=o(e),m(me.$$.fragment,e),Vn=o(e),de=i(e,"P",{"data-svelte-h":!0}),u(de)!=="svelte-b4piob"&&(de.innerHTML=fo),Ln=o(e),m(U.$$.fragment,e),Rn=o(e),m(pe.$$.fragment,e),Fn=o(e),fe=i(e,"P",{"data-svelte-h":!0}),u(fe)!=="svelte-1mi5065"&&(fe.innerHTML=co),Nn=o(e),m(ce.$$.fragment,e),Hn=o(e),ue=i(e,"P",{"data-svelte-h":!0}),u(ue)!=="svelte-1wrdmud"&&(ue.innerHTML=uo),Pn=o(e),m(be.$$.fragment,e),In=o(e),ge=i(e,"P",{"data-svelte-h":!0}),u(ge)!=="svelte-1taffib"&&(ge.innerHTML=bo),An=o(e),m(he.$$.fragment,e),En=o(e),m(_e.$$.fragment,e),Yn=o(e),ye=i(e,"P",{"data-svelte-h":!0}),u(ye)!=="svelte-1q9lmg5"&&(ye.innerHTML=go),Sn=o(e),m($e.$$.fragment,e),Dn=o(e),Me=i(e,"P",{"data-svelte-h":!0}),u(Me)!=="svelte-1mb4sw2"&&(Me.innerHTML=ho),Kn=o(e),m(Te.$$.fragment,e),On=o(e),m(we.$$.fragment,e),el=o(e),ve=i(e,"P",{"data-svelte-h":!0}),u(ve)!=="svelte-rwqpya"&&(ve.innerHTML=_o),tl=o(e),m(Ce.$$.fragment,e),nl=o(e),Je=i(e,"P",{"data-svelte-h":!0}),u(Je)!=="svelte-2djxfj"&&(Je.textContent=yo),ll=o(e),m(ke.$$.fragment,e),al=o(e),xe=i(e,"P",{"data-svelte-h":!0}),u(xe)!=="svelte-16ybbze"&&(xe.innerHTML=$o),ol=o(e),m(qe.$$.fragment,e),sl=o(e),Ue=i(e,"P",{"data-svelte-h":!0}),u(Ue)!=="svelte-cf6ao3"&&(Ue.innerHTML=Mo),il=o(e),m(ze.$$.fragment,e),rl=o(e),w=i(e,"DIV",{class:!0});var v=C(w);m(Ze.$$.fragment,v),ja=o(v),an=i(v,"P",{"data-svelte-h":!0}),u(an)!=="svelte-i3efvr"&&(an.innerHTML=To),Ga=o(v),z=i(v,"DIV",{class:!0});var tn=C(z);m(We.$$.fragment,tn),Qa=o(tn),on=i(tn,"P",{"data-svelte-h":!0}),u(on)!=="svelte-4jdj2l"&&(on.textContent=wo),tn.forEach(n),Ba=o(v),Z=i(v,"DIV",{class:!0});var nn=C(Z);m(je.$$.fragment,nn),Xa=o(nn),sn=i(nn,"P",{"data-svelte-h":!0}),u(sn)!=="svelte-1ozftb6"&&(sn.textContent=vo),nn.forEach(n),Va=o(v),W=i(v,"DIV",{class:!0});var ln=C(W);m(Ge.$$.fragment,ln),La=o(ln),rn=i(ln,"P",{"data-svelte-h":!0}),u(rn)!=="svelte-pjgtd6"&&(rn.textContent=Co),ln.forEach(n),v.forEach(n),ml=o(e),m(Qe.$$.fragment,e),dl=o(e),Be=i(e,"P",{"data-svelte-h":!0}),u(Be)!=="svelte-1lw70bk"&&(Be.innerHTML=Jo),pl=o(e),Xe=i(e,"P",{"data-svelte-h":!0}),u(Xe)!=="svelte-19v0jp6"&&(Xe.innerHTML=ko),fl=o(e),Ve=i(e,"P",{"data-svelte-h":!0}),u(Ve)!=="svelte-1u8e287"&&(Ve.innerHTML=xo),cl=o(e),Le=i(e,"P",{"data-svelte-h":!0}),u(Le)!=="svelte-g9g250"&&(Le.innerHTML=qo),ul=o(e),Re=i(e,"P",{"data-svelte-h":!0}),u(Re)!=="svelte-ouymag"&&(Re.innerHTML=Uo),bl=o(e),m(Fe.$$.fragment,e),gl=o(e),Ne=i(e,"P",{"data-svelte-h":!0}),u(Ne)!=="svelte-qonxi"&&(Ne.innerHTML=zo),hl=o(e),m(He.$$.fragment,e),_l=o(e),Pe=i(e,"P",{"data-svelte-h":!0}),u(Pe)!=="svelte-1rxk684"&&(Pe.innerHTML=Zo),yl=o(e),m(Ie.$$.fragment,e),$l=o(e),m(Ae.$$.fragment,e),Ml=o(e),m(Ee.$$.fragment,e),Tl=o(e),Ye=i(e,"P",{"data-svelte-h":!0}),u(Ye)!=="svelte-1lnnlr4"&&(Ye.textContent=Wo),wl=o(e),Se=i(e,"UL",{"data-svelte-h":!0}),u(Se)!=="svelte-1wl10a1"&&(Se.innerHTML=jo),vl=o(e),m(De.$$.fragment,e),Cl=o(e),Ke=i(e,"UL",{"data-svelte-h":!0}),u(Ke)!=="svelte-iudtgn"&&(Ke.innerHTML=Go),Jl=o(e),m(Oe.$$.fragment,e),kl=o(e),et=i(e,"P",{"data-svelte-h":!0}),u(et)!=="svelte-artgoh"&&(et.innerHTML=Qo),xl=o(e),m(tt.$$.fragment,e),ql=o(e),m(j.$$.fragment,e),Ul=o(e),m(nt.$$.fragment,e),zl=o(e),lt=i(e,"P",{"data-svelte-h":!0}),u(lt)!=="svelte-1ep4vf9"&&(lt.innerHTML=Bo),Zl=o(e),m(at.$$.fragment,e),Wl=o(e),ot=i(e,"P",{"data-svelte-h":!0}),u(ot)!=="svelte-1wye4p4"&&(ot.innerHTML=Xo),jl=o(e),st=i(e,"P",{"data-svelte-h":!0}),u(st)!=="svelte-3h8e3v"&&(st.innerHTML=Vo),Gl=o(e),m(it.$$.fragment,e),Ql=o(e),rt=i(e,"P",{"data-svelte-h":!0}),u(rt)!=="svelte-1qrn2hy"&&(rt.textContent=Lo),Bl=o(e),m(G.$$.fragment,e),Xl=o(e),m(mt.$$.fragment,e),Vl=o(e),dt=i(e,"P",{"data-svelte-h":!0}),u(dt)!=="svelte-brbl2r"&&(dt.textContent=Ro),Ll=o(e),m(pt.$$.fragment,e),Rl=o(e),ft=i(e,"P",{"data-svelte-h":!0}),u(ft)!=="svelte-8cey66"&&(ft.innerHTML=Fo),Fl=o(e),m(ct.$$.fragment,e),Nl=o(e),m(ut.$$.fragment,e),Hl=o(e),bt=i(e,"P",{"data-svelte-h":!0}),u(bt)!=="svelte-1gdahjp"&&(bt.textContent=No),Pl=o(e),m(gt.$$.fragment,e),Il=o(e),m(ht.$$.fragment,e),Al=o(e),_t=i(e,"P",{"data-svelte-h":!0}),u(_t)!=="svelte-1wj738d"&&(_t.textContent=Ho),El=o(e),m(yt.$$.fragment,e),Yl=o(e),m($t.$$.fragment,e),Sl=o(e),Mt=i(e,"P",{"data-svelte-h":!0}),u(Mt)!=="svelte-9if3q8"&&(Mt.innerHTML=Po),Dl=o(e),m(Tt.$$.fragment,e),Kl=o(e),m(Q.$$.fragment,e),Ol=o(e),m(wt.$$.fragment,e),ea=o(e),vt=i(e,"P",{"data-svelte-h":!0}),u(vt)!=="svelte-1am21ml"&&(vt.innerHTML=Io),ta=o(e),m(Ct.$$.fragment,e),na=o(e),Jt=i(e,"P",{"data-svelte-h":!0}),u(Jt)!=="svelte-1wm02p5"&&(Jt.innerHTML=Ao),la=o(e),m(kt.$$.fragment,e),aa=o(e),xt=i(e,"P",{"data-svelte-h":!0}),u(xt)!=="svelte-1xz0d87"&&(xt.textContent=Eo),oa=o(e),m(qt.$$.fragment,e),sa=o(e),Ut=i(e,"P",{"data-svelte-h":!0}),u(Ut)!=="svelte-yl6mei"&&(Ut.innerHTML=Yo),ia=o(e),zt=i(e,"P",{"data-svelte-h":!0}),u(zt)!=="svelte-xbol81"&&(zt.innerHTML=So),ra=o(e),m(Zt.$$.fragment,e),ma=o(e),Wt=i(e,"P",{"data-svelte-h":!0}),u(Wt)!=="svelte-1n9uav7"&&(Wt.innerHTML=Do),da=o(e),m(jt.$$.fragment,e),pa=o(e),Gt=i(e,"P",{"data-svelte-h":!0}),u(Gt)!=="svelte-mnwhfx"&&(Gt.textContent=Ko),fa=o(e),m(Qt.$$.fragment,e),ca=o(e),Bt=i(e,"P",{"data-svelte-h":!0}),u(Bt)!=="svelte-rwyjy1"&&(Bt.textContent=Oo),ua=o(e),m(Xt.$$.fragment,e),ba=o(e),Vt=i(e,"P",{"data-svelte-h":!0}),u(Vt)!=="svelte-1ozkl3z"&&(Vt.innerHTML=es),ga=o(e),m(Lt.$$.fragment,e),ha=o(e),m(Rt.$$.fragment,e),_a=o(e),Ft=i(e,"P",{"data-svelte-h":!0}),u(Ft)!=="svelte-bn1879"&&(Ft.innerHTML=ts),ya=o(e),m(Nt.$$.fragment,e),$a=o(e),m(Ht.$$.fragment,e),Ma=o(e),Pt=i(e,"P",{"data-svelte-h":!0}),u(Pt)!=="svelte-mgdccl"&&(Pt.innerHTML=ns),Ta=o(e),It=i(e,"P",{"data-svelte-h":!0}),u(It)!=="svelte-14qh4dh"&&(It.innerHTML=ls),wa=o(e),m(At.$$.fragment,e),va=o(e),M=i(e,"DIV",{class:!0});var T=C(M);m(Et.$$.fragment,T),Ra=o(T),mn=i(T,"P",{"data-svelte-h":!0}),u(mn)!=="svelte-woamwr"&&(mn.innerHTML=as),Fa=o(T),dn=i(T,"P",{"data-svelte-h":!0}),u(dn)!=="svelte-ki5gis"&&(dn.innerHTML=os),Na=o(T),pn=i(T,"P",{"data-svelte-h":!0}),u(pn)!=="svelte-8qsk2q"&&(pn.innerHTML=ss),Ha=o(T),B=i(T,"DIV",{class:!0});var qa=C(B);m(Yt.$$.fragment,qa),Pa=o(qa),fn=i(qa,"P",{"data-svelte-h":!0}),u(fn)!=="svelte-10tvzyv"&&(fn.innerHTML=is),qa.forEach(n),Ia=o(T),X=i(T,"DIV",{class:!0});var Ua=C(X);m(St.$$.fragment,Ua),Aa=o(Ua),cn=i(Ua,"P",{"data-svelte-h":!0}),u(cn)!=="svelte-gy26u4"&&(cn.textContent=rs),Ua.forEach(n),Ea=o(T),V=i(T,"DIV",{class:!0});var za=C(V);m(Dt.$$.fragment,za),Ya=o(za),un=i(za,"P",{"data-svelte-h":!0}),u(un)!=="svelte-19bn0da"&&(un.innerHTML=ms),za.forEach(n),Sa=o(T),L=i(T,"DIV",{class:!0});var Za=C(L);m(Kt.$$.fragment,Za),Da=o(Za),bn=i(Za,"P",{"data-svelte-h":!0}),u(bn)!=="svelte-1p6bdas"&&(bn.textContent=ds),Za.forEach(n),T.forEach(n),Ca=o(e),m(Ot.$$.fragment,e),Ja=o(e),en=i(e,"P",{"data-svelte-h":!0}),u(en)!=="svelte-p3rxgx"&&(en.innerHTML=ps),ka=o(e),gn=i(e,"P",{}),C(gn).forEach(n),this.h()},h(){J(g,"name","hf:doc:metadata"),J(g,"content",Cs),J(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){b(document.head,g),l(e,y,t),l(e,$,t),l(e,q,t),d(R,e,t),l(e,hn,t),d(F,e,t),l(e,_n,t),l(e,N,t),l(e,yn,t),l(e,H,t),l(e,$n,t),l(e,P,t),l(e,Mn,t),d(I,e,t),l(e,Tn,t),l(e,A,t),l(e,wn,t),l(e,E,t),l(e,vn,t),l(e,Y,t),l(e,Cn,t),d(S,e,t),l(e,Jn,t),l(e,D,t),l(e,kn,t),l(e,K,t),l(e,xn,t),d(O,e,t),l(e,qn,t),l(e,ee,t),l(e,Un,t),d(te,e,t),l(e,zn,t),l(e,ne,t),l(e,Zn,t),d(le,e,t),l(e,Wn,t),d(ae,e,t),l(e,jn,t),l(e,oe,t),l(e,Gn,t),d(se,e,t),l(e,Qn,t),l(e,ie,t),l(e,Bn,t),l(e,re,t),l(e,Xn,t),d(me,e,t),l(e,Vn,t),l(e,de,t),l(e,Ln,t),d(U,e,t),l(e,Rn,t),d(pe,e,t),l(e,Fn,t),l(e,fe,t),l(e,Nn,t),d(ce,e,t),l(e,Hn,t),l(e,ue,t),l(e,Pn,t),d(be,e,t),l(e,In,t),l(e,ge,t),l(e,An,t),d(he,e,t),l(e,En,t),d(_e,e,t),l(e,Yn,t),l(e,ye,t),l(e,Sn,t),d($e,e,t),l(e,Dn,t),l(e,Me,t),l(e,Kn,t),d(Te,e,t),l(e,On,t),d(we,e,t),l(e,el,t),l(e,ve,t),l(e,tl,t),d(Ce,e,t),l(e,nl,t),l(e,Je,t),l(e,ll,t),d(ke,e,t),l(e,al,t),l(e,xe,t),l(e,ol,t),d(qe,e,t),l(e,sl,t),l(e,Ue,t),l(e,il,t),d(ze,e,t),l(e,rl,t),l(e,w,t),d(Ze,w,null),b(w,ja),b(w,an),b(w,Ga),b(w,z),d(We,z,null),b(z,Qa),b(z,on),b(w,Ba),b(w,Z),d(je,Z,null),b(Z,Xa),b(Z,sn),b(w,Va),b(w,W),d(Ge,W,null),b(W,La),b(W,rn),l(e,ml,t),d(Qe,e,t),l(e,dl,t),l(e,Be,t),l(e,pl,t),l(e,Xe,t),l(e,fl,t),l(e,Ve,t),l(e,cl,t),l(e,Le,t),l(e,ul,t),l(e,Re,t),l(e,bl,t),d(Fe,e,t),l(e,gl,t),l(e,Ne,t),l(e,hl,t),d(He,e,t),l(e,_l,t),l(e,Pe,t),l(e,yl,t),d(Ie,e,t),l(e,$l,t),d(Ae,e,t),l(e,Ml,t),d(Ee,e,t),l(e,Tl,t),l(e,Ye,t),l(e,wl,t),l(e,Se,t),l(e,vl,t),d(De,e,t),l(e,Cl,t),l(e,Ke,t),l(e,Jl,t),d(Oe,e,t),l(e,kl,t),l(e,et,t),l(e,xl,t),d(tt,e,t),l(e,ql,t),d(j,e,t),l(e,Ul,t),d(nt,e,t),l(e,zl,t),l(e,lt,t),l(e,Zl,t),d(at,e,t),l(e,Wl,t),l(e,ot,t),l(e,jl,t),l(e,st,t),l(e,Gl,t),d(it,e,t),l(e,Ql,t),l(e,rt,t),l(e,Bl,t),d(G,e,t),l(e,Xl,t),d(mt,e,t),l(e,Vl,t),l(e,dt,t),l(e,Ll,t),d(pt,e,t),l(e,Rl,t),l(e,ft,t),l(e,Fl,t),d(ct,e,t),l(e,Nl,t),d(ut,e,t),l(e,Hl,t),l(e,bt,t),l(e,Pl,t),d(gt,e,t),l(e,Il,t),d(ht,e,t),l(e,Al,t),l(e,_t,t),l(e,El,t),d(yt,e,t),l(e,Yl,t),d($t,e,t),l(e,Sl,t),l(e,Mt,t),l(e,Dl,t),d(Tt,e,t),l(e,Kl,t),d(Q,e,t),l(e,Ol,t),d(wt,e,t),l(e,ea,t),l(e,vt,t),l(e,ta,t),d(Ct,e,t),l(e,na,t),l(e,Jt,t),l(e,la,t),d(kt,e,t),l(e,aa,t),l(e,xt,t),l(e,oa,t),d(qt,e,t),l(e,sa,t),l(e,Ut,t),l(e,ia,t),l(e,zt,t),l(e,ra,t),d(Zt,e,t),l(e,ma,t),l(e,Wt,t),l(e,da,t),d(jt,e,t),l(e,pa,t),l(e,Gt,t),l(e,fa,t),d(Qt,e,t),l(e,ca,t),l(e,Bt,t),l(e,ua,t),d(Xt,e,t),l(e,ba,t),l(e,Vt,t),l(e,ga,t),d(Lt,e,t),l(e,ha,t),d(Rt,e,t),l(e,_a,t),l(e,Ft,t),l(e,ya,t),d(Nt,e,t),l(e,$a,t),d(Ht,e,t),l(e,Ma,t),l(e,Pt,t),l(e,Ta,t),l(e,It,t),l(e,wa,t),d(At,e,t),l(e,va,t),l(e,M,t),d(Et,M,null),b(M,Ra),b(M,mn),b(M,Fa),b(M,dn),b(M,Na),b(M,pn),b(M,Ha),b(M,B),d(Yt,B,null),b(B,Pa),b(B,fn),b(M,Ia),b(M,X),d(St,X,null),b(X,Aa),b(X,cn),b(M,Ea),b(M,V),d(Dt,V,null),b(V,Ya),b(V,un),b(M,Sa),b(M,L),d(Kt,L,null),b(L,Da),b(L,bn),l(e,Ca,t),d(Ot,e,t),l(e,Ja,t),l(e,en,t),l(e,ka,t),l(e,gn,t),xa=!0},p(e,[t]){const v={};t&2&&(v.$$scope={dirty:t,ctx:e}),U.$set(v);const tn={};t&2&&(tn.$$scope={dirty:t,ctx:e}),j.$set(tn);const nn={};t&2&&(nn.$$scope={dirty:t,ctx:e}),G.$set(nn);const ln={};t&2&&(ln.$$scope={dirty:t,ctx:e}),Q.$set(ln)},i(e){xa||(p(R.$$.fragment,e),p(F.$$.fragment,e),p(I.$$.fragment,e),p(S.$$.fragment,e),p(O.$$.fragment,e),p(te.$$.fragment,e),p(le.$$.fragment,e),p(ae.$$.fragment,e),p(se.$$.fragment,e),p(me.$$.fragment,e),p(U.$$.fragment,e),p(pe.$$.fragment,e),p(ce.$$.fragment,e),p(be.$$.fragment,e),p(he.$$.fragment,e),p(_e.$$.fragment,e),p($e.$$.fragment,e),p(Te.$$.fragment,e),p(we.$$.fragment,e),p(Ce.$$.fragment,e),p(ke.$$.fragment,e),p(qe.$$.fragment,e),p(ze.$$.fragment,e),p(Ze.$$.fragment,e),p(We.$$.fragment,e),p(je.$$.fragment,e),p(Ge.$$.fragment,e),p(Qe.$$.fragment,e),p(Fe.$$.fragment,e),p(He.$$.fragment,e),p(Ie.$$.fragment,e),p(Ae.$$.fragment,e),p(Ee.$$.fragment,e),p(De.$$.fragment,e),p(Oe.$$.fragment,e),p(tt.$$.fragment,e),p(j.$$.fragment,e),p(nt.$$.fragment,e),p(at.$$.fragment,e),p(it.$$.fragment,e),p(G.$$.fragment,e),p(mt.$$.fragment,e),p(pt.$$.fragment,e),p(ct.$$.fragment,e),p(ut.$$.fragment,e),p(gt.$$.fragment,e),p(ht.$$.fragment,e),p(yt.$$.fragment,e),p($t.$$.fragment,e),p(Tt.$$.fragment,e),p(Q.$$.fragment,e),p(wt.$$.fragment,e),p(Ct.$$.fragment,e),p(kt.$$.fragment,e),p(qt.$$.fragment,e),p(Zt.$$.fragment,e),p(jt.$$.fragment,e),p(Qt.$$.fragment,e),p(Xt.$$.fragment,e),p(Lt.$$.fragment,e),p(Rt.$$.fragment,e),p(Nt.$$.fragment,e),p(Ht.$$.fragment,e),p(At.$$.fragment,e),p(Et.$$.fragment,e),p(Yt.$$.fragment,e),p(St.$$.fragment,e),p(Dt.$$.fragment,e),p(Kt.$$.fragment,e),p(Ot.$$.fragment,e),xa=!0)},o(e){f(R.$$.fragment,e),f(F.$$.fragment,e),f(I.$$.fragment,e),f(S.$$.fragment,e),f(O.$$.fragment,e),f(te.$$.fragment,e),f(le.$$.fragment,e),f(ae.$$.fragment,e),f(se.$$.fragment,e),f(me.$$.fragment,e),f(U.$$.fragment,e),f(pe.$$.fragment,e),f(ce.$$.fragment,e),f(be.$$.fragment,e),f(he.$$.fragment,e),f(_e.$$.fragment,e),f($e.$$.fragment,e),f(Te.$$.fragment,e),f(we.$$.fragment,e),f(Ce.$$.fragment,e),f(ke.$$.fragment,e),f(qe.$$.fragment,e),f(ze.$$.fragment,e),f(Ze.$$.fragment,e),f(We.$$.fragment,e),f(je.$$.fragment,e),f(Ge.$$.fragment,e),f(Qe.$$.fragment,e),f(Fe.$$.fragment,e),f(He.$$.fragment,e),f(Ie.$$.fragment,e),f(Ae.$$.fragment,e),f(Ee.$$.fragment,e),f(De.$$.fragment,e),f(Oe.$$.fragment,e),f(tt.$$.fragment,e),f(j.$$.fragment,e),f(nt.$$.fragment,e),f(at.$$.fragment,e),f(it.$$.fragment,e),f(G.$$.fragment,e),f(mt.$$.fragment,e),f(pt.$$.fragment,e),f(ct.$$.fragment,e),f(ut.$$.fragment,e),f(gt.$$.fragment,e),f(ht.$$.fragment,e),f(yt.$$.fragment,e),f($t.$$.fragment,e),f(Tt.$$.fragment,e),f(Q.$$.fragment,e),f(wt.$$.fragment,e),f(Ct.$$.fragment,e),f(kt.$$.fragment,e),f(qt.$$.fragment,e),f(Zt.$$.fragment,e),f(jt.$$.fragment,e),f(Qt.$$.fragment,e),f(Xt.$$.fragment,e),f(Lt.$$.fragment,e),f(Rt.$$.fragment,e),f(Nt.$$.fragment,e),f(Ht.$$.fragment,e),f(At.$$.fragment,e),f(Et.$$.fragment,e),f(Yt.$$.fragment,e),f(St.$$.fragment,e),f(Dt.$$.fragment,e),f(Kt.$$.fragment,e),f(Ot.$$.fragment,e),xa=!1},d(e){e&&(n(y),n($),n(q),n(hn),n(_n),n(N),n(yn),n(H),n($n),n(P),n(Mn),n(Tn),n(A),n(wn),n(E),n(vn),n(Y),n(Cn),n(Jn),n(D),n(kn),n(K),n(xn),n(qn),n(ee),n(Un),n(zn),n(ne),n(Zn),n(Wn),n(jn),n(oe),n(Gn),n(Qn),n(ie),n(Bn),n(re),n(Xn),n(Vn),n(de),n(Ln),n(Rn),n(Fn),n(fe),n(Nn),n(Hn),n(ue),n(Pn),n(In),n(ge),n(An),n(En),n(Yn),n(ye),n(Sn),n(Dn),n(Me),n(Kn),n(On),n(el),n(ve),n(tl),n(nl),n(Je),n(ll),n(al),n(xe),n(ol),n(sl),n(Ue),n(il),n(rl),n(w),n(ml),n(dl),n(Be),n(pl),n(Xe),n(fl),n(Ve),n(cl),n(Le),n(ul),n(Re),n(bl),n(gl),n(Ne),n(hl),n(_l),n(Pe),n(yl),n($l),n(Ml),n(Tl),n(Ye),n(wl),n(Se),n(vl),n(Cl),n(Ke),n(Jl),n(kl),n(et),n(xl),n(ql),n(Ul),n(zl),n(lt),n(Zl),n(Wl),n(ot),n(jl),n(st),n(Gl),n(Ql),n(rt),n(Bl),n(Xl),n(Vl),n(dt),n(Ll),n(Rl),n(ft),n(Fl),n(Nl),n(Hl),n(bt),n(Pl),n(Il),n(Al),n(_t),n(El),n(Yl),n(Sl),n(Mt),n(Dl),n(Kl),n(Ol),n(ea),n(vt),n(ta),n(na),n(Jt),n(la),n(aa),n(xt),n(oa),n(sa),n(Ut),n(ia),n(zt),n(ra),n(ma),n(Wt),n(da),n(pa),n(Gt),n(fa),n(ca),n(Bt),n(ua),n(ba),n(Vt),n(ga),n(ha),n(_a),n(Ft),n(ya),n($a),n(Ma),n(Pt),n(Ta),n(It),n(wa),n(va),n(M),n(Ca),n(Ja),n(en),n(ka),n(gn)),n(g),c(R,e),c(F,e),c(I,e),c(S,e),c(O,e),c(te,e),c(le,e),c(ae,e),c(se,e),c(me,e),c(U,e),c(pe,e),c(ce,e),c(be,e),c(he,e),c(_e,e),c($e,e),c(Te,e),c(we,e),c(Ce,e),c(ke,e),c(qe,e),c(ze,e),c(Ze),c(We),c(je),c(Ge),c(Qe,e),c(Fe,e),c(He,e),c(Ie,e),c(Ae,e),c(Ee,e),c(De,e),c(Oe,e),c(tt,e),c(j,e),c(nt,e),c(at,e),c(it,e),c(G,e),c(mt,e),c(pt,e),c(ct,e),c(ut,e),c(gt,e),c(ht,e),c(yt,e),c($t,e),c(Tt,e),c(Q,e),c(wt,e),c(Ct,e),c(kt,e),c(qt,e),c(Zt,e),c(jt,e),c(Qt,e),c(Xt,e),c(Lt,e),c(Rt,e),c(Nt,e),c(Ht,e),c(At,e),c(Et),c(Yt),c(St),c(Dt),c(Kt),c(Ot,e)}}}const Cs='{"title":"Quantize 🤗 Transformers models","local":"quantize--transformers-models","sections":[{"title":"AutoGPTQ Integration","local":"autogptq-integration","sections":[{"title":"Requirements","local":"requirements","sections":[],"depth":3},{"title":"Load and quantize a model","local":"load-and-quantize-a-model","sections":[{"title":"GPTQ Configuration","local":"gptq-configuration","sections":[],"depth":4},{"title":"Quantization","local":"quantization","sections":[],"depth":4}],"depth":3},{"title":"Push quantized model to 🤗 Hub","local":"push-quantized-model-to--hub","sections":[],"depth":3},{"title":"Load a quantized model from the 🤗 Hub","local":"load-a-quantized-model-from-the--hub","sections":[],"depth":3},{"title":"Exllama kernels for faster inference","local":"exllama-kernels-for-faster-inference","sections":[{"title":"Fine-tune a quantized model","local":"fine-tune-a-quantized-model","sections":[],"depth":4}],"depth":3},{"title":"Example demo","local":"example-demo","sections":[],"depth":3},{"title":"GPTQConfig","local":"transformers.GPTQConfig","sections":[],"depth":3}],"depth":2},{"title":"bitsandbytes Integration","local":"bitsandbytes-integration","sections":[{"title":"General usage","local":"general-usage","sections":[],"depth":3},{"title":"FP4 quantization","local":"fp4-quantization","sections":[{"title":"Requirements","local":"requirements","sections":[],"depth":4},{"title":"Tips and best practices","local":"tips-and-best-practices","sections":[],"depth":4},{"title":"Load a large model in 4bit","local":"load-a-large-model-in-4bit","sections":[],"depth":4}],"depth":3},{"title":"Load a large model in 8bit","local":"load-a-large-model-in-8bit","sections":[{"title":"Advanced use cases","local":"advanced-use-cases","sections":[{"title":"Change the compute dtype","local":"change-the-compute-dtype","sections":[],"depth":5},{"title":"Using NF4 (Normal Float 4) data type","local":"using-nf4-normal-float-4-data-type","sections":[],"depth":5},{"title":"Use nested quantization for more memory efficient inference","local":"use-nested-quantization-for-more-memory-efficient-inference","sections":[],"depth":5}],"depth":4}],"depth":3},{"title":"Push quantized models on the 🤗 Hub","local":"push-quantized-models-on-the--hub","sections":[],"depth":3},{"title":"Load a quantized model from the 🤗 Hub","local":"load-a-quantized-model-from-the--hub","sections":[],"depth":3},{"title":"Advanced use cases","local":"advanced-use-cases","sections":[{"title":"Offload between cpu and gpu","local":"offload-between-cpu-and-gpu","sections":[],"depth":4},{"title":"Play with llm_int8_threshold","local":"play-with-llmint8threshold","sections":[],"depth":4},{"title":"Skip the conversion of some modules","local":"skip-the-conversion-of-some-modules","sections":[],"depth":4},{"title":"Fine-tune a model that has been loaded in 8-bit","local":"fine-tune-a-model-that-has-been-loaded-in-8-bit","sections":[],"depth":4}],"depth":3},{"title":"BitsAndBytesConfig","local":"transformers.BitsAndBytesConfig","sections":[],"depth":3}],"depth":2},{"title":"Quantization with 🤗 optimum","local":"quantization-with--optimum","sections":[],"depth":2}],"depth":1}';function Js(k){return gs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ws extends hs{constructor(g){super(),_s(this,g,Js,vs,bs,{})}}export{Ws as component};
