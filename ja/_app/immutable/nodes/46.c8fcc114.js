import{s as bs,o as gs,n as fs}from"../chunks/scheduler.9bc65507.js";import{S as hs,i as _s,g as s,s as a,r,A as ys,h as i,f as n,c as o,j as C,u as m,x as u,k as J,y as b,a as l,v as d,d as p,t as f,w as c,m as cs,n as us}from"../chunks/index.707bf1b6.js";import{T as Wa}from"../chunks/Tip.c2ecdbf4.js";import{D as x}from"../chunks/Docstring.17db21ae.js";import{C as _}from"../chunks/CodeBlock.54a9f38d.js";import{H as h}from"../chunks/Heading.342b1fa6.js";function $s(k){let g;return{c(){g=cs("GPTQ é‡å­åŒ–ã¯ã€ç¾æ™‚ç‚¹ã§ã¯ãƒ†ã‚­ã‚¹ãƒˆ ãƒ¢ãƒ‡ãƒ«ã§ã®ã¿æ©Ÿèƒ½ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€é‡å­åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«ã‚ˆã£ã¦ã¯é•·æ™‚é–“ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ (NVIDIA A100 ã‚’ä½¿ç”¨ã—ãŸå ´åˆã€175B ãƒ¢ãƒ‡ãƒ« = 4 gpu æ™‚é–“)ã€‚ãƒ¢ãƒ‡ãƒ«ã® GPTQ é‡å­åŒ–ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€ãƒãƒ–ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚ãã†ã§ãªã„å ´åˆã¯ã€github ã§è¦æ±‚ã‚’é€ä¿¡ã§ãã¾ã™ã€‚")},l(y){g=us(y,"GPTQ é‡å­åŒ–ã¯ã€ç¾æ™‚ç‚¹ã§ã¯ãƒ†ã‚­ã‚¹ãƒˆ ãƒ¢ãƒ‡ãƒ«ã§ã®ã¿æ©Ÿèƒ½ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€é‡å­åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«ã‚ˆã£ã¦ã¯é•·æ™‚é–“ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ (NVIDIA A100 ã‚’ä½¿ç”¨ã—ãŸå ´åˆã€175B ãƒ¢ãƒ‡ãƒ« = 4 gpu æ™‚é–“)ã€‚ãƒ¢ãƒ‡ãƒ«ã® GPTQ é‡å­åŒ–ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã€ãƒãƒ–ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚ãã†ã§ãªã„å ´åˆã¯ã€github ã§è¦æ±‚ã‚’é€ä¿¡ã§ãã¾ã™ã€‚")},m(y,$){l(y,g,$)},d(y){y&&n(g)}}}function Ms(k){let g,y="ãƒ¢ãƒ‡ãƒ«ãŒ 4 ãƒ“ãƒƒãƒˆã§ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã¨ã€ç¾æ™‚ç‚¹ã§ã¯é‡å­åŒ–ã•ã‚ŒãŸé‡ã¿ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã“ã¨ã¯ã§ããªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ 4 ãƒ“ãƒƒãƒˆã®é‡ã¿ã¯ã¾ã ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ããªã„ã“ã¨ã«ã‚‚æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€4 ãƒ“ãƒƒãƒˆ ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è¿½åŠ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã‚Œã«ã¤ã„ã¦ã¯æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§èª¬æ˜ã—ã¾ã™ã€‚";return{c(){g=s("p"),g.textContent=y},l($){g=i($,"P",{"data-svelte-h":!0}),u(g)!=="svelte-1p6yucr"&&(g.textContent=y)},m($,q){l($,g,q)},p:fs,d($){$&&n(g)}}}function Ts(k){let g;return{c(){g=cs("ãƒ¢ãƒ‡ãƒ«ãŒ 8 ãƒ“ãƒƒãƒˆã§ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã¨ã€æœ€æ–°ã® `transformers`ã¨`bitsandbytes`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã‚’é™¤ãã€é‡å­åŒ–ã•ã‚ŒãŸé‡ã¿ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã“ã¨ã¯ç¾åœ¨ä¸å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ 8 ãƒ“ãƒƒãƒˆã®é‡ã¿ã¯ã¾ã ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ããªã„ã“ã¨ã«ã‚‚æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€8 ãƒ“ãƒƒãƒˆ ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è¿½åŠ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã‚Œã«ã¤ã„ã¦ã¯æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§èª¬æ˜ã—ã¾ã™ã€‚\nã¾ãŸã€`device_map` ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ãŒã€åˆ©ç”¨å¯èƒ½ãªãƒªã‚½ãƒ¼ã‚¹ä¸Šã§ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã™ã‚‹ãŸã‚ã€æ¨è«–ã«ã¯ `device_map = 'auto'` ã‚’è¨­å®šã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚")},l(y){g=us(y,"ãƒ¢ãƒ‡ãƒ«ãŒ 8 ãƒ“ãƒƒãƒˆã§ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã¨ã€æœ€æ–°ã® `transformers`ã¨`bitsandbytes`ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã‚’é™¤ãã€é‡å­åŒ–ã•ã‚ŒãŸé‡ã¿ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã“ã¨ã¯ç¾åœ¨ä¸å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ 8 ãƒ“ãƒƒãƒˆã®é‡ã¿ã¯ã¾ã ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ããªã„ã“ã¨ã«ã‚‚æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€8 ãƒ“ãƒƒãƒˆ ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦è¿½åŠ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã‚Œã«ã¤ã„ã¦ã¯æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§èª¬æ˜ã—ã¾ã™ã€‚\nã¾ãŸã€`device_map` ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ãŒã€åˆ©ç”¨å¯èƒ½ãªãƒªã‚½ãƒ¼ã‚¹ä¸Šã§ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã™ã‚‹ãŸã‚ã€æ¨è«–ã«ã¯ `device_map = 'auto'` ã‚’è¨­å®šã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚")},m(y,$){l(y,g,$)},d(y){y&&n(g)}}}function ws(k){let g,y="å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒãƒ–ä¸Šã§ 8 ãƒ“ãƒƒãƒˆ ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã“ã¨ãŒå¼·ãæ¨å¥¨ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¯ãƒ¡ãƒ¢ãƒª ãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã®å‰Šæ¸›ã¨ã€ãŸã¨ãˆã° Google Colab ã§ã®å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«ã‚ˆã‚‹æ©æµã‚’å—ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚";return{c(){g=s("p"),g.textContent=y},l($){g=i($,"P",{"data-svelte-h":!0}),u(g)!=="svelte-1bkhcu"&&(g.textContent=y)},m($,q){l($,g,q)},p:fs,d($){$&&n(g)}}}function vs(k){let g,y,$,q,R,hn,F,_n,N,Ka="ğŸ¤— Transformers ã«ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã§ GPTQ é‡å­åŒ–ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã® <code>optimum</code> API ãŒçµ±åˆã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¤§å¹…ã«ä½ä¸‹ã•ã›ã‚‹ã“ã¨ãªãã€æ¨è«–é€Ÿåº¦ã‚’é«˜é€ŸåŒ–ã™ã‚‹ã“ã¨ãªãã€ãƒ¢ãƒ‡ãƒ«ã‚’ 8ã€4ã€3ã€ã•ã‚‰ã«ã¯ 2 ãƒ“ãƒƒãƒˆã§ãƒ­ãƒ¼ãƒ‰ãŠã‚ˆã³é‡å­åŒ–ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€ã»ã¨ã‚“ã©ã® GPU ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚",yn,H,Oa="é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",$n,P,eo='<li><a href="https://arxiv.org/pdf/2210.17323.pdf" rel="nofollow">GPTQ</a> è«–æ–‡</li> <li>GPTQ é‡å­åŒ–ã«é–¢ã™ã‚‹ <code>optimum</code> <a href="https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization" rel="nofollow">ã‚¬ã‚¤ãƒ‰</a></li> <li>ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã‚‹ <a href="https://github.com/PanQiWei/AutoGPTQ" rel="nofollow"><code>AutoGPTQ</code></a> ãƒ©ã‚¤ãƒ–ãƒ©ãƒª</li>',Mn,I,Tn,A,to="ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ã€ä»¥ä¸‹ã®è¦ä»¶ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š",wn,E,no=`<li><p>æœ€æ–°ã® <code>AutoGPTQ</code> ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚
<code>pip install auto-gptq</code> ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚</p></li> <li><p>æœ€æ–°ã® <code>optimum</code> ã‚’ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚
<code>git+https://github.com/huggingface/optimum.git</code> ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚</p></li> <li><p>æœ€æ–°ã® <code>transformers</code> ã‚’ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚
æœ€æ–°ã® <code>transformers</code> ã‚’ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ <code>pip install git+https://github.com/huggingface/transformers.git</code></p></li> <li><p>æœ€æ–°ã® <code>accelerate</code> ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã€‚
<code>pip install --upgrade accelerate</code> ã‚’å®Ÿè¡Œã™ã‚‹ã€‚</p></li>`,vn,Y,lo="GPTQçµ±åˆã¯ä»Šã®ã¨ã“ã‚ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã®ã§ã€è¦–è¦šã€éŸ³å£°ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã¯äºˆæœŸã›ã¬æŒ™å‹•ã«é­é‡ã™ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚",Cn,S,Jn,D,ao="GPTQ ã¯ã€é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«é‡ã¿ã®ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å¿…è¦ã¨ã™ã‚‹é‡å­åŒ–æ–¹æ³•ã§ã™ã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ ãƒ¢ãƒ‡ãƒ«ã‚’æœ€åˆã‹ã‚‰é‡å­åŒ–ã™ã‚‹å ´åˆã¯ã€é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ã¾ã§ã«æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ (<code>facebook/opt-350m</code>ãƒ¢ãƒ‡ãƒ«ã® Google colab ã§ã¯ç´„ 5 åˆ†)ã€‚",kn,K,oo="ã—ãŸãŒã£ã¦ã€GPTQ é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‚·ãƒŠãƒªã‚ªã¯ 2 ã¤ã‚ã‚Šã¾ã™ã€‚æœ€åˆã®ä½¿ç”¨ä¾‹ã¯ã€ãƒãƒ–ã§åˆ©ç”¨å¯èƒ½ãªä»–ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ã™ã§ã«é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã“ã¨ã§ã™ã€‚2 ç•ªç›®ã®ä½¿ç”¨ä¾‹ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’æœ€åˆã‹ã‚‰é‡å­åŒ–ã—ã€ä¿å­˜ã™ã‚‹ã‹ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦ã€ä»–ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§ã™ã€‚ãã‚Œã‚‚ä½¿ã£ã¦ãã ã•ã„ã€‚",xn,O,qn,ee,so='ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦é‡å­åŒ–ã™ã‚‹ã«ã¯ã€<a href="/docs/transformers/main/ja/main_classes/quantization#transformers.GPTQConfig">GPTQConfig</a> ã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æº–å‚™ã™ã‚‹ã«ã¯ã€<code>bits</code>ã®æ•°ã€é‡å­åŒ–ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã®<code>dataset</code>ã€ãŠã‚ˆã³ãƒ¢ãƒ‡ãƒ«ã®<code>Tokenizer</code>ã‚’æ¸¡ã™å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚',Un,te,zn,ne,io="ç‹¬è‡ªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ–‡å­—åˆ—ã®ãƒªã‚¹ãƒˆã¨ã—ã¦æ¸¡ã™ã“ã¨ãŒã§ãã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€GPTQ è«–æ–‡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚",Zn,le,Wn,ae,jn,oe,ro="<code>from_pretrained</code> ã‚’ä½¿ç”¨ã—ã€<code>quantization_config</code> ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–ã§ãã¾ã™ã€‚",Gn,se,Qn,ie,mo="ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–ã™ã‚‹ã«ã¯ GPU ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ CPU ã«é…ç½®ã—ã€é‡å­åŒ–ã™ã‚‹ãŸã‚ã«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ GPU ã«å‰å¾Œã«ç§»å‹•ã•ã›ã¾ã™ã€‚",Bn,re,po="CPU ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã®ä½¿ç”¨ä¸­ã« GPU ã®ä½¿ç”¨é‡ã‚’æœ€å¤§åŒ–ã—ãŸã„å ´åˆã¯ã€<code>device_map = &quot;auto&quot;</code> ã‚’è¨­å®šã§ãã¾ã™ã€‚",Xn,me,Vn,de,fo='ãƒ‡ã‚£ã‚¹ã‚¯ ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã¯ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒåŸå› ã§ãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€<code>from_pretained</code> ã§ <code>max_memory</code> ã‚’æ¸¡ã™å¿…è¦ãŒã‚ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ <code>device_map</code>ã¨<code>max_memory</code>ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€ã“ã® <a href="https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map" rel="nofollow">ã‚¬ã‚¤ãƒ‰</a> ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚',Ln,U,Rn,pe,Fn,fe,co="ä»–ã® ğŸ¤— ãƒ¢ãƒ‡ãƒ«ã¨åŒæ§˜ã«ã€<code>push_to_hub</code> ã‚’ä½¿ç”¨ã—ã¦é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã§ãã¾ã™ã€‚é‡å­åŒ–æ§‹æˆã¯ä¿å­˜ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã«æ²¿ã£ã¦ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚",Nn,ce,Hn,ue,uo="é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ« ãƒã‚·ãƒ³ã«ä¿å­˜ã—ãŸã„å ´åˆã¯ã€<code>save_pretrained</code> ã‚’ä½¿ç”¨ã—ã¦è¡Œã†ã“ã¨ã‚‚ã§ãã¾ã™ã€‚",Pn,be,In,ge,bo="<code>device_map</code> ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–ã—ãŸå ´åˆã¯ã€ä¿å­˜ã™ã‚‹å‰ã«ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’ GPU ã¾ãŸã¯ <code>cpu</code> ã®ã„ãšã‚Œã‹ã«ç§»å‹•ã—ã¦ãã ã•ã„ã€‚",An,he,En,_e,Yn,ye,go=`<code>from_pretrained</code>ã‚’ä½¿ç”¨ã—ã¦ã€é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚
å±æ€§ <code>quantization_config</code> ãŒãƒ¢ãƒ‡ãƒ«è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã€ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸé‡ã¿ãŒé‡å­åŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚`,Sn,$e,Dn,Me,ho="å¿…è¦ä»¥ä¸Šã®ãƒ¡ãƒ¢ãƒªã‚’å‰²ã‚Šå½“ã¦ãšã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ˆã‚Šé€Ÿããƒ­ãƒ¼ãƒ‰ã—ãŸã„å ´åˆã¯ã€<code>device_map</code> å¼•æ•°ã¯é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã§ã‚‚æ©Ÿèƒ½ã—ã¾ã™ã€‚ <code>accelerate</code>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",Kn,Te,On,we,el,ve,_o='4 ãƒ“ãƒƒãƒˆ ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€æ¨è«–é€Ÿåº¦ã‚’é«˜ã‚ã‚‹ãŸã‚ã« exllama ã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§æœ‰åŠ¹ã«ãªã£ã¦ã„ã¾ã™ã€‚ <a href="/docs/transformers/main/ja/main_classes/quantization#transformers.GPTQConfig">GPTQConfig</a> ã§ <code>disable_exllama</code> ã‚’æ¸¡ã™ã“ã¨ã§ã€ãã®å‹•ä½œã‚’å¤‰æ›´ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¨­å®šã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹é‡å­åŒ–è¨­å®šãŒä¸Šæ›¸ãã•ã‚Œã¾ã™ã€‚ã‚«ãƒ¼ãƒãƒ«ã«é–¢é€£ã™ã‚‹å±æ€§ã®ã¿ã‚’ä¸Šæ›¸ãã§ãã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«ã€exllama ã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã—ãŸã„å ´åˆã¯ã€ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’ GPU ä¸Šã«ç½®ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚',tl,Ce,nl,Je,yo="ç¾æ™‚ç‚¹ã§ã¯ 4 ãƒ“ãƒƒãƒˆ ãƒ¢ãƒ‡ãƒ«ã®ã¿ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã•ã‚‰ã«ã€peft ã‚’ä½¿ç”¨ã—ã¦é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¦ã„ã‚‹å ´åˆã¯ã€exllama ã‚«ãƒ¼ãƒãƒ«ã‚’éã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚",ll,ke,al,xe,$o=`Hugging Face ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®å…¬å¼ã‚µãƒãƒ¼ãƒˆã«ã‚ˆã‚Šã€GPTQ ã§é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã§ãã¾ã™ã€‚
è©³ç´°ã«ã¤ã„ã¦ã¯ã€<a href="https://github.com/huggingface/peft" rel="nofollow"><code>peft</code></a> ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã”è¦§ãã ã•ã„ã€‚`,ol,qe,sl,Ue,Mo='GPTQ ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–ã™ã‚‹æ–¹æ³•ã¨ã€peft ã‚’ä½¿ç”¨ã—ã¦é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€Google Colab <a href="https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94iLKUFu6ZX4ceb?usp=sharing" rel="nofollow">ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯</a> ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚',il,ze,rl,w,Ze,ja,an,To=`This is a wrapper class about all possible attributes and features that you can play with a model that has been
loaded using <code>optimum</code> api for gptq quantization relying on auto_gptq backend.`,Ga,z,We,Qa,on,wo="Get compatible class with optimum gptq config dict",Ba,Z,je,Xa,sn,vo="Safety checker that arguments are correct",Va,W,Ge,La,rn,Co="Get compatible dict for optimum gptq config",ml,Qe,dl,Be,Jo=`ğŸ¤— Transformers ã¯ã€<code>bitsandbytes</code> ã§æœ€ã‚‚ã‚ˆãä½¿ç”¨ã•ã‚Œã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ç·Šå¯†ã«çµ±åˆã•ã‚Œã¦ã„ã¾ã™ã€‚æ•°è¡Œã®ã‚³ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’ 8 ãƒ“ãƒƒãƒˆç²¾åº¦ã§ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚
ã“ã‚Œã¯ã€<code>bitsandbytes</code>ã® <code>0.37.0</code>ãƒªãƒªãƒ¼ã‚¹ä»¥é™ã€ã»ã¨ã‚“ã©ã® GPU ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ã€‚`,pl,Xe,ko='é‡å­åŒ–æ–¹æ³•ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€<a href="https://arxiv.org/abs/2208.07339" rel="nofollow">LLM.int8()</a> è«–æ–‡ã€ã¾ãŸã¯ <a href="https://huggingface.co/blog/hf-bitsandbytes-" rel="nofollow">ãƒ–ãƒ­ã‚°æŠ•ç¨¿</a> ã‚’ã”è¦§ãã ã•ã„ã€‚çµ±åˆï¼‰ã‚³ãƒ©ãƒœãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¤ã„ã¦ã€‚',fl,Ve,xo="<code>0.39.0</code>ãƒªãƒªãƒ¼ã‚¹ä»¥é™ã€FP4 ãƒ‡ãƒ¼ã‚¿å‹ã‚’æ´»ç”¨ã—ã€4 ãƒ“ãƒƒãƒˆé‡å­åŒ–ã‚’ä½¿ç”¨ã—ã¦<code>device_map</code>ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚",cl,Le,qo='ç‹¬è‡ªã® pytorch ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–ã—ãŸã„å ´åˆã¯ã€ğŸ¤— Accelerate ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® <a href="https://huggingface.co/docs/accelerate/main/en/usage_guides/quantization" rel="nofollow">ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ</a> ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚',ul,Re,Uo="<code>bitsandbytes</code>çµ±åˆã‚’ä½¿ç”¨ã—ã¦ã§ãã‚‹ã“ã¨ã¯æ¬¡ã®ã¨ãŠã‚Šã§ã™",bl,Fe,gl,Ne,zo='ãƒ¢ãƒ‡ãƒ«ãŒ ğŸ¤— Accelerate ã«ã‚ˆã‚‹èª­ã¿è¾¼ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€<code>torch.nn.Linear</code> ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹é™ã‚Šã€ <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã¨ãã« <code>load_in_8bit</code> ã¾ãŸã¯ <code>load_in_4bit</code> å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’é‡å­åŒ–ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã©ã®ã‚ˆã†ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã§ã‚‚åŒæ§˜ã«æ©Ÿèƒ½ã™ã‚‹ã¯ãšã§ã™ã€‚',hl,He,_l,Pe,Zo="ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ä»–ã®ã™ã¹ã¦ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (ä¾‹: <code>torch.nn.LayerNorm</code>) ã¯ <code>torch.float16</code> ã«å¤‰æ›ã•ã‚Œã¾ã™ãŒã€ãã® <code>dtype</code> ã‚’å¤‰æ›´ã—ãŸã„å ´åˆã¯ã€<code>torch_dtype</code> å¼•æ•°ã‚’ä¸Šæ›¸ãã§ãã¾ã™ã€‚",yl,Ie,$l,Ae,Ml,Ee,Tl,Ye,Wo="ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ ã‚¹ãƒ‹ãƒšãƒƒãƒˆã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€ä»¥ä¸‹ã®è¦ä»¶ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",wl,Se,jo=`<li><p>æœ€æ–°ã®<code>bitsandbytes</code>ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
<code>pip install bitsandbytes&gt;=0.39.0</code></p></li> <li><p>æœ€æ–°ã®<code>accelerate</code>ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹
<code>pip install --upgrade accelerate</code></p></li> <li><p>æœ€æ–°ã® <code>transformers</code> ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹
<code>pip install --upgrade transformers</code></p></li>`,vl,De,Cl,Ke,Go='<li><p><strong>é«˜åº¦ãªä½¿ç”¨æ³•:</strong> å¯èƒ½ãªã™ã¹ã¦ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ãŸ 4 ãƒ“ãƒƒãƒˆé‡å­åŒ–ã®é«˜åº¦ãªä½¿ç”¨æ³•ã«ã¤ã„ã¦ã¯ã€<a href="https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf" rel="nofollow">ã“ã® Google Colab ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯</a> ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚</p></li> <li><p><strong><code>batch_size=1</code> ã«ã‚ˆã‚‹é«˜é€Ÿæ¨è«– :</strong> bitsandbytes ã® <code>0.40.0</code> ãƒªãƒªãƒ¼ã‚¹ä»¥é™ã€<code>batch_size=1</code> ã§ã¯é«˜é€Ÿæ¨è«–ã®æ©æµã‚’å—ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ <a href="https://github.com/TimDettmers/bitsandbytes/releases/tag/0.40.0" rel="nofollow">ã“ã‚Œã‚‰ã®ãƒªãƒªãƒ¼ã‚¹ ãƒãƒ¼ãƒˆ</a> ã‚’ç¢ºèªã—ã€ã“ã®æ©Ÿèƒ½ã‚’æ´»ç”¨ã™ã‚‹ã«ã¯<code>0.40.0</code>ä»¥é™ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ç®±ã®ã€‚</p></li> <li><p><strong>ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°:</strong> <a href="https://arxiv.org/abs/2305.14314" rel="nofollow">QLoRA è«–æ–‡</a> ã«ã‚ˆã‚‹ã¨ã€4 ãƒ“ãƒƒãƒˆåŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆ (ä¾‹: LoRA ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½¿ç”¨)ã€<code>bnb_4bit_quant_type=&#39;nf4&#39;</code> ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ ã€‚</p></li> <li><p><strong>æ¨è«–:</strong> æ¨è«–ã®å ´åˆã€<code>bnb_4bit_quant_type</code> ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å¤§ããªå½±éŸ¿ã‚’ä¸ãˆã¾ã›ã‚“ã€‚ãŸã ã—ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¨ã®ä¸€è²«æ€§ã‚’ä¿ã¤ãŸã‚ã«ã€å¿…ãšåŒã˜ <code>bnb_4bit_compute_dtype</code> ãŠã‚ˆã³ <code>torch_dtype</code> å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚</p></li>',Jl,Oe,kl,et,Qo="<code>.from_pretrained</code> ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã¨ãã« <code>load_in_4bit=True</code> ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ (ãŠãŠã‚ˆã) 4 ã§å‰²ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚",xl,tt,ql,j,Ul,nt,zl,lt,Bo="<code>.from_pretrained</code> ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™ã¨ãã« <code>load_in_8bit=True</code> å¼•æ•°ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’ãŠã‚ˆãåŠåˆ†ã«ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚",Zl,at,Wl,ot,Xo='æ¬¡ã«ã€é€šå¸¸ <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel">PreTrainedModel</a> ã‚’ä½¿ç”¨ã™ã‚‹ã®ã¨åŒã˜ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚',jl,st,Vo="<code>get_memory_footprint</code> ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ¡ãƒ¢ãƒª ãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã‚’ç¢ºèªã§ãã¾ã™ã€‚",Gl,it,Ql,rt,Lo="ã“ã®çµ±åˆã«ã‚ˆã‚Šã€å¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’å°ã•ãªãƒ‡ãƒã‚¤ã‚¹ã«ãƒ­ãƒ¼ãƒ‰ã—ã€å•é¡Œãªãå®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚",Bl,G,Xl,mt,Vl,dt,Ro="ã“ã“ã§ã¯ã€FP4 é‡å­åŒ–ã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œã§ãã‚‹ã„ãã¤ã‹ã®é«˜åº¦ãªä½¿ç”¨ä¾‹ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚",Ll,pt,Rl,ft,Fo="compute dtype ã¯ã€è¨ˆç®—ä¸­ã«ä½¿ç”¨ã•ã‚Œã‚‹ dtype ã‚’å¤‰æ›´ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ãŸã¨ãˆã°ã€éš ã—çŠ¶æ…‹ã¯<code>float32</code>ã«ã‚ã‚Šã¾ã™ãŒã€é«˜é€ŸåŒ–ã®ãŸã‚ã«è¨ˆç®—ã‚’ bf16 ã«è¨­å®šã§ãã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€compute dtype ã¯ <code>float32</code> ã«è¨­å®šã•ã‚Œã¾ã™ã€‚",Fl,ct,Nl,ut,Hl,bt,No="NF4 ãƒ‡ãƒ¼ã‚¿å‹ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€æ­£è¦åˆ†å¸ƒã‚’ä½¿ç”¨ã—ã¦åˆæœŸåŒ–ã•ã‚ŒãŸé‡ã¿ã«é©åˆã—ãŸæ–°ã—ã„ 4 ãƒ“ãƒƒãƒˆ ãƒ‡ãƒ¼ã‚¿å‹ã§ã™ã€‚ãã®å®Ÿè¡Œã®ãŸã‚ã«:",Pl,gt,Il,ht,Al,_t,Ho="ã¾ãŸã€ãƒã‚¹ãƒˆã•ã‚ŒãŸé‡å­åŒ–æ‰‹æ³•ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ãªãã€ã‚ˆã‚Šå¤šãã®ãƒ¡ãƒ¢ãƒªãŒç¯€ç´„ã•ã‚Œã¾ã™ã€‚çµŒé¨“çš„ãªè¦³å¯Ÿã‹ã‚‰ã€ã“ã‚Œã«ã‚ˆã‚Šã€NVIDIA-T4 16GB ä¸Šã§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· 1024ã€ãƒãƒƒãƒ ã‚µã‚¤ã‚º 1ã€å‹¾é…ç´¯ç©ã‚¹ãƒ†ãƒƒãƒ— 4 ã® llama-13b ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚",El,yt,Yl,$t,Sl,Mt,Po=`<code>push_to_hub</code>ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å˜ç´”ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æœ€åˆã«é‡å­åŒ–æ§‹æˆãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã€æ¬¡ã«é‡å­åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãŒãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚
ã“ã®æ©Ÿèƒ½ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã«ã¯ã€å¿…ãš <code>bitsandbytes&gt;0.37.2</code> ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ (ã“ã®è¨˜äº‹ã®åŸ·ç­†æ™‚ç‚¹ã§ã¯ã€<code>bitsandbytes==0.38.0.post1</code> ã§ãƒ†ã‚¹ãƒˆã—ã¾ã—ãŸ)ã€‚`,Dl,Tt,Kl,Q,Ol,wt,ea,vt,Io="<code>from_pretrained</code>ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒãƒ–ã‹ã‚‰é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚å±æ€§ <code>quantization_config</code> ãŒãƒ¢ãƒ‡ãƒ«è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã€ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸé‡ã¿ãŒé‡å­åŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚",ta,Ct,na,Jt,Ao=`ã“ã®å ´åˆã€å¼•æ•° <code>load_in_8bit=True</code> ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€<code>bitsandbytes</code> ã¨ <code>accelerate</code> ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€<code>device_map</code> ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ãŒã€åˆ©ç”¨å¯èƒ½ãªãƒªã‚½ãƒ¼ã‚¹ä¸Šã§ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã™ã‚‹ãŸã‚ã€æ¨è«–ã«ã¯ <code>device_map = &#39;auto&#39;</code> ã‚’è¨­å®šã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚`,la,kt,aa,xt,Eo="ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯ã€8 ãƒ“ãƒƒãƒˆ ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¨å®Ÿè¡Œä»¥å¤–ã«ä½•ãŒã§ãã‚‹ã‹ã‚’æ¢æ±‚ã—ãŸã„ä¸Šç´šãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å¯¾è±¡ã¨ã—ã¦ã„ã¾ã™ã€‚",oa,qt,sa,Ut,Yo="ã“ã®é«˜åº¦ãªä½¿ç”¨ä¾‹ã® 1 ã¤ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€<code>CPU</code>ã¨<code>GPU</code>ã®é–“ã§é‡ã¿ã‚’ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã§ãã‚‹ã“ã¨ã§ã™ã€‚ CPU ä¸Šã§ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã•ã‚Œã‚‹é‡ã¿ã¯ <strong>8 ãƒ“ãƒƒãƒˆã«å¤‰æ›ã•ã‚Œãªã„</strong>ãŸã‚ã€<code>float32</code>ã«ä¿æŒã•ã‚Œã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã®æ©Ÿèƒ½ã¯ã€éå¸¸ã«å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã‚’é©åˆã•ã›ã€ãã®ãƒ¢ãƒ‡ãƒ«ã‚’ GPU ã¨ CPU ã®é–“ã§ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã—ãŸã„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å¯¾è±¡ã¨ã—ã¦ã„ã¾ã™ã€‚",ia,zt,So='ã¾ãšã€<code>transformers</code> ã‹ã‚‰ <a href="/docs/transformers/main/ja/main_classes/quantization#transformers.BitsAndBytesConfig">BitsAndBytesConfig</a> ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€å±æ€§ <code>llm_int8_enable_fp32_cpu_offload</code> ã‚’ <code>True</code> ã«è¨­å®šã—ã¾ã™ã€‚',ra,Zt,ma,Wt,Do="<code>bigscience/bloom-1b7</code>ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€<code>lm_head</code>ã‚’é™¤ããƒ¢ãƒ‡ãƒ«å…¨ä½“ã«â€‹â€‹é©åˆã™ã‚‹ã®ã«ååˆ†ãª GPU RAM ãŒã‚ã‚‹ã¨ã—ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€æ¬¡ã®ã‚ˆã†ã«ã‚«ã‚¹ã‚¿ãƒ  device_map ã‚’ä½œæˆã—ã¾ã™ã€‚",da,jt,pa,Gt,Ko="ãã—ã¦ã€æ¬¡ã®ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚",fa,Qt,ca,Bt,Oo="ä»¥ä¸Šã§ã™ï¼ãƒ¢ãƒ‡ãƒ«ã‚’æ¥½ã—ã‚“ã§ãã ã•ã„ï¼",ua,Xt,ba,Vt,es=`<code>llm_int8_threshold</code> å¼•æ•°ã‚’æ“ä½œã—ã¦ã€å¤–ã‚Œå€¤ã®ã—ãã„å€¤ã‚’å¤‰æ›´ã§ãã¾ã™ã€‚ å¤–ã‚Œå€¤ ã¨ã¯ã€ç‰¹å®šã®ã—ãã„å€¤ã‚ˆã‚Šå¤§ãã„éš ã‚ŒãŸçŠ¶æ…‹ã®å€¤ã§ã™ã€‚
ã“ã‚Œã¯ã€<code>LLM.int8()</code>è«–æ–‡ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹å¤–ã‚Œå€¤æ¤œå‡ºã®å¤–ã‚Œå€¤ã—ãã„å€¤ã«å¯¾å¿œã—ã¾ã™ã€‚ã“ã®ã—ãã„å€¤ã‚’è¶…ãˆã‚‹éš ã—çŠ¶æ…‹ã®å€¤ã¯å¤–ã‚Œå€¤ã¨ã¿ãªã•ã‚Œã€ãã‚Œã‚‰ã®å€¤ã«å¯¾ã™ã‚‹æ“ä½œã¯ fp16 ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚é€šå¸¸ã€å€¤ã¯æ­£è¦åˆ†å¸ƒã—ã¾ã™ã€‚ã¤ã¾ã‚Šã€ã»ã¨ã‚“ã©ã®å€¤ã¯ [-3.5, 3.5] ã®ç¯„å›²å†…ã«ã‚ã‚Šã¾ã™ãŒã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã¯å¤§ããç•°ãªã‚‹åˆ†å¸ƒã‚’ç¤ºã™ä¾‹å¤–çš„ãªç³»çµ±çš„å¤–ã‚Œå€¤ãŒã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®å¤–ã‚Œå€¤ã¯ã€å¤šãã®å ´åˆ [-60, -6] ã¾ãŸã¯ [6, 60] ã®ç¯„å›²å†…ã«ã‚ã‚Šã¾ã™ã€‚ Int8 é‡å­åŒ–ã¯ã€å¤§ãã•ãŒ 5 ç¨‹åº¦ã¾ã§ã®å€¤ã§ã¯ã†ã¾ãæ©Ÿèƒ½ã—ã¾ã™ãŒã€ãã‚Œã‚’è¶…ãˆã‚‹ã¨ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤§å¹…ã«ä½ä¸‹ã—ã¾ã™ã€‚é©åˆ‡ãªãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã—ãã„å€¤ã¯ 6 ã§ã™ãŒã€ã‚ˆã‚Šä¸å®‰å®šãªãƒ¢ãƒ‡ãƒ« (å°è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã€å¾®èª¿æ•´) ã§ã¯ã€ã‚ˆã‚Šä½ã„ã—ãã„å€¤ãŒå¿…è¦ã«ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚
ã“ã®å¼•æ•°ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–é€Ÿåº¦ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è©¦ã—ã¦ã¿ã¦ã€ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«æœ€é©ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚`,ga,Lt,ha,Rt,_a,Ft,ts="ä¸€éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯ã€å®‰å®šæ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã« 8 ãƒ“ãƒƒãƒˆã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒãªã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚ãŸã¨ãˆã°ã€ã‚¸ãƒ¥ãƒ¼ã‚¯ãƒœãƒƒã‚¯ã‚¹ ãƒ¢ãƒ‡ãƒ«ã«ã¯ã€ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã„ãã¤ã‹ã® <code>lm_head</code> ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚ <code>llm_int8_skip_modules</code> ã§éŠã‚“ã§ã¿ã‚‹",ya,Nt,$a,Ht,Ma,Pt,ns=`Hugging Face ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®å…¬å¼ã‚µãƒãƒ¼ãƒˆã«ã‚ˆã‚Šã€8 ãƒ“ãƒƒãƒˆã§ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã§ãã¾ã™ã€‚
ã“ã‚Œã«ã‚ˆã‚Šã€å˜ä¸€ã® Google Colab ã§<code>flan-t5-large</code>ã‚„<code>facebook/opt-6.7b</code>ãªã©ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€<a href="https://github.com/huggingface/peft" rel="nofollow"><code>peft</code></a> ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã”è¦§ãã ã•ã„ã€‚`,Ta,It,ls="ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ãã« <code>device_map</code> ã‚’æ¸¡ã™å¿…è¦ãŒãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ãƒ¢ãƒ‡ãƒ«ãŒ GPU ã«è‡ªå‹•çš„ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚å¿…è¦ã«å¿œã˜ã¦ã€ãƒ‡ãƒã‚¤ã‚¹ ãƒãƒƒãƒ—ã‚’ç‰¹å®šã®ãƒ‡ãƒã‚¤ã‚¹ã«è¨­å®šã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ (ä¾‹: <code>cuda:0</code>ã€<code>0</code>ã€<code>torch.device(&#39;cuda:0&#39;)</code>)ã€‚ <code>device_map=auto</code>ã¯æ¨è«–ã®ã¿ã«ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚",wa,At,va,M,Et,Ra,mn,as=`This is a wrapper class about all possible attributes and features that you can play with a model that has been
loaded using <code>bitsandbytes</code>.`,Fa,dn,os="This replaces <code>load_in_8bit</code> or <code>load_in_4bit</code>therefore both options are mutually exclusive.",Na,pn,ss=`Currently only supports <code>LLM.int8()</code>, <code>FP4</code>, and <code>NF4</code> quantization. If more methods are added to <code>bitsandbytes</code>,
then more arguments will be added to this class.`,Ha,B,Yt,Pa,fn,is="Returns <code>True</code> if the model is quantizable, <code>False</code> otherwise.",Ia,X,St,Aa,cn,rs="Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.",Ea,V,Dt,Ya,un,ms=`This method returns the quantization method used for the model. If the model is not quantizable, it returns
<code>None</code>.`,Sa,L,Kt,Da,bn,ds=`Removes all attributes from config which correspond to the default config attributes for better readability and
serializes to a Python dictionary.`,Ca,Ot,Ja,en,ps='<code>optimum</code>ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹é‡å­åŒ–æ–¹æ³•ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€<a href="https://huggingface.co/docs/optimum/index" rel="nofollow">Optimum ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ</a> ã‚’å‚ç…§ã—ã€ã“ã‚Œã‚‰ãŒè‡ªåˆ†ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«é©ç”¨ã§ãã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚',ka,gn,xa;return R=new h({props:{title:"Quantize ğŸ¤— Transformers models",local:"quantize--transformers-models",headingTag:"h1"}}),F=new h({props:{title:"AutoGPTQ Integration",local:"autogptq-integration",headingTag:"h2"}}),I=new h({props:{title:"Requirements",local:"requirements",headingTag:"h3"}}),S=new h({props:{title:"Load and quantize a model",local:"load-and-quantize-a-model",headingTag:"h3"}}),O=new h({props:{title:"GPTQ Configuration",local:"gptq-configuration",headingTag:"h4"}}),te=new _({props:{code:"bW9kZWxfaWQlMjAlM0QlMjAlMjJmYWNlYm9vayUyRm9wdC0xMjVtJTIyJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBZ3B0cV9jb25maWclMjAlM0QlMjBHUFRRQ29uZmlnKGJpdHMlM0Q0JTJDJTIwZGF0YXNldCUyMCUzRCUyMCUyMmM0JTIyJTJDJTIwdG9rZW5pemVyJTNEdG9rZW5pemVyKQ==",highlighted:`model_id = <span class="hljs-string">&quot;facebook/opt-125m&quot;</span>
tokenizer = AutoTokenizer.from_pretrained(model_id)
gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>, dataset = <span class="hljs-string">&quot;c4&quot;</span>, tokenizer=tokenizer)`,wrap:!1}}),le=new _({props:{code:"ZGF0YXNldCUyMCUzRCUyMCU1QiUyMmF1dG8tZ3B0cSUyMGlzJTIwYW4lMjBlYXN5LXRvLXVzZSUyMG1vZGVsJTIwcXVhbnRpemF0aW9uJTIwbGlicmFyeSUyMHdpdGglMjB1c2VyLWZyaWVuZGx5JTIwYXBpcyUyQyUyMGJhc2VkJTIwb24lMjBHUFRRJTIwYWxnb3JpdGhtLiUyMiU1RCUwQXF1YW50aXphdGlvbiUyMCUzRCUyMEdQVFFDb25maWcoYml0cyUzRDQlMkMlMjBkYXRhc2V0JTIwJTNEJTIwZGF0YXNldCUyQyUyMHRva2VuaXplciUzRHRva2VuaXplcik=",highlighted:`dataset = [<span class="hljs-string">&quot;auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.&quot;</span>]
quantization = GPTQConfig(bits=<span class="hljs-number">4</span>, dataset = dataset, tokenizer=tokenizer)`,wrap:!1}}),ae=new h({props:{title:"Quantization",local:"quantization",headingTag:"h4"}}),se=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEZ3B0cV9jb25maWcp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=gptq_config)`,wrap:!1}}),me=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMHF1YW50aXphdGlvbl9jb25maWclM0RncHRxX2NvbmZpZyk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config=gptq_config)`,wrap:!1}}),U=new Wa({props:{warning:!0,$$slots:{default:[$s]},$$scope:{ctx:k}}}),pe=new h({props:{title:"Push quantized model to ğŸ¤— Hub",local:"push-quantized-model-to--hub",headingTag:"h3"}}),ce=new _({props:{code:"cXVhbnRpemVkX21vZGVsLnB1c2hfdG9faHViKCUyMm9wdC0xMjVtLWdwdHElMjIpJTBBdG9rZW5pemVyLnB1c2hfdG9faHViKCUyMm9wdC0xMjVtLWdwdHElMjIp",highlighted:`quantized_model.push_to_hub(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)
tokenizer.push_to_hub(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),be=new _({props:{code:"cXVhbnRpemVkX21vZGVsLnNhdmVfcHJldHJhaW5lZCglMjJvcHQtMTI1bS1ncHRxJTIyKSUwQXRva2VuaXplci5zYXZlX3ByZXRyYWluZWQoJTIyb3B0LTEyNW0tZ3B0cSUyMik=",highlighted:`quantized_model.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)
tokenizer.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),he=new _({props:{code:"cXVhbnRpemVkX21vZGVsLnRvKCUyMmNwdSUyMiklMEFxdWFudGl6ZWRfbW9kZWwuc2F2ZV9wcmV0cmFpbmVkKCUyMm9wdC0xMjVtLWdwdHElMjIp",highlighted:`quantized_model.to(<span class="hljs-string">&quot;cpu&quot;</span>)
quantized_model.save_pretrained(<span class="hljs-string">&quot;opt-125m-gptq&quot;</span>)`,wrap:!1}}),_e=new h({props:{title:"Load a quantized model from the ğŸ¤— Hub",local:"load-a-quantized-model-from-the--hub",headingTag:"h3"}}),$e=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyJTdCeW91cl91c2VybmFtZSU3RCUyRm9wdC0xMjVtLWdwdHElMjIp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>)`,wrap:!1}}),Te=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyJTdCeW91cl91c2VybmFtZSU3RCUyRm9wdC0xMjVtLWdwdHElMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),we=new h({props:{title:"Exllama kernels for faster inference",local:"exllama-kernels-for-faster-inference",headingTag:"h3"}}),Ce=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFncHRxX2NvbmZpZyUyMCUzRCUyMEdQVFFDb25maWcoYml0cyUzRDQlMkMlMjBkaXNhYmxlX2V4bGxhbWElM0RGYWxzZSklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjIlN0J5b3VyX3VzZXJuYW1lJTdEJTJGb3B0LTEyNW0tZ3B0cSUyMiUyQyUyMGRldmljZV9tYXAlM0QlMjJhdXRvJTIyJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMGdwdHFfY29uZmlnKQ==",highlighted:`<span class="hljs-keyword">import</span> torch
gptq_config = GPTQConfig(bits=<span class="hljs-number">4</span>, disable_exllama=<span class="hljs-literal">False</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/opt-125m-gptq&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, quantization_config = gptq_config)`,wrap:!1}}),ke=new h({props:{title:"Fine-tune a quantized model",local:"fine-tune-a-quantized-model",headingTag:"h4"}}),qe=new h({props:{title:"Example demo",local:"example-demo",headingTag:"h3"}}),ze=new h({props:{title:"GPTQConfig",local:"transformers.GPTQConfig",headingTag:"h3"}}),Ze=new x({props:{name:"class transformers.GPTQConfig",anchor:"transformers.GPTQConfig",parameters:[{name:"bits",val:": int"},{name:"tokenizer",val:": Any = None"},{name:"dataset",val:": Union = None"},{name:"group_size",val:": int = 128"},{name:"damp_percent",val:": float = 0.1"},{name:"desc_act",val:": bool = False"},{name:"sym",val:": bool = True"},{name:"true_sequential",val:": bool = True"},{name:"use_cuda_fp16",val:": bool = False"},{name:"model_seqlen",val:": Optional = None"},{name:"block_name_to_quantize",val:": Optional = None"},{name:"module_name_preceding_first_block",val:": Optional = None"},{name:"batch_size",val:": int = 1"},{name:"pad_token_id",val:": Optional = None"},{name:"use_exllama",val:": Optional = None"},{name:"max_input_length",val:": Optional = None"},{name:"exllama_config",val:": Optional = None"},{name:"cache_block_outputs",val:": bool = True"},{name:"modules_in_block_to_quantize",val:": Optional = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.GPTQConfig.bits",description:`<strong>bits</strong> (<code>int</code>) &#x2014;
The number of bits to quantize to, supported numbers are (2, 3, 4, 8).`,name:"bits"},{anchor:"transformers.GPTQConfig.tokenizer",description:`<strong>tokenizer</strong> (<code>str</code> or <code>PreTrainedTokenizerBase</code>, <em>optional</em>) &#x2014;
The tokenizer used to process the dataset. You can pass either:<ul>
<li>A custom tokenizer object.</li>
<li>A string, the <em>model id</em> of a predefined tokenizer hosted inside a model repo on huggingface.co.</li>
<li>A path to a <em>directory</em> containing vocabulary files required by the tokenizer, for instance saved
using the <a href="/docs/transformers/main/ja/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.save_pretrained">save_pretrained()</a> method, e.g., <code>./my_model_directory/</code>.</li>
</ul>`,name:"tokenizer"},{anchor:"transformers.GPTQConfig.dataset",description:`<strong>dataset</strong> (<code>Union[List[str]]</code>, <em>optional</em>) &#x2014;
The dataset used for quantization. You can provide your own dataset in a list of string or just use the
original datasets used in GPTQ paper [&#x2018;wikitext2&#x2019;,&#x2018;c4&#x2019;,&#x2018;c4-new&#x2019;,&#x2018;ptb&#x2019;,&#x2018;ptb-new&#x2019;]`,name:"dataset"},{anchor:"transformers.GPTQConfig.group_size",description:`<strong>group_size</strong> (<code>int</code>, <em>optional</em>, defaults to 128) &#x2014;
The group size to use for quantization. Recommended value is 128 and -1 uses per-column quantization.`,name:"group_size"},{anchor:"transformers.GPTQConfig.damp_percent",description:`<strong>damp_percent</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The percent of the average Hessian diagonal to use for dampening. Recommended value is 0.1.`,name:"damp_percent"},{anchor:"transformers.GPTQConfig.desc_act",description:`<strong>desc_act</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether to quantize columns in order of decreasing activation size. Setting it to False can significantly
speed up inference but the perplexity may become slightly worse. Also known as act-order.`,name:"desc_act"},{anchor:"transformers.GPTQConfig.sym",description:`<strong>sym</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to use symetric quantization.`,name:"sym"},{anchor:"transformers.GPTQConfig.true_sequential",description:`<strong>true_sequential</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to perform sequential quantization even within a single Transformer block. Instead of quantizing
the entire block at once, we perform layer-wise quantization. As a result, each layer undergoes
quantization using inputs that have passed through the previously quantized layers.`,name:"true_sequential"},{anchor:"transformers.GPTQConfig.use_cuda_fp16",description:`<strong>use_cuda_fp16</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
Whether or not to use optimized cuda kernel for fp16 model. Need to have model in fp16.`,name:"use_cuda_fp16"},{anchor:"transformers.GPTQConfig.model_seqlen",description:`<strong>model_seqlen</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum sequence length that the model can take.`,name:"model_seqlen"},{anchor:"transformers.GPTQConfig.block_name_to_quantize",description:`<strong>block_name_to_quantize</strong> (<code>str</code>, <em>optional</em>) &#x2014;
The transformers block name to quantize. If None, we will infer the block name using common patterns (e.g. model.layers)`,name:"block_name_to_quantize"},{anchor:"transformers.GPTQConfig.module_name_preceding_first_block",description:`<strong>module_name_preceding_first_block</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
The layers that are preceding the first Transformer block.`,name:"module_name_preceding_first_block"},{anchor:"transformers.GPTQConfig.batch_size",description:`<strong>batch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The batch size used when processing the dataset`,name:"batch_size"},{anchor:"transformers.GPTQConfig.pad_token_id",description:`<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The pad token id. Needed to prepare the dataset when <code>batch_size</code> &gt; 1.`,name:"pad_token_id"},{anchor:"transformers.GPTQConfig.use_exllama",description:`<strong>use_exllama</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether to use exllama backend. Defaults to <code>True</code> if unset. Only works with <code>bits</code> = 4.`,name:"use_exllama"},{anchor:"transformers.GPTQConfig.max_input_length",description:`<strong>max_input_length</strong> (<code>int</code>, <em>optional</em>) &#x2014;
The maximum input length. This is needed to initialize a buffer that depends on the maximum expected input
length. It is specific to the exllama backend with act-order.`,name:"max_input_length"},{anchor:"transformers.GPTQConfig.exllama_config",description:`<strong>exllama_config</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
The exllama config. You can specify the version of the exllama kernel through the <code>version</code> key. Defaults
to <code>{&quot;version&quot;: 1}</code> if unset.`,name:"exllama_config"},{anchor:"transformers.GPTQConfig.cache_block_outputs",description:`<strong>cache_block_outputs</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to cache block outputs to reuse as inputs for the succeeding block.`,name:"cache_block_outputs"},{anchor:"transformers.GPTQConfig.modules_in_block_to_quantize",description:`<strong>modules_in_block_to_quantize</strong> (<code>List[List[str]]</code>, <em>optional</em>) &#x2014;
List of list of module names to quantize in the specified block. This argument is useful to exclude certain linear modules from being quantized.
The block to quantize can be specified by setting <code>block_name_to_quantize</code>. We will quantize each list sequentially. If not set, we will quantize all linear layers.
Example: <code>modules_in_block_to_quantize =[[&quot;self_attn.k_proj&quot;, &quot;self_attn.v_proj&quot;, &quot;self_attn.q_proj&quot;], [&quot;self_attn.o_proj&quot;]]</code>.
In this example, we will first quantize the q,k,v layers simultaneously since they are independent.
Then, we will quantize <code>self_attn.o_proj</code> layer with the q,k,v layers quantized. This way, we will get
better results since it reflects the real input <code>self_attn.o_proj</code> will get when the model is quantized.`,name:"modules_in_block_to_quantize"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L383"}}),We=new x({props:{name:"from_dict_optimum",anchor:"transformers.GPTQConfig.from_dict_optimum",parameters:[{name:"config_dict",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L580"}}),je=new x({props:{name:"post_init",anchor:"transformers.GPTQConfig.post_init",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L497"}}),Ge=new x({props:{name:"to_dict_optimum",anchor:"transformers.GPTQConfig.to_dict_optimum",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L571"}}),Qe=new h({props:{title:"bitsandbytes Integration",local:"bitsandbytes-integration",headingTag:"h2"}}),Fe=new h({props:{title:"General usage",local:"general-usage",headingTag:"h3"}}),He=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm9wdC0zNTBtJTIyJTJDJTIwbG9hZF9pbl84Yml0JTNEVHJ1ZSklMEFtb2RlbF80Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGb3B0LTM1MG0lMjIlMkMlMjBsb2FkX2luXzRiaXQlM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model_8bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)
model_4bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),Ie=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMEElMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGb3B0LTM1MG0lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlJTJDJTIwdG9yY2hfZHR5cGUlM0R0b3JjaC5mbG9hdDMyKSUwQW1vZGVsXzhiaXQubW9kZWwuZGVjb2Rlci5sYXllcnMlNUItMSU1RC5maW5hbF9sYXllcl9ub3JtLndlaWdodC5kdHlwZQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

<span class="hljs-meta">&gt;&gt;&gt; </span>model_8bit = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>, torch_dtype=torch.float32)
<span class="hljs-meta">&gt;&gt;&gt; </span>model_8bit.model.decoder.layers[-<span class="hljs-number">1</span>].final_layer_norm.weight.dtype
torch.float32`,wrap:!1}}),Ae=new h({props:{title:"FP4 quantization",local:"fp4-quantization",headingTag:"h3"}}),Ee=new h({props:{title:"Requirements",local:"requirements",headingTag:"h4"}}),De=new h({props:{title:"Tips and best practices",local:"tips-and-best-practices",headingTag:"h4"}}),Oe=new h({props:{title:"Load a large model in 4bit",local:"load-a-large-model-in-4bit",headingTag:"h4"}}),tt=new _({props:{code:"JTIzJTIwcGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGFjY2VsZXJhdGUlMjBiaXRzYW5kYnl0ZXMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fNGJpdCUzRFRydWUp",highlighted:`<span class="hljs-comment"># pip install transformers accelerate bitsandbytes</span>
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_4bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),j=new Wa({props:{warning:!0,$$slots:{default:[Ms]},$$scope:{ctx:k}}}),nt=new h({props:{title:"Load a large model in 8bit",local:"load-a-large-model-in-8bit",headingTag:"h3"}}),at=new _({props:{code:"JTIzJTIwcGlwJTIwaW5zdGFsbCUyMHRyYW5zZm9ybWVycyUyMGFjY2VsZXJhdGUlMjBiaXRzYW5kYnl0ZXMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBbW9kZWxfaWQlMjAlM0QlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQpJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMiUyQyUyMGxvYWRfaW5fOGJpdCUzRFRydWUp",highlighted:`<span class="hljs-comment"># pip install transformers accelerate bitsandbytes</span>
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)`,wrap:!1}}),it=new _({props:{code:"cHJpbnQobW9kZWwuZ2V0X21lbW9yeV9mb290cHJpbnQoKSk=",highlighted:'<span class="hljs-built_in">print</span>(model.get_memory_footprint())',wrap:!1}}),G=new Wa({props:{warning:!0,$$slots:{default:[Ts]},$$scope:{ctx:k}}}),mt=new h({props:{title:"Advanced use cases",local:"advanced-use-cases",headingTag:"h4"}}),pt=new h({props:{title:"Change the compute dtype",local:"change-the-compute-dtype",headingTag:"h5"}}),ct=new _({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyhsb2FkX2luXzRiaXQlM0RUcnVlJTJDJTIwYm5iXzRiaXRfY29tcHV0ZV9kdHlwZSUzRHRvcmNoLmJmbG9hdDE2KQ==",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(load_in_4bit=<span class="hljs-literal">True</span>, bnb_4bit_compute_dtype=torch.bfloat16)`,wrap:!1}}),ut=new h({props:{title:"Using NF4 (Normal Float 4) data type",local:"using-nf4-normal-float-4-data-type",headingTag:"h5"}}),gt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW5mNF9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbG9hZF9pbl80Yml0JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGJuYl80Yml0X3F1YW50X3R5cGUlM0QlMjJuZjQlMjIlMkMlMEEpJTBBJTBBbW9kZWxfbmY0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKG1vZGVsX2lkJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRG5mNF9jb25maWcp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

nf4_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_quant_type=<span class="hljs-string">&quot;nf4&quot;</span>,
)

model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)`,wrap:!1}}),ht=new h({props:{title:"Use nested quantization for more memory efficient inference",local:"use-nested-quantization-for-more-memory-efficient-inference",headingTag:"h5"}}),yt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQWRvdWJsZV9xdWFudF9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbG9hZF9pbl80Yml0JTNEVHJ1ZSUyQyUwQSUyMCUyMCUyMCUyMGJuYl80Yml0X3VzZV9kb3VibGVfcXVhbnQlM0RUcnVlJTJDJTBBKSUwQSUwQW1vZGVsX2RvdWJsZV9xdWFudCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCUyQyUyMHF1YW50aXphdGlvbl9jb25maWclM0Rkb3VibGVfcXVhbnRfY29uZmlnKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig

double_quant_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_use_double_quant=<span class="hljs-literal">True</span>,
)

model_double_quant = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=double_quant_config)`,wrap:!1}}),$t=new h({props:{title:"Push quantized models on the ğŸ¤— Hub",local:"push-quantized-models-on-the--hub",headingTag:"h3"}}),Tt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMmJpZ3NjaWVuY2UlMkZibG9vbS01NjBtJTIyJTJDJTIwZGV2aWNlX21hcCUzRCUyMmF1dG8lMjIlMkMlMjBsb2FkX2luXzhiaXQlM0RUcnVlKSUwQXRva2VuaXplciUyMCUzRCUyMEF1dG9Ub2tlbml6ZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmJpZ3NjaWVuY2UlMkZibG9vbS01NjBtJTIyKSUwQSUwQW1vZGVsLnB1c2hfdG9faHViKCUyMmJsb29tLTU2MG0tOGJpdCUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom-560m&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>, load_in_8bit=<span class="hljs-literal">True</span>)
tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;bigscience/bloom-560m&quot;</span>)

model.push_to_hub(<span class="hljs-string">&quot;bloom-560m-8bit&quot;</span>)`,wrap:!1}}),Q=new Wa({props:{warning:!0,$$slots:{default:[ws]},$$scope:{ctx:k}}}),wt=new h({props:{title:"Load a quantized model from the ğŸ¤— Hub",local:"load-a-quantized-model-from-the--hub",headingTag:"h3"}}),Ct=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUwQSUwQW1vZGVsJTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUyMiU3QnlvdXJfdXNlcm5hbWUlN0QlMkZibG9vbS01NjBtLThiaXQlMjIlMkMlMjBkZXZpY2VfbWFwJTNEJTIyYXV0byUyMik=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;{your_username}/bloom-560m-8bit&quot;</span>, device_map=<span class="hljs-string">&quot;auto&quot;</span>)`,wrap:!1}}),kt=new h({props:{title:"Advanced use cases",local:"advanced-use-cases",headingTag:"h3"}}),qt=new h({props:{title:"Offload between cpu and gpu",local:"offload-between-cpu-and-gpu",headingTag:"h4"}}),Zt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcobGxtX2ludDhfZW5hYmxlX2ZwMzJfY3B1X29mZmxvYWQlM0RUcnVlKQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=<span class="hljs-literal">True</span>)`,wrap:!1}}),jt=new _({props:{code:"ZGV2aWNlX21hcCUyMCUzRCUyMCU3QiUwQSUyMCUyMCUyMCUyMCUyMnRyYW5zZm9ybWVyLndvcmRfZW1iZWRkaW5ncyUyMiUzQSUyMDAlMkMlMEElMjAlMjAlMjAlMjAlMjJ0cmFuc2Zvcm1lci53b3JkX2VtYmVkZGluZ3NfbGF5ZXJub3JtJTIyJTNBJTIwMCUyQyUwQSUyMCUyMCUyMCUyMCUyMmxtX2hlYWQlMjIlM0ElMjAlMjJjcHUlMjIlMkMlMEElMjAlMjAlMjAlMjAlMjJ0cmFuc2Zvcm1lci5oJTIyJTNBJTIwMCUyQyUwQSUyMCUyMCUyMCUyMCUyMnRyYW5zZm9ybWVyLmxuX2YlMjIlM0ElMjAwJTJDJTBBJTdE",highlighted:`device_map = {
    <span class="hljs-string">&quot;transformer.word_embeddings&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;transformer.word_embeddings_layernorm&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;lm_head&quot;</span>: <span class="hljs-string">&quot;cpu&quot;</span>,
    <span class="hljs-string">&quot;transformer.h&quot;</span>: <span class="hljs-number">0</span>,
    <span class="hljs-string">&quot;transformer.ln_f&quot;</span>: <span class="hljs-number">0</span>,
}`,wrap:!1}}),Qt=new _({props:{code:"bW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjAlMjJiaWdzY2llbmNlJTJGYmxvb20tMWI3JTIyJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRGRldmljZV9tYXAlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUyQyUwQSk=",highlighted:`model_8bit = AutoModelForCausalLM.from_pretrained(
    <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>,
    device_map=device_map,
    quantization_config=quantization_config,
)`,wrap:!1}}),Xt=new h({props:{title:"Play with llm_int8_threshold",local:"play-with-llmint8threshold",headingTag:"h4"}}),Lt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyYmlnc2NpZW5jZSUyRmJsb29tLTFiNyUyMiUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbGxtX2ludDhfdGhyZXNob2xkJTNEMTAlMkMlMEEpJTBBJTBBbW9kZWxfOGJpdCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMEElMjAlMjAlMjAlMjBtb2RlbF9pZCUyQyUwQSUyMCUyMCUyMCUyMGRldmljZV9tYXAlM0RkZXZpY2VfbWFwJTJDJTBBJTIwJTIwJTIwJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWclMkMlMEEpJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQobW9kZWxfaWQp",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

quantization_config = BitsAndBytesConfig(
    llm_int8_threshold=<span class="hljs-number">10</span>,
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)`,wrap:!1}}),Rt=new h({props:{title:"Skip the conversion of some modules",local:"skip-the-conversion-of-some-modules",headingTag:"h4"}}),Nt=new _({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTJDJTIwQXV0b1Rva2VuaXplciUyQyUyMEJpdHNBbmRCeXRlc0NvbmZpZyUwQSUwQW1vZGVsX2lkJTIwJTNEJTIwJTIyYmlnc2NpZW5jZSUyRmJsb29tLTFiNyUyMiUwQSUwQXF1YW50aXphdGlvbl9jb25maWclMjAlM0QlMjBCaXRzQW5kQnl0ZXNDb25maWcoJTBBJTIwJTIwJTIwJTIwbGxtX2ludDhfc2tpcF9tb2R1bGVzJTNEJTVCJTIybG1faGVhZCUyMiU1RCUyQyUwQSklMEElMEFtb2RlbF84Yml0JTIwJTNEJTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0uZnJvbV9wcmV0cmFpbmVkKCUwQSUyMCUyMCUyMCUyMG1vZGVsX2lkJTJDJTBBJTIwJTIwJTIwJTIwZGV2aWNlX21hcCUzRGRldmljZV9tYXAlMkMlMEElMjAlMjAlMjAlMjBxdWFudGl6YXRpb25fY29uZmlnJTNEcXVhbnRpemF0aW9uX2NvbmZpZyUyQyUwQSklMEF0b2tlbml6ZXIlMjAlM0QlMjBBdXRvVG9rZW5pemVyLmZyb21fcHJldHJhaW5lZChtb2RlbF9pZCk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_id = <span class="hljs-string">&quot;bigscience/bloom-1b7&quot;</span>

quantization_config = BitsAndBytesConfig(
    llm_int8_skip_modules=[<span class="hljs-string">&quot;lm_head&quot;</span>],
)

model_8bit = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=device_map,
    quantization_config=quantization_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)`,wrap:!1}}),Ht=new h({props:{title:"Fine-tune a model that has been loaded in 8-bit",local:"fine-tune-a-model-that-has-been-loaded-in-8-bit",headingTag:"h4"}}),At=new h({props:{title:"BitsAndBytesConfig",local:"transformers.BitsAndBytesConfig",headingTag:"h3"}}),Et=new x({props:{name:"class transformers.BitsAndBytesConfig",anchor:"transformers.BitsAndBytesConfig",parameters:[{name:"load_in_8bit",val:" = False"},{name:"load_in_4bit",val:" = False"},{name:"llm_int8_threshold",val:" = 6.0"},{name:"llm_int8_skip_modules",val:" = None"},{name:"llm_int8_enable_fp32_cpu_offload",val:" = False"},{name:"llm_int8_has_fp16_weight",val:" = False"},{name:"bnb_4bit_compute_dtype",val:" = None"},{name:"bnb_4bit_quant_type",val:" = 'fp4'"},{name:"bnb_4bit_use_double_quant",val:" = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.BitsAndBytesConfig.load_in_8bit",description:`<strong>load_in_8bit</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used to enable 8-bit quantization with LLM.int8().`,name:"load_in_8bit"},{anchor:"transformers.BitsAndBytesConfig.load_in_4bit",description:`<strong>load_in_4bit</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4/NF4 layers from
<code>bitsandbytes</code>.`,name:"load_in_4bit"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_threshold",description:`<strong>llm_int8_threshold</strong> (<code>float</code>, <em>optional</em>, defaults to 6.0) &#x2014;
This corresponds to the outlier threshold for outlier detection as described in <code>LLM.int8() : 8-bit Matrix Multiplication for Transformers at Scale</code> paper: <a href="https://arxiv.org/abs/2208.07339" rel="nofollow">https://arxiv.org/abs/2208.07339</a> Any hidden states value
that is above this threshold will be considered an outlier and the operation on those values will be done
in fp16. Values are usually normally distributed, that is, most values are in the range [-3.5, 3.5], but
there are some exceptional systematic outliers that are very differently distributed for large models.
These outliers are often in the interval [-60, -6] or [6, 60]. Int8 quantization works well for values of
magnitude ~5, but beyond that, there is a significant performance penalty. A good default threshold is 6,
but a lower threshold might be needed for more unstable models (small models, fine-tuning).`,name:"llm_int8_threshold"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_skip_modules",description:`<strong>llm_int8_skip_modules</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
An explicit list of the modules that we do not want to convert in 8-bit. This is useful for models such as
Jukebox that has several heads in different places and not necessarily at the last position. For example
for <code>CausalLM</code> models, the last <code>lm_head</code> is kept in its original <code>dtype</code>.`,name:"llm_int8_skip_modules"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_enable_fp32_cpu_offload",description:`<strong>llm_int8_enable_fp32_cpu_offload</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used for advanced use cases and users that are aware of this feature. If you want to split
your model in different parts and run some parts in int8 on GPU and some parts in fp32 on CPU, you can use
this flag. This is useful for offloading large models such as <code>google/flan-t5-xxl</code>. Note that the int8
operations will not be run on CPU.`,name:"llm_int8_enable_fp32_cpu_offload"},{anchor:"transformers.BitsAndBytesConfig.llm_int8_has_fp16_weight",description:`<strong>llm_int8_has_fp16_weight</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag runs LLM.int8() with 16-bit main weights. This is useful for fine-tuning as the weights do not
have to be converted back and forth for the backward pass.`,name:"llm_int8_has_fp16_weight"},{anchor:"transformers.BitsAndBytesConfig.bnb_4bit_compute_dtype",description:`<strong>bnb_4bit_compute_dtype</strong> (<code>torch.dtype</code> or str, <em>optional</em>, defaults to <code>torch.float32</code>) &#x2014;
This sets the computational type which might be different than the input time. For example, inputs might be
fp32, but computation can be set to bf16 for speedups.`,name:"bnb_4bit_compute_dtype"},{anchor:"transformers.BitsAndBytesConfig.bnb_4bit_quant_type",description:`<strong>bnb_4bit_quant_type</strong> (<code>str</code>,  <em>optional</em>, defaults to <code>&quot;fp4&quot;</code>) &#x2014;
This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types
which are specified by <code>fp4</code> or <code>nf4</code>.`,name:"bnb_4bit_quant_type"},{anchor:"transformers.BitsAndBytesConfig.bnb_4bit_use_double_quant",description:`<strong>bnb_4bit_use_double_quant</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>False</code>) &#x2014;
This flag is used for nested quantization where the quantization constants from the first quantization are
quantized again.`,name:"bnb_4bit_use_double_quant"},{anchor:"transformers.BitsAndBytesConfig.kwargs",description:`<strong>kwargs</strong> (<code>Dict[str, Any]</code>, <em>optional</em>) &#x2014;
Additional parameters from which to initialize the configuration object.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L179"}}),Yt=new x({props:{name:"is_quantizable",anchor:"transformers.BitsAndBytesConfig.is_quantizable",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L319"}}),St=new x({props:{name:"post_init",anchor:"transformers.BitsAndBytesConfig.post_init",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L288"}}),Dt=new x({props:{name:"quantization_method",anchor:"transformers.BitsAndBytesConfig.quantization_method",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L325"}}),Kt=new x({props:{name:"to_diff_dict",anchor:"transformers.BitsAndBytesConfig.to_diff_dict",parameters:[],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/utils/quantization_config.py#L355",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>Dictionary of all the attributes that make up this configuration instance,</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>Dict[str, Any]</code></p>
`}}),Ot=new h({props:{title:"Quantization with ğŸ¤— optimum",local:"quantization-with--optimum",headingTag:"h2"}}),{c(){g=s("meta"),y=a(),$=s("p"),q=a(),r(R.$$.fragment),hn=a(),r(F.$$.fragment),_n=a(),N=s("p"),N.innerHTML=Ka,yn=a(),H=s("p"),H.textContent=Oa,$n=a(),P=s("ul"),P.innerHTML=eo,Mn=a(),r(I.$$.fragment),Tn=a(),A=s("p"),A.textContent=to,wn=a(),E=s("ul"),E.innerHTML=no,vn=a(),Y=s("p"),Y.textContent=lo,Cn=a(),r(S.$$.fragment),Jn=a(),D=s("p"),D.innerHTML=ao,kn=a(),K=s("p"),K.textContent=oo,xn=a(),r(O.$$.fragment),qn=a(),ee=s("p"),ee.innerHTML=so,Un=a(),r(te.$$.fragment),zn=a(),ne=s("p"),ne.textContent=io,Zn=a(),r(le.$$.fragment),Wn=a(),r(ae.$$.fragment),jn=a(),oe=s("p"),oe.innerHTML=ro,Gn=a(),r(se.$$.fragment),Qn=a(),ie=s("p"),ie.textContent=mo,Bn=a(),re=s("p"),re.innerHTML=po,Xn=a(),r(me.$$.fragment),Vn=a(),de=s("p"),de.innerHTML=fo,Ln=a(),r(U.$$.fragment),Rn=a(),r(pe.$$.fragment),Fn=a(),fe=s("p"),fe.innerHTML=co,Nn=a(),r(ce.$$.fragment),Hn=a(),ue=s("p"),ue.innerHTML=uo,Pn=a(),r(be.$$.fragment),In=a(),ge=s("p"),ge.innerHTML=bo,An=a(),r(he.$$.fragment),En=a(),r(_e.$$.fragment),Yn=a(),ye=s("p"),ye.innerHTML=go,Sn=a(),r($e.$$.fragment),Dn=a(),Me=s("p"),Me.innerHTML=ho,Kn=a(),r(Te.$$.fragment),On=a(),r(we.$$.fragment),el=a(),ve=s("p"),ve.innerHTML=_o,tl=a(),r(Ce.$$.fragment),nl=a(),Je=s("p"),Je.textContent=yo,ll=a(),r(ke.$$.fragment),al=a(),xe=s("p"),xe.innerHTML=$o,ol=a(),r(qe.$$.fragment),sl=a(),Ue=s("p"),Ue.innerHTML=Mo,il=a(),r(ze.$$.fragment),rl=a(),w=s("div"),r(Ze.$$.fragment),ja=a(),an=s("p"),an.innerHTML=To,Ga=a(),z=s("div"),r(We.$$.fragment),Qa=a(),on=s("p"),on.textContent=wo,Ba=a(),Z=s("div"),r(je.$$.fragment),Xa=a(),sn=s("p"),sn.textContent=vo,Va=a(),W=s("div"),r(Ge.$$.fragment),La=a(),rn=s("p"),rn.textContent=Co,ml=a(),r(Qe.$$.fragment),dl=a(),Be=s("p"),Be.innerHTML=Jo,pl=a(),Xe=s("p"),Xe.innerHTML=ko,fl=a(),Ve=s("p"),Ve.innerHTML=xo,cl=a(),Le=s("p"),Le.innerHTML=qo,ul=a(),Re=s("p"),Re.innerHTML=Uo,bl=a(),r(Fe.$$.fragment),gl=a(),Ne=s("p"),Ne.innerHTML=zo,hl=a(),r(He.$$.fragment),_l=a(),Pe=s("p"),Pe.innerHTML=Zo,yl=a(),r(Ie.$$.fragment),$l=a(),r(Ae.$$.fragment),Ml=a(),r(Ee.$$.fragment),Tl=a(),Ye=s("p"),Ye.textContent=Wo,wl=a(),Se=s("ul"),Se.innerHTML=jo,vl=a(),r(De.$$.fragment),Cl=a(),Ke=s("ul"),Ke.innerHTML=Go,Jl=a(),r(Oe.$$.fragment),kl=a(),et=s("p"),et.innerHTML=Qo,xl=a(),r(tt.$$.fragment),ql=a(),r(j.$$.fragment),Ul=a(),r(nt.$$.fragment),zl=a(),lt=s("p"),lt.innerHTML=Bo,Zl=a(),r(at.$$.fragment),Wl=a(),ot=s("p"),ot.innerHTML=Xo,jl=a(),st=s("p"),st.innerHTML=Vo,Gl=a(),r(it.$$.fragment),Ql=a(),rt=s("p"),rt.textContent=Lo,Bl=a(),r(G.$$.fragment),Xl=a(),r(mt.$$.fragment),Vl=a(),dt=s("p"),dt.textContent=Ro,Ll=a(),r(pt.$$.fragment),Rl=a(),ft=s("p"),ft.innerHTML=Fo,Fl=a(),r(ct.$$.fragment),Nl=a(),r(ut.$$.fragment),Hl=a(),bt=s("p"),bt.textContent=No,Pl=a(),r(gt.$$.fragment),Il=a(),r(ht.$$.fragment),Al=a(),_t=s("p"),_t.textContent=Ho,El=a(),r(yt.$$.fragment),Yl=a(),r($t.$$.fragment),Sl=a(),Mt=s("p"),Mt.innerHTML=Po,Dl=a(),r(Tt.$$.fragment),Kl=a(),r(Q.$$.fragment),Ol=a(),r(wt.$$.fragment),ea=a(),vt=s("p"),vt.innerHTML=Io,ta=a(),r(Ct.$$.fragment),na=a(),Jt=s("p"),Jt.innerHTML=Ao,la=a(),r(kt.$$.fragment),aa=a(),xt=s("p"),xt.textContent=Eo,oa=a(),r(qt.$$.fragment),sa=a(),Ut=s("p"),Ut.innerHTML=Yo,ia=a(),zt=s("p"),zt.innerHTML=So,ra=a(),r(Zt.$$.fragment),ma=a(),Wt=s("p"),Wt.innerHTML=Do,da=a(),r(jt.$$.fragment),pa=a(),Gt=s("p"),Gt.textContent=Ko,fa=a(),r(Qt.$$.fragment),ca=a(),Bt=s("p"),Bt.textContent=Oo,ua=a(),r(Xt.$$.fragment),ba=a(),Vt=s("p"),Vt.innerHTML=es,ga=a(),r(Lt.$$.fragment),ha=a(),r(Rt.$$.fragment),_a=a(),Ft=s("p"),Ft.innerHTML=ts,ya=a(),r(Nt.$$.fragment),$a=a(),r(Ht.$$.fragment),Ma=a(),Pt=s("p"),Pt.innerHTML=ns,Ta=a(),It=s("p"),It.innerHTML=ls,wa=a(),r(At.$$.fragment),va=a(),M=s("div"),r(Et.$$.fragment),Ra=a(),mn=s("p"),mn.innerHTML=as,Fa=a(),dn=s("p"),dn.innerHTML=os,Na=a(),pn=s("p"),pn.innerHTML=ss,Ha=a(),B=s("div"),r(Yt.$$.fragment),Pa=a(),fn=s("p"),fn.innerHTML=is,Ia=a(),X=s("div"),r(St.$$.fragment),Aa=a(),cn=s("p"),cn.textContent=rs,Ea=a(),V=s("div"),r(Dt.$$.fragment),Ya=a(),un=s("p"),un.innerHTML=ms,Sa=a(),L=s("div"),r(Kt.$$.fragment),Da=a(),bn=s("p"),bn.textContent=ds,Ca=a(),r(Ot.$$.fragment),Ja=a(),en=s("p"),en.innerHTML=ps,ka=a(),gn=s("p"),this.h()},l(e){const t=ys("svelte-u9bgzb",document.head);g=i(t,"META",{name:!0,content:!0}),t.forEach(n),y=o(e),$=i(e,"P",{}),C($).forEach(n),q=o(e),m(R.$$.fragment,e),hn=o(e),m(F.$$.fragment,e),_n=o(e),N=i(e,"P",{"data-svelte-h":!0}),u(N)!=="svelte-1ag92u2"&&(N.innerHTML=Ka),yn=o(e),H=i(e,"P",{"data-svelte-h":!0}),u(H)!=="svelte-1gbdty9"&&(H.textContent=Oa),$n=o(e),P=i(e,"UL",{"data-svelte-h":!0}),u(P)!=="svelte-z5jal9"&&(P.innerHTML=eo),Mn=o(e),m(I.$$.fragment,e),Tn=o(e),A=i(e,"P",{"data-svelte-h":!0}),u(A)!=="svelte-1l7cenj"&&(A.textContent=to),wn=o(e),E=i(e,"UL",{"data-svelte-h":!0}),u(E)!=="svelte-hwx8is"&&(E.innerHTML=no),vn=o(e),Y=i(e,"P",{"data-svelte-h":!0}),u(Y)!=="svelte-mtpiza"&&(Y.textContent=lo),Cn=o(e),m(S.$$.fragment,e),Jn=o(e),D=i(e,"P",{"data-svelte-h":!0}),u(D)!=="svelte-1cu78c7"&&(D.innerHTML=ao),kn=o(e),K=i(e,"P",{"data-svelte-h":!0}),u(K)!=="svelte-173mkyz"&&(K.textContent=oo),xn=o(e),m(O.$$.fragment,e),qn=o(e),ee=i(e,"P",{"data-svelte-h":!0}),u(ee)!=="svelte-8sbga0"&&(ee.innerHTML=so),Un=o(e),m(te.$$.fragment,e),zn=o(e),ne=i(e,"P",{"data-svelte-h":!0}),u(ne)!=="svelte-o6s3th"&&(ne.textContent=io),Zn=o(e),m(le.$$.fragment,e),Wn=o(e),m(ae.$$.fragment,e),jn=o(e),oe=i(e,"P",{"data-svelte-h":!0}),u(oe)!=="svelte-1h9rpz3"&&(oe.innerHTML=ro),Gn=o(e),m(se.$$.fragment,e),Qn=o(e),ie=i(e,"P",{"data-svelte-h":!0}),u(ie)!=="svelte-1wifo6l"&&(ie.textContent=mo),Bn=o(e),re=i(e,"P",{"data-svelte-h":!0}),u(re)!=="svelte-1dkzvn5"&&(re.innerHTML=po),Xn=o(e),m(me.$$.fragment,e),Vn=o(e),de=i(e,"P",{"data-svelte-h":!0}),u(de)!=="svelte-b4piob"&&(de.innerHTML=fo),Ln=o(e),m(U.$$.fragment,e),Rn=o(e),m(pe.$$.fragment,e),Fn=o(e),fe=i(e,"P",{"data-svelte-h":!0}),u(fe)!=="svelte-1mi5065"&&(fe.innerHTML=co),Nn=o(e),m(ce.$$.fragment,e),Hn=o(e),ue=i(e,"P",{"data-svelte-h":!0}),u(ue)!=="svelte-1wrdmud"&&(ue.innerHTML=uo),Pn=o(e),m(be.$$.fragment,e),In=o(e),ge=i(e,"P",{"data-svelte-h":!0}),u(ge)!=="svelte-1taffib"&&(ge.innerHTML=bo),An=o(e),m(he.$$.fragment,e),En=o(e),m(_e.$$.fragment,e),Yn=o(e),ye=i(e,"P",{"data-svelte-h":!0}),u(ye)!=="svelte-1q9lmg5"&&(ye.innerHTML=go),Sn=o(e),m($e.$$.fragment,e),Dn=o(e),Me=i(e,"P",{"data-svelte-h":!0}),u(Me)!=="svelte-1mb4sw2"&&(Me.innerHTML=ho),Kn=o(e),m(Te.$$.fragment,e),On=o(e),m(we.$$.fragment,e),el=o(e),ve=i(e,"P",{"data-svelte-h":!0}),u(ve)!=="svelte-rwqpya"&&(ve.innerHTML=_o),tl=o(e),m(Ce.$$.fragment,e),nl=o(e),Je=i(e,"P",{"data-svelte-h":!0}),u(Je)!=="svelte-2djxfj"&&(Je.textContent=yo),ll=o(e),m(ke.$$.fragment,e),al=o(e),xe=i(e,"P",{"data-svelte-h":!0}),u(xe)!=="svelte-16ybbze"&&(xe.innerHTML=$o),ol=o(e),m(qe.$$.fragment,e),sl=o(e),Ue=i(e,"P",{"data-svelte-h":!0}),u(Ue)!=="svelte-cf6ao3"&&(Ue.innerHTML=Mo),il=o(e),m(ze.$$.fragment,e),rl=o(e),w=i(e,"DIV",{class:!0});var v=C(w);m(Ze.$$.fragment,v),ja=o(v),an=i(v,"P",{"data-svelte-h":!0}),u(an)!=="svelte-i3efvr"&&(an.innerHTML=To),Ga=o(v),z=i(v,"DIV",{class:!0});var tn=C(z);m(We.$$.fragment,tn),Qa=o(tn),on=i(tn,"P",{"data-svelte-h":!0}),u(on)!=="svelte-4jdj2l"&&(on.textContent=wo),tn.forEach(n),Ba=o(v),Z=i(v,"DIV",{class:!0});var nn=C(Z);m(je.$$.fragment,nn),Xa=o(nn),sn=i(nn,"P",{"data-svelte-h":!0}),u(sn)!=="svelte-1ozftb6"&&(sn.textContent=vo),nn.forEach(n),Va=o(v),W=i(v,"DIV",{class:!0});var ln=C(W);m(Ge.$$.fragment,ln),La=o(ln),rn=i(ln,"P",{"data-svelte-h":!0}),u(rn)!=="svelte-pjgtd6"&&(rn.textContent=Co),ln.forEach(n),v.forEach(n),ml=o(e),m(Qe.$$.fragment,e),dl=o(e),Be=i(e,"P",{"data-svelte-h":!0}),u(Be)!=="svelte-1lw70bk"&&(Be.innerHTML=Jo),pl=o(e),Xe=i(e,"P",{"data-svelte-h":!0}),u(Xe)!=="svelte-19v0jp6"&&(Xe.innerHTML=ko),fl=o(e),Ve=i(e,"P",{"data-svelte-h":!0}),u(Ve)!=="svelte-1u8e287"&&(Ve.innerHTML=xo),cl=o(e),Le=i(e,"P",{"data-svelte-h":!0}),u(Le)!=="svelte-g9g250"&&(Le.innerHTML=qo),ul=o(e),Re=i(e,"P",{"data-svelte-h":!0}),u(Re)!=="svelte-ouymag"&&(Re.innerHTML=Uo),bl=o(e),m(Fe.$$.fragment,e),gl=o(e),Ne=i(e,"P",{"data-svelte-h":!0}),u(Ne)!=="svelte-qonxi"&&(Ne.innerHTML=zo),hl=o(e),m(He.$$.fragment,e),_l=o(e),Pe=i(e,"P",{"data-svelte-h":!0}),u(Pe)!=="svelte-1rxk684"&&(Pe.innerHTML=Zo),yl=o(e),m(Ie.$$.fragment,e),$l=o(e),m(Ae.$$.fragment,e),Ml=o(e),m(Ee.$$.fragment,e),Tl=o(e),Ye=i(e,"P",{"data-svelte-h":!0}),u(Ye)!=="svelte-1lnnlr4"&&(Ye.textContent=Wo),wl=o(e),Se=i(e,"UL",{"data-svelte-h":!0}),u(Se)!=="svelte-1wl10a1"&&(Se.innerHTML=jo),vl=o(e),m(De.$$.fragment,e),Cl=o(e),Ke=i(e,"UL",{"data-svelte-h":!0}),u(Ke)!=="svelte-iudtgn"&&(Ke.innerHTML=Go),Jl=o(e),m(Oe.$$.fragment,e),kl=o(e),et=i(e,"P",{"data-svelte-h":!0}),u(et)!=="svelte-artgoh"&&(et.innerHTML=Qo),xl=o(e),m(tt.$$.fragment,e),ql=o(e),m(j.$$.fragment,e),Ul=o(e),m(nt.$$.fragment,e),zl=o(e),lt=i(e,"P",{"data-svelte-h":!0}),u(lt)!=="svelte-1ep4vf9"&&(lt.innerHTML=Bo),Zl=o(e),m(at.$$.fragment,e),Wl=o(e),ot=i(e,"P",{"data-svelte-h":!0}),u(ot)!=="svelte-1wye4p4"&&(ot.innerHTML=Xo),jl=o(e),st=i(e,"P",{"data-svelte-h":!0}),u(st)!=="svelte-3h8e3v"&&(st.innerHTML=Vo),Gl=o(e),m(it.$$.fragment,e),Ql=o(e),rt=i(e,"P",{"data-svelte-h":!0}),u(rt)!=="svelte-1qrn2hy"&&(rt.textContent=Lo),Bl=o(e),m(G.$$.fragment,e),Xl=o(e),m(mt.$$.fragment,e),Vl=o(e),dt=i(e,"P",{"data-svelte-h":!0}),u(dt)!=="svelte-brbl2r"&&(dt.textContent=Ro),Ll=o(e),m(pt.$$.fragment,e),Rl=o(e),ft=i(e,"P",{"data-svelte-h":!0}),u(ft)!=="svelte-8cey66"&&(ft.innerHTML=Fo),Fl=o(e),m(ct.$$.fragment,e),Nl=o(e),m(ut.$$.fragment,e),Hl=o(e),bt=i(e,"P",{"data-svelte-h":!0}),u(bt)!=="svelte-1gdahjp"&&(bt.textContent=No),Pl=o(e),m(gt.$$.fragment,e),Il=o(e),m(ht.$$.fragment,e),Al=o(e),_t=i(e,"P",{"data-svelte-h":!0}),u(_t)!=="svelte-1wj738d"&&(_t.textContent=Ho),El=o(e),m(yt.$$.fragment,e),Yl=o(e),m($t.$$.fragment,e),Sl=o(e),Mt=i(e,"P",{"data-svelte-h":!0}),u(Mt)!=="svelte-9if3q8"&&(Mt.innerHTML=Po),Dl=o(e),m(Tt.$$.fragment,e),Kl=o(e),m(Q.$$.fragment,e),Ol=o(e),m(wt.$$.fragment,e),ea=o(e),vt=i(e,"P",{"data-svelte-h":!0}),u(vt)!=="svelte-1am21ml"&&(vt.innerHTML=Io),ta=o(e),m(Ct.$$.fragment,e),na=o(e),Jt=i(e,"P",{"data-svelte-h":!0}),u(Jt)!=="svelte-1wm02p5"&&(Jt.innerHTML=Ao),la=o(e),m(kt.$$.fragment,e),aa=o(e),xt=i(e,"P",{"data-svelte-h":!0}),u(xt)!=="svelte-1xz0d87"&&(xt.textContent=Eo),oa=o(e),m(qt.$$.fragment,e),sa=o(e),Ut=i(e,"P",{"data-svelte-h":!0}),u(Ut)!=="svelte-yl6mei"&&(Ut.innerHTML=Yo),ia=o(e),zt=i(e,"P",{"data-svelte-h":!0}),u(zt)!=="svelte-xbol81"&&(zt.innerHTML=So),ra=o(e),m(Zt.$$.fragment,e),ma=o(e),Wt=i(e,"P",{"data-svelte-h":!0}),u(Wt)!=="svelte-1n9uav7"&&(Wt.innerHTML=Do),da=o(e),m(jt.$$.fragment,e),pa=o(e),Gt=i(e,"P",{"data-svelte-h":!0}),u(Gt)!=="svelte-mnwhfx"&&(Gt.textContent=Ko),fa=o(e),m(Qt.$$.fragment,e),ca=o(e),Bt=i(e,"P",{"data-svelte-h":!0}),u(Bt)!=="svelte-rwyjy1"&&(Bt.textContent=Oo),ua=o(e),m(Xt.$$.fragment,e),ba=o(e),Vt=i(e,"P",{"data-svelte-h":!0}),u(Vt)!=="svelte-1ozkl3z"&&(Vt.innerHTML=es),ga=o(e),m(Lt.$$.fragment,e),ha=o(e),m(Rt.$$.fragment,e),_a=o(e),Ft=i(e,"P",{"data-svelte-h":!0}),u(Ft)!=="svelte-bn1879"&&(Ft.innerHTML=ts),ya=o(e),m(Nt.$$.fragment,e),$a=o(e),m(Ht.$$.fragment,e),Ma=o(e),Pt=i(e,"P",{"data-svelte-h":!0}),u(Pt)!=="svelte-mgdccl"&&(Pt.innerHTML=ns),Ta=o(e),It=i(e,"P",{"data-svelte-h":!0}),u(It)!=="svelte-14qh4dh"&&(It.innerHTML=ls),wa=o(e),m(At.$$.fragment,e),va=o(e),M=i(e,"DIV",{class:!0});var T=C(M);m(Et.$$.fragment,T),Ra=o(T),mn=i(T,"P",{"data-svelte-h":!0}),u(mn)!=="svelte-woamwr"&&(mn.innerHTML=as),Fa=o(T),dn=i(T,"P",{"data-svelte-h":!0}),u(dn)!=="svelte-ki5gis"&&(dn.innerHTML=os),Na=o(T),pn=i(T,"P",{"data-svelte-h":!0}),u(pn)!=="svelte-8qsk2q"&&(pn.innerHTML=ss),Ha=o(T),B=i(T,"DIV",{class:!0});var qa=C(B);m(Yt.$$.fragment,qa),Pa=o(qa),fn=i(qa,"P",{"data-svelte-h":!0}),u(fn)!=="svelte-10tvzyv"&&(fn.innerHTML=is),qa.forEach(n),Ia=o(T),X=i(T,"DIV",{class:!0});var Ua=C(X);m(St.$$.fragment,Ua),Aa=o(Ua),cn=i(Ua,"P",{"data-svelte-h":!0}),u(cn)!=="svelte-gy26u4"&&(cn.textContent=rs),Ua.forEach(n),Ea=o(T),V=i(T,"DIV",{class:!0});var za=C(V);m(Dt.$$.fragment,za),Ya=o(za),un=i(za,"P",{"data-svelte-h":!0}),u(un)!=="svelte-19bn0da"&&(un.innerHTML=ms),za.forEach(n),Sa=o(T),L=i(T,"DIV",{class:!0});var Za=C(L);m(Kt.$$.fragment,Za),Da=o(Za),bn=i(Za,"P",{"data-svelte-h":!0}),u(bn)!=="svelte-1p6bdas"&&(bn.textContent=ds),Za.forEach(n),T.forEach(n),Ca=o(e),m(Ot.$$.fragment,e),Ja=o(e),en=i(e,"P",{"data-svelte-h":!0}),u(en)!=="svelte-p3rxgx"&&(en.innerHTML=ps),ka=o(e),gn=i(e,"P",{}),C(gn).forEach(n),this.h()},h(){J(g,"name","hf:doc:metadata"),J(g,"content",Cs),J(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(w,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(M,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,t){b(document.head,g),l(e,y,t),l(e,$,t),l(e,q,t),d(R,e,t),l(e,hn,t),d(F,e,t),l(e,_n,t),l(e,N,t),l(e,yn,t),l(e,H,t),l(e,$n,t),l(e,P,t),l(e,Mn,t),d(I,e,t),l(e,Tn,t),l(e,A,t),l(e,wn,t),l(e,E,t),l(e,vn,t),l(e,Y,t),l(e,Cn,t),d(S,e,t),l(e,Jn,t),l(e,D,t),l(e,kn,t),l(e,K,t),l(e,xn,t),d(O,e,t),l(e,qn,t),l(e,ee,t),l(e,Un,t),d(te,e,t),l(e,zn,t),l(e,ne,t),l(e,Zn,t),d(le,e,t),l(e,Wn,t),d(ae,e,t),l(e,jn,t),l(e,oe,t),l(e,Gn,t),d(se,e,t),l(e,Qn,t),l(e,ie,t),l(e,Bn,t),l(e,re,t),l(e,Xn,t),d(me,e,t),l(e,Vn,t),l(e,de,t),l(e,Ln,t),d(U,e,t),l(e,Rn,t),d(pe,e,t),l(e,Fn,t),l(e,fe,t),l(e,Nn,t),d(ce,e,t),l(e,Hn,t),l(e,ue,t),l(e,Pn,t),d(be,e,t),l(e,In,t),l(e,ge,t),l(e,An,t),d(he,e,t),l(e,En,t),d(_e,e,t),l(e,Yn,t),l(e,ye,t),l(e,Sn,t),d($e,e,t),l(e,Dn,t),l(e,Me,t),l(e,Kn,t),d(Te,e,t),l(e,On,t),d(we,e,t),l(e,el,t),l(e,ve,t),l(e,tl,t),d(Ce,e,t),l(e,nl,t),l(e,Je,t),l(e,ll,t),d(ke,e,t),l(e,al,t),l(e,xe,t),l(e,ol,t),d(qe,e,t),l(e,sl,t),l(e,Ue,t),l(e,il,t),d(ze,e,t),l(e,rl,t),l(e,w,t),d(Ze,w,null),b(w,ja),b(w,an),b(w,Ga),b(w,z),d(We,z,null),b(z,Qa),b(z,on),b(w,Ba),b(w,Z),d(je,Z,null),b(Z,Xa),b(Z,sn),b(w,Va),b(w,W),d(Ge,W,null),b(W,La),b(W,rn),l(e,ml,t),d(Qe,e,t),l(e,dl,t),l(e,Be,t),l(e,pl,t),l(e,Xe,t),l(e,fl,t),l(e,Ve,t),l(e,cl,t),l(e,Le,t),l(e,ul,t),l(e,Re,t),l(e,bl,t),d(Fe,e,t),l(e,gl,t),l(e,Ne,t),l(e,hl,t),d(He,e,t),l(e,_l,t),l(e,Pe,t),l(e,yl,t),d(Ie,e,t),l(e,$l,t),d(Ae,e,t),l(e,Ml,t),d(Ee,e,t),l(e,Tl,t),l(e,Ye,t),l(e,wl,t),l(e,Se,t),l(e,vl,t),d(De,e,t),l(e,Cl,t),l(e,Ke,t),l(e,Jl,t),d(Oe,e,t),l(e,kl,t),l(e,et,t),l(e,xl,t),d(tt,e,t),l(e,ql,t),d(j,e,t),l(e,Ul,t),d(nt,e,t),l(e,zl,t),l(e,lt,t),l(e,Zl,t),d(at,e,t),l(e,Wl,t),l(e,ot,t),l(e,jl,t),l(e,st,t),l(e,Gl,t),d(it,e,t),l(e,Ql,t),l(e,rt,t),l(e,Bl,t),d(G,e,t),l(e,Xl,t),d(mt,e,t),l(e,Vl,t),l(e,dt,t),l(e,Ll,t),d(pt,e,t),l(e,Rl,t),l(e,ft,t),l(e,Fl,t),d(ct,e,t),l(e,Nl,t),d(ut,e,t),l(e,Hl,t),l(e,bt,t),l(e,Pl,t),d(gt,e,t),l(e,Il,t),d(ht,e,t),l(e,Al,t),l(e,_t,t),l(e,El,t),d(yt,e,t),l(e,Yl,t),d($t,e,t),l(e,Sl,t),l(e,Mt,t),l(e,Dl,t),d(Tt,e,t),l(e,Kl,t),d(Q,e,t),l(e,Ol,t),d(wt,e,t),l(e,ea,t),l(e,vt,t),l(e,ta,t),d(Ct,e,t),l(e,na,t),l(e,Jt,t),l(e,la,t),d(kt,e,t),l(e,aa,t),l(e,xt,t),l(e,oa,t),d(qt,e,t),l(e,sa,t),l(e,Ut,t),l(e,ia,t),l(e,zt,t),l(e,ra,t),d(Zt,e,t),l(e,ma,t),l(e,Wt,t),l(e,da,t),d(jt,e,t),l(e,pa,t),l(e,Gt,t),l(e,fa,t),d(Qt,e,t),l(e,ca,t),l(e,Bt,t),l(e,ua,t),d(Xt,e,t),l(e,ba,t),l(e,Vt,t),l(e,ga,t),d(Lt,e,t),l(e,ha,t),d(Rt,e,t),l(e,_a,t),l(e,Ft,t),l(e,ya,t),d(Nt,e,t),l(e,$a,t),d(Ht,e,t),l(e,Ma,t),l(e,Pt,t),l(e,Ta,t),l(e,It,t),l(e,wa,t),d(At,e,t),l(e,va,t),l(e,M,t),d(Et,M,null),b(M,Ra),b(M,mn),b(M,Fa),b(M,dn),b(M,Na),b(M,pn),b(M,Ha),b(M,B),d(Yt,B,null),b(B,Pa),b(B,fn),b(M,Ia),b(M,X),d(St,X,null),b(X,Aa),b(X,cn),b(M,Ea),b(M,V),d(Dt,V,null),b(V,Ya),b(V,un),b(M,Sa),b(M,L),d(Kt,L,null),b(L,Da),b(L,bn),l(e,Ca,t),d(Ot,e,t),l(e,Ja,t),l(e,en,t),l(e,ka,t),l(e,gn,t),xa=!0},p(e,[t]){const v={};t&2&&(v.$$scope={dirty:t,ctx:e}),U.$set(v);const tn={};t&2&&(tn.$$scope={dirty:t,ctx:e}),j.$set(tn);const nn={};t&2&&(nn.$$scope={dirty:t,ctx:e}),G.$set(nn);const ln={};t&2&&(ln.$$scope={dirty:t,ctx:e}),Q.$set(ln)},i(e){xa||(p(R.$$.fragment,e),p(F.$$.fragment,e),p(I.$$.fragment,e),p(S.$$.fragment,e),p(O.$$.fragment,e),p(te.$$.fragment,e),p(le.$$.fragment,e),p(ae.$$.fragment,e),p(se.$$.fragment,e),p(me.$$.fragment,e),p(U.$$.fragment,e),p(pe.$$.fragment,e),p(ce.$$.fragment,e),p(be.$$.fragment,e),p(he.$$.fragment,e),p(_e.$$.fragment,e),p($e.$$.fragment,e),p(Te.$$.fragment,e),p(we.$$.fragment,e),p(Ce.$$.fragment,e),p(ke.$$.fragment,e),p(qe.$$.fragment,e),p(ze.$$.fragment,e),p(Ze.$$.fragment,e),p(We.$$.fragment,e),p(je.$$.fragment,e),p(Ge.$$.fragment,e),p(Qe.$$.fragment,e),p(Fe.$$.fragment,e),p(He.$$.fragment,e),p(Ie.$$.fragment,e),p(Ae.$$.fragment,e),p(Ee.$$.fragment,e),p(De.$$.fragment,e),p(Oe.$$.fragment,e),p(tt.$$.fragment,e),p(j.$$.fragment,e),p(nt.$$.fragment,e),p(at.$$.fragment,e),p(it.$$.fragment,e),p(G.$$.fragment,e),p(mt.$$.fragment,e),p(pt.$$.fragment,e),p(ct.$$.fragment,e),p(ut.$$.fragment,e),p(gt.$$.fragment,e),p(ht.$$.fragment,e),p(yt.$$.fragment,e),p($t.$$.fragment,e),p(Tt.$$.fragment,e),p(Q.$$.fragment,e),p(wt.$$.fragment,e),p(Ct.$$.fragment,e),p(kt.$$.fragment,e),p(qt.$$.fragment,e),p(Zt.$$.fragment,e),p(jt.$$.fragment,e),p(Qt.$$.fragment,e),p(Xt.$$.fragment,e),p(Lt.$$.fragment,e),p(Rt.$$.fragment,e),p(Nt.$$.fragment,e),p(Ht.$$.fragment,e),p(At.$$.fragment,e),p(Et.$$.fragment,e),p(Yt.$$.fragment,e),p(St.$$.fragment,e),p(Dt.$$.fragment,e),p(Kt.$$.fragment,e),p(Ot.$$.fragment,e),xa=!0)},o(e){f(R.$$.fragment,e),f(F.$$.fragment,e),f(I.$$.fragment,e),f(S.$$.fragment,e),f(O.$$.fragment,e),f(te.$$.fragment,e),f(le.$$.fragment,e),f(ae.$$.fragment,e),f(se.$$.fragment,e),f(me.$$.fragment,e),f(U.$$.fragment,e),f(pe.$$.fragment,e),f(ce.$$.fragment,e),f(be.$$.fragment,e),f(he.$$.fragment,e),f(_e.$$.fragment,e),f($e.$$.fragment,e),f(Te.$$.fragment,e),f(we.$$.fragment,e),f(Ce.$$.fragment,e),f(ke.$$.fragment,e),f(qe.$$.fragment,e),f(ze.$$.fragment,e),f(Ze.$$.fragment,e),f(We.$$.fragment,e),f(je.$$.fragment,e),f(Ge.$$.fragment,e),f(Qe.$$.fragment,e),f(Fe.$$.fragment,e),f(He.$$.fragment,e),f(Ie.$$.fragment,e),f(Ae.$$.fragment,e),f(Ee.$$.fragment,e),f(De.$$.fragment,e),f(Oe.$$.fragment,e),f(tt.$$.fragment,e),f(j.$$.fragment,e),f(nt.$$.fragment,e),f(at.$$.fragment,e),f(it.$$.fragment,e),f(G.$$.fragment,e),f(mt.$$.fragment,e),f(pt.$$.fragment,e),f(ct.$$.fragment,e),f(ut.$$.fragment,e),f(gt.$$.fragment,e),f(ht.$$.fragment,e),f(yt.$$.fragment,e),f($t.$$.fragment,e),f(Tt.$$.fragment,e),f(Q.$$.fragment,e),f(wt.$$.fragment,e),f(Ct.$$.fragment,e),f(kt.$$.fragment,e),f(qt.$$.fragment,e),f(Zt.$$.fragment,e),f(jt.$$.fragment,e),f(Qt.$$.fragment,e),f(Xt.$$.fragment,e),f(Lt.$$.fragment,e),f(Rt.$$.fragment,e),f(Nt.$$.fragment,e),f(Ht.$$.fragment,e),f(At.$$.fragment,e),f(Et.$$.fragment,e),f(Yt.$$.fragment,e),f(St.$$.fragment,e),f(Dt.$$.fragment,e),f(Kt.$$.fragment,e),f(Ot.$$.fragment,e),xa=!1},d(e){e&&(n(y),n($),n(q),n(hn),n(_n),n(N),n(yn),n(H),n($n),n(P),n(Mn),n(Tn),n(A),n(wn),n(E),n(vn),n(Y),n(Cn),n(Jn),n(D),n(kn),n(K),n(xn),n(qn),n(ee),n(Un),n(zn),n(ne),n(Zn),n(Wn),n(jn),n(oe),n(Gn),n(Qn),n(ie),n(Bn),n(re),n(Xn),n(Vn),n(de),n(Ln),n(Rn),n(Fn),n(fe),n(Nn),n(Hn),n(ue),n(Pn),n(In),n(ge),n(An),n(En),n(Yn),n(ye),n(Sn),n(Dn),n(Me),n(Kn),n(On),n(el),n(ve),n(tl),n(nl),n(Je),n(ll),n(al),n(xe),n(ol),n(sl),n(Ue),n(il),n(rl),n(w),n(ml),n(dl),n(Be),n(pl),n(Xe),n(fl),n(Ve),n(cl),n(Le),n(ul),n(Re),n(bl),n(gl),n(Ne),n(hl),n(_l),n(Pe),n(yl),n($l),n(Ml),n(Tl),n(Ye),n(wl),n(Se),n(vl),n(Cl),n(Ke),n(Jl),n(kl),n(et),n(xl),n(ql),n(Ul),n(zl),n(lt),n(Zl),n(Wl),n(ot),n(jl),n(st),n(Gl),n(Ql),n(rt),n(Bl),n(Xl),n(Vl),n(dt),n(Ll),n(Rl),n(ft),n(Fl),n(Nl),n(Hl),n(bt),n(Pl),n(Il),n(Al),n(_t),n(El),n(Yl),n(Sl),n(Mt),n(Dl),n(Kl),n(Ol),n(ea),n(vt),n(ta),n(na),n(Jt),n(la),n(aa),n(xt),n(oa),n(sa),n(Ut),n(ia),n(zt),n(ra),n(ma),n(Wt),n(da),n(pa),n(Gt),n(fa),n(ca),n(Bt),n(ua),n(ba),n(Vt),n(ga),n(ha),n(_a),n(Ft),n(ya),n($a),n(Ma),n(Pt),n(Ta),n(It),n(wa),n(va),n(M),n(Ca),n(Ja),n(en),n(ka),n(gn)),n(g),c(R,e),c(F,e),c(I,e),c(S,e),c(O,e),c(te,e),c(le,e),c(ae,e),c(se,e),c(me,e),c(U,e),c(pe,e),c(ce,e),c(be,e),c(he,e),c(_e,e),c($e,e),c(Te,e),c(we,e),c(Ce,e),c(ke,e),c(qe,e),c(ze,e),c(Ze),c(We),c(je),c(Ge),c(Qe,e),c(Fe,e),c(He,e),c(Ie,e),c(Ae,e),c(Ee,e),c(De,e),c(Oe,e),c(tt,e),c(j,e),c(nt,e),c(at,e),c(it,e),c(G,e),c(mt,e),c(pt,e),c(ct,e),c(ut,e),c(gt,e),c(ht,e),c(yt,e),c($t,e),c(Tt,e),c(Q,e),c(wt,e),c(Ct,e),c(kt,e),c(qt,e),c(Zt,e),c(jt,e),c(Qt,e),c(Xt,e),c(Lt,e),c(Rt,e),c(Nt,e),c(Ht,e),c(At,e),c(Et),c(Yt),c(St),c(Dt),c(Kt),c(Ot,e)}}}const Cs='{"title":"Quantize ğŸ¤— Transformers models","local":"quantize--transformers-models","sections":[{"title":"AutoGPTQ Integration","local":"autogptq-integration","sections":[{"title":"Requirements","local":"requirements","sections":[],"depth":3},{"title":"Load and quantize a model","local":"load-and-quantize-a-model","sections":[{"title":"GPTQ Configuration","local":"gptq-configuration","sections":[],"depth":4},{"title":"Quantization","local":"quantization","sections":[],"depth":4}],"depth":3},{"title":"Push quantized model to ğŸ¤— Hub","local":"push-quantized-model-to--hub","sections":[],"depth":3},{"title":"Load a quantized model from the ğŸ¤— Hub","local":"load-a-quantized-model-from-the--hub","sections":[],"depth":3},{"title":"Exllama kernels for faster inference","local":"exllama-kernels-for-faster-inference","sections":[{"title":"Fine-tune a quantized model","local":"fine-tune-a-quantized-model","sections":[],"depth":4}],"depth":3},{"title":"Example demo","local":"example-demo","sections":[],"depth":3},{"title":"GPTQConfig","local":"transformers.GPTQConfig","sections":[],"depth":3}],"depth":2},{"title":"bitsandbytes Integration","local":"bitsandbytes-integration","sections":[{"title":"General usage","local":"general-usage","sections":[],"depth":3},{"title":"FP4 quantization","local":"fp4-quantization","sections":[{"title":"Requirements","local":"requirements","sections":[],"depth":4},{"title":"Tips and best practices","local":"tips-and-best-practices","sections":[],"depth":4},{"title":"Load a large model in 4bit","local":"load-a-large-model-in-4bit","sections":[],"depth":4}],"depth":3},{"title":"Load a large model in 8bit","local":"load-a-large-model-in-8bit","sections":[{"title":"Advanced use cases","local":"advanced-use-cases","sections":[{"title":"Change the compute dtype","local":"change-the-compute-dtype","sections":[],"depth":5},{"title":"Using NF4 (Normal Float 4) data type","local":"using-nf4-normal-float-4-data-type","sections":[],"depth":5},{"title":"Use nested quantization for more memory efficient inference","local":"use-nested-quantization-for-more-memory-efficient-inference","sections":[],"depth":5}],"depth":4}],"depth":3},{"title":"Push quantized models on the ğŸ¤— Hub","local":"push-quantized-models-on-the--hub","sections":[],"depth":3},{"title":"Load a quantized model from the ğŸ¤— Hub","local":"load-a-quantized-model-from-the--hub","sections":[],"depth":3},{"title":"Advanced use cases","local":"advanced-use-cases","sections":[{"title":"Offload between cpu and gpu","local":"offload-between-cpu-and-gpu","sections":[],"depth":4},{"title":"Play with llm_int8_threshold","local":"play-with-llmint8threshold","sections":[],"depth":4},{"title":"Skip the conversion of some modules","local":"skip-the-conversion-of-some-modules","sections":[],"depth":4},{"title":"Fine-tune a model that has been loaded in 8-bit","local":"fine-tune-a-model-that-has-been-loaded-in-8-bit","sections":[],"depth":4}],"depth":3},{"title":"BitsAndBytesConfig","local":"transformers.BitsAndBytesConfig","sections":[],"depth":3}],"depth":2},{"title":"Quantization with ğŸ¤— optimum","local":"quantization-with--optimum","sections":[],"depth":2}],"depth":1}';function Js(k){return gs(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Ws extends hs{constructor(g){super(),_s(this,g,Js,vs,bs,{})}}export{Ws as component};
