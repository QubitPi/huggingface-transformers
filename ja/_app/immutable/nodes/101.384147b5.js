import{s as re,o as oe,n as ce}from"../chunks/scheduler.9bc65507.js";import{S as me,i as ue,g as r,s as a,r as R,A as de,h as o,f as l,c as i,j as ae,u as U,x as m,k as ie,y as fe,a as s,v,d as x,t as Z,w as B}from"../chunks/index.707bf1b6.js";import{T as Me}from"../chunks/Tip.c2ecdbf4.js";import{C as pe}from"../chunks/CodeBlock.54a9f38d.js";import{H as Q}from"../chunks/Heading.342b1fa6.js";function he(G){let n,u='DePlot は、<code>Pix2Struct</code>アーキテクチャを使用してトレーニングされたモデルです。 API リファレンスについては、<a href="pix2struct"><code>Pix2Struct</code> ドキュメント</a> を参照してください。';return{c(){n=r("p"),n.innerHTML=u},l(p){n=o(p,"P",{"data-svelte-h":!0}),m(n)!=="svelte-kq05cd"&&(n.innerHTML=u)},m(p,P){s(p,n,P)},p:ce,d(p){p&&l(n)}}}function ye(G){let n,u,p,P,d,X,f,k,M,q='DePlot は、Fangyu Liu、Julian Martin Aisenschlos、Francesco Piccinno、Syrine Krichene、Chenxi Pang, Kenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, Yasemin Altun. の論文 <a href="https://arxiv.org/abs/2212.10505" rel="nofollow">DePlot: One-shot visual language reasoning by plot-to-table translation</a> で提案されました。パン・',C,h,K="論文の要約には次のように記載されています。",V,y,O="<em>チャートやプロットなどの視覚言語は人間の世界に遍在しています。プロットやチャートを理解するには、強力な推論スキルが必要です。従来の最先端 (SOTA) モデルには少なくとも数万のトレーニング サンプルが必要であり、その推論能力は、特に人間が作成した複雑なクエリでは依然として大幅に制限されています。この論文では、視覚言語推論に対する最初のワンショット ソリューションを紹介します。私たちは、視覚言語推論の課題を 2 つのステップに分解します。(1) プロットからテキストへの翻訳と、(2) 翻訳されたテキストに対する推論です。この方法の鍵となるのは、プロットまたはチャートの画像を線形化されたテーブルに変換する、DePlot という名前のモダリティ変換モジュールです。その後、DePlot の出力を直接使用して、事前トレーニング済みの大規模言語モデル (LLM) をプロンプトし、LLM の少数ショット推論機能を利用できます。 DePlot を取得するには、統一されたタスク形式とメトリクスを確立することでプロットからテーブルへのタスクを標準化し、このタスクで DePlot をエンドツーエンドでトレーニングします。 DePlot は、プラグアンドプレイ方式で LLM とともに既製で使用できます。 28,000 を超えるデータ ポイントで微調整された SOTA モデルと比較して、ワンショット プロンプトのみを使用する DePlot+LLM は、チャート QA タスクからの人が作成したクエリに関して、微調整された SOTA より 24.0% の改善を達成しました。</em>",H,w,ee=`DePlot は、<code>Pix2Struct</code> アーキテクチャを使用してトレーニングされたモデルです。 <code>Pix2Struct</code> の詳細については、<a href="https://huggingface.co/docs/transformers/main/en/model_doc/pix2struct" rel="nofollow">Pix2Struct ドキュメント</a> を参照してください。
DePlot は、<code>Pix2Struct</code> アーキテクチャの Visual Question Answering サブセットです。入力された質問を画像上にレンダリングし、答えを予測します。`,Y,b,I,g,te="現在、DePlot で使用できるチェックポイントは 1 つです。",z,J,le="<li><code>google/deplot</code>: ChartQA データセットで微調整された DePlot</li>",D,T,F,$,L,j,se='DePlot を微調整するには、pix2struct <a href="https://github.com/huggingface/notebooks/blob/main/examples/image_captioning_pix2struct.ipynb" rel="nofollow">微調整ノートブック</a> を参照してください。 <code>Pix2Struct</code> モデルの場合、Adafactor とコサイン学習率スケジューラを使用してモデルを微調整すると、収束が高速化されることがわかりました。',S,_,A,c,E,W,N;return d=new Q({props:{title:"DePlot",local:"deplot",headingTag:"h1"}}),f=new Q({props:{title:"Overview",local:"overview",headingTag:"h2"}}),b=new Q({props:{title:"Usage example",local:"usage-example",headingTag:"h2"}}),T=new pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBQaXgyU3RydWN0Rm9yQ29uZGl0aW9uYWxHZW5lcmF0aW9uJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEElMEFtb2RlbCUyMCUzRCUyMFBpeDJTdHJ1Y3RGb3JDb25kaXRpb25hbEdlbmVyYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmdvb2dsZSUyRmRlcGxvdCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJnb29nbGUlMkZkZXBsb3QlMjIpJTBBdXJsJTIwJTNEJTIwJTIyaHR0cHMlM0ElMkYlMkZyYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tJTJGdmlzLW5scCUyRkNoYXJ0UUElMkZtYWluJTJGQ2hhcnRRQSUyNTIwRGF0YXNldCUyRnZhbCUyRnBuZyUyRjUwOTAucG5nJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwdGV4dCUzRCUyMkdlbmVyYXRlJTIwdW5kZXJseWluZyUyMGRhdGElMjB0YWJsZSUyMG9mJTIwdGhlJTIwZmlndXJlJTIwYmVsb3clM0ElMjIlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQXByZWRpY3Rpb25zJTIwJTNEJTIwbW9kZWwuZ2VuZXJhdGUoKippbnB1dHMlMkMlMjBtYXhfbmV3X3Rva2VucyUzRDUxMiklMEFwcmludChwcm9jZXNzb3IuZGVjb2RlKHByZWRpY3Rpb25zJTVCMCU1RCUyQyUyMHNraXBfc3BlY2lhbF90b2tlbnMlM0RUcnVlKSk=",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, Pix2StructForConditionalGeneration
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image

model = Pix2StructForConditionalGeneration.from_pretrained(<span class="hljs-string">&quot;google/deplot&quot;</span>)
processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;google/deplot&quot;</span>)
url = <span class="hljs-string">&quot;https://raw.githubusercontent.com/vis-nlp/ChartQA/main/ChartQA%20Dataset/val/png/5090.png&quot;</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

inputs = processor(images=image, text=<span class="hljs-string">&quot;Generate underlying data table of the figure below:&quot;</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
predictions = model.generate(**inputs, max_new_tokens=<span class="hljs-number">512</span>)
<span class="hljs-built_in">print</span>(processor.decode(predictions[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),$=new Q({props:{title:"Fine-tuning",local:"fine-tuning",headingTag:"h2"}}),_=new pe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycy5vcHRpbWl6YXRpb24lMjBpbXBvcnQlMjBBZGFmYWN0b3IlMkMlMjBnZXRfY29zaW5lX3NjaGVkdWxlX3dpdGhfd2FybXVwJTBBJTBBb3B0aW1pemVyJTIwJTNEJTIwQWRhZmFjdG9yKHNlbGYucGFyYW1ldGVycygpJTJDJTIwc2NhbGVfcGFyYW1ldGVyJTNERmFsc2UlMkMlMjByZWxhdGl2ZV9zdGVwJTNERmFsc2UlMkMlMjBsciUzRDAuMDElMkMlMjB3ZWlnaHRfZGVjYXklM0QxZS0wNSklMEFzY2hlZHVsZXIlMjAlM0QlMjBnZXRfY29zaW5lX3NjaGVkdWxlX3dpdGhfd2FybXVwKG9wdGltaXplciUyQyUyMG51bV93YXJtdXBfc3RlcHMlM0QxMDAwJTJDJTIwbnVtX3RyYWluaW5nX3N0ZXBzJTNENDAwMDAp",highlighted:`<span class="hljs-keyword">from</span> transformers.optimization <span class="hljs-keyword">import</span> Adafactor, get_cosine_schedule_with_warmup

optimizer = Adafactor(self.parameters(), scale_parameter=<span class="hljs-literal">False</span>, relative_step=<span class="hljs-literal">False</span>, lr=<span class="hljs-number">0.01</span>, weight_decay=<span class="hljs-number">1e-05</span>)
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=<span class="hljs-number">1000</span>, num_training_steps=<span class="hljs-number">40000</span>)`,wrap:!1}}),c=new Me({props:{$$slots:{default:[he]},$$scope:{ctx:G}}}),{c(){n=r("meta"),u=a(),p=r("p"),P=a(),R(d.$$.fragment),X=a(),R(f.$$.fragment),k=a(),M=r("p"),M.innerHTML=q,C=a(),h=r("p"),h.textContent=K,V=a(),y=r("p"),y.innerHTML=O,H=a(),w=r("p"),w.innerHTML=ee,Y=a(),R(b.$$.fragment),I=a(),g=r("p"),g.textContent=te,z=a(),J=r("ul"),J.innerHTML=le,D=a(),R(T.$$.fragment),F=a(),R($.$$.fragment),L=a(),j=r("p"),j.innerHTML=se,S=a(),R(_.$$.fragment),A=a(),R(c.$$.fragment),E=a(),W=r("p"),this.h()},l(e){const t=de("svelte-u9bgzb",document.head);n=o(t,"META",{name:!0,content:!0}),t.forEach(l),u=i(e),p=o(e,"P",{}),ae(p).forEach(l),P=i(e),U(d.$$.fragment,e),X=i(e),U(f.$$.fragment,e),k=i(e),M=o(e,"P",{"data-svelte-h":!0}),m(M)!=="svelte-19rngdb"&&(M.innerHTML=q),C=i(e),h=o(e,"P",{"data-svelte-h":!0}),m(h)!=="svelte-1uuglsv"&&(h.textContent=K),V=i(e),y=o(e,"P",{"data-svelte-h":!0}),m(y)!=="svelte-xylhq2"&&(y.innerHTML=O),H=i(e),w=o(e,"P",{"data-svelte-h":!0}),m(w)!=="svelte-15ir0gu"&&(w.innerHTML=ee),Y=i(e),U(b.$$.fragment,e),I=i(e),g=o(e,"P",{"data-svelte-h":!0}),m(g)!=="svelte-v403fs"&&(g.textContent=te),z=i(e),J=o(e,"UL",{"data-svelte-h":!0}),m(J)!=="svelte-7pt61s"&&(J.innerHTML=le),D=i(e),U(T.$$.fragment,e),F=i(e),U($.$$.fragment,e),L=i(e),j=o(e,"P",{"data-svelte-h":!0}),m(j)!=="svelte-1ub3f1b"&&(j.innerHTML=se),S=i(e),U(_.$$.fragment,e),A=i(e),U(c.$$.fragment,e),E=i(e),W=o(e,"P",{}),ae(W).forEach(l),this.h()},h(){ie(n,"name","hf:doc:metadata"),ie(n,"content",we)},m(e,t){fe(document.head,n),s(e,u,t),s(e,p,t),s(e,P,t),v(d,e,t),s(e,X,t),v(f,e,t),s(e,k,t),s(e,M,t),s(e,C,t),s(e,h,t),s(e,V,t),s(e,y,t),s(e,H,t),s(e,w,t),s(e,Y,t),v(b,e,t),s(e,I,t),s(e,g,t),s(e,z,t),s(e,J,t),s(e,D,t),v(T,e,t),s(e,F,t),v($,e,t),s(e,L,t),s(e,j,t),s(e,S,t),v(_,e,t),s(e,A,t),v(c,e,t),s(e,E,t),s(e,W,t),N=!0},p(e,[t]){const ne={};t&2&&(ne.$$scope={dirty:t,ctx:e}),c.$set(ne)},i(e){N||(x(d.$$.fragment,e),x(f.$$.fragment,e),x(b.$$.fragment,e),x(T.$$.fragment,e),x($.$$.fragment,e),x(_.$$.fragment,e),x(c.$$.fragment,e),N=!0)},o(e){Z(d.$$.fragment,e),Z(f.$$.fragment,e),Z(b.$$.fragment,e),Z(T.$$.fragment,e),Z($.$$.fragment,e),Z(_.$$.fragment,e),Z(c.$$.fragment,e),N=!1},d(e){e&&(l(u),l(p),l(P),l(X),l(k),l(M),l(C),l(h),l(V),l(y),l(H),l(w),l(Y),l(I),l(g),l(z),l(J),l(D),l(F),l(L),l(j),l(S),l(A),l(E),l(W)),l(n),B(d,e),B(f,e),B(b,e),B(T,e),B($,e),B(_,e),B(c,e)}}}const we='{"title":"DePlot","local":"deplot","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage example","local":"usage-example","sections":[],"depth":2},{"title":"Fine-tuning","local":"fine-tuning","sections":[],"depth":2}],"depth":1}';function be(G){return oe(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class _e extends me{constructor(n){super(),ue(this,n,be,ye,re,{})}}export{_e as component};
