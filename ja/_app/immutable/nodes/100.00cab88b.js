import{s as ut,o as _t,n as _e}from"../chunks/scheduler.9bc65507.js";import{S as Tt,i as bt,g as p,s as i,r as u,A as Mt,h,f as n,c as l,j as H,u as _,x as v,k as Q,y as d,a as m,v as T,d as b,t as M,w as y}from"../chunks/index.707bf1b6.js";import{T as dt}from"../chunks/Tip.c2ecdbf4.js";import{D as se}from"../chunks/Docstring.17db21ae.js";import{C as mt}from"../chunks/CodeBlock.54a9f38d.js";import{F as yt,M as gt}from"../chunks/Markdown.8ab98a13.js";import{E as ct}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as wt}from"../chunks/PipelineTag.44585822.js";import{H as De}from"../chunks/Heading.342b1fa6.js";function $t(C){let e,f="Example:",s,a,w;return a=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMERlaVRDb25maWclMkMlMjBEZWlUTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwRGVpVCUyMGRlaXQtYmFzZS1kaXN0aWxsZWQtcGF0Y2gxNi0yMjQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwRGVpVENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBkZWl0LWJhc2UtZGlzdGlsbGVkLXBhdGNoMTYtMjI0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBEZWlUTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> DeiTConfig, DeiTModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a DeiT deit-base-distilled-patch16-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = DeiTConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the deit-base-distilled-patch16-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DeiTModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){e=p("p"),e.textContent=f,s=i(),u(a.$$.fragment)},l(o){e=h(o,"P",{"data-svelte-h":!0}),v(e)!=="svelte-11lpom8"&&(e.textContent=f),s=l(o),_(a.$$.fragment,o)},m(o,F){m(o,e,F),m(o,s,F),T(a,o,F),w=!0},p:_e,i(o){w||(b(a.$$.fragment,o),w=!0)},o(o){M(a.$$.fragment,o),w=!1},d(o){o&&(n(e),n(s)),y(a,o)}}}function vt(C){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=p("p"),e.innerHTML=f},l(s){e=h(s,"P",{"data-svelte-h":!0}),v(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(s,a){m(s,e,a)},p:_e,d(s){s&&n(e)}}}function jt(C){let e,f="Example:",s,a,w;return a=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMERlaVRNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZWl0LWJhc2UtZGlzdGlsbGVkLXBhdGNoMTYtMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwRGVpVE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmRlaXQtYmFzZS1kaXN0aWxsZWQtcGF0Y2gxNi0yMjQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, DeiTModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DeiTModel.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">198</span>, <span class="hljs-number">768</span>]`,wrap:!1}}),{c(){e=p("p"),e.textContent=f,s=i(),u(a.$$.fragment)},l(o){e=h(o,"P",{"data-svelte-h":!0}),v(e)!=="svelte-11lpom8"&&(e.textContent=f),s=l(o),_(a.$$.fragment,o)},m(o,F){m(o,e,F),m(o,s,F),T(a,o,F),w=!0},p:_e,i(o){w||(b(a.$$.fragment,o),w=!0)},o(o){M(a.$$.fragment,o),w=!1},d(o){o&&(n(e),n(s)),y(a,o)}}}function Ft(C){let e,f=`Note that we provide a script to pre-train this model on custom data in our <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining" rel="nofollow">examples
directory</a>.`;return{c(){e=p("p"),e.innerHTML=f},l(s){e=h(s,"P",{"data-svelte-h":!0}),v(e)!=="svelte-7i3y9o"&&(e.innerHTML=f)},m(s,a){m(s,e,a)},p:_e,d(s){s&&n(e)}}}function Ct(C){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=p("p"),e.innerHTML=f},l(s){e=h(s,"P",{"data-svelte-h":!0}),v(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(s,a){m(s,e,a)},p:_e,d(s){s&&n(e)}}}function It(C){let e,f="Examples:",s,a,w;return a=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMERlaVRGb3JNYXNrZWRJbWFnZU1vZGVsaW5nJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZGVpdC1iYXNlLWRpc3RpbGxlZC1wYXRjaDE2LTIyNCUyMiklMEFtb2RlbCUyMCUzRCUyMERlaVRGb3JNYXNrZWRJbWFnZU1vZGVsaW5nLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmRlaXQtYmFzZS1kaXN0aWxsZWQtcGF0Y2gxNi0yMjQlMjIpJTBBJTBBbnVtX3BhdGNoZXMlMjAlM0QlMjAobW9kZWwuY29uZmlnLmltYWdlX3NpemUlMjAlMkYlMkYlMjBtb2RlbC5jb25maWcucGF0Y2hfc2l6ZSklMjAqKiUyMDIlMEFwaXhlbF92YWx1ZXMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKS5waXhlbF92YWx1ZXMlMEElMjMlMjBjcmVhdGUlMjByYW5kb20lMjBib29sZWFuJTIwbWFzayUyMG9mJTIwc2hhcGUlMjAoYmF0Y2hfc2l6ZSUyQyUyMG51bV9wYXRjaGVzKSUwQWJvb2xfbWFza2VkX3BvcyUyMCUzRCUyMHRvcmNoLnJhbmRpbnQobG93JTNEMCUyQyUyMGhpZ2glM0QyJTJDJTIwc2l6ZSUzRCgxJTJDJTIwbnVtX3BhdGNoZXMpKS5ib29sKCklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwocGl4ZWxfdmFsdWVzJTJDJTIwYm9vbF9tYXNrZWRfcG9zJTNEYm9vbF9tYXNrZWRfcG9zKSUwQWxvc3MlMkMlMjByZWNvbnN0cnVjdGVkX3BpeGVsX3ZhbHVlcyUyMCUzRCUyMG91dHB1dHMubG9zcyUyQyUyMG91dHB1dHMucmVjb25zdHJ1Y3Rpb24lMEFsaXN0KHJlY29uc3RydWN0ZWRfcGl4ZWxfdmFsdWVzLnNoYXBlKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, DeiTForMaskedImageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DeiTForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>num_patches = (model.config.image_size // model.config.patch_size) ** <span class="hljs-number">2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create random boolean mask of shape (batch_size, num_patches)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>bool_masked_pos = torch.randint(low=<span class="hljs-number">0</span>, high=<span class="hljs-number">2</span>, size=(<span class="hljs-number">1</span>, num_patches)).<span class="hljs-built_in">bool</span>()

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, reconstructed_pixel_values = outputs.loss, outputs.reconstruction
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(reconstructed_pixel_values.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>]`,wrap:!1}}),{c(){e=p("p"),e.textContent=f,s=i(),u(a.$$.fragment)},l(o){e=h(o,"P",{"data-svelte-h":!0}),v(e)!=="svelte-kvfsh7"&&(e.textContent=f),s=l(o),_(a.$$.fragment,o)},m(o,F){m(o,e,F),m(o,s,F),T(a,o,F),w=!0},p:_e,i(o){w||(b(a.$$.fragment,o),w=!0)},o(o){M(a.$$.fragment,o),w=!1},d(o){o&&(n(e),n(s)),y(a,o)}}}function kt(C){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=p("p"),e.innerHTML=f},l(s){e=h(s,"P",{"data-svelte-h":!0}),v(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(s,a){m(s,e,a)},p:_e,d(s){s&&n(e)}}}function xt(C){let e,f="Examples:",s,a,w;return a=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMERlaVRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXRvcmNoLm1hbnVhbF9zZWVkKDMpJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQSUyMyUyMG5vdGUlM0ElMjB3ZSUyMGFyZSUyMGxvYWRpbmclMjBhJTIwRGVpVEZvckltYWdlQ2xhc3NpZmljYXRpb25XaXRoVGVhY2hlciUyMGZyb20lMjB0aGUlMjBodWIlMjBoZXJlJTJDJTBBJTIzJTIwc28lMjB0aGUlMjBoZWFkJTIwd2lsbCUyMGJlJTIwcmFuZG9tbHklMjBpbml0aWFsaXplZCUyQyUyMGhlbmNlJTIwdGhlJTIwcHJlZGljdGlvbnMlMjB3aWxsJTIwYmUlMjByYW5kb20lMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZGVpdC1iYXNlLWRpc3RpbGxlZC1wYXRjaDE2LTIyNCUyMiklMEFtb2RlbCUyMCUzRCUyMERlaVRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmRlaXQtYmFzZS1kaXN0aWxsZWQtcGF0Y2gxNi0yMjQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbG9naXRzJTIwJTNEJTIwb3V0cHV0cy5sb2dpdHMlMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2NsYXNzX2lkeCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KCUyMlByZWRpY3RlZCUyMGNsYXNzJTNBJTIyJTJDJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2NsYXNzX2lkeCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, DeiTForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>torch.manual_seed(<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># note: we are loading a DeiTForImageClassificationWithTeacher from the hub here,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># so the head will be randomly initialized, hence the predictions will be random</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DeiTForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_idx = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[predicted_class_idx])
Predicted <span class="hljs-keyword">class</span>: magpie`,wrap:!1}}),{c(){e=p("p"),e.textContent=f,s=i(),u(a.$$.fragment)},l(o){e=h(o,"P",{"data-svelte-h":!0}),v(e)!=="svelte-kvfsh7"&&(e.textContent=f),s=l(o),_(a.$$.fragment,o)},m(o,F){m(o,e,F),m(o,s,F),T(a,o,F),w=!0},p:_e,i(o){w||(b(a.$$.fragment,o),w=!0)},o(o){M(a.$$.fragment,o),w=!1},d(o){o&&(n(e),n(s)),y(a,o)}}}function Wt(C){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=p("p"),e.innerHTML=f},l(s){e=h(s,"P",{"data-svelte-h":!0}),v(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(s,a){m(s,e,a)},p:_e,d(s){s&&n(e)}}}function Jt(C){let e,f="Example:",s,a,w;return a=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMERlaVRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uV2l0aFRlYWNoZXIlMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZGVpdC1iYXNlLWRpc3RpbGxlZC1wYXRjaDE2LTIyNCUyMiklMEFtb2RlbCUyMCUzRCUyMERlaVRGb3JJbWFnZUNsYXNzaWZpY2F0aW9uV2l0aFRlYWNoZXIuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZGVpdC1iYXNlLWRpc3RpbGxlZC1wYXRjaDE2LTIyNCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, DeiTForImageClassificationWithTeacher
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = DeiTForImageClassificationWithTeacher.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){e=p("p"),e.textContent=f,s=i(),u(a.$$.fragment)},l(o){e=h(o,"P",{"data-svelte-h":!0}),v(e)!=="svelte-11lpom8"&&(e.textContent=f),s=l(o),_(a.$$.fragment,o)},m(o,F){m(o,e,F),m(o,s,F),T(a,o,F),w=!0},p:_e,i(o){w||(b(a.$$.fragment,o),w=!0)},o(o){M(a.$$.fragment,o),w=!1},d(o){o&&(n(e),n(s)),y(a,o)}}}function Dt(C){let e,f,s,a,w,o,F=`The bare DeiT Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Ve,x,pe,Xe,ee,pt='The <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTModel">DeiTModel</a> forward method, overrides the <code>__call__</code> special method.',Ye,U,tt,S,Te,he,Ze,I,ae,je,ve,ot='DeiT Model with a decoder on top for masked image modeling, as proposed in <a href="https://arxiv.org/abs/2111.09886" rel="nofollow">SimMIM</a>.',Fe,Z,re,Re,Ue=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,ze,te,G,st,ne,Ce='The <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> forward method, overrides the <code>__call__</code> special method.',Ne,fe,z,ie,Be,q,at,A,O,Se,R,Y=`DeiT Model transformer with an image classification head on top (a linear layer on top of the final hidden state of
the [CLS] token) e.g. for ImageNet.`,Ie,be,Pe=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,nt,P,le,qe,B,Ee='The <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',de,ge,W,V,Le,E,ke,N,oe,lt,xe,Me=`DeiT Model transformer with image classification heads on top (a linear layer on top of the final hidden state of
the [CLS] token and a linear layer on top of the final hidden state of the distillation token) e.g. for ImageNet.`,Ae,K,ye=".. warning::",rt,we,ue=`This model supports inference-only. Fine-tuning with distillation (i.e. with a teacher) is not yet
supported.`,We,ce,$e=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,it,L,me,He,X,Qe='The <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a> forward method, overrides the <code>__call__</code> special method.',c,j,t,g,D;return e=new De({props:{title:"DeiTModel",local:"transformers.DeiTModel",headingTag:"h2"}}),a=new se({props:{name:"class transformers.DeiTModel",anchor:"transformers.DeiTModel",parameters:[{name:"config",val:": DeiTConfig"},{name:"add_pooling_layer",val:": bool = True"},{name:"use_mask_token",val:": bool = False"}],parametersDescription:[{anchor:"transformers.DeiTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_deit.py#L447"}}),pe=new se({props:{name:"forward",anchor:"transformers.DeiTModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.DeiTModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">DeiTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.DeiTModel.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DeiTModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DeiTModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DeiTModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DeiTModel.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>, <em>optional</em>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_deit.py#L476",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig"
>DeiTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),U=new dt({props:{$$slots:{default:[vt]},$$scope:{ctx:C}}}),S=new ct({props:{anchor:"transformers.DeiTModel.forward.example",$$slots:{default:[jt]},$$scope:{ctx:C}}}),he=new De({props:{title:"DeiTForMaskedImageModeling",local:"transformers.DeiTForMaskedImageModeling",headingTag:"h2"}}),ae=new se({props:{name:"class transformers.DeiTForMaskedImageModeling",anchor:"transformers.DeiTForMaskedImageModeling",parameters:[{name:"config",val:": DeiTConfig"}],parametersDescription:[{anchor:"transformers.DeiTForMaskedImageModeling.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_deit.py#L559"}}),Z=new dt({props:{$$slots:{default:[Ft]},$$scope:{ctx:C}}}),G=new se({props:{name:"forward",anchor:"transformers.DeiTForMaskedImageModeling.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"bool_masked_pos",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.DeiTForMaskedImageModeling.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">DeiTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.DeiTForMaskedImageModeling.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DeiTForMaskedImageModeling.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DeiTForMaskedImageModeling.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DeiTForMaskedImageModeling.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DeiTForMaskedImageModeling.forward.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>torch.BoolTensor</code> of shape <code>(batch_size, num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_deit.py#L589",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.MaskedImageModelingOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig"
>DeiTConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>bool_masked_pos</code> is provided) — Reconstruction loss.</li>
<li><strong>reconstruction</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Reconstructed / completed images.</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or</li>
<li><strong>when</strong> <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when</li>
<li><strong><code>config.output_attentions=True</code>):</strong>
Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the weighted average in
the self-attention heads.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.MaskedImageModelingOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),fe=new dt({props:{$$slots:{default:[Ct]},$$scope:{ctx:C}}}),ie=new ct({props:{anchor:"transformers.DeiTForMaskedImageModeling.forward.example",$$slots:{default:[It]},$$scope:{ctx:C}}}),q=new De({props:{title:"DeiTForImageClassification",local:"transformers.DeiTForImageClassification",headingTag:"h2"}}),O=new se({props:{name:"class transformers.DeiTForImageClassification",anchor:"transformers.DeiTForImageClassification",parameters:[{name:"config",val:": DeiTConfig"}],parametersDescription:[{anchor:"transformers.DeiTForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_deit.py#L676"}}),le=new se({props:{name:"forward",anchor:"transformers.DeiTForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.DeiTForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">DeiTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.DeiTForImageClassification.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DeiTForImageClassification.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DeiTForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DeiTForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.DeiTForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_deit.py#L696",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig"
>DeiTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states
(also called feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutput"
>transformers.modeling_outputs.ImageClassifierOutput</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ge=new dt({props:{$$slots:{default:[kt]},$$scope:{ctx:C}}}),V=new ct({props:{anchor:"transformers.DeiTForImageClassification.forward.example",$$slots:{default:[xt]},$$scope:{ctx:C}}}),E=new De({props:{title:"DeiTForImageClassificationWithTeacher",local:"transformers.DeiTForImageClassificationWithTeacher",headingTag:"h2"}}),oe=new se({props:{name:"class transformers.DeiTForImageClassificationWithTeacher",anchor:"transformers.DeiTForImageClassificationWithTeacher",parameters:[{name:"config",val:": DeiTConfig"}],parametersDescription:[{anchor:"transformers.DeiTForImageClassificationWithTeacher.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_deit.py#L821"}}),me=new se({props:{name:"forward",anchor:"transformers.DeiTForImageClassificationWithTeacher.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.DeiTForImageClassificationWithTeacher.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">DeiTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.DeiTForImageClassificationWithTeacher.forward.head_mask",description:`<strong>head_mask</strong> (<code>torch.FloatTensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.DeiTForImageClassificationWithTeacher.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.DeiTForImageClassificationWithTeacher.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.DeiTForImageClassificationWithTeacher.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_deit.py#L851",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.deit.modeling_deit.DeiTForImageClassificationWithTeacherOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig"
>DeiTConfig</a>) and inputs.</p>
<ul>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Prediction scores as the average of the cls_logits and distillation logits.</li>
<li><strong>cls_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Prediction scores of the classification head (i.e. the linear layer on top of the final hidden state of the
class token).</li>
<li><strong>distillation_logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Prediction scores of the distillation head (i.e. the linear layer on top of the final hidden state of the
distillation token).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of each layer
plus the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the weighted average in
the self-attention heads.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.deit.modeling_deit.DeiTForImageClassificationWithTeacherOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),j=new dt({props:{$$slots:{default:[Wt]},$$scope:{ctx:C}}}),g=new ct({props:{anchor:"transformers.DeiTForImageClassificationWithTeacher.forward.example",$$slots:{default:[Jt]},$$scope:{ctx:C}}}),{c(){u(e.$$.fragment),f=i(),s=p("div"),u(a.$$.fragment),w=i(),o=p("p"),o.innerHTML=F,Ve=i(),x=p("div"),u(pe.$$.fragment),Xe=i(),ee=p("p"),ee.innerHTML=pt,Ye=i(),u(U.$$.fragment),tt=i(),u(S.$$.fragment),Te=i(),u(he.$$.fragment),Ze=i(),I=p("div"),u(ae.$$.fragment),je=i(),ve=p("p"),ve.innerHTML=ot,Fe=i(),u(Z.$$.fragment),re=i(),Re=p("p"),Re.innerHTML=Ue,ze=i(),te=p("div"),u(G.$$.fragment),st=i(),ne=p("p"),ne.innerHTML=Ce,Ne=i(),u(fe.$$.fragment),z=i(),u(ie.$$.fragment),Be=i(),u(q.$$.fragment),at=i(),A=p("div"),u(O.$$.fragment),Se=i(),R=p("p"),R.textContent=Y,Ie=i(),be=p("p"),be.innerHTML=Pe,nt=i(),P=p("div"),u(le.$$.fragment),qe=i(),B=p("p"),B.innerHTML=Ee,de=i(),u(ge.$$.fragment),W=i(),u(V.$$.fragment),Le=i(),u(E.$$.fragment),ke=i(),N=p("div"),u(oe.$$.fragment),lt=i(),xe=p("p"),xe.textContent=Me,Ae=i(),K=p("p"),K.textContent=ye,rt=i(),we=p("p"),we.textContent=ue,We=i(),ce=p("p"),ce.innerHTML=$e,it=i(),L=p("div"),u(me.$$.fragment),He=i(),X=p("p"),X.innerHTML=Qe,c=i(),u(j.$$.fragment),t=i(),u(g.$$.fragment),this.h()},l(r){_(e.$$.fragment,r),f=l(r),s=h(r,"DIV",{class:!0});var $=H(s);_(a.$$.fragment,$),w=l($),o=h($,"P",{"data-svelte-h":!0}),v(o)!=="svelte-1g5epvj"&&(o.innerHTML=F),Ve=l($),x=h($,"DIV",{class:!0});var k=H(x);_(pe.$$.fragment,k),Xe=l(k),ee=h(k,"P",{"data-svelte-h":!0}),v(ee)!=="svelte-hajh57"&&(ee.innerHTML=pt),Ye=l(k),_(U.$$.fragment,k),tt=l(k),_(S.$$.fragment,k),k.forEach(n),$.forEach(n),Te=l(r),_(he.$$.fragment,r),Ze=l(r),I=h(r,"DIV",{class:!0});var J=H(I);_(ae.$$.fragment,J),je=l(J),ve=h(J,"P",{"data-svelte-h":!0}),v(ve)!=="svelte-1lv61v1"&&(ve.innerHTML=ot),Fe=l(J),_(Z.$$.fragment,J),re=l(J),Re=h(J,"P",{"data-svelte-h":!0}),v(Re)!=="svelte-1gjh92c"&&(Re.innerHTML=Ue),ze=l(J),te=h(J,"DIV",{class:!0});var Ge=H(te);_(G.$$.fragment,Ge),st=l(Ge),ne=h(Ge,"P",{"data-svelte-h":!0}),v(ne)!=="svelte-a5wdxz"&&(ne.innerHTML=Ce),Ne=l(Ge),_(fe.$$.fragment,Ge),z=l(Ge),_(ie.$$.fragment,Ge),Ge.forEach(n),J.forEach(n),Be=l(r),_(q.$$.fragment,r),at=l(r),A=h(r,"DIV",{class:!0});var Oe=H(A);_(O.$$.fragment,Oe),Se=l(Oe),R=h(Oe,"P",{"data-svelte-h":!0}),v(R)!=="svelte-1azp76l"&&(R.textContent=Y),Ie=l(Oe),be=h(Oe,"P",{"data-svelte-h":!0}),v(be)!=="svelte-1gjh92c"&&(be.innerHTML=Pe),nt=l(Oe),P=h(Oe,"DIV",{class:!0});var Ke=H(P);_(le.$$.fragment,Ke),qe=l(Ke),B=h(Ke,"P",{"data-svelte-h":!0}),v(B)!=="svelte-1i58pgv"&&(B.innerHTML=Ee),de=l(Ke),_(ge.$$.fragment,Ke),W=l(Ke),_(V.$$.fragment,Ke),Ke.forEach(n),Oe.forEach(n),Le=l(r),_(E.$$.fragment,r),ke=l(r),N=h(r,"DIV",{class:!0});var Je=H(N);_(oe.$$.fragment,Je),lt=l(Je),xe=h(Je,"P",{"data-svelte-h":!0}),v(xe)!=="svelte-17o4rh5"&&(xe.textContent=Me),Ae=l(Je),K=h(Je,"P",{"data-svelte-h":!0}),v(K)!=="svelte-zri3pk"&&(K.textContent=ye),rt=l(Je),we=h(Je,"P",{"data-svelte-h":!0}),v(we)!=="svelte-1gp6z48"&&(we.textContent=ue),We=l(Je),ce=h(Je,"P",{"data-svelte-h":!0}),v(ce)!=="svelte-1gjh92c"&&(ce.innerHTML=$e),it=l(Je),L=h(Je,"DIV",{class:!0});var et=H(L);_(me.$$.fragment,et),He=l(et),X=h(et,"P",{"data-svelte-h":!0}),v(X)!=="svelte-1qt0tnb"&&(X.innerHTML=Qe),c=l(et),_(j.$$.fragment,et),t=l(et),_(g.$$.fragment,et),et.forEach(n),Je.forEach(n),this.h()},h(){Q(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(s,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(te,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(r,$){T(e,r,$),m(r,f,$),m(r,s,$),T(a,s,null),d(s,w),d(s,o),d(s,Ve),d(s,x),T(pe,x,null),d(x,Xe),d(x,ee),d(x,Ye),T(U,x,null),d(x,tt),T(S,x,null),m(r,Te,$),T(he,r,$),m(r,Ze,$),m(r,I,$),T(ae,I,null),d(I,je),d(I,ve),d(I,Fe),T(Z,I,null),d(I,re),d(I,Re),d(I,ze),d(I,te),T(G,te,null),d(te,st),d(te,ne),d(te,Ne),T(fe,te,null),d(te,z),T(ie,te,null),m(r,Be,$),T(q,r,$),m(r,at,$),m(r,A,$),T(O,A,null),d(A,Se),d(A,R),d(A,Ie),d(A,be),d(A,nt),d(A,P),T(le,P,null),d(P,qe),d(P,B),d(P,de),T(ge,P,null),d(P,W),T(V,P,null),m(r,Le,$),T(E,r,$),m(r,ke,$),m(r,N,$),T(oe,N,null),d(N,lt),d(N,xe),d(N,Ae),d(N,K),d(N,rt),d(N,we),d(N,We),d(N,ce),d(N,it),d(N,L),T(me,L,null),d(L,He),d(L,X),d(L,c),T(j,L,null),d(L,t),T(g,L,null),D=!0},p(r,$){const k={};$&2&&(k.$$scope={dirty:$,ctx:r}),U.$set(k);const J={};$&2&&(J.$$scope={dirty:$,ctx:r}),S.$set(J);const Ge={};$&2&&(Ge.$$scope={dirty:$,ctx:r}),Z.$set(Ge);const Oe={};$&2&&(Oe.$$scope={dirty:$,ctx:r}),fe.$set(Oe);const Ke={};$&2&&(Ke.$$scope={dirty:$,ctx:r}),ie.$set(Ke);const Je={};$&2&&(Je.$$scope={dirty:$,ctx:r}),ge.$set(Je);const et={};$&2&&(et.$$scope={dirty:$,ctx:r}),V.$set(et);const ht={};$&2&&(ht.$$scope={dirty:$,ctx:r}),j.$set(ht);const ft={};$&2&&(ft.$$scope={dirty:$,ctx:r}),g.$set(ft)},i(r){D||(b(e.$$.fragment,r),b(a.$$.fragment,r),b(pe.$$.fragment,r),b(U.$$.fragment,r),b(S.$$.fragment,r),b(he.$$.fragment,r),b(ae.$$.fragment,r),b(Z.$$.fragment,r),b(G.$$.fragment,r),b(fe.$$.fragment,r),b(ie.$$.fragment,r),b(q.$$.fragment,r),b(O.$$.fragment,r),b(le.$$.fragment,r),b(ge.$$.fragment,r),b(V.$$.fragment,r),b(E.$$.fragment,r),b(oe.$$.fragment,r),b(me.$$.fragment,r),b(j.$$.fragment,r),b(g.$$.fragment,r),D=!0)},o(r){M(e.$$.fragment,r),M(a.$$.fragment,r),M(pe.$$.fragment,r),M(U.$$.fragment,r),M(S.$$.fragment,r),M(he.$$.fragment,r),M(ae.$$.fragment,r),M(Z.$$.fragment,r),M(G.$$.fragment,r),M(fe.$$.fragment,r),M(ie.$$.fragment,r),M(q.$$.fragment,r),M(O.$$.fragment,r),M(le.$$.fragment,r),M(ge.$$.fragment,r),M(V.$$.fragment,r),M(E.$$.fragment,r),M(oe.$$.fragment,r),M(me.$$.fragment,r),M(j.$$.fragment,r),M(g.$$.fragment,r),D=!1},d(r){r&&(n(f),n(s),n(Te),n(Ze),n(I),n(Be),n(at),n(A),n(Le),n(ke),n(N)),y(e,r),y(a),y(pe),y(U),y(S),y(he,r),y(ae),y(Z),y(G),y(fe),y(ie),y(q,r),y(O),y(le),y(ge),y(V),y(E,r),y(oe),y(me),y(j),y(g)}}}function Zt(C){let e,f;return e=new gt({props:{$$slots:{default:[Dt]},$$scope:{ctx:C}}}),{c(){u(e.$$.fragment)},l(s){_(e.$$.fragment,s)},m(s,a){T(e,s,a),f=!0},p(s,a){const w={};a&2&&(w.$$scope={dirty:a,ctx:s}),e.$set(w)},i(s){f||(b(e.$$.fragment,s),f=!0)},o(s){M(e.$$.fragment,s),f=!1},d(s){y(e,s)}}}function Ut(C){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=p("p"),e.innerHTML=f},l(s){e=h(s,"P",{"data-svelte-h":!0}),v(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(s,a){m(s,e,a)},p:_e,d(s){s&&n(e)}}}function zt(C){let e,f="Example:",s,a,w;return a=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGRGVpVE1vZGVsJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZWl0LWJhc2UtZGlzdGlsbGVkLXBhdGNoMTYtMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZEZWlUTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGZGVpdC1iYXNlLWRpc3RpbGxlZC1wYXRjaDE2LTIyNCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFDeiTModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDeiTModel.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">198</span>, <span class="hljs-number">768</span>]`,wrap:!1}}),{c(){e=p("p"),e.textContent=f,s=i(),u(a.$$.fragment)},l(o){e=h(o,"P",{"data-svelte-h":!0}),v(e)!=="svelte-11lpom8"&&(e.textContent=f),s=l(o),_(a.$$.fragment,o)},m(o,F){m(o,e,F),m(o,s,F),T(a,o,F),w=!0},p:_e,i(o){w||(b(a.$$.fragment,o),w=!0)},o(o){M(a.$$.fragment,o),w=!1},d(o){o&&(n(e),n(s)),y(a,o)}}}function Nt(C){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=p("p"),e.innerHTML=f},l(s){e=h(s,"P",{"data-svelte-h":!0}),v(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(s,a){m(s,e,a)},p:_e,d(s){s&&n(e)}}}function Gt(C){let e,f="Examples:",s,a,w;return a=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGRGVpVEZvck1hc2tlZEltYWdlTW9kZWxpbmclMEFpbXBvcnQlMjB0ZW5zb3JmbG93JTIwYXMlMjB0ZiUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZWl0LWJhc2UtZGlzdGlsbGVkLXBhdGNoMTYtMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZEZWlURm9yTWFza2VkSW1hZ2VNb2RlbGluZy5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZWl0LWJhc2UtZGlzdGlsbGVkLXBhdGNoMTYtMjI0JTIyKSUwQSUwQW51bV9wYXRjaGVzJTIwJTNEJTIwKG1vZGVsLmNvbmZpZy5pbWFnZV9zaXplJTIwJTJGJTJGJTIwbW9kZWwuY29uZmlnLnBhdGNoX3NpemUpJTIwKiolMjAyJTBBcGl4ZWxfdmFsdWVzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMikucGl4ZWxfdmFsdWVzJTBBJTIzJTIwY3JlYXRlJTIwcmFuZG9tJTIwYm9vbGVhbiUyMG1hc2slMjBvZiUyMHNoYXBlJTIwKGJhdGNoX3NpemUlMkMlMjBudW1fcGF0Y2hlcyklMEFib29sX21hc2tlZF9wb3MlMjAlM0QlMjB0Zi5jYXN0KHRmLnJhbmRvbS51bmlmb3JtKCgxJTJDJTIwbnVtX3BhdGNoZXMpJTJDJTIwbWludmFsJTNEMCUyQyUyMG1heHZhbCUzRDIlMkMlMjBkdHlwZSUzRHRmLmludDMyKSUyQyUyMHRmLmJvb2wpJTBBJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKHBpeGVsX3ZhbHVlcyUyQyUyMGJvb2xfbWFza2VkX3BvcyUzRGJvb2xfbWFza2VkX3BvcyklMEFsb3NzJTJDJTIwcmVjb25zdHJ1Y3RlZF9waXhlbF92YWx1ZXMlMjAlM0QlMjBvdXRwdXRzLmxvc3MlMkMlMjBvdXRwdXRzLnJlY29uc3RydWN0aW9uJTBBbGlzdChyZWNvbnN0cnVjdGVkX3BpeGVsX3ZhbHVlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFDeiTForMaskedImageModeling
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDeiTForMaskedImageModeling.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>num_patches = (model.config.image_size // model.config.patch_size) ** <span class="hljs-number">2</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>pixel_values = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>).pixel_values
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># create random boolean mask of shape (batch_size, num_patches)</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>bool_masked_pos = tf.cast(tf.random.uniform((<span class="hljs-number">1</span>, num_patches), minval=<span class="hljs-number">0</span>, maxval=<span class="hljs-number">2</span>, dtype=tf.int32), tf.<span class="hljs-built_in">bool</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)
<span class="hljs-meta">&gt;&gt;&gt; </span>loss, reconstructed_pixel_values = outputs.loss, outputs.reconstruction
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(reconstructed_pixel_values.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>]`,wrap:!1}}),{c(){e=p("p"),e.textContent=f,s=i(),u(a.$$.fragment)},l(o){e=h(o,"P",{"data-svelte-h":!0}),v(e)!=="svelte-kvfsh7"&&(e.textContent=f),s=l(o),_(a.$$.fragment,o)},m(o,F){m(o,e,F),m(o,s,F),T(a,o,F),w=!0},p:_e,i(o){w||(b(a.$$.fragment,o),w=!0)},o(o){M(a.$$.fragment,o),w=!1},d(o){o&&(n(e),n(s)),y(a,o)}}}function Rt(C){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=p("p"),e.innerHTML=f},l(s){e=h(s,"P",{"data-svelte-h":!0}),v(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(s,a){m(s,e,a)},p:_e,d(s){s&&n(e)}}}function Bt(C){let e,f="Examples:",s,a,w;return a=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGRGVpVEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0ZW5zb3JmbG93JTIwYXMlMjB0ZiUwQWZyb20lMjBQSUwlMjBpbXBvcnQlMjBJbWFnZSUwQWltcG9ydCUyMHJlcXVlc3RzJTBBJTBBa2VyYXMudXRpbHMuc2V0X3JhbmRvbV9zZWVkKDMpJTBBdXJsJTIwJTNEJTIwJTIyaHR0cCUzQSUyRiUyRmltYWdlcy5jb2NvZGF0YXNldC5vcmclMkZ2YWwyMDE3JTJGMDAwMDAwMDM5NzY5LmpwZyUyMiUwQWltYWdlJTIwJTNEJTIwSW1hZ2Uub3BlbihyZXF1ZXN0cy5nZXQodXJsJTJDJTIwc3RyZWFtJTNEVHJ1ZSkucmF3KSUwQSUwQSUyMyUyMG5vdGUlM0ElMjB3ZSUyMGFyZSUyMGxvYWRpbmclMjBhJTIwVEZEZWlURm9ySW1hZ2VDbGFzc2lmaWNhdGlvbldpdGhUZWFjaGVyJTIwZnJvbSUyMHRoZSUyMGh1YiUyMGhlcmUlMkMlMEElMjMlMjBzbyUyMHRoZSUyMGhlYWQlMjB3aWxsJTIwYmUlMjByYW5kb21seSUyMGluaXRpYWxpemVkJTJDJTIwaGVuY2UlMjB0aGUlMjBwcmVkaWN0aW9ucyUyMHdpbGwlMjBiZSUyMHJhbmRvbSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZWl0LWJhc2UtZGlzdGlsbGVkLXBhdGNoMTYtMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZEZWlURm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZWl0LWJhc2UtZGlzdGlsbGVkLXBhdGNoMTYtMjI0JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIydGYlMjIpJTBBb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQWxvZ2l0cyUyMCUzRCUyMG91dHB1dHMubG9naXRzJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9jbGFzc19pZHglMjAlM0QlMjB0Zi5tYXRoLmFyZ21heChsb2dpdHMlMkMlMjBheGlzJTNELTEpJTVCMCU1RCUwQXByaW50KCUyMlByZWRpY3RlZCUyMGNsYXNzJTNBJTIyJTJDJTIwbW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCaW50KHByZWRpY3RlZF9jbGFzc19pZHgpJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFDeiTForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>keras.utils.set_random_seed(<span class="hljs-number">3</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># note: we are loading a TFDeiTForImageClassificationWithTeacher from the hub here,</span>
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># so the head will be randomly initialized, hence the predictions will be random</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDeiTForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_idx = tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[<span class="hljs-built_in">int</span>(predicted_class_idx)])
Predicted <span class="hljs-keyword">class</span>: little blue heron, Egretta caerulea`,wrap:!1}}),{c(){e=p("p"),e.textContent=f,s=i(),u(a.$$.fragment)},l(o){e=h(o,"P",{"data-svelte-h":!0}),v(e)!=="svelte-kvfsh7"&&(e.textContent=f),s=l(o),_(a.$$.fragment,o)},m(o,F){m(o,e,F),m(o,s,F),T(a,o,F),w=!0},p:_e,i(o){w||(b(a.$$.fragment,o),w=!0)},o(o){M(a.$$.fragment,o),w=!1},d(o){o&&(n(e),n(s)),y(a,o)}}}function Vt(C){let e,f=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=p("p"),e.innerHTML=f},l(s){e=h(s,"P",{"data-svelte-h":!0}),v(e)!=="svelte-fincs2"&&(e.innerHTML=f)},m(s,a){m(s,e,a)},p:_e,d(s){s&&n(e)}}}function Xt(C){let e,f="Example:",s,a,w;return a=new mt({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGRGVpVEZvckltYWdlQ2xhc3NpZmljYXRpb25XaXRoVGVhY2hlciUwQWltcG9ydCUyMHRlbnNvcmZsb3clMjBhcyUyMHRmJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZkZWl0LWJhc2UtZGlzdGlsbGVkLXBhdGNoMTYtMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZEZWlURm9ySW1hZ2VDbGFzc2lmaWNhdGlvbldpdGhUZWFjaGVyLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmRlaXQtYmFzZS1kaXN0aWxsZWQtcGF0Y2gxNi0yMjQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJ0ZiUyMiklMEFsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGludCh0Zi5tYXRoLmFyZ21heChsb2dpdHMlMkMlMjBheGlzJTNELTEpKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFDeiTForImageClassificationWithTeacher
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFDeiTForImageClassificationWithTeacher.from_pretrained(<span class="hljs-string">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){e=p("p"),e.textContent=f,s=i(),u(a.$$.fragment)},l(o){e=h(o,"P",{"data-svelte-h":!0}),v(e)!=="svelte-11lpom8"&&(e.textContent=f),s=l(o),_(a.$$.fragment,o)},m(o,F){m(o,e,F),m(o,s,F),T(a,o,F),w=!0},p:_e,i(o){w||(b(a.$$.fragment,o),w=!0)},o(o){M(a.$$.fragment,o),w=!1},d(o){o&&(n(e),n(s)),y(a,o)}}}function Yt(C){let e,f,s,a,w,o,F=`The bare DeiT Model transformer outputting raw hidden-states without any specific head on top.
This model is a TensorFlow
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" rel="nofollow">keras.layers.Layer</a>. Use it as a regular
TensorFlow Module and refer to the TensorFlow documentation for all matter related to general usage and behavior.`,Ve,x,pe,Xe,ee,pt='The <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTModel">TFDeiTModel</a> forward method, overrides the <code>__call__</code> special method.',Ye,U,tt,S,Te,he,Ze,I,ae,je,ve,ot=`DeiT Model with a decoder on top for masked image modeling, as proposed in <a href="https://arxiv.org/abs/2111.09886" rel="nofollow">SimMIM</a>.
This model is a TensorFlow
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" rel="nofollow">keras.layers.Layer</a>. Use it as a regular
TensorFlow Module and refer to the TensorFlow documentation for all matter related to general usage and behavior.`,Fe,Z,re,Re,Ue,ze='The <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTForMaskedImageModeling">TFDeiTForMaskedImageModeling</a> forward method, overrides the <code>__call__</code> special method.',te,G,st,ne,Ce,Ne,fe,z,ie,Be,q,at=`DeiT Model transformer with an image classification head on top (a linear layer on top of the final hidden state of
the [CLS] token) e.g. for ImageNet.`,A,O,Se=`This model is a TensorFlow
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" rel="nofollow">keras.layers.Layer</a>. Use it as a regular
TensorFlow Module and refer to the TensorFlow documentation for all matter related to general usage and behavior.`,R,Y,Ie,be,Pe,nt='The <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTForImageClassification">TFDeiTForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',P,le,qe,B,Ee,de,ge,W,V,Le,E,ke=`DeiT Model transformer with image classification heads on top (a linear layer on top of the final hidden state of
the [CLS] token and a linear layer on top of the final hidden state of the distillation token) e.g. for ImageNet.`,N,oe,lt=".. warning::",xe,Me,Ae=`This model supports inference-only. Fine-tuning with distillation (i.e. with a teacher) is not yet
supported.`,K,ye,rt=`This model is a TensorFlow
<a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer" rel="nofollow">keras.layers.Layer</a>. Use it as a regular
TensorFlow Module and refer to the TensorFlow documentation for all matter related to general usage and behavior.`,we,ue,We,ce,$e,it='The <a href="/docs/transformers/main/ja/model_doc/deit#transformers.TFDeiTForImageClassificationWithTeacher">TFDeiTForImageClassificationWithTeacher</a> forward method, overrides the <code>__call__</code> special method.',L,me,He,X,Qe;return e=new De({props:{title:"TFDeiTModel",local:"transformers.TFDeiTModel",headingTag:"h2"}}),a=new se({props:{name:"class transformers.TFDeiTModel",anchor:"transformers.TFDeiTModel",parameters:[{name:"config",val:": DeiTConfig"},{name:"add_pooling_layer",val:": bool = True"},{name:"use_mask_token",val:": bool = False"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFDeiTModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_tf_deit.py#L720"}}),pe=new se({props:{name:"call",anchor:"transformers.TFDeiTModel.call",parameters:[{name:"pixel_values",val:": tf.Tensor | None = None"},{name:"bool_masked_pos",val:": tf.Tensor | None = None"},{name:"head_mask",val:": tf.Tensor | None = None"},{name:"output_attentions",val:": Optional[bool] = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDeiTModel.call.pixel_values",description:`<strong>pixel_values</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">DeiTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFDeiTModel.call.head_mask",description:`<strong>head_mask</strong> (<code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDeiTModel.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TFDeiTModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TFDeiTModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_tf_deit.py#L734",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"
>transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling</a> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig"
>DeiTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) further processed by a
Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
prediction (classification) objective during pretraining.</p>
<p>This output is usually <em>not</em> a good summary of the semantic content of the input, you’re often better with
averaging or pooling the sequence of hidden-states for the whole input sequence.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling"
>transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling</a> or <code>tuple(tf.Tensor)</code></p>
`}}),U=new dt({props:{$$slots:{default:[Ut]},$$scope:{ctx:C}}}),S=new ct({props:{anchor:"transformers.TFDeiTModel.call.example",$$slots:{default:[zt]},$$scope:{ctx:C}}}),he=new De({props:{title:"TFDeiTForMaskedImageModeling",local:"transformers.TFDeiTForMaskedImageModeling",headingTag:"h2"}}),ae=new se({props:{name:"class transformers.TFDeiTForMaskedImageModeling",anchor:"transformers.TFDeiTForMaskedImageModeling",parameters:[{name:"config",val:": DeiTConfig"}],parametersDescription:[{anchor:"transformers.TFDeiTForMaskedImageModeling.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_tf_deit.py#L856"}}),re=new se({props:{name:"call",anchor:"transformers.TFDeiTForMaskedImageModeling.call",parameters:[{name:"pixel_values",val:": tf.Tensor | None = None"},{name:"bool_masked_pos",val:": tf.Tensor | None = None"},{name:"head_mask",val:": tf.Tensor | None = None"},{name:"output_attentions",val:": Optional[bool] = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDeiTForMaskedImageModeling.call.pixel_values",description:`<strong>pixel_values</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">DeiTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFDeiTForMaskedImageModeling.call.head_mask",description:`<strong>head_mask</strong> (<code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDeiTForMaskedImageModeling.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TFDeiTForMaskedImageModeling.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TFDeiTForMaskedImageModeling.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.TFDeiTForMaskedImageModeling.call.bool_masked_pos",description:`<strong>bool_masked_pos</strong> (<code>tf.Tensor</code> of type bool and shape <code>(batch_size, num_patches)</code>) &#x2014;
Boolean masked positions. Indicates which patches are masked (1) and which aren&#x2019;t (0).`,name:"bool_masked_pos"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_tf_deit.py#L868",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_tf_outputs.TFMaskedImageModelingOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig"
>DeiTConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>bool_masked_pos</code> is provided) — Reconstruction loss.</li>
<li><strong>reconstruction</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Reconstructed / completed images.</li>
<li><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when</li>
<li><strong><code>config.output_hidden_states=True</code>):</strong>
Tuple of <code>tf.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, + one for
the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states (also called
feature maps) of the model at the output of each stage.</li>
<li><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when</li>
<li><strong><code>config.output_attentions=True</code>):</strong>
Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.
Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_tf_outputs.TFMaskedImageModelingOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),G=new dt({props:{$$slots:{default:[Nt]},$$scope:{ctx:C}}}),ne=new ct({props:{anchor:"transformers.TFDeiTForMaskedImageModeling.call.example",$$slots:{default:[Gt]},$$scope:{ctx:C}}}),Ne=new De({props:{title:"TFDeiTForImageClassification",local:"transformers.TFDeiTForImageClassification",headingTag:"h2"}}),ie=new se({props:{name:"class transformers.TFDeiTForImageClassification",anchor:"transformers.TFDeiTForImageClassification",parameters:[{name:"config",val:": DeiTConfig"}],parametersDescription:[{anchor:"transformers.TFDeiTForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_tf_deit.py#L980"}}),Ie=new se({props:{name:"call",anchor:"transformers.TFDeiTForImageClassification.call",parameters:[{name:"pixel_values",val:": tf.Tensor | None = None"},{name:"head_mask",val:": tf.Tensor | None = None"},{name:"labels",val:": tf.Tensor | None = None"},{name:"output_attentions",val:": Optional[bool] = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDeiTForImageClassification.call.pixel_values",description:`<strong>pixel_values</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">DeiTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFDeiTForImageClassification.call.head_mask",description:`<strong>head_mask</strong> (<code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDeiTForImageClassification.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TFDeiTForImageClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TFDeiTForImageClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.TFDeiTForImageClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_tf_deit.py#L1002",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_tf_outputs.TFImageClassifierOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig"
>DeiTConfig</a>) and inputs.</p>
<ul>
<li>
<p><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</p>
</li>
<li>
<p><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, + one for
the output of each stage) of shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states (also called
feature maps) of the model at the output of each stage.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, patch_size, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_tf_outputs.TFImageClassifierOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),le=new dt({props:{$$slots:{default:[Rt]},$$scope:{ctx:C}}}),B=new ct({props:{anchor:"transformers.TFDeiTForImageClassification.call.example",$$slots:{default:[Bt]},$$scope:{ctx:C}}}),de=new De({props:{title:"TFDeiTForImageClassificationWithTeacher",local:"transformers.TFDeiTForImageClassificationWithTeacher",headingTag:"h2"}}),V=new se({props:{name:"class transformers.TFDeiTForImageClassificationWithTeacher",anchor:"transformers.TFDeiTForImageClassificationWithTeacher",parameters:[{name:"config",val:": DeiTConfig"}],parametersDescription:[{anchor:"transformers.TFDeiTForImageClassificationWithTeacher.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig">DeiTConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_tf_deit.py#L1089"}}),We=new se({props:{name:"call",anchor:"transformers.TFDeiTForImageClassificationWithTeacher.call",parameters:[{name:"pixel_values",val:": tf.Tensor | None = None"},{name:"head_mask",val:": tf.Tensor | None = None"},{name:"output_attentions",val:": Optional[bool] = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFDeiTForImageClassificationWithTeacher.call.pixel_values",description:`<strong>pixel_values</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">DeiTImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFDeiTForImageClassificationWithTeacher.call.head_mask",description:`<strong>head_mask</strong> (<code>tf.Tensor</code> of shape <code>(num_heads,)</code> or <code>(num_layers, num_heads)</code>, <em>optional</em>) &#x2014;
Mask to nullify selected heads of the self-attention modules. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 indicates the head is <strong>not masked</strong>,</li>
<li>0 indicates the head is <strong>masked</strong>.</li>
</ul>`,name:"head_mask"},{anchor:"transformers.TFDeiTForImageClassificationWithTeacher.call.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.TFDeiTForImageClassificationWithTeacher.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.TFDeiTForImageClassificationWithTeacher.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/modeling_tf_deit.py#L1121",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.deit.modeling_tf_deit.TFDeiTForImageClassificationWithTeacherOutput</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTConfig"
>DeiTConfig</a>) and inputs.</p>
<ul>
<li><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Prediction scores as the average of the cls_logits and distillation logits.</li>
<li><strong>cls_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Prediction scores of the classification head (i.e. the linear layer on top of the final hidden state of the
class token).</li>
<li><strong>distillation_logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Prediction scores of the distillation head (i.e. the linear layer on top of the final hidden state of the
distillation token).</li>
<li><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of each layer plus
the initial embedding outputs.</li>
<li><strong>attentions</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>tf.Tensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>. Attentions weights after the attention softmax, used to compute the weighted average in
the self-attention heads.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.deit.modeling_tf_deit.TFDeiTForImageClassificationWithTeacherOutput</code> or <code>tuple(tf.Tensor)</code></p>
`}}),me=new dt({props:{$$slots:{default:[Vt]},$$scope:{ctx:C}}}),X=new ct({props:{anchor:"transformers.TFDeiTForImageClassificationWithTeacher.call.example",$$slots:{default:[Xt]},$$scope:{ctx:C}}}),{c(){u(e.$$.fragment),f=i(),s=p("div"),u(a.$$.fragment),w=i(),o=p("p"),o.innerHTML=F,Ve=i(),x=p("div"),u(pe.$$.fragment),Xe=i(),ee=p("p"),ee.innerHTML=pt,Ye=i(),u(U.$$.fragment),tt=i(),u(S.$$.fragment),Te=i(),u(he.$$.fragment),Ze=i(),I=p("div"),u(ae.$$.fragment),je=i(),ve=p("p"),ve.innerHTML=ot,Fe=i(),Z=p("div"),u(re.$$.fragment),Re=i(),Ue=p("p"),Ue.innerHTML=ze,te=i(),u(G.$$.fragment),st=i(),u(ne.$$.fragment),Ce=i(),u(Ne.$$.fragment),fe=i(),z=p("div"),u(ie.$$.fragment),Be=i(),q=p("p"),q.textContent=at,A=i(),O=p("p"),O.innerHTML=Se,R=i(),Y=p("div"),u(Ie.$$.fragment),be=i(),Pe=p("p"),Pe.innerHTML=nt,P=i(),u(le.$$.fragment),qe=i(),u(B.$$.fragment),Ee=i(),u(de.$$.fragment),ge=i(),W=p("div"),u(V.$$.fragment),Le=i(),E=p("p"),E.textContent=ke,N=i(),oe=p("p"),oe.textContent=lt,xe=i(),Me=p("p"),Me.textContent=Ae,K=i(),ye=p("p"),ye.innerHTML=rt,we=i(),ue=p("div"),u(We.$$.fragment),ce=i(),$e=p("p"),$e.innerHTML=it,L=i(),u(me.$$.fragment),He=i(),u(X.$$.fragment),this.h()},l(c){_(e.$$.fragment,c),f=l(c),s=h(c,"DIV",{class:!0});var j=H(s);_(a.$$.fragment,j),w=l(j),o=h(j,"P",{"data-svelte-h":!0}),v(o)!=="svelte-12cbu7j"&&(o.innerHTML=F),Ve=l(j),x=h(j,"DIV",{class:!0});var t=H(x);_(pe.$$.fragment,t),Xe=l(t),ee=h(t,"P",{"data-svelte-h":!0}),v(ee)!=="svelte-brrvij"&&(ee.innerHTML=pt),Ye=l(t),_(U.$$.fragment,t),tt=l(t),_(S.$$.fragment,t),t.forEach(n),j.forEach(n),Te=l(c),_(he.$$.fragment,c),Ze=l(c),I=h(c,"DIV",{class:!0});var g=H(I);_(ae.$$.fragment,g),je=l(g),ve=h(g,"P",{"data-svelte-h":!0}),v(ve)!=="svelte-1phjouv"&&(ve.innerHTML=ot),Fe=l(g),Z=h(g,"DIV",{class:!0});var D=H(Z);_(re.$$.fragment,D),Re=l(D),Ue=h(D,"P",{"data-svelte-h":!0}),v(Ue)!=="svelte-2m0uqf"&&(Ue.innerHTML=ze),te=l(D),_(G.$$.fragment,D),st=l(D),_(ne.$$.fragment,D),D.forEach(n),g.forEach(n),Ce=l(c),_(Ne.$$.fragment,c),fe=l(c),z=h(c,"DIV",{class:!0});var r=H(z);_(ie.$$.fragment,r),Be=l(r),q=h(r,"P",{"data-svelte-h":!0}),v(q)!=="svelte-1azp76l"&&(q.textContent=at),A=l(r),O=h(r,"P",{"data-svelte-h":!0}),v(O)!=="svelte-1jzqql0"&&(O.innerHTML=Se),R=l(r),Y=h(r,"DIV",{class:!0});var $=H(Y);_(Ie.$$.fragment,$),be=l($),Pe=h($,"P",{"data-svelte-h":!0}),v(Pe)!=="svelte-1dh0iqn"&&(Pe.innerHTML=nt),P=l($),_(le.$$.fragment,$),qe=l($),_(B.$$.fragment,$),$.forEach(n),r.forEach(n),Ee=l(c),_(de.$$.fragment,c),ge=l(c),W=h(c,"DIV",{class:!0});var k=H(W);_(V.$$.fragment,k),Le=l(k),E=h(k,"P",{"data-svelte-h":!0}),v(E)!=="svelte-17o4rh5"&&(E.textContent=ke),N=l(k),oe=h(k,"P",{"data-svelte-h":!0}),v(oe)!=="svelte-zri3pk"&&(oe.textContent=lt),xe=l(k),Me=h(k,"P",{"data-svelte-h":!0}),v(Me)!=="svelte-1gp6z48"&&(Me.textContent=Ae),K=l(k),ye=h(k,"P",{"data-svelte-h":!0}),v(ye)!=="svelte-1jzqql0"&&(ye.innerHTML=rt),we=l(k),ue=h(k,"DIV",{class:!0});var J=H(ue);_(We.$$.fragment,J),ce=l(J),$e=h(J,"P",{"data-svelte-h":!0}),v($e)!=="svelte-g5hxsv"&&($e.innerHTML=it),L=l(J),_(me.$$.fragment,J),He=l(J),_(X.$$.fragment,J),J.forEach(n),k.forEach(n),this.h()},h(){Q(x,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(s,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(Y,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(ue,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(c,j){T(e,c,j),m(c,f,j),m(c,s,j),T(a,s,null),d(s,w),d(s,o),d(s,Ve),d(s,x),T(pe,x,null),d(x,Xe),d(x,ee),d(x,Ye),T(U,x,null),d(x,tt),T(S,x,null),m(c,Te,j),T(he,c,j),m(c,Ze,j),m(c,I,j),T(ae,I,null),d(I,je),d(I,ve),d(I,Fe),d(I,Z),T(re,Z,null),d(Z,Re),d(Z,Ue),d(Z,te),T(G,Z,null),d(Z,st),T(ne,Z,null),m(c,Ce,j),T(Ne,c,j),m(c,fe,j),m(c,z,j),T(ie,z,null),d(z,Be),d(z,q),d(z,A),d(z,O),d(z,R),d(z,Y),T(Ie,Y,null),d(Y,be),d(Y,Pe),d(Y,P),T(le,Y,null),d(Y,qe),T(B,Y,null),m(c,Ee,j),T(de,c,j),m(c,ge,j),m(c,W,j),T(V,W,null),d(W,Le),d(W,E),d(W,N),d(W,oe),d(W,xe),d(W,Me),d(W,K),d(W,ye),d(W,we),d(W,ue),T(We,ue,null),d(ue,ce),d(ue,$e),d(ue,L),T(me,ue,null),d(ue,He),T(X,ue,null),Qe=!0},p(c,j){const t={};j&2&&(t.$$scope={dirty:j,ctx:c}),U.$set(t);const g={};j&2&&(g.$$scope={dirty:j,ctx:c}),S.$set(g);const D={};j&2&&(D.$$scope={dirty:j,ctx:c}),G.$set(D);const r={};j&2&&(r.$$scope={dirty:j,ctx:c}),ne.$set(r);const $={};j&2&&($.$$scope={dirty:j,ctx:c}),le.$set($);const k={};j&2&&(k.$$scope={dirty:j,ctx:c}),B.$set(k);const J={};j&2&&(J.$$scope={dirty:j,ctx:c}),me.$set(J);const Ge={};j&2&&(Ge.$$scope={dirty:j,ctx:c}),X.$set(Ge)},i(c){Qe||(b(e.$$.fragment,c),b(a.$$.fragment,c),b(pe.$$.fragment,c),b(U.$$.fragment,c),b(S.$$.fragment,c),b(he.$$.fragment,c),b(ae.$$.fragment,c),b(re.$$.fragment,c),b(G.$$.fragment,c),b(ne.$$.fragment,c),b(Ne.$$.fragment,c),b(ie.$$.fragment,c),b(Ie.$$.fragment,c),b(le.$$.fragment,c),b(B.$$.fragment,c),b(de.$$.fragment,c),b(V.$$.fragment,c),b(We.$$.fragment,c),b(me.$$.fragment,c),b(X.$$.fragment,c),Qe=!0)},o(c){M(e.$$.fragment,c),M(a.$$.fragment,c),M(pe.$$.fragment,c),M(U.$$.fragment,c),M(S.$$.fragment,c),M(he.$$.fragment,c),M(ae.$$.fragment,c),M(re.$$.fragment,c),M(G.$$.fragment,c),M(ne.$$.fragment,c),M(Ne.$$.fragment,c),M(ie.$$.fragment,c),M(Ie.$$.fragment,c),M(le.$$.fragment,c),M(B.$$.fragment,c),M(de.$$.fragment,c),M(V.$$.fragment,c),M(We.$$.fragment,c),M(me.$$.fragment,c),M(X.$$.fragment,c),Qe=!1},d(c){c&&(n(f),n(s),n(Te),n(Ze),n(I),n(Ce),n(fe),n(z),n(Ee),n(ge),n(W)),y(e,c),y(a),y(pe),y(U),y(S),y(he,c),y(ae),y(re),y(G),y(ne),y(Ne,c),y(ie),y(Ie),y(le),y(B),y(de,c),y(V),y(We),y(me),y(X)}}}function Pt(C){let e,f;return e=new gt({props:{$$slots:{default:[Yt]},$$scope:{ctx:C}}}),{c(){u(e.$$.fragment)},l(s){_(e.$$.fragment,s)},m(s,a){T(e,s,a),f=!0},p(s,a){const w={};a&2&&(w.$$scope={dirty:a,ctx:s}),e.$set(w)},i(s){f||(b(e.$$.fragment,s),f=!0)},o(s){M(e.$$.fragment,s),f=!1},d(s){y(e,s)}}}function Et(C){let e,f,s,a,w,o,F,Ve,x,pe=`DeiT モデルは、Hugo Touvron、Matthieu Cord、Matthijs Douze、Francisco Massa、Alexandre
Sablayrolles, Hervé Jégou.によって <a href="https://arxiv.org/abs/2012.12877" rel="nofollow">Training data-efficient image Transformers &amp; distillation through attention</a> で提案されました。
サブレイロール、エルヴェ・ジェグー。 <a href="https://arxiv.org/abs/2010.11929" rel="nofollow">Dosovitskiy et al., 2020</a> で紹介された <a href="vit">Vision Transformer (ViT)</a> は、既存の畳み込みニューラルと同等、またはそれを上回るパフォーマンスを発揮できることを示しました。
Transformer エンコーダ (BERT のような) を使用したネットワーク。ただし、その論文で紹介された ViT モデルには、次のトレーニングが必要でした。
外部データを使用して、数週間にわたる高価なインフラストラクチャ。 DeiT (データ効率の高い画像変換器) はさらに優れています
画像分類用に効率的にトレーニングされたトランスフォーマーにより、必要なデータとコンピューティング リソースがはるかに少なくなります。
オリジナルの ViT モデルとの比較。`,Xe,ee,pt="論文の要約は次のとおりです。",Ye,U,tt=`<em>最近、純粋に注意に基づくニューラル ネットワークが、画像などの画像理解タスクに対処できることが示されました。
分類。ただし、これらのビジュアル トランスフォーマーは、
インフラストラクチャが高価であるため、その採用が制限されています。この作業では、コンボリューションフリーの競争力のあるゲームを作成します。
Imagenet のみでトレーニングしてトランスフォーマーを作成します。 1 台のコンピューターで 3 日以内にトレーニングを行います。私たちの基準となるビジョン
トランス (86M パラメータ) は、外部なしで ImageNet 上で 83.1% (単一クロップ評価) のトップ 1 の精度を達成します。
データ。さらに重要なのは、トランスフォーマーに特有の教師と生徒の戦略を導入することです。蒸留に依存している
学生が注意を払って教師から学ぶことを保証するトークン。私たちはこのトークンベースに興味を示します
特に convnet を教師として使用する場合。これにより、convnet と競合する結果を報告できるようになります。
Imagenet (最大 85.2% の精度が得られます) と他のタスクに転送するときの両方で。私たちはコードを共有し、
モデル。</em>`,S,Te,he='このモデルは、<a href="https://huggingface.co/nielsr" rel="nofollow">nielsr</a> によって提供されました。このモデルの TensorFlow バージョンは、<a href="https://huggingface.co/amyeroberts" rel="nofollow">amyeroberts</a> によって追加されました。',Ze,I,ae,je,ve=`<li>ViT と比較して、DeiT モデルはいわゆる蒸留トークンを使用して教師から効果的に学習します (これは、
DeiT 論文は、ResNet のようなモデルです)。蒸留トークンは、バックプロパゲーションを通じて、と対話することによって学習されます。
セルフアテンション層を介したクラス ([CLS]) とパッチ トークン。</li> <li>抽出されたモデルを微調整するには 2 つの方法があります。(1) 上部に予測ヘッドを配置するだけの古典的な方法。
クラス トークンの最終的な非表示状態を抽出し、蒸留シグナルを使用しない、または (2) 両方の
予測ヘッドはクラス トークンの上と蒸留トークンの上にあります。その場合、[CLS] 予測は
head は、head の予測とグラウンド トゥルース ラベル間の通常のクロスエントロピーを使用してトレーニングされます。
蒸留予測ヘッドは、硬蒸留 (予測と予測の間のクロスエントロピー) を使用してトレーニングされます。
蒸留ヘッドと教師が予測したラベル）。推論時に、平均予測を取得します。
最終的な予測として両頭の間で。 (2) は「蒸留による微調整」とも呼ばれます。
下流のデータセットですでに微調整されている教師。モデル的には (1) に相当します。
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> と (2) に対応します。
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForImageClassificationWithTeacher">DeiTForImageClassificationWithTeacher</a>。</li> <li>著者らは (2) についてもソフト蒸留を試みたことに注意してください (この場合、蒸留予測ヘッドは
教師のソフトマックス出力に一致するように KL ダイバージェンスを使用してトレーニングしました）が、ハード蒸留が最良の結果をもたらしました。</li> <li>リリースされたすべてのチェックポイントは、ImageNet-1k のみで事前トレーニングおよび微調整されました。外部データは使用されませんでした。これは
JFT-300M データセット/Imagenet-21k などの外部データを使用した元の ViT モデルとは対照的です。
事前トレーニング。</li> <li>DeiT の作者は、より効率的にトレーニングされた ViT モデルもリリースしました。これは、直接プラグインできます。
<code>ViTModel</code> または <code>ViTForImageClassification</code>。データなどのテクニック
はるかに大規模なデータセットでのトレーニングをシミュレートするために、拡張、最適化、正則化が使用されました。
(ただし、事前トレーニングには ImageNet-1k のみを使用します)。 4 つのバリエーション (3 つの異なるサイズ) が利用可能です。
<em>facebook/deit-tiny-patch16-224</em>、<em>facebook/deit-small-patch16-224</em>、<em>facebook/deit-base-patch16-224</em> および
<em>facebook/deit-base-patch16-384</em>。以下を行うには <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTImageProcessor">DeiTImageProcessor</a> を使用する必要があることに注意してください。
モデル用の画像を準備します。</li>`,ot,Fe,Z,re,Re="DeiT を始めるのに役立つ公式 Hugging Face およびコミュニティ (🌎 で示されている) リソースのリスト。",Ue,ze,te,G,st='<li><a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForImageClassification">DeiTForImageClassification</a> は、この <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">サンプル スクリプト</a> および <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">ノートブック</a>。</li> <li>参照: <a href="../tasks/image_classification">画像分類タスク ガイド</a></li>',ne,Ce,Ne="それに加えて:",fe,z,ie='<li><a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTForMaskedImageModeling">DeiTForMaskedImageModeling</a> は、この <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining" rel="nofollow">サンプル スクリプト</a> でサポートされています。</li>',Be,q,at="ここに含めるリソースの送信に興味がある場合は、お気軽にプル リクエストを開いてください。審査させていただきます。リソースは、既存のリソースを複製するのではなく、何か新しいものを示すことが理想的です。",A,O,Se,R,Y,Ie,be,Pe=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTModel">DeiTModel</a>. It is used to instantiate an DeiT
model according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the DeiT
<a href="https://huggingface.co/facebook/deit-base-distilled-patch16-224" rel="nofollow">facebook/deit-base-distilled-patch16-224</a>
architecture.`,nt,P,le=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,qe,B,Ee,de,ge,W,V,Le,E,ke,N,oe,lt="Preprocess an image or a batch of images.",xe,Me,Ae,K,ye,rt,we,ue="Constructs a DeiT image processor.",We,ce,$e,it,L,me="Preprocess an image or batch of images.",He,X,Qe,c,j;return w=new De({props:{title:"DeiT",local:"deit",headingTag:"h1"}}),F=new De({props:{title:"Overview",local:"overview",headingTag:"h2"}}),I=new De({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),Fe=new De({props:{title:"Resources",local:"resources",headingTag:"h2"}}),ze=new wt({props:{pipeline:"image-classification"}}),O=new De({props:{title:"DeiTConfig",local:"transformers.DeiTConfig",headingTag:"h2"}}),Y=new se({props:{name:"class transformers.DeiTConfig",anchor:"transformers.DeiTConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"intermediate_size",val:" = 3072"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.0"},{name:"attention_probs_dropout_prob",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 16"},{name:"num_channels",val:" = 3"},{name:"qkv_bias",val:" = True"},{name:"encoder_stride",val:" = 16"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DeiTConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.DeiTConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.DeiTConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.DeiTConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.DeiTConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.DeiTConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.DeiTConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.DeiTConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.DeiTConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.DeiTConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.DeiTConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.DeiTConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.DeiTConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to add a bias to the queries, keys and values.`,name:"qkv_bias"},{anchor:"transformers.DeiTConfig.encoder_stride",description:`<strong>encoder_stride</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Factor to increase the spatial resolution by in the decoder head for masked image modeling.`,name:"encoder_stride"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/configuration_deit.py#L37"}}),B=new ct({props:{anchor:"transformers.DeiTConfig.example",$$slots:{default:[$t]},$$scope:{ctx:C}}}),de=new De({props:{title:"DeiTFeatureExtractor",local:"transformers.DeiTFeatureExtractor",headingTag:"h2"}}),V=new se({props:{name:"class transformers.DeiTFeatureExtractor",anchor:"transformers.DeiTFeatureExtractor",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/feature_extraction_deit.py#L26"}}),ke=new se({props:{name:"__call__",anchor:"transformers.DeiTFeatureExtractor.__call__",parameters:[{name:"images",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/image_processing_utils.py#L549"}}),Me=new De({props:{title:"DeiTImageProcessor",local:"transformers.DeiTImageProcessor",headingTag:"h2"}}),ye=new se({props:{name:"class transformers.DeiTImageProcessor",anchor:"transformers.DeiTImageProcessor",parameters:[{name:"do_resize",val:": bool = True"},{name:"size",val:": Dict = None"},{name:"resample",val:": Resampling = 3"},{name:"do_center_crop",val:": bool = True"},{name:"crop_size",val:": Dict = None"},{name:"rescale_factor",val:": Union = 0.00392156862745098"},{name:"do_rescale",val:": bool = True"},{name:"do_normalize",val:": bool = True"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DeiTImageProcessor.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to resize the image&#x2019;s (height, width) dimensions to the specified <code>size</code>. Can be overridden by
<code>do_resize</code> in <code>preprocess</code>.`,name:"do_resize"},{anchor:"transformers.DeiTImageProcessor.size",description:`<strong>size</strong> (<code>Dict[str, int]</code> <em>optional</em>, defaults to <code>{&quot;height&quot; -- 256, &quot;width&quot;: 256}</code>):
Size of the image after <code>resize</code>. Can be overridden by <code>size</code> in <code>preprocess</code>.`,name:"size"},{anchor:"transformers.DeiTImageProcessor.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code> filter, <em>optional</em>, defaults to <code>Resampling.BICUBIC</code>) &#x2014;
Resampling filter to use if resizing the image. Can be overridden by <code>resample</code> in <code>preprocess</code>.`,name:"resample"},{anchor:"transformers.DeiTImageProcessor.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to center crop the image. If the input size is smaller than <code>crop_size</code> along any edge, the image
is padded with 0&#x2019;s and then center cropped. Can be overridden by <code>do_center_crop</code> in <code>preprocess</code>.`,name:"do_center_crop"},{anchor:"transformers.DeiTImageProcessor.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>{&quot;height&quot; -- 224, &quot;width&quot;: 224}</code>):
Desired output size when applying center-cropping. Can be overridden by <code>crop_size</code> in <code>preprocess</code>.`,name:"crop_size"},{anchor:"transformers.DeiTImageProcessor.rescale_factor",description:`<strong>rescale_factor</strong> (<code>int</code> or <code>float</code>, <em>optional</em>, defaults to <code>1/255</code>) &#x2014;
Scale factor to use if rescaling the image. Can be overridden by the <code>rescale_factor</code> parameter in the
<code>preprocess</code> method.`,name:"rescale_factor"},{anchor:"transformers.DeiTImageProcessor.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to rescale the image by the specified scale <code>rescale_factor</code>. Can be overridden by the <code>do_rescale</code>
parameter in the <code>preprocess</code> method.`,name:"do_rescale"},{anchor:"transformers.DeiTImageProcessor.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether to normalize the image. Can be overridden by the <code>do_normalize</code> parameter in the <code>preprocess</code>
method.`,name:"do_normalize"},{anchor:"transformers.DeiTImageProcessor.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_MEAN</code>) &#x2014;
Mean to use if normalizing the image. This is a float or list of floats the length of the number of
channels in the image. Can be overridden by the <code>image_mean</code> parameter in the <code>preprocess</code> method.`,name:"image_mean"},{anchor:"transformers.DeiTImageProcessor.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>IMAGENET_STANDARD_STD</code>) &#x2014;
Standard deviation to use if normalizing the image. This is a float or list of floats the length of the
number of channels in the image. Can be overridden by the <code>image_std</code> parameter in the <code>preprocess</code> method.`,name:"image_std"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/image_processing_deit.py#L45"}}),$e=new se({props:{name:"preprocess",anchor:"transformers.DeiTImageProcessor.preprocess",parameters:[{name:"images",val:": Union"},{name:"do_resize",val:": bool = None"},{name:"size",val:": Dict = None"},{name:"resample",val:" = None"},{name:"do_center_crop",val:": bool = None"},{name:"crop_size",val:": Dict = None"},{name:"do_rescale",val:": bool = None"},{name:"rescale_factor",val:": float = None"},{name:"do_normalize",val:": bool = None"},{name:"image_mean",val:": Union = None"},{name:"image_std",val:": Union = None"},{name:"return_tensors",val:": Union = None"},{name:"data_format",val:": ChannelDimension = <ChannelDimension.FIRST: 'channels_first'>"},{name:"input_data_format",val:": Union = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.DeiTImageProcessor.preprocess.images",description:`<strong>images</strong> (<code>ImageInput</code>) &#x2014;
Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If
passing in images with pixel values between 0 and 1, set <code>do_rescale=False</code>.`,name:"images"},{anchor:"transformers.DeiTImageProcessor.preprocess.do_resize",description:`<strong>do_resize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_resize</code>) &#x2014;
Whether to resize the image.`,name:"do_resize"},{anchor:"transformers.DeiTImageProcessor.preprocess.size",description:`<strong>size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.size</code>) &#x2014;
Size of the image after <code>resize</code>.`,name:"size"},{anchor:"transformers.DeiTImageProcessor.preprocess.resample",description:`<strong>resample</strong> (<code>PILImageResampling</code>, <em>optional</em>, defaults to <code>self.resample</code>) &#x2014;
PILImageResampling filter to use if resizing the image Only has an effect if <code>do_resize</code> is set to
<code>True</code>.`,name:"resample"},{anchor:"transformers.DeiTImageProcessor.preprocess.do_center_crop",description:`<strong>do_center_crop</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_center_crop</code>) &#x2014;
Whether to center crop the image.`,name:"do_center_crop"},{anchor:"transformers.DeiTImageProcessor.preprocess.crop_size",description:`<strong>crop_size</strong> (<code>Dict[str, int]</code>, <em>optional</em>, defaults to <code>self.crop_size</code>) &#x2014;
Size of the image after center crop. If one edge the image is smaller than <code>crop_size</code>, it will be
padded with zeros and then cropped`,name:"crop_size"},{anchor:"transformers.DeiTImageProcessor.preprocess.do_rescale",description:`<strong>do_rescale</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_rescale</code>) &#x2014;
Whether to rescale the image values between [0 - 1].`,name:"do_rescale"},{anchor:"transformers.DeiTImageProcessor.preprocess.rescale_factor",description:`<strong>rescale_factor</strong> (<code>float</code>, <em>optional</em>, defaults to <code>self.rescale_factor</code>) &#x2014;
Rescale factor to rescale the image by if <code>do_rescale</code> is set to <code>True</code>.`,name:"rescale_factor"},{anchor:"transformers.DeiTImageProcessor.preprocess.do_normalize",description:`<strong>do_normalize</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>self.do_normalize</code>) &#x2014;
Whether to normalize the image.`,name:"do_normalize"},{anchor:"transformers.DeiTImageProcessor.preprocess.image_mean",description:`<strong>image_mean</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_mean</code>) &#x2014;
Image mean.`,name:"image_mean"},{anchor:"transformers.DeiTImageProcessor.preprocess.image_std",description:`<strong>image_std</strong> (<code>float</code> or <code>List[float]</code>, <em>optional</em>, defaults to <code>self.image_std</code>) &#x2014;
Image standard deviation.`,name:"image_std"},{anchor:"transformers.DeiTImageProcessor.preprocess.return_tensors",description:`<strong>return_tensors</strong> (<code>str</code> or <code>TensorType</code>, <em>optional</em>) &#x2014;
The type of tensors to return. Can be one of:<ul>
<li><code>None</code>: Return a list of <code>np.ndarray</code>.</li>
<li><code>TensorType.TENSORFLOW</code> or <code>&apos;tf&apos;</code>: Return a batch of type <code>tf.Tensor</code>.</li>
<li><code>TensorType.PYTORCH</code> or <code>&apos;pt&apos;</code>: Return a batch of type <code>torch.Tensor</code>.</li>
<li><code>TensorType.NUMPY</code> or <code>&apos;np&apos;</code>: Return a batch of type <code>np.ndarray</code>.</li>
<li><code>TensorType.JAX</code> or <code>&apos;jax&apos;</code>: Return a batch of type <code>jax.numpy.ndarray</code>.</li>
</ul>`,name:"return_tensors"},{anchor:"transformers.DeiTImageProcessor.preprocess.data_format",description:`<strong>data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>, defaults to <code>ChannelDimension.FIRST</code>) &#x2014;
The channel dimension format for the output image. Can be one of:<ul>
<li><code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
</ul>`,name:"data_format"},{anchor:"transformers.DeiTImageProcessor.preprocess.input_data_format",description:`<strong>input_data_format</strong> (<code>ChannelDimension</code> or <code>str</code>, <em>optional</em>) &#x2014;
The channel dimension format for the input image. If unset, the channel dimension format is inferred
from the input image. Can be one of:<ul>
<li><code>&quot;channels_first&quot;</code> or <code>ChannelDimension.FIRST</code>: image in (num_channels, height, width) format.</li>
<li><code>&quot;channels_last&quot;</code> or <code>ChannelDimension.LAST</code>: image in (height, width, num_channels) format.</li>
<li><code>&quot;none&quot;</code> or <code>ChannelDimension.NONE</code>: image in (height, width) format.</li>
</ul>`,name:"input_data_format"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/deit/image_processing_deit.py#L161"}}),X=new yt({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[Pt],pytorch:[Zt]},$$scope:{ctx:C}}}),{c(){e=p("meta"),f=i(),s=p("p"),a=i(),u(w.$$.fragment),o=i(),u(F.$$.fragment),Ve=i(),x=p("p"),x.innerHTML=pe,Xe=i(),ee=p("p"),ee.textContent=pt,Ye=i(),U=p("p"),U.innerHTML=tt,S=i(),Te=p("p"),Te.innerHTML=he,Ze=i(),u(I.$$.fragment),ae=i(),je=p("ul"),je.innerHTML=ve,ot=i(),u(Fe.$$.fragment),Z=i(),re=p("p"),re.textContent=Re,Ue=i(),u(ze.$$.fragment),te=i(),G=p("ul"),G.innerHTML=st,ne=i(),Ce=p("p"),Ce.textContent=Ne,fe=i(),z=p("ul"),z.innerHTML=ie,Be=i(),q=p("p"),q.textContent=at,A=i(),u(O.$$.fragment),Se=i(),R=p("div"),u(Y.$$.fragment),Ie=i(),be=p("p"),be.innerHTML=Pe,nt=i(),P=p("p"),P.innerHTML=le,qe=i(),u(B.$$.fragment),Ee=i(),u(de.$$.fragment),ge=i(),W=p("div"),u(V.$$.fragment),Le=i(),E=p("div"),u(ke.$$.fragment),N=i(),oe=p("p"),oe.textContent=lt,xe=i(),u(Me.$$.fragment),Ae=i(),K=p("div"),u(ye.$$.fragment),rt=i(),we=p("p"),we.textContent=ue,We=i(),ce=p("div"),u($e.$$.fragment),it=i(),L=p("p"),L.textContent=me,He=i(),u(X.$$.fragment),Qe=i(),c=p("p"),this.h()},l(t){const g=Mt("svelte-u9bgzb",document.head);e=h(g,"META",{name:!0,content:!0}),g.forEach(n),f=l(t),s=h(t,"P",{}),H(s).forEach(n),a=l(t),_(w.$$.fragment,t),o=l(t),_(F.$$.fragment,t),Ve=l(t),x=h(t,"P",{"data-svelte-h":!0}),v(x)!=="svelte-mafcr4"&&(x.innerHTML=pe),Xe=l(t),ee=h(t,"P",{"data-svelte-h":!0}),v(ee)!=="svelte-1cv3nri"&&(ee.textContent=pt),Ye=l(t),U=h(t,"P",{"data-svelte-h":!0}),v(U)!=="svelte-19kjohr"&&(U.innerHTML=tt),S=l(t),Te=h(t,"P",{"data-svelte-h":!0}),v(Te)!=="svelte-nvgx1x"&&(Te.innerHTML=he),Ze=l(t),_(I.$$.fragment,t),ae=l(t),je=h(t,"UL",{"data-svelte-h":!0}),v(je)!=="svelte-1b4pob6"&&(je.innerHTML=ve),ot=l(t),_(Fe.$$.fragment,t),Z=l(t),re=h(t,"P",{"data-svelte-h":!0}),v(re)!=="svelte-5xp01e"&&(re.textContent=Re),Ue=l(t),_(ze.$$.fragment,t),te=l(t),G=h(t,"UL",{"data-svelte-h":!0}),v(G)!=="svelte-axai4f"&&(G.innerHTML=st),ne=l(t),Ce=h(t,"P",{"data-svelte-h":!0}),v(Ce)!=="svelte-1ix811i"&&(Ce.textContent=Ne),fe=l(t),z=h(t,"UL",{"data-svelte-h":!0}),v(z)!=="svelte-zactuu"&&(z.innerHTML=ie),Be=l(t),q=h(t,"P",{"data-svelte-h":!0}),v(q)!=="svelte-17ytafw"&&(q.textContent=at),A=l(t),_(O.$$.fragment,t),Se=l(t),R=h(t,"DIV",{class:!0});var D=H(R);_(Y.$$.fragment,D),Ie=l(D),be=h(D,"P",{"data-svelte-h":!0}),v(be)!=="svelte-16liyg5"&&(be.innerHTML=Pe),nt=l(D),P=h(D,"P",{"data-svelte-h":!0}),v(P)!=="svelte-1s6wgpv"&&(P.innerHTML=le),qe=l(D),_(B.$$.fragment,D),D.forEach(n),Ee=l(t),_(de.$$.fragment,t),ge=l(t),W=h(t,"DIV",{class:!0});var r=H(W);_(V.$$.fragment,r),Le=l(r),E=h(r,"DIV",{class:!0});var $=H(E);_(ke.$$.fragment,$),N=l($),oe=h($,"P",{"data-svelte-h":!0}),v(oe)!=="svelte-khengj"&&(oe.textContent=lt),$.forEach(n),r.forEach(n),xe=l(t),_(Me.$$.fragment,t),Ae=l(t),K=h(t,"DIV",{class:!0});var k=H(K);_(ye.$$.fragment,k),rt=l(k),we=h(k,"P",{"data-svelte-h":!0}),v(we)!=="svelte-19jpkyi"&&(we.textContent=ue),We=l(k),ce=h(k,"DIV",{class:!0});var J=H(ce);_($e.$$.fragment,J),it=l(J),L=h(J,"P",{"data-svelte-h":!0}),v(L)!=="svelte-1x3yxsa"&&(L.textContent=me),J.forEach(n),k.forEach(n),He=l(t),_(X.$$.fragment,t),Qe=l(t),c=h(t,"P",{}),H(c).forEach(n),this.h()},h(){Q(e,"name","hf:doc:metadata"),Q(e,"content",Lt),Q(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(E,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(ce,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),Q(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,g){d(document.head,e),m(t,f,g),m(t,s,g),m(t,a,g),T(w,t,g),m(t,o,g),T(F,t,g),m(t,Ve,g),m(t,x,g),m(t,Xe,g),m(t,ee,g),m(t,Ye,g),m(t,U,g),m(t,S,g),m(t,Te,g),m(t,Ze,g),T(I,t,g),m(t,ae,g),m(t,je,g),m(t,ot,g),T(Fe,t,g),m(t,Z,g),m(t,re,g),m(t,Ue,g),T(ze,t,g),m(t,te,g),m(t,G,g),m(t,ne,g),m(t,Ce,g),m(t,fe,g),m(t,z,g),m(t,Be,g),m(t,q,g),m(t,A,g),T(O,t,g),m(t,Se,g),m(t,R,g),T(Y,R,null),d(R,Ie),d(R,be),d(R,nt),d(R,P),d(R,qe),T(B,R,null),m(t,Ee,g),T(de,t,g),m(t,ge,g),m(t,W,g),T(V,W,null),d(W,Le),d(W,E),T(ke,E,null),d(E,N),d(E,oe),m(t,xe,g),T(Me,t,g),m(t,Ae,g),m(t,K,g),T(ye,K,null),d(K,rt),d(K,we),d(K,We),d(K,ce),T($e,ce,null),d(ce,it),d(ce,L),m(t,He,g),T(X,t,g),m(t,Qe,g),m(t,c,g),j=!0},p(t,[g]){const D={};g&2&&(D.$$scope={dirty:g,ctx:t}),B.$set(D);const r={};g&2&&(r.$$scope={dirty:g,ctx:t}),X.$set(r)},i(t){j||(b(w.$$.fragment,t),b(F.$$.fragment,t),b(I.$$.fragment,t),b(Fe.$$.fragment,t),b(ze.$$.fragment,t),b(O.$$.fragment,t),b(Y.$$.fragment,t),b(B.$$.fragment,t),b(de.$$.fragment,t),b(V.$$.fragment,t),b(ke.$$.fragment,t),b(Me.$$.fragment,t),b(ye.$$.fragment,t),b($e.$$.fragment,t),b(X.$$.fragment,t),j=!0)},o(t){M(w.$$.fragment,t),M(F.$$.fragment,t),M(I.$$.fragment,t),M(Fe.$$.fragment,t),M(ze.$$.fragment,t),M(O.$$.fragment,t),M(Y.$$.fragment,t),M(B.$$.fragment,t),M(de.$$.fragment,t),M(V.$$.fragment,t),M(ke.$$.fragment,t),M(Me.$$.fragment,t),M(ye.$$.fragment,t),M($e.$$.fragment,t),M(X.$$.fragment,t),j=!1},d(t){t&&(n(f),n(s),n(a),n(o),n(Ve),n(x),n(Xe),n(ee),n(Ye),n(U),n(S),n(Te),n(Ze),n(ae),n(je),n(ot),n(Z),n(re),n(Ue),n(te),n(G),n(ne),n(Ce),n(fe),n(z),n(Be),n(q),n(A),n(Se),n(R),n(Ee),n(ge),n(W),n(xe),n(Ae),n(K),n(He),n(Qe),n(c)),n(e),y(w,t),y(F,t),y(I,t),y(Fe,t),y(ze,t),y(O,t),y(Y),y(B),y(de,t),y(V),y(ke),y(Me,t),y(ye),y($e),y(X,t)}}}const Lt='{"title":"DeiT","local":"deit","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"DeiTConfig","local":"transformers.DeiTConfig","sections":[],"depth":2},{"title":"DeiTFeatureExtractor","local":"transformers.DeiTFeatureExtractor","sections":[],"depth":2},{"title":"DeiTImageProcessor","local":"transformers.DeiTImageProcessor","sections":[],"depth":2},{"title":"DeiTModel","local":"transformers.DeiTModel","sections":[],"depth":2},{"title":"DeiTForMaskedImageModeling","local":"transformers.DeiTForMaskedImageModeling","sections":[],"depth":2},{"title":"DeiTForImageClassification","local":"transformers.DeiTForImageClassification","sections":[],"depth":2},{"title":"DeiTForImageClassificationWithTeacher","local":"transformers.DeiTForImageClassificationWithTeacher","sections":[],"depth":2},{"title":"TFDeiTModel","local":"transformers.TFDeiTModel","sections":[],"depth":2},{"title":"TFDeiTForMaskedImageModeling","local":"transformers.TFDeiTForMaskedImageModeling","sections":[],"depth":2},{"title":"TFDeiTForImageClassification","local":"transformers.TFDeiTForImageClassification","sections":[],"depth":2},{"title":"TFDeiTForImageClassificationWithTeacher","local":"transformers.TFDeiTForImageClassificationWithTeacher","sections":[],"depth":2}],"depth":1}';function Ht(C){return _t(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class so extends Tt{constructor(e){super(),bt(this,e,Ht,Et,ut,{})}}export{so as component};
