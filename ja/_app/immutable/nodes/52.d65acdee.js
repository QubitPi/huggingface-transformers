import{s as Oo,o as Ko,n as x}from"../chunks/scheduler.9bc65507.js";import{S as en,i as tn,g as m,s as l,r as f,A as on,h as p,f as s,c as i,j as $,u as g,x as I,k as v,y as c,a as d,v as u,d as h,t as _,w as M}from"../chunks/index.707bf1b6.js";import{T as ut}from"../chunks/Tip.c2ecdbf4.js";import{D as w}from"../chunks/Docstring.17db21ae.js";import{C as Y}from"../chunks/CodeBlock.54a9f38d.js";import{E as ue}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{H as q}from"../chunks/Heading.342b1fa6.js";function nn(y){let o,b='このモデルは<code>CLIPModel</code>をベースにしており、オリジナルの<a href="clip">CLIP</a>と同じように使用してください。';return{c(){o=m("p"),o.innerHTML=b},l(a){o=p(a,"P",{"data-svelte-h":!0}),I(o)!=="svelte-oj8fjs"&&(o.innerHTML=b)},m(a,r){d(a,o,r)},p:x,d(a){a&&s(o)}}}function sn(y){let o,b="Example:",a,r,C;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsdENMSVBDb25maWclMkMlMjBBbHRDTElQTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQWx0Q0xJUENvbmZpZyUyMHdpdGglMjBCQUFJJTJGQWx0Q0xJUCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbHRDTElQQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQWx0Q0xJUE1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBCQUFJJTJGQWx0Q0xJUCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZyUwQSUwQSUyMyUyMFdlJTIwY2FuJTIwYWxzbyUyMGluaXRpYWxpemUlMjBhJTIwQWx0Q0xJUENvbmZpZyUyMGZyb20lMjBhJTIwQWx0Q0xJUFRleHRDb25maWclMjBhbmQlMjBhJTIwQWx0Q0xJUFZpc2lvbkNvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBBbHRDTElQVGV4dCUyMGFuZCUyMEFsdENMSVBWaXNpb24lMjBjb25maWd1cmF0aW9uJTBBY29uZmlnX3RleHQlMjAlM0QlMjBBbHRDTElQVGV4dENvbmZpZygpJTBBY29uZmlnX3Zpc2lvbiUyMCUzRCUyMEFsdENMSVBWaXNpb25Db25maWcoKSUwQSUwQWNvbmZpZyUyMCUzRCUyMEFsdENMSVBDb25maWcuZnJvbV90ZXh0X3Zpc2lvbl9jb25maWdzKGNvbmZpZ190ZXh0JTJDJTIwY29uZmlnX3Zpc2lvbik=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AltCLIPConfig, AltCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPConfig with BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AltCLIPConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPModel (with random weights) from the BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># We can also initialize a AltCLIPConfig from a AltCLIPTextConfig and a AltCLIPVisionConfig</span>

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPText and AltCLIPVision configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>config_text = AltCLIPTextConfig()
<span class="hljs-meta">&gt;&gt;&gt; </span>config_vision = AltCLIPVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span>config = AltCLIPConfig.from_text_vision_configs(config_text, config_vision)`,wrap:!1}}),{c(){o=m("p"),o.textContent=b,a=l(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),I(o)!=="svelte-11lpom8"&&(o.textContent=b),a=i(t),g(r.$$.fragment,t)},m(t,T){d(t,o,T),d(t,a,T),u(r,t,T),C=!0},p:x,i(t){C||(h(r.$$.fragment,t),C=!0)},o(t){_(r.$$.fragment,t),C=!1},d(t){t&&(s(o),s(a)),M(r,t)}}}function an(y){let o,b="Examples:",a,r,C;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsdENMSVBUZXh0TW9kZWwlMkMlMjBBbHRDTElQVGV4dENvbmZpZyUwQSUwQSUyMyUyMEluaXRpYWxpemluZyUyMGElMjBBbHRDTElQVGV4dENvbmZpZyUyMHdpdGglMjBCQUFJJTJGQWx0Q0xJUCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbHRDTElQVGV4dENvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEFsdENMSVBUZXh0TW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMEJBQUklMkZBbHRDTElQJTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBBbHRDTElQVGV4dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AltCLIPTextModel, AltCLIPTextConfig

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPTextConfig with BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AltCLIPTextConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPTextModel (with random weights) from the BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPTextModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=m("p"),o.textContent=b,a=l(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),I(o)!=="svelte-kvfsh7"&&(o.textContent=b),a=i(t),g(r.$$.fragment,t)},m(t,T){d(t,o,T),d(t,a,T),u(r,t,T),C=!0},p:x,i(t){C||(h(r.$$.fragment,t),C=!0)},o(t){_(r.$$.fragment,t),C=!1},d(t){t&&(s(o),s(a)),M(r,t)}}}function rn(y){let o,b="Example:",a,r,C;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEFsdENMSVBWaXNpb25Db25maWclMkMlMjBBbHRDTElQVmlzaW9uTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQWx0Q0xJUFZpc2lvbkNvbmZpZyUyMHdpdGglMjBCQUFJJTJGQWx0Q0xJUCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQWNvbmZpZ3VyYXRpb24lMjAlM0QlMjBBbHRDTElQVmlzaW9uQ29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQWx0Q0xJUFZpc2lvbk1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBCQUFJJTJGQWx0Q0xJUCUyMHN0eWxlJTIwY29uZmlndXJhdGlvbiUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUFZpc2lvbk1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AltCLIPVisionConfig, AltCLIPVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPVisionConfig with BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = AltCLIPVisionConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a AltCLIPVisionModel (with random weights) from the BAAI/AltCLIP style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPVisionModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){o=m("p"),o.textContent=b,a=l(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),I(o)!=="svelte-11lpom8"&&(o.textContent=b),a=i(t),g(r.$$.fragment,t)},m(t,T){d(t,o,T),d(t,a,T),u(r,t,T),C=!0},p:x,i(t){C||(h(r.$$.fragment,t),C=!0)},o(t){_(r.$$.fragment,t),C=!1},d(t){t&&(s(o),s(a)),M(r,t)}}}function ln(y){let o,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=b},l(a){o=p(a,"P",{"data-svelte-h":!0}),I(o)!=="svelte-fincs2"&&(o.innerHTML=b)},m(a,r){d(a,o,r)},p:x,d(a){a&&s(o)}}}function dn(y){let o,b="Examples:",a,r,C;return r=new Y({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEFsdENMSVBNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKCUwQSUyMCUyMCUyMCUyMHRleHQlM0QlNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUwQSklMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbG9naXRzX3Blcl9pbWFnZSUyMCUzRCUyMG91dHB1dHMubG9naXRzX3Blcl9pbWFnZSUyMCUyMCUyMyUyMHRoaXMlMjBpcyUyMHRoZSUyMGltYWdlLXRleHQlMjBzaW1pbGFyaXR5JTIwc2NvcmUlMEFwcm9icyUyMCUzRCUyMGxvZ2l0c19wZXJfaW1hZ2Uuc29mdG1heChkaW0lM0QxKSUyMCUyMCUyMyUyMHdlJTIwY2FuJTIwdGFrZSUyMHRoZSUyMHNvZnRtYXglMjB0byUyMGdldCUyMHRoZSUyMGxhYmVsJTIwcHJvYmFiaWxpdGllcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AltCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(
<span class="hljs-meta">... </span>    text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>
<span class="hljs-meta">... </span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),{c(){o=m("p"),o.textContent=b,a=l(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),I(o)!=="svelte-kvfsh7"&&(o.textContent=b),a=i(t),g(r.$$.fragment,t)},m(t,T){d(t,o,T),d(t,a,T),u(r,t,T),C=!0},p:x,i(t){C||(h(r.$$.fragment,t),C=!0)},o(t){_(r.$$.fragment,t),C=!1},d(t){t&&(s(o),s(a)),M(r,t)}}}function cn(y){let o,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=b},l(a){o=p(a,"P",{"data-svelte-h":!0}),I(o)!=="svelte-fincs2"&&(o.innerHTML=b)},m(a,r){d(a,o,r)},p:x,d(a){a&&s(o)}}}function mn(y){let o,b="Examples:",a,r,C;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBbHRDTElQTW9kZWwlMEElMEFtb2RlbCUyMCUzRCUyMEFsdENMSVBNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIyQkFBSSUyRkFsdENMSVAlMjIpJTBBcHJvY2Vzc29yJTIwJTNEJTIwQXV0b1Byb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyQkFBSSUyRkFsdENMSVAlMjIpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0QlNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBwYWRkaW5nJTNEVHJ1ZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBdGV4dF9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF90ZXh0X2ZlYXR1cmVzKCoqaW5wdXRzKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AltCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>text_features = model.get_text_features(**inputs)`,wrap:!1}}),{c(){o=m("p"),o.textContent=b,a=l(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),I(o)!=="svelte-kvfsh7"&&(o.textContent=b),a=i(t),g(r.$$.fragment,t)},m(t,T){d(t,o,T),d(t,a,T),u(r,t,T),C=!0},p:x,i(t){C||(h(r.$$.fragment,t),C=!0)},o(t){_(r.$$.fragment,t),C=!1},d(t){t&&(s(o),s(a)),M(r,t)}}}function pn(y){let o,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=b},l(a){o=p(a,"P",{"data-svelte-h":!0}),I(o)!=="svelte-fincs2"&&(o.innerHTML=b)},m(a,r){d(a,o,r)},p:x,d(a){a&&s(o)}}}function fn(y){let o,b="Examples:",a,r,C;return r=new Y({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEFsdENMSVBNb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEFpbWFnZV9mZWF0dXJlcyUyMCUzRCUyMG1vZGVsLmdldF9pbWFnZV9mZWF0dXJlcygqKmlucHV0cyk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AltCLIPModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image_features = model.get_image_features(**inputs)`,wrap:!1}}),{c(){o=m("p"),o.textContent=b,a=l(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),I(o)!=="svelte-kvfsh7"&&(o.textContent=b),a=i(t),g(r.$$.fragment,t)},m(t,T){d(t,o,T),d(t,a,T),u(r,t,T),C=!0},p:x,i(t){C||(h(r.$$.fragment,t),C=!0)},o(t){_(r.$$.fragment,t),C=!1},d(t){t&&(s(o),s(a)),M(r,t)}}}function gn(y){let o,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=b},l(a){o=p(a,"P",{"data-svelte-h":!0}),I(o)!=="svelte-fincs2"&&(o.innerHTML=b)},m(a,r){d(a,o,r)},p:x,d(a){a&&s(o)}}}function un(y){let o,b="Examples:",a,r,C;return r=new Y({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Qcm9jZXNzb3IlMkMlMjBBbHRDTElQVGV4dE1vZGVsJTBBJTBBbW9kZWwlMjAlM0QlMjBBbHRDTElQVGV4dE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEElMEF0ZXh0cyUyMCUzRCUyMCU1QiUyMml0J3MlMjBhJTIwY2F0JTIyJTJDJTIwJTIyaXQncyUyMGElMjBkb2clMjIlNUQlMEElMEFpbnB1dHMlMjAlM0QlMjBwcm9jZXNzb3IodGV4dCUzRHRleHRzJTJDJTIwcGFkZGluZyUzRFRydWUlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZSUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFwb29sZWRfb3V0cHV0JTIwJTNEJTIwb3V0cHV0cy5wb29sZXJfb3V0cHV0JTIwJTIwJTIzJTIwcG9vbGVkJTIwQ0xTJTIwc3RhdGVz",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AltCLIPTextModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPTextModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>texts = [<span class="hljs-string">&quot;it&#x27;s a cat&quot;</span>, <span class="hljs-string">&quot;it&#x27;s a dog&quot;</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=texts, padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){o=m("p"),o.textContent=b,a=l(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),I(o)!=="svelte-kvfsh7"&&(o.textContent=b),a=i(t),g(r.$$.fragment,t)},m(t,T){d(t,o,T),d(t,a,T),u(r,t,T),C=!0},p:x,i(t){C||(h(r.$$.fragment,t),C=!0)},o(t){_(r.$$.fragment,t),C=!1},d(t){t&&(s(o),s(a)),M(r,t)}}}function hn(y){let o,b=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){o=m("p"),o.innerHTML=b},l(a){o=p(a,"P",{"data-svelte-h":!0}),I(o)!=="svelte-fincs2"&&(o.innerHTML=b)},m(a,r){d(a,o,r)},p:x,d(a){a&&s(o)}}}function _n(y){let o,b="Examples:",a,r,C;return r=new Y({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b1Byb2Nlc3NvciUyQyUyMEFsdENMSVBWaXNpb25Nb2RlbCUwQSUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUFZpc2lvbk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBdXRvUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKGltYWdlcyUzRGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbGFzdF9oaWRkZW5fc3RhdGUlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBcG9vbGVkX291dHB1dCUyMCUzRCUyMG91dHB1dHMucG9vbGVyX291dHB1dCUyMCUyMCUyMyUyMHBvb2xlZCUyMENMUyUyMHN0YXRlcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoProcessor, AltCLIPVisionModel

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPVisionModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AutoProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_state = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span>pooled_output = outputs.pooler_output  <span class="hljs-comment"># pooled CLS states</span>`,wrap:!1}}),{c(){o=m("p"),o.textContent=b,a=l(),f(r.$$.fragment)},l(t){o=p(t,"P",{"data-svelte-h":!0}),I(o)!=="svelte-kvfsh7"&&(o.textContent=b),a=i(t),g(r.$$.fragment,t)},m(t,T){d(t,o,T),d(t,a,T),u(r,t,T),C=!0},p:x,i(t){C||(h(r.$$.fragment,t),C=!0)},o(t){_(r.$$.fragment,t),C=!1},d(t){t&&(s(o),s(a)),M(r,t)}}}function Mn(y){let o,b,a,r,C,t,T,_t,he,wo='AltCLIPモデルは、「<a href="https://arxiv.org/abs/2211.06679v2" rel="nofollow">AltCLIP: Altering the Language Encoder in CLIP for Extended Language Capabilities</a>」という論文でZhongzhi Chen、Guang Liu、Bo-Wen Zhang、Fulong Ye、Qinghong Yang、Ledell Wuによって提案されました。AltCLIP（CLIPの言語エンコーダーの代替）は、様々な画像-テキストペアおよびテキスト-テキストペアでトレーニングされたニューラルネットワークです。CLIPのテキストエンコーダーを事前学習済みの多言語テキストエンコーダーXLM-Rに置き換えることで、ほぼ全てのタスクでCLIPに非常に近い性能を得られ、オリジナルのCLIPの能力を多言語理解などに拡張しました。',Mt,_e,xo="論文の要旨は以下の通りです：",Ct,Me,Jo="<em>この研究では、強力なバイリンガルマルチモーダル表現モデルを訓練するための概念的に単純で効果的な方法を提案します。OpenAIによってリリースされたマルチモーダル表現モデルCLIPから開始し、そのテキストエンコーダを事前学習済みの多言語テキストエンコーダXLM-Rに交換し、教師学習と対照学習からなる2段階のトレーニングスキーマを用いて言語と画像の表現を整合させました。幅広いタスクの評価を通じて、我々の方法を検証します。ImageNet-CN、Flicker30k-CN、COCO-CNを含む多くのタスクで新たな最先端の性能を達成しました。さらに、ほぼすべてのタスクでCLIPに非常に近い性能を得ており、これはCLIPのテキストエンコーダを変更するだけで、多言語理解などの拡張を実現できることを示唆しています。</em>",bt,Ce,Lo='このモデルは<a href="https://huggingface.co/jongjyh" rel="nofollow">jongjyh</a>により提供されました。',Tt,be,It,Te,Ao="AltCLIPの使用方法はCLIPに非常に似ています。CLIPとの違いはテキストエンコーダーにあります。私たちはカジュアルアテンションではなく双方向アテンションを使用し、XLM-Rの[CLS]トークンをテキスト埋め込みを表すものとして取ることに留意してください。",yt,Ie,ko="AltCLIPはマルチモーダルな視覚言語モデルです。これは画像とテキストの類似度や、ゼロショット画像分類に使用できます。AltCLIPはViTのようなTransformerを使用して視覚的特徴を、双方向言語モデルを使用してテキスト特徴を取得します。テキストと視覚の両方の特徴は、同一の次元を持つ潜在空間に射影されます。射影された画像とテキスト特徴間のドット積が類似度スコアとして使用されます。",$t,ye,Uo='Transformerエンコーダーに画像を与えるには、各画像を固定サイズの重複しないパッチの系列に分割し、それらを線形に埋め込みます。画像全体を表現するための[CLS]トークンが追加されます。著者は絶対位置埋め込みも追加し、結果として得られるベクトルの系列を標準的なTransformerエンコーダーに供給します。<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>を使用して、モデルのために画像のサイズ変更（または拡大縮小）と正規化を行うことができます。',vt,$e,Bo='<a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a>は、テキストのエンコードと画像の前処理を両方行うために、<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>と<code>XLMRobertaTokenizer</code>を単一のインスタンスにラップします。以下の例は、<a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a>と<a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a>を使用して画像-テキスト類似スコアを取得する方法を示しています。',jt,ve,Pt,D,wt,je,xt,j,Pe,Ft,Ke,zo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a>. It is used to instantiate an
AltCLIP model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the AltCLIP
<a href="https://huggingface.co/BAAI/AltCLIP" rel="nofollow">BAAI/AltCLIP</a> architecture.`,Nt,et,Wo=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Rt,O,qt,K,we,Xt,tt,Zo=`Instantiate a <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPConfig">AltCLIPConfig</a> (or a derived class) from altclip text model configuration and altclip vision
model configuration.`,Jt,xe,Lt,J,Je,St,ot,Vo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPTextModel">AltCLIPTextModel</a>. It is used to instantiate a
AltCLIP text model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the AltCLIP
<a href="https://huggingface.co/BAAI/AltCLIP" rel="nofollow">BAAI/AltCLIP</a> architecture.`,Yt,nt,Ho=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Dt,ee,At,Le,kt,L,Ae,Ot,st,Qo=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a>. It is used to instantiate an
AltCLIP model according to the specified arguments, defining the model architecture. Instantiating a configuration
with the defaults will yield a similar configuration to that of the AltCLIP
<a href="https://huggingface.co/BAAI/AltCLIP" rel="nofollow">BAAI/AltCLIP</a> architecture.`,Kt,at,Eo=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,eo,te,Ut,ke,Bt,P,Ue,to,rt,Go=`Constructs a AltCLIP processor which wraps a CLIP image processor and a XLM-Roberta tokenizer into a single
processor.`,oo,lt,Fo=`<a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPProcessor">AltCLIPProcessor</a> offers all the functionalities of <a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a> and <code>XLMRobertaTokenizerFast</code>. See
the <code>__call__()</code> and <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPProcessor.decode">decode()</a> for more information.`,no,oe,Be,so,it,No=`This method forwards all its arguments to XLMRobertaTokenizerFast’s <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.batch_decode">batch_decode()</a>.
Please refer to the docstring of this method for more information.`,ao,ne,ze,ro,dt,Ro=`This method forwards all its arguments to XLMRobertaTokenizerFast’s <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.decode">decode()</a>. Please
refer to the docstring of this method for more information.`,zt,We,Wt,A,Ze,lo,B,Ve,io,ct,qo='The <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> forward method, overrides the <code>__call__</code> special method.',co,se,mo,ae,po,z,He,fo,mt,Xo='The <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> forward method, overrides the <code>__call__</code> special method.',go,re,uo,le,ho,W,Qe,_o,pt,So='The <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPModel">AltCLIPModel</a> forward method, overrides the <code>__call__</code> special method.',Mo,ie,Co,de,Zt,Ee,Vt,X,Ge,bo,Z,Fe,To,ft,Yo='The <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPTextModel">AltCLIPTextModel</a> forward method, overrides the <code>__call__</code> special method.',Io,ce,yo,me,Ht,Ne,Qt,S,Re,$o,V,qe,vo,gt,Do='The <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPVisionModel">AltCLIPVisionModel</a> forward method, overrides the <code>__call__</code> special method.',jo,pe,Po,fe,Et,ht,Gt;return C=new q({props:{title:"AltCLIP",local:"altclip",headingTag:"h1"}}),T=new q({props:{title:"概要",local:"概要",headingTag:"h2"}}),be=new q({props:{title:"使用上のヒントと使用例",local:"使用上のヒントと使用例",headingTag:"h2"}}),ve=new Y({props:{code:"ZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQWx0Q0xJUE1vZGVsJTJDJTIwQWx0Q0xJUFByb2Nlc3NvciUwQSUwQW1vZGVsJTIwJTNEJTIwQWx0Q0xJUE1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEFwcm9jZXNzb3IlMjAlM0QlMjBBbHRDTElQUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJCQUFJJTJGQWx0Q0xJUCUyMiklMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW5wdXRzJTIwJTNEJTIwcHJvY2Vzc29yKHRleHQlM0QlNUIlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBjYXQlMjIlMkMlMjAlMjJhJTIwcGhvdG8lMjBvZiUyMGElMjBkb2clMjIlNUQlMkMlMjBpbWFnZXMlM0RpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIlMkMlMjBwYWRkaW5nJTNEVHJ1ZSklMEElMEFvdXRwdXRzJTIwJTNEJTIwbW9kZWwoKippbnB1dHMpJTBBbG9naXRzX3Blcl9pbWFnZSUyMCUzRCUyMG91dHB1dHMubG9naXRzX3Blcl9pbWFnZSUyMCUyMCUyMyUyMHRoaXMlMjBpcyUyMHRoZSUyMGltYWdlLXRleHQlMjBzaW1pbGFyaXR5JTIwc2NvcmUlMEFwcm9icyUyMCUzRCUyMGxvZ2l0c19wZXJfaW1hZ2Uuc29mdG1heChkaW0lM0QxKSUyMCUyMCUyMyUyMHdlJTIwY2FuJTIwdGFrZSUyMHRoZSUyMHNvZnRtYXglMjB0byUyMGdldCUyMHRoZSUyMGxhYmVsJTIwcHJvYmFiaWxpdGllcw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AltCLIPModel, AltCLIPProcessor

<span class="hljs-meta">&gt;&gt;&gt; </span>model = AltCLIPModel.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>processor = AltCLIPProcessor.from_pretrained(<span class="hljs-string">&quot;BAAI/AltCLIP&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = processor(text=[<span class="hljs-string">&quot;a photo of a cat&quot;</span>, <span class="hljs-string">&quot;a photo of a dog&quot;</span>], images=image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits_per_image = outputs.logits_per_image  <span class="hljs-comment"># this is the image-text similarity score</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>probs = logits_per_image.softmax(dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># we can take the softmax to get the label probabilities</span>`,wrap:!1}}),D=new ut({props:{$$slots:{default:[nn]},$$scope:{ctx:y}}}),je=new q({props:{title:"AltCLIPConfig",local:"transformers.AltCLIPConfig",headingTag:"h2"}}),Pe=new w({props:{name:"class transformers.AltCLIPConfig",anchor:"transformers.AltCLIPConfig",parameters:[{name:"text_config",val:" = None"},{name:"vision_config",val:" = None"},{name:"projection_dim",val:" = 768"},{name:"logit_scale_init_value",val:" = 2.6592"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AltCLIPConfig.text_config",description:`<strong>text_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPTextConfig">AltCLIPTextConfig</a>.`,name:"text_config"},{anchor:"transformers.AltCLIPConfig.vision_config",description:`<strong>vision_config</strong> (<code>dict</code>, <em>optional</em>) &#x2014;
Dictionary of configuration options used to initialize <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPVisionConfig">AltCLIPVisionConfig</a>.`,name:"vision_config"},{anchor:"transformers.AltCLIPConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimentionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.AltCLIPConfig.logit_scale_init_value",description:`<strong>logit_scale_init_value</strong> (<code>float</code>, <em>optional</em>, defaults to 2.6592) &#x2014;
The inital value of the <em>logit_scale</em> paramter. Default is used as per the original CLIP implementation.`,name:"logit_scale_init_value"},{anchor:"transformers.AltCLIPConfig.kwargs",description:`<strong>kwargs</strong> (<em>optional</em>) &#x2014;
Dictionary of keyword arguments.`,name:"kwargs"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/configuration_altclip.py#L259"}}),O=new ue({props:{anchor:"transformers.AltCLIPConfig.example",$$slots:{default:[sn]},$$scope:{ctx:y}}}),we=new w({props:{name:"from_text_vision_configs",anchor:"transformers.AltCLIPConfig.from_text_vision_configs",parameters:[{name:"text_config",val:": AltCLIPTextConfig"},{name:"vision_config",val:": AltCLIPVisionConfig"},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/configuration_altclip.py#L394",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>An instance of a configuration object</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPConfig"
>AltCLIPConfig</a></p>
`}}),xe=new q({props:{title:"AltCLIPTextConfig",local:"transformers.AltCLIPTextConfig",headingTag:"h2"}}),Je=new w({props:{name:"class transformers.AltCLIPTextConfig",anchor:"transformers.AltCLIPTextConfig",parameters:[{name:"vocab_size",val:" = 250002"},{name:"hidden_size",val:" = 1024"},{name:"num_hidden_layers",val:" = 24"},{name:"num_attention_heads",val:" = 16"},{name:"intermediate_size",val:" = 4096"},{name:"hidden_act",val:" = 'gelu'"},{name:"hidden_dropout_prob",val:" = 0.1"},{name:"attention_probs_dropout_prob",val:" = 0.1"},{name:"max_position_embeddings",val:" = 514"},{name:"type_vocab_size",val:" = 1"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"pad_token_id",val:" = 1"},{name:"bos_token_id",val:" = 0"},{name:"eos_token_id",val:" = 2"},{name:"position_embedding_type",val:" = 'absolute'"},{name:"use_cache",val:" = True"},{name:"project_dim",val:" = 768"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AltCLIPTextConfig.vocab_size",description:`<strong>vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 250002) &#x2014;
Vocabulary size of the AltCLIP model. Defines the number of different tokens that can be represented by the
<code>inputs_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPTextModel">AltCLIPTextModel</a>.`,name:"vocab_size"},{anchor:"transformers.AltCLIPTextConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1024) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.AltCLIPTextConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 24) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.AltCLIPTextConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 16) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.AltCLIPTextConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 4096) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (often named feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.AltCLIPTextConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>Callable</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;silu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.AltCLIPTextConfig.hidden_dropout_prob",description:`<strong>hidden_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.`,name:"hidden_dropout_prob"},{anchor:"transformers.AltCLIPTextConfig.attention_probs_dropout_prob",description:`<strong>attention_probs_dropout_prob</strong> (<code>float</code>, <em>optional</em>, defaults to 0.1) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_probs_dropout_prob"},{anchor:"transformers.AltCLIPTextConfig.max_position_embeddings",description:`<strong>max_position_embeddings</strong> (<code>int</code>, <em>optional</em>, defaults to 514) &#x2014;
The maximum sequence length that this model might ever be used with. Typically set this to something large
just in case (e.g., 512 or 1024 or 2048).`,name:"max_position_embeddings"},{anchor:"transformers.AltCLIPTextConfig.type_vocab_size",description:`<strong>type_vocab_size</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014;
The vocabulary size of the <code>token_type_ids</code> passed when calling <a href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPTextModel">AltCLIPTextModel</a>`,name:"type_vocab_size"},{anchor:"transformers.AltCLIPTextConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.AltCLIPTextConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"},{anchor:"transformers.AltCLIPTextConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.AltCLIPTextConfig.pad_token_id",description:"<strong>pad_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 1) &#x2014; The id of the <em>padding</em> token.",name:"pad_token_id"},{anchor:"transformers.AltCLIPTextConfig.bos_token_id",description:"<strong>bos_token_id</strong> (<code>int</code>, <em>optional</em>, defaults to 0) &#x2014; The id of the <em>beginning-of-sequence</em> token.",name:"bos_token_id"},{anchor:"transformers.AltCLIPTextConfig.eos_token_id",description:`<strong>eos_token_id</strong> (<code>Union[int, List[int]]</code>, <em>optional</em>, defaults to 2) &#x2014;
The id of the <em>end-of-sequence</em> token. Optionally, use a list to set multiple <em>end-of-sequence</em> tokens.`,name:"eos_token_id"},{anchor:"transformers.AltCLIPTextConfig.position_embedding_type",description:`<strong>position_embedding_type</strong> (<code>str</code>, <em>optional</em>, defaults to <code>&quot;absolute&quot;</code>) &#x2014;
Type of position embedding. Choose one of <code>&quot;absolute&quot;</code>, <code>&quot;relative_key&quot;</code>, <code>&quot;relative_key_query&quot;</code>. For
positional embeddings use <code>&quot;absolute&quot;</code>. For more information on <code>&quot;relative_key&quot;</code>, please refer to
<a href="https://arxiv.org/abs/1803.02155" rel="nofollow">Self-Attention with Relative Position Representations (Shaw et al.)</a>.
For more information on <code>&quot;relative_key_query&quot;</code>, please refer to <em>Method 4</em> in <a href="https://arxiv.org/abs/2009.13658" rel="nofollow">Improve Transformer Models
with Better Relative Position Embeddings (Huang et al.)</a>.`,name:"position_embedding_type"},{anchor:"transformers.AltCLIPTextConfig.use_cache",description:`<strong>use_cache</strong> (<code>bool</code>, <em>optional</em>, defaults to <code>True</code>) &#x2014;
Whether or not the model should return the last key/values attentions (not used by all models). Only
relevant if <code>config.is_decoder=True</code>.`,name:"use_cache"},{anchor:"transformers.AltCLIPTextConfig.project_dim",description:`<strong>project_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
The dimentions of the teacher model before the mapping layer.`,name:"project_dim"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/configuration_altclip.py#L31"}}),ee=new ue({props:{anchor:"transformers.AltCLIPTextConfig.example",$$slots:{default:[an]},$$scope:{ctx:y}}}),Le=new q({props:{title:"AltCLIPVisionConfig",local:"transformers.AltCLIPVisionConfig",headingTag:"h2"}}),Ae=new w({props:{name:"class transformers.AltCLIPVisionConfig",anchor:"transformers.AltCLIPVisionConfig",parameters:[{name:"hidden_size",val:" = 768"},{name:"intermediate_size",val:" = 3072"},{name:"projection_dim",val:" = 512"},{name:"num_hidden_layers",val:" = 12"},{name:"num_attention_heads",val:" = 12"},{name:"num_channels",val:" = 3"},{name:"image_size",val:" = 224"},{name:"patch_size",val:" = 32"},{name:"hidden_act",val:" = 'quick_gelu'"},{name:"layer_norm_eps",val:" = 1e-05"},{name:"attention_dropout",val:" = 0.0"},{name:"initializer_range",val:" = 0.02"},{name:"initializer_factor",val:" = 1.0"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AltCLIPVisionConfig.hidden_size",description:`<strong>hidden_size</strong> (<code>int</code>, <em>optional</em>, defaults to 768) &#x2014;
Dimensionality of the encoder layers and the pooler layer.`,name:"hidden_size"},{anchor:"transformers.AltCLIPVisionConfig.intermediate_size",description:`<strong>intermediate_size</strong> (<code>int</code>, <em>optional</em>, defaults to 3072) &#x2014;
Dimensionality of the &#x201C;intermediate&#x201D; (i.e., feed-forward) layer in the Transformer encoder.`,name:"intermediate_size"},{anchor:"transformers.AltCLIPVisionConfig.projection_dim",description:`<strong>projection_dim</strong> (<code>int</code>, <em>optional</em>, defaults to 512) &#x2014;
Dimentionality of text and vision projection layers.`,name:"projection_dim"},{anchor:"transformers.AltCLIPVisionConfig.num_hidden_layers",description:`<strong>num_hidden_layers</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of hidden layers in the Transformer encoder.`,name:"num_hidden_layers"},{anchor:"transformers.AltCLIPVisionConfig.num_attention_heads",description:`<strong>num_attention_heads</strong> (<code>int</code>, <em>optional</em>, defaults to 12) &#x2014;
Number of attention heads for each attention layer in the Transformer encoder.`,name:"num_attention_heads"},{anchor:"transformers.AltCLIPVisionConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.AltCLIPVisionConfig.image_size",description:`<strong>image_size</strong> (<code>int</code>, <em>optional</em>, defaults to 224) &#x2014;
The size (resolution) of each image.`,name:"image_size"},{anchor:"transformers.AltCLIPVisionConfig.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, <em>optional</em>, defaults to 32) &#x2014;
The size (resolution) of each patch.`,name:"patch_size"},{anchor:"transformers.AltCLIPVisionConfig.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;quick_gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in the encoder and pooler. If string, <code>&quot;gelu&quot;</code>,
<code>&quot;relu&quot;</code>, <code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> \`<code>&quot;quick_gelu&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.AltCLIPVisionConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-05) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.AltCLIPVisionConfig.attention_dropout",description:`<strong>attention_dropout</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_dropout"},{anchor:"transformers.AltCLIPVisionConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.AltCLIPVisionConfig.initializer_factor",description:`<strong>initializer_factor</strong> (<code>float</code>, <em>optional</em>, defaults to 1.0) &#x2014;
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).`,name:"initializer_factor"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/configuration_altclip.py#L149"}}),te=new ue({props:{anchor:"transformers.AltCLIPVisionConfig.example",$$slots:{default:[rn]},$$scope:{ctx:y}}}),ke=new q({props:{title:"AltCLIPProcessor",local:"transformers.AltCLIPProcessor",headingTag:"h2"}}),Ue=new w({props:{name:"class transformers.AltCLIPProcessor",anchor:"transformers.AltCLIPProcessor",parameters:[{name:"image_processor",val:" = None"},{name:"tokenizer",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.AltCLIPProcessor.image_processor",description:`<strong>image_processor</strong> (<a href="/docs/transformers/main/ja/model_doc/clip#transformers.CLIPImageProcessor">CLIPImageProcessor</a>, <em>optional</em>) &#x2014;
The image processor is a required input.`,name:"image_processor"},{anchor:"transformers.AltCLIPProcessor.tokenizer",description:`<strong>tokenizer</strong> (<code>XLMRobertaTokenizerFast</code>, <em>optional</em>) &#x2014;
The tokenizer is a required input.`,name:"tokenizer"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/processing_altclip.py#L24"}}),Be=new w({props:{name:"batch_decode",anchor:"transformers.AltCLIPProcessor.batch_decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/processing_altclip.py#L114"}}),ze=new w({props:{name:"decode",anchor:"transformers.AltCLIPProcessor.decode",parameters:[{name:"*args",val:""},{name:"**kwargs",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/processing_altclip.py#L121"}}),We=new q({props:{title:"AltCLIPModel",local:"transformers.AltCLIPModel",headingTag:"h2"}}),Ze=new w({props:{name:"class transformers.AltCLIPModel",anchor:"transformers.AltCLIPModel",parameters:[{name:"config",val:": AltCLIPConfig"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py#L1461"}}),Ve=new w({props:{name:"forward",anchor:"transformers.AltCLIPModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"pixel_values",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"return_loss",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AltCLIPModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.AltCLIPModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.AltCLIPModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.AltCLIPModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.AltCLIPModel.forward.return_loss",description:`<strong>return_loss</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the contrastive loss.`,name:"return_loss"},{anchor:"transformers.AltCLIPModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AltCLIPModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AltCLIPModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py#L1588",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.altclip.modeling_altclip.AltCLIPOutput</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.altclip.configuration_altclip.AltCLIPConfig'&gt;</code>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>return_loss</code> is <code>True</code>) — Contrastive loss for image-text similarity.</li>
<li><strong>logits_per_image:(<code>torch.FloatTensor</code></strong> of shape <code>(image_batch_size, text_batch_size)</code>) — The scaled dot product scores between <code>image_embeds</code> and <code>text_embeds</code>. This represents the image-text
similarity scores.</li>
<li><strong>logits_per_text:(<code>torch.FloatTensor</code></strong> of shape <code>(text_batch_size, image_batch_size)</code>) — The scaled dot product scores between <code>text_embeds</code> and <code>image_embeds</code>. This represents the text-image
similarity scores.</li>
<li><strong>text_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) — The text embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPTextModel"
>AltCLIPTextModel</a>.</li>
<li><strong>image_embeds(<code>torch.FloatTensor</code></strong> of shape <code>(batch_size, output_dim</code>) — The image embeddings obtained by applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPVisionModel"
>AltCLIPVisionModel</a>.</li>
<li><strong>text_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPTextModel"
>AltCLIPTextModel</a>.</li>
<li><strong>vision_model_output(<code>BaseModelOutputWithPooling</code>):</strong>
The output of the <a
  href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPVisionModel"
>AltCLIPVisionModel</a>.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.altclip.modeling_altclip.AltCLIPOutput</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),se=new ut({props:{$$slots:{default:[ln]},$$scope:{ctx:y}}}),ae=new ue({props:{anchor:"transformers.AltCLIPModel.forward.example",$$slots:{default:[dn]},$$scope:{ctx:y}}}),He=new w({props:{name:"get_text_features",anchor:"transformers.AltCLIPModel.get_text_features",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"token_type_ids",val:" = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AltCLIPModel.get_text_features.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.AltCLIPModel.get_text_features.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.AltCLIPModel.get_text_features.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.AltCLIPModel.get_text_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AltCLIPModel.get_text_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AltCLIPModel.get_text_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py#L1495",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The text embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPTextModel"
>AltCLIPTextModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>text_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),re=new ut({props:{$$slots:{default:[cn]},$$scope:{ctx:y}}}),le=new ue({props:{anchor:"transformers.AltCLIPModel.get_text_features.example",$$slots:{default:[mn]},$$scope:{ctx:y}}}),Qe=new w({props:{name:"get_image_features",anchor:"transformers.AltCLIPModel.get_image_features",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AltCLIPModel.get_image_features.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.AltCLIPModel.get_image_features.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AltCLIPModel.get_image_features.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AltCLIPModel.get_image_features.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py#L1542",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>The image embeddings obtained by
applying the projection layer to the pooled output of <a
  href="/docs/transformers/main/ja/model_doc/altclip#transformers.AltCLIPVisionModel"
>AltCLIPVisionModel</a>.</p>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p>image_features (<code>torch.FloatTensor</code> of shape <code>(batch_size, output_dim</code>)</p>
`}}),ie=new ut({props:{$$slots:{default:[pn]},$$scope:{ctx:y}}}),de=new ue({props:{anchor:"transformers.AltCLIPModel.get_image_features.example",$$slots:{default:[fn]},$$scope:{ctx:y}}}),Ee=new q({props:{title:"AltCLIPTextModel",local:"transformers.AltCLIPTextModel",headingTag:"h2"}}),Ge=new w({props:{name:"class transformers.AltCLIPTextModel",anchor:"transformers.AltCLIPTextModel",parameters:[{name:"config",val:""}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py#L1369"}}),Fe=new w({props:{name:"forward",anchor:"transformers.AltCLIPTextModel.forward",parameters:[{name:"input_ids",val:": Optional = None"},{name:"attention_mask",val:": Optional = None"},{name:"token_type_ids",val:": Optional = None"},{name:"position_ids",val:": Optional = None"},{name:"head_mask",val:": Optional = None"},{name:"inputs_embeds",val:": Optional = None"},{name:"encoder_hidden_states",val:": Optional = None"},{name:"encoder_attention_mask",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"return_dict",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AltCLIPTextModel.forward.input_ids",description:`<strong>input_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>) &#x2014;
Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide
it.</p>
<p>Indices can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoTokenizer">AutoTokenizer</a>. See <a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.encode">PreTrainedTokenizer.encode()</a> and
<a href="/docs/transformers/main/ja/main_classes/tokenizer#transformers.PreTrainedTokenizerFast.__call__">PreTrainedTokenizer.<strong>call</strong>()</a> for details.</p>
<p><a href="../glossary#input-ids">What are input IDs?</a>`,name:"input_ids"},{anchor:"transformers.AltCLIPTextModel.forward.attention_mask",description:`<strong>attention_mask</strong> (<code>torch.Tensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Mask to avoid performing attention on padding token indices. Mask values selected in <code>[0, 1]</code>:</p>
<ul>
<li>1 for tokens that are <strong>not masked</strong>,</li>
<li>0 for tokens that are <strong>masked</strong>.</li>
</ul>
<p><a href="../glossary#attention-mask">What are attention masks?</a>`,name:"attention_mask"},{anchor:"transformers.AltCLIPTextModel.forward.position_ids",description:`<strong>position_ids</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size, sequence_length)</code>, <em>optional</em>) &#x2014;
Indices of positions of each input sequence tokens in the position embeddings. Selected in the range <code>[0, config.max_position_embeddings - 1]</code>.</p>
<p><a href="../glossary#position-ids">What are position IDs?</a>`,name:"position_ids"},{anchor:"transformers.AltCLIPTextModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AltCLIPTextModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AltCLIPTextModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py#L1388",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndProjection</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.altclip.configuration_altclip.AltCLIPTextConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
<li>
<p><strong>projection_state</strong> (<code>tuple(torch.FloatTensor)</code>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> of shape <code>(batch_size,config.project_dim)</code>.</p>
<p>Text embeddings before the projection layer, used to mimic the last hidden state of the teacher encoder.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndProjection</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ce=new ut({props:{$$slots:{default:[gn]},$$scope:{ctx:y}}}),me=new ue({props:{anchor:"transformers.AltCLIPTextModel.forward.example",$$slots:{default:[un]},$$scope:{ctx:y}}}),Ne=new q({props:{title:"AltCLIPVisionModel",local:"transformers.AltCLIPVisionModel",headingTag:"h2"}}),Re=new w({props:{name:"class transformers.AltCLIPVisionModel",anchor:"transformers.AltCLIPVisionModel",parameters:[{name:"config",val:": AltCLIPVisionConfig"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py#L1138"}}),qe=new w({props:{name:"forward",anchor:"transformers.AltCLIPVisionModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_attentions",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.AltCLIPVisionModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using
<a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">CLIPImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.AltCLIPVisionModel.forward.output_attentions",description:`<strong>output_attentions</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the attentions tensors of all attention layers. See <code>attentions</code> under returned
tensors for more detail.`,name:"output_attentions"},{anchor:"transformers.AltCLIPVisionModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.AltCLIPVisionModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/altclip/modeling_altclip.py#L1151",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<code>&lt;class 'transformers.models.altclip.configuration_altclip.AltCLIPVisionConfig'&gt;</code>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state of the first token of the sequence (classification token) after further processing
through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
the classification token after processing through a linear layer and a tanh activation function. The linear
layer weights are trained from the next sentence prediction (classification) objective during pretraining.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, sequence_length, hidden_size)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
<li>
<p><strong>attentions</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_attentions=True</code> is passed or when <code>config.output_attentions=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for each layer) of shape <code>(batch_size, num_heads, sequence_length, sequence_length)</code>.</p>
<p>Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
heads.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.BaseModelOutputWithPooling"
>transformers.modeling_outputs.BaseModelOutputWithPooling</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),pe=new ut({props:{$$slots:{default:[hn]},$$scope:{ctx:y}}}),fe=new ue({props:{anchor:"transformers.AltCLIPVisionModel.forward.example",$$slots:{default:[_n]},$$scope:{ctx:y}}}),{c(){o=m("meta"),b=l(),a=m("p"),r=l(),f(C.$$.fragment),t=l(),f(T.$$.fragment),_t=l(),he=m("p"),he.innerHTML=wo,Mt=l(),_e=m("p"),_e.textContent=xo,Ct=l(),Me=m("p"),Me.innerHTML=Jo,bt=l(),Ce=m("p"),Ce.innerHTML=Lo,Tt=l(),f(be.$$.fragment),It=l(),Te=m("p"),Te.textContent=Ao,yt=l(),Ie=m("p"),Ie.textContent=ko,$t=l(),ye=m("p"),ye.innerHTML=Uo,vt=l(),$e=m("p"),$e.innerHTML=Bo,jt=l(),f(ve.$$.fragment),Pt=l(),f(D.$$.fragment),wt=l(),f(je.$$.fragment),xt=l(),j=m("div"),f(Pe.$$.fragment),Ft=l(),Ke=m("p"),Ke.innerHTML=zo,Nt=l(),et=m("p"),et.innerHTML=Wo,Rt=l(),f(O.$$.fragment),qt=l(),K=m("div"),f(we.$$.fragment),Xt=l(),tt=m("p"),tt.innerHTML=Zo,Jt=l(),f(xe.$$.fragment),Lt=l(),J=m("div"),f(Je.$$.fragment),St=l(),ot=m("p"),ot.innerHTML=Vo,Yt=l(),nt=m("p"),nt.innerHTML=Ho,Dt=l(),f(ee.$$.fragment),At=l(),f(Le.$$.fragment),kt=l(),L=m("div"),f(Ae.$$.fragment),Ot=l(),st=m("p"),st.innerHTML=Qo,Kt=l(),at=m("p"),at.innerHTML=Eo,eo=l(),f(te.$$.fragment),Ut=l(),f(ke.$$.fragment),Bt=l(),P=m("div"),f(Ue.$$.fragment),to=l(),rt=m("p"),rt.textContent=Go,oo=l(),lt=m("p"),lt.innerHTML=Fo,no=l(),oe=m("div"),f(Be.$$.fragment),so=l(),it=m("p"),it.innerHTML=No,ao=l(),ne=m("div"),f(ze.$$.fragment),ro=l(),dt=m("p"),dt.innerHTML=Ro,zt=l(),f(We.$$.fragment),Wt=l(),A=m("div"),f(Ze.$$.fragment),lo=l(),B=m("div"),f(Ve.$$.fragment),io=l(),ct=m("p"),ct.innerHTML=qo,co=l(),f(se.$$.fragment),mo=l(),f(ae.$$.fragment),po=l(),z=m("div"),f(He.$$.fragment),fo=l(),mt=m("p"),mt.innerHTML=Xo,go=l(),f(re.$$.fragment),uo=l(),f(le.$$.fragment),ho=l(),W=m("div"),f(Qe.$$.fragment),_o=l(),pt=m("p"),pt.innerHTML=So,Mo=l(),f(ie.$$.fragment),Co=l(),f(de.$$.fragment),Zt=l(),f(Ee.$$.fragment),Vt=l(),X=m("div"),f(Ge.$$.fragment),bo=l(),Z=m("div"),f(Fe.$$.fragment),To=l(),ft=m("p"),ft.innerHTML=Yo,Io=l(),f(ce.$$.fragment),yo=l(),f(me.$$.fragment),Ht=l(),f(Ne.$$.fragment),Qt=l(),S=m("div"),f(Re.$$.fragment),$o=l(),V=m("div"),f(qe.$$.fragment),vo=l(),gt=m("p"),gt.innerHTML=Do,jo=l(),f(pe.$$.fragment),Po=l(),f(fe.$$.fragment),Et=l(),ht=m("p"),this.h()},l(e){const n=on("svelte-u9bgzb",document.head);o=p(n,"META",{name:!0,content:!0}),n.forEach(s),b=i(e),a=p(e,"P",{}),$(a).forEach(s),r=i(e),g(C.$$.fragment,e),t=i(e),g(T.$$.fragment,e),_t=i(e),he=p(e,"P",{"data-svelte-h":!0}),I(he)!=="svelte-14pk3xo"&&(he.innerHTML=wo),Mt=i(e),_e=p(e,"P",{"data-svelte-h":!0}),I(_e)!=="svelte-1pvwld5"&&(_e.textContent=xo),Ct=i(e),Me=p(e,"P",{"data-svelte-h":!0}),I(Me)!=="svelte-4lp1ea"&&(Me.innerHTML=Jo),bt=i(e),Ce=p(e,"P",{"data-svelte-h":!0}),I(Ce)!=="svelte-36nfoc"&&(Ce.innerHTML=Lo),Tt=i(e),g(be.$$.fragment,e),It=i(e),Te=p(e,"P",{"data-svelte-h":!0}),I(Te)!=="svelte-17r5bsi"&&(Te.textContent=Ao),yt=i(e),Ie=p(e,"P",{"data-svelte-h":!0}),I(Ie)!=="svelte-1dj6t5k"&&(Ie.textContent=ko),$t=i(e),ye=p(e,"P",{"data-svelte-h":!0}),I(ye)!=="svelte-pyvt95"&&(ye.innerHTML=Uo),vt=i(e),$e=p(e,"P",{"data-svelte-h":!0}),I($e)!=="svelte-5rd041"&&($e.innerHTML=Bo),jt=i(e),g(ve.$$.fragment,e),Pt=i(e),g(D.$$.fragment,e),wt=i(e),g(je.$$.fragment,e),xt=i(e),j=p(e,"DIV",{class:!0});var k=$(j);g(Pe.$$.fragment,k),Ft=i(k),Ke=p(k,"P",{"data-svelte-h":!0}),I(Ke)!=="svelte-u2p3oo"&&(Ke.innerHTML=zo),Nt=i(k),et=p(k,"P",{"data-svelte-h":!0}),I(et)!=="svelte-1s6wgpv"&&(et.innerHTML=Wo),Rt=i(k),g(O.$$.fragment,k),qt=i(k),K=p(k,"DIV",{class:!0});var Xe=$(K);g(we.$$.fragment,Xe),Xt=i(Xe),tt=p(Xe,"P",{"data-svelte-h":!0}),I(tt)!=="svelte-1wc9uoh"&&(tt.innerHTML=Zo),Xe.forEach(s),k.forEach(s),Jt=i(e),g(xe.$$.fragment,e),Lt=i(e),J=p(e,"DIV",{class:!0});var H=$(J);g(Je.$$.fragment,H),St=i(H),ot=p(H,"P",{"data-svelte-h":!0}),I(ot)!=="svelte-za4883"&&(ot.innerHTML=Vo),Yt=i(H),nt=p(H,"P",{"data-svelte-h":!0}),I(nt)!=="svelte-1s6wgpv"&&(nt.innerHTML=Ho),Dt=i(H),g(ee.$$.fragment,H),H.forEach(s),At=i(e),g(Le.$$.fragment,e),kt=i(e),L=p(e,"DIV",{class:!0});var Q=$(L);g(Ae.$$.fragment,Q),Ot=i(Q),st=p(Q,"P",{"data-svelte-h":!0}),I(st)!=="svelte-u2p3oo"&&(st.innerHTML=Qo),Kt=i(Q),at=p(Q,"P",{"data-svelte-h":!0}),I(at)!=="svelte-1s6wgpv"&&(at.innerHTML=Eo),eo=i(Q),g(te.$$.fragment,Q),Q.forEach(s),Ut=i(e),g(ke.$$.fragment,e),Bt=i(e),P=p(e,"DIV",{class:!0});var U=$(P);g(Ue.$$.fragment,U),to=i(U),rt=p(U,"P",{"data-svelte-h":!0}),I(rt)!=="svelte-1rdismf"&&(rt.textContent=Go),oo=i(U),lt=p(U,"P",{"data-svelte-h":!0}),I(lt)!=="svelte-1udeqjk"&&(lt.innerHTML=Fo),no=i(U),oe=p(U,"DIV",{class:!0});var Se=$(oe);g(Be.$$.fragment,Se),so=i(Se),it=p(Se,"P",{"data-svelte-h":!0}),I(it)!=="svelte-1069qq5"&&(it.innerHTML=No),Se.forEach(s),ao=i(U),ne=p(U,"DIV",{class:!0});var Ye=$(ne);g(ze.$$.fragment,Ye),ro=i(Ye),dt=p(Ye,"P",{"data-svelte-h":!0}),I(dt)!=="svelte-1snuiej"&&(dt.innerHTML=Ro),Ye.forEach(s),U.forEach(s),zt=i(e),g(We.$$.fragment,e),Wt=i(e),A=p(e,"DIV",{class:!0});var E=$(A);g(Ze.$$.fragment,E),lo=i(E),B=p(E,"DIV",{class:!0});var G=$(B);g(Ve.$$.fragment,G),io=i(G),ct=p(G,"P",{"data-svelte-h":!0}),I(ct)!=="svelte-1lva24"&&(ct.innerHTML=qo),co=i(G),g(se.$$.fragment,G),mo=i(G),g(ae.$$.fragment,G),G.forEach(s),po=i(E),z=p(E,"DIV",{class:!0});var F=$(z);g(He.$$.fragment,F),fo=i(F),mt=p(F,"P",{"data-svelte-h":!0}),I(mt)!=="svelte-1lva24"&&(mt.innerHTML=Xo),go=i(F),g(re.$$.fragment,F),uo=i(F),g(le.$$.fragment,F),F.forEach(s),ho=i(E),W=p(E,"DIV",{class:!0});var N=$(W);g(Qe.$$.fragment,N),_o=i(N),pt=p(N,"P",{"data-svelte-h":!0}),I(pt)!=="svelte-1lva24"&&(pt.innerHTML=So),Mo=i(N),g(ie.$$.fragment,N),Co=i(N),g(de.$$.fragment,N),N.forEach(s),E.forEach(s),Zt=i(e),g(Ee.$$.fragment,e),Vt=i(e),X=p(e,"DIV",{class:!0});var De=$(X);g(Ge.$$.fragment,De),bo=i(De),Z=p(De,"DIV",{class:!0});var R=$(Z);g(Fe.$$.fragment,R),To=i(R),ft=p(R,"P",{"data-svelte-h":!0}),I(ft)!=="svelte-yc4ca"&&(ft.innerHTML=Yo),Io=i(R),g(ce.$$.fragment,R),yo=i(R),g(me.$$.fragment,R),R.forEach(s),De.forEach(s),Ht=i(e),g(Ne.$$.fragment,e),Qt=i(e),S=p(e,"DIV",{class:!0});var Oe=$(S);g(Re.$$.fragment,Oe),$o=i(Oe),V=p(Oe,"DIV",{class:!0});var ge=$(V);g(qe.$$.fragment,ge),vo=i(ge),gt=p(ge,"P",{"data-svelte-h":!0}),I(gt)!=="svelte-1mmln6k"&&(gt.innerHTML=Do),jo=i(ge),g(pe.$$.fragment,ge),Po=i(ge),g(fe.$$.fragment,ge),ge.forEach(s),Oe.forEach(s),Et=i(e),ht=p(e,"P",{}),$(ht).forEach(s),this.h()},h(){v(o,"name","hf:doc:metadata"),v(o,"content",Cn),v(K,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(j,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(J,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(L,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(oe,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(ne,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(P,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(A,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(V,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),v(S,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,n){c(document.head,o),d(e,b,n),d(e,a,n),d(e,r,n),u(C,e,n),d(e,t,n),u(T,e,n),d(e,_t,n),d(e,he,n),d(e,Mt,n),d(e,_e,n),d(e,Ct,n),d(e,Me,n),d(e,bt,n),d(e,Ce,n),d(e,Tt,n),u(be,e,n),d(e,It,n),d(e,Te,n),d(e,yt,n),d(e,Ie,n),d(e,$t,n),d(e,ye,n),d(e,vt,n),d(e,$e,n),d(e,jt,n),u(ve,e,n),d(e,Pt,n),u(D,e,n),d(e,wt,n),u(je,e,n),d(e,xt,n),d(e,j,n),u(Pe,j,null),c(j,Ft),c(j,Ke),c(j,Nt),c(j,et),c(j,Rt),u(O,j,null),c(j,qt),c(j,K),u(we,K,null),c(K,Xt),c(K,tt),d(e,Jt,n),u(xe,e,n),d(e,Lt,n),d(e,J,n),u(Je,J,null),c(J,St),c(J,ot),c(J,Yt),c(J,nt),c(J,Dt),u(ee,J,null),d(e,At,n),u(Le,e,n),d(e,kt,n),d(e,L,n),u(Ae,L,null),c(L,Ot),c(L,st),c(L,Kt),c(L,at),c(L,eo),u(te,L,null),d(e,Ut,n),u(ke,e,n),d(e,Bt,n),d(e,P,n),u(Ue,P,null),c(P,to),c(P,rt),c(P,oo),c(P,lt),c(P,no),c(P,oe),u(Be,oe,null),c(oe,so),c(oe,it),c(P,ao),c(P,ne),u(ze,ne,null),c(ne,ro),c(ne,dt),d(e,zt,n),u(We,e,n),d(e,Wt,n),d(e,A,n),u(Ze,A,null),c(A,lo),c(A,B),u(Ve,B,null),c(B,io),c(B,ct),c(B,co),u(se,B,null),c(B,mo),u(ae,B,null),c(A,po),c(A,z),u(He,z,null),c(z,fo),c(z,mt),c(z,go),u(re,z,null),c(z,uo),u(le,z,null),c(A,ho),c(A,W),u(Qe,W,null),c(W,_o),c(W,pt),c(W,Mo),u(ie,W,null),c(W,Co),u(de,W,null),d(e,Zt,n),u(Ee,e,n),d(e,Vt,n),d(e,X,n),u(Ge,X,null),c(X,bo),c(X,Z),u(Fe,Z,null),c(Z,To),c(Z,ft),c(Z,Io),u(ce,Z,null),c(Z,yo),u(me,Z,null),d(e,Ht,n),u(Ne,e,n),d(e,Qt,n),d(e,S,n),u(Re,S,null),c(S,$o),c(S,V),u(qe,V,null),c(V,vo),c(V,gt),c(V,jo),u(pe,V,null),c(V,Po),u(fe,V,null),d(e,Et,n),d(e,ht,n),Gt=!0},p(e,[n]){const k={};n&2&&(k.$$scope={dirty:n,ctx:e}),D.$set(k);const Xe={};n&2&&(Xe.$$scope={dirty:n,ctx:e}),O.$set(Xe);const H={};n&2&&(H.$$scope={dirty:n,ctx:e}),ee.$set(H);const Q={};n&2&&(Q.$$scope={dirty:n,ctx:e}),te.$set(Q);const U={};n&2&&(U.$$scope={dirty:n,ctx:e}),se.$set(U);const Se={};n&2&&(Se.$$scope={dirty:n,ctx:e}),ae.$set(Se);const Ye={};n&2&&(Ye.$$scope={dirty:n,ctx:e}),re.$set(Ye);const E={};n&2&&(E.$$scope={dirty:n,ctx:e}),le.$set(E);const G={};n&2&&(G.$$scope={dirty:n,ctx:e}),ie.$set(G);const F={};n&2&&(F.$$scope={dirty:n,ctx:e}),de.$set(F);const N={};n&2&&(N.$$scope={dirty:n,ctx:e}),ce.$set(N);const De={};n&2&&(De.$$scope={dirty:n,ctx:e}),me.$set(De);const R={};n&2&&(R.$$scope={dirty:n,ctx:e}),pe.$set(R);const Oe={};n&2&&(Oe.$$scope={dirty:n,ctx:e}),fe.$set(Oe)},i(e){Gt||(h(C.$$.fragment,e),h(T.$$.fragment,e),h(be.$$.fragment,e),h(ve.$$.fragment,e),h(D.$$.fragment,e),h(je.$$.fragment,e),h(Pe.$$.fragment,e),h(O.$$.fragment,e),h(we.$$.fragment,e),h(xe.$$.fragment,e),h(Je.$$.fragment,e),h(ee.$$.fragment,e),h(Le.$$.fragment,e),h(Ae.$$.fragment,e),h(te.$$.fragment,e),h(ke.$$.fragment,e),h(Ue.$$.fragment,e),h(Be.$$.fragment,e),h(ze.$$.fragment,e),h(We.$$.fragment,e),h(Ze.$$.fragment,e),h(Ve.$$.fragment,e),h(se.$$.fragment,e),h(ae.$$.fragment,e),h(He.$$.fragment,e),h(re.$$.fragment,e),h(le.$$.fragment,e),h(Qe.$$.fragment,e),h(ie.$$.fragment,e),h(de.$$.fragment,e),h(Ee.$$.fragment,e),h(Ge.$$.fragment,e),h(Fe.$$.fragment,e),h(ce.$$.fragment,e),h(me.$$.fragment,e),h(Ne.$$.fragment,e),h(Re.$$.fragment,e),h(qe.$$.fragment,e),h(pe.$$.fragment,e),h(fe.$$.fragment,e),Gt=!0)},o(e){_(C.$$.fragment,e),_(T.$$.fragment,e),_(be.$$.fragment,e),_(ve.$$.fragment,e),_(D.$$.fragment,e),_(je.$$.fragment,e),_(Pe.$$.fragment,e),_(O.$$.fragment,e),_(we.$$.fragment,e),_(xe.$$.fragment,e),_(Je.$$.fragment,e),_(ee.$$.fragment,e),_(Le.$$.fragment,e),_(Ae.$$.fragment,e),_(te.$$.fragment,e),_(ke.$$.fragment,e),_(Ue.$$.fragment,e),_(Be.$$.fragment,e),_(ze.$$.fragment,e),_(We.$$.fragment,e),_(Ze.$$.fragment,e),_(Ve.$$.fragment,e),_(se.$$.fragment,e),_(ae.$$.fragment,e),_(He.$$.fragment,e),_(re.$$.fragment,e),_(le.$$.fragment,e),_(Qe.$$.fragment,e),_(ie.$$.fragment,e),_(de.$$.fragment,e),_(Ee.$$.fragment,e),_(Ge.$$.fragment,e),_(Fe.$$.fragment,e),_(ce.$$.fragment,e),_(me.$$.fragment,e),_(Ne.$$.fragment,e),_(Re.$$.fragment,e),_(qe.$$.fragment,e),_(pe.$$.fragment,e),_(fe.$$.fragment,e),Gt=!1},d(e){e&&(s(b),s(a),s(r),s(t),s(_t),s(he),s(Mt),s(_e),s(Ct),s(Me),s(bt),s(Ce),s(Tt),s(It),s(Te),s(yt),s(Ie),s($t),s(ye),s(vt),s($e),s(jt),s(Pt),s(wt),s(xt),s(j),s(Jt),s(Lt),s(J),s(At),s(kt),s(L),s(Ut),s(Bt),s(P),s(zt),s(Wt),s(A),s(Zt),s(Vt),s(X),s(Ht),s(Qt),s(S),s(Et),s(ht)),s(o),M(C,e),M(T,e),M(be,e),M(ve,e),M(D,e),M(je,e),M(Pe),M(O),M(we),M(xe,e),M(Je),M(ee),M(Le,e),M(Ae),M(te),M(ke,e),M(Ue),M(Be),M(ze),M(We,e),M(Ze),M(Ve),M(se),M(ae),M(He),M(re),M(le),M(Qe),M(ie),M(de),M(Ee,e),M(Ge),M(Fe),M(ce),M(me),M(Ne,e),M(Re),M(qe),M(pe),M(fe)}}}const Cn='{"title":"AltCLIP","local":"altclip","sections":[{"title":"概要","local":"概要","sections":[],"depth":2},{"title":"使用上のヒントと使用例","local":"使用上のヒントと使用例","sections":[],"depth":2},{"title":"AltCLIPConfig","local":"transformers.AltCLIPConfig","sections":[],"depth":2},{"title":"AltCLIPTextConfig","local":"transformers.AltCLIPTextConfig","sections":[],"depth":2},{"title":"AltCLIPVisionConfig","local":"transformers.AltCLIPVisionConfig","sections":[],"depth":2},{"title":"AltCLIPProcessor","local":"transformers.AltCLIPProcessor","sections":[],"depth":2},{"title":"AltCLIPModel","local":"transformers.AltCLIPModel","sections":[],"depth":2},{"title":"AltCLIPTextModel","local":"transformers.AltCLIPTextModel","sections":[],"depth":2},{"title":"AltCLIPVisionModel","local":"transformers.AltCLIPVisionModel","sections":[],"depth":2}],"depth":1}';function bn(y){return Ko(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class wn extends en{constructor(o){super(),tn(this,o,bn,Mn,Oo,{})}}export{wn as component};
