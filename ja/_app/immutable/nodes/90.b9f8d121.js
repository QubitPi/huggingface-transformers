import{s as vn,f as _n,o as bn,n as S}from"../chunks/scheduler.9bc65507.js";import{S as Mn,i as Cn,g as c,s as l,r as _,A as yn,h as m,f as o,c as i,j as Q,u as b,x as h,k as J,y as f,a,v as M,d as C,t as y,w as x}from"../chunks/index.707bf1b6.js";import{T as Se}from"../chunks/Tip.c2ecdbf4.js";import{D as ee}from"../chunks/Docstring.17db21ae.js";import{C as Oe}from"../chunks/CodeBlock.54a9f38d.js";import{E as De}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as xn}from"../chunks/PipelineTag.44585822.js";import{H as fe}from"../chunks/Heading.342b1fa6.js";function Tn(T){let t,v="Example:",d,r,g;return r=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENvbnZOZVhUVjJDb25maWclMkMlMjBDb252TmV4dFYyTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ29udk5lWFRWMiUyMGNvbnZuZXh0djItdGlueS0xay0yMjQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQ29udk5lWFRWMkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBjb252bmV4dHYyLXRpbnktMWstMjI0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBDb252TmV4dFYyTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ConvNeXTV2Config, ConvNextV2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ConvNeXTV2 convnextv2-tiny-1k-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ConvNeXTV2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the convnextv2-tiny-1k-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ConvNextV2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=c("p"),t.textContent=v,d=l(),_(r.$$.fragment)},l(n){t=m(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-11lpom8"&&(t.textContent=v),d=i(n),b(r.$$.fragment,n)},m(n,u){a(n,t,u),a(n,d,u),M(r,n,u),g=!0},p:S,i(n){g||(C(r.$$.fragment,n),g=!0)},o(n){y(r.$$.fragment,n),g=!1},d(n){n&&(o(t),o(d)),x(r,n)}}}function $n(T){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=v},l(d){t=m(d,"P",{"data-svelte-h":!0}),h(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(d,r){a(d,t,r)},p:S,d(d){d&&o(t)}}}function wn(T){let t,v="Example:",d,r,g;return r=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMENvbnZOZXh0VjJNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dHYyLXRpbnktMWstMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwQ29udk5leHRWMk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmNvbnZuZXh0djItdGlueS0xay0yMjQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ConvNextV2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ConvNextV2Model.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">768</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`,wrap:!1}}),{c(){t=c("p"),t.textContent=v,d=l(),_(r.$$.fragment)},l(n){t=m(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-11lpom8"&&(t.textContent=v),d=i(n),b(r.$$.fragment,n)},m(n,u){a(n,t,u),a(n,d,u),M(r,n,u),g=!0},p:S,i(n){g||(C(r.$$.fragment,n),g=!0)},o(n){y(r.$$.fragment,n),g=!1},d(n){n&&(o(t),o(d)),x(r,n)}}}function jn(T){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=v},l(d){t=m(d,"P",{"data-svelte-h":!0}),h(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(d,r){a(d,t,r)},p:S,d(d){d&&o(t)}}}function Nn(T){let t,v="Example:",d,r,g;return r=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMENvbnZOZXh0VjJGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmh1Z2dpbmdmYWNlJTJGY2F0cy1pbWFnZSUyMiklMEFpbWFnZSUyMCUzRCUyMGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTVCJTIyaW1hZ2UlMjIlNUQlNUIwJTVEJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmNvbnZuZXh0djItdGlueS0xay0yMjQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDb252TmV4dFYyRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dHYyLXRpbnktMWstMjI0JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ConvNextV2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ConvNextV2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){t=c("p"),t.textContent=v,d=l(),_(r.$$.fragment)},l(n){t=m(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-11lpom8"&&(t.textContent=v),d=i(n),b(r.$$.fragment,n)},m(n,u){a(n,t,u),a(n,d,u),M(r,n,u),g=!0},p:S,i(n){g||(C(r.$$.fragment,n),g=!0)},o(n){y(r.$$.fragment,n),g=!1},d(n){n&&(o(t),o(d)),x(r,n)}}}function Vn(T){let t,v="TensorFlow models and layers in <code>transformers</code> accept two formats as input:",d,r,g="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional argument.</li>",n,u,A=`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like <code>model.fit()</code> things should “just work” for you - just
pass your inputs and labels in any format that <code>model.fit()</code> supports! If, however, you want to use the second
format outside of Keras methods like <code>fit()</code> and <code>predict()</code>, such as when creating your own layers or models with
the Keras <code>Functional</code> API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`,N,w,D=`<li>a single Tensor with <code>pixel_values</code> only and nothing else: <code>model(pixel_values)</code></li> <li>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code>model([pixel_values, attention_mask])</code> or <code>model([pixel_values, attention_mask, token_type_ids])</code></li> <li>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code>model({&quot;pixel_values&quot;: pixel_values, &quot;token_type_ids&quot;: token_type_ids})</code></li>`,V,j,O=`Note that when creating models and layers with
<a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="nofollow">subclassing</a> then you don’t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`;return{c(){t=c("p"),t.innerHTML=v,d=l(),r=c("ul"),r.innerHTML=g,n=l(),u=c("p"),u.innerHTML=A,N=l(),w=c("ul"),w.innerHTML=D,V=l(),j=c("p"),j.innerHTML=O},l(p){t=m(p,"P",{"data-svelte-h":!0}),h(t)!=="svelte-1ajbfxg"&&(t.innerHTML=v),d=i(p),r=m(p,"UL",{"data-svelte-h":!0}),h(r)!=="svelte-qm1t26"&&(r.innerHTML=g),n=i(p),u=m(p,"P",{"data-svelte-h":!0}),h(u)!=="svelte-1v9qsc5"&&(u.innerHTML=A),N=i(p),w=m(p,"UL",{"data-svelte-h":!0}),h(w)!=="svelte-99h8aq"&&(w.innerHTML=D),V=i(p),j=m(p,"P",{"data-svelte-h":!0}),h(j)!=="svelte-1an3odd"&&(j.innerHTML=O)},m(p,$){a(p,t,$),a(p,d,$),a(p,r,$),a(p,n,$),a(p,u,$),a(p,N,$),a(p,w,$),a(p,V,$),a(p,j,$)},p:S,d(p){p&&(o(t),o(d),o(r),o(n),o(u),o(N),o(w),o(V),o(j))}}}function Fn(T){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=v},l(d){t=m(d,"P",{"data-svelte-h":!0}),h(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(d,r){a(d,t,r)},p:S,d(d){d&&o(t)}}}function In(T){let t,v="Example:",d,r,g;return r=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGQ29udk5leHRWMk1vZGVsJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dHYyLXRpbnktMWstMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZDb252TmV4dFYyTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGY29udm5leHR2Mi10aW55LTFrLTIyNCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFConvNextV2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFConvNextV2Model.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">768</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`,wrap:!1}}),{c(){t=c("p"),t.textContent=v,d=l(),_(r.$$.fragment)},l(n){t=m(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-11lpom8"&&(t.textContent=v),d=i(n),b(r.$$.fragment,n)},m(n,u){a(n,t,u),a(n,d,u),M(r,n,u),g=!0},p:S,i(n){g||(C(r.$$.fragment,n),g=!0)},o(n){y(r.$$.fragment,n),g=!1},d(n){n&&(o(t),o(d)),x(r,n)}}}function kn(T){let t,v="TensorFlow models and layers in <code>transformers</code> accept two formats as input:",d,r,g="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional argument.</li>",n,u,A=`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like <code>model.fit()</code> things should “just work” for you - just
pass your inputs and labels in any format that <code>model.fit()</code> supports! If, however, you want to use the second
format outside of Keras methods like <code>fit()</code> and <code>predict()</code>, such as when creating your own layers or models with
the Keras <code>Functional</code> API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`,N,w,D=`<li>a single Tensor with <code>pixel_values</code> only and nothing else: <code>model(pixel_values)</code></li> <li>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code>model([pixel_values, attention_mask])</code> or <code>model([pixel_values, attention_mask, token_type_ids])</code></li> <li>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code>model({&quot;pixel_values&quot;: pixel_values, &quot;token_type_ids&quot;: token_type_ids})</code></li>`,V,j,O=`Note that when creating models and layers with
<a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="nofollow">subclassing</a> then you don’t need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`;return{c(){t=c("p"),t.innerHTML=v,d=l(),r=c("ul"),r.innerHTML=g,n=l(),u=c("p"),u.innerHTML=A,N=l(),w=c("ul"),w.innerHTML=D,V=l(),j=c("p"),j.innerHTML=O},l(p){t=m(p,"P",{"data-svelte-h":!0}),h(t)!=="svelte-1ajbfxg"&&(t.innerHTML=v),d=i(p),r=m(p,"UL",{"data-svelte-h":!0}),h(r)!=="svelte-qm1t26"&&(r.innerHTML=g),n=i(p),u=m(p,"P",{"data-svelte-h":!0}),h(u)!=="svelte-1v9qsc5"&&(u.innerHTML=A),N=i(p),w=m(p,"UL",{"data-svelte-h":!0}),h(w)!=="svelte-99h8aq"&&(w.innerHTML=D),V=i(p),j=m(p,"P",{"data-svelte-h":!0}),h(j)!=="svelte-1an3odd"&&(j.innerHTML=O)},m(p,$){a(p,t,$),a(p,d,$),a(p,r,$),a(p,n,$),a(p,u,$),a(p,N,$),a(p,w,$),a(p,V,$),a(p,j,$)},p:S,d(p){p&&(o(t),o(d),o(r),o(n),o(u),o(N),o(w),o(V),o(j))}}}function Jn(T){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=v},l(d){t=m(d,"P",{"data-svelte-h":!0}),h(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(d,r){a(d,t,r)},p:S,d(d){d&&o(t)}}}function Zn(T){let t,v="Example:",d,r,g;return r=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGQ29udk5leHRWMkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0ZW5zb3JmbG93JTIwYXMlMjB0ZiUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGY29udm5leHR2Mi10aW55LTFrLTIyNCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQ29udk5leHRWMkZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGY29udm5leHR2Mi10aW55LTFrLTIyNCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQWxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwaW50KHRmLm1hdGguYXJnbWF4KGxvZ2l0cyUyQyUyMGF4aXMlM0QtMSkpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFConvNextV2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFConvNextV2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){t=c("p"),t.textContent=v,d=l(),_(r.$$.fragment)},l(n){t=m(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-11lpom8"&&(t.textContent=v),d=i(n),b(r.$$.fragment,n)},m(n,u){a(n,t,u),a(n,d,u),M(r,n,u),g=!0},p:S,i(n){g||(C(r.$$.fragment,n),g=!0)},o(n){y(r.$$.fragment,n),g=!1},d(n){n&&(o(t),o(d)),x(r,n)}}}function Wn(T){let t,v,d,r,g,n,u,A,N,w=`ConvNeXt V2 モデルは、Sanghyun Woo、Shobhik Debnath、Ronghang Hu、Xinlei Chen、Zhuang Liu, In So Kweon, Saining Xie. によって <a href="https://arxiv.org/abs/2301.00808" rel="nofollow">ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</a> で提案されました。
ConvNeXt V2 は、Vision Transformers の設計からインスピレーションを得た純粋な畳み込みモデル (ConvNet) であり、<a href="convnext">ConvNeXT</a> の後継です。`,D,V,j="論文の要約は次のとおりです。",O,p,$="<em>アーキテクチャの改善と表現学習フレームワークの改善により、視覚認識の分野は 2020 年代初頭に急速な近代化とパフォーマンスの向上を実現しました。たとえば、ConvNeXt に代表される最新の ConvNet は、さまざまなシナリオで強力なパフォーマンスを実証しています。これらのモデルはもともと ImageNet ラベルを使用した教師あり学習用に設計されましたが、マスク オートエンコーダー (MAE) などの自己教師あり学習手法からも潜在的に恩恵を受けることができます。ただし、これら 2 つのアプローチを単純に組み合わせると、パフォーマンスが標準以下になることがわかりました。この論文では、完全畳み込みマスク オートエンコーダ フレームワークと、チャネル間の機能競合を強化するために ConvNeXt アーキテクチャに追加できる新しい Global Response Normalization (GRN) 層を提案します。この自己教師あり学習手法とアーキテクチャの改善の共同設計により、ConvNeXt V2 と呼ばれる新しいモデル ファミリが誕生しました。これにより、ImageNet 分類、COCO 検出、ADE20K セグメンテーションなどのさまざまな認識ベンチマークにおける純粋な ConvNet のパフォーマンスが大幅に向上します。また、ImageNet でトップ 1 の精度 76.7% を誇る効率的な 370 万パラメータの Atto モデルから、最先端の 88.9% を達成する 650M Huge モデルまで、さまざまなサイズの事前トレーニング済み ConvNeXt V2 モデルも提供しています。公開トレーニング データのみを使用した精度</em>。",Ke,te,Qt,et,ue,St='ConvNeXt V2 アーキテクチャ。 <a href="https://arxiv.org/abs/2301.00808">元の論文</a>から抜粋。',tt,he,At='このモデルは <a href="https://huggingface.co/adirik" rel="nofollow">adirik</a> によって提供されました。元のコードは <a href="https://github.com/facebookresearch/ConvNeXt-V2" rel="nofollow">こちら</a> にあります。',nt,ge,ot,ve,Dt="ConvNeXt V2 の使用を開始するのに役立つ公式 Hugging Face およびコミュニティ (🌎 で示される) リソースのリスト。",st,_e,at,be,Ot='<li><a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification">ConvNextV2ForImageClassification</a> は、この <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">サンプル スクリプト</a> および <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">ノートブック</a>。</li>',rt,Me,Kt="ここに含めるリソースの送信に興味がある場合は、お気軽にプル リクエストを開いてください。審査させていただきます。リソースは、既存のリソースを複製するのではなく、何か新しいものを示すことが理想的です。",lt,Ce,it,Z,ye,bt,We,en=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Model">ConvNextV2Model</a>. It is used to instantiate an
ConvNeXTV2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the ConvNeXTV2
<a href="https://huggingface.co/facebook/convnextv2-tiny-1k-224" rel="nofollow">facebook/convnextv2-tiny-1k-224</a> architecture.`,Mt,Ue,tn=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ct,ne,dt,xe,ct,X,Te,yt,He,nn=`The bare ConvNextV2 model outputting raw features without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,xt,H,$e,Tt,Ge,on='The <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Model">ConvNextV2Model</a> forward method, overrides the <code>__call__</code> special method.',$t,oe,wt,se,mt,we,pt,W,je,jt,Re,sn=`ConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,Nt,Be,an=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Vt,G,Ne,Ft,Xe,rn='The <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification">ConvNextV2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',It,ae,kt,re,ft,Ve,ut,I,Fe,Jt,ze,ln=`The bare ConvNextV2 model outputting raw features without any specific head on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Zt,Le,dn=`This model is also a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a> subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`,Wt,le,Ut,R,Ie,Ht,Pe,cn='The <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.TFConvNextV2Model">TFConvNextV2Model</a> forward method, overrides the <code>__call__</code> special method.',Gt,ie,Rt,de,ht,ke,gt,F,Je,Bt,Ee,mn=`ConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,Xt,Ye,pn=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,zt,qe,fn=`This model is also a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a> subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`,Lt,ce,Pt,B,Ze,Et,Qe,un='The <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.TFConvNextV2ForImageClassification">TFConvNextV2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',Yt,me,qt,pe,vt,Ae,_t;return g=new fe({props:{title:"ConvNeXt V2",local:"convnext-v2",headingTag:"h1"}}),u=new fe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ge=new fe({props:{title:"Resources",local:"resources",headingTag:"h2"}}),_e=new xn({props:{pipeline:"image-classification"}}),Ce=new fe({props:{title:"ConvNextV2Config",local:"transformers.ConvNextV2Config",headingTag:"h2"}}),ye=new ee({props:{name:"class transformers.ConvNextV2Config",anchor:"transformers.ConvNextV2Config",parameters:[{name:"num_channels",val:" = 3"},{name:"patch_size",val:" = 4"},{name:"num_stages",val:" = 4"},{name:"hidden_sizes",val:" = None"},{name:"depths",val:" = None"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"drop_path_rate",val:" = 0.0"},{name:"image_size",val:" = 224"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ConvNextV2Config.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.ConvNextV2Config.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, optional, defaults to 4) &#x2014;
Patch size to use in the patch embedding layer.`,name:"patch_size"},{anchor:"transformers.ConvNextV2Config.num_stages",description:`<strong>num_stages</strong> (<code>int</code>, optional, defaults to 4) &#x2014;
The number of stages in the model.`,name:"num_stages"},{anchor:"transformers.ConvNextV2Config.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[96, 192, 384, 768]</code>) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.ConvNextV2Config.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 9, 3]</code>) &#x2014;
Depth (number of blocks) for each stage.`,name:"depths"},{anchor:"transformers.ConvNextV2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each block. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.ConvNextV2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ConvNextV2Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.ConvNextV2Config.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The drop rate for stochastic depth.`,name:"drop_path_rate"},{anchor:"transformers.ConvNextV2Config.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.ConvNextV2Config.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/configuration_convnextv2.py#L30"}}),ne=new De({props:{anchor:"transformers.ConvNextV2Config.example",$$slots:{default:[Tn]},$$scope:{ctx:T}}}),xe=new fe({props:{title:"ConvNextV2Model",local:"transformers.ConvNextV2Model",headingTag:"h2"}}),Te=new ee({props:{name:"class transformers.ConvNextV2Model",anchor:"transformers.ConvNextV2Model",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ConvNextV2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_convnextv2.py#L344"}}),$e=new ee({props:{name:"forward",anchor:"transformers.ConvNextV2Model.forward",parameters:[{name:"pixel_values",val:": FloatTensor = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ConvNextV2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.ConvNextV2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ConvNextV2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_convnextv2.py#L363",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config"
>ConvNextV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oe=new Se({props:{$$slots:{default:[$n]},$$scope:{ctx:T}}}),se=new De({props:{anchor:"transformers.ConvNextV2Model.forward.example",$$slots:{default:[wn]},$$scope:{ctx:T}}}),we=new fe({props:{title:"ConvNextV2ForImageClassification",local:"transformers.ConvNextV2ForImageClassification",headingTag:"h2"}}),je=new ee({props:{name:"class transformers.ConvNextV2ForImageClassification",anchor:"transformers.ConvNextV2ForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ConvNextV2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_convnextv2.py#L408"}}),Ne=new ee({props:{name:"forward",anchor:"transformers.ConvNextV2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": FloatTensor = None"},{name:"labels",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ConvNextV2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.ConvNextV2ForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ConvNextV2ForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ConvNextV2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_convnextv2.py#L431",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config"
>ConvNextV2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ae=new Se({props:{$$slots:{default:[jn]},$$scope:{ctx:T}}}),re=new De({props:{anchor:"transformers.ConvNextV2ForImageClassification.forward.example",$$slots:{default:[Nn]},$$scope:{ctx:T}}}),Ve=new fe({props:{title:"TFConvNextV2Model",local:"transformers.TFConvNextV2Model",headingTag:"h2"}}),Fe=new ee({props:{name:"class transformers.TFConvNextV2Model",anchor:"transformers.TFConvNextV2Model",parameters:[{name:"config",val:": ConvNextV2Config"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFConvNextV2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L545"}}),le=new Se({props:{$$slots:{default:[Vn]},$$scope:{ctx:T}}}),Ie=new ee({props:{name:"call",anchor:"transformers.TFConvNextV2Model.call",parameters:[{name:"pixel_values",val:": TFModelInputType | None = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFConvNextV2Model.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code>, <code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFConvNextV2Model.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFConvNextV2Model.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to <code>True</code>.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L554",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config"
>ConvNextV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) — Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) — Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, + one for
the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(tf.Tensor)</code></p>
`}}),ie=new Se({props:{$$slots:{default:[Fn]},$$scope:{ctx:T}}}),de=new De({props:{anchor:"transformers.TFConvNextV2Model.call.example",$$slots:{default:[In]},$$scope:{ctx:T}}}),ke=new fe({props:{title:"TFConvNextV2ForImageClassification",local:"transformers.TFConvNextV2ForImageClassification",headingTag:"h2"}}),Je=new ee({props:{name:"class transformers.TFConvNextV2ForImageClassification",anchor:"transformers.TFConvNextV2ForImageClassification",parameters:[{name:"config",val:": ConvNextV2Config"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFConvNextV2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L603"}}),ce=new Se({props:{$$slots:{default:[kn]},$$scope:{ctx:T}}}),Ze=new ee({props:{name:"call",anchor:"transformers.TFConvNextV2ForImageClassification.call",parameters:[{name:"pixel_values",val:": TFModelInputType | None = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"labels",val:": np.ndarray | tf.Tensor | None = None"},{name:"training",val:": Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFConvNextV2ForImageClassification.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code>, <code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFConvNextV2ForImageClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFConvNextV2ForImageClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to <code>True</code>.`,name:"return_dict"},{anchor:"transformers.TFConvNextV2ForImageClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> or <code>np.ndarray</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L625",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config"
>ConvNextV2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, + one for
the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also called
feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention</code> or <code>tuple(tf.Tensor)</code></p>
`}}),me=new Se({props:{$$slots:{default:[Jn]},$$scope:{ctx:T}}}),pe=new De({props:{anchor:"transformers.TFConvNextV2ForImageClassification.call.example",$$slots:{default:[Zn]},$$scope:{ctx:T}}}),{c(){t=c("meta"),v=l(),d=c("p"),r=l(),_(g.$$.fragment),n=l(),_(u.$$.fragment),A=l(),N=c("p"),N.innerHTML=w,D=l(),V=c("p"),V.textContent=j,O=l(),p=c("p"),p.innerHTML=$,Ke=l(),te=c("img"),et=l(),ue=c("small"),ue.innerHTML=St,tt=l(),he=c("p"),he.innerHTML=At,nt=l(),_(ge.$$.fragment),ot=l(),ve=c("p"),ve.textContent=Dt,st=l(),_(_e.$$.fragment),at=l(),be=c("ul"),be.innerHTML=Ot,rt=l(),Me=c("p"),Me.textContent=Kt,lt=l(),_(Ce.$$.fragment),it=l(),Z=c("div"),_(ye.$$.fragment),bt=l(),We=c("p"),We.innerHTML=en,Mt=l(),Ue=c("p"),Ue.innerHTML=tn,Ct=l(),_(ne.$$.fragment),dt=l(),_(xe.$$.fragment),ct=l(),X=c("div"),_(Te.$$.fragment),yt=l(),He=c("p"),He.innerHTML=nn,xt=l(),H=c("div"),_($e.$$.fragment),Tt=l(),Ge=c("p"),Ge.innerHTML=on,$t=l(),_(oe.$$.fragment),wt=l(),_(se.$$.fragment),mt=l(),_(we.$$.fragment),pt=l(),W=c("div"),_(je.$$.fragment),jt=l(),Re=c("p"),Re.textContent=sn,Nt=l(),Be=c("p"),Be.innerHTML=an,Vt=l(),G=c("div"),_(Ne.$$.fragment),Ft=l(),Xe=c("p"),Xe.innerHTML=rn,It=l(),_(ae.$$.fragment),kt=l(),_(re.$$.fragment),ft=l(),_(Ve.$$.fragment),ut=l(),I=c("div"),_(Fe.$$.fragment),Jt=l(),ze=c("p"),ze.innerHTML=ln,Zt=l(),Le=c("p"),Le.innerHTML=dn,Wt=l(),_(le.$$.fragment),Ut=l(),R=c("div"),_(Ie.$$.fragment),Ht=l(),Pe=c("p"),Pe.innerHTML=cn,Gt=l(),_(ie.$$.fragment),Rt=l(),_(de.$$.fragment),ht=l(),_(ke.$$.fragment),gt=l(),F=c("div"),_(Je.$$.fragment),Bt=l(),Ee=c("p"),Ee.textContent=mn,Xt=l(),Ye=c("p"),Ye.innerHTML=pn,zt=l(),qe=c("p"),qe.innerHTML=fn,Lt=l(),_(ce.$$.fragment),Pt=l(),B=c("div"),_(Ze.$$.fragment),Et=l(),Qe=c("p"),Qe.innerHTML=un,Yt=l(),_(me.$$.fragment),qt=l(),_(pe.$$.fragment),vt=l(),Ae=c("p"),this.h()},l(e){const s=yn("svelte-u9bgzb",document.head);t=m(s,"META",{name:!0,content:!0}),s.forEach(o),v=i(e),d=m(e,"P",{}),Q(d).forEach(o),r=i(e),b(g.$$.fragment,e),n=i(e),b(u.$$.fragment,e),A=i(e),N=m(e,"P",{"data-svelte-h":!0}),h(N)!=="svelte-1geak2k"&&(N.innerHTML=w),D=i(e),V=m(e,"P",{"data-svelte-h":!0}),h(V)!=="svelte-1cv3nri"&&(V.textContent=j),O=i(e),p=m(e,"P",{"data-svelte-h":!0}),h(p)!=="svelte-16r1jgk"&&(p.innerHTML=$),Ke=i(e),te=m(e,"IMG",{src:!0,alt:!0,width:!0}),et=i(e),ue=m(e,"SMALL",{"data-svelte-h":!0}),h(ue)!=="svelte-6iufyc"&&(ue.innerHTML=St),tt=i(e),he=m(e,"P",{"data-svelte-h":!0}),h(he)!=="svelte-w90m9d"&&(he.innerHTML=At),nt=i(e),b(ge.$$.fragment,e),ot=i(e),ve=m(e,"P",{"data-svelte-h":!0}),h(ve)!=="svelte-1oqd6dv"&&(ve.textContent=Dt),st=i(e),b(_e.$$.fragment,e),at=i(e),be=m(e,"UL",{"data-svelte-h":!0}),h(be)!=="svelte-1xhl1et"&&(be.innerHTML=Ot),rt=i(e),Me=m(e,"P",{"data-svelte-h":!0}),h(Me)!=="svelte-17ytafw"&&(Me.textContent=Kt),lt=i(e),b(Ce.$$.fragment,e),it=i(e),Z=m(e,"DIV",{class:!0});var z=Q(Z);b(ye.$$.fragment,z),bt=i(z),We=m(z,"P",{"data-svelte-h":!0}),h(We)!=="svelte-14yqaym"&&(We.innerHTML=en),Mt=i(z),Ue=m(z,"P",{"data-svelte-h":!0}),h(Ue)!=="svelte-1s6wgpv"&&(Ue.innerHTML=tn),Ct=i(z),b(ne.$$.fragment,z),z.forEach(o),dt=i(e),b(xe.$$.fragment,e),ct=i(e),X=m(e,"DIV",{class:!0});var K=Q(X);b(Te.$$.fragment,K),yt=i(K),He=m(K,"P",{"data-svelte-h":!0}),h(He)!=="svelte-vxpmpx"&&(He.innerHTML=nn),xt=i(K),H=m(K,"DIV",{class:!0});var L=Q(H);b($e.$$.fragment,L),Tt=i(L),Ge=m(L,"P",{"data-svelte-h":!0}),h(Ge)!=="svelte-q3lg4c"&&(Ge.innerHTML=on),$t=i(L),b(oe.$$.fragment,L),wt=i(L),b(se.$$.fragment,L),L.forEach(o),K.forEach(o),mt=i(e),b(we.$$.fragment,e),pt=i(e),W=m(e,"DIV",{class:!0});var P=Q(W);b(je.$$.fragment,P),jt=i(P),Re=m(P,"P",{"data-svelte-h":!0}),h(Re)!=="svelte-14zttn9"&&(Re.textContent=sn),Nt=i(P),Be=m(P,"P",{"data-svelte-h":!0}),h(Be)!=="svelte-1gjh92c"&&(Be.innerHTML=an),Vt=i(P),G=m(P,"DIV",{class:!0});var E=Q(G);b(Ne.$$.fragment,E),Ft=i(E),Xe=m(E,"P",{"data-svelte-h":!0}),h(Xe)!=="svelte-1h2vmw6"&&(Xe.innerHTML=rn),It=i(E),b(ae.$$.fragment,E),kt=i(E),b(re.$$.fragment,E),E.forEach(o),P.forEach(o),ft=i(e),b(Ve.$$.fragment,e),ut=i(e),I=m(e,"DIV",{class:!0});var U=Q(I);b(Fe.$$.fragment,U),Jt=i(U),ze=m(U,"P",{"data-svelte-h":!0}),h(ze)!=="svelte-t68ebv"&&(ze.innerHTML=ln),Zt=i(U),Le=m(U,"P",{"data-svelte-h":!0}),h(Le)!=="svelte-1be7e3c"&&(Le.innerHTML=dn),Wt=i(U),b(le.$$.fragment,U),Ut=i(U),R=m(U,"DIV",{class:!0});var Y=Q(R);b(Ie.$$.fragment,Y),Ht=i(Y),Pe=m(Y,"P",{"data-svelte-h":!0}),h(Pe)!=="svelte-hsiw8s"&&(Pe.innerHTML=cn),Gt=i(Y),b(ie.$$.fragment,Y),Rt=i(Y),b(de.$$.fragment,Y),Y.forEach(o),U.forEach(o),ht=i(e),b(ke.$$.fragment,e),gt=i(e),F=m(e,"DIV",{class:!0});var k=Q(F);b(Je.$$.fragment,k),Bt=i(k),Ee=m(k,"P",{"data-svelte-h":!0}),h(Ee)!=="svelte-14zttn9"&&(Ee.textContent=mn),Xt=i(k),Ye=m(k,"P",{"data-svelte-h":!0}),h(Ye)!=="svelte-x53t1u"&&(Ye.innerHTML=pn),zt=i(k),qe=m(k,"P",{"data-svelte-h":!0}),h(qe)!=="svelte-1be7e3c"&&(qe.innerHTML=fn),Lt=i(k),b(ce.$$.fragment,k),Pt=i(k),B=m(k,"DIV",{class:!0});var q=Q(B);b(Ze.$$.fragment,q),Et=i(q),Qe=m(q,"P",{"data-svelte-h":!0}),h(Qe)!=="svelte-2rgvu2"&&(Qe.innerHTML=un),Yt=i(q),b(me.$$.fragment,q),qt=i(q),b(pe.$$.fragment,q),q.forEach(o),k.forEach(o),vt=i(e),Ae=m(e,"P",{}),Q(Ae).forEach(o),this.h()},h(){J(t,"name","hf:doc:metadata"),J(t,"content",Un),_n(te.src,Qt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnextv2_architecture.png")||J(te,"src",Qt),J(te,"alt","描画"),J(te,"width","600"),J(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){f(document.head,t),a(e,v,s),a(e,d,s),a(e,r,s),M(g,e,s),a(e,n,s),M(u,e,s),a(e,A,s),a(e,N,s),a(e,D,s),a(e,V,s),a(e,O,s),a(e,p,s),a(e,Ke,s),a(e,te,s),a(e,et,s),a(e,ue,s),a(e,tt,s),a(e,he,s),a(e,nt,s),M(ge,e,s),a(e,ot,s),a(e,ve,s),a(e,st,s),M(_e,e,s),a(e,at,s),a(e,be,s),a(e,rt,s),a(e,Me,s),a(e,lt,s),M(Ce,e,s),a(e,it,s),a(e,Z,s),M(ye,Z,null),f(Z,bt),f(Z,We),f(Z,Mt),f(Z,Ue),f(Z,Ct),M(ne,Z,null),a(e,dt,s),M(xe,e,s),a(e,ct,s),a(e,X,s),M(Te,X,null),f(X,yt),f(X,He),f(X,xt),f(X,H),M($e,H,null),f(H,Tt),f(H,Ge),f(H,$t),M(oe,H,null),f(H,wt),M(se,H,null),a(e,mt,s),M(we,e,s),a(e,pt,s),a(e,W,s),M(je,W,null),f(W,jt),f(W,Re),f(W,Nt),f(W,Be),f(W,Vt),f(W,G),M(Ne,G,null),f(G,Ft),f(G,Xe),f(G,It),M(ae,G,null),f(G,kt),M(re,G,null),a(e,ft,s),M(Ve,e,s),a(e,ut,s),a(e,I,s),M(Fe,I,null),f(I,Jt),f(I,ze),f(I,Zt),f(I,Le),f(I,Wt),M(le,I,null),f(I,Ut),f(I,R),M(Ie,R,null),f(R,Ht),f(R,Pe),f(R,Gt),M(ie,R,null),f(R,Rt),M(de,R,null),a(e,ht,s),M(ke,e,s),a(e,gt,s),a(e,F,s),M(Je,F,null),f(F,Bt),f(F,Ee),f(F,Xt),f(F,Ye),f(F,zt),f(F,qe),f(F,Lt),M(ce,F,null),f(F,Pt),f(F,B),M(Ze,B,null),f(B,Et),f(B,Qe),f(B,Yt),M(me,B,null),f(B,qt),M(pe,B,null),a(e,vt,s),a(e,Ae,s),_t=!0},p(e,[s]){const z={};s&2&&(z.$$scope={dirty:s,ctx:e}),ne.$set(z);const K={};s&2&&(K.$$scope={dirty:s,ctx:e}),oe.$set(K);const L={};s&2&&(L.$$scope={dirty:s,ctx:e}),se.$set(L);const P={};s&2&&(P.$$scope={dirty:s,ctx:e}),ae.$set(P);const E={};s&2&&(E.$$scope={dirty:s,ctx:e}),re.$set(E);const U={};s&2&&(U.$$scope={dirty:s,ctx:e}),le.$set(U);const Y={};s&2&&(Y.$$scope={dirty:s,ctx:e}),ie.$set(Y);const k={};s&2&&(k.$$scope={dirty:s,ctx:e}),de.$set(k);const q={};s&2&&(q.$$scope={dirty:s,ctx:e}),ce.$set(q);const hn={};s&2&&(hn.$$scope={dirty:s,ctx:e}),me.$set(hn);const gn={};s&2&&(gn.$$scope={dirty:s,ctx:e}),pe.$set(gn)},i(e){_t||(C(g.$$.fragment,e),C(u.$$.fragment,e),C(ge.$$.fragment,e),C(_e.$$.fragment,e),C(Ce.$$.fragment,e),C(ye.$$.fragment,e),C(ne.$$.fragment,e),C(xe.$$.fragment,e),C(Te.$$.fragment,e),C($e.$$.fragment,e),C(oe.$$.fragment,e),C(se.$$.fragment,e),C(we.$$.fragment,e),C(je.$$.fragment,e),C(Ne.$$.fragment,e),C(ae.$$.fragment,e),C(re.$$.fragment,e),C(Ve.$$.fragment,e),C(Fe.$$.fragment,e),C(le.$$.fragment,e),C(Ie.$$.fragment,e),C(ie.$$.fragment,e),C(de.$$.fragment,e),C(ke.$$.fragment,e),C(Je.$$.fragment,e),C(ce.$$.fragment,e),C(Ze.$$.fragment,e),C(me.$$.fragment,e),C(pe.$$.fragment,e),_t=!0)},o(e){y(g.$$.fragment,e),y(u.$$.fragment,e),y(ge.$$.fragment,e),y(_e.$$.fragment,e),y(Ce.$$.fragment,e),y(ye.$$.fragment,e),y(ne.$$.fragment,e),y(xe.$$.fragment,e),y(Te.$$.fragment,e),y($e.$$.fragment,e),y(oe.$$.fragment,e),y(se.$$.fragment,e),y(we.$$.fragment,e),y(je.$$.fragment,e),y(Ne.$$.fragment,e),y(ae.$$.fragment,e),y(re.$$.fragment,e),y(Ve.$$.fragment,e),y(Fe.$$.fragment,e),y(le.$$.fragment,e),y(Ie.$$.fragment,e),y(ie.$$.fragment,e),y(de.$$.fragment,e),y(ke.$$.fragment,e),y(Je.$$.fragment,e),y(ce.$$.fragment,e),y(Ze.$$.fragment,e),y(me.$$.fragment,e),y(pe.$$.fragment,e),_t=!1},d(e){e&&(o(v),o(d),o(r),o(n),o(A),o(N),o(D),o(V),o(O),o(p),o(Ke),o(te),o(et),o(ue),o(tt),o(he),o(nt),o(ot),o(ve),o(st),o(at),o(be),o(rt),o(Me),o(lt),o(it),o(Z),o(dt),o(ct),o(X),o(mt),o(pt),o(W),o(ft),o(ut),o(I),o(ht),o(gt),o(F),o(vt),o(Ae)),o(t),x(g,e),x(u,e),x(ge,e),x(_e,e),x(Ce,e),x(ye),x(ne),x(xe,e),x(Te),x($e),x(oe),x(se),x(we,e),x(je),x(Ne),x(ae),x(re),x(Ve,e),x(Fe),x(le),x(Ie),x(ie),x(de),x(ke,e),x(Je),x(ce),x(Ze),x(me),x(pe)}}}const Un='{"title":"ConvNeXt V2","local":"convnext-v2","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"ConvNextV2Config","local":"transformers.ConvNextV2Config","sections":[],"depth":2},{"title":"ConvNextV2Model","local":"transformers.ConvNextV2Model","sections":[],"depth":2},{"title":"ConvNextV2ForImageClassification","local":"transformers.ConvNextV2ForImageClassification","sections":[],"depth":2},{"title":"TFConvNextV2Model","local":"transformers.TFConvNextV2Model","sections":[],"depth":2},{"title":"TFConvNextV2ForImageClassification","local":"transformers.TFConvNextV2ForImageClassification","sections":[],"depth":2}],"depth":1}';function Hn(T){return bn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Yn extends Mn{constructor(t){super(),Cn(this,t,Hn,Wn,vn,{})}}export{Yn as component};
