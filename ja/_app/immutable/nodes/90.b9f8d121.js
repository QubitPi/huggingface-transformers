import{s as vn,f as _n,o as bn,n as S}from"../chunks/scheduler.9bc65507.js";import{S as Mn,i as Cn,g as c,s as l,r as _,A as yn,h as m,f as o,c as i,j as Q,u as b,x as h,k as J,y as f,a,v as M,d as C,t as y,w as x}from"../chunks/index.707bf1b6.js";import{T as Se}from"../chunks/Tip.c2ecdbf4.js";import{D as ee}from"../chunks/Docstring.17db21ae.js";import{C as Oe}from"../chunks/CodeBlock.54a9f38d.js";import{E as De}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as xn}from"../chunks/PipelineTag.44585822.js";import{H as fe}from"../chunks/Heading.342b1fa6.js";function Tn(T){let t,v="Example:",d,r,g;return r=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMENvbnZOZVhUVjJDb25maWclMkMlMjBDb252TmV4dFYyTW9kZWwlMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwQ29udk5lWFRWMiUyMGNvbnZuZXh0djItdGlueS0xay0yMjQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQ29udk5lWFRWMkNvbmZpZygpJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMG1vZGVsJTIwKHdpdGglMjByYW5kb20lMjB3ZWlnaHRzKSUyMGZyb20lMjB0aGUlMjBjb252bmV4dHYyLXRpbnktMWstMjI0JTIwc3R5bGUlMjBjb25maWd1cmF0aW9uJTBBbW9kZWwlMjAlM0QlMjBDb252TmV4dFYyTW9kZWwoY29uZmlndXJhdGlvbiklMEElMEElMjMlMjBBY2Nlc3NpbmclMjB0aGUlMjBtb2RlbCUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwbW9kZWwuY29uZmln",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ConvNeXTV2Config, ConvNextV2Model

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a ConvNeXTV2 convnextv2-tiny-1k-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = ConvNeXTV2Config()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the convnextv2-tiny-1k-224 style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ConvNextV2Model(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){t=c("p"),t.textContent=v,d=l(),_(r.$$.fragment)},l(n){t=m(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-11lpom8"&&(t.textContent=v),d=i(n),b(r.$$.fragment,n)},m(n,u){a(n,t,u),a(n,d,u),M(r,n,u),g=!0},p:S,i(n){g||(C(r.$$.fragment,n),g=!0)},o(n){y(r.$$.fragment,n),g=!1},d(n){n&&(o(t),o(d)),x(r,n)}}}function $n(T){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=v},l(d){t=m(d,"P",{"data-svelte-h":!0}),h(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(d,r){a(d,t,r)},p:S,d(d){d&&o(t)}}}function wn(T){let t,v="Example:",d,r,g;return r=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMENvbnZOZXh0VjJNb2RlbCUwQWltcG9ydCUyMHRvcmNoJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dHYyLXRpbnktMWstMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwQ29udk5leHRWMk1vZGVsLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmNvbnZuZXh0djItdGlueS0xay0yMjQlMjIpJTBBJTBBaW5wdXRzJTIwJTNEJTIwaW1hZ2VfcHJvY2Vzc29yKGltYWdlJTJDJTIwcmV0dXJuX3RlbnNvcnMlM0QlMjJwdCUyMiklMEElMEF3aXRoJTIwdG9yY2gubm9fZ3JhZCgpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKSUwQSUwQWxhc3RfaGlkZGVuX3N0YXRlcyUyMCUzRCUyMG91dHB1dHMubGFzdF9oaWRkZW5fc3RhdGUlMEFsaXN0KGxhc3RfaGlkZGVuX3N0YXRlcy5zaGFwZSk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ConvNextV2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ConvNextV2Model.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">768</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`,wrap:!1}}),{c(){t=c("p"),t.textContent=v,d=l(),_(r.$$.fragment)},l(n){t=m(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-11lpom8"&&(t.textContent=v),d=i(n),b(r.$$.fragment,n)},m(n,u){a(n,t,u),a(n,d,u),M(r,n,u),g=!0},p:S,i(n){g||(C(r.$$.fragment,n),g=!0)},o(n){y(r.$$.fragment,n),g=!1},d(n){n&&(o(t),o(d)),x(r,n)}}}function jn(T){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=v},l(d){t=m(d,"P",{"data-svelte-h":!0}),h(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(d,r){a(d,t,r)},p:S,d(d){d&&o(t)}}}function Nn(T){let t,v="Example:",d,r,g;return r=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMENvbnZOZXh0VjJGb3JJbWFnZUNsYXNzaWZpY2F0aW9uJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmh1Z2dpbmdmYWNlJTJGY2F0cy1pbWFnZSUyMiklMEFpbWFnZSUyMCUzRCUyMGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTVCJTIyaW1hZ2UlMjIlNUQlNUIwJTVEJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRmNvbnZuZXh0djItdGlueS0xay0yMjQlMjIpJTBBbW9kZWwlMjAlM0QlMjBDb252TmV4dFYyRm9ySW1hZ2VDbGFzc2lmaWNhdGlvbi5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dHYyLXRpbnktMWstMjI0JTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMGxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwbG9naXRzLmFyZ21heCgtMSkuaXRlbSgpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, ConvNextV2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = ConvNextV2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){t=c("p"),t.textContent=v,d=l(),_(r.$$.fragment)},l(n){t=m(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-11lpom8"&&(t.textContent=v),d=i(n),b(r.$$.fragment,n)},m(n,u){a(n,t,u),a(n,d,u),M(r,n,u),g=!0},p:S,i(n){g||(C(r.$$.fragment,n),g=!0)},o(n){y(r.$$.fragment,n),g=!1},d(n){n&&(o(t),o(d)),x(r,n)}}}function Vn(T){let t,v="TensorFlow models and layers in <code>transformers</code> accept two formats as input:",d,r,g="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional argument.</li>",n,u,A=`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like <code>model.fit()</code> things should ‚Äújust work‚Äù for you - just
pass your inputs and labels in any format that <code>model.fit()</code> supports! If, however, you want to use the second
format outside of Keras methods like <code>fit()</code> and <code>predict()</code>, such as when creating your own layers or models with
the Keras <code>Functional</code> API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`,N,w,D=`<li>a single Tensor with <code>pixel_values</code> only and nothing else: <code>model(pixel_values)</code></li> <li>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code>model([pixel_values, attention_mask])</code> or <code>model([pixel_values, attention_mask, token_type_ids])</code></li> <li>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code>model({&quot;pixel_values&quot;: pixel_values, &quot;token_type_ids&quot;: token_type_ids})</code></li>`,V,j,O=`Note that when creating models and layers with
<a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="nofollow">subclassing</a> then you don‚Äôt need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`;return{c(){t=c("p"),t.innerHTML=v,d=l(),r=c("ul"),r.innerHTML=g,n=l(),u=c("p"),u.innerHTML=A,N=l(),w=c("ul"),w.innerHTML=D,V=l(),j=c("p"),j.innerHTML=O},l(p){t=m(p,"P",{"data-svelte-h":!0}),h(t)!=="svelte-1ajbfxg"&&(t.innerHTML=v),d=i(p),r=m(p,"UL",{"data-svelte-h":!0}),h(r)!=="svelte-qm1t26"&&(r.innerHTML=g),n=i(p),u=m(p,"P",{"data-svelte-h":!0}),h(u)!=="svelte-1v9qsc5"&&(u.innerHTML=A),N=i(p),w=m(p,"UL",{"data-svelte-h":!0}),h(w)!=="svelte-99h8aq"&&(w.innerHTML=D),V=i(p),j=m(p,"P",{"data-svelte-h":!0}),h(j)!=="svelte-1an3odd"&&(j.innerHTML=O)},m(p,$){a(p,t,$),a(p,d,$),a(p,r,$),a(p,n,$),a(p,u,$),a(p,N,$),a(p,w,$),a(p,V,$),a(p,j,$)},p:S,d(p){p&&(o(t),o(d),o(r),o(n),o(u),o(N),o(w),o(V),o(j))}}}function Fn(T){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=v},l(d){t=m(d,"P",{"data-svelte-h":!0}),h(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(d,r){a(d,t,r)},p:S,d(d){d&&o(t)}}}function In(T){let t,v="Example:",d,r,g;return r=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGQ29udk5leHRWMk1vZGVsJTBBZnJvbSUyMGRhdGFzZXRzJTIwaW1wb3J0JTIwbG9hZF9kYXRhc2V0JTBBJTBBZGF0YXNldCUyMCUzRCUyMGxvYWRfZGF0YXNldCglMjJodWdnaW5nZmFjZSUyRmNhdHMtaW1hZ2UlMjIpJTBBaW1hZ2UlMjAlM0QlMjBkYXRhc2V0JTVCJTIydGVzdCUyMiU1RCU1QiUyMmltYWdlJTIyJTVEJTVCMCU1RCUwQSUwQWltYWdlX3Byb2Nlc3NvciUyMCUzRCUyMEF1dG9JbWFnZVByb2Nlc3Nvci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZjb252bmV4dHYyLXRpbnktMWstMjI0JTIyKSUwQW1vZGVsJTIwJTNEJTIwVEZDb252TmV4dFYyTW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGY29udm5leHR2Mi10aW55LTFrLTIyNCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFConvNextV2Model
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFConvNextV2Model.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">768</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>]`,wrap:!1}}),{c(){t=c("p"),t.textContent=v,d=l(),_(r.$$.fragment)},l(n){t=m(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-11lpom8"&&(t.textContent=v),d=i(n),b(r.$$.fragment,n)},m(n,u){a(n,t,u),a(n,d,u),M(r,n,u),g=!0},p:S,i(n){g||(C(r.$$.fragment,n),g=!0)},o(n){y(r.$$.fragment,n),g=!1},d(n){n&&(o(t),o(d)),x(r,n)}}}function kn(T){let t,v="TensorFlow models and layers in <code>transformers</code> accept two formats as input:",d,r,g="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional argument.</li>",n,u,A=`The reason the second format is supported is that Keras methods prefer this format when passing inputs to models
and layers. Because of this support, when using methods like <code>model.fit()</code> things should ‚Äújust work‚Äù for you - just
pass your inputs and labels in any format that <code>model.fit()</code> supports! If, however, you want to use the second
format outside of Keras methods like <code>fit()</code> and <code>predict()</code>, such as when creating your own layers or models with
the Keras <code>Functional</code> API, there are three possibilities you can use to gather all the input Tensors in the first
positional argument:`,N,w,D=`<li>a single Tensor with <code>pixel_values</code> only and nothing else: <code>model(pixel_values)</code></li> <li>a list of varying length with one or several input Tensors IN THE ORDER given in the docstring:
<code>model([pixel_values, attention_mask])</code> or <code>model([pixel_values, attention_mask, token_type_ids])</code></li> <li>a dictionary with one or several input Tensors associated to the input names given in the docstring:
<code>model({&quot;pixel_values&quot;: pixel_values, &quot;token_type_ids&quot;: token_type_ids})</code></li>`,V,j,O=`Note that when creating models and layers with
<a href="https://keras.io/guides/making_new_layers_and_models_via_subclassing/" rel="nofollow">subclassing</a> then you don‚Äôt need to worry
about any of this, as you can just pass inputs like you would to any other Python function!`;return{c(){t=c("p"),t.innerHTML=v,d=l(),r=c("ul"),r.innerHTML=g,n=l(),u=c("p"),u.innerHTML=A,N=l(),w=c("ul"),w.innerHTML=D,V=l(),j=c("p"),j.innerHTML=O},l(p){t=m(p,"P",{"data-svelte-h":!0}),h(t)!=="svelte-1ajbfxg"&&(t.innerHTML=v),d=i(p),r=m(p,"UL",{"data-svelte-h":!0}),h(r)!=="svelte-qm1t26"&&(r.innerHTML=g),n=i(p),u=m(p,"P",{"data-svelte-h":!0}),h(u)!=="svelte-1v9qsc5"&&(u.innerHTML=A),N=i(p),w=m(p,"UL",{"data-svelte-h":!0}),h(w)!=="svelte-99h8aq"&&(w.innerHTML=D),V=i(p),j=m(p,"P",{"data-svelte-h":!0}),h(j)!=="svelte-1an3odd"&&(j.innerHTML=O)},m(p,$){a(p,t,$),a(p,d,$),a(p,r,$),a(p,n,$),a(p,u,$),a(p,N,$),a(p,w,$),a(p,V,$),a(p,j,$)},p:S,d(p){p&&(o(t),o(d),o(r),o(n),o(u),o(N),o(w),o(V),o(j))}}}function Jn(T){let t,v=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){t=c("p"),t.innerHTML=v},l(d){t=m(d,"P",{"data-svelte-h":!0}),h(t)!=="svelte-fincs2"&&(t.innerHTML=v)},m(d,r){a(d,t,r)},p:S,d(d){d&&o(t)}}}function Zn(T){let t,v="Example:",d,r,g;return r=new Oe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGQ29udk5leHRWMkZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0ZW5zb3JmbG93JTIwYXMlMjB0ZiUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGY29udm5leHR2Mi10aW55LTFrLTIyNCUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQ29udk5leHRWMkZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMmZhY2Vib29rJTJGY29udm5leHR2Mi10aW55LTFrLTIyNCUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQWxvZ2l0cyUyMCUzRCUyMG1vZGVsKCoqaW5wdXRzKS5sb2dpdHMlMEElMEElMjMlMjBtb2RlbCUyMHByZWRpY3RzJTIwb25lJTIwb2YlMjB0aGUlMjAxMDAwJTIwSW1hZ2VOZXQlMjBjbGFzc2VzJTBBcHJlZGljdGVkX2xhYmVsJTIwJTNEJTIwaW50KHRmLm1hdGguYXJnbWF4KGxvZ2l0cyUyQyUyMGF4aXMlM0QtMSkpJTBBcHJpbnQobW9kZWwuY29uZmlnLmlkMmxhYmVsJTVCcHJlZGljdGVkX2xhYmVsJTVEKQ==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFConvNextV2ForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFConvNextV2ForImageClassification.from_pretrained(<span class="hljs-string">&quot;facebook/convnextv2-tiny-1k-224&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = <span class="hljs-built_in">int</span>(tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>))
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){t=c("p"),t.textContent=v,d=l(),_(r.$$.fragment)},l(n){t=m(n,"P",{"data-svelte-h":!0}),h(t)!=="svelte-11lpom8"&&(t.textContent=v),d=i(n),b(r.$$.fragment,n)},m(n,u){a(n,t,u),a(n,d,u),M(r,n,u),g=!0},p:S,i(n){g||(C(r.$$.fragment,n),g=!0)},o(n){y(r.$$.fragment,n),g=!1},d(n){n&&(o(t),o(d)),x(r,n)}}}function Wn(T){let t,v,d,r,g,n,u,A,N,w=`ConvNeXt V2 „É¢„Éá„É´„ÅØ„ÄÅSanghyun Woo„ÄÅShobhik Debnath„ÄÅRonghang Hu„ÄÅXinlei Chen„ÄÅZhuang Liu, In So Kweon, Saining Xie. „Å´„Çà„Å£„Å¶ <a href="https://arxiv.org/abs/2301.00808" rel="nofollow">ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders</a> „ÅßÊèêÊ°à„Åï„Çå„Åæ„Åó„Åü„ÄÇ
ConvNeXt V2 „ÅØ„ÄÅVision Transformers „ÅÆË®≠Ë®à„Åã„Çâ„Ç§„É≥„Çπ„Éî„É¨„Éº„Ç∑„Éß„É≥„ÇíÂæó„ÅüÁ¥îÁ≤ã„Å™Áï≥„ÅøËæº„Åø„É¢„Éá„É´ (ConvNet) „Åß„ÅÇ„Çä„ÄÅ<a href="convnext">ConvNeXT</a> „ÅÆÂæåÁ∂ô„Åß„Åô„ÄÇ`,D,V,j="Ë´ñÊñá„ÅÆË¶ÅÁ¥Ñ„ÅØÊ¨°„ÅÆ„Å®„Åä„Çä„Åß„Åô„ÄÇ",O,p,$="<em>„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÊîπÂñÑ„Å®Ë°®ÁèæÂ≠¶Áøí„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅÆÊîπÂñÑ„Å´„Çà„Çä„ÄÅË¶ñË¶öË™çË≠ò„ÅÆÂàÜÈáé„ÅØ 2020 Âπ¥‰ª£ÂàùÈ†≠„Å´ÊÄ•ÈÄü„Å™Ëøë‰ª£Âåñ„Å®„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅÆÂêë‰∏ä„ÇíÂÆüÁèæ„Åó„Åæ„Åó„Åü„ÄÇ„Åü„Å®„Åà„Å∞„ÄÅConvNeXt „Å´‰ª£Ë°®„Åï„Çå„ÇãÊúÄÊñ∞„ÅÆ ConvNet „ÅØ„ÄÅ„Åï„Åæ„Åñ„Åæ„Å™„Ç∑„Éä„É™„Ç™„ÅßÂº∑Âäõ„Å™„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíÂÆüË®º„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åì„Çå„Çâ„ÅÆ„É¢„Éá„É´„ÅØ„ÇÇ„Å®„ÇÇ„Å® ImageNet „É©„Éô„É´„Çí‰ΩøÁî®„Åó„ÅüÊïôÂ∏´„ÅÇ„ÇäÂ≠¶ÁøíÁî®„Å´Ë®≠Ë®à„Åï„Çå„Åæ„Åó„Åü„Åå„ÄÅ„Éû„Çπ„ÇØ „Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ„Éº (MAE) „Å™„Å©„ÅÆËá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶ÁøíÊâãÊ≥ï„Åã„Çâ„ÇÇÊΩúÂú®ÁöÑ„Å´ÊÅ©ÊÅµ„ÇíÂèó„Åë„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ„Åü„Å†„Åó„ÄÅ„Åì„Çå„Çâ 2 „Å§„ÅÆ„Ç¢„Éó„É≠„Éº„ÉÅ„ÇíÂçòÁ¥î„Å´ÁµÑ„ÅøÂêà„Çè„Åõ„Çã„Å®„ÄÅ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅåÊ®ôÊ∫ñ‰ª•‰∏ã„Å´„Å™„Çã„Åì„Å®„Åå„Çè„Åã„Çä„Åæ„Åó„Åü„ÄÇ„Åì„ÅÆË´ñÊñá„Åß„ÅØ„ÄÅÂÆåÂÖ®Áï≥„ÅøËæº„Åø„Éû„Çπ„ÇØ „Ç™„Éº„Éà„Ç®„É≥„Ç≥„Éº„ÉÄ „Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Å®„ÄÅ„ÉÅ„É£„Éç„É´Èñì„ÅÆÊ©üËÉΩÁ´∂Âêà„ÇíÂº∑Âåñ„Åô„Çã„Åü„ÇÅ„Å´ ConvNeXt „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„Å´ËøΩÂä†„Åß„Åç„ÇãÊñ∞„Åó„ÅÑ Global Response Normalization (GRN) Â±§„ÇíÊèêÊ°à„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆËá™Â∑±ÊïôÂ∏´„ÅÇ„ÇäÂ≠¶ÁøíÊâãÊ≥ï„Å®„Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÅÆÊîπÂñÑ„ÅÆÂÖ±ÂêåË®≠Ë®à„Å´„Çà„Çä„ÄÅConvNeXt V2 „Å®Âëº„Å∞„Çå„ÇãÊñ∞„Åó„ÅÑ„É¢„Éá„É´ „Éï„Ç°„Éü„É™„ÅåË™ïÁîü„Åó„Åæ„Åó„Åü„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅImageNet ÂàÜÈ°û„ÄÅCOCO Ê§úÂá∫„ÄÅADE20K „Çª„Ç∞„É°„É≥„ÉÜ„Éº„Ç∑„Éß„É≥„Å™„Å©„ÅÆ„Åï„Åæ„Åñ„Åæ„Å™Ë™çË≠ò„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Å´„Åä„Åë„ÇãÁ¥îÁ≤ã„Å™ ConvNet „ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅåÂ§ßÂπÖ„Å´Âêë‰∏ä„Åó„Åæ„Åô„ÄÇ„Åæ„Åü„ÄÅImageNet „Åß„Éà„ÉÉ„Éó 1 „ÅÆÁ≤æÂ∫¶ 76.7% „ÇíË™á„ÇãÂäπÁéáÁöÑ„Å™ 370 ‰∏á„Éë„É©„É°„Éº„Çø„ÅÆ Atto „É¢„Éá„É´„Åã„Çâ„ÄÅÊúÄÂÖàÁ´Ø„ÅÆ 88.9% „ÇíÈÅîÊàê„Åô„Çã 650M Huge „É¢„Éá„É´„Åæ„Åß„ÄÅ„Åï„Åæ„Åñ„Åæ„Å™„Çµ„Ç§„Ç∫„ÅÆ‰∫ãÂâç„Éà„É¨„Éº„Éã„É≥„Ç∞Ê∏à„Åø ConvNeXt V2 „É¢„Éá„É´„ÇÇÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇÂÖ¨Èñã„Éà„É¨„Éº„Éã„É≥„Ç∞ „Éá„Éº„Çø„ÅÆ„Åø„Çí‰ΩøÁî®„Åó„ÅüÁ≤æÂ∫¶</em>„ÄÇ",Ke,te,Qt,et,ue,St='ConvNeXt V2 „Ç¢„Éº„Ç≠„ÉÜ„ÇØ„ÉÅ„É£„ÄÇ <a href="https://arxiv.org/abs/2301.00808">ÂÖÉ„ÅÆË´ñÊñá</a>„Åã„ÇâÊäúÁ≤ã„ÄÇ',tt,he,At='„Åì„ÅÆ„É¢„Éá„É´„ÅØ <a href="https://huggingface.co/adirik" rel="nofollow">adirik</a> „Å´„Çà„Å£„Å¶Êèê‰æõ„Åï„Çå„Åæ„Åó„Åü„ÄÇÂÖÉ„ÅÆ„Ç≥„Éº„Éâ„ÅØ <a href="https://github.com/facebookresearch/ConvNeXt-V2" rel="nofollow">„Åì„Å°„Çâ</a> „Å´„ÅÇ„Çä„Åæ„Åô„ÄÇ',nt,ge,ot,ve,Dt="ConvNeXt V2 „ÅÆ‰ΩøÁî®„ÇíÈñãÂßã„Åô„Çã„ÅÆ„Å´ÂΩπÁ´ã„Å§ÂÖ¨Âºè Hugging Face „Åä„Çà„Å≥„Ç≥„Éü„É•„Éã„ÉÜ„Ç£ (üåé „ÅßÁ§∫„Åï„Çå„Çã) „É™„ÇΩ„Éº„Çπ„ÅÆ„É™„Çπ„Éà„ÄÇ",st,_e,at,be,Ot='<li><a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification">ConvNextV2ForImageClassification</a> „ÅØ„ÄÅ„Åì„ÅÆ <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">„Çµ„É≥„Éó„É´ „Çπ„ÇØ„É™„Éó„Éà</a> „Åä„Çà„Å≥ <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">„Éé„Éº„Éà„Éñ„ÉÉ„ÇØ</a>„ÄÇ</li>',rt,Me,Kt="„Åì„Åì„Å´Âê´„ÇÅ„Çã„É™„ÇΩ„Éº„Çπ„ÅÆÈÄÅ‰ø°„Å´ËààÂë≥„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅ„ÅäÊ∞óËªΩ„Å´„Éó„É´ „É™„ÇØ„Ç®„Çπ„Éà„ÇíÈñã„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂØ©Êüª„Åï„Åõ„Å¶„ÅÑ„Åü„Å†„Åç„Åæ„Åô„ÄÇ„É™„ÇΩ„Éº„Çπ„ÅØ„ÄÅÊó¢Â≠ò„ÅÆ„É™„ÇΩ„Éº„Çπ„ÇíË§áË£Ω„Åô„Çã„ÅÆ„Åß„ÅØ„Å™„Åè„ÄÅ‰Ωï„ÅãÊñ∞„Åó„ÅÑ„ÇÇ„ÅÆ„ÇíÁ§∫„Åô„Åì„Å®„ÅåÁêÜÊÉ≥ÁöÑ„Åß„Åô„ÄÇ",lt,Ce,it,Z,ye,bt,We,en=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Model">ConvNextV2Model</a>. It is used to instantiate an
ConvNeXTV2 model according to the specified arguments, defining the model architecture. Instantiating a
configuration with the defaults will yield a similar configuration to that of the ConvNeXTV2
<a href="https://huggingface.co/facebook/convnextv2-tiny-1k-224" rel="nofollow">facebook/convnextv2-tiny-1k-224</a> architecture.`,Mt,Ue,tn=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,Ct,ne,dt,xe,ct,X,Te,yt,He,nn=`The bare ConvNextV2 model outputting raw features without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,xt,H,$e,Tt,Ge,on='The <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Model">ConvNextV2Model</a> forward method, overrides the <code>__call__</code> special method.',$t,oe,wt,se,mt,we,pt,W,je,jt,Re,sn=`ConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,Nt,Be,an=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,Vt,G,Ne,Ft,Xe,rn='The <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2ForImageClassification">ConvNextV2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',It,ae,kt,re,ft,Ve,ut,I,Fe,Jt,ze,ln=`The bare ConvNextV2 model outputting raw features without any specific head on top.
This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,Zt,Le,dn=`This model is also a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a> subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`,Wt,le,Ut,R,Ie,Ht,Pe,cn='The <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.TFConvNextV2Model">TFConvNextV2Model</a> forward method, overrides the <code>__call__</code> special method.',Gt,ie,Rt,de,ht,ke,gt,F,Je,Bt,Ee,mn=`ConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for
ImageNet.`,Xt,Ye,pn=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,zt,qe,fn=`This model is also a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a> subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`,Lt,ce,Pt,B,Ze,Et,Qe,un='The <a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.TFConvNextV2ForImageClassification">TFConvNextV2ForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',Yt,me,qt,pe,vt,Ae,_t;return g=new fe({props:{title:"ConvNeXt V2",local:"convnext-v2",headingTag:"h1"}}),u=new fe({props:{title:"Overview",local:"overview",headingTag:"h2"}}),ge=new fe({props:{title:"Resources",local:"resources",headingTag:"h2"}}),_e=new xn({props:{pipeline:"image-classification"}}),Ce=new fe({props:{title:"ConvNextV2Config",local:"transformers.ConvNextV2Config",headingTag:"h2"}}),ye=new ee({props:{name:"class transformers.ConvNextV2Config",anchor:"transformers.ConvNextV2Config",parameters:[{name:"num_channels",val:" = 3"},{name:"patch_size",val:" = 4"},{name:"num_stages",val:" = 4"},{name:"hidden_sizes",val:" = None"},{name:"depths",val:" = None"},{name:"hidden_act",val:" = 'gelu'"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"drop_path_rate",val:" = 0.0"},{name:"image_size",val:" = 224"},{name:"out_features",val:" = None"},{name:"out_indices",val:" = None"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.ConvNextV2Config.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.ConvNextV2Config.patch_size",description:`<strong>patch_size</strong> (<code>int</code>, optional, defaults to 4) &#x2014;
Patch size to use in the patch embedding layer.`,name:"patch_size"},{anchor:"transformers.ConvNextV2Config.num_stages",description:`<strong>num_stages</strong> (<code>int</code>, optional, defaults to 4) &#x2014;
The number of stages in the model.`,name:"num_stages"},{anchor:"transformers.ConvNextV2Config.hidden_sizes",description:`<strong>hidden_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[96, 192, 384, 768]</code>) &#x2014;
Dimensionality (hidden size) at each stage.`,name:"hidden_sizes"},{anchor:"transformers.ConvNextV2Config.depths",description:`<strong>depths</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 9, 3]</code>) &#x2014;
Depth (number of blocks) for each stage.`,name:"depths"},{anchor:"transformers.ConvNextV2Config.hidden_act",description:`<strong>hidden_act</strong> (<code>str</code> or <code>function</code>, <em>optional</em>, defaults to <code>&quot;gelu&quot;</code>) &#x2014;
The non-linear activation function (function or string) in each block. If string, <code>&quot;gelu&quot;</code>, <code>&quot;relu&quot;</code>,
<code>&quot;selu&quot;</code> and <code>&quot;gelu_new&quot;</code> are supported.`,name:"hidden_act"},{anchor:"transformers.ConvNextV2Config.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.ConvNextV2Config.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-12) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"},{anchor:"transformers.ConvNextV2Config.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>float</code>, <em>optional</em>, defaults to 0.0) &#x2014;
The drop rate for stochastic depth.`,name:"drop_path_rate"},{anchor:"transformers.ConvNextV2Config.out_features",description:`<strong>out_features</strong> (<code>List[str]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of features to output. Can be any of <code>&quot;stem&quot;</code>, <code>&quot;stage1&quot;</code>, <code>&quot;stage2&quot;</code>, etc.
(depending on how many stages the model has). If unset and <code>out_indices</code> is set, will default to the
corresponding stages. If unset and <code>out_indices</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_features"},{anchor:"transformers.ConvNextV2Config.out_indices",description:`<strong>out_indices</strong> (<code>List[int]</code>, <em>optional</em>) &#x2014;
If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how
many stages the model has). If unset and <code>out_features</code> is set, will default to the corresponding stages.
If unset and <code>out_features</code> is unset, will default to the last stage. Must be in the
same order as defined in the <code>stage_names</code> attribute.`,name:"out_indices"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/configuration_convnextv2.py#L30"}}),ne=new De({props:{anchor:"transformers.ConvNextV2Config.example",$$slots:{default:[Tn]},$$scope:{ctx:T}}}),xe=new fe({props:{title:"ConvNextV2Model",local:"transformers.ConvNextV2Model",headingTag:"h2"}}),Te=new ee({props:{name:"class transformers.ConvNextV2Model",anchor:"transformers.ConvNextV2Model",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ConvNextV2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_convnextv2.py#L344"}}),$e=new ee({props:{name:"forward",anchor:"transformers.ConvNextV2Model.forward",parameters:[{name:"pixel_values",val:": FloatTensor = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ConvNextV2Model.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.ConvNextV2Model.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ConvNextV2Model.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_convnextv2.py#L363",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config"
>ConvNextV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) ‚Äî Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, hidden_size)</code>) ‚Äî Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_outputs.BaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),oe=new Se({props:{$$slots:{default:[$n]},$$scope:{ctx:T}}}),se=new De({props:{anchor:"transformers.ConvNextV2Model.forward.example",$$slots:{default:[wn]},$$scope:{ctx:T}}}),we=new fe({props:{title:"ConvNextV2ForImageClassification",local:"transformers.ConvNextV2ForImageClassification",headingTag:"h2"}}),je=new ee({props:{name:"class transformers.ConvNextV2ForImageClassification",anchor:"transformers.ConvNextV2ForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.ConvNextV2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_convnextv2.py#L408"}}),Ne=new ee({props:{name:"forward",anchor:"transformers.ConvNextV2ForImageClassification.forward",parameters:[{name:"pixel_values",val:": FloatTensor = None"},{name:"labels",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.ConvNextV2ForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/convnext#transformers.ConvNextImageProcessor">ConvNextImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.ConvNextV2ForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.ConvNextV2ForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.ConvNextV2ForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_convnextv2.py#L431",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config"
>ConvNextV2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) ‚Äî Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) ‚Äî Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),ae=new Se({props:{$$slots:{default:[jn]},$$scope:{ctx:T}}}),re=new De({props:{anchor:"transformers.ConvNextV2ForImageClassification.forward.example",$$slots:{default:[Nn]},$$scope:{ctx:T}}}),Ve=new fe({props:{title:"TFConvNextV2Model",local:"transformers.TFConvNextV2Model",headingTag:"h2"}}),Fe=new ee({props:{name:"class transformers.TFConvNextV2Model",anchor:"transformers.TFConvNextV2Model",parameters:[{name:"config",val:": ConvNextV2Config"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFConvNextV2Model.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L545"}}),le=new Se({props:{$$slots:{default:[Vn]},$$scope:{ctx:T}}}),Ie=new ee({props:{name:"call",anchor:"transformers.TFConvNextV2Model.call",parameters:[{name:"pixel_values",val:": TFModelInputType | None = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": bool = False"}],parametersDescription:[{anchor:"transformers.TFConvNextV2Model.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code>, <code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFConvNextV2Model.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFConvNextV2Model.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to <code>True</code>.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L554",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config"
>ConvNextV2Config</a>) and inputs.</p>
<ul>
<li>
<p><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) ‚Äî Sequence of hidden-states at the output of the last layer of the model.</p>
</li>
<li>
<p><strong>pooler_output</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, hidden_size)</code>) ‚Äî Last layer hidden-state after a pooling operation on the spatial dimensions.</p>
</li>
<li>
<p><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>tf.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, + one for
the output of each layer) of shape <code>(batch_size, num_channels, height, width)</code>.</p>
<p>Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.</p>
</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndNoAttention</code> or <code>tuple(tf.Tensor)</code></p>
`}}),ie=new Se({props:{$$slots:{default:[Fn]},$$scope:{ctx:T}}}),de=new De({props:{anchor:"transformers.TFConvNextV2Model.call.example",$$slots:{default:[In]},$$scope:{ctx:T}}}),ke=new fe({props:{title:"TFConvNextV2ForImageClassification",local:"transformers.TFConvNextV2ForImageClassification",headingTag:"h2"}}),Je=new ee({props:{name:"class transformers.TFConvNextV2ForImageClassification",anchor:"transformers.TFConvNextV2ForImageClassification",parameters:[{name:"config",val:": ConvNextV2Config"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFConvNextV2ForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config">ConvNextV2Config</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L603"}}),ce=new Se({props:{$$slots:{default:[kn]},$$scope:{ctx:T}}}),Ze=new ee({props:{name:"call",anchor:"transformers.TFConvNextV2ForImageClassification.call",parameters:[{name:"pixel_values",val:": TFModelInputType | None = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"labels",val:": np.ndarray | tf.Tensor | None = None"},{name:"training",val:": Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFConvNextV2ForImageClassification.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code>, <code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See
<a href="/docs/transformers/main/ja/model_doc/deit#transformers.DeiTFeatureExtractor.__call__">ConvNextImageProcessor.<strong>call</strong>()</a> for details.`,name:"pixel_values"},{anchor:"transformers.TFConvNextV2ForImageClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFConvNextV2ForImageClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to <code>True</code>.`,name:"return_dict"},{anchor:"transformers.TFConvNextV2ForImageClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> or <code>np.ndarray</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/convnextv2/modeling_tf_convnextv2.py#L625",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/convnextv2#transformers.ConvNextV2Config"
>ConvNextV2Config</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) ‚Äî Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) ‚Äî Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) ‚Äî Tuple of <code>tf.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, + one for
the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also called
feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention</code> or <code>tuple(tf.Tensor)</code></p>
`}}),me=new Se({props:{$$slots:{default:[Jn]},$$scope:{ctx:T}}}),pe=new De({props:{anchor:"transformers.TFConvNextV2ForImageClassification.call.example",$$slots:{default:[Zn]},$$scope:{ctx:T}}}),{c(){t=c("meta"),v=l(),d=c("p"),r=l(),_(g.$$.fragment),n=l(),_(u.$$.fragment),A=l(),N=c("p"),N.innerHTML=w,D=l(),V=c("p"),V.textContent=j,O=l(),p=c("p"),p.innerHTML=$,Ke=l(),te=c("img"),et=l(),ue=c("small"),ue.innerHTML=St,tt=l(),he=c("p"),he.innerHTML=At,nt=l(),_(ge.$$.fragment),ot=l(),ve=c("p"),ve.textContent=Dt,st=l(),_(_e.$$.fragment),at=l(),be=c("ul"),be.innerHTML=Ot,rt=l(),Me=c("p"),Me.textContent=Kt,lt=l(),_(Ce.$$.fragment),it=l(),Z=c("div"),_(ye.$$.fragment),bt=l(),We=c("p"),We.innerHTML=en,Mt=l(),Ue=c("p"),Ue.innerHTML=tn,Ct=l(),_(ne.$$.fragment),dt=l(),_(xe.$$.fragment),ct=l(),X=c("div"),_(Te.$$.fragment),yt=l(),He=c("p"),He.innerHTML=nn,xt=l(),H=c("div"),_($e.$$.fragment),Tt=l(),Ge=c("p"),Ge.innerHTML=on,$t=l(),_(oe.$$.fragment),wt=l(),_(se.$$.fragment),mt=l(),_(we.$$.fragment),pt=l(),W=c("div"),_(je.$$.fragment),jt=l(),Re=c("p"),Re.textContent=sn,Nt=l(),Be=c("p"),Be.innerHTML=an,Vt=l(),G=c("div"),_(Ne.$$.fragment),Ft=l(),Xe=c("p"),Xe.innerHTML=rn,It=l(),_(ae.$$.fragment),kt=l(),_(re.$$.fragment),ft=l(),_(Ve.$$.fragment),ut=l(),I=c("div"),_(Fe.$$.fragment),Jt=l(),ze=c("p"),ze.innerHTML=ln,Zt=l(),Le=c("p"),Le.innerHTML=dn,Wt=l(),_(le.$$.fragment),Ut=l(),R=c("div"),_(Ie.$$.fragment),Ht=l(),Pe=c("p"),Pe.innerHTML=cn,Gt=l(),_(ie.$$.fragment),Rt=l(),_(de.$$.fragment),ht=l(),_(ke.$$.fragment),gt=l(),F=c("div"),_(Je.$$.fragment),Bt=l(),Ee=c("p"),Ee.textContent=mn,Xt=l(),Ye=c("p"),Ye.innerHTML=pn,zt=l(),qe=c("p"),qe.innerHTML=fn,Lt=l(),_(ce.$$.fragment),Pt=l(),B=c("div"),_(Ze.$$.fragment),Et=l(),Qe=c("p"),Qe.innerHTML=un,Yt=l(),_(me.$$.fragment),qt=l(),_(pe.$$.fragment),vt=l(),Ae=c("p"),this.h()},l(e){const s=yn("svelte-u9bgzb",document.head);t=m(s,"META",{name:!0,content:!0}),s.forEach(o),v=i(e),d=m(e,"P",{}),Q(d).forEach(o),r=i(e),b(g.$$.fragment,e),n=i(e),b(u.$$.fragment,e),A=i(e),N=m(e,"P",{"data-svelte-h":!0}),h(N)!=="svelte-1geak2k"&&(N.innerHTML=w),D=i(e),V=m(e,"P",{"data-svelte-h":!0}),h(V)!=="svelte-1cv3nri"&&(V.textContent=j),O=i(e),p=m(e,"P",{"data-svelte-h":!0}),h(p)!=="svelte-16r1jgk"&&(p.innerHTML=$),Ke=i(e),te=m(e,"IMG",{src:!0,alt:!0,width:!0}),et=i(e),ue=m(e,"SMALL",{"data-svelte-h":!0}),h(ue)!=="svelte-6iufyc"&&(ue.innerHTML=St),tt=i(e),he=m(e,"P",{"data-svelte-h":!0}),h(he)!=="svelte-w90m9d"&&(he.innerHTML=At),nt=i(e),b(ge.$$.fragment,e),ot=i(e),ve=m(e,"P",{"data-svelte-h":!0}),h(ve)!=="svelte-1oqd6dv"&&(ve.textContent=Dt),st=i(e),b(_e.$$.fragment,e),at=i(e),be=m(e,"UL",{"data-svelte-h":!0}),h(be)!=="svelte-1xhl1et"&&(be.innerHTML=Ot),rt=i(e),Me=m(e,"P",{"data-svelte-h":!0}),h(Me)!=="svelte-17ytafw"&&(Me.textContent=Kt),lt=i(e),b(Ce.$$.fragment,e),it=i(e),Z=m(e,"DIV",{class:!0});var z=Q(Z);b(ye.$$.fragment,z),bt=i(z),We=m(z,"P",{"data-svelte-h":!0}),h(We)!=="svelte-14yqaym"&&(We.innerHTML=en),Mt=i(z),Ue=m(z,"P",{"data-svelte-h":!0}),h(Ue)!=="svelte-1s6wgpv"&&(Ue.innerHTML=tn),Ct=i(z),b(ne.$$.fragment,z),z.forEach(o),dt=i(e),b(xe.$$.fragment,e),ct=i(e),X=m(e,"DIV",{class:!0});var K=Q(X);b(Te.$$.fragment,K),yt=i(K),He=m(K,"P",{"data-svelte-h":!0}),h(He)!=="svelte-vxpmpx"&&(He.innerHTML=nn),xt=i(K),H=m(K,"DIV",{class:!0});var L=Q(H);b($e.$$.fragment,L),Tt=i(L),Ge=m(L,"P",{"data-svelte-h":!0}),h(Ge)!=="svelte-q3lg4c"&&(Ge.innerHTML=on),$t=i(L),b(oe.$$.fragment,L),wt=i(L),b(se.$$.fragment,L),L.forEach(o),K.forEach(o),mt=i(e),b(we.$$.fragment,e),pt=i(e),W=m(e,"DIV",{class:!0});var P=Q(W);b(je.$$.fragment,P),jt=i(P),Re=m(P,"P",{"data-svelte-h":!0}),h(Re)!=="svelte-14zttn9"&&(Re.textContent=sn),Nt=i(P),Be=m(P,"P",{"data-svelte-h":!0}),h(Be)!=="svelte-1gjh92c"&&(Be.innerHTML=an),Vt=i(P),G=m(P,"DIV",{class:!0});var E=Q(G);b(Ne.$$.fragment,E),Ft=i(E),Xe=m(E,"P",{"data-svelte-h":!0}),h(Xe)!=="svelte-1h2vmw6"&&(Xe.innerHTML=rn),It=i(E),b(ae.$$.fragment,E),kt=i(E),b(re.$$.fragment,E),E.forEach(o),P.forEach(o),ft=i(e),b(Ve.$$.fragment,e),ut=i(e),I=m(e,"DIV",{class:!0});var U=Q(I);b(Fe.$$.fragment,U),Jt=i(U),ze=m(U,"P",{"data-svelte-h":!0}),h(ze)!=="svelte-t68ebv"&&(ze.innerHTML=ln),Zt=i(U),Le=m(U,"P",{"data-svelte-h":!0}),h(Le)!=="svelte-1be7e3c"&&(Le.innerHTML=dn),Wt=i(U),b(le.$$.fragment,U),Ut=i(U),R=m(U,"DIV",{class:!0});var Y=Q(R);b(Ie.$$.fragment,Y),Ht=i(Y),Pe=m(Y,"P",{"data-svelte-h":!0}),h(Pe)!=="svelte-hsiw8s"&&(Pe.innerHTML=cn),Gt=i(Y),b(ie.$$.fragment,Y),Rt=i(Y),b(de.$$.fragment,Y),Y.forEach(o),U.forEach(o),ht=i(e),b(ke.$$.fragment,e),gt=i(e),F=m(e,"DIV",{class:!0});var k=Q(F);b(Je.$$.fragment,k),Bt=i(k),Ee=m(k,"P",{"data-svelte-h":!0}),h(Ee)!=="svelte-14zttn9"&&(Ee.textContent=mn),Xt=i(k),Ye=m(k,"P",{"data-svelte-h":!0}),h(Ye)!=="svelte-x53t1u"&&(Ye.innerHTML=pn),zt=i(k),qe=m(k,"P",{"data-svelte-h":!0}),h(qe)!=="svelte-1be7e3c"&&(qe.innerHTML=fn),Lt=i(k),b(ce.$$.fragment,k),Pt=i(k),B=m(k,"DIV",{class:!0});var q=Q(B);b(Ze.$$.fragment,q),Et=i(q),Qe=m(q,"P",{"data-svelte-h":!0}),h(Qe)!=="svelte-2rgvu2"&&(Qe.innerHTML=un),Yt=i(q),b(me.$$.fragment,q),qt=i(q),b(pe.$$.fragment,q),q.forEach(o),k.forEach(o),vt=i(e),Ae=m(e,"P",{}),Q(Ae).forEach(o),this.h()},h(){J(t,"name","hf:doc:metadata"),J(t,"content",Un),_n(te.src,Qt="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/convnextv2_architecture.png")||J(te,"src",Qt),J(te,"alt","ÊèèÁîª"),J(te,"width","600"),J(Z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(H,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(X,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(G,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(R,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(I,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(B,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),J(F,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(e,s){f(document.head,t),a(e,v,s),a(e,d,s),a(e,r,s),M(g,e,s),a(e,n,s),M(u,e,s),a(e,A,s),a(e,N,s),a(e,D,s),a(e,V,s),a(e,O,s),a(e,p,s),a(e,Ke,s),a(e,te,s),a(e,et,s),a(e,ue,s),a(e,tt,s),a(e,he,s),a(e,nt,s),M(ge,e,s),a(e,ot,s),a(e,ve,s),a(e,st,s),M(_e,e,s),a(e,at,s),a(e,be,s),a(e,rt,s),a(e,Me,s),a(e,lt,s),M(Ce,e,s),a(e,it,s),a(e,Z,s),M(ye,Z,null),f(Z,bt),f(Z,We),f(Z,Mt),f(Z,Ue),f(Z,Ct),M(ne,Z,null),a(e,dt,s),M(xe,e,s),a(e,ct,s),a(e,X,s),M(Te,X,null),f(X,yt),f(X,He),f(X,xt),f(X,H),M($e,H,null),f(H,Tt),f(H,Ge),f(H,$t),M(oe,H,null),f(H,wt),M(se,H,null),a(e,mt,s),M(we,e,s),a(e,pt,s),a(e,W,s),M(je,W,null),f(W,jt),f(W,Re),f(W,Nt),f(W,Be),f(W,Vt),f(W,G),M(Ne,G,null),f(G,Ft),f(G,Xe),f(G,It),M(ae,G,null),f(G,kt),M(re,G,null),a(e,ft,s),M(Ve,e,s),a(e,ut,s),a(e,I,s),M(Fe,I,null),f(I,Jt),f(I,ze),f(I,Zt),f(I,Le),f(I,Wt),M(le,I,null),f(I,Ut),f(I,R),M(Ie,R,null),f(R,Ht),f(R,Pe),f(R,Gt),M(ie,R,null),f(R,Rt),M(de,R,null),a(e,ht,s),M(ke,e,s),a(e,gt,s),a(e,F,s),M(Je,F,null),f(F,Bt),f(F,Ee),f(F,Xt),f(F,Ye),f(F,zt),f(F,qe),f(F,Lt),M(ce,F,null),f(F,Pt),f(F,B),M(Ze,B,null),f(B,Et),f(B,Qe),f(B,Yt),M(me,B,null),f(B,qt),M(pe,B,null),a(e,vt,s),a(e,Ae,s),_t=!0},p(e,[s]){const z={};s&2&&(z.$$scope={dirty:s,ctx:e}),ne.$set(z);const K={};s&2&&(K.$$scope={dirty:s,ctx:e}),oe.$set(K);const L={};s&2&&(L.$$scope={dirty:s,ctx:e}),se.$set(L);const P={};s&2&&(P.$$scope={dirty:s,ctx:e}),ae.$set(P);const E={};s&2&&(E.$$scope={dirty:s,ctx:e}),re.$set(E);const U={};s&2&&(U.$$scope={dirty:s,ctx:e}),le.$set(U);const Y={};s&2&&(Y.$$scope={dirty:s,ctx:e}),ie.$set(Y);const k={};s&2&&(k.$$scope={dirty:s,ctx:e}),de.$set(k);const q={};s&2&&(q.$$scope={dirty:s,ctx:e}),ce.$set(q);const hn={};s&2&&(hn.$$scope={dirty:s,ctx:e}),me.$set(hn);const gn={};s&2&&(gn.$$scope={dirty:s,ctx:e}),pe.$set(gn)},i(e){_t||(C(g.$$.fragment,e),C(u.$$.fragment,e),C(ge.$$.fragment,e),C(_e.$$.fragment,e),C(Ce.$$.fragment,e),C(ye.$$.fragment,e),C(ne.$$.fragment,e),C(xe.$$.fragment,e),C(Te.$$.fragment,e),C($e.$$.fragment,e),C(oe.$$.fragment,e),C(se.$$.fragment,e),C(we.$$.fragment,e),C(je.$$.fragment,e),C(Ne.$$.fragment,e),C(ae.$$.fragment,e),C(re.$$.fragment,e),C(Ve.$$.fragment,e),C(Fe.$$.fragment,e),C(le.$$.fragment,e),C(Ie.$$.fragment,e),C(ie.$$.fragment,e),C(de.$$.fragment,e),C(ke.$$.fragment,e),C(Je.$$.fragment,e),C(ce.$$.fragment,e),C(Ze.$$.fragment,e),C(me.$$.fragment,e),C(pe.$$.fragment,e),_t=!0)},o(e){y(g.$$.fragment,e),y(u.$$.fragment,e),y(ge.$$.fragment,e),y(_e.$$.fragment,e),y(Ce.$$.fragment,e),y(ye.$$.fragment,e),y(ne.$$.fragment,e),y(xe.$$.fragment,e),y(Te.$$.fragment,e),y($e.$$.fragment,e),y(oe.$$.fragment,e),y(se.$$.fragment,e),y(we.$$.fragment,e),y(je.$$.fragment,e),y(Ne.$$.fragment,e),y(ae.$$.fragment,e),y(re.$$.fragment,e),y(Ve.$$.fragment,e),y(Fe.$$.fragment,e),y(le.$$.fragment,e),y(Ie.$$.fragment,e),y(ie.$$.fragment,e),y(de.$$.fragment,e),y(ke.$$.fragment,e),y(Je.$$.fragment,e),y(ce.$$.fragment,e),y(Ze.$$.fragment,e),y(me.$$.fragment,e),y(pe.$$.fragment,e),_t=!1},d(e){e&&(o(v),o(d),o(r),o(n),o(A),o(N),o(D),o(V),o(O),o(p),o(Ke),o(te),o(et),o(ue),o(tt),o(he),o(nt),o(ot),o(ve),o(st),o(at),o(be),o(rt),o(Me),o(lt),o(it),o(Z),o(dt),o(ct),o(X),o(mt),o(pt),o(W),o(ft),o(ut),o(I),o(ht),o(gt),o(F),o(vt),o(Ae)),o(t),x(g,e),x(u,e),x(ge,e),x(_e,e),x(Ce,e),x(ye),x(ne),x(xe,e),x(Te),x($e),x(oe),x(se),x(we,e),x(je),x(Ne),x(ae),x(re),x(Ve,e),x(Fe),x(le),x(Ie),x(ie),x(de),x(ke,e),x(Je),x(ce),x(Ze),x(me),x(pe)}}}const Un='{"title":"ConvNeXt V2","local":"convnext-v2","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"ConvNextV2Config","local":"transformers.ConvNextV2Config","sections":[],"depth":2},{"title":"ConvNextV2Model","local":"transformers.ConvNextV2Model","sections":[],"depth":2},{"title":"ConvNextV2ForImageClassification","local":"transformers.ConvNextV2ForImageClassification","sections":[],"depth":2},{"title":"TFConvNextV2Model","local":"transformers.TFConvNextV2Model","sections":[],"depth":2},{"title":"TFConvNextV2ForImageClassification","local":"transformers.TFConvNextV2ForImageClassification","sections":[],"depth":2}],"depth":1}';function Hn(T){return bn(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class Yn extends Mn{constructor(t){super(),Cn(this,t,Hn,Wn,vn,{})}}export{Yn as component};
