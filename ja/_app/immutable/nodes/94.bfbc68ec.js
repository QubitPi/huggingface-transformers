import{s as Ue,o as Ie,n as fe}from"../chunks/scheduler.9bc65507.js";import{S as Ze,i as Je,g as u,s as l,r as M,A as We,h as v,f as a,c as i,j as me,u as $,x,k as pe,y as g,a as r,v as T,d as y,t as w,w as j}from"../chunks/index.707bf1b6.js";import{T as ye}from"../chunks/Tip.c2ecdbf4.js";import{D as Me}from"../chunks/Docstring.17db21ae.js";import{C as xe}from"../chunks/CodeBlock.54a9f38d.js";import{F as Ne,M as ke}from"../chunks/Markdown.8ab98a13.js";import{E as je}from"../chunks/ExampleCodeBlock.4f515aa9.js";import{P as ze}from"../chunks/PipelineTag.44585822.js";import{H as be}from"../chunks/Heading.342b1fa6.js";function Le(F){let e,d="Example:",s,n,h;return n=new xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEN2dENvbmZpZyUyQyUyMEN2dE1vZGVsJTBBJTBBJTIzJTIwSW5pdGlhbGl6aW5nJTIwYSUyMEN2dCUyMG1zZnQlMkZjdnQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFjb25maWd1cmF0aW9uJTIwJTNEJTIwQ3Z0Q29uZmlnKCklMEElMEElMjMlMjBJbml0aWFsaXppbmclMjBhJTIwbW9kZWwlMjAod2l0aCUyMHJhbmRvbSUyMHdlaWdodHMpJTIwZnJvbSUyMHRoZSUyMG1zZnQlMkZjdnQlMjBzdHlsZSUyMGNvbmZpZ3VyYXRpb24lMEFtb2RlbCUyMCUzRCUyMEN2dE1vZGVsKGNvbmZpZ3VyYXRpb24pJTBBJTBBJTIzJTIwQWNjZXNzaW5nJTIwdGhlJTIwbW9kZWwlMjBjb25maWd1cmF0aW9uJTBBY29uZmlndXJhdGlvbiUyMCUzRCUyMG1vZGVsLmNvbmZpZw==",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CvtConfig, CvtModel

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a Cvt msft/cvt style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = CvtConfig()

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Initializing a model (with random weights) from the msft/cvt style configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CvtModel(configuration)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># Accessing the model configuration</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>configuration = model.config`,wrap:!1}}),{c(){e=u("p"),e.textContent=d,s=l(),M(n.$$.fragment)},l(o){e=v(o,"P",{"data-svelte-h":!0}),x(e)!=="svelte-11lpom8"&&(e.textContent=d),s=i(o),$(n.$$.fragment,o)},m(o,_){r(o,e,_),r(o,s,_),T(n,o,_),h=!0},p:fe,i(o){h||(y(n.$$.fragment,o),h=!0)},o(o){w(n.$$.fragment,o),h=!1},d(o){o&&(a(e),a(s)),j(n,o)}}}function He(F){let e,d=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=u("p"),e.innerHTML=d},l(s){e=v(s,"P",{"data-svelte-h":!0}),x(e)!=="svelte-fincs2"&&(e.innerHTML=d)},m(s,n){r(s,e,n)},p:fe,d(s){s&&a(e)}}}function Re(F){let e,d="Example:",s,n,h;return n=new xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEN2dE1vZGVsJTBBaW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwZGF0YXNldHMlMjBpbXBvcnQlMjBsb2FkX2RhdGFzZXQlMEElMEFkYXRhc2V0JTIwJTNEJTIwbG9hZF9kYXRhc2V0KCUyMmh1Z2dpbmdmYWNlJTJGY2F0cy1pbWFnZSUyMiklMEFpbWFnZSUyMCUzRCUyMGRhdGFzZXQlNUIlMjJ0ZXN0JTIyJTVEJTVCJTIyaW1hZ2UlMjIlNUQlNUIwJTVEJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZjdnQtMTMlMjIpJTBBbW9kZWwlMjAlM0QlMjBDdnRNb2RlbC5mcm9tX3ByZXRyYWluZWQoJTIybWljcm9zb2Z0JTJGY3Z0LTEzJTIyKSUwQSUwQWlucHV0cyUyMCUzRCUyMGltYWdlX3Byb2Nlc3NvcihpbWFnZSUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpJTBBJTBBd2l0aCUyMHRvcmNoLm5vX2dyYWQoKSUzQSUwQSUyMCUyMCUyMCUyMG91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEElMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRlJTBBbGlzdChsYXN0X2hpZGRlbl9zdGF0ZXMuc2hhcGUp",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, CvtModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/cvt-13&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CvtModel.from_pretrained(<span class="hljs-string">&quot;microsoft/cvt-13&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    outputs = model(**inputs)

<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">list</span>(last_hidden_states.shape)
[<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>]`,wrap:!1}}),{c(){e=u("p"),e.textContent=d,s=l(),M(n.$$.fragment)},l(o){e=v(o,"P",{"data-svelte-h":!0}),x(e)!=="svelte-11lpom8"&&(e.textContent=d),s=i(o),$(n.$$.fragment,o)},m(o,_){r(o,e,_),r(o,s,_),T(n,o,_),h=!0},p:fe,i(o){h||(y(n.$$.fragment,o),h=!0)},o(o){w(n.$$.fragment,o),h=!1},d(o){o&&(a(e),a(s)),j(n,o)}}}function Ve(F){let e,d=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=u("p"),e.innerHTML=d},l(s){e=v(s,"P",{"data-svelte-h":!0}),x(e)!=="svelte-fincs2"&&(e.innerHTML=d)},m(s,n){r(s,e,n)},p:fe,d(s){s&&a(e)}}}function Be(F){let e,d="Example:",s,n,h;return n=new xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMEN2dEZvckltYWdlQ2xhc3NpZmljYXRpb24lMEFpbXBvcnQlMjB0b3JjaCUwQWZyb20lMjBkYXRhc2V0cyUyMGltcG9ydCUyMGxvYWRfZGF0YXNldCUwQSUwQWRhdGFzZXQlMjAlM0QlMjBsb2FkX2RhdGFzZXQoJTIyaHVnZ2luZ2ZhY2UlMkZjYXRzLWltYWdlJTIyKSUwQWltYWdlJTIwJTNEJTIwZGF0YXNldCU1QiUyMnRlc3QlMjIlNUQlNUIlMjJpbWFnZSUyMiU1RCU1QjAlNUQlMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmN2dC0xMyUyMiklMEFtb2RlbCUyMCUzRCUyMEN2dEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmN2dC0xMyUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnB0JTIyKSUwQSUwQXdpdGglMjB0b3JjaC5ub19ncmFkKCklM0ElMEElMjAlMjAlMjAlMjBsb2dpdHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cykubG9naXRzJTBBJTBBJTIzJTIwbW9kZWwlMjBwcmVkaWN0cyUyMG9uZSUyMG9mJTIwdGhlJTIwMTAwMCUyMEltYWdlTmV0JTIwY2xhc3NlcyUwQXByZWRpY3RlZF9sYWJlbCUyMCUzRCUyMGxvZ2l0cy5hcmdtYXgoLTEpLml0ZW0oKSUwQXByaW50KG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QnByZWRpY3RlZF9sYWJlbCU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, CvtForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> torch
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset

<span class="hljs-meta">&gt;&gt;&gt; </span>dataset = load_dataset(<span class="hljs-string">&quot;huggingface/cats-image&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>image = dataset[<span class="hljs-string">&quot;test&quot;</span>][<span class="hljs-string">&quot;image&quot;</span>][<span class="hljs-number">0</span>]

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/cvt-13&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = CvtForImageClassification.from_pretrained(<span class="hljs-string">&quot;microsoft/cvt-13&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(image, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">with</span> torch.no_grad():
<span class="hljs-meta">... </span>    logits = model(**inputs).logits

<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_label = logits.argmax(-<span class="hljs-number">1</span>).item()
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(model.config.id2label[predicted_label])
tabby, tabby cat`,wrap:!1}}),{c(){e=u("p"),e.textContent=d,s=l(),M(n.$$.fragment)},l(o){e=v(o,"P",{"data-svelte-h":!0}),x(e)!=="svelte-11lpom8"&&(e.textContent=d),s=i(o),$(n.$$.fragment,o)},m(o,_){r(o,e,_),r(o,s,_),T(n,o,_),h=!0},p:fe,i(o){h||(y(n.$$.fragment,o),h=!0)},o(o){w(n.$$.fragment,o),h=!1},d(o){o&&(a(e),a(s)),j(n,o)}}}function Ee(F){let e,d,s,n,h,o,_=`The bare Cvt Model transformer outputting raw hidden-states without any specific head on top.
This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,q,p,Z,le,H,$e='The <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtModel">CvtModel</a> forward method, overrides the <code>__call__</code> special method.',ie,J,he,U,R,se,X,N,Y,V,ne,S=`Cvt Model transformer with an image classification head on top (a linear layer on top of the final hidden state of
the [CLS] token) e.g. for ImageNet.`,A,E,O=`This model is a PyTorch <a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module" rel="nofollow">torch.nn.Module</a> subclass. Use it
as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and
behavior.`,z,W,P,K,G,ve='The <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtForImageClassification">CvtForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',ee,Q,ge,B,te;return e=new be({props:{title:"CvtModel",local:"transformers.CvtModel",headingTag:"h2"}}),n=new Me({props:{name:"class transformers.CvtModel",anchor:"transformers.CvtModel",parameters:[{name:"config",val:""},{name:"add_pooling_layer",val:" = True"}],parametersDescription:[{anchor:"transformers.CvtModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig">CvtConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/cvt/modeling_cvt.py#L586"}}),Z=new Me({props:{name:"forward",anchor:"transformers.CvtModel.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CvtModel.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <code>CvtImageProcessor.__call__</code>
for details.`,name:"pixel_values"},{anchor:"transformers.CvtModel.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CvtModel.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/cvt/modeling_cvt.py#L605",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken</code> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig"
>CvtConfig</a>) and inputs.</p>
<ul>
<li><strong>last_hidden_state</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</li>
<li><strong>cls_token_value</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, 1, hidden_size)</code>) — Classification token at the output of the last layer of the model.</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings + one for the output of each layer) of
shape <code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of each layer
plus the initial embedding outputs.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.cvt.modeling_cvt.BaseModelOutputWithCLSToken</code> or <code>tuple(torch.FloatTensor)</code></p>
`}}),J=new ye({props:{$$slots:{default:[He]},$$scope:{ctx:F}}}),U=new je({props:{anchor:"transformers.CvtModel.forward.example",$$slots:{default:[Re]},$$scope:{ctx:F}}}),se=new be({props:{title:"CvtForImageClassification",local:"transformers.CvtForImageClassification",headingTag:"h2"}}),Y=new Me({props:{name:"class transformers.CvtForImageClassification",anchor:"transformers.CvtForImageClassification",parameters:[{name:"config",val:""}],parametersDescription:[{anchor:"transformers.CvtForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig">CvtConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.PreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/cvt/modeling_cvt.py#L644"}}),P=new Me({props:{name:"forward",anchor:"transformers.CvtForImageClassification.forward",parameters:[{name:"pixel_values",val:": Optional = None"},{name:"labels",val:": Optional = None"},{name:"output_hidden_states",val:": Optional = None"},{name:"return_dict",val:": Optional = None"}],parametersDescription:[{anchor:"transformers.CvtForImageClassification.forward.pixel_values",description:`<strong>pixel_values</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <code>CvtImageProcessor.__call__</code>
for details.`,name:"pixel_values"},{anchor:"transformers.CvtForImageClassification.forward.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail.`,name:"output_hidden_states"},{anchor:"transformers.CvtForImageClassification.forward.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple.`,name:"return_dict"},{anchor:"transformers.CvtForImageClassification.forward.labels",description:`<strong>labels</strong> (<code>torch.LongTensor</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/cvt/modeling_cvt.py#L666",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or a tuple of
<code>torch.FloatTensor</code> (if <code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various
elements depending on the configuration (<a
  href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig"
>CvtConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>torch.FloatTensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>torch.FloatTensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(torch.FloatTensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>torch.FloatTensor</code> (one for the output of the embeddings, if the model has an embedding layer, +
one for the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also
called feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><a
  href="/docs/transformers/main/ja/main_classes/output#transformers.modeling_outputs.ImageClassifierOutputWithNoAttention"
>transformers.modeling_outputs.ImageClassifierOutputWithNoAttention</a> or <code>tuple(torch.FloatTensor)</code></p>
`}}),Q=new ye({props:{$$slots:{default:[Ve]},$$scope:{ctx:F}}}),B=new je({props:{anchor:"transformers.CvtForImageClassification.forward.example",$$slots:{default:[Be]},$$scope:{ctx:F}}}),{c(){M(e.$$.fragment),d=l(),s=u("div"),M(n.$$.fragment),h=l(),o=u("p"),o.innerHTML=_,q=l(),p=u("div"),M(Z.$$.fragment),le=l(),H=u("p"),H.innerHTML=$e,ie=l(),M(J.$$.fragment),he=l(),M(U.$$.fragment),R=l(),M(se.$$.fragment),X=l(),N=u("div"),M(Y.$$.fragment),V=l(),ne=u("p"),ne.textContent=S,A=l(),E=u("p"),E.innerHTML=O,z=l(),W=u("div"),M(P.$$.fragment),K=l(),G=u("p"),G.innerHTML=ve,ee=l(),M(Q.$$.fragment),ge=l(),M(B.$$.fragment),this.h()},l(f){$(e.$$.fragment,f),d=i(f),s=v(f,"DIV",{class:!0});var C=me(s);$(n.$$.fragment,C),h=i(C),o=v(C,"P",{"data-svelte-h":!0}),x(o)!=="svelte-1qa4t0c"&&(o.innerHTML=_),q=i(C),p=v(C,"DIV",{class:!0});var L=me(p);$(Z.$$.fragment,L),le=i(L),H=v(L,"P",{"data-svelte-h":!0}),x(H)!=="svelte-11royay"&&(H.innerHTML=$e),ie=i(L),$(J.$$.fragment,L),he=i(L),$(U.$$.fragment,L),L.forEach(a),C.forEach(a),R=i(f),$(se.$$.fragment,f),X=i(f),N=v(f,"DIV",{class:!0});var k=me(N);$(Y.$$.fragment,k),V=i(k),ne=v(k,"P",{"data-svelte-h":!0}),x(ne)!=="svelte-ztj7cy"&&(ne.textContent=S),A=i(k),E=v(k,"P",{"data-svelte-h":!0}),x(E)!=="svelte-1gjh92c"&&(E.innerHTML=O),z=i(k),W=v(k,"DIV",{class:!0});var I=me(W);$(P.$$.fragment,I),K=i(I),G=v(I,"P",{"data-svelte-h":!0}),x(G)!=="svelte-kgo2b2"&&(G.innerHTML=ve),ee=i(I),$(Q.$$.fragment,I),ge=i(I),$(B.$$.fragment,I),I.forEach(a),k.forEach(a),this.h()},h(){pe(p,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),pe(s,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),pe(W,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),pe(N,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(f,C){T(e,f,C),r(f,d,C),r(f,s,C),T(n,s,null),g(s,h),g(s,o),g(s,q),g(s,p),T(Z,p,null),g(p,le),g(p,H),g(p,ie),T(J,p,null),g(p,he),T(U,p,null),r(f,R,C),T(se,f,C),r(f,X,C),r(f,N,C),T(Y,N,null),g(N,V),g(N,ne),g(N,A),g(N,E),g(N,z),g(N,W),T(P,W,null),g(W,K),g(W,G),g(W,ee),T(Q,W,null),g(W,ge),T(B,W,null),te=!0},p(f,C){const L={};C&2&&(L.$$scope={dirty:C,ctx:f}),J.$set(L);const k={};C&2&&(k.$$scope={dirty:C,ctx:f}),U.$set(k);const I={};C&2&&(I.$$scope={dirty:C,ctx:f}),Q.$set(I);const _e={};C&2&&(_e.$$scope={dirty:C,ctx:f}),B.$set(_e)},i(f){te||(y(e.$$.fragment,f),y(n.$$.fragment,f),y(Z.$$.fragment,f),y(J.$$.fragment,f),y(U.$$.fragment,f),y(se.$$.fragment,f),y(Y.$$.fragment,f),y(P.$$.fragment,f),y(Q.$$.fragment,f),y(B.$$.fragment,f),te=!0)},o(f){w(e.$$.fragment,f),w(n.$$.fragment,f),w(Z.$$.fragment,f),w(J.$$.fragment,f),w(U.$$.fragment,f),w(se.$$.fragment,f),w(Y.$$.fragment,f),w(P.$$.fragment,f),w(Q.$$.fragment,f),w(B.$$.fragment,f),te=!1},d(f){f&&(a(d),a(s),a(R),a(X),a(N)),j(e,f),j(n),j(Z),j(J),j(U),j(se,f),j(Y),j(P),j(Q),j(B)}}}function Pe(F){let e,d;return e=new ke({props:{$$slots:{default:[Ee]},$$scope:{ctx:F}}}),{c(){M(e.$$.fragment)},l(s){$(e.$$.fragment,s)},m(s,n){T(e,s,n),d=!0},p(s,n){const h={};n&2&&(h.$$scope={dirty:n,ctx:s}),e.$set(h)},i(s){d||(y(e.$$.fragment,s),d=!0)},o(s){w(e.$$.fragment,s),d=!1},d(s){j(e,s)}}}function Qe(F){let e,d="TF 2.0 models accepts two formats as inputs:",s,n,h="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional arguments.</li>",o,_,q=`This second option is useful when using <code>keras.Model.fit</code> method which currently requires having all the
tensors in the first argument of the model call function: <code>model(inputs)</code>.`;return{c(){e=u("p"),e.textContent=d,s=l(),n=u("ul"),n.innerHTML=h,o=l(),_=u("p"),_.innerHTML=q},l(p){e=v(p,"P",{"data-svelte-h":!0}),x(e)!=="svelte-ha0dwg"&&(e.textContent=d),s=i(p),n=v(p,"UL",{"data-svelte-h":!0}),x(n)!=="svelte-1ar3osv"&&(n.innerHTML=h),o=i(p),_=v(p,"P",{"data-svelte-h":!0}),x(_)!=="svelte-14kkttf"&&(_.innerHTML=q)},m(p,Z){r(p,e,Z),r(p,s,Z),r(p,n,Z),r(p,o,Z),r(p,_,Z)},p:fe,d(p){p&&(a(e),a(s),a(n),a(o),a(_))}}}function qe(F){let e,d=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=u("p"),e.innerHTML=d},l(s){e=v(s,"P",{"data-svelte-h":!0}),x(e)!=="svelte-fincs2"&&(e.innerHTML=d)},m(s,n){r(s,e,n)},p:fe,d(s){s&&a(e)}}}function Ge(F){let e,d="Examples:",s,n,h;return n=new xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGQ3Z0TW9kZWwlMEFmcm9tJTIwUElMJTIwaW1wb3J0JTIwSW1hZ2UlMEFpbXBvcnQlMjByZXF1ZXN0cyUwQSUwQXVybCUyMCUzRCUyMCUyMmh0dHAlM0ElMkYlMkZpbWFnZXMuY29jb2RhdGFzZXQub3JnJTJGdmFsMjAxNyUyRjAwMDAwMDAzOTc2OS5qcGclMjIlMEFpbWFnZSUyMCUzRCUyMEltYWdlLm9wZW4ocmVxdWVzdHMuZ2V0KHVybCUyQyUyMHN0cmVhbSUzRFRydWUpLnJhdyklMEElMEFpbWFnZV9wcm9jZXNzb3IlMjAlM0QlMjBBdXRvSW1hZ2VQcm9jZXNzb3IuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmN2dC0xMyUyMiklMEFtb2RlbCUyMCUzRCUyMFRGQ3Z0TW9kZWwuZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmN2dC0xMyUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsYXN0X2hpZGRlbl9zdGF0ZXMlMjAlM0QlMjBvdXRwdXRzLmxhc3RfaGlkZGVuX3N0YXRl",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFCvtModel
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/cvt-13&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFCvtModel.from_pretrained(<span class="hljs-string">&quot;microsoft/cvt-13&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>last_hidden_states = outputs.last_hidden_state`,wrap:!1}}),{c(){e=u("p"),e.textContent=d,s=l(),M(n.$$.fragment)},l(o){e=v(o,"P",{"data-svelte-h":!0}),x(e)!=="svelte-kvfsh7"&&(e.textContent=d),s=i(o),$(n.$$.fragment,o)},m(o,_){r(o,e,_),r(o,s,_),T(n,o,_),h=!0},p:fe,i(o){h||(y(n.$$.fragment,o),h=!0)},o(o){w(n.$$.fragment,o),h=!1},d(o){o&&(a(e),a(s)),j(n,o)}}}function Xe(F){let e,d="TF 2.0 models accepts two formats as inputs:",s,n,h="<li>having all inputs as keyword arguments (like PyTorch models), or</li> <li>having all inputs as a list, tuple or dict in the first positional arguments.</li>",o,_,q=`This second option is useful when using <code>keras.Model.fit</code> method which currently requires having all the
tensors in the first argument of the model call function: <code>model(inputs)</code>.`;return{c(){e=u("p"),e.textContent=d,s=l(),n=u("ul"),n.innerHTML=h,o=l(),_=u("p"),_.innerHTML=q},l(p){e=v(p,"P",{"data-svelte-h":!0}),x(e)!=="svelte-ha0dwg"&&(e.textContent=d),s=i(p),n=v(p,"UL",{"data-svelte-h":!0}),x(n)!=="svelte-1ar3osv"&&(n.innerHTML=h),o=i(p),_=v(p,"P",{"data-svelte-h":!0}),x(_)!=="svelte-14kkttf"&&(_.innerHTML=q)},m(p,Z){r(p,e,Z),r(p,s,Z),r(p,n,Z),r(p,o,Z),r(p,_,Z)},p:fe,d(p){p&&(a(e),a(s),a(n),a(o),a(_))}}}function Ye(F){let e,d=`Although the recipe for forward pass needs to be defined within this function, one should call the <code>Module</code>
instance afterwards instead of this since the former takes care of running the pre and post processing steps while
the latter silently ignores them.`;return{c(){e=u("p"),e.innerHTML=d},l(s){e=v(s,"P",{"data-svelte-h":!0}),x(e)!=="svelte-fincs2"&&(e.innerHTML=d)},m(s,n){r(s,e,n)},p:fe,d(s){s&&a(e)}}}function Se(F){let e,d="Examples:",s,n,h;return n=new xe({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9JbWFnZVByb2Nlc3NvciUyQyUyMFRGQ3Z0Rm9ySW1hZ2VDbGFzc2lmaWNhdGlvbiUwQWltcG9ydCUyMHRlbnNvcmZsb3clMjBhcyUyMHRmJTBBZnJvbSUyMFBJTCUyMGltcG9ydCUyMEltYWdlJTBBaW1wb3J0JTIwcmVxdWVzdHMlMEElMEF1cmwlMjAlM0QlMjAlMjJodHRwJTNBJTJGJTJGaW1hZ2VzLmNvY29kYXRhc2V0Lm9yZyUyRnZhbDIwMTclMkYwMDAwMDAwMzk3NjkuanBnJTIyJTBBaW1hZ2UlMjAlM0QlMjBJbWFnZS5vcGVuKHJlcXVlc3RzLmdldCh1cmwlMkMlMjBzdHJlYW0lM0RUcnVlKS5yYXcpJTBBJTBBaW1hZ2VfcHJvY2Vzc29yJTIwJTNEJTIwQXV0b0ltYWdlUHJvY2Vzc29yLmZyb21fcHJldHJhaW5lZCglMjJtaWNyb3NvZnQlMkZjdnQtMTMlMjIpJTBBbW9kZWwlMjAlM0QlMjBURkN2dEZvckltYWdlQ2xhc3NpZmljYXRpb24uZnJvbV9wcmV0cmFpbmVkKCUyMm1pY3Jvc29mdCUyRmN2dC0xMyUyMiklMEElMEFpbnB1dHMlMjAlM0QlMjBpbWFnZV9wcm9jZXNzb3IoaW1hZ2VzJTNEaW1hZ2UlMkMlMjByZXR1cm5fdGVuc29ycyUzRCUyMnRmJTIyKSUwQW91dHB1dHMlMjAlM0QlMjBtb2RlbCgqKmlucHV0cyklMEFsb2dpdHMlMjAlM0QlMjBvdXRwdXRzLmxvZ2l0cyUwQSUyMyUyMG1vZGVsJTIwcHJlZGljdHMlMjBvbmUlMjBvZiUyMHRoZSUyMDEwMDAlMjBJbWFnZU5ldCUyMGNsYXNzZXMlMEFwcmVkaWN0ZWRfY2xhc3NfaWR4JTIwJTNEJTIwdGYubWF0aC5hcmdtYXgobG9naXRzJTJDJTIwYXhpcyUzRC0xKSU1QjAlNUQlMEFwcmludCglMjJQcmVkaWN0ZWQlMjBjbGFzcyUzQSUyMiUyQyUyMG1vZGVsLmNvbmZpZy5pZDJsYWJlbCU1QmludChwcmVkaWN0ZWRfY2xhc3NfaWR4KSU1RCk=",highlighted:`<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoImageProcessor, TFCvtForImageClassification
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">import</span> requests

<span class="hljs-meta">&gt;&gt;&gt; </span>url = <span class="hljs-string">&quot;http://images.cocodataset.org/val2017/000000039769.jpg&quot;</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)

<span class="hljs-meta">&gt;&gt;&gt; </span>image_processor = AutoImageProcessor.from_pretrained(<span class="hljs-string">&quot;microsoft/cvt-13&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>model = TFCvtForImageClassification.from_pretrained(<span class="hljs-string">&quot;microsoft/cvt-13&quot;</span>)

<span class="hljs-meta">&gt;&gt;&gt; </span>inputs = image_processor(images=image, return_tensors=<span class="hljs-string">&quot;tf&quot;</span>)
<span class="hljs-meta">&gt;&gt;&gt; </span>outputs = model(**inputs)
<span class="hljs-meta">&gt;&gt;&gt; </span>logits = outputs.logits
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-comment"># model predicts one of the 1000 ImageNet classes</span>
<span class="hljs-meta">&gt;&gt;&gt; </span>predicted_class_idx = tf.math.argmax(logits, axis=-<span class="hljs-number">1</span>)[<span class="hljs-number">0</span>]
<span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Predicted class:&quot;</span>, model.config.id2label[<span class="hljs-built_in">int</span>(predicted_class_idx)])`,wrap:!1}}),{c(){e=u("p"),e.textContent=d,s=l(),M(n.$$.fragment)},l(o){e=v(o,"P",{"data-svelte-h":!0}),x(e)!=="svelte-kvfsh7"&&(e.textContent=d),s=i(o),$(n.$$.fragment,o)},m(o,_){r(o,e,_),r(o,s,_),T(n,o,_),h=!0},p:fe,i(o){h||(y(n.$$.fragment,o),h=!0)},o(o){w(n.$$.fragment,o),h=!1},d(o){o&&(a(e),a(s)),j(n,o)}}}function Ae(F){let e,d,s,n,h,o,_="The bare Cvt Model transformer outputting raw hidden-states without any specific head on top.",q,p,Z=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,le,H,$e=`This model is also a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a> subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`,ie,J,he,U,R,se,X,N='The <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.TFCvtModel">TFCvtModel</a> forward method, overrides the <code>__call__</code> special method.',Y,V,ne,S,A,E,O,z,W,P,K,G=`Cvt Model transformer with an image classification head on top (a linear layer on top of the final hidden state of
the [CLS] token) e.g. for ImageNet.`,ve,ee,Q=`This model inherits from <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel">TFPreTrainedModel</a>. Check the superclass documentation for the generic methods the
library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads
etc.)`,ge,B,te=`This model is also a <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model" rel="nofollow">keras.Model</a> subclass. Use it
as a regular TF 2.0 Keras Model and refer to the TF 2.0 documentation for all matter related to general usage and
behavior.`,f,C,L,k,I,_e,ue,Ce='The <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.TFCvtForImageClassification">TFCvtForImageClassification</a> forward method, overrides the <code>__call__</code> special method.',Te,de,ce,ae,re;return e=new be({props:{title:"TFCvtModel",local:"transformers.TFCvtModel",headingTag:"h2"}}),n=new Me({props:{name:"class transformers.TFCvtModel",anchor:"transformers.TFCvtModel",parameters:[{name:"config",val:": CvtConfig"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFCvtModel.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig">CvtConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/cvt/modeling_tf_cvt.py#L927"}}),J=new ye({props:{$$slots:{default:[Qe]},$$scope:{ctx:F}}}),R=new Me({props:{name:"call",anchor:"transformers.TFCvtModel.call",parameters:[{name:"pixel_values",val:": tf.Tensor | None = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFCvtModel.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <code>CvtImageProcessor.__call__</code>
for details.`,name:"pixel_values"},{anchor:"transformers.TFCvtModel.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFCvtModel.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFCvtModel.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to \`False&#x201C;) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/cvt/modeling_tf_cvt.py#L937",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig"
>CvtConfig</a>) and inputs.</p>
<ul>
<li><strong>last_hidden_state</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, sequence_length, hidden_size)</code>) — Sequence of hidden-states at the output of the last layer of the model.</li>
<li><strong>cls_token_value</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, 1, hidden_size)</code>) — Classification token at the output of the last layer of the model.</li>
<li><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings + one for the output of each layer) of shape
<code>(batch_size, sequence_length, hidden_size)</code>. Hidden-states of the model at the output of each layer plus
the initial embedding outputs.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.models.cvt.modeling_tf_cvt.TFBaseModelOutputWithCLSToken</code> or <code>tuple(tf.Tensor)</code></p>
`}}),V=new ye({props:{$$slots:{default:[qe]},$$scope:{ctx:F}}}),S=new je({props:{anchor:"transformers.TFCvtModel.call.example",$$slots:{default:[Ge]},$$scope:{ctx:F}}}),E=new be({props:{title:"TFCvtForImageClassification",local:"transformers.TFCvtForImageClassification",headingTag:"h2"}}),W=new Me({props:{name:"class transformers.TFCvtForImageClassification",anchor:"transformers.TFCvtForImageClassification",parameters:[{name:"config",val:": CvtConfig"},{name:"*inputs",val:""},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.TFCvtForImageClassification.config",description:`<strong>config</strong> (<a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig">CvtConfig</a>) &#x2014; Model configuration class with all the parameters of the model.
Initializing with a config file does not load the weights associated with the model, only the
configuration. Check out the <a href="/docs/transformers/main/ja/main_classes/model#transformers.TFPreTrainedModel.from_pretrained">from_pretrained()</a> method to load the model weights.`,name:"config"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/cvt/modeling_tf_cvt.py#L996"}}),C=new ye({props:{$$slots:{default:[Xe]},$$scope:{ctx:F}}}),I=new Me({props:{name:"call",anchor:"transformers.TFCvtForImageClassification.call",parameters:[{name:"pixel_values",val:": tf.Tensor | None = None"},{name:"labels",val:": tf.Tensor | None = None"},{name:"output_hidden_states",val:": Optional[bool] = None"},{name:"return_dict",val:": Optional[bool] = None"},{name:"training",val:": Optional[bool] = False"}],parametersDescription:[{anchor:"transformers.TFCvtForImageClassification.call.pixel_values",description:`<strong>pixel_values</strong> (<code>np.ndarray</code>, <code>tf.Tensor</code>, <code>List[tf.Tensor]</code> \`<code>Dict[str, tf.Tensor]</code> or <code>Dict[str, np.ndarray]</code> and each example must have the shape <code>(batch_size, num_channels, height, width)</code>) &#x2014;
Pixel values. Pixel values can be obtained using <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a>. See <code>CvtImageProcessor.__call__</code>
for details.`,name:"pixel_values"},{anchor:"transformers.TFCvtForImageClassification.call.output_hidden_states",description:`<strong>output_hidden_states</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return the hidden states of all layers. See <code>hidden_states</code> under returned tensors for
more detail. This argument can be used only in eager mode, in graph mode the value in the config will be
used instead.`,name:"output_hidden_states"},{anchor:"transformers.TFCvtForImageClassification.call.return_dict",description:`<strong>return_dict</strong> (<code>bool</code>, <em>optional</em>) &#x2014;
Whether or not to return a <a href="/docs/transformers/main/ja/main_classes/output#transformers.utils.ModelOutput">ModelOutput</a> instead of a plain tuple. This argument can be used in
eager mode, in graph mode the value will always be set to True.`,name:"return_dict"},{anchor:"transformers.TFCvtForImageClassification.call.training",description:`<strong>training</strong> (<code>bool</code>, <em>optional</em>, defaults to \`False&#x201C;) &#x2014;
Whether or not to use the model in training mode (some modules like dropout modules have different
behaviors between training and evaluation).`,name:"training"},{anchor:"transformers.TFCvtForImageClassification.call.labels",description:`<strong>labels</strong> (<code>tf.Tensor</code> or <code>np.ndarray</code> of shape <code>(batch_size,)</code>, <em>optional</em>) &#x2014;
Labels for computing the image classification/regression loss. Indices should be in <code>[0, ..., config.num_labels - 1]</code>. If <code>config.num_labels == 1</code> a regression loss is computed (Mean-Square loss), If
<code>config.num_labels &gt; 1</code> a classification loss is computed (Cross-Entropy).`,name:"labels"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/cvt/modeling_tf_cvt.py#L1022",returnDescription:`<script context="module">export const metadata = 'undefined';<\/script>


<p>A <code>transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention</code> or a tuple of <code>tf.Tensor</code> (if
<code>return_dict=False</code> is passed or when <code>config.return_dict=False</code>) comprising various elements depending on the
configuration (<a
  href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtConfig"
>CvtConfig</a>) and inputs.</p>
<ul>
<li><strong>loss</strong> (<code>tf.Tensor</code> of shape <code>(1,)</code>, <em>optional</em>, returned when <code>labels</code> is provided) — Classification (or regression if config.num_labels==1) loss.</li>
<li><strong>logits</strong> (<code>tf.Tensor</code> of shape <code>(batch_size, config.num_labels)</code>) — Classification (or regression if config.num_labels==1) scores (before SoftMax).</li>
<li><strong>hidden_states</strong> (<code>tuple(tf.Tensor)</code>, <em>optional</em>, returned when <code>output_hidden_states=True</code> is passed or when <code>config.output_hidden_states=True</code>) — Tuple of <code>tf.Tensor</code> (one for the output of the embeddings, if the model has an embedding layer, + one for
the output of each stage) of shape <code>(batch_size, num_channels, height, width)</code>. Hidden-states (also called
feature maps) of the model at the output of each stage.</li>
</ul>
`,returnType:`<script context="module">export const metadata = 'undefined';<\/script>


<p><code>transformers.modeling_tf_outputs.TFImageClassifierOutputWithNoAttention</code> or <code>tuple(tf.Tensor)</code></p>
`}}),de=new ye({props:{$$slots:{default:[Ye]},$$scope:{ctx:F}}}),ae=new je({props:{anchor:"transformers.TFCvtForImageClassification.call.example",$$slots:{default:[Se]},$$scope:{ctx:F}}}),{c(){M(e.$$.fragment),d=l(),s=u("div"),M(n.$$.fragment),h=l(),o=u("p"),o.textContent=_,q=l(),p=u("p"),p.innerHTML=Z,le=l(),H=u("p"),H.innerHTML=$e,ie=l(),M(J.$$.fragment),he=l(),U=u("div"),M(R.$$.fragment),se=l(),X=u("p"),X.innerHTML=N,Y=l(),M(V.$$.fragment),ne=l(),M(S.$$.fragment),A=l(),M(E.$$.fragment),O=l(),z=u("div"),M(W.$$.fragment),P=l(),K=u("p"),K.textContent=G,ve=l(),ee=u("p"),ee.innerHTML=Q,ge=l(),B=u("p"),B.innerHTML=te,f=l(),M(C.$$.fragment),L=l(),k=u("div"),M(I.$$.fragment),_e=l(),ue=u("p"),ue.innerHTML=Ce,Te=l(),M(de.$$.fragment),ce=l(),M(ae.$$.fragment),this.h()},l(c){$(e.$$.fragment,c),d=i(c),s=v(c,"DIV",{class:!0});var b=me(s);$(n.$$.fragment,b),h=i(b),o=v(b,"P",{"data-svelte-h":!0}),x(o)!=="svelte-5yjryq"&&(o.textContent=_),q=i(b),p=v(b,"P",{"data-svelte-h":!0}),x(p)!=="svelte-x53t1u"&&(p.innerHTML=Z),le=i(b),H=v(b,"P",{"data-svelte-h":!0}),x(H)!=="svelte-1be7e3c"&&(H.innerHTML=$e),ie=i(b),$(J.$$.fragment,b),he=i(b),U=v(b,"DIV",{class:!0});var D=me(U);$(R.$$.fragment,D),se=i(D),X=v(D,"P",{"data-svelte-h":!0}),x(X)!=="svelte-175v0m6"&&(X.innerHTML=N),Y=i(D),$(V.$$.fragment,D),ne=i(D),$(S.$$.fragment,D),D.forEach(a),b.forEach(a),A=i(c),$(E.$$.fragment,c),O=i(c),z=v(c,"DIV",{class:!0});var t=me(z);$(W.$$.fragment,t),P=i(t),K=v(t,"P",{"data-svelte-h":!0}),x(K)!=="svelte-ztj7cy"&&(K.textContent=G),ve=i(t),ee=v(t,"P",{"data-svelte-h":!0}),x(ee)!=="svelte-x53t1u"&&(ee.innerHTML=Q),ge=i(t),B=v(t,"P",{"data-svelte-h":!0}),x(B)!=="svelte-1be7e3c"&&(B.innerHTML=te),f=i(t),$(C.$$.fragment,t),L=i(t),k=v(t,"DIV",{class:!0});var m=me(k);$(I.$$.fragment,m),_e=i(m),ue=v(m,"P",{"data-svelte-h":!0}),x(ue)!=="svelte-1ii6nkm"&&(ue.innerHTML=Ce),Te=i(m),$(de.$$.fragment,m),ce=i(m),$(ae.$$.fragment,m),m.forEach(a),t.forEach(a),this.h()},h(){pe(U,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),pe(s,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),pe(k,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8"),pe(z,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(c,b){T(e,c,b),r(c,d,b),r(c,s,b),T(n,s,null),g(s,h),g(s,o),g(s,q),g(s,p),g(s,le),g(s,H),g(s,ie),T(J,s,null),g(s,he),g(s,U),T(R,U,null),g(U,se),g(U,X),g(U,Y),T(V,U,null),g(U,ne),T(S,U,null),r(c,A,b),T(E,c,b),r(c,O,b),r(c,z,b),T(W,z,null),g(z,P),g(z,K),g(z,ve),g(z,ee),g(z,ge),g(z,B),g(z,f),T(C,z,null),g(z,L),g(z,k),T(I,k,null),g(k,_e),g(k,ue),g(k,Te),T(de,k,null),g(k,ce),T(ae,k,null),re=!0},p(c,b){const D={};b&2&&(D.$$scope={dirty:b,ctx:c}),J.$set(D);const t={};b&2&&(t.$$scope={dirty:b,ctx:c}),V.$set(t);const m={};b&2&&(m.$$scope={dirty:b,ctx:c}),S.$set(m);const oe={};b&2&&(oe.$$scope={dirty:b,ctx:c}),C.$set(oe);const we={};b&2&&(we.$$scope={dirty:b,ctx:c}),de.$set(we);const Fe={};b&2&&(Fe.$$scope={dirty:b,ctx:c}),ae.$set(Fe)},i(c){re||(y(e.$$.fragment,c),y(n.$$.fragment,c),y(J.$$.fragment,c),y(R.$$.fragment,c),y(V.$$.fragment,c),y(S.$$.fragment,c),y(E.$$.fragment,c),y(W.$$.fragment,c),y(C.$$.fragment,c),y(I.$$.fragment,c),y(de.$$.fragment,c),y(ae.$$.fragment,c),re=!0)},o(c){w(e.$$.fragment,c),w(n.$$.fragment,c),w(J.$$.fragment,c),w(R.$$.fragment,c),w(V.$$.fragment,c),w(S.$$.fragment,c),w(E.$$.fragment,c),w(W.$$.fragment,c),w(C.$$.fragment,c),w(I.$$.fragment,c),w(de.$$.fragment,c),w(ae.$$.fragment,c),re=!1},d(c){c&&(a(d),a(s),a(A),a(O),a(z)),j(e,c),j(n),j(J),j(R),j(V),j(S),j(E,c),j(W),j(C),j(I),j(de),j(ae)}}}function De(F){let e,d;return e=new ke({props:{$$slots:{default:[Ae]},$$scope:{ctx:F}}}),{c(){M(e.$$.fragment)},l(s){$(e.$$.fragment,s)},m(s,n){T(e,s,n),d=!0},p(s,n){const h={};n&2&&(h.$$scope={dirty:n,ctx:s}),e.$set(h)},i(s){d||(y(e.$$.fragment,s),d=!0)},o(s){w(e.$$.fragment,s),d=!1},d(s){j(e,s)}}}function Oe(F){let e,d,s,n,h,o,_,q,p,Z='CvT モデルは、Haping Wu、Bin Xiao、Noel Codella、Mengchen Liu、Xiyang Dai、Lu Yuan、Lei Zhang によって <a href="https://arxiv.org/abs/2103.15808" rel="nofollow">CvT: Introduction Convolutions to Vision Transformers</a> で提案されました。畳み込みビジョン トランスフォーマー (CvT) は、ViT に畳み込みを導入して両方の設計の長所を引き出すことにより、<a href="vit">ビジョン トランスフォーマー (ViT)</a> のパフォーマンスと効率を向上させます。',le,H,$e="論文の要約は次のとおりです。",ie,J,he=`<em>この論文では、ビジョン トランスフォーマー (ViT) を改善する、畳み込みビジョン トランスフォーマー (CvT) と呼ばれる新しいアーキテクチャを紹介します。
ViT に畳み込みを導入して両方の設計の長所を引き出すことで、パフォーマンスと効率を向上させます。これは次のようにして実現されます。
2 つの主要な変更: 新しい畳み込みトークンの埋め込みを含むトランスフォーマーの階層と、畳み込みトランスフォーマー
畳み込み射影を利用したブロック。これらの変更により、畳み込みニューラル ネットワーク (CNN) の望ましい特性が導入されます。
トランスフォーマーの利点 (動的な注意力、
グローバルなコンテキストとより良い一般化)。私たちは広範な実験を実施することで CvT を検証し、このアプローチが達成できることを示しています。
ImageNet-1k 上の他のビジョン トランスフォーマーや ResNet よりも、パラメータが少なく、FLOP が低い、最先端のパフォーマンスを実現します。加えて、
より大きなデータセット (例: ImageNet-22k) で事前トレーニングし、下流のタスクに合わせて微調整すると、パフォーマンスの向上が維持されます。事前トレーニング済み
ImageNet-22k、当社の CvT-W24 は、ImageNet-1k val set で 87.7\\% というトップ 1 の精度を獲得しています。最後に、私たちの結果は、位置エンコーディングが、
既存のビジョン トランスフォーマーの重要なコンポーネントであるこのコンポーネントは、モデルでは安全に削除できるため、高解像度のビジョン タスクの設計が簡素化されます。</em>`,U,R,se='このモデルは <a href="https://huggingface.co/anugunj" rel="nofollow">anugunj</a> によって提供されました。元のコードは <a href="https://github.com/microsoft/CvT" rel="nofollow">ここ</a> にあります。',X,N,Y,V,ne=`<li>CvT モデルは通常の Vision Transformer ですが、畳み込みでトレーニングされています。 ImageNet-1K および CIFAR-100 で微調整すると、<a href="vit">オリジナル モデル (ViT)</a> よりも優れたパフォーマンスを発揮します。</li> <li>カスタム データの微調整だけでなく推論に関するデモ ノートブックも <a href="https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer" rel="nofollow">ここ</a> で確認できます (<code>ViTFeatureExtractor を置き換えるだけで済みます)</code> による <a href="/docs/transformers/main/ja/model_doc/auto#transformers.AutoImageProcessor">AutoImageProcessor</a> および <code>ViTForImageClassification</code> による <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtForImageClassification">CvtForImageClassification</a>)。</li> <li>利用可能なチェックポイントは、(1) <a href="http://www.image-net.org/" rel="nofollow">ImageNet-22k</a> (1,400 万の画像と 22,000 のクラスのコレクション) でのみ事前トレーニングされている、(2) も問題ありません。 ImageNet-22k で調整、または (3) <a href="http://www.image-net.org/challenges/LSVRC/2012/" rel="nofollow">ImageNet-1k</a> (ILSVRC 2012 とも呼ばれるコレクション) でも微調整130万の
画像と 1,000 クラス)。</li>`,S,A,E,O,z="CvT を始めるのに役立つ公式 Hugging Face およびコミュニティ (🌎 で示される) リソースのリスト。",W,P,K,G,ve='<li><a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtForImageClassification">CvtForImageClassification</a> は、この <a href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-classification" rel="nofollow">サンプル スクリプト</a> および <a href="https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb" rel="nofollow">ノートブック</a>。</li> <li>参照: <a href="../tasks/image_classification">画像分類タスク ガイド</a></li>',ee,Q,ge="ここに含めるリソースの送信に興味がある場合は、お気軽にプル リクエストを開いてください。審査させていただきます。リソースは、既存のリソースを複製するのではなく、何か新しいものを示すことが理想的です。",B,te,f,C,L,k,I,_e=`This is the configuration class to store the configuration of a <a href="/docs/transformers/main/ja/model_doc/cvt#transformers.CvtModel">CvtModel</a>. It is used to instantiate a CvT model
according to the specified arguments, defining the model architecture. Instantiating a configuration with the
defaults will yield a similar configuration to that of the CvT
<a href="https://huggingface.co/microsoft/cvt-13" rel="nofollow">microsoft/cvt-13</a> architecture.`,ue,Ce,Te=`Configuration objects inherit from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> and can be used to control the model outputs. Read the
documentation from <a href="/docs/transformers/main/ja/main_classes/configuration#transformers.PretrainedConfig">PretrainedConfig</a> for more information.`,de,ce,ae,re,c,b,D;return h=new be({props:{title:"Convolutional Vision Transformer (CvT)",local:"convolutional-vision-transformer-cvt",headingTag:"h1"}}),_=new be({props:{title:"Overview",local:"overview",headingTag:"h2"}}),N=new be({props:{title:"Usage tips",local:"usage-tips",headingTag:"h2"}}),A=new be({props:{title:"Resources",local:"resources",headingTag:"h2"}}),P=new ze({props:{pipeline:"image-classification"}}),te=new be({props:{title:"CvtConfig",local:"transformers.CvtConfig",headingTag:"h2"}}),L=new Me({props:{name:"class transformers.CvtConfig",anchor:"transformers.CvtConfig",parameters:[{name:"num_channels",val:" = 3"},{name:"patch_sizes",val:" = [7, 3, 3]"},{name:"patch_stride",val:" = [4, 2, 2]"},{name:"patch_padding",val:" = [2, 1, 1]"},{name:"embed_dim",val:" = [64, 192, 384]"},{name:"num_heads",val:" = [1, 3, 6]"},{name:"depth",val:" = [1, 2, 10]"},{name:"mlp_ratio",val:" = [4.0, 4.0, 4.0]"},{name:"attention_drop_rate",val:" = [0.0, 0.0, 0.0]"},{name:"drop_rate",val:" = [0.0, 0.0, 0.0]"},{name:"drop_path_rate",val:" = [0.0, 0.0, 0.1]"},{name:"qkv_bias",val:" = [True, True, True]"},{name:"cls_token",val:" = [False, False, True]"},{name:"qkv_projection_method",val:" = ['dw_bn', 'dw_bn', 'dw_bn']"},{name:"kernel_qkv",val:" = [3, 3, 3]"},{name:"padding_kv",val:" = [1, 1, 1]"},{name:"stride_kv",val:" = [2, 2, 2]"},{name:"padding_q",val:" = [1, 1, 1]"},{name:"stride_q",val:" = [1, 1, 1]"},{name:"initializer_range",val:" = 0.02"},{name:"layer_norm_eps",val:" = 1e-12"},{name:"**kwargs",val:""}],parametersDescription:[{anchor:"transformers.CvtConfig.num_channels",description:`<strong>num_channels</strong> (<code>int</code>, <em>optional</em>, defaults to 3) &#x2014;
The number of input channels.`,name:"num_channels"},{anchor:"transformers.CvtConfig.patch_sizes",description:`<strong>patch_sizes</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[7, 3, 3]</code>) &#x2014;
The kernel size of each encoder&#x2019;s patch embedding.`,name:"patch_sizes"},{anchor:"transformers.CvtConfig.patch_stride",description:`<strong>patch_stride</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[4, 2, 2]</code>) &#x2014;
The stride size of each encoder&#x2019;s patch embedding.`,name:"patch_stride"},{anchor:"transformers.CvtConfig.patch_padding",description:`<strong>patch_padding</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[2, 1, 1]</code>) &#x2014;
The padding size of each encoder&#x2019;s patch embedding.`,name:"patch_padding"},{anchor:"transformers.CvtConfig.embed_dim",description:`<strong>embed_dim</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[64, 192, 384]</code>) &#x2014;
Dimension of each of the encoder blocks.`,name:"embed_dim"},{anchor:"transformers.CvtConfig.num_heads",description:`<strong>num_heads</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 3, 6]</code>) &#x2014;
Number of attention heads for each attention layer in each block of the Transformer encoder.`,name:"num_heads"},{anchor:"transformers.CvtConfig.depth",description:`<strong>depth</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 2, 10]</code>) &#x2014;
The number of layers in each encoder block.`,name:"depth"},{anchor:"transformers.CvtConfig.mlp_ratios",description:`<strong>mlp_ratios</strong> (<code>List[float]</code>, <em>optional</em>, defaults to <code>[4.0, 4.0, 4.0, 4.0]</code>) &#x2014;
Ratio of the size of the hidden layer compared to the size of the input layer of the Mix FFNs in the
encoder blocks.`,name:"mlp_ratios"},{anchor:"transformers.CvtConfig.attention_drop_rate",description:`<strong>attention_drop_rate</strong> (<code>List[float]</code>, <em>optional</em>, defaults to <code>[0.0, 0.0, 0.0]</code>) &#x2014;
The dropout ratio for the attention probabilities.`,name:"attention_drop_rate"},{anchor:"transformers.CvtConfig.drop_rate",description:`<strong>drop_rate</strong> (<code>List[float]</code>, <em>optional</em>, defaults to <code>[0.0, 0.0, 0.0]</code>) &#x2014;
The dropout ratio for the patch embeddings probabilities.`,name:"drop_rate"},{anchor:"transformers.CvtConfig.drop_path_rate",description:`<strong>drop_path_rate</strong> (<code>List[float]</code>, <em>optional</em>, defaults to <code>[0.0, 0.0, 0.1]</code>) &#x2014;
The dropout probability for stochastic depth, used in the blocks of the Transformer encoder.`,name:"drop_path_rate"},{anchor:"transformers.CvtConfig.qkv_bias",description:`<strong>qkv_bias</strong> (<code>List[bool]</code>, <em>optional</em>, defaults to <code>[True, True, True]</code>) &#x2014;
The bias bool for query, key and value in attentions`,name:"qkv_bias"},{anchor:"transformers.CvtConfig.cls_token",description:`<strong>cls_token</strong> (<code>List[bool]</code>, <em>optional</em>, defaults to <code>[False, False, True]</code>) &#x2014;
Whether or not to add a classification token to the output of each of the last 3 stages.`,name:"cls_token"},{anchor:"transformers.CvtConfig.qkv_projection_method",description:`<strong>qkv_projection_method</strong> (<code>List[string]</code>, <em>optional</em>, defaults to [&#x201C;dw_bn&#x201D;, &#x201C;dw_bn&#x201D;, &#x201C;dw_bn&#x201D;]\`) &#x2014;
The projection method for query, key and value Default is depth-wise convolutions with batch norm. For
Linear projection use &#x201C;avg&#x201D;.`,name:"qkv_projection_method"},{anchor:"transformers.CvtConfig.kernel_qkv",description:`<strong>kernel_qkv</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[3, 3, 3]</code>) &#x2014;
The kernel size for query, key and value in attention layer`,name:"kernel_qkv"},{anchor:"transformers.CvtConfig.padding_kv",description:`<strong>padding_kv</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 1, 1]</code>) &#x2014;
The padding size for key and value in attention layer`,name:"padding_kv"},{anchor:"transformers.CvtConfig.stride_kv",description:`<strong>stride_kv</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[2, 2, 2]</code>) &#x2014;
The stride size for key and value in attention layer`,name:"stride_kv"},{anchor:"transformers.CvtConfig.padding_q",description:`<strong>padding_q</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 1, 1]</code>) &#x2014;
The padding size for query in attention layer`,name:"padding_q"},{anchor:"transformers.CvtConfig.stride_q",description:`<strong>stride_q</strong> (<code>List[int]</code>, <em>optional</em>, defaults to <code>[1, 1, 1]</code>) &#x2014;
The stride size for query in attention layer`,name:"stride_q"},{anchor:"transformers.CvtConfig.initializer_range",description:`<strong>initializer_range</strong> (<code>float</code>, <em>optional</em>, defaults to 0.02) &#x2014;
The standard deviation of the truncated_normal_initializer for initializing all weight matrices.`,name:"initializer_range"},{anchor:"transformers.CvtConfig.layer_norm_eps",description:`<strong>layer_norm_eps</strong> (<code>float</code>, <em>optional</em>, defaults to 1e-6) &#x2014;
The epsilon used by the layer normalization layers.`,name:"layer_norm_eps"}],source:"https://github.com/QubitPi/huggingface-transformers/blob/main/src/transformers/models/cvt/configuration_cvt.py#L29"}}),ce=new je({props:{anchor:"transformers.CvtConfig.example",$$slots:{default:[Le]},$$scope:{ctx:F}}}),re=new Ne({props:{pytorch:!0,tensorflow:!0,jax:!1,$$slots:{tensorflow:[De],pytorch:[Pe]},$$scope:{ctx:F}}}),{c(){e=u("meta"),d=l(),s=u("p"),n=l(),M(h.$$.fragment),o=l(),M(_.$$.fragment),q=l(),p=u("p"),p.innerHTML=Z,le=l(),H=u("p"),H.textContent=$e,ie=l(),J=u("p"),J.innerHTML=he,U=l(),R=u("p"),R.innerHTML=se,X=l(),M(N.$$.fragment),Y=l(),V=u("ul"),V.innerHTML=ne,S=l(),M(A.$$.fragment),E=l(),O=u("p"),O.textContent=z,W=l(),M(P.$$.fragment),K=l(),G=u("ul"),G.innerHTML=ve,ee=l(),Q=u("p"),Q.textContent=ge,B=l(),M(te.$$.fragment),f=l(),C=u("div"),M(L.$$.fragment),k=l(),I=u("p"),I.innerHTML=_e,ue=l(),Ce=u("p"),Ce.innerHTML=Te,de=l(),M(ce.$$.fragment),ae=l(),M(re.$$.fragment),c=l(),b=u("p"),this.h()},l(t){const m=We("svelte-u9bgzb",document.head);e=v(m,"META",{name:!0,content:!0}),m.forEach(a),d=i(t),s=v(t,"P",{}),me(s).forEach(a),n=i(t),$(h.$$.fragment,t),o=i(t),$(_.$$.fragment,t),q=i(t),p=v(t,"P",{"data-svelte-h":!0}),x(p)!=="svelte-f3bmm3"&&(p.innerHTML=Z),le=i(t),H=v(t,"P",{"data-svelte-h":!0}),x(H)!=="svelte-1cv3nri"&&(H.textContent=$e),ie=i(t),J=v(t,"P",{"data-svelte-h":!0}),x(J)!=="svelte-vfb256"&&(J.innerHTML=he),U=i(t),R=v(t,"P",{"data-svelte-h":!0}),x(R)!=="svelte-1nuuuh6"&&(R.innerHTML=se),X=i(t),$(N.$$.fragment,t),Y=i(t),V=v(t,"UL",{"data-svelte-h":!0}),x(V)!=="svelte-13r5qdx"&&(V.innerHTML=ne),S=i(t),$(A.$$.fragment,t),E=i(t),O=v(t,"P",{"data-svelte-h":!0}),x(O)!=="svelte-1dr0h45"&&(O.textContent=z),W=i(t),$(P.$$.fragment,t),K=i(t),G=v(t,"UL",{"data-svelte-h":!0}),x(G)!=="svelte-1ggdqyk"&&(G.innerHTML=ve),ee=i(t),Q=v(t,"P",{"data-svelte-h":!0}),x(Q)!=="svelte-17ytafw"&&(Q.textContent=ge),B=i(t),$(te.$$.fragment,t),f=i(t),C=v(t,"DIV",{class:!0});var oe=me(C);$(L.$$.fragment,oe),k=i(oe),I=v(oe,"P",{"data-svelte-h":!0}),x(I)!=="svelte-dzr2y8"&&(I.innerHTML=_e),ue=i(oe),Ce=v(oe,"P",{"data-svelte-h":!0}),x(Ce)!=="svelte-1s6wgpv"&&(Ce.innerHTML=Te),de=i(oe),$(ce.$$.fragment,oe),oe.forEach(a),ae=i(t),$(re.$$.fragment,t),c=i(t),b=v(t,"P",{}),me(b).forEach(a),this.h()},h(){pe(e,"name","hf:doc:metadata"),pe(e,"content",Ke),pe(C,"class","docstring border-l-2 border-t-2 pl-4 pt-3.5 border-gray-100 rounded-tl-xl mb-6 mt-8")},m(t,m){g(document.head,e),r(t,d,m),r(t,s,m),r(t,n,m),T(h,t,m),r(t,o,m),T(_,t,m),r(t,q,m),r(t,p,m),r(t,le,m),r(t,H,m),r(t,ie,m),r(t,J,m),r(t,U,m),r(t,R,m),r(t,X,m),T(N,t,m),r(t,Y,m),r(t,V,m),r(t,S,m),T(A,t,m),r(t,E,m),r(t,O,m),r(t,W,m),T(P,t,m),r(t,K,m),r(t,G,m),r(t,ee,m),r(t,Q,m),r(t,B,m),T(te,t,m),r(t,f,m),r(t,C,m),T(L,C,null),g(C,k),g(C,I),g(C,ue),g(C,Ce),g(C,de),T(ce,C,null),r(t,ae,m),T(re,t,m),r(t,c,m),r(t,b,m),D=!0},p(t,[m]){const oe={};m&2&&(oe.$$scope={dirty:m,ctx:t}),ce.$set(oe);const we={};m&2&&(we.$$scope={dirty:m,ctx:t}),re.$set(we)},i(t){D||(y(h.$$.fragment,t),y(_.$$.fragment,t),y(N.$$.fragment,t),y(A.$$.fragment,t),y(P.$$.fragment,t),y(te.$$.fragment,t),y(L.$$.fragment,t),y(ce.$$.fragment,t),y(re.$$.fragment,t),D=!0)},o(t){w(h.$$.fragment,t),w(_.$$.fragment,t),w(N.$$.fragment,t),w(A.$$.fragment,t),w(P.$$.fragment,t),w(te.$$.fragment,t),w(L.$$.fragment,t),w(ce.$$.fragment,t),w(re.$$.fragment,t),D=!1},d(t){t&&(a(d),a(s),a(n),a(o),a(q),a(p),a(le),a(H),a(ie),a(J),a(U),a(R),a(X),a(Y),a(V),a(S),a(E),a(O),a(W),a(K),a(G),a(ee),a(Q),a(B),a(f),a(C),a(ae),a(c),a(b)),a(e),j(h,t),j(_,t),j(N,t),j(A,t),j(P,t),j(te,t),j(L),j(ce),j(re,t)}}}const Ke='{"title":"Convolutional Vision Transformer (CvT)","local":"convolutional-vision-transformer-cvt","sections":[{"title":"Overview","local":"overview","sections":[],"depth":2},{"title":"Usage tips","local":"usage-tips","sections":[],"depth":2},{"title":"Resources","local":"resources","sections":[],"depth":2},{"title":"CvtConfig","local":"transformers.CvtConfig","sections":[],"depth":2},{"title":"CvtModel","local":"transformers.CvtModel","sections":[],"depth":2},{"title":"CvtForImageClassification","local":"transformers.CvtForImageClassification","sections":[],"depth":2},{"title":"TFCvtModel","local":"transformers.TFCvtModel","sections":[],"depth":2},{"title":"TFCvtForImageClassification","local":"transformers.TFCvtForImageClassification","sections":[],"depth":2}],"depth":1}';function et(F){return Ie(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class ct extends Ze{constructor(e){super(),Je(this,e,et,Oe,Ue,{})}}export{ct as component};
