import{s as At,o as Pt,n as Et}from"../chunks/scheduler.9bc65507.js";import{S as Lt,i as Qt,g as s,s as r,r as f,A as Yt,h as m,f as l,c as a,j as Nt,u as c,x as i,k as zt,y as qt,a as n,v as d,d as u,t as M,w as b}from"../chunks/index.707bf1b6.js";import{T as Ft}from"../chunks/Tip.c2ecdbf4.js";import{C as q}from"../chunks/CodeBlock.54a9f38d.js";import{H as Q}from"../chunks/Heading.342b1fa6.js";function St(L){let o,T='注意: 複数のGPUセットアップは、<a href="./perf_infer_gpu_one">単一のGPUセクション</a>で説明されているほとんどの戦略を使用できます。ただし、より良い使用法のために使用できる簡単なテクニックについても認識しておく必要があります。';return{c(){o=s("p"),o.innerHTML=T},l(p){o=m(p,"P",{"data-svelte-h":!0}),i(o)!=="svelte-612xsh"&&(o.innerHTML=T)},m(p,y){n(p,o,y)},p:Et,d(p){p&&l(o)}}}function Dt(L){let o,T="Flash Attentionは、fp16またはbf16 dtypeを使用しているモデルにのみ使用できます。BetterTransformerを使用する前に、モデルを適切なdtypeにキャストしてください。";return{c(){o=s("p"),o.textContent=T},l(p){o=m(p,"P",{"data-svelte-h":!0}),i(o)!=="svelte-1kooxcx"&&(o.textContent=T)},m(p,y){n(p,o,y)},p:Et,d(p){p&&l(o)}}}function Kt(L){let o,T,p,y,w,S,$,Bt="この文書には、複数のGPUで効率的に推論を行う方法に関する情報が含まれています。",D,J,K,Z,O,B,Ut='Flash Attention 2の統合は、複数のGPUセットアップでも機能します。詳細については、<a href="./perf_infer_gpu_one#Flash-Attention-2">単一のGPUセクション</a>の適切なセクションをご覧ください。',tt,U,et,v,vt='<a href="https://huggingface.co/docs/optimum/bettertransformer/overview" rel="nofollow">BetterTransformer</a>は、🤗 TransformersモデルをPyTorchネイティブの高速実行パスを使用するように変換し、その下でFlash Attentionなどの最適化されたカーネルを呼び出します。',lt,_,_t="BetterTransformerは、テキスト、画像、音声モデルの単一GPUおよび複数GPUでの高速推論もサポートしています。",nt,h,rt,g,at,C,gt='テキストモデル、特にデコーダーベースのモデル（GPT、T5、Llamaなど）の場合、BetterTransformer APIはすべての注意操作を<a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention" rel="nofollow"><code>torch.nn.functional.scaled_dot_product_attention</code>オペレーター</a>（SDPA）を使用するように変換します。これはPyTorch 2.0以降でのみ使用可能です。',ot,G,Ct="モデルをBetterTransformerに変換するには：",st,W,mt,j,Gt='SDPAは、ハードウェアや問題のサイズなどの特定の設定で<a href="https://arxiv.org/abs/2205.14135" rel="nofollow">Flash Attention</a>カーネルを呼び出すこともできます。Flash Attentionを有効にするか、特定の設定（ハードウェア、問題のサイズ）で利用可能かを確認するには、<a href="https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel" rel="nofollow"><code>torch.backends.cuda.sdp_kernel</code></a>をコンテキストマネージャとして使用します。',pt,k,it,R,Wt="もしトレースバックで次のようなエラーメッセージが表示された場合：",ft,x,ct,V,jt="当日、Flash Attentionのカバレッジが広範囲である可能性があるPyTorch Nightlyバージョンを試すようにお勧めします。",dt,H,ut,I,kt='<a href="https://pytorch.org/blog/out-of-the-box-acceleration/" rel="nofollow">このブログ投稿</a>をチェックして、BetterTransformer + SDPA APIで可能なことについて詳しく学びましょう。',Mt,X,bt,N,Rt='推論中のエンコーダーモデルでは、BetterTransformerはエンコーダーレイヤーのforward呼び出しを、エンコーダーレイヤーの<a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html" rel="nofollow"><code>torch.nn.TransformerEncoderLayer</code></a>の相当するものにディスパッチします。これにより、エンコーダーレイヤーの高速実装が実行されます。',Tt,z,xt="<code>torch.nn.TransformerEncoderLayer</code>の高速実装はトレーニングをサポートしていないため、代わりに<code>torch.nn.functional.scaled_dot_product_attention</code>にディスパッチされます。これにより、ネストされたテンソルを活用しないFlash AttentionまたはMemory-Efficient Attentionの融合カーネルを使用できます。",yt,F,Vt='BetterTransformerのパフォーマンスの詳細については、この<a href="https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2" rel="nofollow">ブログ投稿</a>をご覧いただけます。また、エンコーダーモデル用のBetterTransformerについては、この<a href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/" rel="nofollow">ブログ</a>で詳しく学ぶことができます。',Jt,E,ht,A,Ht="モデルの最良のパフォーマンスを得るために、上記で説明した異なる方法を組み合わせることができます。例えば、FP4ミックスプレシジョン推論+Flash Attentionを使用したBetterTransformerを組み合わせることができます。",wt,P,$t,Y,Zt;return w=new Q({props:{title:"Efficient Inference on a Multiple GPUs",local:"efficient-inference-on-a-multiple-gpus",headingTag:"h1"}}),J=new Ft({props:{$$slots:{default:[St]},$$scope:{ctx:L}}}),Z=new Q({props:{title:"Flash Attention 2",local:"flash-attention-2",headingTag:"h2"}}),U=new Q({props:{title:"BetterTransformer",local:"bettertransformer",headingTag:"h2"}}),h=new Ft({props:{$$slots:{default:[Dt]},$$scope:{ctx:L}}}),g=new Q({props:{title:"Decoder models",local:"decoder-models",headingTag:"h3"}}),W=new q({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZvcHQtMzUwbSUyMiklMEElMjMlMjBjb252ZXJ0JTIwdGhlJTIwbW9kZWwlMjB0byUyMEJldHRlclRyYW5zZm9ybWVyJTBBbW9kZWwudG9fYmV0dGVydHJhbnNmb3JtZXIoKSUwQSUwQSUyMyUyMFVzZSUyMGl0JTIwZm9yJTIwdHJhaW5pbmclMjBvciUyMGluZmVyZW5jZQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>)
<span class="hljs-comment"># convert the model to BetterTransformer</span>
model.to_bettertransformer()

<span class="hljs-comment"># Use it for training or inference</span>`,wrap:!1}}),k=new q({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZvcHQtMzUwbSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm9wdC0zNTBtJTIyKS50byglMjJjdWRhJTIyKSUwQSUyMyUyMGNvbnZlcnQlMjB0aGUlMjBtb2RlbCUyMHRvJTIwQmV0dGVyVHJhbnNmb3JtZXIlMEFtb2RlbC50b19iZXR0ZXJ0cmFuc2Zvcm1lcigpJTBBJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMkhlbGxvJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjBhbmQlMjIlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoaW5wdXRfdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKCUyMmN1ZGElMjIpJTBBJTBBJTJCJTIwd2l0aCUyMHRvcmNoLmJhY2tlbmRzLmN1ZGEuc2RwX2tlcm5lbChlbmFibGVfZmxhc2glM0RUcnVlJTJDJTIwZW5hYmxlX21hdGglM0RGYWxzZSUyQyUyMGVuYWJsZV9tZW1fZWZmaWNpZW50JTNERmFsc2UpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0cyU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/opt-350m&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;facebook/opt-350m&quot;).to(&quot;cuda&quot;)
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = &quot;Hello my dog is cute and&quot;
inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)

<span class="hljs-addition">+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):</span>
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))`,wrap:!1}}),x=new q({props:{code:"UnVudGltZUVycm9yJTNBJTIwTm8lMjBhdmFpbGFibGUlMjBrZXJuZWwuJTIwJTIwQWJvcnRpbmclMjBleGVjdXRpb24u",highlighted:"RuntimeError: No available kernel.  Aborting execution.",wrap:!1}}),H=new q({props:{code:"cGlwMyUyMGluc3RhbGwlMjAtVSUyMC0tcHJlJTIwdG9yY2glMjB0b3JjaHZpc2lvbiUyMHRvcmNoYXVkaW8lMjAtLWluZGV4LXVybCUyMGh0dHBzJTNBJTJGJTJGZG93bmxvYWQucHl0b3JjaC5vcmclMkZ3aGwlMkZuaWdodGx5JTJGY3UxMTg=",highlighted:"pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118",wrap:!1}}),X=new Q({props:{title:"Encoder Models",local:"encoder-models",headingTag:"h3"}}),E=new Q({props:{title:"Advanced usage: mixing FP4 (or Int8) and BetterTransformer",local:"advanced-usage-mixing-fp4-or-int8-and-bettertransformer",headingTag:"h2"}}),P=new q({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyglMEElMjAlMjAlMjAlMjBsb2FkX2luXzRiaXQlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYm5iXzRiaXRfY29tcHV0ZV9kdHlwZSUzRHRvcmNoLmZsb2F0MTYlMEEpJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZvcHQtMzUwbSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm9wdC0zNTBtJTIyJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWcpJTBBJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMkhlbGxvJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjBhbmQlMjIlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoaW5wdXRfdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKCUyMmN1ZGElMjIpJTBBJTBBd2l0aCUyMHRvcmNoLmJhY2tlbmRzLmN1ZGEuc2RwX2tlcm5lbChlbmFibGVfZmxhc2glM0RUcnVlJTJDJTIwZW5hYmxlX21hdGglM0RGYWxzZSUyQyUyMGVuYWJsZV9tZW1fZWZmaWNpZW50JTNERmFsc2UpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0cyU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, quantization_config=quantization_config)

input_text = <span class="hljs-string">&quot;Hello my dog is cute and&quot;</span>
inputs = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-keyword">with</span> torch.backends.cuda.sdp_kernel(enable_flash=<span class="hljs-literal">True</span>, enable_math=<span class="hljs-literal">False</span>, enable_mem_efficient=<span class="hljs-literal">False</span>):
    outputs = model.generate(**inputs)

<span class="hljs-built_in">print</span>(tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){o=s("meta"),T=r(),p=s("p"),y=r(),f(w.$$.fragment),S=r(),$=s("p"),$.textContent=Bt,D=r(),f(J.$$.fragment),K=r(),f(Z.$$.fragment),O=r(),B=s("p"),B.innerHTML=Ut,tt=r(),f(U.$$.fragment),et=r(),v=s("p"),v.innerHTML=vt,lt=r(),_=s("p"),_.textContent=_t,nt=r(),f(h.$$.fragment),rt=r(),f(g.$$.fragment),at=r(),C=s("p"),C.innerHTML=gt,ot=r(),G=s("p"),G.textContent=Ct,st=r(),f(W.$$.fragment),mt=r(),j=s("p"),j.innerHTML=Gt,pt=r(),f(k.$$.fragment),it=r(),R=s("p"),R.textContent=Wt,ft=r(),f(x.$$.fragment),ct=r(),V=s("p"),V.textContent=jt,dt=r(),f(H.$$.fragment),ut=r(),I=s("p"),I.innerHTML=kt,Mt=r(),f(X.$$.fragment),bt=r(),N=s("p"),N.innerHTML=Rt,Tt=r(),z=s("p"),z.innerHTML=xt,yt=r(),F=s("p"),F.innerHTML=Vt,Jt=r(),f(E.$$.fragment),ht=r(),A=s("p"),A.textContent=Ht,wt=r(),f(P.$$.fragment),$t=r(),Y=s("p"),this.h()},l(t){const e=Yt("svelte-u9bgzb",document.head);o=m(e,"META",{name:!0,content:!0}),e.forEach(l),T=a(t),p=m(t,"P",{}),Nt(p).forEach(l),y=a(t),c(w.$$.fragment,t),S=a(t),$=m(t,"P",{"data-svelte-h":!0}),i($)!=="svelte-10hzl65"&&($.textContent=Bt),D=a(t),c(J.$$.fragment,t),K=a(t),c(Z.$$.fragment,t),O=a(t),B=m(t,"P",{"data-svelte-h":!0}),i(B)!=="svelte-127b4hj"&&(B.innerHTML=Ut),tt=a(t),c(U.$$.fragment,t),et=a(t),v=m(t,"P",{"data-svelte-h":!0}),i(v)!=="svelte-1iuxtx2"&&(v.innerHTML=vt),lt=a(t),_=m(t,"P",{"data-svelte-h":!0}),i(_)!=="svelte-xfb372"&&(_.textContent=_t),nt=a(t),c(h.$$.fragment,t),rt=a(t),c(g.$$.fragment,t),at=a(t),C=m(t,"P",{"data-svelte-h":!0}),i(C)!=="svelte-1gojc9b"&&(C.innerHTML=gt),ot=a(t),G=m(t,"P",{"data-svelte-h":!0}),i(G)!=="svelte-1mme928"&&(G.textContent=Ct),st=a(t),c(W.$$.fragment,t),mt=a(t),j=m(t,"P",{"data-svelte-h":!0}),i(j)!=="svelte-188y3vx"&&(j.innerHTML=Gt),pt=a(t),c(k.$$.fragment,t),it=a(t),R=m(t,"P",{"data-svelte-h":!0}),i(R)!=="svelte-198zu50"&&(R.textContent=Wt),ft=a(t),c(x.$$.fragment,t),ct=a(t),V=m(t,"P",{"data-svelte-h":!0}),i(V)!=="svelte-9agg8u"&&(V.textContent=jt),dt=a(t),c(H.$$.fragment,t),ut=a(t),I=m(t,"P",{"data-svelte-h":!0}),i(I)!=="svelte-gwrgf0"&&(I.innerHTML=kt),Mt=a(t),c(X.$$.fragment,t),bt=a(t),N=m(t,"P",{"data-svelte-h":!0}),i(N)!=="svelte-nvioyf"&&(N.innerHTML=Rt),Tt=a(t),z=m(t,"P",{"data-svelte-h":!0}),i(z)!=="svelte-10v3lf3"&&(z.innerHTML=xt),yt=a(t),F=m(t,"P",{"data-svelte-h":!0}),i(F)!=="svelte-1bfu1uk"&&(F.innerHTML=Vt),Jt=a(t),c(E.$$.fragment,t),ht=a(t),A=m(t,"P",{"data-svelte-h":!0}),i(A)!=="svelte-18z05f5"&&(A.textContent=Ht),wt=a(t),c(P.$$.fragment,t),$t=a(t),Y=m(t,"P",{}),Nt(Y).forEach(l),this.h()},h(){zt(o,"name","hf:doc:metadata"),zt(o,"content",Ot)},m(t,e){qt(document.head,o),n(t,T,e),n(t,p,e),n(t,y,e),d(w,t,e),n(t,S,e),n(t,$,e),n(t,D,e),d(J,t,e),n(t,K,e),d(Z,t,e),n(t,O,e),n(t,B,e),n(t,tt,e),d(U,t,e),n(t,et,e),n(t,v,e),n(t,lt,e),n(t,_,e),n(t,nt,e),d(h,t,e),n(t,rt,e),d(g,t,e),n(t,at,e),n(t,C,e),n(t,ot,e),n(t,G,e),n(t,st,e),d(W,t,e),n(t,mt,e),n(t,j,e),n(t,pt,e),d(k,t,e),n(t,it,e),n(t,R,e),n(t,ft,e),d(x,t,e),n(t,ct,e),n(t,V,e),n(t,dt,e),d(H,t,e),n(t,ut,e),n(t,I,e),n(t,Mt,e),d(X,t,e),n(t,bt,e),n(t,N,e),n(t,Tt,e),n(t,z,e),n(t,yt,e),n(t,F,e),n(t,Jt,e),d(E,t,e),n(t,ht,e),n(t,A,e),n(t,wt,e),d(P,t,e),n(t,$t,e),n(t,Y,e),Zt=!0},p(t,[e]){const It={};e&2&&(It.$$scope={dirty:e,ctx:t}),J.$set(It);const Xt={};e&2&&(Xt.$$scope={dirty:e,ctx:t}),h.$set(Xt)},i(t){Zt||(u(w.$$.fragment,t),u(J.$$.fragment,t),u(Z.$$.fragment,t),u(U.$$.fragment,t),u(h.$$.fragment,t),u(g.$$.fragment,t),u(W.$$.fragment,t),u(k.$$.fragment,t),u(x.$$.fragment,t),u(H.$$.fragment,t),u(X.$$.fragment,t),u(E.$$.fragment,t),u(P.$$.fragment,t),Zt=!0)},o(t){M(w.$$.fragment,t),M(J.$$.fragment,t),M(Z.$$.fragment,t),M(U.$$.fragment,t),M(h.$$.fragment,t),M(g.$$.fragment,t),M(W.$$.fragment,t),M(k.$$.fragment,t),M(x.$$.fragment,t),M(H.$$.fragment,t),M(X.$$.fragment,t),M(E.$$.fragment,t),M(P.$$.fragment,t),Zt=!1},d(t){t&&(l(T),l(p),l(y),l(S),l($),l(D),l(K),l(O),l(B),l(tt),l(et),l(v),l(lt),l(_),l(nt),l(rt),l(at),l(C),l(ot),l(G),l(st),l(mt),l(j),l(pt),l(it),l(R),l(ft),l(ct),l(V),l(dt),l(ut),l(I),l(Mt),l(bt),l(N),l(Tt),l(z),l(yt),l(F),l(Jt),l(ht),l(A),l(wt),l($t),l(Y)),l(o),b(w,t),b(J,t),b(Z,t),b(U,t),b(h,t),b(g,t),b(W,t),b(k,t),b(x,t),b(H,t),b(X,t),b(E,t),b(P,t)}}}const Ot='{"title":"Efficient Inference on a Multiple GPUs","local":"efficient-inference-on-a-multiple-gpus","sections":[{"title":"Flash Attention 2","local":"flash-attention-2","sections":[],"depth":2},{"title":"BetterTransformer","local":"bettertransformer","sections":[{"title":"Decoder models","local":"decoder-models","sections":[],"depth":3},{"title":"Encoder Models","local":"encoder-models","sections":[],"depth":3}],"depth":2},{"title":"Advanced usage: mixing FP4 (or Int8) and BetterTransformer","local":"advanced-usage-mixing-fp4-or-int8-and-bettertransformer","sections":[],"depth":2}],"depth":1}';function te(L){return Pt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class oe extends Lt{constructor(o){super(),Qt(this,o,te,Kt,At,{})}}export{oe as component};
