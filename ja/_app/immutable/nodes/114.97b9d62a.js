import{s as At,o as Pt,n as Et}from"../chunks/scheduler.9bc65507.js";import{S as Lt,i as Qt,g as s,s as r,r as f,A as Yt,h as m,f as l,c as a,j as Nt,u as c,x as i,k as zt,y as qt,a as n,v as d,d as u,t as M,w as b}from"../chunks/index.707bf1b6.js";import{T as Ft}from"../chunks/Tip.c2ecdbf4.js";import{C as q}from"../chunks/CodeBlock.54a9f38d.js";import{H as Q}from"../chunks/Heading.342b1fa6.js";function St(L){let o,T='æ³¨æ„: è¤‡æ•°ã®GPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯ã€<a href="./perf_infer_gpu_one">å˜ä¸€ã®GPUã‚»ã‚¯ã‚·ãƒ§ãƒ³</a>ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã»ã¨ã‚“ã©ã®æˆ¦ç•¥ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ãŸã ã—ã€ã‚ˆã‚Šè‰¯ã„ä½¿ç”¨æ³•ã®ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹ç°¡å˜ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã«ã¤ã„ã¦ã‚‚èªè­˜ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚';return{c(){o=s("p"),o.innerHTML=T},l(p){o=m(p,"P",{"data-svelte-h":!0}),i(o)!=="svelte-612xsh"&&(o.innerHTML=T)},m(p,y){n(p,o,y)},p:Et,d(p){p&&l(o)}}}function Dt(L){let o,T="Flash Attentionã¯ã€fp16ã¾ãŸã¯bf16 dtypeã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã«ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚BetterTransformerã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆ‡ãªdtypeã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚";return{c(){o=s("p"),o.textContent=T},l(p){o=m(p,"P",{"data-svelte-h":!0}),i(o)!=="svelte-1kooxcx"&&(o.textContent=T)},m(p,y){n(p,o,y)},p:Et,d(p){p&&l(o)}}}function Kt(L){let o,T,p,y,w,S,$,Bt="ã“ã®æ–‡æ›¸ã«ã¯ã€è¤‡æ•°ã®GPUã§åŠ¹ç‡çš„ã«æ¨è«–ã‚’è¡Œã†æ–¹æ³•ã«é–¢ã™ã‚‹æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚",D,J,K,Z,O,B,Ut='Flash Attention 2ã®çµ±åˆã¯ã€è¤‡æ•°ã®GPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã‚‚æ©Ÿèƒ½ã—ã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€<a href="./perf_infer_gpu_one#Flash-Attention-2">å˜ä¸€ã®GPUã‚»ã‚¯ã‚·ãƒ§ãƒ³</a>ã®é©åˆ‡ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã”è¦§ãã ã•ã„ã€‚',tt,U,et,v,vt='<a href="https://huggingface.co/docs/optimum/bettertransformer/overview" rel="nofollow">BetterTransformer</a>ã¯ã€ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã®é«˜é€Ÿå®Ÿè¡Œãƒ‘ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã€ãã®ä¸‹ã§Flash Attentionãªã©ã®æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚',lt,_,_t="BetterTransformerã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€GPUãŠã‚ˆã³è¤‡æ•°GPUã§ã®é«˜é€Ÿæ¨è«–ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚",nt,h,rt,g,at,C,gt='ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€ç‰¹ã«ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTã€T5ã€Llamaãªã©ï¼‰ã®å ´åˆã€BetterTransformer APIã¯ã™ã¹ã¦ã®æ³¨æ„æ“ä½œã‚’<a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention" rel="nofollow"><code>torch.nn.functional.scaled_dot_product_attention</code>ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼</a>ï¼ˆSDPAï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã‚Œã¯PyTorch 2.0ä»¥é™ã§ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚',ot,G,Ct="ãƒ¢ãƒ‡ãƒ«ã‚’BetterTransformerã«å¤‰æ›ã™ã‚‹ã«ã¯ï¼š",st,W,mt,j,Gt='SDPAã¯ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚„å•é¡Œã®ã‚µã‚¤ã‚ºãªã©ã®ç‰¹å®šã®è¨­å®šã§<a href="https://arxiv.org/abs/2205.14135" rel="nofollow">Flash Attention</a>ã‚«ãƒ¼ãƒãƒ«ã‚’å‘¼ã³å‡ºã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚Flash Attentionã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã€ç‰¹å®šã®è¨­å®šï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã€å•é¡Œã®ã‚µã‚¤ã‚ºï¼‰ã§åˆ©ç”¨å¯èƒ½ã‹ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€<a href="https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel" rel="nofollow"><code>torch.backends.cuda.sdp_kernel</code></a>ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚',pt,k,it,R,Wt="ã‚‚ã—ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã§æ¬¡ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚ŒãŸå ´åˆï¼š",ft,x,ct,V,jt="å½“æ—¥ã€Flash Attentionã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒåºƒç¯„å›²ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹PyTorch Nightlyãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è©¦ã™ã‚ˆã†ã«ãŠå‹§ã‚ã—ã¾ã™ã€‚",dt,H,ut,I,kt='<a href="https://pytorch.org/blog/out-of-the-box-acceleration/" rel="nofollow">ã“ã®ãƒ–ãƒ­ã‚°æŠ•ç¨¿</a>ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€BetterTransformer + SDPA APIã§å¯èƒ½ãªã“ã¨ã«ã¤ã„ã¦è©³ã—ãå­¦ã³ã¾ã—ã‚‡ã†ã€‚',Mt,X,bt,N,Rt='æ¨è«–ä¸­ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€BetterTransformerã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®forwardå‘¼ã³å‡ºã—ã‚’ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®<a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html" rel="nofollow"><code>torch.nn.TransformerEncoderLayer</code></a>ã®ç›¸å½“ã™ã‚‹ã‚‚ã®ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é«˜é€Ÿå®Ÿè£…ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚',Tt,z,xt="<code>torch.nn.TransformerEncoderLayer</code>ã®é«˜é€Ÿå®Ÿè£…ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã€ä»£ã‚ã‚Šã«<code>torch.nn.functional.scaled_dot_product_attention</code>ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’æ´»ç”¨ã—ãªã„Flash Attentionã¾ãŸã¯Memory-Efficient Attentionã®èåˆã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚",yt,F,Vt='BetterTransformerã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€ã“ã®<a href="https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2" rel="nofollow">ãƒ–ãƒ­ã‚°æŠ•ç¨¿</a>ã‚’ã”è¦§ã„ãŸã ã‘ã¾ã™ã€‚ã¾ãŸã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ç”¨ã®BetterTransformerã«ã¤ã„ã¦ã¯ã€ã“ã®<a href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/" rel="nofollow">ãƒ–ãƒ­ã‚°</a>ã§è©³ã—ãå­¦ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚',Jt,E,ht,A,Ht="ãƒ¢ãƒ‡ãƒ«ã®æœ€è‰¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ãŸã‚ã«ã€ä¸Šè¨˜ã§èª¬æ˜ã—ãŸç•°ãªã‚‹æ–¹æ³•ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€FP4ãƒŸãƒƒã‚¯ã‚¹ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³æ¨è«–+Flash Attentionã‚’ä½¿ç”¨ã—ãŸBetterTransformerã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚",wt,P,$t,Y,Zt;return w=new Q({props:{title:"Efficient Inference on a Multiple GPUs",local:"efficient-inference-on-a-multiple-gpus",headingTag:"h1"}}),J=new Ft({props:{$$slots:{default:[St]},$$scope:{ctx:L}}}),Z=new Q({props:{title:"Flash Attention 2",local:"flash-attention-2",headingTag:"h2"}}),U=new Q({props:{title:"BetterTransformer",local:"bettertransformer",headingTag:"h2"}}),h=new Ft({props:{$$slots:{default:[Dt]},$$scope:{ctx:L}}}),g=new Q({props:{title:"Decoder models",local:"decoder-models",headingTag:"h3"}}),W=new q({props:{code:"ZnJvbSUyMHRyYW5zZm9ybWVycyUyMGltcG9ydCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNJTBBJTBBbW9kZWwlMjAlM0QlMjBBdXRvTW9kZWxGb3JDYXVzYWxMTS5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZvcHQtMzUwbSUyMiklMEElMjMlMjBjb252ZXJ0JTIwdGhlJTIwbW9kZWwlMjB0byUyMEJldHRlclRyYW5zZm9ybWVyJTBBbW9kZWwudG9fYmV0dGVydHJhbnNmb3JtZXIoKSUwQSUwQSUyMyUyMFVzZSUyMGl0JTIwZm9yJTIwdHJhaW5pbmclMjBvciUyMGluZmVyZW5jZQ==",highlighted:`<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>)
<span class="hljs-comment"># convert the model to BetterTransformer</span>
model.to_bettertransformer()

<span class="hljs-comment"># Use it for training or inference</span>`,wrap:!1}}),k=new q({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZvcHQtMzUwbSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm9wdC0zNTBtJTIyKS50byglMjJjdWRhJTIyKSUwQSUyMyUyMGNvbnZlcnQlMjB0aGUlMjBtb2RlbCUyMHRvJTIwQmV0dGVyVHJhbnNmb3JtZXIlMEFtb2RlbC50b19iZXR0ZXJ0cmFuc2Zvcm1lcigpJTBBJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMkhlbGxvJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjBhbmQlMjIlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoaW5wdXRfdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKCUyMmN1ZGElMjIpJTBBJTBBJTJCJTIwd2l0aCUyMHRvcmNoLmJhY2tlbmRzLmN1ZGEuc2RwX2tlcm5lbChlbmFibGVfZmxhc2glM0RUcnVlJTJDJTIwZW5hYmxlX21hdGglM0RGYWxzZSUyQyUyMGVuYWJsZV9tZW1fZWZmaWNpZW50JTNERmFsc2UpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0cyU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/opt-350m&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;facebook/opt-350m&quot;).to(&quot;cuda&quot;)
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = &quot;Hello my dog is cute and&quot;
inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)

<span class="hljs-addition">+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):</span>
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))`,wrap:!1}}),x=new q({props:{code:"UnVudGltZUVycm9yJTNBJTIwTm8lMjBhdmFpbGFibGUlMjBrZXJuZWwuJTIwJTIwQWJvcnRpbmclMjBleGVjdXRpb24u",highlighted:"RuntimeError: No available kernel.  Aborting execution.",wrap:!1}}),H=new q({props:{code:"cGlwMyUyMGluc3RhbGwlMjAtVSUyMC0tcHJlJTIwdG9yY2glMjB0b3JjaHZpc2lvbiUyMHRvcmNoYXVkaW8lMjAtLWluZGV4LXVybCUyMGh0dHBzJTNBJTJGJTJGZG93bmxvYWQucHl0b3JjaC5vcmclMkZ3aGwlMkZuaWdodGx5JTJGY3UxMTg=",highlighted:"pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118",wrap:!1}}),X=new Q({props:{title:"Encoder Models",local:"encoder-models",headingTag:"h3"}}),E=new Q({props:{title:"Advanced usage: mixing FP4 (or Int8) and BetterTransformer",local:"advanced-usage-mixing-fp4-or-int8-and-bettertransformer",headingTag:"h2"}}),P=new q({props:{code:"aW1wb3J0JTIwdG9yY2glMEFmcm9tJTIwdHJhbnNmb3JtZXJzJTIwaW1wb3J0JTIwQXV0b01vZGVsRm9yQ2F1c2FsTE0lMkMlMjBBdXRvVG9rZW5pemVyJTJDJTIwQml0c0FuZEJ5dGVzQ29uZmlnJTBBJTBBcXVhbnRpemF0aW9uX2NvbmZpZyUyMCUzRCUyMEJpdHNBbmRCeXRlc0NvbmZpZyglMEElMjAlMjAlMjAlMjBsb2FkX2luXzRiaXQlM0RUcnVlJTJDJTBBJTIwJTIwJTIwJTIwYm5iXzRiaXRfY29tcHV0ZV9kdHlwZSUzRHRvcmNoLmZsb2F0MTYlMEEpJTBBJTBBdG9rZW5pemVyJTIwJTNEJTIwQXV0b1Rva2VuaXplci5mcm9tX3ByZXRyYWluZWQoJTIyZmFjZWJvb2slMkZvcHQtMzUwbSUyMiklMEFtb2RlbCUyMCUzRCUyMEF1dG9Nb2RlbEZvckNhdXNhbExNLmZyb21fcHJldHJhaW5lZCglMjJmYWNlYm9vayUyRm9wdC0zNTBtJTIyJTJDJTIwcXVhbnRpemF0aW9uX2NvbmZpZyUzRHF1YW50aXphdGlvbl9jb25maWcpJTBBJTBBaW5wdXRfdGV4dCUyMCUzRCUyMCUyMkhlbGxvJTIwbXklMjBkb2clMjBpcyUyMGN1dGUlMjBhbmQlMjIlMEFpbnB1dHMlMjAlM0QlMjB0b2tlbml6ZXIoaW5wdXRfdGV4dCUyQyUyMHJldHVybl90ZW5zb3JzJTNEJTIycHQlMjIpLnRvKCUyMmN1ZGElMjIpJTBBJTBBd2l0aCUyMHRvcmNoLmJhY2tlbmRzLmN1ZGEuc2RwX2tlcm5lbChlbmFibGVfZmxhc2glM0RUcnVlJTJDJTIwZW5hYmxlX21hdGglM0RGYWxzZSUyQyUyMGVuYWJsZV9tZW1fZWZmaWNpZW50JTNERmFsc2UpJTNBJTBBJTIwJTIwJTIwJTIwb3V0cHV0cyUyMCUzRCUyMG1vZGVsLmdlbmVyYXRlKCoqaW5wdXRzKSUwQSUwQXByaW50KHRva2VuaXplci5kZWNvZGUob3V0cHV0cyU1QjAlNUQlMkMlMjBza2lwX3NwZWNpYWxfdG9rZW5zJTNEVHJ1ZSkp",highlighted:`<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, quantization_config=quantization_config)

input_text = <span class="hljs-string">&quot;Hello my dog is cute and&quot;</span>
inputs = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-keyword">with</span> torch.backends.cuda.sdp_kernel(enable_flash=<span class="hljs-literal">True</span>, enable_math=<span class="hljs-literal">False</span>, enable_mem_efficient=<span class="hljs-literal">False</span>):
    outputs = model.generate(**inputs)

<span class="hljs-built_in">print</span>(tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))`,wrap:!1}}),{c(){o=s("meta"),T=r(),p=s("p"),y=r(),f(w.$$.fragment),S=r(),$=s("p"),$.textContent=Bt,D=r(),f(J.$$.fragment),K=r(),f(Z.$$.fragment),O=r(),B=s("p"),B.innerHTML=Ut,tt=r(),f(U.$$.fragment),et=r(),v=s("p"),v.innerHTML=vt,lt=r(),_=s("p"),_.textContent=_t,nt=r(),f(h.$$.fragment),rt=r(),f(g.$$.fragment),at=r(),C=s("p"),C.innerHTML=gt,ot=r(),G=s("p"),G.textContent=Ct,st=r(),f(W.$$.fragment),mt=r(),j=s("p"),j.innerHTML=Gt,pt=r(),f(k.$$.fragment),it=r(),R=s("p"),R.textContent=Wt,ft=r(),f(x.$$.fragment),ct=r(),V=s("p"),V.textContent=jt,dt=r(),f(H.$$.fragment),ut=r(),I=s("p"),I.innerHTML=kt,Mt=r(),f(X.$$.fragment),bt=r(),N=s("p"),N.innerHTML=Rt,Tt=r(),z=s("p"),z.innerHTML=xt,yt=r(),F=s("p"),F.innerHTML=Vt,Jt=r(),f(E.$$.fragment),ht=r(),A=s("p"),A.textContent=Ht,wt=r(),f(P.$$.fragment),$t=r(),Y=s("p"),this.h()},l(t){const e=Yt("svelte-u9bgzb",document.head);o=m(e,"META",{name:!0,content:!0}),e.forEach(l),T=a(t),p=m(t,"P",{}),Nt(p).forEach(l),y=a(t),c(w.$$.fragment,t),S=a(t),$=m(t,"P",{"data-svelte-h":!0}),i($)!=="svelte-10hzl65"&&($.textContent=Bt),D=a(t),c(J.$$.fragment,t),K=a(t),c(Z.$$.fragment,t),O=a(t),B=m(t,"P",{"data-svelte-h":!0}),i(B)!=="svelte-127b4hj"&&(B.innerHTML=Ut),tt=a(t),c(U.$$.fragment,t),et=a(t),v=m(t,"P",{"data-svelte-h":!0}),i(v)!=="svelte-1iuxtx2"&&(v.innerHTML=vt),lt=a(t),_=m(t,"P",{"data-svelte-h":!0}),i(_)!=="svelte-xfb372"&&(_.textContent=_t),nt=a(t),c(h.$$.fragment,t),rt=a(t),c(g.$$.fragment,t),at=a(t),C=m(t,"P",{"data-svelte-h":!0}),i(C)!=="svelte-1gojc9b"&&(C.innerHTML=gt),ot=a(t),G=m(t,"P",{"data-svelte-h":!0}),i(G)!=="svelte-1mme928"&&(G.textContent=Ct),st=a(t),c(W.$$.fragment,t),mt=a(t),j=m(t,"P",{"data-svelte-h":!0}),i(j)!=="svelte-188y3vx"&&(j.innerHTML=Gt),pt=a(t),c(k.$$.fragment,t),it=a(t),R=m(t,"P",{"data-svelte-h":!0}),i(R)!=="svelte-198zu50"&&(R.textContent=Wt),ft=a(t),c(x.$$.fragment,t),ct=a(t),V=m(t,"P",{"data-svelte-h":!0}),i(V)!=="svelte-9agg8u"&&(V.textContent=jt),dt=a(t),c(H.$$.fragment,t),ut=a(t),I=m(t,"P",{"data-svelte-h":!0}),i(I)!=="svelte-gwrgf0"&&(I.innerHTML=kt),Mt=a(t),c(X.$$.fragment,t),bt=a(t),N=m(t,"P",{"data-svelte-h":!0}),i(N)!=="svelte-nvioyf"&&(N.innerHTML=Rt),Tt=a(t),z=m(t,"P",{"data-svelte-h":!0}),i(z)!=="svelte-10v3lf3"&&(z.innerHTML=xt),yt=a(t),F=m(t,"P",{"data-svelte-h":!0}),i(F)!=="svelte-1bfu1uk"&&(F.innerHTML=Vt),Jt=a(t),c(E.$$.fragment,t),ht=a(t),A=m(t,"P",{"data-svelte-h":!0}),i(A)!=="svelte-18z05f5"&&(A.textContent=Ht),wt=a(t),c(P.$$.fragment,t),$t=a(t),Y=m(t,"P",{}),Nt(Y).forEach(l),this.h()},h(){zt(o,"name","hf:doc:metadata"),zt(o,"content",Ot)},m(t,e){qt(document.head,o),n(t,T,e),n(t,p,e),n(t,y,e),d(w,t,e),n(t,S,e),n(t,$,e),n(t,D,e),d(J,t,e),n(t,K,e),d(Z,t,e),n(t,O,e),n(t,B,e),n(t,tt,e),d(U,t,e),n(t,et,e),n(t,v,e),n(t,lt,e),n(t,_,e),n(t,nt,e),d(h,t,e),n(t,rt,e),d(g,t,e),n(t,at,e),n(t,C,e),n(t,ot,e),n(t,G,e),n(t,st,e),d(W,t,e),n(t,mt,e),n(t,j,e),n(t,pt,e),d(k,t,e),n(t,it,e),n(t,R,e),n(t,ft,e),d(x,t,e),n(t,ct,e),n(t,V,e),n(t,dt,e),d(H,t,e),n(t,ut,e),n(t,I,e),n(t,Mt,e),d(X,t,e),n(t,bt,e),n(t,N,e),n(t,Tt,e),n(t,z,e),n(t,yt,e),n(t,F,e),n(t,Jt,e),d(E,t,e),n(t,ht,e),n(t,A,e),n(t,wt,e),d(P,t,e),n(t,$t,e),n(t,Y,e),Zt=!0},p(t,[e]){const It={};e&2&&(It.$$scope={dirty:e,ctx:t}),J.$set(It);const Xt={};e&2&&(Xt.$$scope={dirty:e,ctx:t}),h.$set(Xt)},i(t){Zt||(u(w.$$.fragment,t),u(J.$$.fragment,t),u(Z.$$.fragment,t),u(U.$$.fragment,t),u(h.$$.fragment,t),u(g.$$.fragment,t),u(W.$$.fragment,t),u(k.$$.fragment,t),u(x.$$.fragment,t),u(H.$$.fragment,t),u(X.$$.fragment,t),u(E.$$.fragment,t),u(P.$$.fragment,t),Zt=!0)},o(t){M(w.$$.fragment,t),M(J.$$.fragment,t),M(Z.$$.fragment,t),M(U.$$.fragment,t),M(h.$$.fragment,t),M(g.$$.fragment,t),M(W.$$.fragment,t),M(k.$$.fragment,t),M(x.$$.fragment,t),M(H.$$.fragment,t),M(X.$$.fragment,t),M(E.$$.fragment,t),M(P.$$.fragment,t),Zt=!1},d(t){t&&(l(T),l(p),l(y),l(S),l($),l(D),l(K),l(O),l(B),l(tt),l(et),l(v),l(lt),l(_),l(nt),l(rt),l(at),l(C),l(ot),l(G),l(st),l(mt),l(j),l(pt),l(it),l(R),l(ft),l(ct),l(V),l(dt),l(ut),l(I),l(Mt),l(bt),l(N),l(Tt),l(z),l(yt),l(F),l(Jt),l(ht),l(A),l(wt),l($t),l(Y)),l(o),b(w,t),b(J,t),b(Z,t),b(U,t),b(h,t),b(g,t),b(W,t),b(k,t),b(x,t),b(H,t),b(X,t),b(E,t),b(P,t)}}}const Ot='{"title":"Efficient Inference on a Multiple GPUs","local":"efficient-inference-on-a-multiple-gpus","sections":[{"title":"Flash Attention 2","local":"flash-attention-2","sections":[],"depth":2},{"title":"BetterTransformer","local":"bettertransformer","sections":[{"title":"Decoder models","local":"decoder-models","sections":[],"depth":3},{"title":"Encoder Models","local":"encoder-models","sections":[],"depth":3}],"depth":2},{"title":"Advanced usage: mixing FP4 (or Int8) and BetterTransformer","local":"advanced-usage-mixing-fp4-or-int8-and-bettertransformer","sections":[],"depth":2}],"depth":1}';function te(L){return Pt(()=>{new URLSearchParams(window.location.search).get("fw")}),[]}class oe extends Lt{constructor(o){super(),Qt(this,o,te,Kt,At,{})}}export{oe as component};
