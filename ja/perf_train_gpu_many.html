<meta charset="utf-8" /><meta name="hf:doc:metadata" content="{&quot;title&quot;:&quot;Efficient Training on Multiple GPUs&quot;,&quot;local&quot;:&quot;efficient-training-on-multiple-gpus&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Concepts&quot;,&quot;local&quot;:&quot;concepts&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Scalability Strategy&quot;,&quot;local&quot;:&quot;scalability-strategy&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Data Parallelism&quot;,&quot;local&quot;:&quot;data-parallelism&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;DP vs DDP&quot;,&quot;local&quot;:&quot;dp-vs-ddp&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3}],&quot;depth&quot;:2},{&quot;title&quot;:&quot;ZeRO Data Parallelism&quot;,&quot;local&quot;:&quot;zero-data-parallelism&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Naive Model Parallelism (Vertical) and Pipeline Parallelism&quot;,&quot;local&quot;:&quot;naive-model-parallelism-vertical-and-pipeline-parallelism&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Tensor Parallelism&quot;,&quot;local&quot;:&quot;tensor-parallelism&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;DP+PP&quot;,&quot;local&quot;:&quot;dppp&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;DP+PP+TP&quot;,&quot;local&quot;:&quot;dppptp&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;ZeRO DP+PP+TP&quot;,&quot;local&quot;:&quot;zero-dppptp&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;FlexFlow&quot;,&quot;local&quot;:&quot;flexflow&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Which Strategy To Use When&quot;,&quot;local&quot;:&quot;which-strategy-to-use-when&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2}],&quot;depth&quot;:1}">
		<link href="/huggingface-transformers/en/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/entry/start.bc37dae2.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/scheduler.9bc65507.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/singletons.487085d6.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/index.3b203c72.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/paths.2a179143.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/entry/app.5e64da0e.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/index.707bf1b6.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/nodes/0.bd0f3500.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/each.e59479a4.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/nodes/120.e9d3cdc9.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/Tip.c2ecdbf4.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/CodeBlock.54a9f38d.js">
		<link rel="modulepreload" href="/huggingface-transformers/en/_app/immutable/chunks/Heading.342b1fa6.js"><!-- HEAD_svelte-u9bgzb_START --><meta name="hf:doc:metadata" content="{&quot;title&quot;:&quot;Efficient Training on Multiple GPUs&quot;,&quot;local&quot;:&quot;efficient-training-on-multiple-gpus&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Concepts&quot;,&quot;local&quot;:&quot;concepts&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Scalability Strategy&quot;,&quot;local&quot;:&quot;scalability-strategy&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Data Parallelism&quot;,&quot;local&quot;:&quot;data-parallelism&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;DP vs DDP&quot;,&quot;local&quot;:&quot;dp-vs-ddp&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3}],&quot;depth&quot;:2},{&quot;title&quot;:&quot;ZeRO Data Parallelism&quot;,&quot;local&quot;:&quot;zero-data-parallelism&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Naive Model Parallelism (Vertical) and Pipeline Parallelism&quot;,&quot;local&quot;:&quot;naive-model-parallelism-vertical-and-pipeline-parallelism&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Tensor Parallelism&quot;,&quot;local&quot;:&quot;tensor-parallelism&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;DP+PP&quot;,&quot;local&quot;:&quot;dppp&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;DP+PP+TP&quot;,&quot;local&quot;:&quot;dppptp&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;ZeRO DP+PP+TP&quot;,&quot;local&quot;:&quot;zero-dppptp&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;FlexFlow&quot;,&quot;local&quot;:&quot;flexflow&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Which Strategy To Use When&quot;,&quot;local&quot;:&quot;which-strategy-to-use-when&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2}],&quot;depth&quot;:1}"><!-- HEAD_svelte-u9bgzb_END -->      <p></p>   <h1 class="relative group"><a id="efficient-training-on-multiple-gpus" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#efficient-training-on-multiple-gpus"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Efficient Training on Multiple GPUs</span></h1> <p data-svelte-h="svelte-p00wl7">単一のGPUでのトレーニングが遅すぎる場合や、モデルの重みが単一のGPUのメモリに収まらない場合、複数のGPUを使用したセットアップが必要となります。単一のGPUから複数のGPUへの切り替えには、ワークロードを分散するためのある種の並列処理が必要です。データ、テンソル、またはパイプラインの並列処理など、さまざまな並列処理技術があります。ただし、すべてに適した一つの解決策は存在せず、最適な設定は使用するハードウェアに依存します。この記事は、おそらく他のフレームワークにも適用される主要な概念に焦点を当てつつ、PyTorchベースの実装に焦点を当てています。</p>  <div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400"><p data-svelte-h="svelte-178vvsa"><strong>注意</strong>: <a href="perf_train_gpu_one">単一GPUセクション</a> で紹介された多くの戦略（混合精度トレーニングや勾配蓄積など）は一般的であり、モデルのトレーニングに一般的に適用されます。したがって、マルチGPUやCPUトレーニングなどの次のセクションに入る前に、それを確認してください。</p></div> <p data-svelte-h="svelte-1kh3km3">まず、さまざまな1D並列処理技術とその利点および欠点について詳しく説明し、それらを2Dおよび3D並列処理に組み合わせてさらに高速なトレーニングを実現し、より大きなモデルをサポートする方法を検討します。さまざまな他の強力な代替手法も紹介されます。</p>  <h2 class="relative group"><a id="concepts" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#concepts"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Concepts</span></h2> <p data-svelte-h="svelte-3k4cjb">以下は、この文書で後で詳しく説明される主要な概念の簡単な説明です。</p> <ol data-svelte-h="svelte-19i5an3"><li><strong>DataParallel (DP)</strong> - 同じセットアップが複数回複製され、各セットアップにデータのスライスが供給されます。処理は並行して行われ、各セットアップはトレーニングステップの最後に同期されます。</li> <li><strong>TensorParallel (TP)</strong> - 各テンソルは複数のチャンクに分割され、単一のGPUにテンソル全体が存在するのではなく、テンソルの各シャードが指定されたGPUに存在します。処理中に、各シャードは別々に並行して処理され、異なるGPUで同期され、ステップの最後に結果が同期されます。これは水平並列処理と呼ばれるもので、分割は水平レベルで行われます。</li> <li><strong>PipelineParallel (PP)</strong> - モデルは垂直（レイヤーレベル）に複数のGPUに分割され、モデルの単一または複数のレイヤーが単一のGPUに配置されます。各GPUはパイプラインの異なるステージを並行して処理し、バッチの小さなチャンクで作業します。</li> <li><strong>Zero Redundancy Optimizer (ZeRO)</strong> - TPといくらか似たようなテンソルのシャーディングを実行しますが、前向きまたは後向きの計算のためにテンソル全体が再構築されるため、モデルを変更する必要はありません。また、GPUメモリが制限されている場合に補償するためのさまざまなオフロード技術をサポートします。</li> <li><strong>Sharded DDP</strong> - Sharded DDPは、さまざまなZeRO実装で使用される基本的なZeROコンセプトの別名です。</li></ol> <p data-svelte-h="svelte-1axfwuf">各コンセプトの詳細に深入りする前に、大規模なインフラストラクチャで大規模なモデルをトレーニングする際の大まかな決定プロセスを見てみましょう。</p>  <h2 class="relative group"><a id="scalability-strategy" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#scalability-strategy"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Scalability Strategy</span></h2> <p data-svelte-h="svelte-i7ves8"><strong>⇨ シングルノード / マルチGPU</strong></p> <ul data-svelte-h="svelte-1xp2u7g"><li><p>モデルが単一のGPUに収まる場合：</p> <ol><li>DDP - 分散データ並列</li> <li>ZeRO - 状況と使用される構成に応じて速いかどうかが異なります</li></ol></li> <li><p>モデルが単一のGPUに収まらない場合：</p> <ol><li><p>PP</p></li> <li><p>ZeRO</p></li> <li><p>TP</p> <p>非常に高速なノード内接続（NVLINKまたはNVSwitchなど）があれば、これらの3つはほぼ同じ速度になるはずで、これらがない場合、PPはTPまたはZeROよりも速くなります。TPの程度も差を生じるかもしれません。特定のセットアップでの勝者を見つけるために実験することが最善です。</p> <p>TPはほとんどの場合、単一ノード内で使用されます。つまり、TPサイズ &lt;= ノードごとのGPU数です。</p></li></ol></li> <li><p>最大のレイヤーが単一のGPUに収まらない場合：</p> <ol><li>ZeROを使用しない場合 - TPを使用する必要があります。PP単独では収まらないでしょう。</li> <li>ZeROを使用する場合 - “シングルGPU”のエントリと同じものを参照してください</li></ol></li></ul> <p data-svelte-h="svelte-17lwh0r"><strong>⇨ マルチノード / マルチGPU</strong></p> <ul data-svelte-h="svelte-jug9hc"><li><p>ノード間の高速接続がある場合：</p> <ol><li>ZeRO - モデルへのほとんどの変更が不要です</li> <li>PP+TP+DP - 通信が少なく、モデルへの大規模な変更が必要です</li></ol></li> <li><p>ノード間の接続が遅く、GPUメモリがまだ不足している場合：</p> <ol><li>DP+PP+TP+ZeRO-1</li></ol></li></ul>  <h2 class="relative group"><a id="data-parallelism" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#data-parallelism"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Data Parallelism</span></h2> <p data-svelte-h="svelte-5hcykd">2つのGPUを持つほとんどのユーザーは、<code>DataParallel</code>（DP）と<code>DistributedDataParallel</code>（DDP）によって提供されるトレーニング速度の向上をすでに享受しています。これらはほぼ自明に使用できるPyTorchの組み込み機能です。一般的に、すべてのモデルで動作するDDPを使用することをお勧めします。DPは一部のモデルで失敗する可能性があるためです。<a href="https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html" rel="nofollow">PyTorchのドキュメンテーション</a>自体もDDPの使用を推奨しています。</p>  <h3 class="relative group"><a id="dp-vs-ddp" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#dp-vs-ddp"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>DP vs DDP</span></h3> <p data-svelte-h="svelte-gheqny"><code>DistributedDataParallel</code>（DDP）は通常、<code>DataParallel</code>（DP）よりも高速ですが、常にそうとは限りません：</p> <ul data-svelte-h="svelte-1lsfqcs"><li>DPはPythonスレッドベースですが、DDPはマルチプロセスベースです。そのため、GIL（Global Interpreter Lock）などのPythonスレッドの制約がないためです。</li> <li>一方、GPUカード間の遅い相互接続性は、DDPの場合に実際には遅い結果をもたらす可能性があります。</li></ul> <p data-svelte-h="svelte-1pif46a">以下は、2つのモード間のGPU間通信の主な違いです：</p> <p data-svelte-h="svelte-1v8b0bi"><a href="https://pytorch.org/docs/master/notes/ddp.html" rel="nofollow">DDP</a>:</p> <ul data-svelte-h="svelte-uzzpkk"><li>開始時、メインプロセスはモデルをGPU 0から他のGPUに複製します。</li> <li>それから各バッチごとに:<ol><li>各GPUは各自のミニバッチのデータを直接消費します。</li> <li><code>backward</code>中、ローカル勾配が準備できると、それらはすべてのプロセスで平均化されます。</li></ol></li></ul> <p data-svelte-h="svelte-2za96j"><a href="https://pytorch.org/docs/master/generated/torch.nn.DataParallel.html" rel="nofollow">DP</a>:</p> <p data-svelte-h="svelte-1v6i5n3">各バッチごとに:</p> <ol data-svelte-h="svelte-vfltgt"><li>GPU 0はデータバッチを読み取り、それから各GPUにミニバッチを送信します。</li> <li>GPU 0から各GPUに最新のモデルを複製します。</li> <li><code>forward</code>を実行し、各GPUからGPU 0に出力を送信し、損失を計算します。</li> <li>GPU 0からすべてのGPUに損失を分散し、<code>backward</code>を実行します。</li> <li>各GPUからGPU 0に勾配を送信し、それらを平均化します。</li></ol> <p data-svelte-h="svelte-130pc3o">DDPはバッチごとに行う通信は勾配の送信のみであり、一方、DPはバッチごとに5つの異なるデータ交換を行います。</p> <p data-svelte-h="svelte-23i2i2">DPはプロセス内でデータをPythonスレッドを介してコピーしますが、DDPは<a href="https://pytorch.org/docs/master/distributed.html" rel="nofollow">torch.distributed</a>を介してデータをコピーします。</p> <p data-svelte-h="svelte-ufyyk6">DPではGPU 0は他のGPUよりもはるかに多くの作業を行うため、GPUの未使用率が高くなります。</p> <p data-svelte-h="svelte-1gyvr2l">DDPは複数のマシン間で使用できますが、DPの場合はそうではありません。</p> <p data-svelte-h="svelte-1nvxoat">DPとDDPの他にも違いがありますが、この議論には関係ありません。</p> <p data-svelte-h="svelte-ndepvz">これら2つのモードを深く理解したい場合、この<a href="https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/" rel="nofollow">記事</a>を強くお勧めします。素晴らしいダイアグラムを含み、さまざまなハードウェアでの複数のベンチマークとプロファイラの出力を示し、知っておく必要があるすべての微妙なニュアンスを説明しています。</p> <p data-svelte-h="svelte-hpqigq">実際のベンチマークを見てみましょう：</p> <table data-svelte-h="svelte-1glyygb"><thead><tr><th align="left">Type</th> <th>NVlink</th> <th align="right">Time</th></tr></thead> <tbody><tr><td align="left">2:DP</td> <td>Y</td> <td align="right">110s</td></tr> <tr><td align="left">2:DDP</td> <td>Y</td> <td align="right">101s</td></tr> <tr><td align="left">2:DDP</td> <td>N</td> <td align="right">131s</td></tr></tbody></table> <p data-svelte-h="svelte-j7a5ab">解析：</p> <p data-svelte-h="svelte-v4b9q2">ここで、DPはNVlinkを使用したDDPに比べて約10％遅く、NVlinkを使用しないDDPに比べて約15％高速であることが示されています。</p> <p data-svelte-h="svelte-117kghy">実際の違いは、各GPUが他のGPUと同期する必要があるデータの量に依存します。同期するデータが多いほど、遅いリンクが合計の実行時間を遅くする可能性が高くなります。</p> <p data-svelte-h="svelte-188felc">以下は完全なベンチマークコードと出力です：</p> <p data-svelte-h="svelte-q5i4nx"><code>NCCL_P2P_DISABLE=1</code>を使用して、対応するベンチマークでNVLink機能を無効にしました。</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START -->
<span class="hljs-comment"># DP</span>
<span class="hljs-built_in">rm</span> -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \
python examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path openai-community/gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: 110.5948, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: 1.808, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.69}

<span class="hljs-comment"># DDP w/ NVlink</span>
<span class="hljs-built_in">rm</span> -r /tmp/test-clm; CUDA_VISIBLE_DEVICES=0,1 \
torchrun --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path openai-community/gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: 101.9003, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: 1.963, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.69}

<span class="hljs-comment"># DDP w/o NVlink</span>
<span class="hljs-built_in">rm</span> -r /tmp/test-clm; NCCL_P2P_DISABLE=1 CUDA_VISIBLE_DEVICES=0,1 \
torchrun --nproc_per_node 2 examples/pytorch/language-modeling/run_clm.py \
--model_name_or_path openai-community/gpt2 --dataset_name wikitext --dataset_config_name wikitext-2-raw-v1 \
--do_train --output_dir /tmp/test-clm --per_device_train_batch_size 4 --max_steps 200

{<span class="hljs-string">&#x27;train_runtime&#x27;</span>: 131.4367, <span class="hljs-string">&#x27;train_samples_per_second&#x27;</span>: 1.522, <span class="hljs-string">&#x27;epoch&#x27;</span>: 0.69}<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-lu79sn">ハードウェア: 2x TITAN RTX、各24GB + 2つのNVLink（<code>nvidia-smi topo -m</code>で <code>NV2</code>）</p> <p data-svelte-h="svelte-18h87eb">ソフトウェア: <code>pytorch-1.8-to-be</code> + <code>cuda-11.0</code> / <code>transformers==4.3.0.dev0</code></p>  <h2 class="relative group"><a id="zero-data-parallelism" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#zero-data-parallelism"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>ZeRO Data Parallelism</span></h2> <p data-svelte-h="svelte-juqsxg">ZeROパワードデータ並列処理（ZeRO-DP）は、次の<a href="https://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/" rel="nofollow">ブログ投稿</a>のダイアグラムで説明されています。
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero.png" alt="DeepSpeed-Image-1"></p> <p data-svelte-h="svelte-1i0mdv7">これは理解が難しいかもしれませんが、実際にはこの概念は非常にシンプルです。これは通常の<code>DataParallel</code>（DP）ですが、完全なモデルパラメータ、勾配、およびオプティマイザの状態を複製する代わりに、各GPUはそれぞれのスライスのみを保存します。そして、実行時に、特定のレイヤーに必要な完全なレイヤーパラメータが必要な場合、すべてのGPUが同期して、お互いに不足している部分を提供します。それがすべてです。</p> <p data-svelte-h="svelte-1wipobe">3つのレイヤーからなる単純なモデルを考えてみましょう。各レイヤーには3つのパラメータがあります：</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START -->La |<span class="hljs-string"> Lb </span>|<span class="hljs-string"> Lc
---</span>|<span class="hljs-string">----</span>|<span class="hljs-string">---
a0 </span>|<span class="hljs-string"> b0 </span>|<span class="hljs-string"> c0
a1 </span>|<span class="hljs-string"> b1 </span>|<span class="hljs-string"> c1
a2 </span>|<span class="hljs-string"> b2 </span>|<span class="hljs-string"> c2</span><!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-3cvmfq">レイヤーLaには、重みa0、a1、およびa2があります。</p> <p data-svelte-h="svelte-1yhjowm">3つのGPUがある場合、Sharded DDP（= Zero-DP）はモデルを3つのGPUに次のように分割します：</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START -->GPU0:
La |<span class="hljs-string"> Lb </span>|<span class="hljs-string"> Lc
---</span>|<span class="hljs-string">----</span>|<span class="hljs-string">---
a0 </span>|<span class="hljs-string"> b0 </span>|<span class="hljs-string"> c0

GPU1:
La </span>|<span class="hljs-string"> Lb </span>|<span class="hljs-string"> Lc
---</span>|<span class="hljs-string">----</span>|<span class="hljs-string">---
a1 </span>|<span class="hljs-string"> b1 </span>|<span class="hljs-string"> c1

GPU2:
La </span>|<span class="hljs-string"> Lb </span>|<span class="hljs-string"> Lc
---</span>|<span class="hljs-string">----</span>|<span class="hljs-string">---
a2 </span>|<span class="hljs-string"> b2 </span>|<span class="hljs-string"> c2</span><!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-103bb0r">これは、典型的なディープニューラルネットワーク（DNN）のダイアグラムを想像すると、テンソル並列処理と同様の水平スライスであるようなものです。垂直スライスは、異なるGPUに完全な層グループを配置する方法です。しかし、これは単なる出発点に過ぎません。</p> <p data-svelte-h="svelte-12w85z6">これから、各GPUは通常のデータ並列処理（DP）と同様に、通常のミニバッチを受け取ります：</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-attribute">x0</span> <span class="hljs-operator">=</span>&gt; GPU0
<span class="hljs-attribute">x1</span> <span class="hljs-operator">=</span>&gt; GPU1
<span class="hljs-attribute">x2</span> <span class="hljs-operator">=</span>&gt; GPU2<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-ekaxx1">最初に、入力データはレイヤーLaに適用されます。</p> <p data-svelte-h="svelte-iccih7">GPU0に焦点を当てましょう：x0は、その前向きパスを実行するためにa0、a1、a2のパラメータが必要ですが、GPU0にはa0しかありません。GPU1からa1を、GPU2からa2を受け取り、モデルの各部分をまとめます。</p> <p data-svelte-h="svelte-7uyuw7">同様に、GPU1はミニバッチx1を受け取り、a1しか持っていませんが、a0とa2のパラメータが必要です。これらはGPU0とGPU2から取得します。</p> <p data-svelte-h="svelte-i9uex1">GPU2もx2を受け取ります。a0とa1はGPU0とGPU1から受け取り、a2とともに完全なテンソルを再構築します。</p> <p data-svelte-h="svelte-19oov4o">3つのGPUは完全なテンソルを再構築し、前向き計算が行われます。</p> <p data-svelte-h="svelte-p97sj8">計算が完了すると、不要になったデータは削除されます。計算中だけ使用され、再構築は事前にフェッチを使用して効率的に行われます。</p> <p data-svelte-h="svelte-5ocmnf">そして、このプロセス全体がレイヤーLb、次に前向きでLc、そして逆方向でLc -&gt; Lb -&gt; Laに対して繰り返されます。</p> <p data-svelte-h="svelte-1n2597e">私にとって、これは効率的なグループでの重みの分散戦略のように聞こえます：</p> <ol data-svelte-h="svelte-1xfpr0y"><li>人Aはテントを持っています。</li> <li>人Bはストーブを持っています。</li> <li>人Cは斧を持っています。</li></ol> <p data-svelte-h="svelte-l6mji4">今、彼らは毎晩持っているものを共有し、他の人から持っていないものをもらい、朝には割り当てられたタイプのギアを詰めて旅を続けます。これがSharded DDP / Zero DPです。</p> <p data-svelte-h="svelte-1rje20u">この戦略を、各人が独自のテント、ストーブ、斧を持って運ばなければならないシンプルな戦略と比較してみてください。これがPyTorchのDataParallel（DPおよびDDP）です。</p> <p data-svelte-h="svelte-104xoaf">このトピックの文献を読む際に、以下の類義語に出会うかもしれません：Sharded、Partitioned。</p> <p data-svelte-h="svelte-1rxc69e">ZeROがモデルの重みを分割する方法に注意を払うと、これはテンソルパラレリズムと非常に似ているように見えます。これは後で議論される垂直モデルパラレリズムとは異なり、各レイヤーの重みをパーティション/シャーディングします。</p> <p data-svelte-h="svelte-1b2mzqh">Implementations:</p> <ul data-svelte-h="svelte-1kdc4z9"><li><a href="https://www.deepspeed.ai/tutorials/zero/" rel="nofollow">DeepSpeed</a> ZeRO-DP stages 1+2+3</li> <li><a href="main_classes/trainer#trainer-integrations"><code>transformers</code> integration</a></li></ul>  <h2 class="relative group"><a id="naive-model-parallelism-vertical-and-pipeline-parallelism" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#naive-model-parallelism-vertical-and-pipeline-parallelism"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Naive Model Parallelism (Vertical) and Pipeline Parallelism</span></h2> <p data-svelte-h="svelte-1knls5m">ナイーブモデルパラレリズム（MP）は、モデルの層を複数のGPUに分散させる方法です。このメカニズムは比較的単純で、希望する層を<code>.to()</code>メソッドを使用して特定のデバイスに切り替えるだけです。これにより、データがこれらの層を通過するたびに、データも層と同じデバイスに切り替えられ、残りの部分は変更されません。</p> <p data-svelte-h="svelte-1tk8x6l">私たちはこれを「垂直MP」と呼びます。なぜなら、ほとんどのモデルがどのように描かれるかを思い出すと、層を垂直にスライスするからです。たとえば、以下の図は8層のモデルを示しています：</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START -->===================  ===================
|<span class="hljs-string">  0 </span>|<span class="hljs-string"> 1 </span>|<span class="hljs-string"> 2 </span>|<span class="hljs-string"> 3  </span>|<span class="hljs-string">  </span>|<span class="hljs-string">  4 </span>|<span class="hljs-string"> 5 </span>|<span class="hljs-string"> 6 </span>|<span class="hljs-string"> 7  </span>|
===================  ===================
        gpu0                 gpu1<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-tc8d5e">我々は、モデルを垂直に2つに分割し、レイヤー0から3をGPU0に配置し、レイヤー4から7をGPU1に配置しました。</p> <p data-svelte-h="svelte-1bhur3f">データがレイヤー0から1、1から2、2から3に移動する間は通常のモデルと同じです。しかし、データがレイヤー3からレイヤー4に移動する必要がある場合、GPU0からGPU1への移動が発生し、通信のオーバーヘッドが発生します。参加しているGPUが同じコンピュートノード（例：同じ物理マシン）にある場合、このコピーは非常に高速ですが、異なるコンピュートノード（例：複数のマシン）にある場合、通信のオーバーヘッドは大幅に増加する可能性があります。</p> <p data-svelte-h="svelte-1phbvvb">その後、レイヤー4から5、6から7までは通常のモデルと同様に動作し、7番目のレイヤーが完了すると、データをしばしばレイヤー0に戻す必要があります（またはラベルを最後のレイヤーに送信します）。これで損失を計算し、オプティマイザが作業を開始できます。</p> <p data-svelte-h="svelte-1w5dvkg">問題点：</p> <ul data-svelte-h="svelte-1ks7vrz"><li>主な欠点、およびなぜこれを「単純な」MPと呼ぶのかは、1つを除いてすべてのGPUがどんな瞬間でもアイドル状態であることです。したがって、4つのGPUを使用する場合、単純なMPは、1つのGPUのメモリ容量を4倍にするのとほぼ同じであり、ハードウェアの残りを無視します。さらに、データのコピーのオーバーヘッドがあることを忘れてはいけません。したがって、4枚の6GBのカードは、データのコピーのオーバーヘッドがない1枚の24GBのカードと同じサイズを収容できるでしょうが、後者はトレーニングをより迅速に完了します。ただし、たとえば40GBのカードがあり、45GBのモデルを収める必要がある場合、勾配とオプティマイザの状態のためにほとんど収めることができません。</li> <li>共有の埋め込みは、GPU間でコピーする必要があるかもしれません。</li></ul> <p data-svelte-h="svelte-bj9kti">パイプライン並列処理（PP）は、ほぼ単純なMPと同じですが、GPUがアイドル状態になる問題を解決し、入力バッチをマイクロバッチに分割し、パイプラインを人工的に作成することにより、異なるGPUが計算プロセスに同時に参加できるようにします。</p> <p data-svelte-h="svelte-edjnsa">以下は、<a href="https://ai.googleblog.com/2019/03/introducing-gpipe-open-source-library.html" rel="nofollow">GPipe論文</a>からの図で、上部には単純なMP、下部にはPPが示されています：</p> <p data-svelte-h="svelte-1q7hzge"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-gpipe-bubble.png" alt="mp-pp"></p> <p data-svelte-h="svelte-8fnl63">この図から、PPがGPUがアイドル状態の領域である「バブル」を少なく持つことがわかります。アイドル状態の部分は「バブル」と呼ばれます。</p> <p data-svelte-h="svelte-1v2yyww">図の両方の部分は、4つのGPUがパイプラインに参加している4の次元の並列性を示しています。つまり、4つのパイプステージF0、F1、F2、F3のフォワードパスがあり、逆順のバックワードパスB3、B2、B1、B0があります。</p> <p data-svelte-h="svelte-17bhy87">PPは調整する新しいハイパーパラメータを導入します。それは <code>chunks</code> で、同じパイプステージを通じて連続して送信されるデータのチャンクの数を定義します。たとえば、下の図では <code>chunks=4</code> が表示されています。GPU0はチャンク0、1、2、3（F0,0、F0,1、F0,2、F0,3）で同じフォワードパスを実行し、他のGPUが作業を開始し始めるのを待ってから、GPU0はチャンク3、2、1、0（B0,3、B0,2、B0,1、B0,0）で逆順パスを実行します。</p> <p data-svelte-h="svelte-1n6ze23">注意すべきは、概念的にはこれが勾配蓄積ステップ（GAS）と同じコンセプトであることです。PyTorchは <code>chunks</code> を使用し、DeepSpeedは同じハイパーパラメータをGASと呼びます。</p> <p data-svelte-h="svelte-2frj83"><code>chunks</code> の導入により、PPはマイクロバッチ（MBS）の概念を導入します。DPはグローバルデータバッチサイズをミニバッチに分割します。したがって、DPの次数が4で、グローバルバッチサイズが1024の場合、4つのミニバッチ（それぞれ256）に分割されます（1024/4）。そして、<code>chunks</code>（またはGAS）の数が32である場合、マイクロバッチサイズは8になります（256/32）。各パイプラインステージは1つのマイクロバッチで作業します。</p> <p data-svelte-h="svelte-155mvto">DP + PPセットアップのグローバルバッチサイズを計算するには、<code>mbs*chunks*dp_degree</code>（<code>8*32*4=1024</code>）を行います。</p> <p data-svelte-h="svelte-165iud9">図に戻りましょう。</p> <p data-svelte-h="svelte-1r4hea4"><code>chunks=1</code> であれば、非効率な単純なMPになります。非常に大きな <code>chunks</code> 値を使用すると、非常に小さなマイクロバッチサイズになり、効率があまり高くないかもしれません。したがって、GPUの効率的な利用を最大化する値を見つけるために実験する必要があります。これは、バブルのサイズを最小限にすることに対応する、すべての参加GPUにわたる高い並行GPU利用を可能にするためです。</p> <p data-svelte-h="svelte-qga8n1">2つのソリューショングループがあります。従来のパイプラインAPIソリューションと、ユーザーのモデルを大幅に変更する必要があるより現代的なソリューションです。</p> <p data-svelte-h="svelte-1r8j2il">従来のパイプラインAPIソリューション：</p> <ul data-svelte-h="svelte-1m54oo0"><li>PyTorch</li> <li>DeepSpeed</li> <li>Megatron-LM</li></ul> <p data-svelte-h="svelte-x7onsk">現代的なソリューション：</p> <ul data-svelte-h="svelte-n2xihd"><li>Varuna</li> <li>Sagemaker</li></ul> <p data-svelte-h="svelte-1iww0fh">従来のパイプラインAPIソリューションの問題点：</p> <ul data-svelte-h="svelte-1fhhlbg"><li>モデルをかなり変更する必要があるため、Pipelineはモジュールの通常のフローを<code>nn.Sequential</code>シーケンスに再書き込む必要があり、モデルの設計を変更することが必要です。</li> <li>現在、Pipeline APIは非常に制限的です。最初のパイプラインステージに渡されるPython変数のセットがある場合、回避策を見つける必要があります。現在、パイプラインインターフェースでは、唯一のテンソルまたはテンソルのタプルを入力と出力として要求しています。これらのテンソルはバッチサイズを最初の次元として持っている必要があります。パイプラインはミニバッチをマイクロバッチに分割します。可能な改善点については、こちらの議論が行われています：<a href="https://github.com/pytorch/pytorch/pull/50693" rel="nofollow">https://github.com/pytorch/pytorch/pull/50693</a></li> <li>パイプステージのレベルでの条件付き制御フローは不可能です。例えば、T5のようなエンコーダーデコーダーモデルは、条件付きエンコーダーステージを処理するために特別な回避策が必要です。</li> <li>各レイヤーを配置する必要があるため、1つのモデルの出力が他のモデルの入力になるようにします。</li></ul> <p data-svelte-h="svelte-1by8oug">VarunaとSageMakerとの実験はまだ行っていませんが、彼らの論文によれば、上記で述べた問題のリストを克服し、ユーザーのモデルにははるかに小さな変更しか必要としないと報告されています。</p> <p data-svelte-h="svelte-1ddoxaq">実装：</p> <ul data-svelte-h="svelte-aek6za"><li><a href="https://pytorch.org/docs/stable/pipeline.html" rel="nofollow">Pytorch</a> (initial support in pytorch-1.8, and progressively getting improved in 1.9 and more so in 1.10). Some <a href="https://github.com/pytorch/pytorch/blob/master/benchmarks/distributed/pipeline/pipe.py" rel="nofollow">examples</a></li> <li><a href="https://www.deepspeed.ai/tutorials/pipeline/" rel="nofollow">DeepSpeed</a></li> <li><a href="https://github.com/NVIDIA/Megatron-LM" rel="nofollow">Megatron-LM</a> has an internal implementation - no API.</li> <li><a href="https://github.com/microsoft/varuna" rel="nofollow">Varuna</a></li> <li><a href="https://arxiv.org/abs/2111.05972" rel="nofollow">SageMaker</a> - this is a proprietary solution that can only be used on AWS.</li> <li><a href="https://github.com/tunib-ai/oslo" rel="nofollow">OSLO</a> - この実装は、Hugging Face Transformersに基づいています。</li></ul> <p data-svelte-h="svelte-1gyn38w">🤗 Transformersのステータス: この執筆時点では、いずれのモデルも完全なPP（パイプライン並列処理）をサポートしていません。GPT2モデルとT5モデルは単純なMP（モデル並列処理）サポートを持っています。主な障害は、モデルを<code>nn.Sequential</code>に変換できず、すべての入力がテンソルである必要があることです。現在のモデルには、変換を非常に複雑にする多くの機能が含まれており、これらを削除する必要があります。</p> <p data-svelte-h="svelte-1oz3i0j">他のアプローチ：</p> <p data-svelte-h="svelte-yyebt8">DeepSpeed、Varuna、およびSageMakerは、<a href="https://docs.aws.amazon.com/sagemaker/latest/dg/model-parallel-core-features.html" rel="nofollow">交互にパイプラインを実行</a>するコンセプトを使用しています。ここでは、バックワードパスを優先させてバブル（アイドル時間）をさらに最小限に抑えます。</p> <p data-svelte-h="svelte-1so65ff">Varunaは、最適なスケジュールを発見するためにシミュレーションを使用してスケジュールをさらに改善しようとします。</p> <p data-svelte-h="svelte-1569xz">OSLOは、<code>nn.Sequential</code>の変換なしでTransformersに基づくパイプライン並列処理を実装しています。</p>  <h2 class="relative group"><a id="tensor-parallelism" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#tensor-parallelism"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Tensor Parallelism</span></h2> <p data-svelte-h="svelte-9pb2dz">テンソル並列処理では、各GPUがテンソルのスライスのみを処理し、全体が必要な操作のためにのみ完全なテンソルを集約します。</p> <p data-svelte-h="svelte-69l0ue">このセクションでは、<a href="https://github.com/NVIDIA/Megatron-LM" rel="nofollow">Megatron-LM</a>論文からのコンセプトと図を使用します：<a href="https://arxiv.org/abs/2104.04473" rel="nofollow">GPUクラスタでの効率的な大規模言語モデルトレーニング</a>。</p> <p data-svelte-h="svelte-12qk9af">どのトランスフォーマの主要な構築要素は、完全に接続された<code>nn.Linear</code>に続く非線形アクティベーション<code>GeLU</code>です。</p> <p data-svelte-h="svelte-1is9b6u">Megatronの論文の表記法に従って、行列の乗算部分を<code>Y = GeLU(XA)</code>と書くことができます。ここで、<code>X</code>と<code>Y</code>は入力ベクトルと出力ベクトルで、<code>A</code>は重み行列です。</p> <p data-svelte-h="svelte-bjqcm4">行列の計算を行列形式で見ると、行列乗算を複数のGPUで分割できる方法が簡単に理解できます：
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel_gemm.png" alt="Parallel GEMM"></p> <p data-svelte-h="svelte-wd2okf">重み行列<code>A</code>を<code>N</code>個のGPUに対して列ごとに分割し、並列で行列乗算<code>XA_1</code>から<code>XA_n</code>を実行すると、<code>N</code>個の出力ベクトル<code>Y_1、Y_2、...、Y_n</code>が得られ、それらを独立して<code>GeLU</code>に供給できます：
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-independent-gelu.png" alt="独立したGeLU"></p> <p data-svelte-h="svelte-17wn2am">この原理を使用して、最後まで同期が必要ないまま、任意の深さのMLPを更新できます。Megatron-LMの著者はそのための有用なイラストを提供しています：
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel_shard_processing.png" alt="並列シャード処理"></p> <p data-svelte-h="svelte-v5hz18">マルチヘッドアテンションレイヤーを並列化することはさらに簡単です。それらは既に複数の独立したヘッドを持っているため、本質的に並列です！
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-tp-parallel_self_attention.png" alt="並列セルフアテンション"></p> <p data-svelte-h="svelte-1ntqvm3">特別な考慮事項：TPには非常に高速なネットワークが必要であり、したがって1つのノードを超えてTPを実行しないことがお勧めされません。実際には、1つのノードに4つのGPUがある場合、最大のTP度数は4です。TP度数8が必要な場合は、少なくとも8つのGPUを持つノードを使用する必要があります。</p> <p data-svelte-h="svelte-ii1wqh">このセクションは、元のより詳細な<a href="https://github.com/huggingface/transformers/issues/10321#issuecomment-783543530" rel="nofollow">TPの概要</a>に基づいています。
by <a href="https://github.com/anton-l" rel="nofollow">@anton-l</a>。</p> <p data-svelte-h="svelte-1xslm35">SageMakerは、より効率的な処理のためにTPとDPを組み合わせて使用します。</p> <p data-svelte-h="svelte-sw9zg7">代替名：</p> <ul data-svelte-h="svelte-16pqr2f"><li><a href="https://github.com/microsoft/DeepSpeed" rel="nofollow">DeepSpeed</a>はこれを「テンソルスライシング」と呼びます。詳細は<a href="https://www.deepspeed.ai/training/#model-parallelism" rel="nofollow">DeepSpeedの特徴</a>をご覧ください。</li></ul> <p data-svelte-h="svelte-1t1nzmn">実装例:</p> <ul data-svelte-h="svelte-1o6x54n"><li><a href="https://github.com/NVIDIA/Megatron-LM" rel="nofollow">Megatron-LM</a>には、モデル固有の内部実装があります。</li> <li><a href="https://github.com/tunib-ai/parallelformers" rel="nofollow">parallelformers</a>（現時点では推論のみ）。</li> <li><a href="https://arxiv.org/abs/2111.05972" rel="nofollow">SageMaker</a> - これはAWSでのみ使用できるプロプライエタリなソリューションです。</li> <li><a href="https://github.com/tunib-ai/oslo" rel="nofollow">OSLO</a>には、Transformersに基づいたテンソル並列実装があります。</li></ul> <p data-svelte-h="svelte-cztbby">🤗 Transformersの状況:</p> <ul data-svelte-h="svelte-e2tlha"><li>コア: まだコアには実装されていません。</li> <li>ただし、推論が必要な場合、<a href="https://github.com/tunib-ai/parallelformers" rel="nofollow">parallelformers</a>はほとんどのモデルに対してサポートを提供します。これがコアに実装されるまで、これを使用できます。そして、トレーニングモードもサポートされることを期待しています。</li> <li>Deepspeed-Inferenceでは、BERT、GPT-2、およびGPT-NeoモデルをCUDAカーネルベースの高速推論モードでサポートしています。詳細は<a href="https://www.deepspeed.ai/tutorials/inference-tutorial/" rel="nofollow">こちら</a>をご覧ください。</li></ul>  <h2 class="relative group"><a id="dppp" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#dppp"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>DP+PP</span></h2> <p data-svelte-h="svelte-1ft649q">DeepSpeedの<a href="https://www.deepspeed.ai/tutorials/pipeline/" rel="nofollow">パイプラインチュートリアル</a>からの次の図は、DPをPPと組み合わせる方法を示しています。</p> <p data-svelte-h="svelte-qmcl9c"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-zero-dp-pp.png" alt="dp-pp-2d"></p> <p data-svelte-h="svelte-1kdddli">ここで重要なのは、DPランク0がGPU2を見えなくし、DPランク1がGPU3を見えなくすることです。DPにとって、存在するのはGPU 0 と 1 のみで、それらの2つのGPUのようにデータを供給します。GPU0はPPを使用してGPU2に一部の負荷を「秘密裏に」オフロードし、GPU1も同様にGPU3を支援に引き入れます。</p> <p data-svelte-h="svelte-6mun4">各次元には少なくとも2つのGPUが必要ですので、ここでは少なくとも4つのGPUが必要です。</p> <p data-svelte-h="svelte-1t1nzmn">実装例:</p> <ul data-svelte-h="svelte-1ry6c7"><li><a href="https://github.com/microsoft/DeepSpeed" rel="nofollow">DeepSpeed</a></li> <li><a href="https://github.com/NVIDIA/Megatron-LM" rel="nofollow">Megatron-LM</a></li> <li><a href="https://github.com/microsoft/varuna" rel="nofollow">Varuna</a></li> <li><a href="https://arxiv.org/abs/2111.05972" rel="nofollow">SageMaker</a></li> <li><a href="https://github.com/tunib-ai/oslo" rel="nofollow">OSLO</a></li></ul> <p data-svelte-h="svelte-1cmjcnt">🤗 Transformersの状況: まだ実装されていません</p>  <h2 class="relative group"><a id="dppptp" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#dppptp"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>DP+PP+TP</span></h2> <p data-svelte-h="svelte-1qmo80m">さらに効率的なトレーニングを行うために、3Dパラレリズムを使用し、PPをTPとDPと組み合わせます。これは次の図で示されています。</p> <p data-svelte-h="svelte-18rdwc7"><img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/parallelism-deepspeed-3d.png" alt="dp-pp-tp-3d"></p> <p data-svelte-h="svelte-6bvqnr">この図は<a href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/" rel="nofollow">3Dパラレリズム：兆パラメータモデルへのスケーリング</a>というブログ投稿から取得されたもので、おすすめの読み物です。</p> <p data-svelte-h="svelte-1ctxp2k">各次元には少なくとも2つのGPUが必要ですので、ここでは少なくとも8つのGPUが必要です。</p> <p data-svelte-h="svelte-1t1nzmn">実装例:</p> <ul data-svelte-h="svelte-nixdp7"><li><a href="https://github.com/microsoft/DeepSpeed" rel="nofollow">DeepSpeed</a> - DeepSpeedには、さらに効率的なDPであるZeRO-DPと呼ばれるものも含まれています。</li> <li><a href="https://github.com/NVIDIA/Megatron-LM" rel="nofollow">Megatron-LM</a></li> <li><a href="https://github.com/microsoft/varuna" rel="nofollow">Varuna</a></li> <li><a href="https://arxiv.org/abs/2111.05972" rel="nofollow">SageMaker</a></li> <li><a href="https://github.com/tunib-ai/oslo" rel="nofollow">OSLO</a></li></ul> <p data-svelte-h="svelte-1deyqj7">🤗 Transformersの状況: まだ実装されていません。PPとTPがないため。</p>  <h2 class="relative group"><a id="zero-dppptp" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#zero-dppptp"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>ZeRO DP+PP+TP</span></h2> <p data-svelte-h="svelte-1sixjhq">DeepSpeedの主要な機能の1つはZeROで、これはDPの拡張機能です。これについてはすでに「ZeROデータ並列化」で説明されています。通常、これは単独で動作する機能で、PPやTPは必要ありません。しかし、PPとTPと組み合わせることもできます。</p> <p data-svelte-h="svelte-dvn5ga">ZeRO-DPがPPと組み合わされる場合、通常はZeROステージ1（オプティマイザーシャーディング）のみが有効になります。</p> <p data-svelte-h="svelte-18c9zhu">ZeROステージ2（勾配シャーディング）をパイプライン並列化と組み合わせて使用する理論的な可能性はありますが、性能に悪影響を及ぼします。各マイクロバッチごとに勾配をシャーディングする前に、勾配を集約するための追加のリダクションスキャッター集計が必要で、通信オーバーヘッドが発生する可能性があります。パイプライン並列化の性質上、小さなマイクロバッチが使用され、計算の集中度（マイクロバッチサイズ）をバランスにかけ、パイプラインバブル（マイクロバッチ数）を最小限に抑えることに焦点が当てられています。したがって、これらの通信コストは影響を及ぼすでしょう。</p> <p data-svelte-h="svelte-hdy6eh">さらに、PPには通常よりも少ない層が含まれており、メモリの節約はそれほど大きくありません。PPは既に勾配サイズを「1/PP」に削減するため、勾配シャーディングの節約は純粋なDPよりもはるかに重要ではありません。</p> <p data-svelte-h="svelte-mqrqtn">ZeROステージ3も同様の理由で適していません - より多くのノード間通信が必要です。</p> <p data-svelte-h="svelte-1b2fyjn">そして、ZeROを持っているので、もう一つの利点はZeRO-Offloadです。これはステージ1オプティマイザーステートをCPUにオフロードできます。</p> <p data-svelte-h="svelte-1t1nzmn">実装例:</p> <ul data-svelte-h="svelte-1cblhzj"><li><a href="https://github.com/microsoft/Megatron-DeepSpeed" rel="nofollow">Megatron-DeepSpeed</a>と<a href="https://github.com/bigscience-workshop/Megatron-DeepSpeed" rel="nofollow">BigScienceからのMegatron-Deepspeed</a>は、前者のリポジトリのフォークです。</li> <li><a href="https://github.com/tunib-ai/oslo" rel="nofollow">OSLO</a></li></ul> <p data-svelte-h="svelte-1vvympt">重要な論文:</p> <ul data-svelte-h="svelte-1czeqoh"><li><a href="https://arxiv.org/abs/2201.11990" rel="nofollow">DeepSpeedとMegatronを使用したMegatron-Turing NLG 530Bのトレーニング</a></li></ul> <p data-svelte-h="svelte-1deyqj7">🤗 Transformersの状況: まだ実装されていません。PPとTPがないため。</p>  <h2 class="relative group"><a id="flexflow" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#flexflow"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>FlexFlow</span></h2> <p data-svelte-h="svelte-hf9mbc"><a href="https://github.com/flexflow/FlexFlow" rel="nofollow">FlexFlow</a>は、わずかに異なるアプローチで並列化の問題を解決します。</p> <p data-svelte-h="svelte-1ouo6uy">論文: <a href="https://arxiv.org/abs/1807.05358" rel="nofollow">Zhihao Jia、Matei Zaharia、Alex Aikenによる “Deep Neural Networksのデータとモデルの並列化を超えて”</a></p> <p data-svelte-h="svelte-1cr6l18">FlexFlowは、サンプル-オペレータ-属性-パラメータの4D並列化を行います。</p> <ol data-svelte-h="svelte-ipafw6"><li>サンプル = データ並列化（サンプル単位の並列化）</li> <li>オペレータ = 単一の操作をいくつかのサブ操作に並列化</li> <li>属性 = データ並列化（長さ方向の並列化）</li> <li>パラメータ = モデル並列化（次元に関係なく、水平または垂直）</li></ol> <p data-svelte-h="svelte-2h3m3n">例:</p> <ul data-svelte-h="svelte-7owhqp"><li>サンプル</li></ul> <p data-svelte-h="svelte-78y6sq">シーケンス長512の10バッチを考えてみましょう。これらをサンプル次元で2つのデバイスに並列化すると、10 x 512が5 x 2 x 512になります。</p> <ul data-svelte-h="svelte-nr15rw"><li>オペレータ</li></ul> <p data-svelte-h="svelte-yys12u">層正規化を行う場合、まずstdを計算し、次にmeanを計算し、データを正規化できます。オペレータの並列化により、stdとmeanを並列に計算できます。したがって、オペレータ次元で2つのデバイス（cuda:0、cuda:1）に並列化すると、最初に入力データを両方のデバイスにコピーし、cuda:0でstdを計算し、cuda:1でmeanを同時に計算します。</p> <ul data-svelte-h="svelte-txq006"><li>属性</li></ul> <p data-svelte-h="svelte-1i6ea3v">10バッチの512長があります。これらを属性次元で2つのデバイスに並列化すると、10 x 512が10 x 2 x 256になります。</p> <ul data-svelte-h="svelte-1ruobph"><li>パラメータ</li></ul> <p data-svelte-h="svelte-1dun2jw">これはテンソルモデルの並列化または単純な層ごとのモデルの並列化と似ています。</p> <p data-svelte-h="svelte-11zcqf6">このフレームワークの重要性は、（1）GPU/TPU/CPU対（2）RAM/DRAM対（3）高速内部接続/低速外部接続などのリソースを取り、これらすべてをアルゴリズムによって自動的に最適化することです。どの並列化をどこで使用するかをアルゴリズム的に決定します。</p> <p data-svelte-h="svelte-183itn7">非常に重要な側面の1つは、FlexFlowは静的で固定のワークロードを持つモデルのために設計されており、動的な動作を持つモデルはイテレーションごとに異なる並列化戦略を好む場合があることです。</p> <p data-svelte-h="svelte-1pl4g43">したがって、このフレームワークの約束は非常に魅力的です。選択したクラスタで30分間のシミュレーションを実行し、この特定の環境を最適に利用するための最良の戦略を提供します。部分を追加/削除/置換すると、それに対して実行して再最適化プランを作成します。その後、トレーニングできます。異なるセットアップには独自の最適化があります。</p> <p data-svelte-h="svelte-1dwdxtf">🤗 Transformersの現在の状況: まだ統合されていません。すでに<a href="https://github.com/huggingface/transformers/blob/master/src/transformers/utils/fx.py" rel="nofollow">transformers.utils.fx</a>を使用してモデルがFXトレース可能であるため、FlexFlowを動作させるために必要な手順を誰かが見つける必要があります。</p>  <h2 class="relative group"><a id="which-strategy-to-use-when" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#which-strategy-to-use-when"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Which Strategy To Use When</span></h2> <p data-svelte-h="svelte-orwa44">ここでは、どの並列化戦略をいつ使用するかの非常におおまかなアウトラインを示します。各リストの最初が通常よりも速いことが一般的です。</p> <p data-svelte-h="svelte-x7ybmv"><strong>⇨ 単一GPU</strong></p> <ul data-svelte-h="svelte-1cwqz5w"><li><p>モデルが単一GPUに収まる場合：</p> <ol><li>通常の使用</li></ol></li> <li><p>モデルが単一GPUに収まらない場合：</p> <ol><li>ZeRO + CPUをオフロードし、オプションでNVMeをオフロード</li> <li>上記に加えて、最大のレイヤーが単一GPUに収まらない場合、<a href="https://deepspeed.readthedocs.io/en/latest/zero3.html#memory-centric-tiling" rel="nofollow">Memory Centric Tiling</a>（詳細は以下参照）を有効化</li></ol></li> <li><p>最大のレイヤーが単一GPUに収まらない場合：</p> <ol><li>ZeROを使用しない場合 - TPを有効化する必要があります。なぜなら、PPだけでは収めることができないからです。</li> <li>ZeROを使用する場合は、上記の「単一GPU」のエントリと同じものを参照してください</li></ol></li></ul> <p data-svelte-h="svelte-6aq26r"><strong>⇨ 単一ノード/マルチGPU</strong></p> <ul data-svelte-h="svelte-1vipcvs"><li><p>モデルが単一GPUに収まる場合：</p> <ol><li>DDP - 分散データ並列</li> <li>ZeRO - 状況と使用される構成に依存して速いかどうかが異なることがあります</li></ol></li> <li><p>モデルが単一GPUに収まらない場合：</p> <ol><li><p>PP</p></li> <li><p>ZeRO</p></li> <li><p>TP</p> <p>非常に高速なノード内接続がNVLINKまたはNVSwitchである場合、これらのすべてはほとんど同等の性能です。これらがない場合、PPはTPまたはZeROよりも速くなります。TPの度合いも違いを生じるかもしれません。特定のセットアップで勝者を見つけるために実験するのが最善です。</p> <p>TPはほとんど常に単一ノード内で使用されます。つまり、TPサイズ &lt;= ノードあたりのGPUです。</p></li></ol></li> <li><p>最大のレイヤーが単一GPUに収まらない場合：</p> <ol><li>ZeROを使用しない場合 - TPを使用する必要があります。なぜなら、PPだけでは収めることができないからです。</li> <li>ZeROを使用する場合は、上記の「単一GPU」のエントリと同じものを参照してください</li></ol></li></ul> <p data-svelte-h="svelte-13kh319"><strong>⇨ マルチノード/マルチGPU</strong></p> <ul data-svelte-h="svelte-1yau8ud"><li><p>高速なノード間接続がある場合：</p> <ol><li>ZeRO - モデルへのほとんどの変更が不要です</li> <li>PP+TP+DP - 通信が少なく、モデルに大規模な変更が必要です</li></ol></li> <li><p>遅いノード間接続があり、GPUメモリが少ない場合：</p> <ol><li>DP+PP+TP+ZeRO-1</li></ol></li></ul>  <p></p> 
			
			<script>
				{
					__sveltekit_htmg3a = {
						assets: "/huggingface-transformers/en",
						base: "/huggingface-transformers/en",
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("/huggingface-transformers/en/_app/immutable/entry/start.bc37dae2.js"),
						import("/huggingface-transformers/en/_app/immutable/entry/app.5e64da0e.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 120],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		
