<meta charset="utf-8" /><meta name="hf:doc:metadata" content="{&quot;title&quot;:&quot;Efficient Inference on a Multiple GPUs&quot;,&quot;local&quot;:&quot;efficient-inference-on-a-multiple-gpus&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Flash Attention 2&quot;,&quot;local&quot;:&quot;flash-attention-2&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;BetterTransformer&quot;,&quot;local&quot;:&quot;bettertransformer&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Decoder models&quot;,&quot;local&quot;:&quot;decoder-models&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Encoder Models&quot;,&quot;local&quot;:&quot;encoder-models&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3}],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Advanced usage: mixing FP4 (or Int8) and BetterTransformer&quot;,&quot;local&quot;:&quot;advanced-usage-mixing-fp4-or-int8-and-bettertransformer&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2}],&quot;depth&quot;:1}">
		<link href="/docs/transformers/main/ja/_app/immutable/assets/0.e3b0c442.css" rel="modulepreload">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/entry/start.e4e638ff.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/chunks/scheduler.9bc65507.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/chunks/singletons.1a8126ff.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/chunks/index.3b203c72.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/chunks/paths.b54ca2a4.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/entry/app.9eb58bd7.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/chunks/index.707bf1b6.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/nodes/0.aec38eb9.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/chunks/each.e59479a4.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/nodes/114.97b9d62a.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/chunks/Tip.c2ecdbf4.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/chunks/CodeBlock.54a9f38d.js">
		<link rel="modulepreload" href="/docs/transformers/main/ja/_app/immutable/chunks/Heading.342b1fa6.js"><!-- HEAD_svelte-u9bgzb_START --><meta name="hf:doc:metadata" content="{&quot;title&quot;:&quot;Efficient Inference on a Multiple GPUs&quot;,&quot;local&quot;:&quot;efficient-inference-on-a-multiple-gpus&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Flash Attention 2&quot;,&quot;local&quot;:&quot;flash-attention-2&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2},{&quot;title&quot;:&quot;BetterTransformer&quot;,&quot;local&quot;:&quot;bettertransformer&quot;,&quot;sections&quot;:[{&quot;title&quot;:&quot;Decoder models&quot;,&quot;local&quot;:&quot;decoder-models&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3},{&quot;title&quot;:&quot;Encoder Models&quot;,&quot;local&quot;:&quot;encoder-models&quot;,&quot;sections&quot;:[],&quot;depth&quot;:3}],&quot;depth&quot;:2},{&quot;title&quot;:&quot;Advanced usage: mixing FP4 (or Int8) and BetterTransformer&quot;,&quot;local&quot;:&quot;advanced-usage-mixing-fp4-or-int8-and-bettertransformer&quot;,&quot;sections&quot;:[],&quot;depth&quot;:2}],&quot;depth&quot;:1}"><!-- HEAD_svelte-u9bgzb_END -->      <p></p>   <h1 class="relative group"><a id="efficient-inference-on-a-multiple-gpus" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#efficient-inference-on-a-multiple-gpus"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Efficient Inference on a Multiple GPUs</span></h1> <p data-svelte-h="svelte-10hzl65">ã“ã®æ–‡æ›¸ã«ã¯ã€è¤‡æ•°ã®GPUã§åŠ¹ç‡çš„ã«æ¨è«–ã‚’è¡Œã†æ–¹æ³•ã«é–¢ã™ã‚‹æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚</p>  <div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400"><p data-svelte-h="svelte-612xsh">æ³¨æ„: è¤‡æ•°ã®GPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¯ã€<a href="./perf_infer_gpu_one">å˜ä¸€ã®GPUã‚»ã‚¯ã‚·ãƒ§ãƒ³</a>ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã»ã¨ã‚“ã©ã®æˆ¦ç•¥ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ãŸã ã—ã€ã‚ˆã‚Šè‰¯ã„ä½¿ç”¨æ³•ã®ãŸã‚ã«ä½¿ç”¨ã§ãã‚‹ç°¡å˜ãªãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã«ã¤ã„ã¦ã‚‚èªè­˜ã—ã¦ãŠãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚</p></div>  <h2 class="relative group"><a id="flash-attention-2" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#flash-attention-2"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Flash Attention 2</span></h2> <p data-svelte-h="svelte-127b4hj">Flash Attention 2ã®çµ±åˆã¯ã€è¤‡æ•°ã®GPUã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ã‚‚æ©Ÿèƒ½ã—ã¾ã™ã€‚è©³ç´°ã«ã¤ã„ã¦ã¯ã€<a href="./perf_infer_gpu_one#Flash-Attention-2">å˜ä¸€ã®GPUã‚»ã‚¯ã‚·ãƒ§ãƒ³</a>ã®é©åˆ‡ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã”è¦§ãã ã•ã„ã€‚</p>  <h2 class="relative group"><a id="bettertransformer" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#bettertransformer"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>BetterTransformer</span></h2> <p data-svelte-h="svelte-1iuxtx2"><a href="https://huggingface.co/docs/optimum/bettertransformer/overview" rel="nofollow">BetterTransformer</a>ã¯ã€ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ã®é«˜é€Ÿå®Ÿè¡Œãƒ‘ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã€ãã®ä¸‹ã§Flash Attentionãªã©ã®æœ€é©åŒ–ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚</p> <p data-svelte-h="svelte-xfb372">BetterTransformerã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€GPUãŠã‚ˆã³è¤‡æ•°GPUã§ã®é«˜é€Ÿæ¨è«–ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚</p>  <div class="course-tip  bg-gradient-to-br dark:bg-gradient-to-r before:border-green-500 dark:before:border-green-800 from-green-50 dark:from-gray-900 to-white dark:to-gray-950 border border-green-50 text-green-700 dark:text-gray-400"><p data-svelte-h="svelte-1kooxcx">Flash Attentionã¯ã€fp16ã¾ãŸã¯bf16 dtypeã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã«ã®ã¿ä½¿ç”¨ã§ãã¾ã™ã€‚BetterTransformerã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆ‡ãªdtypeã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚</p></div>  <h3 class="relative group"><a id="decoder-models" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#decoder-models"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Decoder models</span></h3> <p data-svelte-h="svelte-1gojc9b">ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã€ç‰¹ã«ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆGPTã€T5ã€Llamaãªã©ï¼‰ã®å ´åˆã€BetterTransformer APIã¯ã™ã¹ã¦ã®æ³¨æ„æ“ä½œã‚’<a href="https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention" rel="nofollow"><code>torch.nn.functional.scaled_dot_product_attention</code>ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼</a>ï¼ˆSDPAï¼‰ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã‚Œã¯PyTorch 2.0ä»¥é™ã§ã®ã¿ä½¿ç”¨å¯èƒ½ã§ã™ã€‚</p> <p data-svelte-h="svelte-1mme928">ãƒ¢ãƒ‡ãƒ«ã‚’BetterTransformerã«å¤‰æ›ã™ã‚‹ã«ã¯ï¼š</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>)
<span class="hljs-comment"># convert the model to BetterTransformer</span>
model.to_bettertransformer()

<span class="hljs-comment"># Use it for training or inference</span><!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-188y3vx">SDPAã¯ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚„å•é¡Œã®ã‚µã‚¤ã‚ºãªã©ã®ç‰¹å®šã®è¨­å®šã§<a href="https://arxiv.org/abs/2205.14135" rel="nofollow">Flash Attention</a>ã‚«ãƒ¼ãƒãƒ«ã‚’å‘¼ã³å‡ºã™ã“ã¨ã‚‚ã§ãã¾ã™ã€‚Flash Attentionã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹ã€ç‰¹å®šã®è¨­å®šï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã€å•é¡Œã®ã‚µã‚¤ã‚ºï¼‰ã§åˆ©ç”¨å¯èƒ½ã‹ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€<a href="https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel" rel="nofollow"><code>torch.backends.cuda.sdp_kernel</code></a>ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START -->import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/opt-350m&quot;)
model = AutoModelForCausalLM.from_pretrained(&quot;facebook/opt-350m&quot;).to(&quot;cuda&quot;)
# convert the model to BetterTransformer
model.to_bettertransformer()

input_text = &quot;Hello my dog is cute and&quot;
inputs = tokenizer(input_text, return_tensors=&quot;pt&quot;).to(&quot;cuda&quot;)

<span class="hljs-addition">+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):</span>
    outputs = model.generate(**inputs)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-198zu50">ã‚‚ã—ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯ã§æ¬¡ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚ŒãŸå ´åˆï¼š</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START -->RuntimeError: No available kernel.  Aborting execution.<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-9agg8u">å½“æ—¥ã€Flash Attentionã®ã‚«ãƒãƒ¬ãƒƒã‚¸ãŒåºƒç¯„å›²ã§ã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹PyTorch Nightlyãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’è©¦ã™ã‚ˆã†ã«ãŠå‹§ã‚ã—ã¾ã™ã€‚</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START -->pip3 install -U --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118<!-- HTML_TAG_END --></pre></div> <p data-svelte-h="svelte-gwrgf0"><a href="https://pytorch.org/blog/out-of-the-box-acceleration/" rel="nofollow">ã“ã®ãƒ–ãƒ­ã‚°æŠ•ç¨¿</a>ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã€BetterTransformer + SDPA APIã§å¯èƒ½ãªã“ã¨ã«ã¤ã„ã¦è©³ã—ãå­¦ã³ã¾ã—ã‚‡ã†ã€‚</p>  <h3 class="relative group"><a id="encoder-models" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#encoder-models"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Encoder Models</span></h3> <p data-svelte-h="svelte-nvioyf">æ¨è«–ä¸­ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€BetterTransformerã¯ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®forwardå‘¼ã³å‡ºã—ã‚’ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®<a href="https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html" rel="nofollow"><code>torch.nn.TransformerEncoderLayer</code></a>ã®ç›¸å½“ã™ã‚‹ã‚‚ã®ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é«˜é€Ÿå®Ÿè£…ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚</p> <p data-svelte-h="svelte-10v3lf3"><code>torch.nn.TransformerEncoderLayer</code>ã®é«˜é€Ÿå®Ÿè£…ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã€ä»£ã‚ã‚Šã«<code>torch.nn.functional.scaled_dot_product_attention</code>ã«ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒã‚¹ãƒˆã•ã‚ŒãŸãƒ†ãƒ³ã‚½ãƒ«ã‚’æ´»ç”¨ã—ãªã„Flash Attentionã¾ãŸã¯Memory-Efficient Attentionã®èåˆã‚«ãƒ¼ãƒãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚</p> <p data-svelte-h="svelte-1bfu1uk">BetterTransformerã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€ã“ã®<a href="https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2" rel="nofollow">ãƒ–ãƒ­ã‚°æŠ•ç¨¿</a>ã‚’ã”è¦§ã„ãŸã ã‘ã¾ã™ã€‚ã¾ãŸã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ç”¨ã®BetterTransformerã«ã¤ã„ã¦ã¯ã€ã“ã®<a href="https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/" rel="nofollow">ãƒ–ãƒ­ã‚°</a>ã§è©³ã—ãå­¦ã¶ã“ã¨ãŒã§ãã¾ã™ã€‚</p>  <h2 class="relative group"><a id="advanced-usage-mixing-fp4-or-int8-and-bettertransformer" class="header-link block pr-1.5 text-lg no-hover:hidden with-hover:absolute with-hover:p-1.5 with-hover:opacity-0 with-hover:group-hover:opacity-100 with-hover:right-full" href="#advanced-usage-mixing-fp4-or-int8-and-bettertransformer"><span><svg class="" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 256"><path d="M167.594 88.393a8.001 8.001 0 0 1 0 11.314l-67.882 67.882a8 8 0 1 1-11.314-11.315l67.882-67.881a8.003 8.003 0 0 1 11.314 0zm-28.287 84.86l-28.284 28.284a40 40 0 0 1-56.567-56.567l28.284-28.284a8 8 0 0 0-11.315-11.315l-28.284 28.284a56 56 0 0 0 79.196 79.197l28.285-28.285a8 8 0 1 0-11.315-11.314zM212.852 43.14a56.002 56.002 0 0 0-79.196 0l-28.284 28.284a8 8 0 1 0 11.314 11.314l28.284-28.284a40 40 0 0 1 56.568 56.567l-28.285 28.285a8 8 0 0 0 11.315 11.314l28.284-28.284a56.065 56.065 0 0 0 0-79.196z" fill="currentColor"></path></svg></span></a> <span>Advanced usage: mixing FP4 (or Int8) and BetterTransformer</span></h2> <p data-svelte-h="svelte-18z05f5">ãƒ¢ãƒ‡ãƒ«ã®æœ€è‰¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¾—ã‚‹ãŸã‚ã«ã€ä¸Šè¨˜ã§èª¬æ˜ã—ãŸç•°ãªã‚‹æ–¹æ³•ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€FP4ãƒŸãƒƒã‚¯ã‚¹ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³æ¨è«–+Flash Attentionã‚’ä½¿ç”¨ã—ãŸBetterTransformerã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚</p> <div class="code-block relative"><div class="absolute top-2.5 right-4"><button class="inline-flex items-center relative text-sm focus:text-green-500 cursor-pointer focus:outline-none transition duration-200 ease-in-out opacity-0 mx-0.5   text-gray-600 " title="code excerpt" type="button"><svg class="" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" fill="currentColor" focusable="false" role="img" width="1em" height="1em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 32 32"><path d="M28,10V28H10V10H28m0-2H10a2,2,0,0,0-2,2V28a2,2,0,0,0,2,2H28a2,2,0,0,0,2-2V10a2,2,0,0,0-2-2Z" transform="translate(0)"></path><path d="M4,18H2V4A2,2,0,0,1,4,2H18V4H4Z" transform="translate(0)"></path><rect fill="none" width="32" height="32"></rect></svg> <div class="absolute pointer-events-none transition-opacity bg-black text-white py-1 px-2 leading-tight rounded font-normal shadow left-1/2 top-full transform -translate-x-1/2 translate-y-2 opacity-0"><div class="absolute bottom-full left-1/2 transform -translate-x-1/2 w-0 h-0 border-black border-4 border-t-0" style="border-left-color: transparent; border-right-color: transparent; "></div> Copied</div></button></div> <pre class=""><!-- HTML_TAG_START --><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=<span class="hljs-literal">True</span>,
    bnb_4bit_compute_dtype=torch.float16
)

tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>)
model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&quot;facebook/opt-350m&quot;</span>, quantization_config=quantization_config)

input_text = <span class="hljs-string">&quot;Hello my dog is cute and&quot;</span>
inputs = tokenizer(input_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>).to(<span class="hljs-string">&quot;cuda&quot;</span>)

<span class="hljs-keyword">with</span> torch.backends.cuda.sdp_kernel(enable_flash=<span class="hljs-literal">True</span>, enable_math=<span class="hljs-literal">False</span>, enable_mem_efficient=<span class="hljs-literal">False</span>):
    outputs = model.generate(**inputs)

<span class="hljs-built_in">print</span>(tokenizer.decode(outputs[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))<!-- HTML_TAG_END --></pre></div>  <p></p> 
			
			<script>
				{
					__sveltekit_l6ewbf = {
						assets: "/docs/transformers/main/ja",
						base: "/docs/transformers/main/ja",
						env: {}
					};

					const element = document.currentScript.parentElement;

					const data = [null,null];

					Promise.all([
						import("/docs/transformers/main/ja/_app/immutable/entry/start.e4e638ff.js"),
						import("/docs/transformers/main/ja/_app/immutable/entry/app.9eb58bd7.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 114],
							data,
							form: null,
							error: null
						});
					});
				}
			</script>
		
